<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-06-11T00:25:30.738Z</updated>
    <generator>osmosfeed 1.10.2</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01981</id>
        <link href="http://arxiv.org/abs/2106.01981"/>
        <updated>2021-06-10T22:40:40.687Z</updated>
        <summary type="html"><![CDATA[Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1"&gt;Boris N. Oreshkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1"&gt;Florent Bocquelet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1"&gt;F&amp;#xe9;lix G. Harvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1"&gt;Bay Raitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1"&gt;Dominic Laflamme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01981</id>
        <link href="http://arxiv.org/abs/2106.01981"/>
        <updated>2021-06-10T22:40:40.536Z</updated>
        <summary type="html"><![CDATA[Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1"&gt;Boris N. Oreshkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1"&gt;Florent Bocquelet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1"&gt;F&amp;#xe9;lix G. Harvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1"&gt;Bay Raitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1"&gt;Dominic Laflamme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison for Anti-noise Robustness of Deep Learning Classification Methods on a Tiny Object Image Dataset: from Convolutional Neural Network to Visual Transformer and Performer. (arXiv:2106.01927v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01927</id>
        <link href="http://arxiv.org/abs/2106.01927"/>
        <updated>2021-06-10T22:40:40.506Z</updated>
        <summary type="html"><![CDATA[Image classification has achieved unprecedented advance with the the rapid
development of deep learning. However, the classification of tiny object images
is still not well investigated. In this paper, we first briefly review the
development of Convolutional Neural Network and Visual Transformer in deep
learning, and introduce the sources and development of conventional noises and
adversarial attacks. Then we use various models of Convolutional Neural Network
and Visual Transformer to conduct a series of experiments on the image dataset
of tiny objects (sperms and impurities), and compare various evaluation metrics
in the experimental results to obtain a model with stable performance. Finally,
we discuss the problems in the classification of tiny objects and make a
prospect for the classification of tiny objects in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Ao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hechen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1"&gt;Shuojia Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations. (arXiv:2106.01978v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01978</id>
        <link href="http://arxiv.org/abs/2106.01978"/>
        <updated>2021-06-10T22:40:40.475Z</updated>
        <summary type="html"><![CDATA[Emotion Recognition in Conversations (ERC) has gained increasing attention
for developing empathetic machines. Recently, many approaches have been devoted
to perceiving conversational context by deep learning models. However, these
approaches are insufficient in understanding the context due to lacking the
ability to extract and integrate emotional clues. In this work, we propose
novel Contextual Reasoning Networks (DialogueCRN) to fully understand the
conversational context from a cognitive perspective. Inspired by the Cognitive
Theory of Emotion, we design multi-turn reasoning modules to extract and
integrate emotional clues. The reasoning module iteratively performs an
intuitive retrieving process and a conscious reasoning process, which imitates
human unique cognitive thinking. Extensive experiments on three public
benchmark datasets demonstrate the effectiveness and superiority of the
proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dou Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Lingwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huai_X/0/1/0/all/0/1"&gt;Xiaoyong Huai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network insensitivity to parameter noise via adversarial regularization. (arXiv:2106.05009v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05009</id>
        <link href="http://arxiv.org/abs/2106.05009"/>
        <updated>2021-06-10T01:56:49.632Z</updated>
        <summary type="html"><![CDATA[Neuromorphic neural network processors, in the form of compute-in-memory
crossbar arrays of memristors, or in the form of subthreshold analog and
mixed-signal ASICs, promise enormous advantages in compute density and energy
efficiency for NN-based ML tasks. However, these technologies are prone to
computational non-idealities, due to process variation and intrinsic device
physics. This degrades the task performance of networks deployed to the
processor, by introducing parameter noise into the deployed model. While it is
possible to calibrate each device, or train networks individually for each
processor, these approaches are expensive and impractical for commercial
deployment. Alternative methods are therefore needed to train networks that are
inherently robust against parameter variation, as a consequence of network
architecture and parameters. We present a new adversarial network optimisation
algorithm that attacks network parameters during training, and promotes robust
performance during inference in the face of parameter variation. Our approach
introduces a regularization term penalising the susceptibility of a network to
weight perturbation. We compare against previous approaches for producing
parameter insensitivity such as dropout, weight smoothing and introducing
parameter noise during training. We show that our approach produces models that
are more robust to targeted parameter variation, and equally robust to random
parameter variation. Our approach finds minima in flatter locations in the
weight-loss landscape compared with other approaches, highlighting that the
networks found by our technique are less sensitive to parameter perturbation.
Our work provides an approach to deploy neural network architectures to
inference devices that suffer from computational non-idealities, with minimal
loss of performance. ...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bucher_J/0/1/0/all/0/1"&gt;Julian B&amp;#xfc;cher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faber_F/0/1/0/all/0/1"&gt;Fynn Faber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muir_D/0/1/0/all/0/1"&gt;Dylan R. Muir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Ensemble Search for Uncertainty Estimation and Dataset Shift. (arXiv:2006.08573v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08573</id>
        <link href="http://arxiv.org/abs/2006.08573"/>
        <updated>2021-06-10T01:56:49.616Z</updated>
        <summary type="html"><![CDATA[Ensembles of neural networks achieve superior performance compared to
stand-alone networks in terms of accuracy, uncertainty calibration and
robustness to dataset shift. \emph{Deep ensembles}, a state-of-the-art method
for uncertainty estimation, only ensemble random initializations of a
\emph{fixed} architecture. Instead, we propose two methods for automatically
constructing ensembles with \emph{varying} architectures, which implicitly
trade-off individual architectures' strengths against the ensemble's diversity
and exploit architectural variation as a source of diversity. On a variety of
classification tasks and modern architecture search spaces, we show that the
resulting ensembles outperform deep ensembles not only in terms of accuracy but
also uncertainty calibration and robustness to dataset shift. Our further
analysis and ablation studies provide evidence of higher ensemble diversity due
to architectural variation, resulting in ensembles that can outperform deep
ensembles, even when having weaker average base learners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1"&gt;Sheheryar Zaidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1"&gt;Arber Zela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsken_T/0/1/0/all/0/1"&gt;Thomas Elsken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1"&gt;Chris Holmes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training. (arXiv:2106.05091v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05091</id>
        <link href="http://arxiv.org/abs/2106.05091"/>
        <updated>2021-06-10T01:56:49.610Z</updated>
        <summary type="html"><![CDATA[Conveying complex objectives to reinforcement learning (RL) agents can often
be difficult, involving meticulous design of reward functions that are
sufficiently informative yet easy enough to provide. Human-in-the-loop RL
methods allow practitioners to instead interactively teach agents through
tailored feedback; however, such approaches have been challenging to scale
since human feedback is very expensive. In this work, we aim to make this
process more sample- and feedback-efficient. We present an off-policy,
interactive RL algorithm that capitalizes on the strengths of both feedback and
off-policy learning. Specifically, we learn a reward model by actively querying
a teacher's preferences between two clips of behavior and use it to train an
agent. To enable off-policy learning, we relabel all the agent's past
experience when its reward model changes. We additionally show that
pre-training our agents with unsupervised exploration substantially increases
the mileage of its queries. We demonstrate that our approach is capable of
learning tasks of higher complexity than previously considered by
human-in-the-loop methods, including a variety of locomotion and robotic
manipulation skills. We also show that our method is able to utilize real-time
human feedback to effectively prevent reward exploitation and learn new
behaviors that are difficult to specify with standard reward functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kimin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1"&gt;Laura Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MICE: A Crosslinguistic Emotion Corpus in Malay, Indonesian, Chinese and English. (arXiv:2106.04831v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04831</id>
        <link href="http://arxiv.org/abs/2106.04831"/>
        <updated>2021-06-10T01:56:49.602Z</updated>
        <summary type="html"><![CDATA[MICE is a corpus of emotion words in four languages which is currently
working progress. There are two sections to this study, Part I: Emotion word
corpus and Part II: Emotion word survey. In Part 1, the method of how the
emotion data is culled for each of the four languages will be described and
very preliminary data will be presented. In total, we identified 3,750 emotion
expressions in Malay, 6,657 in Indonesian, 3,347 in Mandarin Chinese and 8,683
in English. We are currently evaluating and double checking the corpus and
doing further analysis on the distribution of these emotion expressions. Part
II Emotion word survey involved an online language survey which collected
information on how speakers assigned the emotion words into basic emotion
categories, the rating for valence and intensity as well as biographical
information of all the respondents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chin_N/0/1/0/all/0/1"&gt;Ng Bee Chin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Susanto_Y/0/1/0/all/0/1"&gt;Yosephine Susanto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Survival Machines: Fully Parametric Survival Regression and Representation Learning for Censored Data with Competing Risks. (arXiv:2003.01176v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01176</id>
        <link href="http://arxiv.org/abs/2003.01176"/>
        <updated>2021-06-10T01:56:49.592Z</updated>
        <summary type="html"><![CDATA[We describe a new approach to estimating relative risks in time-to-event
prediction problems with censored data in a fully parametric manner. Our
approach does not require making strong assumptions of constant proportional
hazard of the underlying survival distribution, as required by the
Cox-proportional hazard model. By jointly learning deep nonlinear
representations of the input covariates, we demonstrate the benefits of our
approach when used to estimate survival risks through extensive experimentation
on multiple real world datasets with different levels of censoring. We further
demonstrate advantages of our model in the competing risks scenario. To the
best of our knowledge, this is the first work involving fully parametric
estimation of survival times with competing risks in the presence of censoring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagpal_C/0/1/0/all/0/1"&gt;Chirag Nagpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyu Rachel Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1"&gt;Artur Dubrawski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polynomial magic! Hermite polynomials for private data generation. (arXiv:2106.05042v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05042</id>
        <link href="http://arxiv.org/abs/2106.05042"/>
        <updated>2021-06-10T01:56:49.585Z</updated>
        <summary type="html"><![CDATA[Kernel mean embedding is a useful tool to compare probability measures.
Despite its usefulness, kernel mean embedding considers infinite-dimensional
features, which are challenging to handle in the context of differentially
private data generation. A recent work proposes to approximate the kernel mean
embedding of data distribution using finite-dimensional random features, where
the sensitivity of the features becomes analytically tractable. More
importantly, this approach significantly reduces the privacy cost, compared to
other known privatization methods (e.g., DP-SGD), as the approximate kernel
mean embedding of the data distribution is privatized only once and can then be
repeatedly used during training of a generator without incurring any further
privacy cost. However, the required number of random features is excessively
high, often ten thousand to a hundred thousand, which worsens the sensitivity
of the approximate kernel mean embedding. To improve the sensitivity, we
propose to replace random features with Hermite polynomial features. Unlike the
random features, the Hermite polynomial features are ordered, where the
features at the low orders contain more information on the distribution than
those at the high orders. Hence, a relatively low order of Hermite polynomial
features can more accurately approximate the mean embedding of the data
distribution compared to a significantly higher number of random features. As a
result, using the Hermite polynomial features, we significantly improve the
privacy-accuracy trade-off, reflected in the high quality and diversity of the
generated data, when tested on several heterogeneous tabular datasets, as well
as several image benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1"&gt;Mijung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinaroz_M/0/1/0/all/0/1"&gt;Margarita Vinaroz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charusaie_M/0/1/0/all/0/1"&gt;Mohammad-Amin Charusaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harder_F/0/1/0/all/0/1"&gt;Frederik Harder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding. (arXiv:2106.04970v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04970</id>
        <link href="http://arxiv.org/abs/2106.04970"/>
        <updated>2021-06-10T01:56:49.569Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the
online inference efficiency of the Transformer for instantaneous Grammatical
Error Correction (GEC). SAD optimizes the online inference efficiency for GEC
by two innovations: 1) it aggressively decodes as many tokens as possible in
parallel instead of always decoding only one token in each step to improve
computational parallelism; 2) it uses a shallow decoder instead of the
conventional Transformer architecture with balanced encoder-decoder depth to
reduce the computational cost during inference. Experiments in both English and
Chinese GEC benchmarks show that aggressive decoding could yield the same
predictions as greedy decoding but with a significant speedup for online
inference. Its combination with the shallow decoder could offer an even higher
online inference speedup over the powerful Transformer baseline without quality
loss. Not only does our approach allow a single model to achieve the
state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14
and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference
speedup over the Transformer-big model, but also it is easily adapted to other
languages. Our code is available at
https://github.com/AutoTemp/Shallow-Aggressive-Decoding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1"&gt;Tao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Houfeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Black-box density function estimation using recursive partitioning. (arXiv:2010.13632v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13632</id>
        <link href="http://arxiv.org/abs/2010.13632"/>
        <updated>2021-06-10T01:56:49.562Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to Bayesian inference and general Bayesian
computation that is defined through a sequential decision loop. Our method
defines a recursive partitioning of the sample space. It neither relies on
gradients nor requires any problem-specific tuning, and is asymptotically exact
for any density function with a bounded domain. The output is an approximation
to the whole density function including the normalisation constant, via
partitions organised in efficient data structures. Such approximations may be
used for evidence estimation or fast posterior sampling, but also as building
blocks to treat a larger class of estimation problems. The algorithm shows
competitive performance to recent state-of-the-art methods on synthetic and
real-world problems including parameter inference for gravitational-wave
physics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bodin_E/0/1/0/all/0/1"&gt;Erik Bodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zhenwen Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Campbell_N/0/1/0/all/0/1"&gt;Neill D. F. Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ek_C/0/1/0/all/0/1"&gt;Carl Henrik Ek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The dilemma of quantum neural networks. (arXiv:2106.04975v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.04975</id>
        <link href="http://arxiv.org/abs/2106.04975"/>
        <updated>2021-06-10T01:56:49.556Z</updated>
        <summary type="html"><![CDATA[The core of quantum machine learning is to devise quantum models with good
trainability and low generalization error bound than their classical
counterparts to ensure better reliability and interpretability. Recent studies
confirmed that quantum neural networks (QNNs) have the ability to achieve this
goal on specific datasets. With this regard, it is of great importance to
understand whether these advantages are still preserved on real-world tasks.
Through systematic numerical experiments, we empirically observe that current
QNNs fail to provide any benefit over classical learning models. Concretely,
our results deliver two key messages. First, QNNs suffer from the severely
limited effective model capacity, which incurs poor generalization on
real-world datasets. Second, the trainability of QNNs is insensitive to
regularization techniques, which sharply contrasts with the classical scenario.
These empirical results force us to rethink the role of current QNNs and to
design novel protocols for solving real-world problems with quantum advantages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yang Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinbiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuxuan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xingyao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncovering Closed-form Governing Equations of Nonlinear Dynamics from Videos. (arXiv:2106.04776v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04776</id>
        <link href="http://arxiv.org/abs/2106.04776"/>
        <updated>2021-06-10T01:56:49.550Z</updated>
        <summary type="html"><![CDATA[Distilling analytical models from data has the potential to advance our
understanding and prediction of nonlinear dynamics. Although discovery of
governing equations based on observed system states (e.g., trajectory time
series) has revealed success in a wide range of nonlinear dynamics, uncovering
the closed-form equations directly from raw videos still remains an open
challenge. To this end, we introduce a novel end-to-end unsupervised deep
learning framework to uncover the mathematical structure of equations that
governs the dynamics of moving objects in videos. Such an architecture consists
of (1) an encoder-decoder network that learns low-dimensional spatial/pixel
coordinates of the moving object, (2) a learnable Spatial-Physical
Transformation component that creates mapping between the extracted
spatial/pixel coordinates and the latent physical states of dynamics, and (3) a
numerical integrator-based sparse regression module that uncovers the
parsimonious closed-form governing equations of learned physical states and,
meanwhile, serves as a constraint to the autoencoder. The efficacy of the
proposed method is demonstrated by uncovering the governing equations of a
variety of nonlinear dynamical systems depicted by moving objects in videos.
The resulting computational framework enables discovery of parsimonious
interpretable model in a flexible and accessible sensing environment where only
videos are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luan_L/0/1/0/all/0/1"&gt;Lele Luan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepTileBars: Visualizing Term Distribution for Neural Information Retrieval. (arXiv:1811.00606v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.00606</id>
        <link href="http://arxiv.org/abs/1811.00606"/>
        <updated>2021-06-10T01:56:49.544Z</updated>
        <summary type="html"><![CDATA[Most neural Information Retrieval (Neu-IR) models derive query-to-document
ranking scores based on term-level matching. Inspired by TileBars, a classical
term distribution visualization method, in this paper, we propose a novel
Neu-IR model that handles query-to-document matching at the subtopic and higher
levels. Our system first splits the documents into topical segments,
"visualizes" the matchings between the query and the segments, and then feeds
an interaction matrix into a Neu-IR model, DeepTileBars, to obtain the final
ranking scores. DeepTileBars models the relevance signals occurring at
different granularities in a document's topic hierarchy. It better captures the
discourse structure of a document and thus the matching patterns. Although its
design and implementation are light-weight, DeepTileBars outperforms other
state-of-the-art Neu-IR models on benchmark datasets including the Text
REtrieval Conference (TREC) 2010-2012 Web Tracks and LETOR 4.0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhiwen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Grace Hui Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Kernel Representation Learning on Networks. (arXiv:2106.05057v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.05057</id>
        <link href="http://arxiv.org/abs/2106.05057"/>
        <updated>2021-06-10T01:56:49.528Z</updated>
        <summary type="html"><![CDATA[Learning representations of nodes in a low dimensional space is a crucial
task with numerous interesting applications in network analysis, including link
prediction, node classification, and visualization. Two popular approaches for
this problem are matrix factorization and random walk-based models. In this
paper, we aim to bring together the best of both worlds, towards learning node
representations. In particular, we propose a weighted matrix factorization
model that encodes random walk-based information about nodes of the network.
The benefit of this novel formulation is that it enables us to utilize kernel
functions without realizing the exact proximity matrix so that it enhances the
expressiveness of existing matrix decomposition methods with kernels and
alleviates their computational complexities. We extend the approach with a
multiple kernel learning formulation that provides the flexibility of learning
the kernel as the linear combination of a dictionary of kernels in data-driven
fashion. We perform an empirical evaluation on real-world networks, showing
that the proposed model outperforms baseline node embedding algorithms in
downstream machine learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Celikkanat_A/0/1/0/all/0/1"&gt;Abdulkadir Celikkanat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yanning Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1"&gt;Fragkiskos D. Malliaros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Framework for Clustered Federated Learning. (arXiv:2006.04088v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04088</id>
        <link href="http://arxiv.org/abs/2006.04088"/>
        <updated>2021-06-10T01:56:49.522Z</updated>
        <summary type="html"><![CDATA[We address the problem of federated learning (FL) where users are distributed
and partitioned into clusters. This setup captures settings where different
groups of users have their own objectives (learning tasks) but by aggregating
their data with others in the same cluster (same learning task), they can
leverage the strength in numbers in order to perform more efficient federated
learning. For this new framework of clustered federated learning, we propose
the Iterative Federated Clustering Algorithm (IFCA), which alternately
estimates the cluster identities of the users and optimizes model parameters
for the user clusters via gradient descent. We analyze the convergence rate of
this algorithm first in a linear model with squared loss and then for generic
strongly convex and smooth loss functions. We show that in both settings, with
good initialization, IFCA is guaranteed to converge, and discuss the optimality
of the statistical error rate. In particular, for the linear model with two
clusters, we can guarantee that our algorithm converges as long as the
initialization is slightly better than random. When the clustering structure is
ambiguous, we propose to train the models by combining IFCA with the weight
sharing technique in multi-task learning. In the experiments, we show that our
algorithm can succeed even if we relax the requirements on initialization with
random initialization and multiple restarts. We also present experimental
results showing that our algorithm is efficient in non-convex problems such as
neural networks. We demonstrate the benefits of IFCA over the baselines on
several clustered FL benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Avishek Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chung_J/0/1/0/all/0/1"&gt;Jichan Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yin_D/0/1/0/all/0/1"&gt;Dong Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interaction-Grounded Learning. (arXiv:2106.04887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04887</id>
        <link href="http://arxiv.org/abs/2106.04887"/>
        <updated>2021-06-10T01:56:49.517Z</updated>
        <summary type="html"><![CDATA[Consider a prosthetic arm, learning to adapt to its user's control signals.
We propose Interaction-Grounded Learning for this novel setting, in which a
learner's goal is to interact with the environment with no grounding or
explicit reward to optimize its policies. Such a problem evades common RL
solutions which require an explicit reward. The learning agent observes a
multidimensional context vector, takes an action, and then observes a
multidimensional feedback vector. This multidimensional feedback vector has no
explicit reward information. In order to succeed, the algorithm must learn how
to evaluate the feedback vector to discover a latent reward signal, with which
it can ground its policies without supervision. We show that in an
Interaction-Grounded Learning setting, with certain natural assumptions, a
learner can discover the latent reward and ground its policy for successful
interaction. We provide theoretical guarantees and a proof-of-concept empirical
evaluation to demonstrate the effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tengyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1"&gt;John Langford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1"&gt;Paul Mineiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1"&gt;Ida Momennejad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT. (arXiv:2106.05141v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.05141</id>
        <link href="http://arxiv.org/abs/2106.05141"/>
        <updated>2021-06-10T01:56:49.511Z</updated>
        <summary type="html"><![CDATA[The success of Neural Machine Translation (NMT) largely depends on the
availability of large bitext training corpora. Due to the lack of such large
corpora in low-resource language pairs, NMT systems often exhibit poor
performance. Extra relevant monolingual data often helps, but acquiring it
could be quite expensive, especially for low-resource languages. Moreover,
domain mismatch between bitext (train/test) and monolingual data might degrade
the performance. To alleviate such issues, we propose AUGVIC, a novel data
augmentation framework for low-resource NMT which exploits the vicinal samples
of the given bitext without using any extra monolingual data explicitly. It can
diversify the in-domain bitext data with finer level control. Through extensive
experiments on four low-resource language pairs comprising data from different
domains, we have shown that our method is comparable to the traditional
back-translation that uses extra in-domain monolingual data. When we combine
the synthetic parallel data generated from AUGVIC with the ones from the extra
monolingual data, we achieve further improvements. We show that AUGVIC helps to
attenuate the discrepancies between relevant and distant-domain monolingual
data in traditional back-translation. To understand the contributions of
different components of AUGVIC, we perform an in-depth framework analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1"&gt;Tasnim Mohiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1"&gt;M Saiful Bari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI for medical imaging: Explaining pneumothorax diagnoses with Bayesian Teaching. (arXiv:2106.04684v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04684</id>
        <link href="http://arxiv.org/abs/2106.04684"/>
        <updated>2021-06-10T01:56:49.506Z</updated>
        <summary type="html"><![CDATA[Limited expert time is a key bottleneck in medical imaging. Due to advances
in image classification, AI can now serve as decision-support for medical
experts, with the potential for great gains in radiologist productivity and, by
extension, public health. However, these gains are contingent on building and
maintaining experts' trust in the AI agents. Explainable AI may build such
trust by helping medical experts to understand the AI decision processes behind
diagnostic judgements. Here we introduce and evaluate explanations based on
Bayesian Teaching, a formal account of explanation rooted in the cognitive
science of human learning. We find that medical experts exposed to explanations
generated by Bayesian Teaching successfully predict the AI's diagnostic
decisions and are more likely to certify the AI for cases when the AI is
correct than when it is wrong, indicating appropriate trust. These results show
that Explainable AI can be used to support human-AI collaboration in medical
imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Folke_T/0/1/0/all/0/1"&gt;Tomas Folke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Scott Cheng-Hsin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_S/0/1/0/all/0/1"&gt;Sean Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafto_P/0/1/0/all/0/1"&gt;Patrick Shafto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint System-Wise Optimization for Pipeline Goal-Oriented Dialog System. (arXiv:2106.04835v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04835</id>
        <link href="http://arxiv.org/abs/2106.04835"/>
        <updated>2021-06-10T01:56:49.489Z</updated>
        <summary type="html"><![CDATA[Recent work (Takanobu et al., 2020) proposed the system-wise evaluation on
dialog systems and found that improvement on individual components (e.g., NLU,
policy) in prior work may not necessarily bring benefit to pipeline systems in
system-wise evaluation. To improve the system-wise performance, in this paper,
we propose new joint system-wise optimization techniques for the pipeline
dialog system. First, we propose a new data augmentation approach which
automates the labeling process for NLU training. Second, we propose a novel
stochastic policy parameterization with Poisson distribution that enables
better exploration and offers a principled way to compute policy gradient.
Third, we propose a reward bonus to help policy explore successful dialogs. Our
approaches outperform the competitive pipeline systems from Takanobu et al.
(2020) by big margins of 12% success rate in automatic system-wise evaluation
and of 16% success rate in human evaluation on the standard multi-domain
benchmark dataset MultiWOZ 2.1, and also outperform the recent state-of-the-art
end-to-end trained model from DSTC9.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zichuan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bowen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaodong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fractal Structure and Generalization Properties of Stochastic Optimization Algorithms. (arXiv:2106.04881v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04881</id>
        <link href="http://arxiv.org/abs/2106.04881"/>
        <updated>2021-06-10T01:56:49.483Z</updated>
        <summary type="html"><![CDATA[Understanding generalization in deep learning has been one of the major
challenges in statistical learning theory over the last decade. While recent
work has illustrated that the dataset and the training algorithm must be taken
into account in order to obtain meaningful generalization bounds, it is still
theoretically not clear which properties of the data and the algorithm
determine the generalization performance. In this study, we approach this
problem from a dynamical systems theory perspective and represent stochastic
optimization algorithms as random iterated function systems (IFS). Well studied
in the dynamical systems literature, under mild assumptions, such IFSs can be
shown to be ergodic with an invariant measure that is often supported on sets
with a fractal structure. As our main contribution, we prove that the
generalization error of a stochastic optimization algorithm can be bounded
based on the `complexity' of the fractal structure that underlies its invariant
measure. Leveraging results from dynamical systems theory, we show that the
generalization error can be explicitly linked to the choice of the algorithm
(e.g., stochastic gradient descent -- SGD), algorithm hyperparameters (e.g.,
step-size, batch-size), and the geometry of the problem (e.g., Hessian of the
loss). We further specialize our results to specific problems (e.g.,
linear/logistic regression, one hidden-layered neural networks) and algorithms
(e.g., SGD and preconditioned variants), and obtain analytical estimates for
our bound.For modern neural networks, we develop an efficient algorithm to
compute the developed bound and support our theory with various experiments on
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Camuto_A/0/1/0/all/0/1"&gt;Alexander Camuto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1"&gt;George Deligiannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert G&amp;#xfc;rb&amp;#xfc;zbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1"&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingjiong Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scale Free Adversarial Multi Armed Bandits. (arXiv:2106.04700v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04700</id>
        <link href="http://arxiv.org/abs/2106.04700"/>
        <updated>2021-06-10T01:56:49.478Z</updated>
        <summary type="html"><![CDATA[We consider the Scale-Free Adversarial Multi Armed Bandit(MAB) problem, where
the player only knows the number of arms $n$ and not the scale or magnitude of
the losses. It sees bandit feedback about the loss vectors $l_1,\dots, l_T \in
\mathbb{R}^n$. The goal is to bound its regret as a function of $n$ and
$l_1,\dots, l_T$. We design a Follow The Regularized Leader(FTRL) algorithm,
which comes with the first scale-free regret guarantee for MAB. It uses the log
barrier regularizer, the importance weighted estimator, an adaptive learning
rate, and an adaptive exploration parameter. In the analysis, we introduce a
simple, unifying technique for obtaining regret inequalities for FTRL and
Online Mirror Descent(OMD) on the probability simplex using Potential Functions
and Mixed Bregmans. We also develop a new technique for obtaining local-norm
lower bounds for Bregman Divergences, which are crucial in bandit regret
bounds. These tools could be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Putta_S/0/1/0/all/0/1"&gt;Sudeep Raja Putta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Shipra Agrawal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bi-Level Framework for Learning to Solve Combinatorial Optimization on Graphs. (arXiv:2106.04927v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04927</id>
        <link href="http://arxiv.org/abs/2106.04927"/>
        <updated>2021-06-10T01:56:49.472Z</updated>
        <summary type="html"><![CDATA[Combinatorial Optimization (CO) has been a long-standing challenging research
topic featured by its NP-hard nature. Traditionally such problems are
approximately solved with heuristic algorithms which are usually fast but may
sacrifice the solution quality. Currently, machine learning for combinatorial
optimization (MLCO) has become a trending research topic, but most existing
MLCO methods treat CO as a single-level optimization by directly learning the
end-to-end solutions, which are hard to scale up and mostly limited by the
capacity of ML models given the high complexity of CO. In this paper, we
propose a hybrid approach to combine the best of the two worlds, in which a
bi-level framework is developed with an upper-level learning method to optimize
the graph (e.g. add, delete or modify edges in a graph), fused with a
lower-level heuristic algorithm solving on the optimized graph. Such a bi-level
approach simplifies the learning on the original hard CO and can effectively
mitigate the demand for model capacity. The experiments and results on several
popular CO problems like Directed Acyclic Graph scheduling, Graph Edit Distance
and Hamiltonian Cycle Problem show its effectiveness over manually designed
heuristics and single-level learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runzhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_Z/0/1/0/all/0/1"&gt;Zhigang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Gan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiayi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Feng Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Supervised Domain Adaptation by Augmenting Pre-trained Models with Random Units. (arXiv:2106.04935v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04935</id>
        <link href="http://arxiv.org/abs/2106.04935"/>
        <updated>2021-06-10T01:56:49.467Z</updated>
        <summary type="html"><![CDATA[Neural Transfer Learning (TL) is becoming ubiquitous in Natural Language
Processing (NLP), thanks to its high performance on many tasks, especially in
low-resourced scenarios. Notably, TL is widely used for neural domain
adaptation to transfer valuable knowledge from high-resource to low-resource
domains. In the standard fine-tuning scheme of TL, a model is initially
pre-trained on a source domain and subsequently fine-tuned on a target domain
and, therefore, source and target domains are trained using the same
architecture. In this paper, we show through interpretation methods that such
scheme, despite its efficiency, is suffering from a main limitation. Indeed,
although capable of adapting to new domains, pre-trained neurons struggle with
learning certain patterns that are specific to the target domain. Moreover, we
shed light on the hidden negative transfer occurring despite the high
relatedness between source and target domains, which may mitigate the final
gain brought by transfer learning. To address these problems, we propose to
augment the pre-trained model with normalised, weighted and randomly
initialised units that foster a better adaptation while maintaining the
valuable source knowledge. We show that our approach exhibits significant
improvements to the standard fine-tuning scheme for neural domain adaptation
from the news domain to the social media domain on four NLP tasks:
part-of-speech tagging, chunking, named entity recognition and morphosyntactic
tagging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meftah_S/0/1/0/all/0/1"&gt;Sara Meftah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semmar_N/0/1/0/all/0/1"&gt;Nasredine Semmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamaazousti_Y/0/1/0/all/0/1"&gt;Youssef Tamaazousti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Essafi_H/0/1/0/all/0/1"&gt;Hassane Essafi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadat_F/0/1/0/all/0/1"&gt;Fatiha Sadat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning. (arXiv:2106.05221v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.05221</id>
        <link href="http://arxiv.org/abs/2106.05221"/>
        <updated>2021-06-10T01:56:49.460Z</updated>
        <summary type="html"><![CDATA[Graph convolutional network (GCN) has become popular in various natural
language processing (NLP) tasks with its superiority in long-term and
non-consecutive word interactions. However, existing single-hop graph reasoning
in GCN may miss some important non-consecutive dependencies. In this study, we
define the spectral graph convolutional network with the high-order dynamic
Chebyshev approximation (HDGCN), which augments the multi-hop graph reasoning
by fusing messages aggregated from direct and long-term dependencies into one
convolutional layer. To alleviate the over-smoothing in high-order Chebyshev
approximation, a multi-vote-based cross-attention (MVCAttn) with linear
computation complexity is also proposed. The empirical results on four
transductive and inductive NLP tasks and the ablation study verify the efficacy
of the proposed model. Our source code is available at
https://github.com/MathIsAll/HDGCN-pytorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shuoran Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingcai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Baotian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lisai Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Based Models for Code Generation under Compilability Constraints. (arXiv:2106.04985v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04985</id>
        <link href="http://arxiv.org/abs/2106.04985"/>
        <updated>2021-06-10T01:56:49.455Z</updated>
        <summary type="html"><![CDATA[Neural language models can be successfully trained on source code, leading to
applications such as code completion. However, their versatile autoregressive
self-supervision objective overlooks important global sequence-level features
that are present in the data such as syntactic correctness or compilability. In
this work, we pose the problem of learning to generate compilable code as
constraint satisfaction. We define an Energy-Based Model (EBM) representing a
pre-trained generative model with an imposed constraint of generating only
compilable sequences. We then use the KL-Adaptive Distributional Policy
Gradient algorithm (Khalifa et al., 2021) to train a generative model
approximating the EBM. We conduct experiments showing that our proposed
approach is able to improve compilability rates without sacrificing diversity
and complexity of the generated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1"&gt;Tomasz Korbak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1"&gt;Hady Elsahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dymetman_M/0/1/0/all/0/1"&gt;Marc Dymetman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1"&gt;Germ&amp;#xe1;n Kruszewski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages. (arXiv:2012.05628v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05628</id>
        <link href="http://arxiv.org/abs/2012.05628"/>
        <updated>2021-06-10T01:56:49.440Z</updated>
        <summary type="html"><![CDATA[Large generative language models have been very successful for English, but
other languages lag behind, in part due to data and computational limitations.
We propose a method that may overcome these problems by adapting existing
pre-trained models to new languages. Specifically, we describe the adaptation
of English GPT-2 to Italian and Dutch by retraining lexical embeddings without
tuning the Transformer layers. As a result, we obtain lexical embeddings for
Italian and Dutch that are aligned with the original English lexical
embeddings. Additionally, we scale up complexity by transforming relearned
lexical embeddings of GPT-2 small to the GPT-2 medium embedding space. This
method minimises the amount of training and prevents losing information during
adaptation that was learned by GPT-2. English GPT-2 models with relearned
lexical embeddings can generate realistic sentences in Italian and Dutch.
Though on average these sentences are still identifiable as artificial by
humans, they are assessed on par with sentences generated by a GPT-2 model
fully trained from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vries_W/0/1/0/all/0/1"&gt;Wietse de Vries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1"&gt;Malvina Nissim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style. (arXiv:2106.04619v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04619</id>
        <link href="http://arxiv.org/abs/2106.04619"/>
        <updated>2021-06-10T01:56:49.433Z</updated>
        <summary type="html"><![CDATA[Self-supervised representation learning has shown remarkable success in a
number of domains. A common practice is to perform data augmentation via
hand-crafted transformations intended to leave the semantics of the data
invariant. We seek to understand the empirical success of this approach from a
theoretical perspective. We formulate the augmentation process as a latent
variable model by postulating a partition of the latent representation into a
content component, which is assumed invariant to augmentation, and a style
component, which is allowed to change. Unlike prior work on disentanglement and
independent component analysis, we allow for both nontrivial statistical and
causal dependencies in the latent space. We study the identifiability of the
latent representation based on pairs of views of the observations and prove
sufficient conditions that allow us to identify the invariant content partition
up to an invertible mapping in both generative and discriminative settings. We
find numerical simulations with dependent latent variables are consistent with
our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional,
visually complex images with rich causal dependencies, which we use to study
the effect of data augmentations performed in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1"&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1"&gt;Yash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gresele_L/0/1/0/all/0/1"&gt;Luigi Gresele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1"&gt;Michel Besserve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1"&gt;Francesco Locatello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Influence-Augmented Online Planning for Complex Environments. (arXiv:2010.11038v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11038</id>
        <link href="http://arxiv.org/abs/2010.11038"/>
        <updated>2021-06-10T01:56:49.428Z</updated>
        <summary type="html"><![CDATA[How can we plan efficiently in real time to control an agent in a complex
environment that may involve many other agents? While existing sample-based
planners have enjoyed empirical success in large POMDPs, their performance
heavily relies on a fast simulator. However, real-world scenarios are complex
in nature and their simulators are often computationally demanding, which
severely limits the performance of online planners. In this work, we propose
influence-augmented online planning, a principled method to transform a
factored simulator of the entire environment into a local simulator that
samples only the state variables that are most relevant to the observation and
reward of the planning agent and captures the incoming influence from the rest
of the environment using machine learning methods. Our main experimental
results show that planning on this less accurate but much faster local
simulator with POMCP leads to higher real-time planning performance than
planning on the simulator that models the entire environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jinke He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suau_M/0/1/0/all/0/1"&gt;Miguel Suau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliehoek_F/0/1/0/all/0/1"&gt;Frans A. Oliehoek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handcrafted Backdoors in Deep Neural Networks. (arXiv:2106.04690v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.04690</id>
        <link href="http://arxiv.org/abs/2106.04690"/>
        <updated>2021-06-10T01:56:49.413Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs), while accurate, are expensive to train. Many
practitioners, therefore, outsource the training process to third parties or
use pre-trained DNNs. This practice makes DNNs vulnerable to $backdoor$
$attacks$: the third party who trains the model may act maliciously to inject
hidden behaviors into the otherwise accurate model. Until now, the mechanism to
inject backdoors has been limited to $poisoning$.

We argue that such a supply-chain attacker has more attack techniques
available. To study this hypothesis, we introduce a handcrafted attack that
directly manipulates the parameters of a pre-trained model to inject backdoors.
Our handcrafted attacker has more degrees of freedom in manipulating model
parameters than poisoning. This makes it difficult for a defender to identify
or remove the manipulations with straightforward methods, such as statistical
analysis, adding random noises to model parameters, or clipping their values
within a certain range. Further, our attacker can combine the handcrafting
process with additional techniques, $e.g.$, jointly optimizing a trigger
pattern, to inject backdoors into complex networks effectively$-$the
meet-in-the-middle attack.

In evaluations, our handcrafted backdoors remain effective across four
datasets and four network architectures with a success rate above 96%. Our
backdoored models are resilient to both parameter-level backdoor removal
techniques and can evade existing defenses by slightly changing the backdoor
attack configurations. Moreover, we demonstrate the feasibility of suppressing
unwanted behaviors otherwise caused by poisoning. Our results suggest that
further research is needed for understanding the complete space of supply-chain
backdoor attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1"&gt;Sanghyun Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1"&gt;Alexey Kurakin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Loss function based second-order Jensen inequality and its application to particle variational inference. (arXiv:2106.05010v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.05010</id>
        <link href="http://arxiv.org/abs/2106.05010"/>
        <updated>2021-06-10T01:56:49.406Z</updated>
        <summary type="html"><![CDATA[Bayesian model averaging, obtained as the expectation of a likelihood
function by a posterior distribution, has been widely used for prediction,
evaluation of uncertainty, and model selection. Various approaches have been
developed to efficiently capture the information in the posterior distribution;
one such approach is the optimization of a set of models simultaneously with
interaction to ensure the diversity of the individual models in the same way as
ensemble learning. A representative approach is particle variational inference
(PVI), which uses an ensemble of models as an empirical approximation for the
posterior distribution. PVI iteratively updates each model with a repulsion
force to ensure the diversity of the optimized models. However, despite its
promising performance, a theoretical understanding of this repulsion and its
association with the generalization ability remains unclear. In this paper, we
tackle this problem in light of PAC-Bayesian analysis. First, we provide a new
second-order Jensen inequality, which has the repulsion term based on the loss
function. Thanks to the repulsion term, it is tighter than the standard Jensen
inequality. Then, we derive a novel generalization error bound and show that it
can be reduced by enhancing the diversity of models. Finally, we derive a new
PVI that optimizes the generalization error bound directly. Numerical
experiments demonstrate that the performance of the proposed PVI compares
favorably with existing methods in the experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Futami_F/0/1/0/all/0/1"&gt;Futoshi Futami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Iwata_T/0/1/0/all/0/1"&gt;Tomoharu Iwata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ueda_N/0/1/0/all/0/1"&gt;Naonori Ueda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sato_I/0/1/0/all/0/1"&gt;Issei Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Softmax Confidence and Uncertainty. (arXiv:2106.04972v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04972</id>
        <link href="http://arxiv.org/abs/2106.04972"/>
        <updated>2021-06-10T01:56:49.400Z</updated>
        <summary type="html"><![CDATA[It is often remarked that neural networks fail to increase their uncertainty
when predicting on data far from the training distribution. Yet naively using
softmax confidence as a proxy for uncertainty achieves modest success in tasks
exclusively testing for this, e.g., out-of-distribution (OOD) detection. This
paper investigates this contradiction, identifying two implicit biases that do
encourage softmax confidence to correlate with epistemic uncertainty: 1)
Approximately optimal decision boundary structure, and 2) Filtering effects of
deep networks. It describes why low-dimensional intuitions about softmax
confidence are misleading. Diagnostic experiments quantify reasons softmax
confidence can fail, finding that extrapolations are less to blame than overlap
between training and OOD data in final-layer representations.
Pre-trained/fine-tuned networks reduce this overlap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1"&gt;Tim Pearce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1"&gt;Alexandra Brintrup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Inverse Reinforcement Learning. (arXiv:2106.05068v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05068</id>
        <link href="http://arxiv.org/abs/2106.05068"/>
        <updated>2021-06-10T01:56:49.393Z</updated>
        <summary type="html"><![CDATA[The objective of offline RL is to learn optimal policies when a fixed
exploratory demonstrations data-set is available and sampling additional
observations is impossible (typically if this operation is either costly or
rises ethical questions). In order to solve this problem, off the shelf
approaches require a properly defined cost function (or its evaluation on the
provided data-set), which are seldom available in practice. To circumvent this
issue, a reasonable alternative is to query an expert for few optimal
demonstrations in addition to the exploratory data-set. The objective is then
to learn an optimal policy w.r.t. the expert's latent cost function. Current
solutions either solve a behaviour cloning problem (which does not leverage the
exploratory data) or a reinforced imitation learning problem (using a fixed
cost function that discriminates available exploratory trajectories from expert
ones). Inspired by the success of IRL techniques in achieving state of the art
imitation performances in online settings, we exploit GAN based data
augmentation procedures to construct the first offline IRL algorithm. The
obtained policies outperformed the aforementioned solutions on multiple OpenAI
gym environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jarboui_F/0/1/0/all/0/1"&gt;Firas Jarboui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1"&gt;Vianney Perchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data. (arXiv:2106.05001v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05001</id>
        <link href="http://arxiv.org/abs/2106.05001"/>
        <updated>2021-06-10T01:56:49.387Z</updated>
        <summary type="html"><![CDATA[A central challenge in training classification models in the real-world
federated system is learning with non-IID data. To cope with this, most of the
existing works involve enforcing regularization in local optimization or
improving the model aggregation scheme at the server. Other works also share
public datasets or synthesized samples to supplement the training of
under-represented classes or introduce a certain level of personalization.
Though effective, they lack a deep understanding of how the data heterogeneity
affects each layer of a deep classification model. In this paper, we bridge
this gap by performing an experimental analysis of the representations learned
by different layers. Our observations are surprising: (1) there exists a
greater bias in the classifier than other layers, and (2) the classification
performance can be significantly improved by post-calibrating the classifier
after federated training. Motivated by the above findings, we propose a novel
and simple algorithm called Classifier Calibration with Virtual Representations
(CCVR), which adjusts the classifier using virtual representations sampled from
an approximated gaussian mixture model. Experimental results demonstrate that
CCVR achieves state-of-the-art performance on popular federated learning
benchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple
yet effective method can shed some light on the future research of federated
learning with non-IID data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Mi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dapeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Initialization Matters: Regularizing Manifold-informed Initialization for Neural Recommendation Systems. (arXiv:2106.04993v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04993</id>
        <link href="http://arxiv.org/abs/2106.04993"/>
        <updated>2021-06-10T01:56:49.382Z</updated>
        <summary type="html"><![CDATA[Proper initialization is crucial to the optimization and the generalization
of neural networks. However, most existing neural recommendation systems
initialize the user and item embeddings randomly. In this work, we propose a
new initialization scheme for user and item embeddings called Laplacian
Eigenmaps with Popularity-based Regularization for Isolated Data (LEPORID).
LEPORID endows the embeddings with information regarding multi-scale
neighborhood structures on the data manifold and performs adaptive
regularization to compensate for high embedding variance on the tail of the
data distribution. Exploiting matrix sparsity, LEPORID embeddings can be
computed efficiently. We evaluate LEPORID in a wide range of neural
recommendation models. In contrast to the recent surprising finding that the
simple K-nearest-neighbor (KNN) method often outperforms neural recommendation
systems, we show that existing neural systems initialized with LEPORID often
perform on par or better than KNN. To maximize the effects of the
initialization, we propose the Dual-Loss Residual Recommendation (DLR2)
network, which, when initialized with LEPORID, substantially outperforms both
traditional and state-of-the-art neural recommender systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yinan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It Takes Two to Tango: Mixup for Deep Metric Learning. (arXiv:2106.04990v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04990</id>
        <link href="http://arxiv.org/abs/2106.04990"/>
        <updated>2021-06-10T01:56:49.375Z</updated>
        <summary type="html"><![CDATA[Metric learning involves learning a discriminative representation such that
embeddings of similar classes are encouraged to be close, while embeddings of
dissimilar classes are pushed far apart. State-of-the-art methods focus mostly
on sophisticated loss functions or mining strategies. On the one hand, metric
learning losses consider two or more examples at a time. On the other hand,
modern data augmentation methods for classification consider two or more
examples at a time. The combination of the two ideas is under-studied.

In this work, we aim to bridge this gap and improve representations using
mixup, which is a powerful data augmentation approach interpolating two or more
examples and corresponding target labels at a time. This task is challenging
because, unlike classification, the loss functions used in metric learning are
not additive over examples, so the idea of interpolating target labels is not
straightforward. To the best of our knowledge, we are the first to investigate
mixing examples and target labels for deep metric learning. We develop a
generalized formulation that encompasses existing metric learning loss
functions and modify it to accommodate for mixup, introducing Metric Mix, or
Metrix. We show that mixing inputs, intermediate representations or embeddings
along with target labels significantly improves representations and outperforms
state-of-the-art metric learning methods on four benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venkataramanan_S/0/1/0/all/0/1"&gt;Shashanka Venkataramanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Psomas_B/0/1/0/all/0/1"&gt;Bill Psomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1"&gt;Yannis Avrithis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1"&gt;Ewa Kijak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amsaleg_L/0/1/0/all/0/1"&gt;Laurent Amsaleg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karantzalos_K/0/1/0/all/0/1"&gt;Konstantinos Karantzalos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhance Convolutional Neural Networks with Noise Incentive Block. (arXiv:2012.12109v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12109</id>
        <link href="http://arxiv.org/abs/2012.12109"/>
        <updated>2021-06-10T01:56:49.370Z</updated>
        <summary type="html"><![CDATA[As a generic modeling tool, Convolutional Neural Networks (CNNs) have been
widely employed in image generation and translation tasks. However, when fed
with a flat input, current CNN models may fail to generate vivid results due to
the spatially shared convolution kernels. We call it the flatness degradation
of CNNs. Unfortunately, such degradation is the greatest obstacles to generate
a spatially-variant output from a flat input, which has been barely discussed
in the previous literature. To tackle this problem, we propose a model agnostic
solution, i.e. Noise Incentive Block (NIB), which serves as a generic plug-in
for any CNN generation model. The key idea is to break the flat input condition
while keeping the intactness of the original information. Specifically, the NIB
perturbs the input data symmetrically with a noise map and reassembles them in
the feature domain as driven by the objective function. Extensive experiments
show that existing CNN models equipped with NIB survive from the flatness
degradation and are able to generate visually better results with richer
details in some specific image generation tasks given flat inputs, e.g.
semantic image synthesis, data-hidden image generation, and deep neural
dithering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1"&gt;Menghan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1"&gt;Tien-Tsin Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Inference through Early-Exit Networks: Design, Challenges and Directions. (arXiv:2106.05022v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05022</id>
        <link href="http://arxiv.org/abs/2106.05022"/>
        <updated>2021-06-10T01:56:49.352Z</updated>
        <summary type="html"><![CDATA[DNNs are becoming less and less over-parametrised due to recent advances in
efficient model design, through careful hand-crafted or NAS-based methods.
Relying on the fact that not all inputs require the same amount of computation
to yield a confident prediction, adaptive inference is gaining attention as a
prominent approach for pushing the limits of efficient deployment.
Particularly, early-exit networks comprise an emerging direction for tailoring
the computation depth of each input sample at runtime, offering complementary
performance gains to other efficiency optimisations. In this paper, we
decompose the design methodology of early-exit networks to its key components
and survey the recent advances in each one of them. We also position
early-exiting against other efficient inference solutions and provide our
insights on the current challenges and most promising future directions for
research in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laskaridis_S/0/1/0/all/0/1"&gt;Stefanos Laskaridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouris_A/0/1/0/all/0/1"&gt;Alexandros Kouris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas D. Lane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Bellman Operators. (arXiv:2106.05012v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05012</id>
        <link href="http://arxiv.org/abs/2106.05012"/>
        <updated>2021-06-10T01:56:49.342Z</updated>
        <summary type="html"><![CDATA[We introduce a novel perspective on Bayesian reinforcement learning (RL);
whereas existing approaches infer a posterior over the transition distribution
or Q-function, we characterise the uncertainty in the Bellman operator. Our
Bayesian Bellman operator (BBO) framework is motivated by the insight that when
bootstrapping is introduced, model-free approaches actually infer a posterior
over Bellman operators, not value functions. In this paper, we use BBO to
provide a rigorous theoretical analysis of model-free Bayesian RL to better
understand its relationshipto established frequentist RL methodologies. We
prove that Bayesian solutions are consistent with frequentist RL solutions,
even when approximate inference isused, and derive conditions for which
convergence properties hold. Empirically, we demonstrate that algorithms
derived from the BBO framework have sophisticated deep exploration properties
that enable them to solve continuous control tasks at which state-of-the-art
regularised actor-critic algorithms fail catastrophically]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fellows_M/0/1/0/all/0/1"&gt;Matthew Fellows&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartikainen_K/0/1/0/all/0/1"&gt;Kristian Hartikainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study. (arXiv:2106.04995v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04995</id>
        <link href="http://arxiv.org/abs/2106.04995"/>
        <updated>2021-06-10T01:56:49.332Z</updated>
        <summary type="html"><![CDATA[Recent advances in Unsupervised Neural Machine Translation (UNMT) have
minimized the gap between supervised and unsupervised machine translation
performance for closely related language pairs. However, the situation is very
different for distant language pairs. Lack of lexical overlap and low syntactic
similarities such as between English and Indo-Aryan languages leads to poor
translation quality in existing UNMT systems. In this paper, we show that
initializing the embedding layer of UNMT models with cross-lingual embeddings
shows significant improvements in BLEU score over existing approaches with
embeddings randomly initialized. Further, static embeddings (freezing the
embedding layer weights) lead to better gains compared to updating the
embedding layer weights during training (non-static). We experimented using
Masked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT
approaches for three distant language pairs. The proposed cross-lingual
embedding initialization yields BLEU score improvement of as much as ten times
over the baseline for English-Hindi, English-Bengali, and English-Gujarati. Our
analysis shows the importance of cross-lingual embedding, comparisons between
approaches, and the scope of improvements in these systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1"&gt;Tamali Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1"&gt;Rudra Murthy V&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1"&gt;Pushpak Bhattacharyya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04830</id>
        <link href="http://arxiv.org/abs/2012.04830"/>
        <updated>2021-06-10T01:56:49.317Z</updated>
        <summary type="html"><![CDATA[Cataract is one of the leading causes of reversible visual impairment and
blindness globally. Over the years, researchers have achieved significant
progress in developing state-of-the-art artificial intelligence techniques for
automatic cataract classification and grading, helping clinicians prevent and
treat cataract in time. This paper provides a comprehensive survey of recent
advances in machine learning for cataract classification and grading based on
ophthalmic images. We summarize existing literature from two research
directions: conventional machine learning techniques and deep learning
techniques. This paper also provides insights into existing works of both
merits and limitations. In addition, we discuss several challenges of automatic
cataract classification and grading based on machine learning techniques and
present possible solutions to these challenges for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiansheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1"&gt;Risa Higashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeRF in detail: Learning to sample for view synthesis. (arXiv:2106.05264v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05264</id>
        <link href="http://arxiv.org/abs/2106.05264"/>
        <updated>2021-06-10T01:56:49.293Z</updated>
        <summary type="html"><![CDATA[Neural radiance fields (NeRF) methods have demonstrated impressive novel view
synthesis performance. The core approach is to render individual rays by
querying a neural network at points sampled along the ray to obtain the density
and colour of the sampled points, and integrating this information using the
rendering equation. Since dense sampling is computationally prohibitive, a
common solution is to perform coarse-to-fine sampling.

In this work we address a clear limitation of the vanilla coarse-to-fine
approach -- that it is based on a heuristic and not trained end-to-end for the
task at hand. We introduce a differentiable module that learns to propose
samples and their importance for the fine network, and consider and compare
multiple alternatives for its neural architecture. Training the proposal module
from scratch can be unstable due to lack of supervision, so an effective
pre-training strategy is also put forward. The approach, named `NeRF in detail'
(NeRF-ID), achieves superior view synthesis quality over NeRF and the
state-of-the-art on the synthetic Blender benchmark and on par or better
performance on the real LLFF-NeRF scenes. Furthermore, by leveraging the
predicted sample importance, a 25% saving in computation can be achieved
without significantly sacrificing the rendering quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_R/0/1/0/all/0/1"&gt;Relja Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiFair: Training Fair Models with Bilevel Optimization. (arXiv:2106.04757v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04757</id>
        <link href="http://arxiv.org/abs/2106.04757"/>
        <updated>2021-06-10T01:56:49.114Z</updated>
        <summary type="html"><![CDATA[Prior studies have shown that, training machine learning models via empirical
loss minimization to maximize a utility metric (e.g., accuracy), might yield
models that make discriminatory predictions. To alleviate this issue, we
develop a new training algorithm, named BiFair, which jointly minimizes for a
utility, and a fairness loss of interest. Crucially, we do so without directly
modifying the training objective, e.g., by adding regularization terms. Rather,
we learn a set of weights on the training dataset, such that, training on the
weighted dataset ensures both good utility, and fairness. The dataset weights
are learned in concurrence to the model training, which is done by solving a
bilevel optimization problem using a held-out validation dataset. Overall, this
approach yields models with better fairness-utility trade-offs. Particularly,
we compare our algorithm with three other state-of-the-art fair training
algorithms over three real-world datasets, and demonstrate that, BiFair
consistently performs better, i.e., we reach to better values of a given
fairness metric under same, or higher accuracy. Further, our algorithm is
scalable. It is applicable both to simple models, such as logistic regression,
as well as more complex models, such as deep neural networks, as evidenced by
our experimental analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozdayi_M/0/1/0/all/0/1"&gt;Mustafa Safa Ozdayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kantarcioglu_M/0/1/0/all/0/1"&gt;Murat Kantarcioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harmless Overparametrization in Two-layer Neural Networks. (arXiv:2106.04795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04795</id>
        <link href="http://arxiv.org/abs/2106.04795"/>
        <updated>2021-06-10T01:56:49.108Z</updated>
        <summary type="html"><![CDATA[Overparametrized neural networks, where the number of active parameters is
larger than the sample size, prove remarkably effective in modern deep learning
practice. From the classical perspective, however, much fewer parameters are
sufficient for optimal estimation and prediction, whereas overparametrization
can be harmful even in the presence of explicit regularization. To reconcile
this conflict, we present a generalization theory for overparametrized ReLU
networks by incorporating an explicit regularizer based on the scaled variation
norm. Interestingly, this regularizer is equivalent to the ridge from the angle
of gradient-based optimization, but is similar to the group lasso in terms of
controlling model complexity. By exploiting this ridge-lasso duality, we show
that overparametrization is generally harmless to two-layer ReLU networks. In
particular, the overparametrized estimators are minimax optimal up to a
logarithmic factor. By contrast, we show that overparametrized random feature
models suffer from the curse of dimensionality and thus are suboptimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huiyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-efficient SGD: From Local SGD to One-Shot Averaging. (arXiv:2106.04759v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.04759</id>
        <link href="http://arxiv.org/abs/2106.04759"/>
        <updated>2021-06-10T01:56:49.103Z</updated>
        <summary type="html"><![CDATA[We consider speeding up stochastic gradient descent (SGD) by parallelizing it
across multiple workers. We assume the same data set is shared among $N$
workers, who can take SGD steps and coordinate with a central server. While it
is possible to obtain a linear reduction in the variance by averaging all the
stochastic gradients at every step, this requires a lot of communication
between the workers and the server, which can dramatically reduce the gains
from parallelism. The Local SGD method, proposed and analyzed in the earlier
literature, suggests machines should make many local steps between such
communications. While the initial analysis of Local SGD showed it needs $\Omega
( \sqrt{T} )$ communications for $T$ local gradient steps in order for the
error to scale proportionately to $1/(NT)$, this has been successively improved
in a string of papers, with the state-of-the-art requiring $\Omega \left( N
\left( \mbox{ polynomial in log } (T) \right) \right)$ communications. In this
paper, we suggest a Local SGD scheme that communicates less overall by
communicating less frequently as the number of iterations grows. Our analysis
shows that this can achieve an error that scales as $1/(NT)$ with a number of
communications that is completely independent of $T$. In particular, we show
that $\Omega(N)$ communications are sufficient. Empirical evidence suggests
this bound is close to tight as we further show that $\sqrt{N}$ or $N^{3/4}$
communications fail to achieve linear speed-up in simulations. Moreover, we
show that under mild assumptions, the main of which is twice differentiability
on any neighborhood of the optimal solution, one-shot averaging which only uses
a single round of communication can also achieve the optimal convergence rate
asymptotically.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spiridonoff_A/0/1/0/all/0/1"&gt;Artin Spiridonoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olshevsky_A/0/1/0/all/0/1"&gt;Alex Olshevsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paschalidis_I/0/1/0/all/0/1"&gt;Ioannis Ch. Paschalidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Up Graph Neural Networks Via Graph Coarsening. (arXiv:2106.05150v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05150</id>
        <link href="http://arxiv.org/abs/2106.05150"/>
        <updated>2021-06-10T01:56:49.048Z</updated>
        <summary type="html"><![CDATA[Scalability of graph neural networks remains one of the major challenges in
graph machine learning. Since the representation of a node is computed by
recursively aggregating and transforming representation vectors of its
neighboring nodes from previous layers, the receptive fields grow
exponentially, which makes standard stochastic optimization techniques
ineffective. Various approaches have been proposed to alleviate this issue,
e.g., sampling-based methods and techniques based on pre-computation of graph
filters.

In this paper, we take a different approach and propose to use graph
coarsening for scalable training of GNNs, which is generic, extremely simple
and has sublinear memory and time costs during training. We present extensive
theoretical analysis on the effect of using coarsening operations and provides
useful guidance on the choice of coarsening methods. Interestingly, our
theoretical analysis shows that coarsening can also be considered as a type of
regularization and may improve the generalization. Finally, empirical results
on real world datasets show that, simply applying off-the-shelf coarsening
methods, we can reduce the number of nodes by up to a factor of ten without
causing a noticeable downgrade in classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zengfeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengzhong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_C/0/1/0/all/0/1"&gt;Chong Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Min Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers for Modeling Physical Systems. (arXiv:2010.03957v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03957</id>
        <link href="http://arxiv.org/abs/2010.03957"/>
        <updated>2021-06-10T01:56:49.042Z</updated>
        <summary type="html"><![CDATA[Transformers are widely used in natural language processing due to their
ability to model longer-term dependencies in text. Although these models
achieve state-of-the-art performance for many language related tasks, their
applicability outside of the natural language processing field has been
minimal. In this work, we propose the use of transformer models for the
prediction of dynamical systems representative of physical phenomena. The use
of Koopman based embeddings provide a unique and powerful method for projecting
any dynamical system into a vector representation which can then be predicted
by a transformer model. The proposed model is able to accurately predict
various dynamical systems and outperform classical methods that are commonly
used in the scientific machine learning literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geneva_N/0/1/0/all/0/1"&gt;Nicholas Geneva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zabaras_N/0/1/0/all/0/1"&gt;Nicholas Zabaras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning. (arXiv:2010.03110v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03110</id>
        <link href="http://arxiv.org/abs/2010.03110"/>
        <updated>2021-06-10T01:56:49.034Z</updated>
        <summary type="html"><![CDATA[Animals exhibit an innate ability to learn regularities of the world through
interaction. By performing experiments in their environment, they are able to
discern the causal factors of variation and infer how they affect the world's
dynamics. Inspired by this, we attempt to equip reinforcement learning agents
with the ability to perform experiments that facilitate a categorization of the
rolled-out trajectories, and to subsequently infer the causal factors of the
environment in a hierarchical manner. We introduce {\em causal curiosity}, a
novel intrinsic reward, and show that it allows our agents to learn optimal
sequences of actions and discover causal factors in the dynamics of the
environment. The learned behavior allows the agents to infer a binary quantized
representation for the ground-truth causal factors in every environment.
Additionally, we find that these experimental behaviors are semantically
meaningful (e.g., our agents learn to lift blocks to categorize them by
weight), and are learnt in a self-supervised manner with approximately 2.5
times less data than conventional supervised planners. We show that these
behaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or
other downstream tasks). Finally, we show that the knowledge of causal factor
representations aids zero-shot learning for more complex tasks. Visit
https://sites.google.com/usc.edu/causal-curiosity/home for website.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1"&gt;Sumedh A. Sontakke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehrjou_A/0/1/0/all/0/1"&gt;Arash Mehrjou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1"&gt;Laurent Itti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for Investigating Learned Representations. (arXiv:1905.13308v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.13308</id>
        <link href="http://arxiv.org/abs/1905.13308"/>
        <updated>2021-06-10T01:56:49.028Z</updated>
        <summary type="html"><![CDATA[Hierarchy and compositionality are common latent properties in many natural
and scientific datasets. Determining when a deep network's hidden activations
represent hierarchy and compositionality is important both for understanding
deep representation learning and for applying deep networks in domains where
interpretability is crucial. However, current benchmark machine learning
datasets either have little hierarchical or compositional structure, or the
structure is not known. This gap impedes precise analysis of a network's
representations and thus hinders development of new methods that can learn such
properties. To address this gap, we developed a new benchmark dataset with
known hierarchical and compositional structure. The Hangul Fonts Dataset (HFD)
is comprised of 35 fonts from the Korean writing system (Hangul), each with
11,172 blocks (syllables) composed from the product of initial consonant,
medial vowel, and final consonant glyphs. All blocks can be grouped into a few
geometric types which induces a hierarchy across blocks. In addition, each
block is composed of individual glyphs with rotations, translations, scalings,
and naturalistic style variation across fonts. We find that both shallow and
deep unsupervised methods only show modest evidence of hierarchy and
compositionality in their representations of the HFD compared to supervised
deep networks. Supervised deep network representations contain structure
related to the geometrical hierarchy of the characters, but the compositional
structure of the data is not evident. Thus, HFD enables the identification of
shortcomings in existing methods, a critical first step toward developing new
machine learning algorithms to extract hierarchical and compositional structure
in the context of naturalistic variability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Livezey_J/0/1/0/all/0/1"&gt;Jesse A. Livezey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_A/0/1/0/all/0/1"&gt;Ahyeon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_J/0/1/0/all/0/1"&gt;Jacob Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1"&gt;Kristofer E. Bouchard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL. (arXiv:2106.05087v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05087</id>
        <link href="http://arxiv.org/abs/2106.05087"/>
        <updated>2021-06-10T01:56:49.022Z</updated>
        <summary type="html"><![CDATA[Evaluating the worst-case performance of a reinforcement learning (RL) agent
under the strongest/optimal adversarial perturbations on state observations
(within some constraints) is crucial for understanding the robustness of RL
agents. However, finding the optimal adversary is challenging, in terms of both
whether we can find the optimal attack and how efficiently we can find it.
Existing works on adversarial RL either use heuristics-based methods that may
not find the strongest adversary, or directly train an RL-based adversary by
treating the agent as a part of the environment, which can find the optimal
adversary but may become intractable in a large state space. In this paper, we
propose a novel attacking algorithm which has an RL-based "director" searching
for the optimal policy perturbation, and an "actor" crafting state
perturbations following the directions from the director (i.e. the actor
executes targeted attacks). Our proposed algorithm, PA-AD, is theoretically
optimal against an RL agent and significantly improves the efficiency compared
with prior RL-based works in environments with large or pixel state spaces.
Empirical results show that our proposed PA-AD universally outperforms
state-of-the-art attacking methods in a wide range of environments. Our method
can be easily applied to any RL algorithms to evaluate and improve their
robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yanchao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Ruijie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yongyuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Path Integration of Grid Cells: Group Representation and Isotropic Scaling. (arXiv:2006.10259v5 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10259</id>
        <link href="http://arxiv.org/abs/2006.10259"/>
        <updated>2021-06-10T01:56:49.022Z</updated>
        <summary type="html"><![CDATA[Understanding how grid cells perform path integration calculations remains a
fundamental problem. In this paper, we conduct theoretical analysis of a
general representation model of path integration by grid cells, where the 2D
self-position is encoded as a higher dimensional vector, and the 2D self-motion
is represented by a general transformation of the vector. We identify two
conditions on the transformation. One is a group representation condition that
is necessary for path integration. The other is an isotropic scaling condition
that ensures locally conformal embedding, so that the error in the vector
representation translates proportionally to the error in the 2D self-position.
Then we investigate the simplest transformation, i.e., the linear
transformation, uncover its explicit algebraic and geometric structure as
matrix Lie group of rotation, and establish the connection between the
isotropic scaling condition and hexagon grid patterns of grid cells under the
linear transformation. Finally, with our optimization-based approach, we manage
to learn hexagon grid patterns that share similar properties of the grid cells
in the rodent brain. The learned model is capable of accurate long distance
path integration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruiqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jianwen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xue-Xin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Nian Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Neural Networks and Individual Neuron Importance via Information-Ordered Cumulative Ablation. (arXiv:1804.06679v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1804.06679</id>
        <link href="http://arxiv.org/abs/1804.06679"/>
        <updated>2021-06-10T01:56:49.008Z</updated>
        <summary type="html"><![CDATA[In this work, we investigate the use of three information-theoretic
quantities -- entropy, mutual information with the class variable, and a class
selectivity measure based on Kullback-Leibler divergence -- to understand and
study the behavior of already trained fully-connected feed-forward neural
networks. We analyze the connection between these information-theoretic
quantities and classification performance on the test set by cumulatively
ablating neurons in networks trained on MNIST, FashionMNIST, and CIFAR-10. Our
results parallel those recently published by Morcos et al., indicating that
class selectivity is not a good indicator for classification performance.
However, looking at individual layers separately, both mutual information and
class selectivity are positively correlated with classification performance, at
least for networks with ReLU activation functions. We provide explanations for
this phenomenon and conclude that it is ill-advised to compare the proposed
information-theoretic quantities across layers. Furthermore, we show that
cumulative ablation of neurons with ascending or descending
information-theoretic quantities can be used to formulate hypotheses regarding
the joint behavior of multiple neurons, such as redundancy and synergy, with
comparably low computational cost. We also draw connections to the information
bottleneck theory for neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1"&gt;Rana Ali Amjad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kairen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1"&gt;Bernhard C. Geiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSTDP: A More Biologically Plausible Learning. (arXiv:1912.00009v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.00009</id>
        <link href="http://arxiv.org/abs/1912.00009"/>
        <updated>2021-06-10T01:56:48.992Z</updated>
        <summary type="html"><![CDATA[Spike-timing dependent plasticity (STDP) which observed in the brain has
proven to be important in biological learning. On the other hand, artificial
neural networks use a different way to learn, such as Back-Propagation or
Contrastive Hebbian Learning. In this work, we propose a new framework called
mstdp that learn almost the same way biological learning use, it only uses STDP
rules for supervised and unsupervised learning and don' t need a global loss or
other supervise information. The framework works like an auto-encoder by making
each input neuron also an output neuron. It can make predictions or generate
patterns in one model without additional configuration. We also brought a new
iterative inference method using momentum to make the framework more efficient,
which can be used in training and testing phases. Finally, we verified our
framework on MNIST dataset for classification and generation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shiyuan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attacking Adversarial Attacks as A Defense. (arXiv:2106.04938v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04938</id>
        <link href="http://arxiv.org/abs/2106.04938"/>
        <updated>2021-06-10T01:56:48.984Z</updated>
        <summary type="html"><![CDATA[It is well known that adversarial attacks can fool deep neural networks with
imperceptible perturbations. Although adversarial training significantly
improves model robustness, failure cases of defense still broadly exist. In
this work, we find that the adversarial attacks can also be vulnerable to small
perturbations. Namely, on adversarially-trained models, perturbing adversarial
examples with a small random noise may invalidate their misled predictions.
After carefully examining state-of-the-art attacks of various kinds, we find
that all these attacks have this deficiency to different extents. Enlightened
by this finding, we propose to counter attacks by crafting more effective
defensive perturbations. Our defensive perturbations leverage the advantage
that adversarial training endows the ground-truth class with smaller local
Lipschitzness. By simultaneously attacking all the classes, the misled
predictions with larger Lipschitzness can be flipped into correct ones. We
verify our defensive perturbation with both empirical experiments and
theoretical analyses on a linear model. On CIFAR10, it boosts the
state-of-the-art model from 66.16% to 72.66% against the four attacks of
AutoAttack, including 71.76% to 83.30% against the Square attack. On ImageNet,
the top-1 robust accuracy of FastAT is improved from 33.18% to 38.54% under the
100-step PGD attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Boxi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1"&gt;Heng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jindong Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Complex Knowledge Graph Embeddings. (arXiv:2008.03130v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03130</id>
        <link href="http://arxiv.org/abs/2008.03130"/>
        <updated>2021-06-10T01:56:48.979Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of learning continuous vector
representations of knowledge graphs for predicting missing links. We present a
new approach called ConEx, which infers missing links by leveraging the
composition of a 2D convolution with a Hermitian inner product of
complex-valued embedding vectors. We evaluate ConEx against state-of-the-art
approaches on the WN18RR, FB15K-237, KINSHIP and UMLS benchmark datasets. Our
experimental results show that ConEx achieves a performance superior to that of
state-of-the-art approaches such as RotatE, QuatE and TuckER on the link
prediction task on all datasets while requiring at least 8 times fewer
parameters. We ensure the reproducibility of our results by providing an
open-source implementation which includes the training, evaluation scripts
along with pre-trained models at https://github.com/conex-kge/ConEx.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1"&gt;Caglar Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1"&gt;Axel-Cyrille Ngonga Ngomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Neural Network Subspaces. (arXiv:2102.10472v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10472</id>
        <link href="http://arxiv.org/abs/2102.10472"/>
        <updated>2021-06-10T01:56:48.969Z</updated>
        <summary type="html"><![CDATA[Recent observations have advanced our understanding of the neural network
optimization landscape, revealing the existence of (1) paths of high accuracy
containing diverse solutions and (2) wider minima offering improved
performance. Previous methods observing diverse paths require multiple training
runs. In contrast we aim to leverage both property (1) and (2) with a single
method and in a single training run. With a similar computational cost as
training one model, we learn lines, curves, and simplexes of high-accuracy
neural networks. These neural network subspaces contain diverse solutions that
can be ensembled, approaching the ensemble performance of independently trained
networks without the training cost. Moreover, using the subspace midpoint
boosts accuracy, calibration, and robustness to label noise, outperforming
Stochastic Weight Averaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1"&gt;Mitchell Wortsman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1"&gt;Maxwell Horton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1"&gt;Carlos Guestrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1"&gt;Mohammad Rastegari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phraseformer: Multimodal Key-phrase Extraction using Transformer and Graph Embedding. (arXiv:2106.04939v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04939</id>
        <link href="http://arxiv.org/abs/2106.04939"/>
        <updated>2021-06-10T01:56:48.967Z</updated>
        <summary type="html"><![CDATA[Background: Keyword extraction is a popular research topic in the field of
natural language processing. Keywords are terms that describe the most relevant
information in a document. The main problem that researchers are facing is how
to efficiently and accurately extract the core keywords from a document.
However, previous keyword extraction approaches have utilized the text and
graph features, there is the lack of models that can properly learn and combine
these features in a best way.

Methods: In this paper, we develop a multimodal Key-phrase extraction
approach, namely Phraseformer, using transformer and graph embedding
techniques. In Phraseformer, each keyword candidate is presented by a vector
which is the concatenation of the text and structure learning representations.
Phraseformer takes the advantages of recent researches such as BERT and ExEm to
preserve both representations. Also, the Phraseformer treats the key-phrase
extraction task as a sequence labeling problem solved using classification
task.

Results: We analyze the performance of Phraseformer on three datasets
including Inspec, SemEval2010 and SemEval 2017 by F1-score. Also, we
investigate the performance of different classifiers on Phraseformer method
over Inspec dataset. Experimental results demonstrate the effectiveness of
Phraseformer method over the three datasets used. Additionally, the Random
Forest classifier gain the highest F1-score among all classifiers.

Conclusions: Due to the fact that the combination of BERT and ExEm is more
meaningful and can better represent the semantic of words. Hence, Phraseformer
significantly outperforms single-modality methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikzad_Khasmakhi_N/0/1/0/all/0/1"&gt;Narjes Nikzad-Khasmakhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1"&gt;Mohammad-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1"&gt;Meysam Asgari-Chenaghlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1"&gt;Mohammad-Ali Balafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1"&gt;Ali-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahkar_Farshi_T/0/1/0/all/0/1"&gt;Taymaz Rahkar-Farshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1"&gt;Majid Ramezani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jahanbakhsh_Nagadeh_Z/0/1/0/all/0/1"&gt;Zoleikha Jahanbakhsh-Nagadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafarani_Moattar_E/0/1/0/all/0/1"&gt;Elnaz Zafarani-Moattar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1"&gt;Mehrdad Ranjbar-Khadivi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Independent mechanism analysis, a new concept?. (arXiv:2106.05200v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.05200</id>
        <link href="http://arxiv.org/abs/2106.05200"/>
        <updated>2021-06-10T01:56:48.966Z</updated>
        <summary type="html"><![CDATA[Independent component analysis provides a principled framework for
unsupervised representation learning, with solid theory on the identifiability
of the latent code that generated the data, given only observations of mixtures
thereof. Unfortunately, when the mixing is nonlinear, the model is provably
nonidentifiable, since statistical independence alone does not sufficiently
constrain the problem. Identifiability can be recovered in settings where
additional, typically observed variables are included in the generative
process. We investigate an alternative path and consider instead including
assumptions reflecting the principle of independent causal mechanisms exploited
in the field of causality. Specifically, our approach is motivated by thinking
of each source as independently influencing the mixing process. This gives rise
to a framework which we term independent mechanism analysis. We provide
theoretical and empirical evidence that our approach circumvents a number of
nonidentifiability issues arising in nonlinear blind source separation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gresele_L/0/1/0/all/0/1"&gt;Luigi Gresele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1"&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Stimper_V/0/1/0/all/0/1"&gt;Vincent Stimper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1"&gt;Michel Besserve&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiffPD: Differentiable Projective Dynamics. (arXiv:2101.05917v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05917</id>
        <link href="http://arxiv.org/abs/2101.05917"/>
        <updated>2021-06-10T01:56:48.966Z</updated>
        <summary type="html"><![CDATA[We present a novel, fast differentiable simulator for soft-body learning and
control applications. Existing differentiable soft-body simulators can be
classified into two categories based on their time integration methods:
Simulators using explicit time-stepping scheme require tiny time steps to avoid
numerical instabilities in gradient computation, and simulators using implicit
time integration typically compute gradients by employing the adjoint method
and solving the expensive linearized dynamics. Inspired by Projective Dynamics
(PD), we present Differentiable Projective Dynamics (DiffPD), an efficient
differentiable soft-body simulator based on PD with implicit time integration.
The key idea in DiffPD is to speed up backpropagation by exploiting the
prefactorized Cholesky decomposition in forward PD simulation. In terms of
contact handling, DiffPD supports two types of contacts: a penalty-based model
describing contact and friction forces and a complementarity-based model
enforcing non-penetration conditions and static friction. We evaluate the
performance of DiffPD and observe it is 4-19 times faster compared to the
standard Newton's method in various applications including system
identification, inverse design problems, trajectory optimization, and
closed-loop control. We also apply DiffPD in a real-to-sim example with contact
and collisions and show its capability of reconstructing a digital twin of
real-world scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1"&gt;Tao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1"&gt;Pingchuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wah_S/0/1/0/all/0/1"&gt;Sebastien Wah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spielberg_A/0/1/0/all/0/1"&gt;Andrew Spielberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1"&gt;Wojciech Matusik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAM: Understanding Product Images in Cross Product Category Attribute Extraction. (arXiv:2106.04630v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04630</id>
        <link href="http://arxiv.org/abs/2106.04630"/>
        <updated>2021-06-10T01:56:48.964Z</updated>
        <summary type="html"><![CDATA[Understanding product attributes plays an important role in improving online
shopping experience for customers and serves as an integral part for
constructing a product knowledge graph. Most existing methods focus on
attribute extraction from text description or utilize visual information from
product images such as shape and color. Compared to the inputs considered in
prior works, a product image in fact contains more information, represented by
a rich mixture of words and visual clues with a layout carefully designed to
impress customers. This work proposes a more inclusive framework that fully
utilizes these different modalities for attribute extraction. Inspired by
recent works in visual question answering, we use a transformer based sequence
to sequence model to fuse representations of product text, Optical Character
Recognition (OCR) tokens and visual objects detected in the product image. The
framework is further extended with the capability to extract attribute value
across multiple product categories with a single model, by training the decoder
to predict both product category and attribute value and conditioning its
output on product category. The model provides a unified attribute extraction
solution desirable at an e-commerce platform that offers numerous product
categories with a diverse body of product attributes. We evaluated the model on
two product attributes, one with many possible values and one with a small set
of possible values, over 14 product categories and found the model could
achieve 15% gain on the Recall and 10% gain on the F1 score compared to
existing methods using text-only features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jie Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1"&gt;Nasser Zalmout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Luna Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study. (arXiv:2106.04995v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04995</id>
        <link href="http://arxiv.org/abs/2106.04995"/>
        <updated>2021-06-10T01:56:48.964Z</updated>
        <summary type="html"><![CDATA[Recent advances in Unsupervised Neural Machine Translation (UNMT) have
minimized the gap between supervised and unsupervised machine translation
performance for closely related language pairs. However, the situation is very
different for distant language pairs. Lack of lexical overlap and low syntactic
similarities such as between English and Indo-Aryan languages leads to poor
translation quality in existing UNMT systems. In this paper, we show that
initializing the embedding layer of UNMT models with cross-lingual embeddings
shows significant improvements in BLEU score over existing approaches with
embeddings randomly initialized. Further, static embeddings (freezing the
embedding layer weights) lead to better gains compared to updating the
embedding layer weights during training (non-static). We experimented using
Masked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT
approaches for three distant language pairs. The proposed cross-lingual
embedding initialization yields BLEU score improvement of as much as ten times
over the baseline for English-Hindi, English-Bengali, and English-Gujarati. Our
analysis shows the importance of cross-lingual embedding, comparisons between
approaches, and the scope of improvements in these systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1"&gt;Tamali Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1"&gt;Rudra Murthy V&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1"&gt;Pushpak Bhattacharyya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Behavioral and Response Diversity for Open-ended Learning in Zero-sum Games. (arXiv:2106.04958v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2106.04958</id>
        <link href="http://arxiv.org/abs/2106.04958"/>
        <updated>2021-06-10T01:56:48.959Z</updated>
        <summary type="html"><![CDATA[Measuring and promoting policy diversity is critical for solving games with
strong non-transitive dynamics where strategic cycles exist, and there is no
consistent winner (e.g., Rock-Paper-Scissors). With that in mind, maintaining a
pool of diverse policies via open-ended learning is an attractive solution,
which can generate auto-curricula to avoid being exploited. However, in
conventional open-ended learning algorithms, there are no widely accepted
definitions for diversity, making it hard to construct and evaluate the diverse
policies. In this work, we summarize previous concepts of diversity and work
towards offering a unified measure of diversity in multi-agent open-ended
learning to include all elements in Markov games, based on both Behavioral
Diversity (BD) and Response Diversity (RD). At the trajectory distribution
level, we re-define BD in the state-action space as the discrepancies of
occupancy measures. For the reward dynamics, we propose RD to characterize
diversity through the responses of policies when encountering different
opponents. We also show that many current diversity measures fall in one of the
categories of BD or RD but not both. With this unified diversity measure, we
design the corresponding diversity-promoting objective and population
effectivity when seeking the best responses in open-ended learning. We validate
our methods in both relatively simple games like matrix game, non-transitive
mixture model, and the complex \textit{Google Research Football} environment.
The population found by our methods reveals the lowest exploitability, highest
population effectivity in matrix game and non-transitive mixture model, as well
as the largest goal difference when interacting with opponents of various
levels in \textit{Google Research Football}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1"&gt;Hangtian Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Ying Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yujing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhipeng Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realizing GANs via a Tunable Loss Function. (arXiv:2106.05232v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05232</id>
        <link href="http://arxiv.org/abs/2106.05232"/>
        <updated>2021-06-10T01:56:48.954Z</updated>
        <summary type="html"><![CDATA[We introduce a tunable GAN, called $\alpha$-GAN, parameterized by $\alpha \in
(0,\infty]$, which interpolates between various $f$-GANs and Integral
Probability Metric based GANs (under constrained discriminator set). We
construct $\alpha$-GAN using a supervised loss function, namely, $\alpha$-loss,
which is a tunable loss function capturing several canonical losses. We show
that $\alpha$-GAN is intimately related to the Arimoto divergence, which was
first proposed by \"{O}sterriecher (1996), and later studied by Liese and Vajda
(2006). We posit that the holistic understanding that $\alpha$-GAN introduces
will have practical benefits of addressing both the issues of vanishing
gradients and mode collapse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurri_G/0/1/0/all/0/1"&gt;Gowtham R. Kurri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sypherd_T/0/1/0/all/0/1"&gt;Tyler Sypherd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1"&gt;Lalitha Sankar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cooperative Online Learning. (arXiv:2106.04982v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04982</id>
        <link href="http://arxiv.org/abs/2106.04982"/>
        <updated>2021-06-10T01:56:48.939Z</updated>
        <summary type="html"><![CDATA[In this preliminary (and unpolished) version of the paper, we study an
asynchronous online learning setting with a network of agents. At each time
step, some of the agents are activated, requested to make a prediction, and pay
the corresponding loss. Some feedback is then revealed to these agents and is
later propagated through the network. We consider the case of full, bandit, and
semi-bandit feedback. In particular, we construct a reduction to delayed
single-agent learning that applies to both the full and the bandit feedback
case and allows to obtain regret guarantees for both settings. We complement
these results with a near-matching lower bound.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1"&gt;Tommaso R. Cesari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vecchia_R/0/1/0/all/0/1"&gt;Riccardo Della Vecchia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling massive highly-multivariate nonstationary spatial data with the basis graphical lasso. (arXiv:2101.02404v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02404</id>
        <link href="http://arxiv.org/abs/2101.02404"/>
        <updated>2021-06-10T01:56:48.934Z</updated>
        <summary type="html"><![CDATA[We propose a new modeling framework for highly-multivariate spatial processes
that synthesizes ideas from recent multiscale and spectral approaches with
graphical models. The basis graphical lasso writes a univariate Gaussian
process as a linear combination of basis functions weighted with entries of a
Gaussian graphical vector whose graph is estimated from optimizing an $\ell_1$
penalized likelihood. This paper extends the setting to a multivariate Gaussian
process where the basis functions are weighted with Gaussian graphical vectors.
We motivate a model where the basis functions represent different levels of
resolution and the graphical vectors for each level are assumed to be
independent. Using an orthogonal basis grants linear complexity and memory
usage in the number of spatial locations, the number of basis functions, and
the number of realizations. An additional fusion penalty encourages a
parsimonious conditional independence structure in the multilevel graphical
model. We illustrate our method on a large climate ensemble from the National
Center for Atmospheric Research's Community Atmosphere Model that involves 40
spatial processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Krock_M/0/1/0/all/0/1"&gt;Mitchell Krock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kleiber_W/0/1/0/all/0/1"&gt;William Kleiber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hammerling_D/0/1/0/all/0/1"&gt;Dorit Hammerling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Becker_S/0/1/0/all/0/1"&gt;Stephen Becker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks. (arXiv:2011.02511v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02511</id>
        <link href="http://arxiv.org/abs/2011.02511"/>
        <updated>2021-06-10T01:56:48.929Z</updated>
        <summary type="html"><![CDATA[Large volumes of interaction logs can be collected from NLP systems that are
deployed in the real world. How can this wealth of information be leveraged?
Using such interaction logs in an offline reinforcement learning (RL) setting
is a promising approach. However, due to the nature of NLP tasks and the
constraints of production systems, a series of challenges arise. We present a
concise overview of these challenges and discuss possible solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1"&gt;Julia Kreutzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1"&gt;Stefan Riezler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1"&gt;Carolin Lawrence&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-Server Private Linear Transformation: The Individual Privacy Case. (arXiv:2106.05222v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.05222</id>
        <link href="http://arxiv.org/abs/2106.05222"/>
        <updated>2021-06-10T01:56:48.924Z</updated>
        <summary type="html"><![CDATA[This paper considers the single-server Private Linear Transformation (PLT)
problem with individual privacy guarantees. In this problem, there is a user
that wishes to obtain $L$ independent linear combinations of a $D$-subset of
messages belonging to a dataset of $K$ messages stored on a single server. The
goal is to minimize the download cost while keeping the identity of each
message required for the computation individually private. The individual
privacy requirement ensures that the identity of each individual message
required for the computation is kept private. This is in contrast to the
stricter notion of joint privacy that protects the entire set of identities of
all messages used for the computation, including the correlations between these
identities. The notion of individual privacy captures a broad set of practical
applications. For example, such notion is relevant when the dataset contains
information about individuals, each of them requires privacy guarantees for
their data access patterns. We focus on the setting in which the required
linear transformation is associated with a maximum distance separable (MDS)
matrix. In particular, we require that the matrix of coefficients pertaining to
the required linear combinations is the generator matrix of an MDS code. We
establish lower and upper bounds on the capacity of PLT with individual
privacy, where the capacity is defined as the supremum of all achievable
download rates. We show that our bounds are tight under certain conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1"&gt;Anoosheh Heidarzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmati_N/0/1/0/all/0/1"&gt;Nahid Esmati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sprintson_A/0/1/0/all/0/1"&gt;Alex Sprintson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Canonical Transform for Strengthening the Local $L^p$-Type Universal Approximation Property. (arXiv:2006.14378v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14378</id>
        <link href="http://arxiv.org/abs/2006.14378"/>
        <updated>2021-06-10T01:56:48.918Z</updated>
        <summary type="html"><![CDATA[Most $L^p$-type universal approximation theorems guarantee that a given
machine learning model class $\mathscr{F}\subseteq
C(\mathbb{R}^d,\mathbb{R}^D)$ is dense in
$L^p_{\mu}(\mathbb{R}^d,\mathbb{R}^D)$ for any suitable finite Borel measure
$\mu$ on $\mathbb{R}^d$. Unfortunately, this means that the model's
approximation quality can rapidly degenerate outside some compact subset of
$\mathbb{R}^d$, as any such measure is largely concentrated on some bounded
subset of $\mathbb{R}^d$. This paper proposes a generic solution to this
approximation theoretic problem by introducing a canonical transformation which
"upgrades $\mathscr{F}$'s approximation property" in the following sense. The
transformed model class, denoted by $\mathscr{F}\text{-tope}$, is shown to be
dense in $L^p_{\mu,\text{strict}}(\mathbb{R}^d,\mathbb{R}^D)$ which is a
topological space whose elements are locally $p$-integrable functions and whose
topology is much finer than usual norm topology on
$L^p_{\mu}(\mathbb{R}^d,\mathbb{R}^D)$; here $\mu$ is any suitable
$\sigma$-finite Borel measure $\mu$ on $\mathbb{R}^d$. Next, we show that if
$\mathscr{F}$ is any family of analytic functions then there is always a strict
"gap" between $\mathscr{F}\text{-tope}$'s expressibility and that of
$\mathscr{F}$, since we find that $\mathscr{F}$ can never dense in
$L^p_{\mu,\text{strict}}(\mathbb{R}^d,\mathbb{R}^D)$. In the general case,
where $\mathscr{F}$ may contain non-analytic functions, we provide an abstract
form of these results guaranteeing that there always exists some function space
in which $\mathscr{F}\text{-tope}$ is dense but $\mathscr{F}$ is not, while,
the converse is never possible. Applications to feedforward networks,
convolutional neural networks, and polynomial bases are explored.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1"&gt;Anastasis Kratsios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamanlooy_B/0/1/0/all/0/1"&gt;Behnoosh Zamanlooy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit field learning for unsupervised anomaly detection in medical images. (arXiv:2106.05214v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.05214</id>
        <link href="http://arxiv.org/abs/2106.05214"/>
        <updated>2021-06-10T01:56:48.912Z</updated>
        <summary type="html"><![CDATA[We propose a novel unsupervised out-of-distribution detection method for
medical images based on implicit fields image representations. In our approach,
an auto-decoder feed-forward neural network learns the distribution of healthy
images in the form of a mapping between spatial coordinates and probabilities
over a proxy for tissue types. At inference time, the learnt distribution is
used to retrieve, from a given test image, a restoration, i.e. an image
maximally consistent with the input one but belonging to the healthy
distribution. Anomalies are localized using the voxel-wise probability
predicted by our model for the restored image. We tested our approach in the
task of unsupervised localization of gliomas on brain MR images and compared it
to several other VAE-based anomaly detection methods. Results show that the
proposed technique substantially outperforms them (average DICE 0.640 vs 0.518
for the best performing VAE-based alternative) while also requiring
considerably less computing time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Marimont_S/0/1/0/all/0/1"&gt;Sergio Naval Marimont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tarroni_G/0/1/0/all/0/1"&gt;Giacomo Tarroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Subset Selection for Efficient Training and Inference of Neural Networks. (arXiv:2006.14222v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14222</id>
        <link href="http://arxiv.org/abs/2006.14222"/>
        <updated>2021-06-10T01:56:48.893Z</updated>
        <summary type="html"><![CDATA[Current machine learning algorithms are designed to work with huge volumes of
high dimensional data such as images. However, these algorithms are being
increasingly deployed to resource constrained systems such as mobile devices
and embedded systems. Even in cases where large computing infrastructure is
available, the size of each data instance, as well as datasets, can be a
bottleneck in data transfer across communication channels. Also, there is a
huge incentive both in energy and monetary terms in reducing both the
computational and memory requirements of these algorithms. For nonparametric
models that require to leverage the stored training data at inference time, the
increased cost in memory and computation could be even more problematic. In
this work, we aim to reduce the volume of data these algorithms must process
through an end-to-end two-stage neural subset selection model. We first
efficiently obtain a subset of candidate elements by sampling a mask from a
conditionally independent Bernoulli distribution, and then autoregressivley
construct a subset consisting of the most task relevant elements via sampling
the elements from a conditional Categorical distribution. We validate our
method on set reconstruction and classification tasks with feature selection as
well as the selection of representative samples from a given dataset, on which
our method outperforms relevant baselines. We also show in our experiments that
our method enhances scalability of nonparametric models such as Neural
Processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Andreis_B/0/1/0/all/0/1"&gt;Bruno Andreis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;A. Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seanie Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Juho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eunho Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration. (arXiv:2006.01419v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01419</id>
        <link href="http://arxiv.org/abs/2006.01419"/>
        <updated>2021-06-10T01:56:48.887Z</updated>
        <summary type="html"><![CDATA[In this paper, sample-aware policy entropy regularization is proposed to
enhance the conventional policy entropy regularization for better exploration.
Exploiting the sample distribution obtainable from the replay buffer, the
proposed sample-aware entropy regularization maximizes the entropy of the
weighted sum of the policy action distribution and the sample action
distribution from the replay buffer for sample-efficient exploration. A
practical algorithm named diversity actor-critic (DAC) is developed by applying
policy iteration to the objective function with the proposed sample-aware
entropy regularization. Numerical results show that DAC significantly
outperforms existing recent algorithms for reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Seungyul Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1"&gt;Youngchul Sung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Class-Transductive Intent Representations for Zero-shot Intent Detection. (arXiv:2012.01721v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01721</id>
        <link href="http://arxiv.org/abs/2012.01721"/>
        <updated>2021-06-10T01:56:48.882Z</updated>
        <summary type="html"><![CDATA[Zero-shot intent detection (ZSID) aims to deal with the continuously emerging
intents without annotated training data. However, existing ZSID systems suffer
from two limitations: 1) They are not good at modeling the relationship between
seen and unseen intents. 2) They cannot effectively recognize unseen intents
under the generalized intent detection (GZSID) setting. A critical problem
behind these limitations is that the representations of unseen intents cannot
be learned in the training stage. To address this problem, we propose a novel
framework that utilizes unseen class labels to learn Class-Transductive Intent
Representations (CTIR). Specifically, we allow the model to predict unseen
intents during training, with the corresponding label names serving as input
utterances. On this basis, we introduce a multi-task learning objective, which
encourages the model to learn the distinctions among intents, and a similarity
scorer, which estimates the connections among intents more accurately. CTIR is
easy to implement and can be integrated with existing methods. Experiments on
two real-world datasets show that CTIR brings considerable improvement to the
baseline systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1"&gt;Qingyi Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanxin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1"&gt;Peng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiangnan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Facet Clustering Variational Autoencoders. (arXiv:2106.05241v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.05241</id>
        <link href="http://arxiv.org/abs/2106.05241"/>
        <updated>2021-06-10T01:56:48.866Z</updated>
        <summary type="html"><![CDATA[Work in deep clustering focuses on finding a single partition of data.
However, high-dimensional data, such as images, typically feature multiple
interesting characteristics one could cluster over. For example, images of
objects against a background could be clustered over the shape of the object
and separately by the colour of the background. In this paper, we introduce
Multi-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of
variational autoencoders with a hierarchy of latent variables, each with a
Mixture-of-Gaussians prior, that learns multiple clusterings simultaneously,
and is trained fully unsupervised and end-to-end. MFCVAE uses a
progressively-trained ladder architecture which leads to highly stable
performance. We provide novel theoretical results for optimising the ELBO
analytically with respect to the categorical variational posterior
distribution, and corrects earlier influential theoretical work. On image
benchmarks, we demonstrate that our approach separates out and clusters over
different aspects of the data in a disentangled manner. We also show other
advantages of our model: the compositionality of its latent space and that it
provides controlled generation of samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Falck_F/0/1/0/all/0/1"&gt;Fabian Falck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Willetts_M/0/1/0/all/0/1"&gt;Matthew Willetts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nicholson_G/0/1/0/all/0/1"&gt;George Nicholson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yau_C/0/1/0/all/0/1"&gt;Christopher Yau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1"&gt;Christopher C Holmes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural UpFlow: A Scene Flow Learning Approach to Increase the Apparent Resolution of Particle-Based Liquids. (arXiv:2106.05143v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2106.05143</id>
        <link href="http://arxiv.org/abs/2106.05143"/>
        <updated>2021-06-10T01:56:48.860Z</updated>
        <summary type="html"><![CDATA[We present a novel up-resing technique for generating high-resolution liquids
based on scene flow estimation using deep neural networks. Our approach infers
and synthesizes small- and large-scale details solely from a low-resolution
particle-based liquid simulation. The proposed network leverages neighborhood
contributions to encode inherent liquid properties throughout convolutions. We
also propose a particle-based approach to interpolate between liquids generated
from varying simulation discretizations using a state-of-the-art bidirectional
optical flow solver method for fluids in addition to a novel key-event
topological alignment constraint. In conjunction with the neighborhood
contributions, our loss formulation allows the inference model throughout
epochs to reward important differences in regard to significant gaps in
simulation discretizations. Even when applied in an untested simulation setup,
our approach is able to generate plausible high-resolution details. Using this
interpolation approach and the predicted displacements, our approach combines
the input liquid properties with the predicted motion to infer semi-Lagrangian
advection. We furthermore showcase how the proposed interpolation approach can
facilitate generating large simulation datasets with a subset of initial
condition parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Bruno Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poulin_P/0/1/0/all/0/1"&gt;Pierre Poulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paquette_E/0/1/0/all/0/1"&gt;Eric Paquette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An ordinal CNN approach for the assessment of neurological damage in Parkinson's disease patients. (arXiv:2106.05230v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05230</id>
        <link href="http://arxiv.org/abs/2106.05230"/>
        <updated>2021-06-10T01:56:48.845Z</updated>
        <summary type="html"><![CDATA[3D image scans are an assessment tool for neurological damage in Parkinson's
disease (PD) patients. This diagnosis process can be automatized to help
medical staff through Decision Support Systems (DSSs), and Convolutional Neural
Networks (CNNs) are good candidates, because they are effective when applied to
spatial data. This paper proposes a 3D CNN ordinal model for assessing the
level or neurological damage in PD patients. Given that CNNs need large
datasets to achieve acceptable performance, a data augmentation method is
adapted to work with spatial data. We consider the Ordinal Graph-based
Oversampling via Shortest Paths (OGO-SP) method, which applies a gamma
probability distribution for inter-class data generation. A modification of
OGO-SP is proposed, the OGO-SP-$\beta$ algorithm, which applies the beta
distribution for generating synthetic samples in the inter-class region, a
better suited distribution when compared to gamma. The evaluation of the
different methods is based on a novel 3D image dataset provided by the Hospital
Universitario 'Reina Sof\'ia' (C\'ordoba, Spain). We show how the ordinal
methodology improves the performance with respect to the nominal one, and how
OGO-SP-$\beta$ yields better performance than OGO-SP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbero_Gomez_J/0/1/0/all/0/1"&gt;Javier Barbero-G&amp;#xf3;mez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_P/0/1/0/all/0/1"&gt;Pedro-Antonio Guti&amp;#xe9;rrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vargas_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor-Manuel Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vallejo_Casas_J/0/1/0/all/0/1"&gt;Juan-Antonio Vallejo-Casas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hervas_Martinez_C/0/1/0/all/0/1"&gt;C&amp;#xe9;sar Herv&amp;#xe1;s-Mart&amp;#xed;nez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05187</id>
        <link href="http://arxiv.org/abs/2106.05187"/>
        <updated>2021-06-10T01:56:48.839Z</updated>
        <summary type="html"><![CDATA[We present implicit displacement fields, a novel representation for detailed
3D geometry. Inspired by a classic surface deformation technique, displacement
mapping, our method represents a complex surface as a smooth base surface plus
a displacement along the base's normal directions, resulting in a
frequency-based shape decomposition, where the high frequency signal is
constrained geometrically by the low frequency signal. Importantly, this
disentanglement is unsupervised thanks to a tailored architectural design that
has an innate frequency hierarchy by construction. We explore implicit
displacement field surface reconstruction and detail transfer and demonstrate
superior representational power, training stability and generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1"&gt;Wang Yifan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1"&gt;Lukas Rahmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1"&gt;Olga Sorkine-Hornung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Attention Belief Networks. (arXiv:2106.05251v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05251</id>
        <link href="http://arxiv.org/abs/2106.05251"/>
        <updated>2021-06-10T01:56:48.833Z</updated>
        <summary type="html"><![CDATA[Attention-based neural networks have achieved state-of-the-art results on a
wide range of tasks. Most such models use deterministic attention while
stochastic attention is less explored due to the optimization difficulties or
complicated model design. This paper introduces Bayesian attention belief
networks, which construct a decoder network by modeling unnormalized attention
weights with a hierarchy of gamma distributions, and an encoder network by
stacking Weibull distributions with a deterministic-upward-stochastic-downward
structure to approximate the posterior. The resulting auto-encoding networks
can be optimized in a differentiable way with a variational lower bound. It is
simple to convert any models with deterministic attention, including pretrained
ones, to the proposed Bayesian attention belief networks. On a variety of
language understanding tasks, we show that our method outperforms deterministic
attention and state-of-the-art stochastic attention in accuracy, uncertainty
estimation, generalization across domains, and robustness to adversarial
attacks. We further demonstrate the general applicability of our method on
neural machine translation and visual question answering, showing great
potential of incorporating our method into various attention-related tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shujian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xinjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture weights optimisation for Alpha-Divergence Variational Inference. (arXiv:2106.05114v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2106.05114</id>
        <link href="http://arxiv.org/abs/2106.05114"/>
        <updated>2021-06-10T01:56:48.828Z</updated>
        <summary type="html"><![CDATA[This paper focuses on $\alpha$-divergence minimisation methods for
Variational Inference. More precisely, we are interested in algorithms
optimising the mixture weights of any given mixture model, without any
information on the underlying distribution of its mixture components
parameters. The Power Descent, defined for all $\alpha \neq 1$, is one such
algorithm and we establish in our work the full proof of its convergence
towards the optimal mixture weights when $\alpha <1$. Since the
$\alpha$-divergence recovers the widely-used forward Kullback-Leibler when
$\alpha \to 1$, we then extend the Power Descent to the case $\alpha = 1$ and
show that we obtain an Entropic Mirror Descent. This leads us to investigate
the link between Power Descent and Entropic Mirror Descent: first-order
approximations allow us to introduce the Renyi Descent, a novel algorithm for
which we prove an $O(1/N)$ convergence rate. Lastly, we compare numerically the
behavior of the unbiased Power Descent and of the biased Renyi Descent and we
discuss the potential advantages of one algorithm over the other.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Daudel_K/0/1/0/all/0/1"&gt;Kam&amp;#xe9;lia Daudel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Douc_R/0/1/0/all/0/1"&gt;Randal Douc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning normal form autoencoders for data-driven discovery of universal,parameter-dependent governing equations. (arXiv:2106.05102v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05102</id>
        <link href="http://arxiv.org/abs/2106.05102"/>
        <updated>2021-06-10T01:56:48.822Z</updated>
        <summary type="html"><![CDATA[Complex systems manifest a small number of instabilities and bifurcations
that are canonical in nature, resulting in universal pattern forming
characteristics as a function of some parametric dependence. Such parametric
instabilities are mathematically characterized by their universal un-foldings,
or normal form dynamics, whereby a parsimonious model can be used to represent
the dynamics. Although center manifold theory guarantees the existence of such
low-dimensional normal forms, finding them has remained a long standing
challenge. In this work, we introduce deep learning autoencoders to discover
coordinate transformations that capture the underlying parametric dependence of
a dynamical system in terms of its canonical normal form, allowing for a simple
representation of the parametric dependence and bifurcation structure. The
autoencoder constrains the latent variable to adhere to a given normal form,
thus allowing it to learn the appropriate coordinate transformation. We
demonstrate the method on a number of example problems, showing that it can
capture a diverse set of normal forms associated with Hopf, pitchfork,
transcritical and/or saddle node bifurcations. This method shows how normal
forms can be leveraged as canonical and universal building blocks in deep
learning approaches for model discovery and reduced-order modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalia_M/0/1/0/all/0/1"&gt;Manu Kalia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunton_S/0/1/0/all/0/1"&gt;Steven L. Brunton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meijer_H/0/1/0/all/0/1"&gt;Hil G.E. Meijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1"&gt;Christoph Brune&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1"&gt;J. Nathan Kutz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Lipschitz Constant of Self-Attention. (arXiv:2006.04710v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04710</id>
        <link href="http://arxiv.org/abs/2006.04710"/>
        <updated>2021-06-10T01:56:48.808Z</updated>
        <summary type="html"><![CDATA[Lipschitz constants of neural networks have been explored in various contexts
in deep learning, such as provable adversarial robustness, estimating
Wasserstein distance, stabilising training of GANs, and formulating invertible
neural networks. Such works have focused on bounding the Lipschitz constant of
fully connected or convolutional networks, composed of linear maps and
pointwise non-linearities. In this paper, we investigate the Lipschitz constant
of self-attention, a non-linear neural network module widely used in sequence
modelling. We prove that the standard dot-product self-attention is not
Lipschitz for unbounded input domain, and propose an alternative L2
self-attention that is Lipschitz. We derive an upper bound on the Lipschitz
constant of L2 self-attention and provide empirical evidence for its asymptotic
tightness. To demonstrate the practical relevance of our theoretical work, we
formulate invertible self-attention and use it in a Transformer-based
architecture for a character-level language modelling task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunjik Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Papamakarios_G/0/1/0/all/0/1"&gt;George Papamakarios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mnih_A/0/1/0/all/0/1"&gt;Andriy Mnih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Lyapunov-Based Methodology for Constrained Optimization with Bandit Feedback. (arXiv:2106.05165v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05165</id>
        <link href="http://arxiv.org/abs/2106.05165"/>
        <updated>2021-06-10T01:56:48.802Z</updated>
        <summary type="html"><![CDATA[In a wide variety of applications including online advertising, contractual
hiring, and wireless scheduling, the controller is constrained by a stringent
budget constraint on the available resources, which are consumed in a random
amount by each action, and a stochastic feasibility constraint that may impose
important operational limitations on decision-making. In this work, we consider
a general model to address such problems, where each action returns a random
reward, cost, and penalty from an unknown joint distribution, and the
decision-maker aims to maximize the total reward under a budget constraint $B$
on the total cost and a stochastic constraint on the time-average penalty. We
propose a novel low-complexity algorithm based on Lyapunov optimization
methodology, named ${\tt LyOn}$, and prove that it achieves $O(\sqrt{B\log B})$
regret and $O(\log B/B)$ constraint-violation. The low computational cost and
sharp performance bounds of ${\tt LyOn}$ suggest that Lyapunov-based algorithm
design methodology can be effective in solving constrained bandit optimization
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1"&gt;Semih Cayci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yilin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eryilmaz_A/0/1/0/all/0/1"&gt;Atilla Eryilmaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling Image Classifiers in Object Detectors. (arXiv:2106.05209v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05209</id>
        <link href="http://arxiv.org/abs/2106.05209"/>
        <updated>2021-06-10T01:56:48.796Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation constitutes a simple yet effective way to improve the
performance of a compact student network by exploiting the knowledge of a more
powerful teacher. Nevertheless, the knowledge distillation literature remains
limited to the scenario where the student and the teacher tackle the same task.
Here, we investigate the problem of transferring knowledge not only across
architectures but also across tasks. To this end, we study the case of object
detection and, instead of following the standard detector-to-detector
distillation approach, introduce a classifier-to-detector knowledge transfer
framework. In particular, we propose strategies to exploit the classification
teacher to improve both the detector's recognition accuracy and localization
performance. Our experiments on several detectors with different backbones
demonstrate the effectiveness of our approach, allowing us to outperform the
state-of-the-art detector-to-detector distillation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shuxuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of convolutional neural network image classifiers in a hierarchical max-pooling model with additional local pooling. (arXiv:2106.05233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05233</id>
        <link href="http://arxiv.org/abs/2106.05233"/>
        <updated>2021-06-10T01:56:48.791Z</updated>
        <summary type="html"><![CDATA[Image classification is considered, and a hierarchical max-pooling model with
additional local pooling is introduced. Here the additional local pooling
enables the hierachical model to combine parts of the image which have a
variable relative distance towards each other. Various convolutional neural
network image classifiers are introduced and compared in view of their rate of
convergence. The finite sample size performance of the estimates is analyzed by
applying them to simulated and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Walter_B/0/1/0/all/0/1"&gt;Benjamin Walter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Massively Parallel and Asynchronous Tsetlin Machine Architecture Supporting Almost Constant-Time Scaling. (arXiv:2009.04861v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04861</id>
        <link href="http://arxiv.org/abs/2009.04861"/>
        <updated>2021-06-10T01:56:48.782Z</updated>
        <summary type="html"><![CDATA[Using logical clauses to represent patterns, Tsetlin Machines (TMs) have
recently obtained competitive performance in terms of accuracy, memory
footprint, energy, and learning speed on several benchmarks. Each TM clause
votes for or against a particular class, with classification resolved using a
majority vote. While the evaluation of clauses is fast, being based on binary
operators, the voting makes it necessary to synchronize the clause evaluation,
impeding parallelization. In this paper, we propose a novel scheme for
desynchronizing the evaluation of clauses, eliminating the voting bottleneck.
In brief, every clause runs in its own thread for massive native parallelism.
For each training example, we keep track of the class votes obtained from the
clauses in local voting tallies. The local voting tallies allow us to detach
the processing of each clause from the rest of the clauses, supporting
decentralized learning. This means that the TM most of the time will operate on
outdated voting tallies. We evaluated the proposed parallelization across
diverse learning tasks and it turns out that our decentralized TM learning
algorithm copes well with working on outdated data, resulting in no significant
loss in learning accuracy. Furthermore, we show that the proposed approach
provides up to 50 times faster learning. Finally, learning time is almost
constant for reasonable clause amounts (employing from 20 to 7,000 clauses on a
Tesla V100 GPU). For sufficiently large clause numbers, computation time
increases approximately proportionally. Our parallel and asynchronous
architecture thus allows processing of massive datasets and operating with more
clauses for higher accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abeyrathna_K/0/1/0/all/0/1"&gt;K. Darshana Abeyrathna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1"&gt;Bimal Bhattarai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1"&gt;Morten Goodwin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorji_S/0/1/0/all/0/1"&gt;Saeed Gorji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1"&gt;Ole-Christoffer Granmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Lei Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1"&gt;Rupsa Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1"&gt;Rohan K. Yadav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Bag is to Prune. (arXiv:2008.07063v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07063</id>
        <link href="http://arxiv.org/abs/2008.07063"/>
        <updated>2021-06-10T01:56:48.766Z</updated>
        <summary type="html"><![CDATA[It is notoriously difficult to build a bad Random Forest (RF). Concurrently,
RF blatantly overfits in-sample without any apparent consequence out-of-sample.
Standard arguments, like the classic bias-variance trade-off or double descent,
cannot rationalize this paradox. I propose a new explanation: bootstrap
aggregation and model perturbation as implemented by RF automatically prune a
latent "true" tree. More generally, randomized ensembles of greedily optimized
learners implicitly perform optimal early stopping out-of-sample. So there is
no need to tune the stopping point. By construction, novel variants of Boosting
and MARS are also eligible for automatic tuning. I empirically demonstrate the
property, with simulated and real data, by reporting that these new completely
overfitting ensembles perform similarly to their tuned counterparts -- or
better.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Coulombe_P/0/1/0/all/0/1"&gt;Philippe Goulet Coulombe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech to Standard German Text Corpus. (arXiv:2010.02810v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02810</id>
        <link href="http://arxiv.org/abs/2010.02810"/>
        <updated>2021-06-10T01:56:48.761Z</updated>
        <summary type="html"><![CDATA[We present the Swiss Parliaments Corpus (SPC), an automatically aligned Swiss
German speech to Standard German text corpus. This first version of the corpus
is based on publicly available data of the Bernese cantonal parliament and
consists of 293 hours of data. It was created using a novel forced sentence
alignment procedure and an alignment quality estimator, which can be used to
trade off corpus size and quality. We trained Automatic Speech Recognition
(ASR) models as baselines on different subsets of the data and achieved a Word
Error Rate (WER) of 0.278 and a BLEU score of 0.586 on the SPC test set. The
corpus is freely available for download.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pluss_M/0/1/0/all/0/1"&gt;Michel Pl&amp;#xfc;ss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neukom_L/0/1/0/all/0/1"&gt;Lukas Neukom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheller_C/0/1/0/all/0/1"&gt;Christian Scheller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogel_M/0/1/0/all/0/1"&gt;Manfred Vogel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning. (arXiv:2006.10412v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10412</id>
        <link href="http://arxiv.org/abs/2006.10412"/>
        <updated>2021-06-10T01:56:48.755Z</updated>
        <summary type="html"><![CDATA[Ad hoc teamwork is the challenging problem of designing an autonomous agent
which can adapt quickly to collaborate with teammates without prior
coordination mechanisms, including joint training. Prior work in this area has
focused on closed teams in which the number of agents is fixed. In this work,
we consider open teams by allowing agents with different fixed policies to
enter and leave the environment without prior notification. Our solution builds
on graph neural networks to learn agent models and joint-action value models
under varying team compositions. We contribute a novel action-value computation
that integrates the agent model and joint-action value model to produce
action-value estimates. We empirically demonstrate that our approach
successfully models the effects other agents have on the learner, leading to
policies that robustly adapt to dynamic team compositions and significantly
outperform several alternative methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1"&gt;Arrasy Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hopner_N/0/1/0/all/0/1"&gt;Niklas H&amp;#xf6;pner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1"&gt;Filippos Christianos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation. (arXiv:2106.05093v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.05093</id>
        <link href="http://arxiv.org/abs/2106.05093"/>
        <updated>2021-06-10T01:56:48.750Z</updated>
        <summary type="html"><![CDATA[We propose a new training objective named order-agnostic cross entropy (OaXE)
for fully non-autoregressive translation (NAT) models. OaXE improves the
standard cross-entropy loss to ameliorate the effect of word reordering, which
is a common source of the critical multimodality problem in NAT. Concretely,
OaXE removes the penalty for word order errors, and computes the cross entropy
loss based on the best possible alignment between model predictions and target
tokens. Since the log loss is very sensitive to invalid references, we leverage
cross entropy initialization and loss truncation to ensure the model focuses on
a good part of the search space. Extensive experiments on major WMT benchmarks
show that OaXE substantially improves translation performance, setting new
state of the art for fully NAT models. Further analyses show that OaXE
alleviates the multimodality problem by reducing token repetitions and
increasing prediction confidence. Our code, data, and trained models are
available at https://github.com/tencent-ailab/ICML21_OAXE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1"&gt;Cunxiao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback. (arXiv:2106.05203v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05203</id>
        <link href="http://arxiv.org/abs/2106.05203"/>
        <updated>2021-06-10T01:56:48.744Z</updated>
        <summary type="html"><![CDATA[Error feedback (EF), also known as error compensation, is an immensely
popular convergence stabilization mechanism in the context of distributed
training of supervised machine learning models enhanced by the use of
contractive communication compression mechanisms, such as Top-$k$. First
proposed by Seide et al (2014) as a heuristic, EF resisted any theoretical
understanding until recently [Stich et al., 2018, Alistarh et al., 2018].
However, all existing analyses either i) apply to the single node setting only,
ii) rely on very strong and often unreasonable assumptions, such global
boundedness of the gradients, or iterate-dependent assumptions that cannot be
checked a-priori and may not hold in practice, or iii) circumvent these issues
via the introduction of additional unbiased compressors, which increase the
communication cost. In this work we fix all these deficiencies by proposing and
analyzing a new EF mechanism, which we call EF21, which consistently and
substantially outperforms EF in practice. Our theoretical analysis relies on
standard assumptions only, works in the distributed heterogeneous data setting,
and leads to better and more meaningful rates. In particular, we prove that
EF21 enjoys a fast $O(1/T)$ convergence rate for smooth nonconvex problems,
beating the previous bound of $O(1/T^{2/3})$, which was shown a bounded
gradients assumption. We further improve this to a fast linear rate for PL
functions, which is the first linear convergence result for an EF-type method
not relying on unbiased compressors. Since EF has a large number of
applications where it reigns supreme, we believe that our 2021 variant, EF21,
can a large impact on the practice of communication efficient distributed
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sokolov_I/0/1/0/all/0/1"&gt;Igor Sokolov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fatkhullin_I/0/1/0/all/0/1"&gt;Ilyas Fatkhullin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concave Utility Reinforcement Learning: the Mean-field Game viewpoint. (arXiv:2106.03787v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03787</id>
        <link href="http://arxiv.org/abs/2106.03787"/>
        <updated>2021-06-10T01:56:48.730Z</updated>
        <summary type="html"><![CDATA[Concave Utility Reinforcement Learning (CURL) extends RL from linear to
concave utilities in the occupancy measure induced by the agent's policy. This
encompasses not only RL but also imitation learning and exploration, among
others. Yet, this more general paradigm invalidates the classical Bellman
equations, and calls for new algorithms. Mean-field Games (MFGs) are a
continuous approximation of many-agent RL. They consider the limit case of a
continuous distribution of identical agents, anonymous with symmetric
interests, and reduce the problem to the study of a single representative agent
in interaction with the full population. Our core contribution consists in
showing that CURL is a subclass of MFGs. We think this important to bridge
together both communities. It also allows to shed light on aspects of both
fields: we show the equivalence between concavity in CURL and monotonicity in
the associated MFG, between optimality conditions in CURL and Nash equilibrium
in MFG, or that Fictitious Play (FP) for this class of MFGs is simply
Frank-Wolfe, bringing the first convergence rate for discrete-time FP for MFGs.
We also experimentally demonstrate that, using algorithms recently introduced
for solving MFGs, we can address the CURL problem more efficiently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1"&gt;Matthieu Geist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perolat_J/0/1/0/all/0/1"&gt;Julien P&amp;#xe9;rolat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lauriere_M/0/1/0/all/0/1"&gt;Mathieu Lauri&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elie_R/0/1/0/all/0/1"&gt;Romuald Elie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perrin_S/0/1/0/all/0/1"&gt;Sarah Perrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1"&gt;Olivier Bachem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Munos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1"&gt;Olivier Pietquin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neighborhood Contrastive Learning Applied to Online Patient Monitoring. (arXiv:2106.05142v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05142</id>
        <link href="http://arxiv.org/abs/2106.05142"/>
        <updated>2021-06-10T01:56:48.724Z</updated>
        <summary type="html"><![CDATA[Intensive care units (ICU) are increasingly looking towards machine learning
for methods to provide online monitoring of critically ill patients. In machine
learning, online monitoring is often formulated as a supervised learning
problem. Recently, contrastive learning approaches have demonstrated promising
improvements over competitive supervised benchmarks. These methods rely on
well-understood data augmentation techniques developed for image data which do
not apply to online monitoring. In this work, we overcome this limitation by
supplementing time-series data augmentation techniques with a novel contrastive
learning objective which we call neighborhood contrastive learning (NCL). Our
objective explicitly groups together contiguous time segments from each patient
while maintaining state-specific information. Our experiments demonstrate a
marked improvement over existing work applying contrastive methods to medical
time-series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeche_H/0/1/0/all/0/1"&gt;Hugo Y&amp;#xe8;che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dresdner_G/0/1/0/all/0/1"&gt;Gideon Dresdner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1"&gt;Francesco Locatello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huser_M/0/1/0/all/0/1"&gt;Matthias H&amp;#xfc;ser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratsch_G/0/1/0/all/0/1"&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Densely connected multidilated convolutional networks for dense prediction tasks. (arXiv:2011.11844v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11844</id>
        <link href="http://arxiv.org/abs/2011.11844"/>
        <updated>2021-06-10T01:56:48.719Z</updated>
        <summary type="html"><![CDATA[Tasks that involve high-resolution dense prediction require a modeling of
both local and global patterns in a large input field. Although the local and
global structures often depend on each other and their simultaneous modeling is
important, many convolutional neural network (CNN)-based approaches interchange
representations in different resolutions only a few times. In this paper, we
claim the importance of a dense simultaneous modeling of multiresolution
representation and propose a novel CNN architecture called densely connected
multidilated DenseNet (D3Net). D3Net involves a novel multidilated convolution
that has different dilation factors in a single layer to model different
resolutions simultaneously. By combining the multidilated convolution with the
DenseNet architecture, D3Net incorporates multiresolution learning with an
exponentially growing receptive field in almost all layers, while avoiding the
aliasing problem that occurs when we naively incorporate the dilated
convolution in DenseNet. Experiments on the image semantic segmentation task
using Cityscapes and the audio source separation task using MUSDB18 show that
the proposed method has superior performance over state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_N/0/1/0/all/0/1"&gt;Naoya Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1"&gt;Yuki Mitsufuji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Active Search for Combinatorial Optimization Problems. (arXiv:2106.05126v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05126</id>
        <link href="http://arxiv.org/abs/2106.05126"/>
        <updated>2021-06-10T01:56:48.710Z</updated>
        <summary type="html"><![CDATA[Recently numerous machine learning based methods for combinatorial
optimization problems have been proposed that learn to construct solutions in a
sequential decision process via reinforcement learning. While these methods can
be easily combined with search strategies like sampling and beam search, it is
not straightforward to integrate them into a high-level search procedure
offering strong search guidance. Bello et al. (2016) propose active search,
which adjusts the weights of a (trained) model with respect to a single
instance at test time using reinforcement learning. While active search is
simple to implement, it is not competitive with state-of-the-art methods
because adjusting all model weights for each test instance is very time and
memory intensive. Instead of updating all model weights, we propose and
evaluate three efficient active search strategies that only update a subset of
parameters during the search. The proposed methods offer a simple way to
significantly improve the search performance of a given model and outperform
state-of-the-art machine learning based methods on combinatorial problems, even
surpassing the well-known heuristic solver LKH3 on the capacitated vehicle
routing problem. Finally, we show that (efficient) active search enables
learned models to effectively solve instances that are much larger than those
seen during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hottung_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Hottung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1"&gt;Yeong-Dae Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tierney_K/0/1/0/all/0/1"&gt;Kevin Tierney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLPF: Efficient machine-learned particle-flow reconstruction using graph neural networks. (arXiv:2101.08578v3 [physics.data-an] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08578</id>
        <link href="http://arxiv.org/abs/2101.08578"/>
        <updated>2021-06-10T01:56:48.701Z</updated>
        <summary type="html"><![CDATA[In general-purpose particle detectors, the particle-flow algorithm may be
used to reconstruct a comprehensive particle-level view of the event by
combining information from the calorimeters and the trackers, significantly
improving the detector resolution for jets and the missing transverse momentum.
In view of the planned high-luminosity upgrade of the CERN Large Hadron
Collider (LHC), it is necessary to revisit existing reconstruction algorithms
and ensure that both the physics and computational performance are sufficient
in an environment with many simultaneous proton-proton interactions (pileup).
Machine learning may offer a prospect for computationally efficient event
reconstruction that is well-suited to heterogeneous computing platforms, while
significantly improving the reconstruction quality over rule-based algorithms
for granular detectors. We introduce MLPF, a novel, end-to-end trainable,
machine-learned particle-flow algorithm based on parallelizable,
computationally efficient, and scalable graph neural networks optimized using a
multi-task objective on simulated events. We report the physics and
computational performance of the MLPF algorithm on a Monte Carlo dataset of top
quark-antiquark pairs produced in proton-proton collisions in conditions
similar to those expected for the high-luminosity LHC. The MLPF algorithm
improves the physics response with respect to a rule-based benchmark algorithm
and demonstrates computationally scalable particle-flow reconstruction in a
high-pileup environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pata_J/0/1/0/all/0/1"&gt;Joosep Pata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Duarte_J/0/1/0/all/0/1"&gt;Javier Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Vlimant_J/0/1/0/all/0/1"&gt;Jean-Roch Vlimant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pierini_M/0/1/0/all/0/1"&gt;Maurizio Pierini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Spiropulu_M/0/1/0/all/0/1"&gt;Maria Spiropulu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data. (arXiv:2106.05006v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.05006</id>
        <link href="http://arxiv.org/abs/2106.05006"/>
        <updated>2021-06-10T01:56:48.699Z</updated>
        <summary type="html"><![CDATA[Most available semantic parsing datasets, comprising of pairs of natural
utterances and logical forms, were collected solely for the purpose of training
and evaluation of natural language understanding systems. As a result, they do
not contain any of the richness and variety of natural-occurring utterances,
where humans ask about data they need or are curious about. In this work, we
release SEDE, a dataset with 12,023 pairs of utterances and SQL queries
collected from real usage on the Stack Exchange website. We show that these
pairs contain a variety of real-world challenges which were rarely reflected so
far in any other semantic parsing dataset, propose an evaluation metric based
on comparison of partial query clauses that is more suitable for real-world
queries, and conduct experiments with strong baselines, showing a large gap
between the performance on SEDE compared to other common datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hazoom_M/0/1/0/all/0/1"&gt;Moshe Hazoom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_V/0/1/0/all/0/1"&gt;Vibhor Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1"&gt;Ben Bogin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMG: A Shuffling Gradient-Based Method with Momentum. (arXiv:2011.11884v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11884</id>
        <link href="http://arxiv.org/abs/2011.11884"/>
        <updated>2021-06-10T01:56:48.698Z</updated>
        <summary type="html"><![CDATA[We combine two advanced ideas widely used in optimization for machine
learning: shuffling strategy and momentum technique to develop a novel
shuffling gradient-based method with momentum, coined Shuffling Momentum
Gradient (SMG), for non-convex finite-sum optimization problems. While our
method is inspired by momentum techniques, its update is fundamentally
different from existing momentum-based methods. We establish state-of-the-art
convergence rates of SMG for any shuffling strategy using either constant or
diminishing learning rate under standard assumptions (i.e.$L$-smoothness and
bounded variance). When the shuffling strategy is fixed, we develop another new
algorithm that is similar to existing momentum methods, and prove the same
convergence rates for this algorithm under the $L$-smoothness and bounded
gradient assumptions. We demonstrate our algorithms via numerical simulations
on standard datasets and compare them with existing shuffling methods. Our
tests have shown encouraging performance of the new algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Tran_T/0/1/0/all/0/1"&gt;Trang H. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Lam M. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tran_Dinh_Q/0/1/0/all/0/1"&gt;Quoc Tran-Dinh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Algorithms for Finding Densely Connected Clusters. (arXiv:2106.05245v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.05245</id>
        <link href="http://arxiv.org/abs/2106.05245"/>
        <updated>2021-06-10T01:56:48.691Z</updated>
        <summary type="html"><![CDATA[Local graph clustering is an important algorithmic technique for analysing
massive graphs, and has been widely applied in many research fields of data
science. While the objective of most (local) graph clustering algorithms is to
find a vertex set of low conductance, there has been a sequence of recent
studies that highlight the importance of the inter-connection between clusters
when analysing real-world datasets. Following this line of research, in this
work we study local algorithms for finding a pair of vertex sets defined with
respect to their inter-connection and their relationship with the rest of the
graph. The key to our analysis is a new reduction technique that relates the
structure of multiple sets to a single vertex set in the reduced graph. Among
many potential applications, we show that our algorithms successfully recover
densely connected clusters in the Interstate Disputes Dataset and the US
Migration Dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macgregor_P/0/1/0/all/0/1"&gt;Peter Macgregor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;He Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretrained Encoders are All You Need. (arXiv:2106.05139v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05139</id>
        <link href="http://arxiv.org/abs/2106.05139"/>
        <updated>2021-06-10T01:56:48.681Z</updated>
        <summary type="html"><![CDATA[Data-efficiency and generalization are key challenges in deep learning and
deep reinforcement learning as many models are trained on large-scale,
domain-specific, and expensive-to-label datasets. Self-supervised models
trained on large-scale uncurated datasets have shown successful transfer to
diverse settings. We investigate using pretrained image representations and
spatio-temporal attention for state representation learning in Atari. We also
explore fine-tuning pretrained representations with self-supervised techniques,
i.e., contrastive predictive coding, spatio-temporal contrastive learning, and
augmentations. Our results show that pretrained representations are at par with
state-of-the-art self-supervised methods trained on domain-specific data.
Pretrained representations, thus, yield data and compute-efficient state
representations. https://github.com/PAL-ML/PEARL_v1]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mina Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivatsa_P/0/1/0/all/0/1"&gt;P Srivatsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rane_A/0/1/0/all/0/1"&gt;Advait Rane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenniappa_S/0/1/0/all/0/1"&gt;Shriram Chenniappa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_R/0/1/0/all/0/1"&gt;Rishabh Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozair_S/0/1/0/all/0/1"&gt;Sherjil Ozair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maes_P/0/1/0/all/0/1"&gt;Pattie Maes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge distillation: A good teacher is patient and consistent. (arXiv:2106.05237v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05237</id>
        <link href="http://arxiv.org/abs/2106.05237"/>
        <updated>2021-06-10T01:56:48.664Z</updated>
        <summary type="html"><![CDATA[There is a growing discrepancy in computer vision between large-scale models
that achieve state-of-the-art performance and models that are affordable in
practical applications. In this paper we address this issue and significantly
bridge the gap between these two types of models. Throughout our empirical
investigation we do not aim to necessarily propose a new method, but strive to
identify a robust and effective recipe for making state-of-the-art large scale
models affordable in practice. We demonstrate that, when performed correctly,
knowledge distillation can be a powerful tool for reducing the size of large
models without compromising their performance. In particular, we uncover that
there are certain implicit design choices, which may drastically affect the
effectiveness of distillation. Our key contribution is the explicit
identification of these design choices, which were not previously articulated
in the literature. We back up our findings by a comprehensive empirical study,
demonstrate compelling results on a wide range of vision datasets and, in
particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which
achieves 82.8\% top-1 accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Royer_A/0/1/0/all/0/1"&gt;Am&amp;#xe9;lie Royer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1"&gt;Larisa Markeeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1"&gt;Rohan Anil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethink Transfer Learning in Medical Image Classification. (arXiv:2106.05152v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.05152</id>
        <link href="http://arxiv.org/abs/2106.05152"/>
        <updated>2021-06-10T01:56:48.658Z</updated>
        <summary type="html"><![CDATA[Transfer learning (TL) with deep convolutional neural networks (DCNNs) has
proved successful in medical image classification (MIC). However, the current
practice is puzzling, as MIC typically relies only on low- and/or mid-level
features that are learned in the bottom layers of DCNNs. Following this
intuition, we question the current strategies of TL in MIC. In this paper, we
perform careful experimental comparisons between shallow and deep networks for
classification on two chest x-ray datasets, using different TL strategies. We
find that deep models are not always favorable, and finetuning truncated deep
models almost always yields the best performance, especially in data-poor
regimes.

Project webpage:
https://github.com/sun-umn/Transfer-Learning-in-Medical-Imaging

Keywords: Transfer learning, Medical image classification, Feature hierarchy,
Medical imaging, Evaluation metrics, Imbalanced data]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1"&gt;Le Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hengyue Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Taihui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-armed Bandit Requiring Monotone Arm Sequences. (arXiv:2106.03790v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03790</id>
        <link href="http://arxiv.org/abs/2106.03790"/>
        <updated>2021-06-10T01:56:48.652Z</updated>
        <summary type="html"><![CDATA[In many online learning or multi-armed bandit problems, the taken actions or
pulled arms are ordinal and required to be monotone over time. Examples include
dynamic pricing, in which the firms use markup pricing policies to please early
adopters and deter strategic waiting, and clinical trials, in which the dose
allocation usually follows the dose escalation principle to prevent dose
limiting toxicities. We consider the continuum-armed bandit problem when the
arm sequence is required to be monotone. We show that when the unknown
objective function is Lipschitz continuous, the regret is $O(T)$. When in
addition the objective function is unimodal or quasiconcave, the regret is
$\tilde O(T^{3/4})$ under the proposed algorithm, which is also shown to be the
optimal rate. This deviates from the optimal rate $\tilde O(T^{2/3})$ in the
continuous-armed bandit literature and demonstrates the cost to the learning
efficiency brought by the monotonicity requirement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Ningyuan Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Framelets Enhance Graph Neural Networks. (arXiv:2102.06986v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06986</id>
        <link href="http://arxiv.org/abs/2102.06986"/>
        <updated>2021-06-10T01:56:48.638Z</updated>
        <summary type="html"><![CDATA[This paper presents a new approach for assembling graph neural networks based
on framelet transforms. The latter provides a multi-scale representation for
graph-structured data. We decompose an input graph into low-pass and high-pass
frequencies coefficients for network training, which then defines a
framelet-based graph convolution. The framelet decomposition naturally induces
a graph pooling strategy by aggregating the graph feature into low-pass and
high-pass spectra, which considers both the feature values and geometry of the
graph data and conserves the total information. The graph neural networks with
the proposed framelet convolution and pooling achieve state-of-the-art
performance in many node and graph prediction tasks. Moreover, we propose
shrinkage as a new activation for the framelet convolution, which thresholds
high-frequency information at different scales. Compared to ReLU, shrinkage
activation improves model performance on denoising and signal compression:
noises in both node and structure can be significantly reduced by accurately
cutting off the high-pass coefficients from framelet decomposition, and the
signal can be compressed to less than half its original size with
well-preserved prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xuebin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bingxin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Junbin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Guang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Lio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Ming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1"&gt;Guido Montufar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Transformers Really Perform Bad for Graph Representation?. (arXiv:2106.05234v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05234</id>
        <link href="http://arxiv.org/abs/2106.05234"/>
        <updated>2021-06-10T01:56:48.632Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture has become a dominant choice in many domains,
such as natural language processing and computer vision. Yet, it has not
achieved competitive performance on popular leaderboards of graph-level
prediction compared to mainstream GNN variants. Therefore, it remains a mystery
how Transformers could perform well for graph representation learning. In this
paper, we solve this mystery by presenting Graphormer, which is built upon the
standard Transformer architecture, and could attain excellent results on a
broad range of graph representation learning tasks, especially on the recent
OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the
graph is the necessity of effectively encoding the structural information of a
graph into the model. To this end, we propose several simple yet effective
structural encoding methods to help Graphormer better model graph-structured
data. Besides, we mathematically characterize the expressive power of
Graphormer and exhibit that with our ways of encoding the structural
information of graphs, many popular GNN variants could be covered as the
special cases of Graphormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1"&gt;Chengxuan Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shengjie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1"&gt;Guolin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yanming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Clustering based Fair Outlier Detection. (arXiv:2106.05127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05127</id>
        <link href="http://arxiv.org/abs/2106.05127"/>
        <updated>2021-06-10T01:56:48.627Z</updated>
        <summary type="html"><![CDATA[In this paper, we focus on the fairness issues regarding unsupervised outlier
detection. Traditional algorithms, without a specific design for algorithmic
fairness, could implicitly encode and propagate statistical bias in data and
raise societal concerns. To correct such unfairness and deliver a fair set of
potential outlier candidates, we propose Deep Clustering based Fair Outlier
Detection (DCFOD) that learns a good representation for utility maximization
while enforcing the learnable representation to be subgroup-invariant on the
sensitive attribute. Considering the coupled and reciprocal nature between
clustering and outlier detection, we leverage deep clustering to discover the
intrinsic cluster structure and out-of-structure instances. Meanwhile, an
adversarial training erases the sensitive pattern for instances for fairness
adaptation. Technically, we propose an instance-level weighted representation
learning strategy to enhance the joint deep clustering and outlier detection,
where the dynamic weight module re-emphasizes contributions of likely-inliers
while mitigating the negative impact from outliers. Demonstrated by experiments
on eight datasets comparing to 17 outlier detection algorithms, our DCFOD
method consistently achieves superior performance on both the outlier detection
validity and two types of fairness notions in outlier detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hanyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peizhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Noise for Multi-Attack Robustness. (arXiv:2006.12135v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12135</id>
        <link href="http://arxiv.org/abs/2006.12135"/>
        <updated>2021-06-10T01:56:48.619Z</updated>
        <summary type="html"><![CDATA[Adversarial learning has emerged as one of the successful techniques to
circumvent the susceptibility of existing methods against adversarial
perturbations. However, the majority of existing defense methods are tailored
to defend against a single category of adversarial perturbation (e.g.
$\ell_\infty$-attack). In safety-critical applications, this makes these
methods extraneous as the attacker can adopt diverse adversaries to deceive the
system. Moreover, training on multiple perturbations simultaneously
significantly increases the computational overhead during training. To address
these challenges, we propose a novel meta-learning framework that explicitly
learns to generate noise to improve the model's robustness against multiple
types of attacks. Its key component is Meta Noise Generator (MNG) that outputs
optimal noise to stochastically perturb a given sample, such that it helps
lower the error on diverse adversarial perturbations. By utilizing samples
generated by MNG, we train a model by enforcing the label consistency across
multiple perturbations. We validate the robustness of models trained by our
scheme on various datasets and against a wide variety of perturbations,
demonstrating that it significantly outperforms the baselines across multiple
perturbations with a marginal computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1"&gt;Divyam Madaan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-domain Speech Recognition with Unsupervised Character-level Distribution Matching. (arXiv:2104.07491v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07491</id>
        <link href="http://arxiv.org/abs/2104.07491"/>
        <updated>2021-06-10T01:56:48.613Z</updated>
        <summary type="html"><![CDATA[End-to-end automatic speech recognition (ASR) can achieve promising
performance with large-scale training data. However, it is known that domain
mismatch between training and testing data often leads to a degradation of
recognition accuracy. In this work, we focus on the unsupervised domain
adaptation for ASR and propose CMatch, a Character-level distribution matching
method to perform fine-grained adaptation between each character in two
domains. First, to obtain labels for the features belonging to each character,
we achieve frame-level label assignment using the Connectionist Temporal
Classification (CTC) pseudo labels. Then, we match the character-level
distributions using Maximum Mean Discrepancy. We train our algorithm using the
self-training technique. Experiments on the Libri-Adapt dataset show that our
proposed approach achieves 14.39% and 16.50% relative Word Error Rate (WER)
reduction on both cross-device and cross-environment ASR. We also
comprehensively analyze the different strategies for frame-level label
assignment and Transformer adaptations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1"&gt;Wenxin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1"&gt;Takahiro Shinozaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Diagnosing GAN: Diagnosing Underrepresented Samples in Generative Adversarial Networks. (arXiv:2102.12033v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12033</id>
        <link href="http://arxiv.org/abs/2102.12033"/>
        <updated>2021-06-10T01:56:48.606Z</updated>
        <summary type="html"><![CDATA[Despite remarkable performance in producing realistic samples, Generative
Adversarial Networks (GANs) often produce low-quality samples near low-density
regions of the data manifold, especially for samples with minor features. Many
techniques have been developed to improve the quality of generated samples,
either by post-processing generated samples or by pre-processing the empirical
data distribution, but at the cost of reduced diversity. To promote diversity
in sample generation without degrading the overall quality, we propose a simple
yet effective method to diagnose and emphasize underrepresented samples during
training of a GAN. The main idea is to use the statistics of the discrepancy
between the data distribution and the model distribution at each data instance.
Based on the observation that the underrepresented samples have a high average
discrepancy or high variability in discrepancy, we propose a method to
emphasize those samples during training of a GAN. Our experimental results
demonstrate that the proposed method improves GAN performance on various
datasets, and it is especially effective in improving the quality and diversity
of generated samples with minor features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jinhee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Haeri Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Youngkyu Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1"&gt;Hye Won Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crowdsourced Labeling for Worker-Task Specialization Model. (arXiv:2004.00101v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.00101</id>
        <link href="http://arxiv.org/abs/2004.00101"/>
        <updated>2021-06-10T01:56:48.577Z</updated>
        <summary type="html"><![CDATA[We consider crowdsourced labeling under a $d$-type worker-task specialization
model, where each worker and task is associated with one particular type among
a finite set of types and a worker provides a more reliable answer to tasks of
the matched type than to tasks of unmatched types. We design an inference
algorithm that recovers binary task labels (up to any given recovery accuracy)
by using worker clustering, worker skill estimation and weighted majority
voting. The designed inference algorithm does not require any information about
worker/task types, and achieves any targeted recovery accuracy with the best
known performance (minimum number of queries per task).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Doyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1"&gt;Hye Won Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XBNet : An Extremely Boosted Neural Network. (arXiv:2106.05239v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05239</id>
        <link href="http://arxiv.org/abs/2106.05239"/>
        <updated>2021-06-10T01:56:48.571Z</updated>
        <summary type="html"><![CDATA[Neural networks have proved to be very robust at processing unstructured data
like images, text, videos, and audio. However, it has been observed that their
performance is not up to the mark in tabular data; hence tree-based models are
preferred in such scenarios. A popular model for tabular data is boosted trees,
a highly efficacious and extensively used machine learning method, and it also
provides good interpretability compared to neural networks. In this paper, we
describe a novel architecture XBNet, which tries to combine tree-based models
with that of neural networks to create a robust architecture trained by using a
novel optimization technique, Boosted Gradient Descent for Tabular Data which
increases its interpretability and performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_T/0/1/0/all/0/1"&gt;Tushar Sarkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MACE: A Flexible Framework for Membership Privacy Estimation in Generative Models. (arXiv:2009.05683v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05683</id>
        <link href="http://arxiv.org/abs/2009.05683"/>
        <updated>2021-06-10T01:56:48.512Z</updated>
        <summary type="html"><![CDATA[In this work, we formally study the membership privacy risk of generative
models and propose a membership privacy estimation framework. We formulate the
membership privacy risk as a statistical divergence between training samples
and hold-out samples, and propose sample-based methods to estimate this
divergence. Unlike previous works, our proposed metric and estimators make
realistic and flexible assumptions. First, we offer a generalizable metric as
an alternative to accuracy for imbalanced datasets. Second, our estimators are
capable of estimating the membership privacy risk given any scalar or vector
valued attributes from the learned model, while prior work require access to
specific attributes. This allows our framework to provide data-driven
certificates for trained generative models in terms of membership privacy risk.
Finally, we show a connection to differential privacy, which allows our
proposed estimators to be used to understand the privacy budget 'epsilon'
needed for differentially private generative models. We demonstrate the utility
of our framework through experimental demonstrations on different generative
models using various model attributes yielding some new insights about
membership leakage and vulnerabilities of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yixi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1"&gt;Shruti Tople&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Sumit Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09632</id>
        <link href="http://arxiv.org/abs/2105.09632"/>
        <updated>2021-06-10T01:56:48.496Z</updated>
        <summary type="html"><![CDATA[Today, we are seeing an ever-increasing number of clinical notes that contain
clinical results, images, and textual descriptions of patient's health state.
All these data can be analyzed and employed to cater novel services that can
help people and domain experts with their common healthcare tasks. However,
many technologies such as Deep Learning and tools like Word Embeddings have
started to be investigated only recently, and many challenges remain open when
it comes to healthcare domain applications. To address these challenges, we
propose the use of Deep Learning and Word Embeddings for identifying sixteen
morbidity types within textual descriptions of clinical records. For this
purpose, we have used a Deep Learning model based on Bidirectional Long-Short
Term Memory (LSTM) layers which can exploit state-of-the-art vector
representations of data such as Word Embeddings. We have employed pre-trained
Word Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained
on the target domain. Furthermore, we have compared the performances of the
deep learning approaches against the traditional tf-idf using Support Vector
Machine and Multilayer perceptron (our baselines). From the obtained results it
seems that the latter outperforms the combination of Deep Learning approaches
using any word embeddings. Our preliminary results indicate that there are
specific features that make the dataset biased in favour of traditional machine
learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1"&gt;Danilo Dessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1"&gt;Rim Helaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vivek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1"&gt;Diego Reforgiato Recupero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1"&gt;Daniele Riboni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Driven Robust Optimization using Unsupervised Deep Learning. (arXiv:2011.09769v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09769</id>
        <link href="http://arxiv.org/abs/2011.09769"/>
        <updated>2021-06-10T01:56:48.472Z</updated>
        <summary type="html"><![CDATA[Robust optimization has been established as a leading methodology to approach
decision problems under uncertainty. To derive a robust optimization model, a
central ingredient is to identify a suitable model for uncertainty, which is
called the uncertainty set, containing all scenarios against which we wish to
protect. An ongoing challenge in the recent literature is to derive uncertainty
sets from given historical data.

In this paper we use an unsupervised deep learning method to construct
non-convex uncertainty sets from data, which have a more complex structure than
the typically considered sets. We prove that most of the classical uncertainty
classes are special cases of our derived sets and that optimizing over it is
strongly NP-hard. Nevertheless we show that the trained neural networks can be
integrated into a robust optimization model by formulating the adversarial
problem as a convex quadratic mixed-integer program. This allows us to derive
robust solutions through an iterative scenario generation process. We prove
that our class of uncertainty sets contains In extensive computational
experiments, we compare this approach to a similar approach, which derives
uncertainty sets by kernel-based support vector clustering. We find that
uncertainty sets derived by the unsupervised deep learning method can give a
better description of data, leading to robust solutions that often outperform
the comparison method both with respect to objective value and feasibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Goerigk_M/0/1/0/all/0/1"&gt;Marc Goerigk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kurtz_J/0/1/0/all/0/1"&gt;Jannis Kurtz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04632</id>
        <link href="http://arxiv.org/abs/2106.04632"/>
        <updated>2021-06-10T01:56:48.467Z</updated>
        <summary type="html"><![CDATA[Most existing video-and-language (VidL) research focuses on a single dataset,
or multiple datasets of a single task. In reality, a truly useful VidL system
is expected to be easily generalizable to diverse tasks, domains, and datasets.
To facilitate the evaluation of such systems, we introduce Video-And-Language
Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets
over 3 popular tasks: (i) text-to-video retrieval; (ii) video question
answering; and (iii) video captioning. VALUE benchmark aims to cover a broad
range of video genres, video lengths, data volumes, and task difficulty levels.
Rather than focusing on single-channel videos with visual information only,
VALUE promotes models that leverage information from both video frames and
their associated subtitles, as well as models that share knowledge across
multiple tasks. We evaluate various baseline methods with and without
large-scale VidL pre-training, and systematically investigate the impact of
video input channels, fusion methods, and different video representations. We
also study the transferability between tasks, and conduct multi-task learning
under different settings. The significant gap between our best model and human
performance calls for future study for advanced VidL models. VALUE is available
at https://value-leaderboard.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Licheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillai_R/0/1/0/all/0/1"&gt;Rohit Pillai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luowei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Eric Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;William Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara Lee Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multistep Electric Vehicle Charging Station Occupancy Prediction using Mixed LSTM Neural Networks. (arXiv:2106.04986v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04986</id>
        <link href="http://arxiv.org/abs/2106.04986"/>
        <updated>2021-06-10T01:56:48.443Z</updated>
        <summary type="html"><![CDATA[Public charging station occupancy prediction plays key importance in
developing a smart charging strategy to reduce electric vehicle (EV) operator
and user inconvenience. However, existing studies are mainly based on
conventional econometric or time series methodologies with limited accuracy. We
propose a new mixed long short-term memory neural network incorporating both
historical charging state sequences and time-related features for multistep
discrete charging occupancy state prediction. Unlike the existing LSTM
networks, the proposed model separates different types of features and handles
them differently with mixed neural network architecture. The model is compared
to a number of state-of-the-art machine learning and deep learning approaches
based on the EV charging data obtained from the open data portal of the city of
Dundee, UK. The results show that the proposed method produces very accurate
predictions (99.99% and 81.87% for 1 step (10 minutes) and 6 step (1 hour)
ahead, respectively, and outperforms the benchmark approaches significantly
(+22.4% for one-step-ahead prediction and +6.2% for 6 steps ahead). A
sensitivity analysis is conducted to evaluate the impact of the model
parameters on prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tai-Yu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faye_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Faye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Parametric Stochastic Sequential Assignment With Random Arrival Times. (arXiv:2106.04944v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.04944</id>
        <link href="http://arxiv.org/abs/2106.04944"/>
        <updated>2021-06-10T01:56:48.438Z</updated>
        <summary type="html"><![CDATA[We consider a problem wherein jobs arrive at random times and assume random
values. Upon each job arrival, the decision-maker must decide immediately
whether or not to accept the job and gain the value on offer as a reward, with
the constraint that they may only accept at most $n$ jobs over some reference
time period. The decision-maker only has access to $M$ independent realisations
of the job arrival process. We propose an algorithm, Non-Parametric Sequential
Allocation (NPSA), for solving this problem. Moreover, we prove that the
expected reward returned by the NPSA algorithm converges in probability to
optimality as $M$ grows large. We demonstrate the effectiveness of the
algorithm empirically on synthetic data and on public fraud-detection datasets,
from where the motivation for this work is derived.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dervovic_D/0/1/0/all/0/1"&gt;Danial Dervovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassanzadeh_P/0/1/0/all/0/1"&gt;Parisa Hassanzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Assefa_S/0/1/0/all/0/1"&gt;Samuel Assefa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_P/0/1/0/all/0/1"&gt;Prashant Reddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter-Efficient Transfer Learning with Diff Pruning. (arXiv:2012.07463v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07463</id>
        <link href="http://arxiv.org/abs/2012.07463"/>
        <updated>2021-06-10T01:56:48.432Z</updated>
        <summary type="html"><![CDATA[While task-specific finetuning of pretrained networks has led to significant
empirical advances in NLP, the large size of networks makes finetuning
difficult to deploy in multi-task, memory-constrained settings. We propose diff
pruning as a simple approach to enable parameter-efficient transfer learning
within the pretrain-finetune framework. This approach views finetuning as
learning a task-specific diff vector that is applied on top of the pretrained
parameter vector, which remains fixed and is shared across different tasks. The
diff vector is adaptively pruned during training with a differentiable
approximation to the L0-norm penalty to encourage sparsity. Diff pruning
becomes parameter-efficient as the number of tasks increases, as it requires
storing only the nonzero positions and weights of the diff vector for each
task, while the cost of storing the shared pretrained model remains constant.
It further does not require access to all tasks during training, which makes it
attractive in settings where tasks arrive in stream or the set of tasks is
unknown. We find that models finetuned with diff pruning can match the
performance of fully finetuned baselines on the GLUE benchmark while only
modifying 0.5% of the pretrained model's parameters per task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1"&gt;Demi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1"&gt;Alexander M. Rush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yoon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transient Chaos in BERT. (arXiv:2106.03181v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03181</id>
        <link href="http://arxiv.org/abs/2106.03181"/>
        <updated>2021-06-10T01:56:48.417Z</updated>
        <summary type="html"><![CDATA[Language is an outcome of our complex and dynamic human-interactions and the
technique of natural language processing (NLP) is hence built on human
linguistic activities. Bidirectional Encoder Representations from Transformers
(BERT) has recently gained its popularity by establishing the state-of-the-art
scores in several NLP benchmarks. A Lite BERT (ALBERT) is literally
characterized as a lightweight version of BERT, in which the number of BERT
parameters is reduced by repeatedly applying the same neural network called
Transformer's encoder layer. By pre-training the parameters with a massive
amount of natural language data, ALBERT can convert input sentences into
versatile high-dimensional vectors potentially capable of solving multiple NLP
tasks. In that sense, ALBERT can be regarded as a well-designed
high-dimensional dynamical system whose operator is the Transformer's encoder,
and essential structures of human language are thus expected to be encapsulated
in its dynamics. In this study, we investigated the embedded properties of
ALBERT to reveal how NLP tasks are effectively solved by exploiting its
dynamics. We thereby aimed to explore the nature of human language from the
dynamical expressions of the NLP model. Our short-term analysis clarified that
the pre-trained model stably yields trajectories with higher dimensionality,
which would enhance the expressive capacity required for NLP tasks. Also, our
long-term analysis revealed that ALBERT intrinsically shows transient chaos, a
typical nonlinear phenomenon showing chaotic dynamics only in its transient,
and the pre-trained ALBERT model tends to produce the chaotic trajectory for a
significantly longer time period compared to a randomly-initialized one. Our
results imply that local chaoticity would contribute to improving NLP
performance, uncovering a novel aspect in the role of chaotic dynamics in human
language behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1"&gt;Katsuma Inoue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohara_S/0/1/0/all/0/1"&gt;Soh Ohara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1"&gt;Yasuo Kuniyoshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakajima_K/0/1/0/all/0/1"&gt;Kohei Nakajima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling. (arXiv:2106.05223v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05223</id>
        <link href="http://arxiv.org/abs/2106.05223"/>
        <updated>2021-06-10T01:56:48.411Z</updated>
        <summary type="html"><![CDATA[Vast amount of data generated from networks of sensors, wearables, and the
Internet of Things (IoT) devices underscores the need for advanced modeling
techniques that leverage the spatio-temporal structure of decentralized data
due to the need for edge computation and licensing (data access) issues. While
federated learning (FL) has emerged as a framework for model training without
requiring direct data sharing and exchange, effectively modeling the complex
spatio-temporal dependencies to improve forecasting capabilities still remains
an open problem. On the other hand, state-of-the-art spatio-temporal
forecasting models assume unfettered access to the data, neglecting constraints
on data sharing. To bridge this gap, we propose a federated spatio-temporal
model -- Cross-Node Federated Graph Neural Network (CNFGNN) -- which explicitly
encodes the underlying graph structure using graph neural network (GNN)-based
architecture under the constraint of cross-node federated learning, which
requires that data in a network of nodes is generated locally on each node and
remains decentralized. CNFGNN operates by disentangling the temporal dynamics
modeling on devices and spatial dynamics on the server, utilizing alternating
optimization to reduce the communication cost, facilitating computations on the
edge devices. Experiments on the traffic flow forecasting task show that CNFGNN
achieves the best forecasting performance in both transductive and inductive
learning settings with no extra computation cost on edge devices, while
incurring modest communication cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chuizheng Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1"&gt;Sirisha Rambhatla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Paced Context Evaluation for Contextual Reinforcement Learning. (arXiv:2106.05110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05110</id>
        <link href="http://arxiv.org/abs/2106.05110"/>
        <updated>2021-06-10T01:56:48.405Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) has made a lot of advances for solving a single
problem in a given environment; but learning policies that generalize to unseen
variations of a problem remains challenging. To improve sample efficiency for
learning on such instances of a problem domain, we present Self-Paced Context
Evaluation (SPaCE). Based on self-paced learning, \spc automatically generates
\task curricula online with little computational overhead. To this end, SPaCE
leverages information contained in state values during training to accelerate
and improve training performance as well as generalization capabilities to new
instances from the same problem domain. Nevertheless, SPaCE is independent of
the problem domain at hand and can be applied on top of any RL agent with
state-value function approximation. We demonstrate SPaCE's ability to speed up
learning of different value-based RL agents on two environments, showing better
generalization capabilities and up to 10x faster learning compared to naive
approaches such as round robin or SPDRL, as the closest state-of-the-art
approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eimer_T/0/1/0/all/0/1"&gt;Theresa Eimer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Biedenkapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1"&gt;Marius Lindauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EMA2S: An End-to-End Multimodal Articulatory-to-Speech System. (arXiv:2102.03786v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03786</id>
        <link href="http://arxiv.org/abs/2102.03786"/>
        <updated>2021-06-10T01:56:48.400Z</updated>
        <summary type="html"><![CDATA[Synthesized speech from articulatory movements can have real-world use for
patients with vocal cord disorders, situations requiring silent speech, or in
high-noise environments. In this work, we present EMA2S, an end-to-end
multimodal articulatory-to-speech system that directly converts articulatory
movements to speech signals. We use a neural-network-based vocoder combined
with multimodal joint-training, incorporating spectrogram, mel-spectrogram, and
deep features. The experimental results confirm that the multimodal approach of
EMA2S outperforms the baseline system in terms of both objective evaluation and
subjective evaluation metrics. Moreover, results demonstrate that joint
mel-spectrogram and deep feature loss training can effectively improve system
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Wen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hung_K/0/1/0/all/0/1"&gt;Kuo-Hsuan Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chuang_S/0/1/0/all/0/1"&gt;Shang-Yi Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sherman_J/0/1/0/all/0/1"&gt;Jonathan Sherman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wen-Chin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xugang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks. (arXiv:2012.11654v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11654</id>
        <link href="http://arxiv.org/abs/2012.11654"/>
        <updated>2021-06-10T01:56:48.376Z</updated>
        <summary type="html"><![CDATA[A recent line of work has analyzed the theoretical properties of deep neural
networks via the Neural Tangent Kernel (NTK). In particular, the smallest
eigenvalue of the NTK has been related to the memorization capacity, the global
convergence of gradient descent algorithms and the generalization of deep nets.
However, existing results either provide bounds in the two-layer setting or
assume that the spectrum of the NTK matrices is bounded away from 0 for
multi-layer networks. In this paper, we provide tight bounds on the smallest
eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of
infinite widths and for finite widths. In the finite-width setting, the network
architectures we consider are fairly general: we require the existence of a
wide layer with roughly order of $N$ neurons, $N$ being the number of data
samples; and the scaling of the remaining layer widths is arbitrary (up to
logarithmic factors). To obtain our results, we analyze various quantities of
independent interest: we give lower bounds on the smallest singular value of
hidden feature matrices, and upper bounds on the Lipschitz constant of
input-output feature maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quynh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mondelli_M/0/1/0/all/0/1"&gt;Marco Mondelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Montufar_G/0/1/0/all/0/1"&gt;Guido Montufar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A general approach for Explanations in terms of Middle Level Features. (arXiv:2106.05037v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05037</id>
        <link href="http://arxiv.org/abs/2106.05037"/>
        <updated>2021-06-10T01:56:48.368Z</updated>
        <summary type="html"><![CDATA[Nowadays, it is growing interest to make Machine Learning (ML) systems more
understandable and trusting to general users. Thus, generating explanations for
ML system behaviours that are understandable to human beings is a central
scientific and technological issue addressed by the rapidly growing research
area of eXplainable Artificial Intelligence (XAI). Recently, it is becoming
more and more evident that new directions to create better explanations should
take into account what a good explanation is to a human user, and consequently,
develop XAI solutions able to provide user-centred explanations. This paper
suggests taking advantage of developing an XAI general approach that allows
producing explanations for an ML system behaviour in terms of different and
user-selected input features, i.e., explanations composed of input properties
that the human user can select according to his background knowledge and goals.
To this end, we propose an XAI general approach which is able: 1) to construct
explanations in terms of input features that represent more salient and
understandable input properties for a user, which we call here Middle-Level
input Features (MLFs), 2) to be applied to different types of MLFs. We
experimentally tested our approach on two different datasets and using three
different types of MLFs. The results seem encouraging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1"&gt;Andrea Apicella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1"&gt;Francesco Isgr&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1"&gt;Roberto Prevete&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Avoiding Traps in Nonconvex Problems. (arXiv:2106.05206v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.05206</id>
        <link href="http://arxiv.org/abs/2106.05206"/>
        <updated>2021-06-10T01:56:48.307Z</updated>
        <summary type="html"><![CDATA[Iterative projection methods may become trapped at non-solutions when the
constraint sets are nonconvex. Two kinds of parameters are available to help
avoid this behavior and this study gives examples of both. The first kind of
parameter, called a hyperparameter, includes any kind of parameter that appears
in the definition of the iteration rule itself. The second kind comprises
metric parameters in the definition of the constraint sets, a feature that
arises when the problem to be solved has two or more kinds of variables.
Through examples we show the importance of properly tuning both kinds of
parameters and offer heuristic interpretations of the observed behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Deyo_S/0/1/0/all/0/1"&gt;Sean Deyo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Elser_V/0/1/0/all/0/1"&gt;Veit Elser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self Normalizing Flows. (arXiv:2011.07248v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07248</id>
        <link href="http://arxiv.org/abs/2011.07248"/>
        <updated>2021-06-10T01:56:48.302Z</updated>
        <summary type="html"><![CDATA[Efficient gradient computation of the Jacobian determinant term is a core
problem in many machine learning settings, and especially so in the normalizing
flow framework. Most proposed flow models therefore either restrict to a
function class with easy evaluation of the Jacobian determinant, or an
efficient estimator thereof. However, these restrictions limit the performance
of such density models, frequently requiring significant depth to reach desired
performance levels. In this work, we propose Self Normalizing Flows, a flexible
framework for training normalizing flows by replacing expensive terms in the
gradient by learned approximate inverses at each layer. This reduces the
computational complexity of each layer's exact update from $\mathcal{O}(D^3)$
to $\mathcal{O}(D^2)$, allowing for the training of flow architectures which
were otherwise computationally infeasible, while also providing efficient
sampling. We show experimentally that such models are remarkably stable and
optimize to similar data likelihood values as their exact gradient
counterparts, while training more quickly and surpassing the performance of
functionally constrained counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keller_T/0/1/0/all/0/1"&gt;T. Anderson Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jorn W.T. Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaini_P/0/1/0/all/0/1"&gt;Priyank Jaini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1"&gt;Emiel Hoogeboom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1"&gt;Patrick Forr&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Point of Gaze Estimator for Low-Resolution Imaging Systems Using Extracted Ocular Features Based Neural Architecture. (arXiv:2106.05106v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05106</id>
        <link href="http://arxiv.org/abs/2106.05106"/>
        <updated>2021-06-10T01:56:48.297Z</updated>
        <summary type="html"><![CDATA[A user's eyes provide means for Human Computer Interaction (HCI) research as
an important modal. The time to time scientific explorations of the eye has
already seen an upsurge of the benefits in HCI applications from gaze
estimation to the measure of attentiveness of a user looking at a screen for a
given time period. The eye tracking system as an assisting, interactive tool
can be incorporated by physically disabled individuals, fitted best for those
who have eyes as only a limited set of communication. The threefold objective
of this paper is - 1. To introduce a neural network based architecture to
predict users' gaze at 9 positions displayed in the 11.31{\deg} visual range on
the screen, through a low resolution based system such as a webcam in real time
by learning various aspects of eyes as an ocular feature set. 2.A collection of
coarsely supervised feature set obtained in real time which is also validated
through the user case study presented in the paper for 21 individuals ( 17 men
and 4 women ) from whom a 35k set of instances was derived with an accuracy
score of 82.36% and f1_score of 82.2% and 3.A detailed study over applicability
and underlying challenges of such systems. The experimental results verify the
feasibility and validity of the proposed eye gaze tracking model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sahay_A/0/1/0/all/0/1"&gt;Atul Sahay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_I/0/1/0/all/0/1"&gt;Imon Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arya_K/0/1/0/all/0/1"&gt;Kavi Arya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-Server Private Linear Transformation: The Joint Privacy Case. (arXiv:2106.05220v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.05220</id>
        <link href="http://arxiv.org/abs/2106.05220"/>
        <updated>2021-06-10T01:56:48.257Z</updated>
        <summary type="html"><![CDATA[This paper introduces the problem of Private Linear Transformation (PLT)
which generalizes the problems of private information retrieval and private
linear computation. The PLT problem includes one or more remote server(s)
storing (identical copies of) $K$ messages and a user who wants to compute $L$
independent linear combinations of a $D$-subset of messages. The objective of
the user is to perform the computation by downloading minimum possible amount
of information from the server(s), while protecting the identities of the $D$
messages required for the computation. In this work, we focus on the
single-server setting of the PLT problem when the identities of the $D$
messages required for the computation must be protected jointly. We consider
two different models, depending on whether the coefficient matrix of the
required $L$ linear combinations generates a Maximum Distance Separable (MDS)
code. We prove that the capacity for both models is given by $L/(K-D+L)$, where
the capacity is defined as the supremum of all achievable download rates. Our
converse proofs are based on linear-algebraic and information-theoretic
arguments that establish connections between PLT schemes and linear codes. We
also present an achievability scheme for each of the models being considered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1"&gt;Anoosheh Heidarzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmati_N/0/1/0/all/0/1"&gt;Nahid Esmati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sprintson_A/0/1/0/all/0/1"&gt;Alex Sprintson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More than meets the eye: Self-supervised depth reconstruction from brain activity. (arXiv:2106.05113v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05113</id>
        <link href="http://arxiv.org/abs/2106.05113"/>
        <updated>2021-06-10T01:56:48.250Z</updated>
        <summary type="html"><![CDATA[In the past few years, significant advancements were made in reconstruction
of observed natural images from fMRI brain recordings using deep-learning
tools. Here, for the first time, we show that dense 3D depth maps of observed
2D natural images can also be recovered directly from fMRI brain recordings. We
use an off-the-shelf method to estimate the unknown depth maps of natural
images. This is applied to both: (i) the small number of images presented to
subjects in an fMRI scanner (images for which we have fMRI recordings -
referred to as "paired" data), and (ii) a very large number of natural images
with no fMRI recordings ("unpaired data"). The estimated depth maps are then
used as an auxiliary reconstruction criterion to train for depth reconstruction
directly from fMRI. We propose two main approaches: Depth-only recovery and
joint image-depth RGBD recovery. Because the number of available "paired"
training data (images with fMRI) is small, we enrich the training data via
self-supervised cycle-consistent training on many "unpaired" data (natural
images & depth maps without fMRI). This is achieved using our newly defined and
trained Depth-based Perceptual Similarity metric as a reconstruction criterion.
We show that predicting the depth map directly from fMRI outperforms its
indirect sequential recovery from the reconstructed images. We further show
that activations from early cortical visual areas dominate our depth
reconstruction results, and propose means to characterize fMRI voxels by their
degree of depth-information tuning. This work adds an important layer of
decoded information, extending the current envelope of visual brain decoding
capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gaziv_G/0/1/0/all/0/1"&gt;Guy Gaziv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1"&gt;Michal Irani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Significance tests of feature relevance for a blackbox learner. (arXiv:2103.04985v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04985</id>
        <link href="http://arxiv.org/abs/2103.04985"/>
        <updated>2021-06-10T01:56:48.211Z</updated>
        <summary type="html"><![CDATA[An exciting recent development is the uptake of deep learning in many
scientific fields, where the objective is seeking novel scientific insights and
discoveries. To interpret a learning outcome, researchers perform hypothesis
testing for explainable features to advance scientific domain knowledge. In
such a situation, testing for a blackbox learner poses a severe challenge
because of intractable models, unknown limiting distributions of parameter
estimates, and high computational constraints. In this article, we derive two
consistent tests for the feature relevance of a blackbox learner. The first one
evaluates a loss difference with perturbation on an inference sample, which is
independent of an estimation sample used for parameter estimation in model
fitting. The second further splits the inference sample into two but does not
require data perturbation. Also, we develop their combined versions by
aggregating the order statistics of the $p$-values based on repeated sample
splitting. To estimate the splitting ratio and the perturbation size, we
develop adaptive splitting schemes for suitably controlling the Type \rom{1}
error subject to computational constraints. By deflating the
\textit{bias-sd-ratio}, we establish asymptotic null distributions of the test
statistics and their consistency in terms of statistical power. Our theoretical
power analysis and simulations indicate that the one-split test is more
powerful than the two-split test, though the latter is easier to apply for
large datasets. Moreover, the combined tests are more stable while compensating
for a power loss by repeated sample splitting. Numerically, we demonstrate the
utility of the proposed tests on two benchmark examples. Accompanying this
paper is our Python library {\tt dnn-inference}
https://dnn-inference.readthedocs.io/en/latest/ that implements the proposed
tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dai_B/0/1/0/all/0/1"&gt;Ben Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xiaotong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pan_W/0/1/0/all/0/1"&gt;Wei Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recovering AES Keys with a Deep Cold Boot Attack. (arXiv:2106.04876v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.04876</id>
        <link href="http://arxiv.org/abs/2106.04876"/>
        <updated>2021-06-10T01:56:48.205Z</updated>
        <summary type="html"><![CDATA[Cold boot attacks inspect the corrupted random access memory soon after the
power has been shut down. While most of the bits have been corrupted, many
bits, at random locations, have not. Since the keys in many encryption schemes
are being expanded in memory into longer keys with fixed redundancies, the keys
can often be restored. In this work, we combine a novel cryptographic variant
of a deep error correcting code technique with a modified SAT solver scheme to
apply the attack on AES keys. Even though AES consists of Rijndael S-box
elements, that are specifically designed to be resistant to linear and
differential cryptanalysis, our method provides a novel formalization of the
AES key scheduling as a computational graph, which is implemented by a neural
message passing network. Our results show that our methods outperform the state
of the art attack methods by a very large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimerman_I/0/1/0/all/0/1"&gt;Itamar Zimerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1"&gt;Eliya Nachmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias-Robust Bayesian Optimization via Dueling Bandits. (arXiv:2105.11802v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11802</id>
        <link href="http://arxiv.org/abs/2105.11802"/>
        <updated>2021-06-10T01:56:48.182Z</updated>
        <summary type="html"><![CDATA[We consider Bayesian optimization in settings where observations can be
adversarially biased, for example by an uncontrolled hidden confounder. Our
first contribution is a reduction of the confounded setting to the dueling
bandit model. Then we propose a novel approach for dueling bandits based on
information-directed sampling (IDS). Thereby, we obtain the first efficient
kernelized algorithm for dueling bandits that comes with cumulative regret
guarantees. Our analysis further generalizes a previously proposed
semi-parametric linear bandit model to non-linear reward functions, and
uncovers interesting links to doubly-robust estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kirschner_J/0/1/0/all/0/1"&gt;Johannes Kirschner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Graph Learning with Hyperbolic Embedding for Temporal Health Event Prediction. (arXiv:2106.04751v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04751</id>
        <link href="http://arxiv.org/abs/2106.04751"/>
        <updated>2021-06-10T01:56:48.177Z</updated>
        <summary type="html"><![CDATA[Electronic Health Records (EHR) have been heavily used in modern healthcare
systems for recording patients' admission information to hospitals. Many
data-driven approaches employ temporal features in EHR for predicting specific
diseases, readmission times, or diagnoses of patients. However, most existing
predictive models cannot fully utilize EHR data, due to an inherent lack of
labels in supervised training for some temporal events. Moreover, it is hard
for existing works to simultaneously provide generic and personalized
interpretability. To address these challenges, we first propose a hyperbolic
embedding method with information flow to pre-train medical code
representations in a hierarchical structure. We incorporate these pre-trained
representations into a graph neural network to detect disease complications,
and design a multi-level attention method to compute the contributions of
particular diseases and admissions, thus enhancing personalized
interpretability. We present a new hierarchy-enhanced historical prediction
proxy task in our self-supervised learning framework to fully utilize EHR data
and exploit medical domain knowledge. We conduct a comprehensive set of
experiments and case studies on widely used publicly available EHR datasets to
verify the effectiveness of our model. The results demonstrate our model's
strengths in both predictive tasks and interpretable abilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chandan K. Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1"&gt;Yue Ning&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which transformer architecture fits my data? A vocabulary bottleneck in self-attention. (arXiv:2105.03928v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03928</id>
        <link href="http://arxiv.org/abs/2105.03928"/>
        <updated>2021-06-10T01:56:48.168Z</updated>
        <summary type="html"><![CDATA[After their successful debut in natural language processing, Transformer
architectures are now becoming the de-facto standard in many domains. An
obstacle for their deployment over new modalities is the architectural
configuration: the optimal depth-to-width ratio has been shown to dramatically
vary across data types (e.g., $10$x larger over images than over language). We
theoretically predict the existence of an embedding rank bottleneck that limits
the contribution of self-attention width to the Transformer expressivity. We
thus directly tie the input vocabulary size and rank to the optimal
depth-to-width ratio, since a small vocabulary size or rank dictates an added
advantage of depth over width. We empirically demonstrate the existence of this
bottleneck and its implications on the depth-to-width interplay of Transformer
architectures, linking the architecture variability across domains to the often
glossed-over usage of different vocabulary sizes or embedding ranks in
different domains. As an additional benefit, our rank bottlenecking framework
allows us to identify size redundancies of $25\%-50\%$ in leading NLP models
such as ALBERT and T5.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1"&gt;Noam Wies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1"&gt;Yoav Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jannai_D/0/1/0/all/0/1"&gt;Daniel Jannai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1"&gt;Amnon Shashua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Music Generation using Three-layered LSTM. (arXiv:2105.09046v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09046</id>
        <link href="http://arxiv.org/abs/2105.09046"/>
        <updated>2021-06-10T01:56:48.163Z</updated>
        <summary type="html"><![CDATA[This paper explores the idea of utilising Long Short-Term Memory neural
networks (LSTMNN) for the generation of musical sequences in ABC notation. The
proposed approach takes ABC notations from the Nottingham dataset and encodes
it to be fed as input for the neural networks. The primary objective is to
input the neural networks with an arbitrary note, let the network process and
augment a sequence based on the note until a good piece of music is produced.
Multiple calibrations have been done to amend the parameters of the network for
optimal generation. The output is assessed on the basis of rhythm, harmony, and
grammar accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ingale_V/0/1/0/all/0/1"&gt;Vaishali Ingale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1"&gt;Anush Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adlakha_D/0/1/0/all/0/1"&gt;Divit Adlakha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1"&gt;Krishan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Mohit Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Invariant Risk Minimization: A Causal Approach. (arXiv:2102.12353v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12353</id>
        <link href="http://arxiv.org/abs/2102.12353"/>
        <updated>2021-06-10T01:56:48.156Z</updated>
        <summary type="html"><![CDATA[Due to spurious correlations, machine learning systems often fail to
generalize to environments whose distributions differ from the ones used at
training time. Prior work addressing this, either explicitly or implicitly,
attempted to find a data representation that has an invariant relationship with
the target. This is done by leveraging a diverse set of training environments
to reduce the effect of spurious features and build an invariant predictor.
However, these methods have generalization guarantees only when both data
representation and classifiers come from a linear model class. We propose
invariant Causal Representation Learning (iCaRL), an approach that enables
out-of-distribution (OOD) generalization in the nonlinear setting (i.e.,
nonlinear representations and nonlinear classifiers). It builds upon a
practical and general assumption: the prior over the data representation (i.e.,
a set of latent variables encoding the data) given the target and the
environment belongs to general exponential family distributions. Based on this,
we show that it is possible to identify the data representation up to simple
transformations. We also prove that all direct causes of the target can be
fully discovered, which further enables us to obtain generalization guarantees
in the nonlinear setting. Extensive experiments on both synthetic and
real-world datasets show that our approach outperforms a variety of baseline
methods. Finally, in the discussion, we further explore the aforementioned
assumption and propose a more general hypothesis, called the Agnostic
Hypothesis: there exist a set of hidden causal factors affecting both inputs
and outcomes. The Agnostic Hypothesis can provide a unifying view of machine
learning. More importantly, it can inspire a new direction to explore a general
theory for identifying hidden causal factors, which is key to enabling the OOD
generalization guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chaochao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuhuai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1"&gt;Jo&amp;#x15b;e Miguel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Periodic-GP: Learning Periodic World with Gaussian Process Bandits. (arXiv:2105.14422v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14422</id>
        <link href="http://arxiv.org/abs/2105.14422"/>
        <updated>2021-06-10T01:56:48.139Z</updated>
        <summary type="html"><![CDATA[We consider the sequential decision optimization on the periodic environment,
that occurs in a wide variety of real-world applications when the data involves
seasonality, such as the daily demand of drivers in ride-sharing and dynamic
traffic patterns in transportation. In this work, we focus on learning the
stochastic periodic world by leveraging this seasonal law. To deal with the
general action space, we use the bandit based on Gaussian process (GP) as the
base model due to its flexibility and generality, and propose the Periodic-GP
method with a temporal periodic kernel based on the upper confidence bound.
Theoretically, we provide a new regret bound of the proposed method, by
explicitly characterizing the periodic kernel in the periodic stationary model.
Empirically, the proposed algorithm significantly outperforms the existing
methods in both synthetic data experiments and a real data application on
Madrid traffic pollution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hengrui Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cen_Z/0/1/0/all/0/1"&gt;Zhihao Cen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leng_L/0/1/0/all/0/1"&gt;Ling Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Rui Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-based Optimization Methods for Model-Agnostic Meta-Learning. (arXiv:2106.04911v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04911</id>
        <link href="http://arxiv.org/abs/2106.04911"/>
        <updated>2021-06-10T01:56:48.128Z</updated>
        <summary type="html"><![CDATA[Recently, model-agnostic meta-learning (MAML) has garnered tremendous
attention. However, stochastic optimization of MAML is still immature. Existing
algorithms for MAML are based on the ``episode" idea by sampling a number of
tasks and a number of data points for each sampled task at each iteration for
updating the meta-model. However, they either do not necessarily guarantee
convergence with a constant mini-batch size or require processing a larger
number of tasks at every iteration, which is not viable for continual learning
or cross-device federated learning where only a small number of tasks are
available per-iteration or per-round. This paper addresses these issues by (i)
proposing efficient memory-based stochastic algorithms for MAML with a
diminishing convergence error, which only requires sampling a constant number
of tasks and a constant number of examples per-task per-iteration; (ii)
proposing communication-efficient distributed memory-based MAML algorithms for
personalized federated learning in both the cross-device (w/ client sampling)
and the cross-silo (w/o client sampling) settings. The key novelty of the
proposed algorithms is to maintain an individual personalized model (aka
memory) for each task besides the meta-model and only update them for the
sampled tasks by a momentum method that incorporates historical updates at each
iteration. The theoretical results significantly improve the optimization
theory for MAML and the empirical results also corroborate the theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bokun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Zhuoning Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1"&gt;Yiming Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianbao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images. (arXiv:2106.00116v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00116</id>
        <link href="http://arxiv.org/abs/2106.00116"/>
        <updated>2021-06-10T01:56:48.121Z</updated>
        <summary type="html"><![CDATA[Transfer learning aims to exploit pre-trained models for more efficient
follow-up training on wide range of downstream tasks and datasets, enabling
successful training also on small data. Recent line of work posits strong
benefits for model generalization and transfer when model size, data size, and
compute budget are increased for the pre-training. It remains however still
largely unclear whether the observed transfer improvement due to increase in
scale also holds when source and target data distributions are far apart from
each other. In this work we conduct large-scale pre-training on large source
datasets of either natural (ImageNet-21k/1k) or medical chest X-Ray images and
compare full and few-shot transfer using different target datasets from both
natural and medical imaging domains. Our observations provide evidence that
while pre-training and transfer on closely related datasets do show clear
benefit of increasing model and data size during pre-training, such benefits
are not clearly visible when source and target datasets are further apart.
These observations hold across both full and few-shot transfer and indicate
that scaling laws pointing to improvement of generalization and transfer with
increasing model and data size are incomplete and should be revised by taking
into account the type and proximity of the source and target data, to correctly
predict the effect of model and data scale during pre-training on transfer.
Remarkably, in full shot transfer to a large X-Ray chest imaging target
(PadChest), the largest model pre-trained on ImageNet-21k slightly outperforms
best models pre-trained on large X-Ray chest imaging data. This indicates
possibility to obtain high quality models for domain-specific transfer even
without access to large domain-specific data, by pre-training instead on
comparably very large, generic source data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1"&gt;Mehdi Cherti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1"&gt;Jenia Jitsev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TempoRL: Learning When to Act. (arXiv:2106.05262v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05262</id>
        <link href="http://arxiv.org/abs/2106.05262"/>
        <updated>2021-06-10T01:56:48.115Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is a powerful approach to learn behaviour through
interactions with an environment. However, behaviours are usually learned in a
purely reactive fashion, where an appropriate action is selected based on an
observation. In this form, it is challenging to learn when it is necessary to
execute new decisions. This makes learning inefficient, especially in
environments that need various degrees of fine and coarse control. To address
this, we propose a proactive setting in which the agent not only selects an
action in a state but also for how long to commit to that action. Our TempoRL
approach introduces skip connections between states and learns a skip-policy
for repeating the same action along these skips. We demonstrate the
effectiveness of TempoRL on a variety of traditional and deep RL environments,
showing that our approach is capable of learning successful policies up to an
order of magnitude faster than vanilla Q-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Biedenkapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_R/0/1/0/all/0/1"&gt;Raghu Rajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1"&gt;Marius Lindauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Transformers Are Secretly Fast Weight Programmers. (arXiv:2102.11174v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11174</id>
        <link href="http://arxiv.org/abs/2102.11174"/>
        <updated>2021-06-10T01:56:48.101Z</updated>
        <summary type="html"><![CDATA[We show the formal equivalence of linearised self-attention mechanisms and
fast weight controllers from the early '90s, where a ``slow" neural net learns
by gradient descent to program the ``fast weights" of another net through
sequences of elementary programming instructions which are additive outer
products of self-invented activation patterns (today called keys and values).
Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a
finite memory and dynamically interact with it. We infer a memory capacity
limitation of recent linearised softmax attention variants, and replace the
purely additive outer products by a delta rule-like programming instruction,
such that the FWP can more easily learn to correct the current mapping from
keys to values. The FWP also learns to compute dynamically changing learning
rates. We also propose a new kernel function to linearise attention which
balances simplicity and effectiveness. We conduct experiments on synthetic
retrieval problems as well as standard machine translation and language
modelling tasks which demonstrate the benefits of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1"&gt;Imanol Schlag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1"&gt;Kazuki Irie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer. (arXiv:2106.04833v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04833</id>
        <link href="http://arxiv.org/abs/2106.04833"/>
        <updated>2021-06-10T01:56:48.084Z</updated>
        <summary type="html"><![CDATA[End-to-end simultaneous speech translation (SST), which directly translates
speech in one language into text in another language in real-time, is useful in
many scenarios but has not been fully investigated. In this work, we propose
RealTranS, an end-to-end model for SST. To bridge the modality gap between
speech and text, RealTranS gradually downsamples the input speech with
interleaved convolution and unidirectional Transformer layers for acoustic
modeling, and then maps speech features into text space with a
weighted-shrinking operation and a semantic encoder. Besides, to improve the
model performance in simultaneous scenarios, we propose a blank penalty to
enhance the shrinking quality and a Wait-K-Stride-N strategy to allow local
reranking during decoding. Experiments on public and widely-used datasets show
that RealTranS with the Wait-K-Stride-N strategy outperforms prior end-to-end
models as well as cascaded models in diverse latency settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xingshan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liangyou Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GP-ConvCNP: Better Generalization for Convolutional Conditional Neural Processes on Time Series Data. (arXiv:2106.04967v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04967</id>
        <link href="http://arxiv.org/abs/2106.04967"/>
        <updated>2021-06-10T01:56:48.079Z</updated>
        <summary type="html"><![CDATA[Neural Processes (NPs) are a family of conditional generative models that are
able to model a distribution over functions, in a way that allows them to
perform predictions at test time conditioned on a number of context points. A
recent addition to this family, Convolutional Conditional Neural Processes
(ConvCNP), have shown remarkable improvement in performance over prior art, but
we find that they sometimes struggle to generalize when applied to time series
data. In particular, they are not robust to distribution shifts and fail to
extrapolate observed patterns into the future. By incorporating a Gaussian
Process into the model, we are able to remedy this and at the same time improve
performance within distribution. As an added benefit, the Gaussian Process
reintroduces the possibility to sample from the model, a key feature of other
members in the NP family.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1"&gt;Jens Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohler_G/0/1/0/all/0/1"&gt;Gregor K&amp;#xf6;hler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmerer_D/0/1/0/all/0/1"&gt;David Zimmerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1"&gt;Fabian Isensee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jager_P/0/1/0/all/0/1"&gt;Paul F. J&amp;#xe4;ger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1"&gt;Klaus H. Maier-Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09501</id>
        <link href="http://arxiv.org/abs/2105.09501"/>
        <updated>2021-06-10T01:56:48.073Z</updated>
        <summary type="html"><![CDATA[Existing multilingual machine translation approaches mainly focus on
English-centric directions, while the non-English directions still lag behind.
In this work, we aim to build a many-to-many translation system with an
emphasis on the quality of non-English language directions. Our intuition is
based on the hypothesis that a universal cross-language representation leads to
better multilingual translation performance. To this end, we propose mRASP2, a
training method to obtain a single unified multilingual translation model.
mRASP2 is empowered by two techniques: a) a contrastive learning scheme to
close the gap among representations of different languages, and b) data
augmentation on both multiple parallel and monolingual data to further align
token representations. For English-centric directions, mRASP2 outperforms
existing best unified model and achieves competitive or even better performance
than the pre-trained and fine-tuned model mBART on tens of WMT's translation
directions. For non-English directions, mRASP2 achieves an improvement of
average 10+ BLEU compared with the multilingual Transformer baseline. Code,
data and trained models are available at https://github.com/PANXiao1994/mRASP2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Binary Neural Network Operation from 233 K to 398 K via Gate Stack and Bias Optimization of Ferroelectric FinFET Synapses. (arXiv:2103.03111v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03111</id>
        <link href="http://arxiv.org/abs/2103.03111"/>
        <updated>2021-06-10T01:56:48.067Z</updated>
        <summary type="html"><![CDATA[A synergistic approach for optimizing devices, circuits, and neural network
architectures was used to abate junction-temperature-change-induced performance
degradation of a Fe-FinFET-based artificial neural network. We demonstrated
that the digital nature of the binarized neural network, with the "0" state
programmed deep in the subthreshold and the "1" state in strong inversion, is
crucial for robust DNN inference. The performance of a purely software-based
binary neural network (BNN), with 96.1% accuracy for Modified National
Institute of Standards and Technology (MNIST) handwritten digit recognition,
was used as a baseline. The Fe-FinFET-based BNN (including device-to-device
variation at 300 K) achieved 95.7% inference accuracy on the MNIST dataset.
Although substantial inference accuracy degradation with temperature change was
observed in a nonbinary neural network, the BNN with optimized Fe-FinFETs as
synaptic devices had excellent resistance to temperature change effects and
maintained a minimum inference accuracy of 95.2% within a temperature range of
-233K to 398K after gate stack and bias optimization. However, reprogramming to
adjust device conductance was necessary for temperatures higher than 398K.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1"&gt;Sourav De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hoang-Hiep Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1"&gt;Bo-Han Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baig_M/0/1/0/all/0/1"&gt;Md. Aftab Baig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_P/0/1/0/all/0/1"&gt;Po-Jung Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chung Jun Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yao-Jen Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;Darsen D. Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autobahn: Automorphism-based Graph Neural Nets. (arXiv:2103.01710v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01710</id>
        <link href="http://arxiv.org/abs/2103.01710"/>
        <updated>2021-06-10T01:56:48.061Z</updated>
        <summary type="html"><![CDATA[We introduce Automorphism-based graph neural networks (Autobahn), a new
family of graph neural networks. In an Autobahn, we decompose the graph into a
collection of subgraphs and apply local convolutions that are equivariant to
each subgraph's automorphism group. Specific choices of local neighborhoods and
subgraphs recover existing architectures such as message passing neural
networks. Our formalism also encompasses novel architectures: as an example, we
introduce a graph neural network that decomposes the graph into paths and
cycles. The resulting convolutions reflect the natural way that parts of the
graph can transform, preserving the intuitive meaning of convolution without
sacrificing global permutation equivariance. We validate our approach by
applying Autobahn to molecular graphs, where it achieves state-of-the-art
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thiede_E/0/1/0/all/0/1"&gt;Erik Henning Thiede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wenda Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondor_R/0/1/0/all/0/1"&gt;Risi Kondor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Annealing for Automated Feature Selection in Stress Detection. (arXiv:2106.05134v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.05134</id>
        <link href="http://arxiv.org/abs/2106.05134"/>
        <updated>2021-06-10T01:56:48.044Z</updated>
        <summary type="html"><![CDATA[We present a novel methodology for automated feature subset selection from a
pool of physiological signals using Quantum Annealing (QA). As a case study, we
will investigate the effectiveness of QA-based feature selection techniques in
selecting the optimal feature subset for stress detection. Features are
extracted from four signal sources: foot EDA, hand EDA, ECG, and respiration.
The proposed method embeds the feature variables extracted from the
physiological signals in a binary quadratic model. The bias of the feature
variable is calculated using the Pearson correlation coefficient between the
feature variable and the target variable. The weight of the edge connecting the
two feature variables is calculated using the Pearson correlation coefficient
between two feature variables in the binary quadratic model. Subsequently,
D-Wave's clique sampler is used to sample cliques from the binary quadratic
model. The underlying solution is then re-sampled to obtain multiple good
solutions and the clique with the lowest energy is returned as the optimal
solution. The proposed method is compared with commonly used feature selection
techniques for stress detection. Results indicate that QA-based feature subset
selection performed equally as that of classical techniques. However, under
data uncertainty conditions such as limited training data, the performance of
quantum annealing for selecting optimum features remained unaffected, whereas a
significant decrease in performance is observed with classical feature
selection techniques. Preliminary results show the promise of quantum annealing
in optimizing the training phase of a machine learning classifier, especially
under data uncertainty conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Nath_R/0/1/0/all/0/1"&gt;Rajdeep Kumar Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Thapliyal_H/0/1/0/all/0/1"&gt;Himanshu Thapliyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Humble_T/0/1/0/all/0/1"&gt;Travis S. Humble&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and More Powerful Selective Inference for Sparse High-order Interaction Model. (arXiv:2106.04929v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04929</id>
        <link href="http://arxiv.org/abs/2106.04929"/>
        <updated>2021-06-10T01:56:48.038Z</updated>
        <summary type="html"><![CDATA[Automated high-stake decision-making such as medical diagnosis requires
models with high interpretability and reliability. As one of the interpretable
and reliable models with good prediction ability, we consider Sparse High-order
Interaction Model (SHIM) in this study. However, finding statistically
significant high-order interactions is challenging due to the intrinsic high
dimensionality of the combinatorial effects. Another problem in data-driven
modeling is the effect of "cherry-picking" a.k.a. selection bias. Our main
contribution is to extend the recently developed parametric programming
approach for selective inference to high-order interaction models. Exhaustive
search over the cherry tree (all possible interactions) can be daunting and
impractical even for a small-sized problem. We introduced an efficient pruning
strategy and demonstrated the computational efficiency and statistical power of
the proposed method using both synthetic and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Das_D/0/1/0/all/0/1"&gt;Diptesh Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duy_V/0/1/0/all/0/1"&gt;Vo Nguyen Le Duy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hanada_H/0/1/0/all/0/1"&gt;Hiroyuki Hanada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tsuda_K/0/1/0/all/0/1"&gt;Koji Tsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1"&gt;Ichiro Takeuchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it Enough to Optimize CNN Architectures on ImageNet?. (arXiv:2103.09108v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09108</id>
        <link href="http://arxiv.org/abs/2103.09108"/>
        <updated>2021-06-10T01:56:48.032Z</updated>
        <summary type="html"><![CDATA[An implicit but pervasive hypothesis of modern computer vision research is
that convolutional neural network (CNN) architectures that perform better on
ImageNet will also perform better on other vision datasets. We challenge this
hypothesis through an extensive empirical study for which we train 500 sampled
CNN architectures on ImageNet as well as 8 other image classification datasets
from a wide array of application domains. The relationship between architecture
and performance varies wildly, depending on the datasets. For some of them, the
performance correlation with ImageNet is even negative. Clearly, it is not
enough to optimize architectures solely for ImageNet when aiming for progress
that is relevant for all applications. Therefore, we identify two
dataset-specific performance indicators: the cumulative width across layers as
well as the total depth of the network. Lastly, we show that the range of
dataset variability covered by ImageNet can be significantly extended by adding
ImageNet subsets restricted to few classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuggener_L/0/1/0/all/0/1"&gt;Lukas Tuggener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1"&gt;Thilo Stadelmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedDR -- Randomized Douglas-Rachford Splitting Algorithms for Nonconvex Federated Composite Optimization. (arXiv:2103.03452v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03452</id>
        <link href="http://arxiv.org/abs/2103.03452"/>
        <updated>2021-06-10T01:56:48.027Z</updated>
        <summary type="html"><![CDATA[We develop two new algorithms, called, FedDR and asyncFedDR, for solving a
fundamental nonconvex composite optimization problem in federated learning. Our
algorithms rely on a novel combination between a nonconvex Douglas-Rachford
splitting method, randomized block-coordinate strategies, and asynchronous
implementation. They can also handle convex regularizers. Unlike recent methods
in the literature, e.g., FedSplit and FedPD, our algorithms update only a
subset of users at each communication round, and possibly in an asynchronous
manner, making them more practical. These new algorithms also achieve
communication efficiency and more importantly can handle statistical and system
heterogeneity, which are the two main challenges in federated learning. Our
convergence analysis shows that the new algorithms match the communication
complexity lower bound up to a constant factor under standard assumptions. Our
numerical experiments illustrate the advantages of our methods compared to
existing ones on several datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tran_Dinh_Q/0/1/0/all/0/1"&gt;Quoc Tran-Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pham_N/0/1/0/all/0/1"&gt;Nhan H. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Phan_D/0/1/0/all/0/1"&gt;Dzung T. Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Lam M. Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Reliable Process Event Streams and Time Series Data based on Neural Networks. (arXiv:2103.05462v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05462</id>
        <link href="http://arxiv.org/abs/2103.05462"/>
        <updated>2021-06-10T01:56:48.021Z</updated>
        <summary type="html"><![CDATA[Domains such as manufacturing and medicine crave for continuous monitoring
and analysis of their processes, especially in combination with time series as
produced by sensors. Time series data can be exploited to, for example, explain
and predict concept drifts during runtime. Generally, a certain data volume is
required in order to produce meaningful analysis results. However, reliable
data sets are often missing, for example, if event streams and times series
data are collected separately, in case of a new process, or if it is too
expensive to obtain a sufficient data volume. Additional challenges arise with
preparing time series data from multiple event sources, variations in data
collection frequency, and concept drift. This paper proposes the GENLOG
approach to generate reliable event and time series data that follows the
distribution of the underlying input data set. GENLOG employs data resampling
and enables the user to select different parts of the log data to orchestrate
the training of a recurrent neural network for stream generation. The generated
data is sampled back to its original sample rate and is embedded into the
originating log data file. Overall, GENLOG can boost small data sets and
consequently the application of online process mining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herbert_T/0/1/0/all/0/1"&gt;Tobias Herbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangler_J/0/1/0/all/0/1"&gt;Juergen Mangler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rinderle_Ma_S/0/1/0/all/0/1"&gt;Stefanie Rinderle-Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Objective Robustness in Deep Reinforcement Learning. (arXiv:2105.14111v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14111</id>
        <link href="http://arxiv.org/abs/2105.14111"/>
        <updated>2021-06-10T01:56:48.015Z</updated>
        <summary type="html"><![CDATA[We study objective robustness failures, a type of out-of-distribution
robustness failure in reinforcement learning (RL). Objective robustness
failures occur when an RL agent retains its capabilities out-of-distribution
yet pursues the wrong objective. This kind of failure presents different risks
than the robustness problems usually considered in the literature, since it
involves agents that leverage their capabilities to pursue the wrong objective
rather than simply failing to do anything useful. We provide the first explicit
empirical demonstrations of objective robustness failures and present a partial
characterization of its causes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1"&gt;Jack Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1"&gt;Lauro Langosco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfau_J/0/1/0/all/0/1"&gt;Jacob Pfau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_J/0/1/0/all/0/1"&gt;James Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharkey_L/0/1/0/all/0/1"&gt;Lee Sharkey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPINN: Sparse, Physics-based, and Interpretable Neural Networks for PDEs. (arXiv:2102.13037v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13037</id>
        <link href="http://arxiv.org/abs/2102.13037"/>
        <updated>2021-06-10T01:56:47.998Z</updated>
        <summary type="html"><![CDATA[We introduce a class of Sparse, Physics-based, and Interpretable Neural
Networks (SPINN) for solving ordinary and partial differential equations
(PDEs). By reinterpreting a traditional meshless representation of solutions of
PDEs we develop a class of sparse neural network architectures that are
interpretable. The SPINN model we propose here serves as a seamless bridge
between two extreme modeling tools for PDEs, namely dense neural network based
methods like Physics Informed Neural Networks (PINNs) and traditional mesh-free
numerical methods, thereby providing a novel means to develop a new class of
hybrid algorithms that build on the best of both these viewpoints. A unique
feature of the SPINN model that distinguishes it from other neural network
based approximations proposed earlier is that it is (i) interpretable, and (ii)
sparse in the sense that it has much fewer connections than typical dense
neural networks used for PDEs. Further, the SPINN algorithm implicitly encodes
mesh adaptivity and is able to handle discontinuities in the solutions. In
addition, we demonstrate that Fourier series representations can also be
expressed as a special class of SPINN and propose generalized neural network
analogues of Fourier representations. We illustrate the utility of the proposed
method with a variety of examples involving ordinary differential equations,
elliptic, parabolic, hyperbolic and nonlinear partial differential equations,
and an example in fluid dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramabathiran_A/0/1/0/all/0/1"&gt;Amuthan A. Ramabathiran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1"&gt;Prabhu Ramachandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully differentiable model discovery. (arXiv:2106.04886v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04886</id>
        <link href="http://arxiv.org/abs/2106.04886"/>
        <updated>2021-06-10T01:56:47.992Z</updated>
        <summary type="html"><![CDATA[Model discovery aims at autonomously discovering differential equations
underlying a dataset. Approaches based on Physics Informed Neural Networks
(PINNs) have shown great promise, but a fully-differentiable model which
explicitly learns the equation has remained elusive. In this paper we propose
such an approach by combining neural network based surrogates with Sparse
Bayesian Learning (SBL). We start by reinterpreting PINNs as multitask models,
applying multitask learning using uncertainty, and show that this leads to a
natural framework for including Bayesian regression techniques. We then
construct a robust model discovery algorithm by using SBL, which we showcase on
various datasets. Concurrently, the multitask approach allows the use of
probabilistic approximators, and we show a proof of concept using normalizing
flows to directly learn a density model from single particle data. Our work
expands PINNs to various types of neural network architectures, and connects
neural network-based surrogates to the rich field of Bayesian parameter
inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Both_G/0/1/0/all/0/1"&gt;Gert-Jan Both&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kusters_R/0/1/0/all/0/1"&gt;Remy Kusters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WGAN with an Infinitely Wide Generator Has No Spurious Stationary Points. (arXiv:2102.07541v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07541</id>
        <link href="http://arxiv.org/abs/2102.07541"/>
        <updated>2021-06-10T01:56:47.983Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GAN) are a widely used class of deep
generative models, but their minimax training dynamics are not understood very
well. In this work, we show that GANs with a 2-layer infinite-width generator
and a 2-layer finite-width discriminator trained with stochastic gradient
ascent-descent have no spurious stationary points. We then show that when the
width of the generator is finite but wide, there are no spurious stationary
points within a ball whose radius becomes arbitrarily large (to cover the
entire parameter space) as the width goes to infinity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+No_A/0/1/0/all/0/1"&gt;Albert No&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_T/0/1/0/all/0/1"&gt;TaeHo Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1"&gt;Sehyun Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryu_E/0/1/0/all/0/1"&gt;Ernest K. Ryu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does the Adam Optimizer Exacerbate Catastrophic Forgetting?. (arXiv:2102.07686v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07686</id>
        <link href="http://arxiv.org/abs/2102.07686"/>
        <updated>2021-06-10T01:56:47.976Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting remains a severe hindrance to the broad application
of artificial neural networks (ANNs), however, it continues to be a poorly
understood phenomenon. Despite the extensive amount of work on catastrophic
forgetting, we argue that it is still unclear how exactly the phenomenon should
be quantified, and, moreover, to what degree all of the choices we make when
designing learning systems affect the amount of catastrophic forgetting. We use
various testbeds from the reinforcement learning and supervised learning
literature to (1) provide evidence that the choice of which modern
gradient-based optimization algorithm is used to train an ANN has a significant
impact on the amount of catastrophic forgetting and show that-surprisingly-in
many instances classical algorithms such as vanilla SGD experience less
catastrophic forgetting than the more modern algorithms such as Adam. We
empirically compare four different existing metrics for quantifying
catastrophic forgetting and (2) show that the degree to which the learning
systems experience catastrophic forgetting is sufficiently sensitive to the
metric used that a change from one principled metric to another is enough to
change the conclusions of a study dramatically. Our results suggest that a much
more rigorous experimental methodology is required when looking at catastrophic
forgetting. Based on our results, we recommend inter-task forgetting in
supervised learning must be measured with both retention and relearning metrics
concurrently, and intra-task forgetting in reinforcement learning must-at the
very least-be measured with pairwise interference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1"&gt;Dylan R. Ashley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiassian_S/0/1/0/all/0/1"&gt;Sina Ghiassian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1"&gt;Richard S. Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Algorithms for Markovian Gaussian Processes. (arXiv:2103.10710v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10710</id>
        <link href="http://arxiv.org/abs/2103.10710"/>
        <updated>2021-06-10T01:56:47.970Z</updated>
        <summary type="html"><![CDATA[Approximate Bayesian inference methods that scale to very large datasets are
crucial in leveraging probabilistic models for real-world time series. Sparse
Markovian Gaussian processes combine the use of inducing variables with
efficient Kalman filter-like recursions, resulting in algorithms whose
computational and memory requirements scale linearly in the number of inducing
points, whilst also enabling parallel parameter updates and stochastic
optimisation. Under this paradigm, we derive a general site-based approach to
approximate inference, whereby we approximate the non-Gaussian likelihood with
local Gaussian terms, called sites. Our approach results in a suite of novel
sparse extensions to algorithms from both the machine learning and signal
processing literature, including variational inference, expectation
propagation, and the classical nonlinear Kalman smoothers. The derived methods
are suited to large time series, and we also demonstrate their applicability to
spatio-temporal data, where the model has separate inducing points in both time
and space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wilkinson_W/0/1/0/all/0/1"&gt;William J. Wilkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Solin_A/0/1/0/all/0/1"&gt;Arno Solin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Adam_V/0/1/0/all/0/1"&gt;Vincent Adam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization. (arXiv:2106.04923v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04923</id>
        <link href="http://arxiv.org/abs/2106.04923"/>
        <updated>2021-06-10T01:56:47.955Z</updated>
        <summary type="html"><![CDATA[Domain shifts in the training data are common in practical applications of
machine learning, they occur for instance when the data is coming from
different sources. Ideally, a ML model should work well independently of these
shifts, for example, by learning a domain-invariant representation. Moreover,
privacy concerns regarding the source also require a domain-invariant
representation. In this work, we provide theoretical results that link domain
invariant representations -- measured by the Wasserstein distance on the joint
distributions -- to a practical semi-supervised learning objective based on a
cross-entropy classifier and a novel domain critic. Quantitative experiments
demonstrate that the proposed approach is indeed able to practically learn such
an invariant representation (between two domains), and the latter also supports
models with higher predictive accuracy on both domains, comparing favorably to
existing techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Andeol_L/0/1/0/all/0/1"&gt;L&amp;#xe9;o And&amp;#xe9;ol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kawakami_Y/0/1/0/all/0/1"&gt;Yusei Kawakami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wada_Y/0/1/0/all/0/1"&gt;Yuichiro Wada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kanamori_T/0/1/0/all/0/1"&gt;Takafumi Kanamori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Montavon_G/0/1/0/all/0/1"&gt;Gr&amp;#xe9;goire Montavon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal Least Squares Based Fast Feature Selection for Linear Classification. (arXiv:2101.08539v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08539</id>
        <link href="http://arxiv.org/abs/2101.08539"/>
        <updated>2021-06-10T01:56:47.944Z</updated>
        <summary type="html"><![CDATA[An Orthogonal Least Squares (OLS) based feature selection method is proposed
for both binomial and multinomial classification. The novel Squared Orthogonal
Correlation Coefficient (SOCC) is defined based on Error Reduction Ratio (ERR)
in OLS and used as the feature ranking criterion. The equivalence between the
canonical correlation coefficient, Fisher's criterion, and the sum of the SOCCs
is revealed, which unveils the statistical implication of ERR in OLS for the
first time. It is also shown that the OLS based feature selection method has
speed advantages when applied for greedy search. The proposed method is
comprehensively compared with the mutual information based feature selection
methods in 2 synthetic and 7 real world datasets. The results show that the
proposed method is always in the top 5 among the 10 candidate methods. Besides,
the proposed method can be directly applied to continuous features without
discretisation, which is another significant advantage over mutual information
based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sikai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_Z/0/1/0/all/0/1"&gt;Zi-Qiang Lang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Hawkes Processes in Time-Varying System. (arXiv:2106.04844v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04844</id>
        <link href="http://arxiv.org/abs/2106.04844"/>
        <updated>2021-06-10T01:56:47.938Z</updated>
        <summary type="html"><![CDATA[Hawkes processes are a class of point processes that have the ability to
model the self- and mutual-exciting phenomena. Although the classic Hawkes
processes cover a wide range of applications, their expressive ability is
limited due to three key hypotheses: parametric, linear and homogeneous. Recent
work has attempted to address these limitations separately. This work aims to
overcome all three assumptions simultaneously by proposing the flexible
state-switching Hawkes processes: a flexible, nonlinear and nonhomogeneous
variant where a state process is incorporated to interact with the point
processes. The proposed model empowers Hawkes processes to be applied to
time-varying systems. For inference, we utilize the latent variable
augmentation technique to design two efficient Bayesian inference algorithms:
Gibbs sampler and mean-field variational inference, with analytical iterative
updates to estimate the posterior. In experiments, our model achieves superior
performance compared to the state-of-the-art competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quyu Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yixuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Cheng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stable High-order Tuner for General Convex Functions. (arXiv:2011.09996v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09996</id>
        <link href="http://arxiv.org/abs/2011.09996"/>
        <updated>2021-06-10T01:56:47.930Z</updated>
        <summary type="html"><![CDATA[Iterative gradient-based algorithms have been increasingly applied for the
training of a broad variety of machine learning models including large
neural-nets. In particular, momentum-based methods, with accelerated learning
guarantees, have received a lot of attention due to their provable guarantees
of fast learning in certain classes of problems and multiple algorithms have
been derived. However, properties for these methods hold only for constant
regressors. When time-varying regressors occur, which is commonplace in dynamic
systems, many of these momentum-based methods cannot guarantee stability.
Recently, a new High-order Tuner (HT) was developed for linear regression
problems and shown to have 1) stability and asymptotic convergence for
time-varying regressors and 2) non-asymptotic accelerated learning guarantees
for constant regressors. In this paper, we extend and discuss the results of
this same HT for general convex loss functions. Through the exploitation of
convexity and smoothness definitions, we establish similar stability and
asymptotic convergence guarantees. Finally, we provide numerical simulations
supporting the satisfactory behavior of the HT algorithm as well as an
accelerated learning property.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moreu_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; M. Moreu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Annaswamy_A/0/1/0/all/0/1"&gt;Anuradha M. Annaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Explainable Abnormal Infant Movements Identification: A Body-part Based Prediction and Visualisation Framework. (arXiv:2106.04966v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04966</id>
        <link href="http://arxiv.org/abs/2106.04966"/>
        <updated>2021-06-10T01:56:47.924Z</updated>
        <summary type="html"><![CDATA[Providing early diagnosis of cerebral palsy (CP) is key to enhancing the
developmental outcomes for those affected. Diagnostic tools such as the General
Movements Assessment (GMA), have produced promising results in early diagnosis,
however these manual methods can be laborious.

In this paper, we propose a new framework for the automated classification of
infant body movements, based upon the GMA, which unlike previous methods, also
incorporates a visualization framework to aid with interpretability. Our
proposed framework segments extracted features to detect the presence of
Fidgety Movements (FMs) associated with the GMA spatiotemporally. These
features are then used to identify the body-parts with the greatest
contribution towards a classification decision and highlight the related
body-part segment providing visual feedback to the user.

We quantitatively compare the proposed framework's classification performance
with several other methods from the literature and qualitatively evaluate the
visualization's veracity. Our experimental results show that the proposed
method performs more robustly than comparable techniques in this setting whilst
simultaneously providing relevant visual interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McCay_K/0/1/0/all/0/1"&gt;Kevin D. McCay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1"&gt;Edmond S. L. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakkos_D/0/1/0/all/0/1"&gt;Dimitrios Sakkos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1"&gt;Wai Lok Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcroft_C/0/1/0/all/0/1"&gt;Claire Marcroft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dulson_P/0/1/0/all/0/1"&gt;Patricia Dulson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Embleton_N/0/1/0/all/0/1"&gt;Nicholas D. Embleton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Densely connected normalizing flows. (arXiv:2106.04627v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04627</id>
        <link href="http://arxiv.org/abs/2106.04627"/>
        <updated>2021-06-10T01:56:47.907Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are bijective mappings between inputs and latent
representations with a fully factorized distribution. They are very attractive
due to exact likelihood evaluation and efficient sampling. However, their
effective capacity is often insufficient since the bijectivity constraint
limits the model width. We address this issue by incrementally padding
intermediate representations with noise. We precondition the noise in
accordance with previous invertible units, which we describe as cross-unit
coupling. Our invertible glow-like modules express intra-unit affine coupling
as a fusion of a densely connected block and Nystr\"om self-attention. We refer
to our architecture as DenseFlow since both cross-unit and intra-unit couplings
rely on dense connectivity. Experiments show significant improvements due to
the proposed contributions, and reveal state-of-the-art density estimation
among all generative models under moderate computing budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grcic_M/0/1/0/all/0/1"&gt;Matej Grci&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1"&gt;Ivan Grubi&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1"&gt;Sini&amp;#x161;a &amp;#x160;egvi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Tracking Control via Strongly Adaptive Online Learning with Memory. (arXiv:2102.01623v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01623</id>
        <link href="http://arxiv.org/abs/2102.01623"/>
        <updated>2021-06-10T01:56:47.902Z</updated>
        <summary type="html"><![CDATA[We consider tracking adversarial targets in a delayed time-varying linear
system with adversarial disturbances and loss functions, which significantly
generalizes earlier work. To this end, we develop three techniques that each
could be of independent interest. First, we propose a black-box reduction from
adversarial tracking control to strongly adaptive online learning with memory.
Any solution to the latter translates to a tracking controller that pursues the
best action on any time interval. Second, for the resulting online learning
problem we develop a novel approach that further adapts to the observed
gradients. Third, we propose a new algorithm for unconstrained online linear
optimization: for all (unknown) $T\in\mathbb{N}_+$, the cumulative loss and
movement on the time horizon $[1:T]$ is upper-bounded by a user-specified
constant. Combining these individual techniques, we propose a tracking
controller with a sensible performance guarantee even when the adversarial
target has a large range of movement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1"&gt;Ashok Cutkosky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paschalidis_I/0/1/0/all/0/1"&gt;Ioannis Ch. Paschalidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust normalizing flows using Bernstein-type polynomials. (arXiv:2102.03509v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03509</id>
        <link href="http://arxiv.org/abs/2102.03509"/>
        <updated>2021-06-10T01:56:47.896Z</updated>
        <summary type="html"><![CDATA[Modeling real-world distributions can often be challenging due to sample data
that are subjected to perturbations, e.g., instrumentation errors, or added
random noise. Since flow models are typically nonlinear algorithms, they
amplify these initial errors, leading to poor generalizations. This paper
proposes a framework to construct Normalizing Flows (NF), which demonstrates
higher robustness against such initial errors. To this end, we utilize
Bernstein-type polynomials inspired by the optimal stability of the Bernstein
basis. Further, compared to the existing NF frameworks, our method provides
compelling advantages like theoretical upper bounds for the approximation
error, higher interpretability, suitability for compactly supported densities,
and the ability to employ higher degree polynomials without training
instability. We conduct a thorough theoretical analysis and empirically
demonstrate the efficacy of the proposed technique using experiments on both
real-world and synthetic datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1"&gt;Sameera Ramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernando_K/0/1/0/all/0/1"&gt;Kasun Fernando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1"&gt;Nick Barnes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning. (arXiv:2106.04895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04895</id>
        <link href="http://arxiv.org/abs/2106.04895"/>
        <updated>2021-06-10T01:56:47.890Z</updated>
        <summary type="html"><![CDATA[Recent theoretical work studies sample-efficient reinforcement learning (RL)
extensively in two settings: learning interactively in the environment (online
RL), or learning from an offline dataset (offline RL). However, existing
algorithms and theories for learning near-optimal policies in these two
settings are rather different and disconnected. Towards bridging this gap, this
paper initiates the theoretical study of policy finetuning, that is, online RL
where the learner has additional access to a "reference policy" $\mu$ close to
the optimal policy $\pi_\star$ in a certain sense. We consider the policy
finetuning problem in episodic Markov Decision Processes (MDPs) with $S$
states, $A$ actions, and horizon length $H$. We first design a sharp offline
reduction algorithm -- which simply executes $\mu$ and runs offline policy
optimization on the collected dataset -- that finds an $\varepsilon$
near-optimal policy within $\widetilde{O}(H^3SC^\star/\varepsilon^2)$ episodes,
where $C^\star$ is the single-policy concentrability coefficient between $\mu$
and $\pi_\star$. This offline result is the first that matches the sample
complexity lower bound in this setting, and resolves a recent open question in
offline RL. We then establish an $\Omega(H^3S\min\{C^\star, A\}/\varepsilon^2)$
sample complexity lower bound for any policy finetuning algorithm, including
those that can adaptively explore the environment. This implies that -- perhaps
surprisingly -- the optimal policy finetuning algorithm is either offline
reduction or a purely online RL algorithm that does not use $\mu$. Finally, we
design a new hybrid offline/online algorithm for policy finetuning that
achieves better sample complexity than both vanilla offline reduction and
purely online RL algorithms, in a relaxed setting where $\mu$ only satisfies
concentrability partially up to a certain time step.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tengyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yu Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretraining Representations for Data-Efficient Reinforcement Learning. (arXiv:2106.04799v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04799</id>
        <link href="http://arxiv.org/abs/2106.04799"/>
        <updated>2021-06-10T01:56:47.884Z</updated>
        <summary type="html"><![CDATA[Data efficiency is a key challenge for deep reinforcement learning. We
address this problem by using unlabeled data to pretrain an encoder which is
then finetuned on a small amount of task-specific data. To encourage learning
representations which capture diverse aspects of the underlying MDP, we employ
a combination of latent dynamics modelling and unsupervised goal-conditioned
RL. When limited to 100k steps of interaction on Atari games (equivalent to two
hours of human experience), our approach significantly surpasses prior work
combining offline representation pretraining with task-specific finetuning, and
compares favourably with other pretraining methods that require orders of
magnitude more data. Our approach shows particular promise when combined with
larger models as well as more diverse, task-aligned observational data --
approaching human-level performance and data-efficiency on Atari in our best
setting. We provide code associated with this work at
https://github.com/mila-iqia/SGI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1"&gt;Max Schwarzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajkumar_N/0/1/0/all/0/1"&gt;Nitarshan Rajkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noukhovitch_M/0/1/0/all/0/1"&gt;Michael Noukhovitch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Ankesh Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charlin_L/0/1/0/all/0/1"&gt;Laurent Charlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjelm_D/0/1/0/all/0/1"&gt;Devon Hjelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1"&gt;Philip Bachman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Lack of Robust Interpretability of Neural Text Classifiers. (arXiv:2106.04631v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04631</id>
        <link href="http://arxiv.org/abs/2106.04631"/>
        <updated>2021-06-10T01:56:47.868Z</updated>
        <summary type="html"><![CDATA[With the ever-increasing complexity of neural language models, practitioners
have turned to methods for understanding the predictions of these models. One
of the most well-adopted approaches for model interpretability is feature-based
interpretability, i.e., ranking the features in terms of their impact on model
predictions. Several prior studies have focused on assessing the fidelity of
feature-based interpretability methods, i.e., measuring the impact of dropping
the top-ranked features on the model output. However, relatively little work
has been conducted on quantifying the robustness of interpretations. In this
work, we assess the robustness of interpretations of neural text classifiers,
specifically, those based on pretrained Transformer encoders, using two
randomization tests. The first compares the interpretations of two models that
are identical except for their initializations. The second measures whether the
interpretations differ between a model with trained parameters and a model with
random parameters. Both tests show surprising deviations from expected
behavior, raising questions about the extent of insights that practitioners may
draw from interpretations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1"&gt;Muhammad Bilal Zafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donini_M/0/1/0/all/0/1"&gt;Michele Donini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1"&gt;Dylan Slack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Archambeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sanjiv Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1"&gt;Krishnaram Kenthapadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliable Adversarial Distillation with Unreliable Teachers. (arXiv:2106.04928v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04928</id>
        <link href="http://arxiv.org/abs/2106.04928"/>
        <updated>2021-06-10T01:56:47.861Z</updated>
        <summary type="html"><![CDATA[In ordinary distillation, student networks are trained with soft labels (SLs)
given by pretrained teacher networks, and students are expected to improve upon
teachers since SLs are stronger supervision than the original hard labels.
However, when considering adversarial robustness, teachers may become
unreliable and adversarial distillation may not work: teachers are pretrained
on their own adversarial data, and it is too demanding to require that teachers
are also good at every adversarial data queried by students. Therefore, in this
paper, we propose reliable introspective adversarial distillation (IAD) where
students partially instead of fully trust their teachers. Specifically, IAD
distinguishes between three cases given a query of a natural data (ND) and the
corresponding adversarial data (AD): (a) if a teacher is good at AD, its SL is
fully trusted; (b) if a teacher is good at ND but not AD, its SL is partially
trusted and the student also takes its own SL into account; (c) otherwise, the
student only relies on its own SL. Experiments demonstrate the effectiveness of
IAD for improving upon teachers in terms of adversarial robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jianliang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Deep Learning with Probabilistic Neural Networks and Deep Probabilistic Models. (arXiv:2106.00120v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00120</id>
        <link href="http://arxiv.org/abs/2106.00120"/>
        <updated>2021-06-10T01:56:47.856Z</updated>
        <summary type="html"><![CDATA[Probabilistic deep learning is deep learning that accounts for uncertainty,
both model uncertainty and data uncertainty. It is based on the use of
probabilistic models and deep neural networks. We distinguish two approaches to
probabilistic deep learning: probabilistic neural networks and deep
probabilistic models. The former employs deep neural networks that utilize
probabilistic layers which can represent and process uncertainty; the latter
uses probabilistic models that incorporate deep neural network components which
capture complex non-linear stochastic relationships between the random
variables. We discuss some major examples of each approach including Bayesian
neural networks and mixture density networks (for probabilistic neural
networks), and variational autoencoders, deep Gaussian processes and deep mixed
effects models (for deep probabilistic models). TensorFlow Probability is a
library for probabilistic modeling and inference which can be used for both
approaches of probabilistic deep learning. We include its code examples for
illustration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Daniel T. Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Improved Retrosynthetic Planning. (arXiv:2106.04880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04880</id>
        <link href="http://arxiv.org/abs/2106.04880"/>
        <updated>2021-06-10T01:56:47.850Z</updated>
        <summary type="html"><![CDATA[Retrosynthetic planning is a fundamental problem in chemistry for finding a
pathway of reactions to synthesize a target molecule. Recently, search
algorithms have shown promising results for solving this problem by using deep
neural networks (DNNs) to expand their candidate solutions, i.e., adding new
reactions to reaction pathways. However, the existing works on this line are
suboptimal; the retrosynthetic planning problem requires the reaction pathways
to be (a) represented by real-world reactions and (b) executable using
"building block" molecules, yet the DNNs expand reaction pathways without fully
incorporating such requirements. Motivated by this, we propose an end-to-end
framework for directly training the DNNs towards generating reaction pathways
with the desirable properties. Our main idea is based on a self-improving
procedure that trains the model to imitate successful trajectories found by
itself. We also propose a novel reaction augmentation scheme based on a forward
reaction model. Our experiments demonstrate that our scheme significantly
improves the success rate of solving the retrosynthetic problem from 86.84% to
96.32% while maintaining the performance of DNN for predicting valid reactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junsu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungsoo Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hankook Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Deep Industrial Transfer Learning for Anomaly Detection on Time Series Data. (arXiv:2106.04920v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04920</id>
        <link href="http://arxiv.org/abs/2106.04920"/>
        <updated>2021-06-10T01:56:47.836Z</updated>
        <summary type="html"><![CDATA[Deep learning promises performant anomaly detection on time-variant datasets,
but greatly suffers from low availability of suitable training datasets and
frequently changing tasks. Deep transfer learning offers mitigation by letting
algorithms built upon previous knowledge from different tasks or locations. In
this article, a modular deep learning algorithm for anomaly detection on time
series datasets is presented that allows for an easy integration of such
transfer learning capabilities. It is thoroughly tested on a dataset from a
discrete manufacturing process in order to prove its fundamental adequacy
towards deep industrial transfer learning - the transfer of knowledge in
industrial applications' special environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maschler_B/0/1/0/all/0/1"&gt;Benjamin Maschler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knodel_T/0/1/0/all/0/1"&gt;Tim Knodel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weyrich_M/0/1/0/all/0/1"&gt;Michael Weyrich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulating Continuum Mechanics with Multi-Scale Graph Neural Networks. (arXiv:2106.04900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04900</id>
        <link href="http://arxiv.org/abs/2106.04900"/>
        <updated>2021-06-10T01:56:47.826Z</updated>
        <summary type="html"><![CDATA[Continuum mechanics simulators, numerically solving one or more partial
differential equations, are essential tools in many areas of science and
engineering, but their performance often limits application in practice. Recent
modern machine learning approaches have demonstrated their ability to
accelerate spatio-temporal predictions, although, with only moderate accuracy
in comparison. Here we introduce MultiScaleGNN, a novel multi-scale graph
neural network model for learning to infer unsteady continuum mechanics.
MultiScaleGNN represents the physical domain as an unstructured set of nodes,
and it constructs one or more graphs, each of them encoding different scales of
spatial resolution. Successive learnt message passing between these graphs
improves the ability of GNNs to capture and forecast the system state in
problems encompassing a range of length scales. Using graph representations,
MultiScaleGNN can impose periodic boundary conditions as an inductive bias on
the edges in the graphs, and achieve independence to the nodes' positions. We
demonstrate this method on advection problems and incompressible fluid
dynamics. Our results show that the proposed model can generalise from uniform
advection fields to high-gradient fields on complex domains at test time and
infer long-term Navier-Stokes solutions within a range of Reynolds numbers.
Simulations obtained with MultiScaleGNN are between two and four orders of
magnitude faster than the ones on which it was trained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lino_M/0/1/0/all/0/1"&gt;Mario Lino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cantwell_C/0/1/0/all/0/1"&gt;Chris Cantwell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharath_A/0/1/0/all/0/1"&gt;Anil A. Bharath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fotiadis_S/0/1/0/all/0/1"&gt;Stathi Fotiadis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expectation Programming. (arXiv:2106.04953v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04953</id>
        <link href="http://arxiv.org/abs/2106.04953"/>
        <updated>2021-06-10T01:56:47.795Z</updated>
        <summary type="html"><![CDATA[Building on ideas from probabilistic programming, we introduce the concept of
an expectation programming framework (EPF) that automates the calculation of
expectations. Analogous to a probabilistic program, an expectation program is
comprised of a mix of probabilistic constructs and deterministic calculations
that define a conditional distribution over its variables. However, the focus
of the inference engine in an EPF is to directly estimate the resulting
expectation of the program return values, rather than approximate the
conditional distribution itself. This distinction allows us to achieve
substantial performance improvements over the standard probabilistic
programming pipeline by tailoring the inference to the precise expectation we
care about. We realize a particular instantiation of our EPF concept by
extending the probabilistic programming language Turing to allow so-called
target-aware inference to be run automatically, and show that this leads to
significant empirical gains compared to conventional posterior-based inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reichelt_T/0/1/0/all/0/1"&gt;Tim Reichelt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golinski_A/0/1/0/all/0/1"&gt;Adam Goli&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_L/0/1/0/all/0/1"&gt;Luke Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation. (arXiv:2106.04732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04732</id>
        <link href="http://arxiv.org/abs/2106.04732"/>
        <updated>2021-06-10T01:56:47.788Z</updated>
        <summary type="html"><![CDATA[We extend semi-supervised learning to the problem of domain adaptation to
learn significantly higher-accuracy models that train on one data distribution
and test on a different one. With the goal of generality, we introduce
AdaMatch, a method that unifies the tasks of unsupervised domain adaptation
(UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation
(SSDA). In an extensive experimental study, we compare its behavior with
respective state-of-the-art techniques from SSL, SSDA, and UDA on vision
classification tasks. We find AdaMatch either matches or significantly exceeds
the state-of-the-art in each case using the same hyper-parameters regardless of
the dataset or task. For example, AdaMatch nearly doubles the accuracy compared
to that of the prior state-of-the-art on the UDA task for DomainNet and even
exceeds the accuracy of the prior state-of-the-art obtained with pre-training
by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by
providing AdaMatch with just one labeled example per class from the target
domain (i.e., the SSDA setting), we increase the target accuracy by an
additional 6.1%, and with 5 labeled examples, by 13.6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berthelot_D/0/1/0/all/0/1"&gt;David Berthelot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1"&gt;Rebecca Roelofs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1"&gt;Alex Kurakin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Margin-Based Cluster Recovery with Oracle Queries. (arXiv:2106.04913v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04913</id>
        <link href="http://arxiv.org/abs/2106.04913"/>
        <updated>2021-06-10T01:56:47.781Z</updated>
        <summary type="html"><![CDATA[We study an active cluster recovery problem where, given a set of $n$ points
and an oracle answering queries like "are these two points in the same
cluster?", the task is to recover exactly all clusters using as few queries as
possible. We begin by introducing a simple but general notion of margin between
clusters that captures, as special cases, the margins used in previous work,
the classic SVM margin, and standard notions of stability for center-based
clusterings. Then, under our margin assumptions we design algorithms that, in a
variety of settings, recover all clusters exactly using only $O(\log n)$
queries. For the Euclidean case, $\mathbb{R}^m$, we give an algorithm that
recovers arbitrary convex clusters, in polynomial time, and with a number of
queries that is lower than the best existing algorithm by $\Theta(m^m)$
factors. For general pseudometric spaces, where clusters might not be convex or
might not have any notion of shape, we give an algorithm that achieves the
$O(\log n)$ query bound, and is provably near-optimal as a function of the
packing number of the space. Finally, for clusterings realized by binary
concept classes, we give a combinatorial characterization of recoverability
with $O(\log n)$ queries, and we show that, for many concept classes in
Euclidean spaces, this characterization is equivalent to our margin condition.
Our results show a deep connection between cluster margins and active cluster
recoverability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bressan_M/0/1/0/all/0/1"&gt;Marco Bressan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Cesa-Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lattanzi_S/0/1/0/all/0/1"&gt;Silvio Lattanzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paudice_A/0/1/0/all/0/1"&gt;Andrea Paudice&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret and Cumulative Constraint Violation Analysis for Online Convex Optimization with Long Term Constraints. (arXiv:2106.05135v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05135</id>
        <link href="http://arxiv.org/abs/2106.05135"/>
        <updated>2021-06-10T01:56:47.764Z</updated>
        <summary type="html"><![CDATA[This paper considers online convex optimization with long term constraints,
where constraints can be violated in intermediate rounds, but need to be
satisfied in the long run. The cumulative constraint violation is used as the
metric to measure constraint violations, which excludes the situation that
strictly feasible constraints can compensate the effects of violated
constraints. A novel algorithm is first proposed and it achieves an
$\mathcal{O}(T^{\max\{c,1-c\}})$ bound for static regret and an
$\mathcal{O}(T^{(1-c)/2})$ bound for cumulative constraint violation, where
$c\in(0,1)$ is a user-defined trade-off parameter, and thus has improved
performance compared with existing results. Both static regret and cumulative
constraint violation bounds are reduced to $\mathcal{O}(\log(T))$ when the loss
functions are strongly convex, which also improves existing results. %In order
to bound the regret with respect to any comparator sequence, In order to
achieve the optimal regret with respect to any comparator sequence, another
algorithm is then proposed and it achieves the optimal
$\mathcal{O}(\sqrt{T(1+P_T)})$ regret and an $\mathcal{O}(\sqrt{T})$ cumulative
constraint violation, where $P_T$ is the path-length of the comparator
sequence. Finally, numerical simulations are provided to illustrate the
effectiveness of the theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinlei Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiuxian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lihua Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_T/0/1/0/all/0/1"&gt;Tianyou Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansson_K/0/1/0/all/0/1"&gt;Karl H. Johansson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04641</id>
        <link href="http://arxiv.org/abs/2106.04641"/>
        <updated>2021-06-10T01:56:47.731Z</updated>
        <summary type="html"><![CDATA[Transfer learning methods, and in particular domain adaptation, help exploit
labeled data in one domain to improve the performance of a certain task in
another domain. However, it is still not clear what factors affect the success
of domain adaptation. This paper models adaptation success and selection of the
most suitable source domains among several candidates in text similarity. We
use descriptive domain information and cross-domain similarity metrics as
predictive features. While mostly positive, the results also point to some
domains where adaptation success was difficult to predict.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1"&gt;Nicolai Pogrebnyakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1"&gt;Shohreh Shaghaghian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EMFlow: Data Imputation in Latent Space via EM and Deep Flow Models. (arXiv:2106.04804v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04804</id>
        <link href="http://arxiv.org/abs/2106.04804"/>
        <updated>2021-06-10T01:56:47.714Z</updated>
        <summary type="html"><![CDATA[High dimensional incomplete data can be found in a wide range of systems. Due
to the fact that most of the data mining techniques and machine learning
algorithms require complete observations, data imputation is vital for
down-stream analysis. In this work, we introduce an imputation approach, called
EMFlow, that performs imputation in an latent space via an online version of
Expectation-Maximization (EM) algorithm and connects the latent space and the
data space via the normalizing flow (NF). The inference of EMFlow is iterative,
involving updating the parameters of online EM and NF alternatively. Extensive
experimental results on multivariate and image datasets show that the proposed
EMFlow has superior performance to competing methods in terms of both
imputation quality and convergence speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1"&gt;Qi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Sujit K. Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach. (arXiv:2106.04941v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04941</id>
        <link href="http://arxiv.org/abs/2106.04941"/>
        <updated>2021-06-10T01:56:47.676Z</updated>
        <summary type="html"><![CDATA[Learning faithful graph representations as sets of vertex embeddings has
become a fundamental intermediary step in a wide range of machine learning
applications. We propose the systematic use of symmetric spaces in
representation learning, a class encompassing many of the previously used
embedding targets. This enables us to introduce a new method, the use of
Finsler metrics integrated in a Riemannian optimization scheme, that better
adapts to dissimilar structures in the graph. We develop a tool to analyze the
embeddings and infer structural properties of the data sets. For
implementation, we choose Siegel spaces, a versatile family of symmetric
spaces. Our approach outperforms competitive baselines for graph reconstruction
tasks on various synthetic and real-world datasets. We further demonstrate its
applicability on two downstream tasks, recommender systems and node
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_F/0/1/0/all/0/1"&gt;Federico L&amp;#xf3;pez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pozzetti_B/0/1/0/all/0/1"&gt;Beatrice Pozzetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trettel_S/0/1/0/all/0/1"&gt;Steve Trettel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strube_M/0/1/0/all/0/1"&gt;Michael Strube&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wienhard_A/0/1/0/all/0/1"&gt;Anna Wienhard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phase Retrieval using Single-Instance Deep Generative Prior. (arXiv:2106.04812v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04812</id>
        <link href="http://arxiv.org/abs/2106.04812"/>
        <updated>2021-06-10T01:56:47.670Z</updated>
        <summary type="html"><![CDATA[Several deep learning methods for phase retrieval exist, but most of them
fail on realistic data without precise support information. We propose a novel
method based on single-instance deep generative prior that works well on
complex-valued crystal data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1"&gt;Kshitij Tayal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manekar_R/0/1/0/all/0/1"&gt;Raunak Manekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1"&gt;Zhong Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;David Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vipin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_F/0/1/0/all/0/1"&gt;Felix Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Pseudo-Backdoors for Mixed Integer Programs. (arXiv:2106.05080v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05080</id>
        <link href="http://arxiv.org/abs/2106.05080"/>
        <updated>2021-06-10T01:56:47.665Z</updated>
        <summary type="html"><![CDATA[We propose a machine learning approach for quickly solving Mixed Integer
Programs (MIP) by learning to prioritize a set of decision variables, which we
call pseudo-backdoors, for branching that results in faster solution times.
Learning-based approaches have seen success in the area of solving
combinatorial optimization problems by being able to flexibly leverage common
structures in a given distribution of problems. Our approach takes inspiration
from the concept of strong backdoors, which corresponds to a small set of
variables such that only branching on these variables yields an optimal
integral solution and a proof of optimality. Our notion of pseudo-backdoors
corresponds to a small set of variables such that only branching on them leads
to faster solve time (which can be solver dependent). A key advantage of
pseudo-backdoors over strong backdoors is that they are much amenable to
data-driven identification or prediction. Our proposed method learns to
estimate the solver performance of a proposed pseudo-backdoor, using a labeled
dataset collected on a set of training MIP instances. This model can then be
used to identify high-quality pseudo-backdoors on new MIP instances from the
same distribution. We evaluate our method on the generalized independent set
problems and find that our approach can efficiently identify high-quality
pseudo-backdoors. In addition, we compare our learned approach against Gurobi,
a state-of-the-art MIP solver, demonstrating that our method can be used to
improve solver performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferber_A/0/1/0/all/0/1"&gt;Aaron Ferber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jialin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1"&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yisong Yue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Labeled Data Generation with Inexact Supervision. (arXiv:2106.04716v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04716</id>
        <link href="http://arxiv.org/abs/2106.04716"/>
        <updated>2021-06-10T01:56:47.628Z</updated>
        <summary type="html"><![CDATA[The recent advanced deep learning techniques have shown the promising results
in various domains such as computer vision and natural language processing. The
success of deep neural networks in supervised learning heavily relies on a
large amount of labeled data. However, obtaining labeled data with target
labels is often challenging due to various reasons such as cost of labeling and
privacy issues, which challenges existing deep models. In spite of that, it is
relatively easy to obtain data with \textit{inexact supervision}, i.e., having
labels/tags related to the target task. For example, social media platforms are
overwhelmed with billions of posts and images with self-customized tags, which
are not the exact labels for target classification tasks but are usually
related to the target labels. It is promising to leverage these tags (inexact
supervision) and their relations with target classes to generate labeled data
to facilitate the downstream classification tasks. However, the work on this is
rather limited. Therefore, we study a novel problem of labeled data generation
with inexact supervision. We propose a novel generative framework named as
ADDES which can synthesize high-quality labeled data for target classification
tasks by learning from data with inexact supervision and the relations between
inexact supervision and target classes. Experimental results on image and text
datasets demonstrate the effectiveness of the proposed ADDES for generating
realistic labeled data from inexact supervision to facilitate the target
classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_E/0/1/0/all/0/1"&gt;Enyan Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1"&gt;Kai Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yiwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suhang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness in Compressed Neural Networks for Object Detection. (arXiv:2102.05509v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05509</id>
        <link href="http://arxiv.org/abs/2102.05509"/>
        <updated>2021-06-10T01:56:47.537Z</updated>
        <summary type="html"><![CDATA[Model compression techniques allow to significantly reduce the computational
cost associated with data processing by deep neural networks with only a minor
decrease in average accuracy. Simultaneously, reducing the model size may have
a large effect on noisy cases or objects belonging to less frequent classes. It
is a crucial problem from the perspective of the models' safety, especially for
object detection in the autonomous driving setting, which is considered in this
work. It was shown in the paper that the sensitivity of compressed models to
different distortion types is nuanced, and some of the corruptions are heavily
impacted by the compression methods (i.e., additive noise), while others (blur
effect) are only slightly affected. A common way to improve the robustness of
models is to use data augmentation, which was confirmed to positively affect
models' robustness, also for highly compressed models. It was further shown
that while data imbalance methods brought only a slight increase in accuracy
for the baseline model (without compression), the impact was more striking at
higher compression rates for the structured pruning. Finally, methods for
handling data imbalance brought a significant improvement of the pruned models'
worst-detected class accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1"&gt;Sebastian Cygert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czyzewski_A/0/1/0/all/0/1"&gt;Andrzej Czy&amp;#x17c;ewski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracking by Joint Local and Global Search: A Target-aware Attention based Approach. (arXiv:2106.04840v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04840</id>
        <link href="http://arxiv.org/abs/2106.04840"/>
        <updated>2021-06-10T01:56:47.520Z</updated>
        <summary type="html"><![CDATA[Tracking-by-detection is a very popular framework for single object tracking
which attempts to search the target object within a local search window for
each frame. Although such local search mechanism works well on simple videos,
however, it makes the trackers sensitive to extremely challenging scenarios,
such as heavy occlusion and fast motion. In this paper, we propose a novel and
general target-aware attention mechanism (termed TANet) and integrate it with
tracking-by-detection framework to conduct joint local and global search for
robust tracking. Specifically, we extract the features of target object patch
and continuous video frames, then we concatenate and feed them into a decoder
network to generate target-aware global attention maps. More importantly, we
resort to adversarial training for better attention prediction. The appearance
and motion discriminator networks are designed to ensure its consistency in
spatial and temporal views. In the tracking procedure, we integrate the
target-aware attention with multiple trackers by exploring candidate search
regions for robust tracking. Extensive experiments on both short-term and
long-term tracking benchmark datasets all validated the effectiveness of our
algorithm. The project page of this paper can be found at
\url{https://sites.google.com/view/globalattentiontracking/home/extend}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1"&gt;Bin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaowei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intermittent Speech Recovery. (arXiv:2106.05229v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.05229</id>
        <link href="http://arxiv.org/abs/2106.05229"/>
        <updated>2021-06-10T01:56:47.506Z</updated>
        <summary type="html"><![CDATA[A large number of Internet of Things (IoT) devices today are powered by
batteries, which are often expensive to maintain and may cause serious
environmental pollution. To avoid these problems, researchers have begun to
consider the use of energy systems based on energy-harvesting units for such
devices. However, the power harvested from an ambient source is fundamentally
small and unstable, resulting in frequent power failures during the operation
of IoT applications involving, for example, intermittent speech signals and the
streaming of videos. This paper presents a deep-learning-based speech recovery
system that reconstructs intermittent speech signals from self-powered IoT
devices. Our intermittent speech recovery system (ISR) consists of three
stages: interpolation, recovery, and combination. The experimental results show
that our recovery system increases speech quality by up to 707.1%, while
increasing speech intelligibility by up to 92.1%. Most importantly, our ISR
system also enhances the WER scores by up to 65.6%. To the best of our
knowledge, this study is one of the first to reconstruct intermittent speech
signals from self-powered-sensing IoT devices. These promising results suggest
that even though self powered microphone devices function with weak energy
sources, our ISR system can still maintain the performance of most
speech-signal-based applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yu-Chen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1"&gt;Tsun-An Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_K/0/1/0/all/0/1"&gt;Kuo-Hsuan Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garudadri_H/0/1/0/all/0/1"&gt;Harinath Garudadri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_T/0/1/0/all/0/1"&gt;Tei-Wei Kuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIGRAC: Digraph Clustering with Flow Imbalance. (arXiv:2106.05194v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.05194</id>
        <link href="http://arxiv.org/abs/2106.05194"/>
        <updated>2021-06-10T01:56:47.489Z</updated>
        <summary type="html"><![CDATA[Node clustering is a powerful tool in the analysis of networks. Here, we
introduce a graph neural network framework with a novel scalable Directed Mixed
Path Aggregation(DIMPA) scheme to obtain node embeddings for directed networks
in a self-supervised manner, including a novel probabilistic imbalance loss.
The method is end-to-end in combining embedding generation and clustering
without an intermediate step. In contrast to standard approaches in the
literature, in this paper, directionality is not treated as a nuisance, but
rather contains the main signal. In particular, we leverage the recently
introduced cut flow imbalance measure, which is tightly related to
directionality; cut flow imbalance is optimized without resorting to spectral
methods or cluster labels. Experimental results on synthetic data, in the form
of directed stochastic block models and real-world data at different scales,
demonstrate that our method attains state-of-the-art results on directed
clustering, for a wide range of noise and sparsity levels, as well as graph
structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+He_Y/0/1/0/all/0/1"&gt;Yixuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Reinert_G/0/1/0/all/0/1"&gt;Gesine Reinert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cucuringu_M/0/1/0/all/0/1"&gt;Mihai Cucuringu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DPER: Efficient Parameter Estimation for Randomly Missing Data. (arXiv:2106.05190v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.05190</id>
        <link href="http://arxiv.org/abs/2106.05190"/>
        <updated>2021-06-10T01:56:47.470Z</updated>
        <summary type="html"><![CDATA[The missing data problem has been broadly studied in the last few decades and
has various applications in different areas such as statistics or
bioinformatics. Even though many methods have been developed to tackle this
challenge, most of those are imputation techniques that require multiple
iterations through the data before yielding convergence. In addition, such
approaches may introduce extra biases and noises to the estimated parameters.
In this work, we propose novel algorithms to find the maximum likelihood
estimates (MLEs) for a one-class/multiple-class randomly missing data set under
some mild assumptions. As the computation is direct without any imputation, our
algorithms do not require multiple iterations through the data, thus promising
to be less time-consuming than other methods while maintaining superior
estimation performance. We validate these claims by empirical results on
various data sets of different sizes and release all codes in a GitHub
repository to contribute to the research community related to this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thu Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_Duy_K/0/1/0/all/0/1"&gt;Khoi Minh Nguyen-Duy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Duy Ho Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wade_B/0/1/0/all/0/1"&gt;Bruce Alan Wade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I Don't Need $\mathbf{u}$: Identifiable Non-Linear ICA Without Side Information. (arXiv:2106.05238v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05238</id>
        <link href="http://arxiv.org/abs/2106.05238"/>
        <updated>2021-06-10T01:56:47.464Z</updated>
        <summary type="html"><![CDATA[In this work we introduce a new approach for identifiable non-linear ICA
models. Recently there has been a renaissance in identifiability results in
deep generative models, not least for non-linear ICA. These prior works,
however, have assumed access to a sufficiently-informative auxiliary set of
observations, denoted $\mathbf{u}$. We show here how identifiability can be
obtained in the absence of this side-information, rendering possible
fully-unsupervised identifiable non-linear ICA. While previous theoretical
results have established the impossibility of identifiable non-linear ICA in
the presence of infinitely-flexible universal function approximators, here we
rely on the intrinsically-finite modelling capacity of any particular chosen
parameterisation of a deep generative model. In particular, we focus on
generative models which perform clustering in their latent space -- a model
structure which matches previous identifiable models, but with the learnt
clustering providing a synthetic form of auxiliary information. We evaluate our
proposals using VAEs, on synthetic and image datasets, and find that the
learned clusterings function effectively: deep generative models with latent
clusterings are empirically identifiable, to the same degree as models which
rely on side information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Willetts_M/0/1/0/all/0/1"&gt;Matthew Willetts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paige_B/0/1/0/all/0/1"&gt;Brooks Paige&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Regularization in Tensor Factorization. (arXiv:2102.09972v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09972</id>
        <link href="http://arxiv.org/abs/2102.09972"/>
        <updated>2021-06-10T01:56:47.458Z</updated>
        <summary type="html"><![CDATA[Recent efforts to unravel the mystery of implicit regularization in deep
learning have led to a theoretical focus on matrix factorization -- matrix
completion via linear neural network. As a step further towards practical deep
learning, we provide the first theoretical analysis of implicit regularization
in tensor factorization -- tensor completion via certain type of non-linear
neural network. We circumvent the notorious difficulty of tensor problems by
adopting a dynamical systems perspective, and characterizing the evolution
induced by gradient descent. The characterization suggests a form of greedy low
tensor rank search, which we rigorously prove under certain conditions, and
empirically demonstrate under others. Motivated by tensor rank capturing the
implicit regularization of a non-linear neural network, we empirically explore
it as a measure of complexity, and find that it captures the essence of
datasets on which neural networks generalize. This leads us to believe that
tensor rank may pave way to explaining both implicit regularization in deep
learning, and the properties of real-world data translating this implicit
regularization to generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Razin_N/0/1/0/all/0/1"&gt;Noam Razin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maman_A/0/1/0/all/0/1"&gt;Asaf Maman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1"&gt;Nadav Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What causes the test error? Going beyond bias-variance via ANOVA. (arXiv:2010.05170v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05170</id>
        <link href="http://arxiv.org/abs/2010.05170"/>
        <updated>2021-06-10T01:56:47.452Z</updated>
        <summary type="html"><![CDATA[Modern machine learning methods are often overparametrized, allowing
adaptation to the data at a fine level. This can seem puzzling; in the worst
case, such models do not need to generalize. This puzzle inspired a great
amount of work, arguing when overparametrization reduces test error, in a
phenomenon called "double descent". Recent work aimed to understand in greater
depth why overparametrization is helpful for generalization. This leads to
discovering the unimodality of variance as a function of the level of
parametrization, and to decomposing the variance into that arising from label
noise, initialization, and randomness in the training data to understand the
sources of the error.

In this work we develop a deeper understanding of this area. Specifically, we
propose using the analysis of variance (ANOVA) to decompose the variance in the
test error in a symmetric way, for studying the generalization performance of
certain two-layer linear and non-linear networks. The advantage of the analysis
of variance is that it reveals the effects of initialization, label noise, and
training data more clearly than prior approaches. Moreover, we also study the
monotonicity and unimodality of the variance components. While prior work
studied the unimodality of the overall variance, we study the properties of
each term in variance decomposition.

One key insight is that in typical settings, the interaction between training
samples and initialization can dominate the variance; surprisingly being larger
than their marginal effect. Also, we characterize "phase transitions" where the
variance changes from unimodal to monotone. On a technical level, we leverage
advanced deterministic equivalent techniques for Haar random matrices, that --
to our knowledge -- have not yet been used in the area. We also verify our
results in numerical simulations and on empirical data examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_L/0/1/0/all/0/1"&gt;Licong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dobriban_E/0/1/0/all/0/1"&gt;Edgar Dobriban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-layered Network Exploration via Random Walks: From Offline Optimization to Online Learning. (arXiv:2106.05065v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05065</id>
        <link href="http://arxiv.org/abs/2106.05065"/>
        <updated>2021-06-10T01:56:47.446Z</updated>
        <summary type="html"><![CDATA[Multi-layered network exploration (MuLaNE) problem is an important problem
abstracted from many applications. In MuLaNE, there are multiple network layers
where each node has an importance weight and each layer is explored by a random
walk. The MuLaNE task is to allocate total random walk budget $B$ into each
network layer so that the total weights of the unique nodes visited by random
walks are maximized. We systematically study this problem from offline
optimization to online learning. For the offline optimization setting where the
network structure and node weights are known, we provide greedy based
constant-ratio approximation algorithms for overlapping networks, and greedy or
dynamic-programming based optimal solutions for non-overlapping networks. For
the online learning setting, neither the network structure nor the node weights
are known initially. We adapt the combinatorial multi-armed bandit framework
and design algorithms to learn random walk related parameters and node weights
while optimizing the budget allocation in multiple rounds, and prove that they
achieve logarithmic regret bounds. Finally, we conduct experiments on a
real-world social network dataset to validate our theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xutong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1"&gt;Jinhang Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaowei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lui_J/0/1/0/all/0/1"&gt;John C.S. Lui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Open-World Recommendation: An Inductive Model-based Collaborative Filtering Approach. (arXiv:2007.04833v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04833</id>
        <link href="http://arxiv.org/abs/2007.04833"/>
        <updated>2021-06-10T01:56:47.428Z</updated>
        <summary type="html"><![CDATA[Recommendation models can effectively estimate underlying user interests and
predict one's future behaviors by factorizing an observed user-item rating
matrix into products of two sets of latent factors. However, the user-specific
embedding factors can only be learned in a transductive way, making it
difficult to handle new users on-the-fly. In this paper, we propose an
inductive collaborative filtering framework that contains two representation
models. The first model follows conventional matrix factorization which
factorizes a group of key users' rating matrix to obtain meta latents. The
second model resorts to attention-based structure learning that estimates
hidden relations from query to key users and learns to leverage meta latents to
inductively compute embeddings for query users via neural message passing. Our
model enables inductive representation learning for users and meanwhile
guarantees equivalent representation capacity as matrix factorization.
Experiments demonstrate that our model achieves promising results for
recommendation on few-shot users with limited training ratings and new unseen
users which are commonly encountered in open-world recommender systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qitian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hengrui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiaofeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Probability Theorem: A Framework for Probabilistic Learning. (arXiv:1910.09417v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09417</id>
        <link href="http://arxiv.org/abs/1910.09417"/>
        <updated>2021-06-10T01:56:47.423Z</updated>
        <summary type="html"><![CDATA[We present a theoretical framework of probabilistic learning derived by
Maximum Probability (MP) Theorem shown in the current paper. In this
probabilistic framework, a model is defined as an event in the probability
space, and a model or the associated event - either the true underlying model
or the parameterized model - have a quantified probability measure. This
quantification of a model's probability measure is derived by the MP Theorem,
in which we have shown that an event's probability measure has an upper-bound
given its conditional distribution on an arbitrary random variable. Through
this alternative framework, the notion of model parameters is encompassed in
the definition of the model or the associated event. Therefore, this framework
deviates from the conventional approach of assuming a prior on the model
parameters. Instead, the regularizing effects of assuming prior over parameters
is seen through maximizing probabilities of models or according to information
theory, minimizing the information content of a model. The probability of a
model in our framework is invariant to reparameterization and is solely
dependent on the model's likelihood function. Also, rather than maximizing the
posterior in a conventional Bayesian setting, the objective function in our
alternative framework is defined as the probability of set operations (e.g.
intersection) on the event of the true underlying model and the event of the
model at hand. Our theoretical framework, as a derivation of MP theorem, adds
clarity to probabilistic learning through solidifying the definition of
probabilistic models, quantifying their probabilities, and providing a visual
understanding of objective functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marvasti_A/0/1/0/all/0/1"&gt;Amir Emad Marvasti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marvasti_E/0/1/0/all/0/1"&gt;Ehsan Emad Marvasti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1"&gt;Hassan Foroosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Adaptive Doubly Robust Estimator for Policy Evaluation in Adaptive Experiments and a Paradox Concerning Logging Policy. (arXiv:2010.03792v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03792</id>
        <link href="http://arxiv.org/abs/2010.03792"/>
        <updated>2021-06-10T01:56:47.417Z</updated>
        <summary type="html"><![CDATA[The doubly robust (DR) estimator, which consists of two nuisance parameters,
the conditional mean outcome and the logging policy (the probability of
choosing an action), is crucial in causal inference. This paper proposes a DR
estimator for dependent samples obtained from adaptive experiments. To obtain
an asymptotically normal semiparametric estimator from dependent samples with
non-Donsker nuisance estimators, we propose adaptive-fitting as a variant of
sample-splitting. We also report an empirical paradox that our proposed DR
estimator tends to show better performances compared to other estimators
utilizing the true logging policy. While a similar phenomenon is known for
estimators with i.i.d. samples, traditional explanations based on asymptotic
efficiency cannot elucidate our case with dependent samples. We confirm this
hypothesis through simulation studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1"&gt;Masahiro Kato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1"&gt;Shota Yasui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAlinn_K/0/1/0/all/0/1"&gt;Kenichiro McAlinn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quickest change detection with unknown parameters: Constant complexity and near optimality. (arXiv:2106.05061v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05061</id>
        <link href="http://arxiv.org/abs/2106.05061"/>
        <updated>2021-06-10T01:56:47.411Z</updated>
        <summary type="html"><![CDATA[We consider the quickest change detection problem where both the parameters
of pre- and post- change distributions are unknown, which prevents the use of
classical simple hypothesis testing. Without additional assumptions, optimal
solutions are not tractable as they rely on some minimax and robust variant of
the objective. As a consequence, change points might be detected too late for
practical applications (in economics, health care or maintenance for instance).
Available constant complexity techniques typically solve a relaxed version of
the problem, deeply relying on very specific probability distributions and/or
some very precise additional knowledge. We consider a totally different
approach that leverages the theoretical asymptotic properties of optimal
solutions to derive a new scalable approximate algorithm with near optimal
performance that runs~in~$\mathcal{O}(1)$, adapted to even more complex
Markovian settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jarboui_F/0/1/0/all/0/1"&gt;Firas Jarboui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1"&gt;Viannet Perchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dimensionwise Separable 2-D Graph Convolution for Unsupervised and Semi-Supervised Learning on Graphs. (arXiv:1909.12038v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.12038</id>
        <link href="http://arxiv.org/abs/1909.12038"/>
        <updated>2021-06-10T01:56:47.405Z</updated>
        <summary type="html"><![CDATA[Graph convolutional neural networks (GCN) have been the model of choice for
graph representation learning, which is mainly due to the effective design of
graph convolution that computes the representation of a node by aggregating
those of its neighbors. However, existing GCN variants commonly use 1-D graph
convolution that solely operates on the object link graph without exploring
informative relational information among object attributes. This significantly
limits their modeling capability and may lead to inferior performance on noisy
and sparse real-world networks. In this paper, we explore 2-D graph convolution
to jointly model object links and attribute relations for graph representation
learning. Specifically, we propose a computationally efficient dimensionwise
separable 2-D graph convolution (DSGC) for filtering node features.
Theoretically, we show that DSGC can reduce intra-class variance of node
features on both the object dimension and the attribute dimension to learn more
effective representations. Empirically, we demonstrate that by modeling
attribute relations, DSGC achieves significant performance gain over
state-of-the-art methods for node classification and clustering on a variety of
real-world networks. The source code for reproducing the experimental results
is available at https://github.com/liqimai/DSGC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qimai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaotong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Han Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Quanyu Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiao-Ming Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Source Identification on Networks with Statistical Confidence. (arXiv:2106.04800v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.04800</id>
        <link href="http://arxiv.org/abs/2106.04800"/>
        <updated>2021-06-10T01:56:47.388Z</updated>
        <summary type="html"><![CDATA[Diffusion source identification on networks is a problem of fundamental
importance in a broad class of applications, including rumor controlling and
virus identification. Though this problem has received significant recent
attention, most studies have focused only on very restrictive settings and lack
theoretical guarantees for more realistic networks. We introduce a statistical
framework for the study of diffusion source identification and develop a
confidence set inference approach inspired by hypothesis testing. Our method
efficiently produces a small subset of nodes, which provably covers the source
node with any pre-specified confidence level without restrictive assumptions on
network structures. Moreover, we propose multiple Monte Carlo strategies for
the inference procedure based on network topology and the probabilistic
properties that significantly improve the scalability. To our knowledge, this
is the first diffusion source identification method with a practically useful
theoretical guarantee on general networks. We demonstrate our approach via
extensive synthetic experiments on well-known random network models and a
mobility network between cities concerning the COVID-19 spreading.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dawkins_Q/0/1/0/all/0/1"&gt;Quinlan Dawkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haifeng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Space-Time Attention All You Need for Video Understanding?. (arXiv:2102.05095v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05095</id>
        <link href="http://arxiv.org/abs/2102.05095"/>
        <updated>2021-06-10T01:56:47.382Z</updated>
        <summary type="html"><![CDATA[We present a convolution-free approach to video classification built
exclusively on self-attention over space and time. Our method, named
"TimeSformer," adapts the standard Transformer architecture to video by
enabling spatiotemporal feature learning directly from a sequence of
frame-level patches. Our experimental study compares different self-attention
schemes and suggests that "divided attention," where temporal attention and
spatial attention are separately applied within each block, leads to the best
video classification accuracy among the design choices considered. Despite the
radically new design, TimeSformer achieves state-of-the-art results on several
action recognition benchmarks, including the best reported accuracy on
Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks,
our model is faster to train, it can achieve dramatically higher test
efficiency (at a small drop in accuracy), and it can also be applied to much
longer video clips (over one minute long). Code and models are available at:
https://github.com/facebookresearch/TimeSformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1"&gt;Gedas Bertasius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Heng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1"&gt;Lorenzo Torresani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Optimization over Hybrid Spaces. (arXiv:2106.04682v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04682</id>
        <link href="http://arxiv.org/abs/2106.04682"/>
        <updated>2021-06-10T01:56:47.371Z</updated>
        <summary type="html"><![CDATA[We consider the problem of optimizing hybrid structures (mixture of discrete
and continuous input variables) via expensive black-box function evaluations.
This problem arises in many real-world applications. For example, in materials
design optimization via lab experiments, discrete and continuous variables
correspond to the presence/absence of primitive elements and their relative
concentrations respectively. The key challenge is to accurately model the
complex interactions between discrete and continuous variables. In this paper,
we propose a novel approach referred as Hybrid Bayesian Optimization (HyBO) by
utilizing diffusion kernels, which are naturally defined over continuous and
discrete variables. We develop a principled approach for constructing diffusion
kernels over hybrid spaces by utilizing the additive kernel formulation, which
allows additive interactions of all orders in a tractable manner. We
theoretically analyze the modeling strength of additive hybrid kernels and
prove that it has the universal approximation property. Our experiments on
synthetic and six diverse real-world benchmarks show that HyBO significantly
outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deshwal_A/0/1/0/all/0/1"&gt;Aryan Deshwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belakaria_S/0/1/0/all/0/1"&gt;Syrine Belakaria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doppa_J/0/1/0/all/0/1"&gt;Janardhan Rao Doppa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastSeq: Make Sequence Generation Faster. (arXiv:2106.04718v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04718</id>
        <link href="http://arxiv.org/abs/2106.04718"/>
        <updated>2021-06-10T01:56:47.365Z</updated>
        <summary type="html"><![CDATA[Transformer-based models have made tremendous impacts in natural language
generation. However the inference speed is a bottleneck due to large model size
and intensive computing involved in auto-regressive decoding process. We
develop FastSeq framework to accelerate sequence generation without accuracy
loss. The proposed optimization techniques include an attention cache
optimization, an efficient algorithm for detecting repeated n-grams, and an
asynchronous generation pipeline with parallel I/O. These optimizations are
general enough to be applicable to Transformer-based models (e.g., T5, GPT2,
and UniLM). Our benchmark results on a set of widely used and diverse models
demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use
with a simple one-line code change. The source code is available at
https://github.com/microsoft/fastseq.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1"&gt;Fei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1"&gt;Nikhil Bhendawade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1"&gt;Ting Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_D/0/1/0/all/0/1"&gt;Desheng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1"&gt;Bingyu Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruifei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On a class of data-driven combinatorial optimization problems under uncertainty: a distributionally robust approach. (arXiv:2105.14139v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14139</id>
        <link href="http://arxiv.org/abs/2105.14139"/>
        <updated>2021-06-10T01:56:47.357Z</updated>
        <summary type="html"><![CDATA[In this study we analyze linear combinatorial optimization problems where the
cost vector is not known a priori, but is only observable through a finite data
set. In contrast to the related studies, we presume that the number of
observations with respect to particular components of the cost vector may vary.
The goal is to find a procedure that transforms the data set into an estimate
of the expected value of the objective function (which is referred to as a
prediction rule) and a procedure that retrieves a candidate decision (which is
referred to as a prescription rule). We aim at finding the least conservative
prediction and prescription rules, which satisfy some specified asymptotic
guarantees. We demonstrate that the resulting vector optimization problems
admit a weakly optimal solution, which can be obtained by solving a particular
distributionally robust optimization problem. Specifically, the decision-maker
may optimize the worst-case expected loss across all probability distributions
with given component-wise relative entropy distances from the empirical
marginal distributions. Finally, we perform numerical experiments to analyze
the out-of-sample performance of the proposed solution approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ketkov_S/0/1/0/all/0/1"&gt;Sergey S. Ketkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Shilov_A/0/1/0/all/0/1"&gt;Andrei S. Shilov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Prokopyev_O/0/1/0/all/0/1"&gt;Oleg A. Prokopyev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incentivizing Efficient Equilibria in Traffic Networks with Mixed Autonomy. (arXiv:2106.04678v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2106.04678</id>
        <link href="http://arxiv.org/abs/2106.04678"/>
        <updated>2021-06-10T01:56:47.351Z</updated>
        <summary type="html"><![CDATA[Traffic congestion has large economic and social costs. The introduction of
autonomous vehicles can potentially reduce this congestion by increasing road
capacity via vehicle platooning and by creating an avenue for influencing
people's choice of routes. We consider a network of parallel roads with two
modes of transportation: (i) human drivers, who will choose the quickest route
available to them, and (ii) a ride hailing service, which provides an array of
autonomous vehicle route options, each with different prices, to users. We
formalize a model of vehicle flow in mixed autonomy and a model of how
autonomous service users make choices between routes with different prices and
latencies. Developing an algorithm to learn the preferences of the users, we
formulate a planning optimization that chooses prices to maximize a social
objective. We demonstrate the benefit of the proposed scheme by comparing the
results to theoretical benchmarks which we show can be efficiently calculated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1"&gt;Erdem B&amp;#x131;y&amp;#x131;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazar_D/0/1/0/all/0/1"&gt;Daniel A. Lazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedarsani_R/0/1/0/all/0/1"&gt;Ramtin Pedarsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1"&gt;Dorsa Sadigh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ex uno plures: Splitting One Model into an Ensemble of Subnetworks. (arXiv:2106.04767v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04767</id>
        <link href="http://arxiv.org/abs/2106.04767"/>
        <updated>2021-06-10T01:56:47.331Z</updated>
        <summary type="html"><![CDATA[Monte Carlo (MC) dropout is a simple and efficient ensembling method that can
improve the accuracy and confidence calibration of high-capacity deep neural
network models. However, MC dropout is not as effective as more
compute-intensive methods such as deep ensembles. This performance gap can be
attributed to the relatively poor quality of individual models in the MC
dropout ensemble and their lack of diversity. These issues can in turn be
traced back to the coupled training and substantial parameter sharing of the
dropout models. Motivated by this perspective, we propose a strategy to compute
an ensemble of subnetworks, each corresponding to a non-overlapping dropout
mask computed via a pruning strategy and trained independently. We show that
the proposed subnetwork ensembling method can perform as well as standard deep
ensembles in both accuracy and uncertainty estimates, yet with a computational
efficiency similar to MC dropout. Lastly, using several computer vision
datasets like CIFAR10/100, CUB200, and Tiny-Imagenet, we experimentally
demonstrate that subnetwork ensembling also consistently outperforms recently
proposed approaches that efficiently ensemble neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhilu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1"&gt;Vianne R. Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabuncu_M/0/1/0/all/0/1"&gt;Mert R. Sabuncu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Neural Network Subspaces. (arXiv:2102.10472v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10472</id>
        <link href="http://arxiv.org/abs/2102.10472"/>
        <updated>2021-06-10T01:56:47.325Z</updated>
        <summary type="html"><![CDATA[Recent observations have advanced our understanding of the neural network
optimization landscape, revealing the existence of (1) paths of high accuracy
containing diverse solutions and (2) wider minima offering improved
performance. Previous methods observing diverse paths require multiple training
runs. In contrast we aim to leverage both property (1) and (2) with a single
method and in a single training run. With a similar computational cost as
training one model, we learn lines, curves, and simplexes of high-accuracy
neural networks. These neural network subspaces contain diverse solutions that
can be ensembled, approaching the ensemble performance of independently trained
networks without the training cost. Moreover, using the subspace midpoint
boosts accuracy, calibration, and robustness to label noise, outperforming
Stochastic Weight Averaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1"&gt;Mitchell Wortsman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1"&gt;Maxwell Horton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1"&gt;Carlos Guestrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1"&gt;Mohammad Rastegari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhance Convolutional Neural Networks with Noise Incentive Block. (arXiv:2012.12109v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12109</id>
        <link href="http://arxiv.org/abs/2012.12109"/>
        <updated>2021-06-10T01:56:47.319Z</updated>
        <summary type="html"><![CDATA[As a generic modeling tool, Convolutional Neural Networks (CNNs) have been
widely employed in image generation and translation tasks. However, when fed
with a flat input, current CNN models may fail to generate vivid results due to
the spatially shared convolution kernels. We call it the flatness degradation
of CNNs. Unfortunately, such degradation is the greatest obstacles to generate
a spatially-variant output from a flat input, which has been barely discussed
in the previous literature. To tackle this problem, we propose a model agnostic
solution, i.e. Noise Incentive Block (NIB), which serves as a generic plug-in
for any CNN generation model. The key idea is to break the flat input condition
while keeping the intactness of the original information. Specifically, the NIB
perturbs the input data symmetrically with a noise map and reassembles them in
the feature domain as driven by the objective function. Extensive experiments
show that existing CNN models equipped with NIB survive from the flatness
degradation and are able to generate visually better results with richer
details in some specific image generation tasks given flat inputs, e.g.
semantic image synthesis, data-hidden image generation, and deep neural
dithering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1"&gt;Menghan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1"&gt;Tien-Tsin Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervision of Feature Transformation for Further Improving Supervised Learning. (arXiv:2106.04922v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04922</id>
        <link href="http://arxiv.org/abs/2106.04922"/>
        <updated>2021-06-10T01:56:47.312Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning, which benefits from automatically constructing
labels through pre-designed pretext task, has recently been applied for
strengthen supervised learning. Since previous self-supervised pretext tasks
are based on input, they may incur huge additional training overhead. In this
paper we find that features in CNNs can be also used for self-supervision. Thus
we creatively design the \emph{feature-based pretext task} which requires only
a small amount of additional training overhead. In our task we discard
different particular regions of features, and then train the model to
distinguish these different features. In order to fully apply our feature-based
pretext task in supervised learning, we also propose a novel learning framework
containing multi-classifiers for further improvement. Original labels will be
expanded to joint labels via self-supervision of feature transformations. With
more semantic information provided by our self-supervised tasks, this approach
can train CNNs more effectively. Extensive experiments on various supervised
learning tasks demonstrate the accuracy improvement and wide applicability of
our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zilin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuhang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaomin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I Don't Need $\mathbf{u}$: Identifiable Non-Linear ICA Without Side Information. (arXiv:2106.05238v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05238</id>
        <link href="http://arxiv.org/abs/2106.05238"/>
        <updated>2021-06-10T01:56:47.306Z</updated>
        <summary type="html"><![CDATA[In this work we introduce a new approach for identifiable non-linear ICA
models. Recently there has been a renaissance in identifiability results in
deep generative models, not least for non-linear ICA. These prior works,
however, have assumed access to a sufficiently-informative auxiliary set of
observations, denoted $\mathbf{u}$. We show here how identifiability can be
obtained in the absence of this side-information, rendering possible
fully-unsupervised identifiable non-linear ICA. While previous theoretical
results have established the impossibility of identifiable non-linear ICA in
the presence of infinitely-flexible universal function approximators, here we
rely on the intrinsically-finite modelling capacity of any particular chosen
parameterisation of a deep generative model. In particular, we focus on
generative models which perform clustering in their latent space -- a model
structure which matches previous identifiable models, but with the learnt
clustering providing a synthetic form of auxiliary information. We evaluate our
proposals using VAEs, on synthetic and image datasets, and find that the
learned clusterings function effectively: deep generative models with latent
clusterings are empirically identifiable, to the same degree as models which
rely on side information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Willetts_M/0/1/0/all/0/1"&gt;Matthew Willetts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paige_B/0/1/0/all/0/1"&gt;Brooks Paige&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GaitGraph: Graph Convolutional Network for Skeleton-Based Gait Recognition. (arXiv:2101.11228v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11228</id>
        <link href="http://arxiv.org/abs/2101.11228"/>
        <updated>2021-06-10T01:56:47.287Z</updated>
        <summary type="html"><![CDATA[Gait recognition is a promising video-based biometric for identifying
individual walking patterns from a long distance. At present, most gait
recognition methods use silhouette images to represent a person in each frame.
However, silhouette images can lose fine-grained spatial information, and most
papers do not regard how to obtain these silhouettes in complex scenes.
Furthermore, silhouette images contain not only gait features but also other
visual clues that can be recognized. Hence these approaches can not be
considered as strict gait recognition.

We leverage recent advances in human pose estimation to estimate robust
skeleton poses directly from RGB images to bring back model-based gait
recognition with a cleaner representation of gait. Thus, we propose GaitGraph
that combines skeleton poses with Graph Convolutional Network (GCN) to obtain a
modern model-based approach for gait recognition. The main advantages are a
cleaner, more elegant extraction of the gait features and the ability to
incorporate powerful spatio-temporal modeling using GCN. Experiments on the
popular CASIA-B gait dataset show that our method archives state-of-the-art
performance in model-based gait recognition.

The code and models are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teepe_T/0/1/0/all/0/1"&gt;Torben Teepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Ali Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilg_J/0/1/0/all/0/1"&gt;Johannes Gilg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herzog_F/0/1/0/all/0/1"&gt;Fabian Herzog&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1"&gt;Stefan H&amp;#xf6;rmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1"&gt;Gerhard Rigoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit field learning for unsupervised anomaly detection in medical images. (arXiv:2106.05214v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.05214</id>
        <link href="http://arxiv.org/abs/2106.05214"/>
        <updated>2021-06-10T01:56:47.280Z</updated>
        <summary type="html"><![CDATA[We propose a novel unsupervised out-of-distribution detection method for
medical images based on implicit fields image representations. In our approach,
an auto-decoder feed-forward neural network learns the distribution of healthy
images in the form of a mapping between spatial coordinates and probabilities
over a proxy for tissue types. At inference time, the learnt distribution is
used to retrieve, from a given test image, a restoration, i.e. an image
maximally consistent with the input one but belonging to the healthy
distribution. Anomalies are localized using the voxel-wise probability
predicted by our model for the restored image. We tested our approach in the
task of unsupervised localization of gliomas on brain MR images and compared it
to several other VAE-based anomaly detection methods. Results show that the
proposed technique substantially outperforms them (average DICE 0.640 vs 0.518
for the best performing VAE-based alternative) while also requiring
considerably less computing time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Marimont_S/0/1/0/all/0/1"&gt;Sergio Naval Marimont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tarroni_G/0/1/0/all/0/1"&gt;Giacomo Tarroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling False Discovery Rates under Cross-Sectional Correlations. (arXiv:2102.07826v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07826</id>
        <link href="http://arxiv.org/abs/2102.07826"/>
        <updated>2021-06-10T01:56:47.271Z</updated>
        <summary type="html"><![CDATA[We consider controlling the false discovery rate for testing many time series
with an unknown cross-sectional correlation structure. Given a large number of
hypotheses, false and missing discoveries can plague an analysis. While many
procedures have been proposed to control false discovery, most of them either
assume independent hypotheses or lack statistical power. A problem of
particular interest is in financial asset pricing, where the goal is to
determine which ``factors" lead to excess returns out of a large number of
potential factors. Our contribution is two-fold. First, we show the consistency
of Fama and French's prominent method under multiple testing. Second, we
propose a novel method for false discovery control using double bootstrapping.
We achieve superior statistical power to existing methods and prove that the
false discovery rate is controlled. Simulations and a real data application
illustrate the efficacy of our method over existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Komiyama_J/0/1/0/all/0/1"&gt;Junpei Komiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Abe_M/0/1/0/all/0/1"&gt;Masaya Abe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nakagawa_K/0/1/0/all/0/1"&gt;Kei Nakagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+McAlinn_K/0/1/0/all/0/1"&gt;Kenichiro McAlinn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05187</id>
        <link href="http://arxiv.org/abs/2106.05187"/>
        <updated>2021-06-10T01:56:47.266Z</updated>
        <summary type="html"><![CDATA[We present implicit displacement fields, a novel representation for detailed
3D geometry. Inspired by a classic surface deformation technique, displacement
mapping, our method represents a complex surface as a smooth base surface plus
a displacement along the base's normal directions, resulting in a
frequency-based shape decomposition, where the high frequency signal is
constrained geometrically by the low frequency signal. Importantly, this
disentanglement is unsupervised thanks to a tailored architectural design that
has an innate frequency hierarchy by construction. We explore implicit
displacement field surface reconstruction and detail transfer and demonstrate
superior representational power, training stability and generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1"&gt;Wang Yifan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1"&gt;Lukas Rahmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1"&gt;Olga Sorkine-Hornung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PCNet: A Structure Similarity Enhancement Method for Multispectral and Multimodal Image Registration. (arXiv:2106.05124v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05124</id>
        <link href="http://arxiv.org/abs/2106.05124"/>
        <updated>2021-06-10T01:56:47.260Z</updated>
        <summary type="html"><![CDATA[Multispectral and multimodal image processing is important in the community
of computer vision and computational photography. As the acquired multispectral
and multimodal data are generally misaligned due to the alternation or movement
of the image device, the image registration procedure is necessary. The
registration of multispectral or multimodal image is challenging due to the
non-linear intensity and gradient variation. To cope with this challenge, we
propose the phase congruency network (PCNet), which is able to enhance the
structure similarity and alleviate the non-linear intensity and gradient
variation. The images can then be aligned using the similarity enhanced
features produced by the network. PCNet is constructed under the guidance of
the phase congruency prior. The network contains three trainable layers
accompany with the modified learnable Gabor kernels according to the phase
congruency theory. Thanks to the prior knowledge, PCNet is extremely
light-weight and can be trained on quite a small amount of multispectral data.
PCNet can be viewed to be fully convolutional and hence can take input of
arbitrary sizes. Once trained, PCNet is applicable on a variety of
multispectral and multimodal data such as RGB/NIR and flash/no-flash images
without additional further tuning. Experimental results validate that PCNet
outperforms current state-of-the-art registration algorithms, including the
deep-learning based ones that have the number of parameters hundreds times
compared to PCNet. Thanks to the similarity enhancement training, PCNet
outperforms the original phase congruency algorithm with two-thirds less
feature channels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Si-Yuan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Hui-Liang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1"&gt;Lun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shu-Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunguang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Feature Collapse and Deep Kernel Learning for Single Forward Pass Uncertainty. (arXiv:2102.11409v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11409</id>
        <link href="http://arxiv.org/abs/2102.11409"/>
        <updated>2021-06-10T01:56:47.242Z</updated>
        <summary type="html"><![CDATA[Gaussian processes are often considered a gold standard in uncertainty
estimation with low dimensional data, but they have difficulty scaling to high
dimensional inputs. Deep Kernel Learning (DKL) was introduced as a solution to
this problem: a deep feature extractor is used to transform the inputs over
which a Gaussian process' kernel is defined. However, DKL has been shown to
provide unreliable uncertainty estimates in practice. We study why, and show
that for certain feature extractors, "far-away" data points are mapped to the
same features as those of training-set points. With this insight we propose to
constrain DKL's feature extractor to approximately preserve distances through a
bi-Lipschitz constraint, resulting in a feature space favorable to DKL. We
obtain a model, DUE, which demonstrates uncertainty quality outperforming
previous DKL and single forward pass uncertainty methods, while maintaining the
speed and accuracy of softmax neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amersfoort_J/0/1/0/all/0/1"&gt;Joost van Amersfoort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1"&gt;Lewis Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jesson_A/0/1/0/all/0/1"&gt;Andrew Jesson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1"&gt;Oscar Key&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cervical Cytology Classification Using PCA & GWO Enhanced Deep Features Selection. (arXiv:2106.04919v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04919</id>
        <link href="http://arxiv.org/abs/2106.04919"/>
        <updated>2021-06-10T01:56:47.230Z</updated>
        <summary type="html"><![CDATA[Cervical cancer is one of the most deadly and common diseases among women
worldwide. It is completely curable if diagnosed in an early stage, but the
tedious and costly detection procedure makes it unviable to conduct
population-wise screening. Thus, to augment the effort of the clinicians, in
this paper, we propose a fully automated framework that utilizes Deep Learning
and feature selection using evolutionary optimization for cytology image
classification. The proposed framework extracts Deep feature from several
Convolution Neural Network models and uses a two-step feature reduction
approach to ensure reduction in computation cost and faster convergence. The
features extracted from the CNN models form a large feature space whose
dimensionality is reduced using Principal Component Analysis while preserving
99% of the variance. A non-redundant, optimal feature subset is selected from
this feature space using an evolutionary optimization algorithm, the Grey Wolf
Optimizer, thus improving the classification performance. Finally, the selected
feature subset is used to train an SVM classifier for generating the final
predictions. The proposed framework is evaluated on three publicly available
benchmark datasets: Mendeley Liquid Based Cytology (4-class) dataset, Herlev
Pap Smear (7-class) dataset, and the SIPaKMeD Pap Smear (5-class) dataset
achieving classification accuracies of 99.47%, 98.32% and 97.87% respectively,
thus justifying the reliability of the approach. The relevant codes for the
proposed approach can be found in:
https://github.com/DVLP-CMATERJU/Two-Step-Feature-Enhancement]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Basak_H/0/1/0/all/0/1"&gt;Hritam Basak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kundu_R/0/1/0/all/0/1"&gt;Rohit Kundu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Sukanta Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1"&gt;Nibaran Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Marginalizable Density Models. (arXiv:2106.04741v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04741</id>
        <link href="http://arxiv.org/abs/2106.04741"/>
        <updated>2021-06-10T01:56:47.225Z</updated>
        <summary type="html"><![CDATA[Probability density models based on deep networks have achieved remarkable
success in modeling complex high-dimensional datasets. However, unlike kernel
density estimators, modern neural models do not yield marginals or conditionals
in closed form, as these quantities require the evaluation of seldom tractable
integrals. In this work, we present the Marginalizable Density Model
Approximator (MDMA), a novel deep network architecture which provides closed
form expressions for the probabilities, marginals and conditionals of any
subset of the variables. The MDMA learns deep scalar representations for each
individual variable and combines them via learned hierarchical tensor
decompositions into a tractable yet expressive CDF, from which marginals and
conditional densities are easily obtained. We illustrate the advantage of exact
marginalizability in several tasks that are out of reach of previous deep
network-based density estimation models, such as estimating mutual information
between arbitrary subsets of variables, inferring causality by testing for
conditional independence, and inference with missing data without the need for
data imputation, outperforming state-of-the-art models on these tasks. The
model also allows for parallelized sampling with only a logarithmic dependence
of the time complexity on the number of variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gilboa_D/0/1/0/all/0/1"&gt;Dar Gilboa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pakman_A/0/1/0/all/0/1"&gt;Ari Pakman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vatter_T/0/1/0/all/0/1"&gt;Thibault Vatter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Programmable 3D snapshot microscopy with Fourier convolutional networks. (arXiv:2104.10611v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10611</id>
        <link href="http://arxiv.org/abs/2104.10611"/>
        <updated>2021-06-10T01:56:47.217Z</updated>
        <summary type="html"><![CDATA[3D snapshot microscopy enables fast volumetric imaging by capturing a 3D
volume in a single 2D camera image, and has found a variety of biological
applications such as whole brain imaging of fast neural activity in larval
zebrafish. The optimal microscope design for this optical 3D-to-2D encoding is
both sample- and task-dependent, with no general solution known. Highly
programmable optical elements create new possibilities for sample-specific
computational optimization of microscope parameters, e.g. tuning the collection
of light for a given sample structure. We perform such optimization with deep
learning, using a differentiable wave-optics simulation of light propagation
through a programmable microscope and a neural network to reconstruct volumes
from the microscope image. We introduce a class of global kernel Fourier
convolutional neural networks which can efficiently decode information from
multiple depths in the volume, globally encoded across a 3D snapshot image. We
show that our proposed networks succeed in large field of view volume
reconstruction and microscope parameter optimization where traditional networks
fail. We also show that our networks outperform the state-of-the-art learned
reconstruction algorithms for lensless computational photography.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Deb_D/0/1/0/all/0/1"&gt;Diptodip Deb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiao_Z/0/1/0/all/0/1"&gt;Zhenfei Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1"&gt;Alex B. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahrens_M/0/1/0/all/0/1"&gt;Misha B. Ahrens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Podgorski_K/0/1/0/all/0/1"&gt;Kaspar Podgorski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turaga_S/0/1/0/all/0/1"&gt;Srinivas C. Turaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning. (arXiv:2106.03760v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03760</id>
        <link href="http://arxiv.org/abs/2106.03760"/>
        <updated>2021-06-10T01:56:47.211Z</updated>
        <summary type="html"><![CDATA[The Mixture-of-experts (MoE) architecture is showing promising results in
multi-task learning (MTL) and in scaling high-capacity neural networks.
State-of-the-art MoE models use a trainable sparse gate to select a subset of
the experts for each input example. While conceptually appealing, existing
sparse gates, such as Top-k, are not smooth. The lack of smoothness can lead to
convergence and statistical performance issues when training with
gradient-based methods. In this paper, we develop DSelect-k: the first,
continuously differentiable and sparse gate for MoE, based on a novel binary
encoding formulation. Our gate can be trained using first-order methods, such
as stochastic gradient descent, and offers explicit control over the number of
experts to select. We demonstrate the effectiveness of DSelect-k in the context
of MTL, on both synthetic and real datasets with up to 128 tasks. Our
experiments indicate that MoE models based on DSelect-k can achieve
statistically significant improvements in predictive and expert selection
performance. Notably, on a real-world large-scale recommender system, DSelect-k
achieves over 22% average improvement in predictive performance compared to the
Top-k gate. We provide an open-source TensorFlow implementation of our gate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hazimeh_H/0/1/0/all/0/1"&gt;Hussein Hazimeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1"&gt;Aakanksha Chowdhery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sathiamoorthy_M/0/1/0/all/0/1"&gt;Maheswaran Sathiamoorthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yihua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumder_R/0/1/0/all/0/1"&gt;Rahul Mazumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lichan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Text-Only Domain Adaptation of RNN-Transducer Prediction Network. (arXiv:2104.11127v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11127</id>
        <link href="http://arxiv.org/abs/2104.11127"/>
        <updated>2021-06-10T01:56:47.204Z</updated>
        <summary type="html"><![CDATA[Adaption of end-to-end speech recognition systems to new tasks is known to be
challenging. A number of solutions have been proposed which apply external
language models with various fusion methods, possibly with a combination of
two-pass decoding. Also TTS systems have been used to generate adaptation data
for the end-to-end models. In this paper we show that RNN-transducer models can
be effectively adapted to new domains using only small amounts of textual data.
By taking advantage of model's inherent structure, where the prediction network
is interpreted as a language model, we can apply fast adaptation to the model.
Adapting the model avoids the need for complicated decoding time fusions and
external language models. Using appropriate regularization, the prediction
network can be adapted to new domains while still retaining good generalization
capabilities. We show with multiple ASR evaluation tasks how this method can
provide relative gains of 10-45% in target task WER. We also share insights how
RNN-transducer prediction network performs as a language model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pylkkonen_J/0/1/0/all/0/1"&gt;Janne Pylkk&amp;#xf6;nen&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ukkonen_A/0/1/0/all/0/1"&gt;Antti Ukkonen&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Kilpikoski_J/0/1/0/all/0/1"&gt;Juho Kilpikoski&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Tamminen_S/0/1/0/all/0/1"&gt;Samu Tamminen&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Heikinheimo_H/0/1/0/all/0/1"&gt;Hannes Heikinheimo&lt;/a&gt; (1) ((1) Speechly, (2) Department of Computer Science, University of Helsinki, Finland)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic task modelling for meta-learning. (arXiv:2106.04802v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04802</id>
        <link href="http://arxiv.org/abs/2106.04802"/>
        <updated>2021-06-10T01:56:47.187Z</updated>
        <summary type="html"><![CDATA[We propose probabilistic task modelling -- a generative probabilistic model
for collections of tasks used in meta-learning. The proposed model combines
variational auto-encoding and latent Dirichlet allocation to model each task as
a mixture of Gaussian distribution in an embedding space. Such modelling
provides an explicit representation of a task through its task-theme mixture.
We present an efficient approximation inference technique based on variational
inference method for empirical Bayes parameter estimation. We perform empirical
evaluations to validate the task uncertainty and task distance produced by the
proposed method through correlation diagrams of the prediction accuracy on
testing tasks. We also carry out experiments of task selection in meta-learning
to demonstrate how the task relatedness inferred from the proposed model help
to facilitate meta-learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Cuong C. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1"&gt;Thanh-Toan Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1"&gt;Gustavo Carneiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ghosts in Neural Networks: Existence, Structure and Role of Infinite-Dimensional Null Space. (arXiv:2106.04770v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04770</id>
        <link href="http://arxiv.org/abs/2106.04770"/>
        <updated>2021-06-10T01:56:47.181Z</updated>
        <summary type="html"><![CDATA[Overparametrization has been remarkably successful for deep learning studies.
This study investigates an overlooked but important aspect of overparametrized
neural networks, that is, the null components in the parameters of neural
networks, or the ghosts. Since deep learning is not explicitly regularized,
typical deep learning solutions contain null components. In this paper, we
present a structure theorem of the null space for a general class of neural
networks. Specifically, we show that any null element can be uniquely written
by the linear combination of ridgelet transforms. In general, it is quite
difficult to fully characterize the null space of an arbitrarily given
operator. Therefore, the structure theorem is a great advantage for
understanding a complicated landscape of neural network parameters. As
applications, we discuss the roles of ghosts on the generalization performance
of deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sonoda_S/0/1/0/all/0/1"&gt;Sho Sonoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishikawa_I/0/1/0/all/0/1"&gt;Isao Ishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikeda_M/0/1/0/all/0/1"&gt;Masahiro Ikeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Operationalizing Complex Causes:A Pragmatic View of Mediation. (arXiv:2106.05074v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05074</id>
        <link href="http://arxiv.org/abs/2106.05074"/>
        <updated>2021-06-10T01:56:47.176Z</updated>
        <summary type="html"><![CDATA[We examine the problem of causal response estimation for complex objects
(e.g., text, images, genomics). In this setting, classical \emph{atomic}
interventions are often not available (e.g., changes to characters, pixels, DNA
base-pairs). Instead, we only have access to indirect or \emph{crude}
interventions (e.g., enrolling in a writing program, modifying a scene,
applying a gene therapy). In this work, we formalize this problem and provide
an initial solution. Given a collection of candidate mediators, we propose (a)
a two-step method for predicting the causal responses of crude interventions;
and (b) a testing procedure to identify mediators of crude interventions. We
demonstrate, on a range of simulated and real-world-inspired examples, that our
approach allows us to efficiently estimate the effect of crude interventions
with limited data from new treatment regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gultchin_L/0/1/0/all/0/1"&gt;Limor Gultchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watson_D/0/1/0/all/0/1"&gt;David S. Watson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1"&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1"&gt;Ricardo Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Optimization in Games via Control Theory: Connecting Regret, Passivity and Poincar\'e Recurrence. (arXiv:2106.04748v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04748</id>
        <link href="http://arxiv.org/abs/2106.04748"/>
        <updated>2021-06-10T01:56:47.170Z</updated>
        <summary type="html"><![CDATA[We present a novel control-theoretic understanding of online optimization and
learning in games, via the notion of passivity. Passivity is a fundamental
concept in control theory, which abstracts energy conservation and dissipation
in physical systems. It has become a standard tool in analysis of general
feedback systems, to which game dynamics belong. Our starting point is to show
that all continuous-time Follow-the-Regularized-Leader (FTRL) dynamics, which
includes the well-known Replicator Dynamic, are lossless, i.e. it is passive
with no energy dissipation. Interestingly, we prove that passivity implies
bounded regret, connecting two fundamental primitives of control theory and
online optimization.

The observation of energy conservation in FTRL inspires us to present a
family of lossless learning dynamics, each of which has an underlying energy
function with a simple gradient structure. This family is closed under convex
combination; as an immediate corollary, any convex combination of FTRL dynamics
is lossless and thus has bounded regret. This allows us to extend the framework
of Fox and Shamma (Games, 2013) to prove not just global asymptotic stability
results for game dynamics, but Poincar\'e recurrence results as well.
Intuitively, when a lossless game (e.g. graphical constant-sum game) is coupled
with lossless learning dynamic, their interconnection is also lossless, which
results in a pendulum-like energy-preserving recurrent behavior, generalizing
the results of Piliouras and Shamma (SODA, 2014) and Mertikopoulos,
Papadimitriou and Piliouras (SODA, 2018).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_Y/0/1/0/all/0/1"&gt;Yun Kuen Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1"&gt;Georgios Piliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Explainable Abnormal Infant Movements Identification: A Body-part Based Prediction and Visualisation Framework. (arXiv:2106.04966v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04966</id>
        <link href="http://arxiv.org/abs/2106.04966"/>
        <updated>2021-06-10T01:56:47.164Z</updated>
        <summary type="html"><![CDATA[Providing early diagnosis of cerebral palsy (CP) is key to enhancing the
developmental outcomes for those affected. Diagnostic tools such as the General
Movements Assessment (GMA), have produced promising results in early diagnosis,
however these manual methods can be laborious.

In this paper, we propose a new framework for the automated classification of
infant body movements, based upon the GMA, which unlike previous methods, also
incorporates a visualization framework to aid with interpretability. Our
proposed framework segments extracted features to detect the presence of
Fidgety Movements (FMs) associated with the GMA spatiotemporally. These
features are then used to identify the body-parts with the greatest
contribution towards a classification decision and highlight the related
body-part segment providing visual feedback to the user.

We quantitatively compare the proposed framework's classification performance
with several other methods from the literature and qualitatively evaluate the
visualization's veracity. Our experimental results show that the proposed
method performs more robustly than comparable techniques in this setting whilst
simultaneously providing relevant visual interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McCay_K/0/1/0/all/0/1"&gt;Kevin D. McCay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1"&gt;Edmond S. L. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakkos_D/0/1/0/all/0/1"&gt;Dimitrios Sakkos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1"&gt;Wai Lok Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcroft_C/0/1/0/all/0/1"&gt;Claire Marcroft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dulson_P/0/1/0/all/0/1"&gt;Patricia Dulson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Embleton_N/0/1/0/all/0/1"&gt;Nicholas D. Embleton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentence Embeddings using Supervised Contrastive Learning. (arXiv:2106.04791v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04791</id>
        <link href="http://arxiv.org/abs/2106.04791"/>
        <updated>2021-06-10T01:56:47.147Z</updated>
        <summary type="html"><![CDATA[Sentence embeddings encode sentences in fixed dense vectors and have played
an important role in various NLP tasks and systems. Methods for building
sentence embeddings include unsupervised learning such as Quick-Thoughts and
supervised learning such as InferSent. With the success of pretrained NLP
models, recent research shows that fine-tuning pretrained BERT on SNLI and
Multi-NLI data creates state-of-the-art sentence embeddings, outperforming
previous sentence embeddings methods on various evaluation benchmarks. In this
paper, we propose a new method to build sentence embeddings by doing supervised
contrastive learning. Specifically our method fine-tunes pretrained BERT on
SNLI data, incorporating both supervised crossentropy loss and supervised
contrastive loss. Compared with baseline where fine-tuning is only done with
supervised cross-entropy loss similar to current state-of-the-art method SBERT,
our supervised contrastive method improves 2.8% in average on Semantic Textual
Similarity (STS) benchmarks and 1.05% in average on various sentence transfer
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1"&gt;Danqi Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL. (arXiv:2103.09815v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09815</id>
        <link href="http://arxiv.org/abs/2103.09815"/>
        <updated>2021-06-10T01:56:47.141Z</updated>
        <summary type="html"><![CDATA[Training autonomous agents able to generalize to multiple tasks is a key
target of Deep Reinforcement Learning (DRL) research. In parallel to improving
DRL algorithms themselves, Automatic Curriculum Learning (ACL) study how
teacher algorithms can train DRL agents more efficiently by adapting task
selection to their evolving abilities. While multiple standard benchmarks exist
to compare DRL agents, there is currently no such thing for ACL algorithms.
Thus, comparing existing approaches is difficult, as too many experimental
parameters differ from paper to paper. In this work, we identify several key
challenges faced by ACL algorithms. Based on these, we present TeachMyAgent
(TA), a benchmark of current ACL algorithms leveraging procedural task
generation. It includes 1) challenge-specific unit-tests using variants of a
procedural Box2D bipedal walker environment, and 2) a new procedural Parkour
environment combining most ACL challenges, making it ideal for global
performance assessment. We then use TeachMyAgent to conduct a comparative study
of representative existing approaches, showcasing the competitiveness of some
ACL algorithms that do not use expert knowledge. We also show that the Parkour
environment remains an open problem. We open-source our environments, all
studied ACL algorithms (collected from open-source code or re-implemented), and
DRL students in a Python package available at
https://github.com/flowersteam/TeachMyAgent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Romac_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Romac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Portelas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1"&gt;Katja Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1"&gt;Pierre-Yves Oudeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Recommendations and Low-Regret Cutting-Plane Algorithms. (arXiv:2106.04819v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04819</id>
        <link href="http://arxiv.org/abs/2106.04819"/>
        <updated>2021-06-10T01:56:47.135Z</updated>
        <summary type="html"><![CDATA[We consider the following variant of contextual linear bandits motivated by
routing applications in navigational engines and recommendation systems. We
wish to learn a hidden $d$-dimensional value $w^*$. Every round, we are
presented with a subset $\mathcal{X}_t \subseteq \mathbb{R}^d$ of possible
actions. If we choose (i.e. recommend to the user) action $x_t$, we obtain
utility $\langle x_t, w^* \rangle$ but only learn the identity of the best
action $\arg\max_{x \in \mathcal{X}_t} \langle x, w^* \rangle$. We design
algorithms for this problem which achieve regret $O(d\log T)$ and $\exp(O(d
\log d))$. To accomplish this, we design novel cutting-plane algorithms with
low "regret" -- the total distance between the true point $w^*$ and the
hyperplanes the separation oracle returns. We also consider the variant where
we are allowed to provide a list of several recommendations. In this variant,
we give an algorithm with $O(d^2 \log d)$ regret and list size
$\mathrm{poly}(d)$. Finally, we construct nearly tight algorithms for a weaker
variant of this problem where the learner only learns the identity of an action
that is better than the recommendation. Our results rely on new algorithmic
techniques in convex geometry (including a variant of Steiner's formula for the
centroid of a convex set) which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gollapudi_S/0/1/0/all/0/1"&gt;Sreenivas Gollapudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guruganesh_G/0/1/0/all/0/1"&gt;Guru Guruganesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kollias_K/0/1/0/all/0/1"&gt;Kostas Kollias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1"&gt;Pasin Manurangsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leme_R/0/1/0/all/0/1"&gt;Renato Paes Leme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1"&gt;Jon Schneider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vector Quantized Models for Planning. (arXiv:2106.04615v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04615</id>
        <link href="http://arxiv.org/abs/2106.04615"/>
        <updated>2021-06-10T01:56:47.120Z</updated>
        <summary type="html"><![CDATA[Recent developments in the field of model-based RL have proven successful in
a range of environments, especially ones where planning is essential. However,
such successes have been limited to deterministic fully-observed environments.
We present a new approach that handles stochastic and partially-observable
environments. Our key insight is to use discrete autoencoders to capture the
multiple possible effects of an action in a stochastic environment. We use a
stochastic variant of \emph{Monte Carlo tree search} to plan over both the
agent's actions and the discrete latent variables representing the
environment's response. Our approach significantly outperforms an offline
version of MuZero on a stochastic interpretation of chess where the opponent is
considered part of the environment. We also show that our approach scales to
\emph{DeepMind Lab}, a first-person 3D environment with large visual
observations and partial observability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozair_S/0/1/0/all/0/1"&gt;Sherjil Ozair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yazhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razavi_A/0/1/0/all/0/1"&gt;Ali Razavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antonoglou_I/0/1/0/all/0/1"&gt;Ioannis Antonoglou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1"&gt;A&amp;#xe4;ron van den Oord&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Boosting for Linear Mixed Models. (arXiv:2106.04862v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.04862</id>
        <link href="http://arxiv.org/abs/2106.04862"/>
        <updated>2021-06-10T01:56:47.114Z</updated>
        <summary type="html"><![CDATA[Boosting methods are widely used in statistical learning to deal with
high-dimensional data due to their variable selection feature. However, those
methods lack straightforward ways to construct estimators for the precision of
the parameters such as variance or confidence interval, which can be achieved
by conventional statistical methods like Bayesian inference. In this paper, we
propose a new inference method "BayesBoost" that combines boosting and Bayesian
for linear mixed models to make the uncertainty estimation for the random
effects possible on the one hand. On the other hand, the new method overcomes
the shortcomings of Bayesian inference in giving precise and unambiguous
guidelines for the selection of covariates by benefiting from boosting
techniques. The implementation of Bayesian inference leads to the randomness of
model selection criteria like the conditional AIC (cAIC), so we also propose a
cAIC-based model selection criteria that focus on the stabilized regions
instead of the global minimum. The effectiveness of the new approach can be
observed via simulation and in a data example from the field of neurophysiology
focussing on the mechanisms in the brain while listening to unpleasant sounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Boyao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Griesbach_C/0/1/0/all/0/1"&gt;Colin Griesbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kim_C/0/1/0/all/0/1"&gt;Cora Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Muller_Voggel_N/0/1/0/all/0/1"&gt;Nadia M&amp;#xfc;ller-Voggel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bergherr_E/0/1/0/all/0/1"&gt;Elisabeth Bergherr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rate-Distortion Theoretic Model Compression: Successive Refinement for Pruning. (arXiv:2102.08329v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08329</id>
        <link href="http://arxiv.org/abs/2102.08329"/>
        <updated>2021-06-10T01:56:47.106Z</updated>
        <summary type="html"><![CDATA[We study the neural network (NN) compression problem, viewing the tension
between the compression ratio and NN performance through the lens of
rate-distortion theory. We choose a distortion metric that reflects the effect
of NN compression on the model output and then derive the tradeoff between rate
(compression ratio) and distortion. In addition to characterizing theoretical
limits of NN compression, this formulation shows that \emph{pruning},
implicitly or explicitly, must be a part of a good compression algorithm. This
observation bridges a gap between parts of the literature pertaining to NN and
data compression, respectively, providing insight into the empirical success of
pruning for NN compression. Finally, we propose a novel pruning strategy
derived from our information-theoretic formulation and show that it outperforms
the relevant baselines on CIFAR-10 and ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isik_B/0/1/0/all/0/1"&gt;Berivan Isik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+No_A/0/1/0/all/0/1"&gt;Albert No&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weissman_T/0/1/0/all/0/1"&gt;Tsachy Weissman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards the Memorization Effect of Neural Networks in Adversarial Training. (arXiv:2106.04794v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04794</id>
        <link href="http://arxiv.org/abs/2106.04794"/>
        <updated>2021-06-10T01:56:47.081Z</updated>
        <summary type="html"><![CDATA[Recent studies suggest that ``memorization'' is one important factor for
overparameterized deep neural networks (DNNs) to achieve optimal performance.
Specifically, the perfectly fitted DNNs can memorize the labels of many
atypical samples, generalize their memorization to correctly classify test
atypical samples and enjoy better test performance. While, DNNs which are
optimized via adversarial training algorithms can also achieve perfect training
performance by memorizing the labels of atypical samples, as well as the
adversarially perturbed atypical samples. However, adversarially trained models
always suffer from poor generalization, with both relatively low clean accuracy
and robustness on the test set. In this work, we study the effect of
memorization in adversarial trained DNNs and disclose two important findings:
(a) Memorizing atypical samples is only effective to improve DNN's accuracy on
clean atypical samples, but hardly improve their adversarial robustness and (b)
Memorizing certain atypical samples will even hurt the DNN's performance on
typical samples. Based on these two findings, we propose Benign Adversarial
Training (BAT) which can facilitate adversarial training to avoid fitting
``harmful'' atypical samples and fit as more ``benign'' atypical samples as
possible. In our experiments, we validate the effectiveness of BAT, and show it
can achieve better clean accuracy vs. robustness trade-off than baseline
methods, in benchmark datasets such as CIFAR100 and Tiny~ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Han Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaorui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wentao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenbiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Anil Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Evolution of Neuron Communities in a Deep Learning Architecture. (arXiv:2106.04693v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04693</id>
        <link href="http://arxiv.org/abs/2106.04693"/>
        <updated>2021-06-10T01:56:47.075Z</updated>
        <summary type="html"><![CDATA[Deep learning techniques are increasingly being adopted for classification
tasks over the past decade, yet explaining how deep learning architectures can
achieve state-of-the-art performance is still an elusive goal. While all the
training information is embedded deeply in a trained model, we still do not
understand much about its performance by only analyzing the model. This paper
examines the neuron activation patterns of deep learning-based classification
models and explores whether the models' performances can be explained through
neurons' activation behavior. We propose two approaches: one that models
neurons' activation behavior as a graph and examines whether the neurons form
meaningful communities, and the other examines the predictability of neurons'
behavior using entropy. Our comprehensive experimental study reveals that both
the community quality (modularity) and entropy are closely related to the deep
learning models' performances, thus paves a novel way of explaining deep
learning models directly from the neurons' activation pattern.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mostafa_S/0/1/0/all/0/1"&gt;Sakib Mostafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1"&gt;Debajyoti Mondal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embedding Physics to Learn Spatiotemporal Dynamics from Sparse Data. (arXiv:2106.04781v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04781</id>
        <link href="http://arxiv.org/abs/2106.04781"/>
        <updated>2021-06-10T01:56:47.069Z</updated>
        <summary type="html"><![CDATA[Modeling nonlinear spatiotemporal dynamical systems has primarily relied on
partial differential equations (PDEs) that are typically derived from first
principles. However, the explicit formulation of PDEs for many underexplored
processes, such as climate systems, biochemical reaction and epidemiology,
remains uncertain or partially unknown, where very sparse measurement data is
yet available. To tackle this challenge, we propose a novel deep learning
architecture that forcibly embedded known physics knowledge in a
residual-recurrent $\Pi$-block network, to facilitate the learning of the
spatiotemporal dynamics in a data-driven manner. The coercive embedding
mechanism of physics, fundamentally different from physics-informed neural
networks based on loss penalty, ensures the network to rigorously obey given
physics. Numerical experiments demonstrate that the resulting learning paradigm
that embeds physics possesses remarkable accuracy, robustness, interpretability
and generalizability for learning spatiotemporal dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_C/0/1/0/all/0/1"&gt;Chengping Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Machine Learning Safety: A Survey and Primer. (arXiv:2106.04823v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04823</id>
        <link href="http://arxiv.org/abs/2106.04823"/>
        <updated>2021-06-10T01:56:47.064Z</updated>
        <summary type="html"><![CDATA[The open-world deployment of Machine Learning (ML) algorithms in
safety-critical applications such as autonomous vehicles needs to address a
variety of ML vulnerabilities such as interpretability, verifiability, and
performance limitations. Research explores different approaches to improve ML
dependability by proposing new models and training techniques to reduce
generalization error, achieve domain adaptation, and detect outlier examples
and adversarial attacks. In this paper, we review and organize practical ML
techniques that can improve the safety and dependability of ML algorithms and
therefore ML-based software. Our organization maps state-of-the-art ML
techniques to safety strategies in order to enhance the dependability of the ML
algorithm from different aspects, and discuss research gaps as well as
promising solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohseni_S/0/1/0/all/0/1"&gt;Sina Mohseni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadawa_J/0/1/0/all/0/1"&gt;Jay Yadawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Job Dispatching Policies for Queueing Systems with Unknown Service Rates. (arXiv:2106.04707v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.04707</id>
        <link href="http://arxiv.org/abs/2106.04707"/>
        <updated>2021-06-10T01:56:47.058Z</updated>
        <summary type="html"><![CDATA[In multi-server queueing systems where there is no central queue holding all
incoming jobs, job dispatching policies are used to assign incoming jobs to the
queue at one of the servers. Classic job dispatching policies such as
join-the-shortest-queue and shortest expected delay assume that the service
rates and queue lengths of the servers are known to the dispatcher. In this
work, we tackle the problem of job dispatching without the knowledge of service
rates and queue lengths, where the dispatcher can only obtain noisy estimates
of the service rates by observing job departures. This problem presents a novel
exploration-exploitation trade-off between sending jobs to all the servers to
estimate their service rates, and exploiting the currently known fastest
servers to minimize the expected queueing delay. We propose a bandit-based
exploration policy that learns the service rates from observed job departures.
Unlike the standard multi-armed bandit problem where only one out of a finite
set of actions is optimal, here the optimal policy requires identifying the
optimal fraction of incoming jobs to be sent to each server. We present a
regret analysis and simulations to demonstrate the effectiveness of the
proposed bandit-based exploration policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Choudhury_T/0/1/0/all/0/1"&gt;Tuhinangshu Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Joshi_G/0/1/0/all/0/1"&gt;Gauri Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weina Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Submodular + Concave. (arXiv:2106.04769v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.04769</id>
        <link href="http://arxiv.org/abs/2106.04769"/>
        <updated>2021-06-10T01:56:47.043Z</updated>
        <summary type="html"><![CDATA[It has been well established that first order optimization methods can
converge to the maximal objective value of concave functions and provide
constant factor approximation guarantees for (non-convex/non-concave)
continuous submodular functions. In this work, we initiate the study of the
maximization of functions of the form $F(x) = G(x) +C(x)$ over a solvable
convex body $P$, where $G$ is a smooth DR-submodular function and $C$ is a
smooth concave function. This class of functions is a strict extension of both
concave and continuous DR-submodular functions for which no theoretical
guarantee is known. We provide a suite of Frank-Wolfe style algorithms, which,
depending on the nature of the objective function (i.e., if $G$ and $C$ are
monotone or not, and non-negative or not) and on the nature of the set $P$
(i.e., whether it is downward closed or not), provide $1-1/e$, $1/e$, or $1/2$
approximation guarantees. We then use our algorithms to get a framework to
smoothly interpolate between choosing a diverse set of elements from a given
ground set (corresponding to the mode of a determinantal point process) and
choosing a clustered set of elements (corresponding to the maxima of a suitable
concave function). Additionally, we apply our algorithms to various functions
in the above class (DR-submodular + concave) in both constrained and
unconstrained settings, and show that our algorithms consistently outperform
natural baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Mitra_S/0/1/0/all/0/1"&gt;Siddharth Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Feldman_M/0/1/0/all/0/1"&gt;Moran Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Karbasi_A/0/1/0/all/0/1"&gt;Amin Karbasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drones for Medical Delivery Considering Different Demands Classes: A Markov Decision Process Approach for Managing Health Centers Dispatching Medical Products. (arXiv:2106.04729v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.04729</id>
        <link href="http://arxiv.org/abs/2106.04729"/>
        <updated>2021-06-10T01:56:47.037Z</updated>
        <summary type="html"><![CDATA[We consider the problem of optimizing the distribution operations of a hub
using drones to deliver medical supplies to different geographic regions.
Drones are an innovative method with many benefits including low-contact
delivery thereby reducing the spread of pandemic and vaccine-preventable
diseases. While we focus on medical supply delivery for this work, it is
applicable to drone delivery for many other applications, including food,
postal items, and e-commerce delivery. In this paper, our goal is to address
drone delivery challenges by optimizing the distribution operations at a drone
hub that dispatch drones to different geographic locations generating
stochastic demands for medical supplies. By considering different geographic
locations, we consider different classes of demand that require different
flight ranges, which is directly related to the amount of charge held in a
drone battery. We classify the stochastic demands based on their distance from
the drone hub, use a Markov decision process to model the problem, and perform
computational tests using realistic data representing a prominent drone
delivery company. We solve the problem using a reinforcement learning method
and show its high performance compared with the exact solution found using
dynamic programming. Finally, we analyze the results and provide insights for
managing the drone hub operations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Asadi_A/0/1/0/all/0/1"&gt;Amin Asadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pinkley_S/0/1/0/all/0/1"&gt;Sarah Nurre Pinkley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLCC: Contrastive Learning for Color Constancy. (arXiv:2106.04989v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04989</id>
        <link href="http://arxiv.org/abs/2106.04989"/>
        <updated>2021-06-10T01:56:47.032Z</updated>
        <summary type="html"><![CDATA[In this paper, we present CLCC, a novel contrastive learning framework for
color constancy. Contrastive learning has been applied for learning
high-quality visual representations for image classification. One key aspect to
yield useful representations for image classification is to design illuminant
invariant augmentations. However, the illuminant invariant assumption conflicts
with the nature of the color constancy task, which aims to estimate the
illuminant given a raw image. Therefore, we construct effective contrastive
pairs for learning better illuminant-dependent features via a novel raw-domain
color augmentation. On the NUS-8 dataset, our method provides $17.5\%$ relative
improvements over a strong baseline, reaching state-of-the-art performance
without increasing model complexity. Furthermore, our method achieves
competitive performance on the Gehler dataset with $3\times$ fewer parameters
compared to top-ranking deep learning methods. More importantly, we show that
our model is more robust to different scenes under close proximity of
illuminants, significantly reducing $28.7\%$ worst-case error in data-sparse
regions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_Y/0/1/0/all/0/1"&gt;Yi-Chen Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chia-Che Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1"&gt;Hsuan-Chao Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Hao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chia-Ping Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yu-Lin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1"&gt;Kevin Jou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Instance-Wise Classification in Correlated Feature Spaces. (arXiv:2106.04668v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04668</id>
        <link href="http://arxiv.org/abs/2106.04668"/>
        <updated>2021-06-10T01:56:47.025Z</updated>
        <summary type="html"><![CDATA[In a typical supervised machine learning setting, the predictions on all test
instances are based on a common subset of features discovered during model
training. However, using a different subset of features that is most
informative for each test instance individually may not only improve prediction
accuracy, but also the overall interpretability of the model. At the same time,
feature selection methods for classification have been known to be the most
effective when many features are irrelevant and/or uncorrelated. In fact,
feature selection ignoring correlations between features can lead to poor
classification performance. In this work, a Bayesian network is utilized to
model feature dependencies. Using the dependency network, a new method is
proposed that sequentially selects the best feature to evaluate for each test
instance individually, and stops the selection process to make a prediction
once it determines that no further improvement can be achieved with respect to
classification accuracy. The optimum number of features to acquire and the
optimum classification strategy are derived for each test instance. The
theoretical properties of the optimum solution are analyzed, and a new
algorithm is proposed that takes advantage of these properties to implement a
robust and scalable solution for high dimensional settings. The effectiveness,
generalizability, and scalability of the proposed method is illustrated on a
variety of real-world datasets from diverse application domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liyanage_Y/0/1/0/all/0/1"&gt;Yasitha Warahena Liyanage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zois_D/0/1/0/all/0/1"&gt;Daphney-Stavroula Zois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chelmis_C/0/1/0/all/0/1"&gt;Charalampos Chelmis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Neural Network to Quantify Uncertainty of Wind Power Estimation. (arXiv:2106.04656v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.04656</id>
        <link href="http://arxiv.org/abs/2106.04656"/>
        <updated>2021-06-10T01:56:47.019Z</updated>
        <summary type="html"><![CDATA[Each year a growing number of wind farms are being added to power grids to
generate electricity. The power curve of a wind turbine, which exhibits the
relationship between generated power and wind speed, plays a major role in
assessing the performance of a wind farm. Neural networks have been used for
power curve estimation. However, they do not produce a confidence measure for
their output, unless computationally prohibitive Bayesian methods are used. In
this paper, a probabilistic neural network with Monte Carlo dropout is
considered to quantify the model (epistemic) uncertainty of the power curve
estimation. This approach offers a minimal increase in computational complexity
over deterministic approaches. Furthermore, by incorporating a probabilistic
loss function, the noise or aleatoric uncertainty in the data is estimated. The
developed network captures both model and noise uncertainty which is found to
be useful tools in assessing performance. Also, the developed network is
compared with existing ones across a public domain dataset showing superior
performance in terms of prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karami_F/0/1/0/all/0/1"&gt;Farzad Karami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kehtarnavaz_N/0/1/0/all/0/1"&gt;Nasser Kehtarnavaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rotea_M/0/1/0/all/0/1"&gt;Mario Rotea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning subtree pattern importance for Weisfeiler-Lehmanbased graph kernels. (arXiv:2106.04739v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04739</id>
        <link href="http://arxiv.org/abs/2106.04739"/>
        <updated>2021-06-10T01:56:47.004Z</updated>
        <summary type="html"><![CDATA[Graph is an usual representation of relational data, which are ubiquitous in
manydomains such as molecules, biological and social networks. A popular
approach to learningwith graph structured data is to make use of graph kernels,
which measure the similaritybetween graphs and are plugged into a kernel
machine such as a support vector machine.Weisfeiler-Lehman (WL) based graph
kernels, which employ WL labeling scheme to extract subtree patterns and
perform node embedding, are demonstrated to achieve great performance while
being efficiently computable. However, one of the main drawbacks of ageneral
kernel is the decoupling of kernel construction and learning process. For
moleculargraphs, usual kernels such as WL subtree, based on substructures of
the molecules, consider all available substructures having the same importance,
which might not be suitable inpractice. In this paper, we propose a method to
learn the weights of subtree patterns in the framework of WWL kernels, the
state of the art method for graph classification task [14]. To overcome the
computational issue on large scale data sets, we present an efficient learning
algorithm and also derive a generalization gap bound to show its convergence.
Finally, through experiments on synthetic and real-world data sets, we
demonstrate the effectiveness of our proposed method for learning the weights
of subtree patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dai Hai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Canh Hao Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamitsuka_H/0/1/0/all/0/1"&gt;Hiroshi Mamitsuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Streaming Belief Propagation for Community Detection. (arXiv:2106.04805v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04805</id>
        <link href="http://arxiv.org/abs/2106.04805"/>
        <updated>2021-06-10T01:56:46.998Z</updated>
        <summary type="html"><![CDATA[The community detection problem requires to cluster the nodes of a network
into a small number of well-connected "communities". There has been substantial
recent progress in characterizing the fundamental statistical limits of
community detection under simple stochastic block models. However, in
real-world applications, the network structure is typically dynamic, with nodes
that join over time. In this setting, we would like a detection algorithm to
perform only a limited number of updates at each node arrival. While standard
voting approaches satisfy this constraint, it is unclear whether they exploit
the network information optimally. We introduce a simple model for networks
growing over time which we refer to as streaming stochastic block model
(StSBM). Within this model, we prove that voting algorithms have fundamental
limitations. We also develop a streaming belief-propagation (StreamBP)
approach, for which we prove optimality in certain regimes. We validate our
theoretical findings on synthetic and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuchen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bateni_M/0/1/0/all/0/1"&gt;MohammadHossein Bateni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Linhares_A/0/1/0/all/0/1"&gt;Andre Linhares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Almeida_F/0/1/0/all/0/1"&gt;Filipe Miguel Goncalves de Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Montanari_A/0/1/0/all/0/1"&gt;Andrea Montanari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Norouzi_Fard_A/0/1/0/all/0/1"&gt;Ashkan Norouzi-Fard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tardos_J/0/1/0/all/0/1"&gt;Jakab Tardos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[General Rough Modeling of Cluster Analysis. (arXiv:2106.04683v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.04683</id>
        <link href="http://arxiv.org/abs/2106.04683"/>
        <updated>2021-06-10T01:56:46.992Z</updated>
        <summary type="html"><![CDATA[In this research, a general theoretical framework for clustering is proposed
over specific partial algebraic systems by the present author. Her theory helps
in isolating minimal assumptions necessary for different concepts of clustering
information in any form to be realized in a situation (and therefore in a
semantics). \emph{It is well-known that of the limited number of proofs in the
theory of hard and soft clustering that are known to exist, most involve
statistical assumptions}. Many methods seem to work because they seem to work
in specific empirical practice. A new general rough method of analyzing
clusterings is invented, and this opens the subject to clearer conceptions and
contamination-free theoretical proofs. Numeric ideas of validation are also
proposed to be replaced by those based on general rough approximation. The
essence of the approach is explained in brief and supported by an example.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mani_A/0/1/0/all/0/1"&gt;A. Mani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Deep Neural Network Generalization with Perturbation Response Curves. (arXiv:2106.04765v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04765</id>
        <link href="http://arxiv.org/abs/2106.04765"/>
        <updated>2021-06-10T01:56:46.986Z</updated>
        <summary type="html"><![CDATA[The field of Deep Learning is rich with empirical evidence of human-like
performance on a variety of prediction tasks. However, despite these successes,
the recent Predicting Generalization in Deep Learning (PGDL) NeurIPS 2020
competition suggests that there is a need for more robust and efficient
measures of network generalization. In this work, we propose a new framework
for evaluating the generalization capabilities of trained networks. We use
perturbation response (PR) curves that capture the accuracy change of a given
network as a function of varying levels of training sample perturbation. From
these PR curves, we derive novel statistics that capture generalization
capability. Specifically, we introduce two new measures for accurately
predicting generalization gaps: the Gi-score and Pal-score, that are inspired
by the Gini coefficient and Palma ratio (measures of income inequality), that
accurately predict generalization gaps. Using our framework applied to intra
and inter class sample mixup, we attain better predictive scores than the
current state-of-the-art measures on a majority of tasks in the PGDL
competition. In addition, we show that our framework and the proposed
statistics can be used to capture to what extent a trained network is invariant
to a given parametric input transformation, such as rotation or translation.
Therefore, these generalization gap prediction statistics also provide a useful
means for selecting the optimal network architectures and hyperparameters that
are invariant to a certain perturbation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quanz_B/0/1/0/all/0/1"&gt;Brian Quanz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParChain: A Framework for Parallel Hierarchical Agglomerative Clustering using Nearest-Neighbor Chain. (arXiv:2106.04727v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.04727</id>
        <link href="http://arxiv.org/abs/2106.04727"/>
        <updated>2021-06-10T01:56:46.979Z</updated>
        <summary type="html"><![CDATA[This paper studies the hierarchical clustering problem, where the goal is to
produce a dendrogram that represents clusters at varying scales of a data set.
We propose the ParChain framework for designing parallel hierarchical
agglomerative clustering (HAC) algorithms, and using the framework we obtain
novel parallel algorithms for the complete linkage, average linkage, and Ward's
linkage criteria. Compared to most previous parallel HAC algorithms, which
require quadratic memory, our new algorithms require only linear memory, and
are scalable to large data sets. ParChain is based on our parallelization of
the nearest-neighbor chain algorithm, and enables multiple clusters to be
merged on every round. We introduce two key optimizations that are critical for
efficiency: a range query optimization that reduces the number of distance
computations required when finding nearest neighbors of clusters, and a caching
optimization that stores a subset of previously computed distances, which are
likely to be reused.

Experimentally, we show that our highly-optimized implementations using 48
cores with two-way hyper-threading achieve 5.8--110.1x speedup over
state-of-the-art parallel HAC algorithms and achieve 13.75--54.23x
self-relative speedup. Compared to state-of-the-art algorithms, our algorithms
require up to 237.3x less space. Our algorithms are able to scale to data set
sizes with tens of millions of points, which existing algorithms are not able
to handle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shangdi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiqiu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1"&gt;Laxman Dhulipala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shun_J/0/1/0/all/0/1"&gt;Julian Shun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoAtNet: Marrying Convolution and Attention for All Data Sizes. (arXiv:2106.04803v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04803</id>
        <link href="http://arxiv.org/abs/2106.04803"/>
        <updated>2021-06-10T01:56:46.958Z</updated>
        <summary type="html"><![CDATA[Transformers have attracted increasing interests in computer vision, but they
still fall behind state-of-the-art convolutional networks. In this work, we
show that while Transformers tend to have larger model capacity, their
generalization can be worse than convolutional networks due to the lack of the
right inductive bias. To effectively combine the strengths from both
architectures, we present CoAtNets(pronounced "coat" nets), a family of hybrid
models built from two key insights:(1) depthwise Convolution and self-Attention
can be naturally unified via simple relative attention; (2) vertically stacking
convolution layers and attention layers in a principled way is surprisingly
effective in improving generalization, capacity and efficiency. Experiments
show that our CoAtNets achieve state-of-the-art performance under different
resource constraints across various datasets. For example, CoAtNet achieves
86.0% ImageNet top-1 accuracy without extra data, and 89.77% with extra JFT
data, outperforming prior arts of both convolutional networks and Transformers.
Notably, when pre-trained with 13M images fromImageNet-21K, our CoAtNet
achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images
from JFT while using 23x less data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zihang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hanxiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingxing Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChaCha for Online AutoML. (arXiv:2106.04815v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04815</id>
        <link href="http://arxiv.org/abs/2106.04815"/>
        <updated>2021-06-10T01:56:46.951Z</updated>
        <summary type="html"><![CDATA[We propose the ChaCha (Champion-Challengers) algorithm for making an online
choice of hyperparameters in online learning settings. ChaCha handles the
process of determining a champion and scheduling a set of `live' challengers
over time based on sample complexity bounds. It is guaranteed to have sublinear
regret after the optimal configuration is added into consideration by an
application-dependent oracle based on the champions. Empirically, we show that
ChaCha provides good performance across a wide array of datasets when
optimizing over featurization and hyperparameter decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingyun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1"&gt;John Langford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1"&gt;Paul Mineiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_M/0/1/0/all/0/1"&gt;Marco Rossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAM: Understanding Product Images in Cross Product Category Attribute Extraction. (arXiv:2106.04630v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04630</id>
        <link href="http://arxiv.org/abs/2106.04630"/>
        <updated>2021-06-10T01:56:46.943Z</updated>
        <summary type="html"><![CDATA[Understanding product attributes plays an important role in improving online
shopping experience for customers and serves as an integral part for
constructing a product knowledge graph. Most existing methods focus on
attribute extraction from text description or utilize visual information from
product images such as shape and color. Compared to the inputs considered in
prior works, a product image in fact contains more information, represented by
a rich mixture of words and visual clues with a layout carefully designed to
impress customers. This work proposes a more inclusive framework that fully
utilizes these different modalities for attribute extraction. Inspired by
recent works in visual question answering, we use a transformer based sequence
to sequence model to fuse representations of product text, Optical Character
Recognition (OCR) tokens and visual objects detected in the product image. The
framework is further extended with the capability to extract attribute value
across multiple product categories with a single model, by training the decoder
to predict both product category and attribute value and conditioning its
output on product category. The model provides a unified attribute extraction
solution desirable at an e-commerce platform that offers numerous product
categories with a diverse body of product attributes. We evaluated the model on
two product attributes, one with many possible values and one with a small set
of possible values, over 14 product categories and found the model could
achieve 15% gain on the Recall and 10% gain on the F1 score compared to
existing methods using text-only features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jie Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1"&gt;Nasser Zalmout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Luna Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Neural Architecture Search via Proxy Data. (arXiv:2106.04784v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04784</id>
        <link href="http://arxiv.org/abs/2106.04784"/>
        <updated>2021-06-10T01:56:46.937Z</updated>
        <summary type="html"><![CDATA[Despite the increasing interest in neural architecture search (NAS), the
significant computational cost of NAS is a hindrance to researchers. Hence, we
propose to reduce the cost of NAS using proxy data, i.e., a representative
subset of the target data, without sacrificing search performance. Even though
data selection has been used across various fields, our evaluation of existing
selection methods for NAS algorithms offered by NAS-Bench-1shot1 reveals that
they are not always appropriate for NAS and a new selection method is
necessary. By analyzing proxy data constructed using various selection methods
through data entropy, we propose a novel proxy data selection method tailored
for NAS. To empirically demonstrate the effectiveness, we conduct thorough
experiments across diverse datasets, search spaces, and NAS algorithms.
Consequently, NAS algorithms with the proposed selection discover architectures
that are competitive with those obtained using the entire dataset. It
significantly reduces the search cost: executing DARTS with the proposed
selection requires only 40 minutes on CIFAR-10 and 7.5 hours on ImageNet with a
single GPU. Additionally, when the architecture searched on ImageNet using the
proposed selection is inversely transferred to CIFAR-10, a state-of-the-art
test error of 2.4\% is yielded. Our code is available at
https://github.com/nabk89/NAS-with-Proxy-data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1"&gt;Byunggook Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1"&gt;Jisoo Mok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1"&gt;Hyeokjun Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boolean Matrix Factorization via Nonnegative Auxiliary Optimization. (arXiv:2106.04708v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.04708</id>
        <link href="http://arxiv.org/abs/2106.04708"/>
        <updated>2021-06-10T01:56:46.931Z</updated>
        <summary type="html"><![CDATA[A novel approach to Boolean matrix factorization (BMF) is presented. Instead
of solving the BMF problem directly, this approach solves a nonnegative
optimization problem with the constraint over an auxiliary matrix whose Boolean
structure is identical to the initial Boolean data. Then the solution of the
nonnegative auxiliary optimization problem is thresholded to provide a solution
for the BMF problem. We provide the proofs for the equivalencies of the two
solution spaces under the existence of an exact solution. Moreover, the
nonincreasing property of the algorithm is also proven. Experiments on
synthetic and real datasets are conducted to show the effectiveness and
complexity of the algorithm compared to other current methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_D/0/1/0/all/0/1"&gt;Duc P. Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skau_E/0/1/0/all/0/1"&gt;Erik Skau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desantis_D/0/1/0/all/0/1"&gt;Derek Desantis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexandrov_B/0/1/0/all/0/1"&gt;Boian Alexandrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NRGNN: Learning a Label Noise-Resistant Graph Neural Network on Sparsely and Noisily Labeled Graphs. (arXiv:2106.04714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04714</id>
        <link href="http://arxiv.org/abs/2106.04714"/>
        <updated>2021-06-10T01:56:46.925Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have achieved promising results for
semi-supervised learning tasks on graphs such as node classification. Despite
the great success of GNNs, many real-world graphs are often sparsely and
noisily labeled, which could significantly degrade the performance of GNNs, as
the noisy information could propagate to unlabeled nodes via graph structure.
Thus, it is important to develop a label noise-resistant GNN for
semi-supervised node classification. Though extensive studies have been
conducted to learn neural networks with noisy labels, they mostly focus on
independent and identically distributed data and assume a large number of noisy
labels are available, which are not directly applicable for GNNs. Thus, we
investigate a novel problem of learning a robust GNN with noisy and limited
labels. To alleviate the negative effects of label noise, we propose to link
the unlabeled nodes with labeled nodes of high feature similarity to bring more
clean label information. Furthermore, accurate pseudo labels could be obtained
by this strategy to provide more supervision and further reduce the effects of
label noise. Our theoretical and empirical analysis verify the effectiveness of
these two strategies under mild conditions. Extensive experiments on real-world
datasets demonstrate the effectiveness of the proposed method in learning a
robust GNN with noisy and limited labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_E/0/1/0/all/0/1"&gt;Enyan Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1"&gt;Charu Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suhang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style. (arXiv:2106.04619v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04619</id>
        <link href="http://arxiv.org/abs/2106.04619"/>
        <updated>2021-06-10T01:56:46.909Z</updated>
        <summary type="html"><![CDATA[Self-supervised representation learning has shown remarkable success in a
number of domains. A common practice is to perform data augmentation via
hand-crafted transformations intended to leave the semantics of the data
invariant. We seek to understand the empirical success of this approach from a
theoretical perspective. We formulate the augmentation process as a latent
variable model by postulating a partition of the latent representation into a
content component, which is assumed invariant to augmentation, and a style
component, which is allowed to change. Unlike prior work on disentanglement and
independent component analysis, we allow for both nontrivial statistical and
causal dependencies in the latent space. We study the identifiability of the
latent representation based on pairs of views of the observations and prove
sufficient conditions that allow us to identify the invariant content partition
up to an invertible mapping in both generative and discriminative settings. We
find numerical simulations with dependent latent variables are consistent with
our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional,
visually complex images with rich causal dependencies, which we use to study
the effect of data augmentations performed in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1"&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1"&gt;Yash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gresele_L/0/1/0/all/0/1"&gt;Luigi Gresele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1"&gt;Michel Besserve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1"&gt;Francesco Locatello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04763</id>
        <link href="http://arxiv.org/abs/2106.04763"/>
        <updated>2021-06-10T01:56:46.903Z</updated>
        <summary type="html"><![CDATA[We study the problem of best-arm identification (BAI) in contextual bandits
in the fixed-budget setting. We propose a general successive elimination
algorithm that proceeds in stages and eliminates a fixed fraction of suboptimal
arms in each stage. This design takes advantage of the strengths of static and
adaptive allocations. We analyze the algorithm in linear models and obtain a
better error bound than prior work. We also apply it to generalized linear
models (GLMs) and bound its error. This is the first BAI algorithm for GLMs in
the fixed-budget setting. Our extensive numerical experiments show that our
algorithm outperforms the state of art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1"&gt;MohammadJavad Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curriculum Design for Teaching via Demonstrations: Theory and Applications. (arXiv:2106.04696v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04696</id>
        <link href="http://arxiv.org/abs/2106.04696"/>
        <updated>2021-06-10T01:56:46.895Z</updated>
        <summary type="html"><![CDATA[We consider the problem of teaching via demonstrations in sequential
decision-making settings. In particular, we study how to design a personalized
curriculum over demonstrations to speed up the learner's convergence. We
provide a unified curriculum strategy for two popular learner models: Maximum
Causal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy
Behavioral Cloning (CrossEnt-BC). Our unified strategy induces a ranking over
demonstrations based on a notion of difficulty scores computed w.r.t. the
teacher's optimal policy and the learner's current policy. Compared to the
state of the art, our strategy doesn't require access to the learner's internal
dynamics and still enjoys similar convergence guarantees under mild technical
conditions. Furthermore, we adapt our curriculum strategy to teach a learner
using domain knowledge in the form of task-specific difficulty scores when the
teacher's optimal policy is unknown. Experiments on a car driving simulator
environment and shortest path problems in a grid-world environment demonstrate
the effectiveness of our proposed curriculum strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yengera_G/0/1/0/all/0/1"&gt;Gaurav Yengera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devidze_R/0/1/0/all/0/1"&gt;Rati Devidze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamalaruban_P/0/1/0/all/0/1"&gt;Parameswaran Kamalaruban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1"&gt;Adish Singla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Price Against a Moving Target. (arXiv:2106.04689v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.04689</id>
        <link href="http://arxiv.org/abs/2106.04689"/>
        <updated>2021-06-10T01:56:46.888Z</updated>
        <summary type="html"><![CDATA[In the Learning to Price setting, a seller posts prices over time with the
goal of maximizing revenue while learning the buyer's valuation. This problem
is very well understood when values are stationary (fixed or iid). Here we
study the problem where the buyer's value is a moving target, i.e., they change
over time either by a stochastic process or adversarially with bounded
variation. In either case, we provide matching upper and lower bounds on the
optimal revenue loss. Since the target is moving, any information learned soon
becomes out-dated, which forces the algorithms to keep switching between
exploring and exploiting phases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leme_R/0/1/0/all/0/1"&gt;Renato Paes Leme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivan_B/0/1/0/all/0/1"&gt;Balasubramanian Sivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1"&gt;Yifeng Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worah_P/0/1/0/all/0/1"&gt;Pratik Worah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OODIn: An Optimised On-Device Inference Framework for Heterogeneous Mobile Devices. (arXiv:2106.04723v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04723</id>
        <link href="http://arxiv.org/abs/2106.04723"/>
        <updated>2021-06-10T01:56:46.867Z</updated>
        <summary type="html"><![CDATA[Radical progress in the field of deep learning (DL) has led to unprecedented
accuracy in diverse inference tasks. As such, deploying DL models across mobile
platforms is vital to enable the development and broad availability of the
next-generation intelligent apps. Nevertheless, the wide and optimised
deployment of DL models is currently hindered by the vast system heterogeneity
of mobile devices, the varying computational cost of different DL models and
the variability of performance needs across DL applications. This paper
proposes OODIn, a framework for the optimised deployment of DL apps across
heterogeneous mobile devices. OODIn comprises a novel DL-specific software
architecture together with an analytical framework for modelling DL
applications that: (1) counteract the variability in device resources and DL
models by means of a highly parametrised multi-layer design; and (2) perform a
principled optimisation of both model- and system-level parameters through a
multi-objective formulation, designed for DL inference apps, in order to adapt
the deployment to the user-specified performance requirements and device
capabilities. Quantitative evaluation shows that the proposed framework
consistently outperforms status-quo designs across heterogeneous devices and
delivers up to 4.3x and 3.5x performance gain over highly optimised platform-
and model-aware designs respectively, while effectively adapting execution to
dynamic changes in resource availability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1"&gt;Stylianos I. Venieris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panopoulos_I/0/1/0/all/0/1"&gt;Ioannis Panopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_I/0/1/0/all/0/1"&gt;Iakovos S. Venieris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Faster Algorithms for Bilevel Optimization. (arXiv:2106.04692v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04692</id>
        <link href="http://arxiv.org/abs/2106.04692"/>
        <updated>2021-06-10T01:56:46.854Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization has been widely applied in many important machine
learning applications such as hyperparameter optimization and meta-learning.
Recently, several momentum-based algorithms have been proposed to solve bilevel
optimization problems faster. However, those momentum-based algorithms do not
achieve provably better computational complexity than
$\mathcal{O}(\epsilon^{-2})$ of the SGD-based algorithm. In this paper, we
propose two new algorithms for bilevel optimization, where the first algorithm
adopts momentum-based recursive iterations, and the second algorithm adopts
recursive gradient estimations in nested loops to decrease the variance. We
show that both algorithms achieve the complexity of
$\mathcal{O}(\epsilon^{-1.5})$, which outperforms all existing algorithms by
the order of magnitude. Our experiments validate our theoretical results and
demonstrate the superior empirical performance of our algorithms in
hyperparameter applications. Our codes for MRBO, VRBO and other benchmarks are
available $\text{online}^1$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junjie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1"&gt;Kaiyi Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingbin Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Adaptive Swarm System (SASS). (arXiv:2106.04679v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2106.04679</id>
        <link href="http://arxiv.org/abs/2106.04679"/>
        <updated>2021-06-10T01:56:46.837Z</updated>
        <summary type="html"><![CDATA[Distributed artificial intelligence (DAI) studies artificial intelligence
entities working together to reason, plan, solve problems, organize behaviors
and strategies, make collective decisions and learn. This Ph.D. research
proposes a principled Multi-Agent Systems (MAS) cooperation framework,
Self-Adaptive Swarm System (SASS), to bridge the fourth level automation gap
between perception, communication, planning, execution, decision-making, and
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning. (arXiv:2106.04590v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04590</id>
        <link href="http://arxiv.org/abs/2106.04590"/>
        <updated>2021-06-10T01:56:46.832Z</updated>
        <summary type="html"><![CDATA[We propose a new framework of synthesizing data using deep generative models
in a differentially private manner. Within our framework, sensitive data are
sanitized with rigorous privacy guarantees in a one-shot fashion, such that
training deep generative models is possible without re-using the original data.
Hence, no extra privacy costs or model constraints are incurred, in contrast to
popular approaches such as Differentially Private Stochastic Gradient Descent
(DP-SGD), which, among other issues, causes degradation in privacy guarantees
as the training iteration increases. We demonstrate a realization of our
framework by making use of the characteristic function and an adversarial
re-weighting objective, which are of independent interest as well. Our proposal
has theoretical guarantees of performance, and empirical evaluations on
multiple datasets show that our approach outperforms other methods at
reasonable levels of privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1"&gt;Seng Pei Liew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_T/0/1/0/all/0/1"&gt;Tsubasa Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueno_M/0/1/0/all/0/1"&gt;Michihiko Ueno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04830</id>
        <link href="http://arxiv.org/abs/2012.04830"/>
        <updated>2021-06-10T01:56:46.805Z</updated>
        <summary type="html"><![CDATA[Cataract is one of the leading causes of reversible visual impairment and
blindness globally. Over the years, researchers have achieved significant
progress in developing state-of-the-art artificial intelligence techniques for
automatic cataract classification and grading, helping clinicians prevent and
treat cataract in time. This paper provides a comprehensive survey of recent
advances in machine learning for cataract classification and grading based on
ophthalmic images. We summarize existing literature from two research
directions: conventional machine learning techniques and deep learning
techniques. This paper also provides insights into existing works of both
merits and limitations. In addition, we discuss several challenges of automatic
cataract classification and grading based on machine learning techniques and
present possible solutions to these challenges for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiansheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1"&gt;Risa Higashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpeechBrain: A General-Purpose Speech Toolkit. (arXiv:2106.04624v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.04624</id>
        <link href="http://arxiv.org/abs/2106.04624"/>
        <updated>2021-06-10T01:56:46.799Z</updated>
        <summary type="html"><![CDATA[SpeechBrain is an open-source and all-in-one speech toolkit. It is designed
to facilitate the research and development of neural speech processing
technologies by being simple, flexible, user-friendly, and well-documented.
This paper describes the core architecture designed to support several tasks of
common interest, allowing users to naturally conceive, compare and share novel
speech processing pipelines. SpeechBrain achieves competitive or
state-of-the-art performance in a wide range of speech benchmarks. It also
provides training recipes, pretrained models, and inference scripts for popular
speech datasets, as well as tutorials which allow anyone with basic Python
proficiency to familiarize themselves with speech technologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ravanelli_M/0/1/0/all/0/1"&gt;Mirco Ravanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parcollet_T/0/1/0/all/0/1"&gt;Titouan Parcollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Plantinga_P/0/1/0/all/0/1"&gt;Peter Plantinga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rouhe_A/0/1/0/all/0/1"&gt;Aku Rouhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cornell_S/0/1/0/all/0/1"&gt;Samuele Cornell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lugosch_L/0/1/0/all/0/1"&gt;Loren Lugosch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Subakan_C/0/1/0/all/0/1"&gt;Cem Subakan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dawalatabad_N/0/1/0/all/0/1"&gt;Nauman Dawalatabad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heba_A/0/1/0/all/0/1"&gt;Abdelwahab Heba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhong_J/0/1/0/all/0/1"&gt;Jianyuan Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chou_J/0/1/0/all/0/1"&gt;Ju-Chieh Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yeh_S/0/1/0/all/0/1"&gt;Sung-Lin Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_S/0/1/0/all/0/1"&gt;Szu-Wei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liao_C/0/1/0/all/0/1"&gt;Chien-Feng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rastorgueva_E/0/1/0/all/0/1"&gt;Elena Rastorgueva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grondin_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Grondin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aris_W/0/1/0/all/0/1"&gt;William Aris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Na_H/0/1/0/all/0/1"&gt;Hwidong Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mori_R/0/1/0/all/0/1"&gt;Renato De Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EXPObench: Benchmarking Surrogate-based Optimisation Algorithms on Expensive Black-box Functions. (arXiv:2106.04618v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04618</id>
        <link href="http://arxiv.org/abs/2106.04618"/>
        <updated>2021-06-10T01:56:46.788Z</updated>
        <summary type="html"><![CDATA[Surrogate algorithms such as Bayesian optimisation are especially designed
for black-box optimisation problems with expensive objectives, such as
hyperparameter tuning or simulation-based optimisation. In the literature,
these algorithms are usually evaluated with synthetic benchmarks which are well
established but have no expensive objective, and only on one or two real-life
applications which vary wildly between papers. There is a clear lack of
standardisation when it comes to benchmarking surrogate algorithms on
real-life, expensive, black-box objective functions. This makes it very
difficult to draw conclusions on the effect of algorithmic contributions. A new
benchmark library, EXPObench, provides first steps towards such a
standardisation. The library is used to provide an extensive comparison of six
different surrogate algorithms on four expensive optimisation problems from
different real-life applications. This has led to new insights regarding the
relative importance of exploration, the evaluation time of the objective, and
the used model. A further contribution is that we make the algorithms and
benchmark problem instances publicly available, contributing to more uniform
analysis of surrogate algorithms. Most importantly, we include the performance
of the six algorithms on all evaluated problem instances. This results in a
unique new dataset that lowers the bar for researching new methods as the
number of expensive evaluations required for comparison is significantly
reduced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bliek_L/0/1/0/all/0/1"&gt;Laurens Bliek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guijt_A/0/1/0/all/0/1"&gt;Arthur Guijt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_R/0/1/0/all/0/1"&gt;Rickard Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verwer_S/0/1/0/all/0/1"&gt;Sicco Verwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weerdt_M/0/1/0/all/0/1"&gt;Mathijs de Weerdt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Mixture Estimation from Weighted Samples. (arXiv:2106.05109v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.05109</id>
        <link href="http://arxiv.org/abs/2106.05109"/>
        <updated>2021-06-10T01:56:46.754Z</updated>
        <summary type="html"><![CDATA[We consider estimating the parameters of a Gaussian mixture density with a
given number of components best representing a given set of weighted samples.
We adopt a density interpretation of the samples by viewing them as a discrete
Dirac mixture density over a continuous domain with weighted components. Hence,
Gaussian mixture fitting is viewed as density re-approximation. In order to
speed up computation, an expectation-maximization method is proposed that
properly considers not only the sample locations, but also the corresponding
weights. It is shown that methods from literature do not treat the weights
correctly, resulting in wrong estimates. This is demonstrated with simple
counterexamples. The proposed method works in any number of dimensions with the
same computational load as standard Gaussian mixture estimators for unweighted
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Frisch_D/0/1/0/all/0/1"&gt;Daniel Frisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hanebeck_U/0/1/0/all/0/1"&gt;Uwe D. Hanebeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeRF in detail: Learning to sample for view synthesis. (arXiv:2106.05264v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05264</id>
        <link href="http://arxiv.org/abs/2106.05264"/>
        <updated>2021-06-10T01:56:46.723Z</updated>
        <summary type="html"><![CDATA[Neural radiance fields (NeRF) methods have demonstrated impressive novel view
synthesis performance. The core approach is to render individual rays by
querying a neural network at points sampled along the ray to obtain the density
and colour of the sampled points, and integrating this information using the
rendering equation. Since dense sampling is computationally prohibitive, a
common solution is to perform coarse-to-fine sampling.

In this work we address a clear limitation of the vanilla coarse-to-fine
approach -- that it is based on a heuristic and not trained end-to-end for the
task at hand. We introduce a differentiable module that learns to propose
samples and their importance for the fine network, and consider and compare
multiple alternatives for its neural architecture. Training the proposal module
from scratch can be unstable due to lack of supervision, so an effective
pre-training strategy is also put forward. The approach, named `NeRF in detail'
(NeRF-ID), achieves superior view synthesis quality over NeRF and the
state-of-the-art on the synthetic Blender benchmark and on par or better
performance on the real LLFF-NeRF scenes. Furthermore, by leveraging the
predicted sample importance, a 25% saving in computation can be achieved
without significantly sacrificing the rendering quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_R/0/1/0/all/0/1"&gt;Relja Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Densely connected multidilated convolutional networks for dense prediction tasks. (arXiv:2011.11844v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11844</id>
        <link href="http://arxiv.org/abs/2011.11844"/>
        <updated>2021-06-10T01:56:46.711Z</updated>
        <summary type="html"><![CDATA[Tasks that involve high-resolution dense prediction require a modeling of
both local and global patterns in a large input field. Although the local and
global structures often depend on each other and their simultaneous modeling is
important, many convolutional neural network (CNN)-based approaches interchange
representations in different resolutions only a few times. In this paper, we
claim the importance of a dense simultaneous modeling of multiresolution
representation and propose a novel CNN architecture called densely connected
multidilated DenseNet (D3Net). D3Net involves a novel multidilated convolution
that has different dilation factors in a single layer to model different
resolutions simultaneously. By combining the multidilated convolution with the
DenseNet architecture, D3Net incorporates multiresolution learning with an
exponentially growing receptive field in almost all layers, while avoiding the
aliasing problem that occurs when we naively incorporate the dilated
convolution in DenseNet. Experiments on the image semantic segmentation task
using Cityscapes and the audio source separation task using MUSDB18 show that
the proposed method has superior performance over state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_N/0/1/0/all/0/1"&gt;Naoya Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1"&gt;Yuki Mitsufuji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Class Relations: Absolute-relative Supervised and Unsupervised Few-shot Learning. (arXiv:2001.03919v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03919</id>
        <link href="http://arxiv.org/abs/2001.03919"/>
        <updated>2021-06-10T01:56:46.697Z</updated>
        <summary type="html"><![CDATA[The majority of existing few-shot learning methods describe image relations
with binary labels. However, such binary relations are insufficient to teach
the network complicated real-world relations, due to the lack of decision
smoothness. Furthermore, current few-shot learning models capture only the
similarity via relation labels, but they are not exposed to class concepts
associated with objects, which is likely detrimental to the classification
performance due to underutilization of the available class labels. To
paraphrase, children learn the concept of tiger from a few of actual examples
as well as from comparisons of tiger to other animals. Thus, we hypothesize
that in fact both similarity and class concept learning must be occurring
simultaneously. With these observations at hand, we study the fundamental
problem of simplistic class modeling in current few-shot learning methods. We
rethink the relations between class concepts, and propose a novel
Absolute-relative Learning paradigm to fully take advantage of label
information to refine the image representations and correct the relation
understanding in both supervised and unsupervised scenarios. Our proposed
paradigm improves the performance of several the state-of-the-art models on
publicly available datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongguang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1"&gt;Piotr Koniusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1"&gt;Songlei Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongdong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition. (arXiv:2007.01755v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.01755</id>
        <link href="http://arxiv.org/abs/2007.01755"/>
        <updated>2021-06-10T01:56:46.691Z</updated>
        <summary type="html"><![CDATA[Multi-label image recognition is a practical and challenging task compared to
single-label image classification. However, previous works may be suboptimal
because of a great number of object proposals or complex attentional region
generation modules. In this paper, we propose a simple but efficient two-stream
framework to recognize multi-category objects from global image to local
regions, similar to how human beings perceive objects. To bridge the gap
between global and local streams, we propose a multi-class attentional region
module which aims to make the number of attentional regions as small as
possible and keep the diversity of these regions as high as possible. Our
method can efficiently and effectively recognize multi-class objects with an
affordable computation cost and a parameter-free region localization module.
Over three benchmarks on multi-label image classification, we create new
state-of-the-art results with a single model only using image semantics without
label dependency. In addition, the effectiveness of the proposed method is
extensively demonstrated under different factors such as global pooling
strategy, input size and network architecture. Code has been made available
at~\url{https://github.com/gaobb/MCAR}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1"&gt;Bin-Bin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hong-Yu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Training Stronger Video Vision Transformers for EPIC-KITCHENS-100 Action Recognition. (arXiv:2106.05058v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05058</id>
        <link href="http://arxiv.org/abs/2106.05058"/>
        <updated>2021-06-10T01:56:46.685Z</updated>
        <summary type="html"><![CDATA[With the recent surge in the research of vision transformers, they have
demonstrated remarkable potential for various challenging computer vision
applications, such as image recognition, point cloud classification as well as
video understanding. In this paper, we present empirical results for training a
stronger video vision transformer on the EPIC-KITCHENS-100 Action Recognition
dataset. Specifically, we explore training techniques for video vision
transformers, such as augmentations, resolutions as well as initialization,
etc. With our training recipe, a single ViViT model achieves the performance of
47.4\% on the validation set of EPIC-KITCHENS-100 dataset, outperforming what
is reported in the original paper by 3.4%. We found that video transformers are
especially good at predicting the noun in the verb-noun action prediction task.
This makes the overall action prediction accuracy of video transformers notably
higher than convolutional ones. Surprisingly, even the best video transformers
underperform the convolutional networks on the verb prediction. Therefore, we
combine the video vision transformers and some of the convolutional video
networks and present our solution to the EPIC-KITCHENS-100 Action Recognition
competition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jianwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1"&gt;Zhurong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Marcelo H. Ang Jr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affordance Transfer Learning for Human-Object Interaction Detection. (arXiv:2104.02867v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02867</id>
        <link href="http://arxiv.org/abs/2104.02867"/>
        <updated>2021-06-10T01:56:46.666Z</updated>
        <summary type="html"><![CDATA[Reasoning the human-object interactions (HOI) is essential for deeper scene
understanding, while object affordances (or functionalities) are of great
importance for human to discover unseen HOIs with novel objects. Inspired by
this, we introduce an affordance transfer learning approach to jointly detect
HOIs with novel objects and recognize affordances. Specifically, HOI
representations can be decoupled into a combination of affordance and object
representations, making it possible to compose novel interactions by combining
affordance representations and novel object representations from additional
images, i.e. transferring the affordance to novel objects. With the proposed
affordance transfer learning, the model is also capable of inferring the
affordances of novel objects from known affordance representations. The
proposed method can thus be used to 1) improve the performance of HOI
detection, especially for the HOIs with unseen objects; and 2) infer the
affordances of novel objects. Experimental results on two datasets, HICO-DET
and HOI-COCO (from V-COCO), demonstrate significant improvements over recent
state-of-the-art methods for HOI detection and object affordance detection.
Code is available at https://github.com/zhihou7/HOI-CL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1"&gt;Zhi Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Baosheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xiaojiang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Representation Learning for Natural World Image Collections. (arXiv:2103.16483v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16483</id>
        <link href="http://arxiv.org/abs/2103.16483"/>
        <updated>2021-06-10T01:56:46.660Z</updated>
        <summary type="html"><![CDATA[Recent progress in self-supervised learning has resulted in models that are
capable of extracting rich representations from image collections without
requiring any explicit label supervision. However, to date the vast majority of
these approaches have restricted themselves to training on standard benchmark
datasets such as ImageNet. We argue that fine-grained visual categorization
problems, such as plant and animal species classification, provide an
informative testbed for self-supervised learning. In order to facilitate
progress in this area we present two new natural world visual classification
datasets, iNat2021 and NeWT. The former consists of 2.7M images from 10k
different species uploaded by users of the citizen science application
iNaturalist. We designed the latter, NeWT, in collaboration with domain experts
with the aim of benchmarking the performance of representation learning
algorithms on a suite of challenging natural world binary classification tasks
that go beyond standard species classification. These two new datasets allow us
to explore questions related to large-scale representation and transfer
learning in the context of fine-grained categories. We provide a comprehensive
analysis of feature extractors trained with and without supervision on ImageNet
and iNat2021, shedding light on the strengths and weaknesses of different
learned features across a diverse set of tasks. We find that features produced
by standard supervised methods still outperform those produced by
self-supervised approaches such as SimCLR. However, improved self-supervised
learning methods are constantly being released and the iNat2021 and NeWT
datasets are a valuable resource for tracking their progress.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horn_G/0/1/0/all/0/1"&gt;Grant Van Horn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1"&gt;Elijah Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1"&gt;Sara Beery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1"&gt;Kimberly Wilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1"&gt;Serge Belongie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1"&gt;Oisin Mac Aodha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Learning and Adaptation with Membrane Potential and Activation Threshold Homeostasis. (arXiv:2104.10851v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10851</id>
        <link href="http://arxiv.org/abs/2104.10851"/>
        <updated>2021-06-10T01:56:46.654Z</updated>
        <summary type="html"><![CDATA[Most classical (non-spiking) neural network models disregard internal neuron
dynamics and treat neurons as simple input integrators. However, biological
neurons have an internal state governed by complex dynamics that plays a
crucial role in learning, adaptation and the overall network activity and
behaviour. This paper presents the Membrane Potential and Activation Threshold
Homeostasis (MPATH) neuron model, which combines several biologically inspired
mechanisms to efficiently simulate internal neuron dynamics with a single
parameter analogous to the membrane time constant in biological neurons. The
model allows neurons to maintain a form of dynamic equilibrium by automatically
regulating their activity when presented with fluctuating input. One
consequence of the MPATH model is that it imbues neurons with a sense of time
without recurrent connections, paving the way for modelling processes that
depend on temporal aspects of neuron activity. Experiments demonstrate the
model's ability to adapt to and continually learn from its input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hadjiivanov_A/0/1/0/all/0/1"&gt;Alexander Hadjiivanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness in Compressed Neural Networks for Object Detection. (arXiv:2102.05509v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05509</id>
        <link href="http://arxiv.org/abs/2102.05509"/>
        <updated>2021-06-10T01:56:46.649Z</updated>
        <summary type="html"><![CDATA[Model compression techniques allow to significantly reduce the computational
cost associated with data processing by deep neural networks with only a minor
decrease in average accuracy. Simultaneously, reducing the model size may have
a large effect on noisy cases or objects belonging to less frequent classes. It
is a crucial problem from the perspective of the models' safety, especially for
object detection in the autonomous driving setting, which is considered in this
work. It was shown in the paper that the sensitivity of compressed models to
different distortion types is nuanced, and some of the corruptions are heavily
impacted by the compression methods (i.e., additive noise), while others (blur
effect) are only slightly affected. A common way to improve the robustness of
models is to use data augmentation, which was confirmed to positively affect
models' robustness, also for highly compressed models. It was further shown
that while data imbalance methods brought only a slight increase in accuracy
for the baseline model (without compression), the impact was more striking at
higher compression rates for the structured pruning. Finally, methods for
handling data imbalance brought a significant improvement of the pruned models'
worst-detected class accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1"&gt;Sebastian Cygert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czyzewski_A/0/1/0/all/0/1"&gt;Andrzej Czy&amp;#x17c;ewski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Triplet Loss: Uncertainty Quantification in Image Retrieval. (arXiv:2011.12663v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12663</id>
        <link href="http://arxiv.org/abs/2011.12663"/>
        <updated>2021-06-10T01:56:46.641Z</updated>
        <summary type="html"><![CDATA[Uncertainty quantification in image retrieval is crucial for downstream
decisions, yet it remains a challenging and largely unexplored problem. Current
methods for estimating uncertainties are poorly calibrated, computationally
expensive, or based on heuristics. We present a new method that views image
embeddings as stochastic features rather than deterministic features. Our two
main contributions are (1) a likelihood that matches the triplet constraint and
that evaluates the probability of an anchor being closer to a positive than a
negative; and (2) a prior over the feature space that justifies the
conventional l2 normalization. To ensure computational efficiency, we derive a
variational approximation of the posterior, called the Bayesian triplet loss,
that produces state-of-the-art uncertainty estimates and matches the predictive
performance of current state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Warburg_F/0/1/0/all/0/1"&gt;Frederik Warburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorgensen_M/0/1/0/all/0/1"&gt;Martin J&amp;#xf8;rgensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1"&gt;Javier Civera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation. (arXiv:2002.01619v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.01619</id>
        <link href="http://arxiv.org/abs/2002.01619"/>
        <updated>2021-06-10T01:56:46.624Z</updated>
        <summary type="html"><![CDATA[Monocular 3D object detection task aims to predict the 3D bounding boxes of
objects based on monocular RGB images. Since the location recovery in 3D space
is quite difficult on account of absence of depth information, this paper
proposes a novel unified framework which decomposes the detection problem into
a structured polygon prediction task and a depth recovery task. Different from
the widely studied 2D bounding boxes, the proposed novel structured polygon in
the 2D image consists of several projected surfaces of the target object.
Compared to the widely-used 3D bounding box proposals, it is shown to be a
better representation for 3D detection. In order to inversely project the
predicted 2D structured polygon to a cuboid in the 3D physical world, the
following depth recovery task uses the object height prior to complete the
inverse projection transformation with the given camera projection matrix.
Moreover, a fine-grained 3D box refinement scheme is proposed to further
rectify the 3D detection results. Experiments are conducted on the challenging
KITTI benchmark, in which our method achieves state-of-the-art detection
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yingjie Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Buyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1"&gt;Zeyu Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xingyu Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Facet Clustering Variational Autoencoders. (arXiv:2106.05241v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.05241</id>
        <link href="http://arxiv.org/abs/2106.05241"/>
        <updated>2021-06-10T01:56:46.618Z</updated>
        <summary type="html"><![CDATA[Work in deep clustering focuses on finding a single partition of data.
However, high-dimensional data, such as images, typically feature multiple
interesting characteristics one could cluster over. For example, images of
objects against a background could be clustered over the shape of the object
and separately by the colour of the background. In this paper, we introduce
Multi-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of
variational autoencoders with a hierarchy of latent variables, each with a
Mixture-of-Gaussians prior, that learns multiple clusterings simultaneously,
and is trained fully unsupervised and end-to-end. MFCVAE uses a
progressively-trained ladder architecture which leads to highly stable
performance. We provide novel theoretical results for optimising the ELBO
analytically with respect to the categorical variational posterior
distribution, and corrects earlier influential theoretical work. On image
benchmarks, we demonstrate that our approach separates out and clusters over
different aspects of the data in a disentangled manner. We also show other
advantages of our model: the compositionality of its latent space and that it
provides controlled generation of samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Falck_F/0/1/0/all/0/1"&gt;Fabian Falck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Willetts_M/0/1/0/all/0/1"&gt;Matthew Willetts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nicholson_G/0/1/0/all/0/1"&gt;George Nicholson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yau_C/0/1/0/all/0/1"&gt;Christopher Yau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1"&gt;Christopher C Holmes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Noise for Multi-Attack Robustness. (arXiv:2006.12135v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12135</id>
        <link href="http://arxiv.org/abs/2006.12135"/>
        <updated>2021-06-10T01:56:46.613Z</updated>
        <summary type="html"><![CDATA[Adversarial learning has emerged as one of the successful techniques to
circumvent the susceptibility of existing methods against adversarial
perturbations. However, the majority of existing defense methods are tailored
to defend against a single category of adversarial perturbation (e.g.
$\ell_\infty$-attack). In safety-critical applications, this makes these
methods extraneous as the attacker can adopt diverse adversaries to deceive the
system. Moreover, training on multiple perturbations simultaneously
significantly increases the computational overhead during training. To address
these challenges, we propose a novel meta-learning framework that explicitly
learns to generate noise to improve the model's robustness against multiple
types of attacks. Its key component is Meta Noise Generator (MNG) that outputs
optimal noise to stochastically perturb a given sample, such that it helps
lower the error on diverse adversarial perturbations. By utilizing samples
generated by MNG, we train a model by enforcing the label consistency across
multiple perturbations. We validate the robustness of models trained by our
scheme on various datasets and against a wide variety of perturbations,
demonstrating that it significantly outperforms the baselines across multiple
perturbations with a marginal computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1"&gt;Divyam Madaan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Can Always Catch You: Detecting Adversarial Patched Objects WITH or WITHOUT Signature. (arXiv:2106.05261v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05261</id>
        <link href="http://arxiv.org/abs/2106.05261"/>
        <updated>2021-06-10T01:56:46.606Z</updated>
        <summary type="html"><![CDATA[Recently, the object detection based on deep learning has proven to be
vulnerable to adversarial patch attacks. The attackers holding a specially
crafted patch can hide themselves from the state-of-the-art person detectors,
e.g., YOLO, even in the physical world. This kind of attack can bring serious
security threats, such as escaping from surveillance cameras. In this paper, we
deeply explore the detection problems about the adversarial patch attacks to
the object detection. First, we identify a leverageable signature of existing
adversarial patches from the point of the visualization explanation. A fast
signature-based defense method is proposed and demonstrated to be effective.
Second, we design an improved patch generation algorithm to reveal the risk
that the signature-based way may be bypassed by the techniques emerging in the
future. The newly generated adversarial patches can successfully evade the
proposed signature-based defense. Finally, we present a novel
signature-independent detection method based on the internal content semantics
consistency rather than any attack-specific prior knowledge. The fundamental
intuition is that the adversarial object can appear locally but disappear
globally in an input image. The experiments demonstrate that the
signature-independent method can effectively detect the existing and improved
attacks. It has also proven to be a general method by detecting unforeseen and
even other types of attacks without any attack-specific prior knowledge. The
two proposed detection methods can be adopted in different scenarios, and we
believe that combining them can offer a comprehensive protection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1"&gt;Bin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianjun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CARPe Posterum: A Convolutional Approach for Real-time Pedestrian Path Prediction. (arXiv:2005.12469v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12469</id>
        <link href="http://arxiv.org/abs/2005.12469"/>
        <updated>2021-06-10T01:56:46.601Z</updated>
        <summary type="html"><![CDATA[Pedestrian path prediction is an essential topic in computer vision and video
understanding. Having insight into the movement of pedestrians is crucial for
ensuring safe operation in a variety of applications including autonomous
vehicles, social robots, and environmental monitoring. Current works in this
area utilize complex generative or recurrent methods to capture many possible
futures. However, despite the inherent real-time nature of predicting future
paths, little work has been done to explore accurate and computationally
efficient approaches for this task. To this end, we propose a convolutional
approach for real-time pedestrian path prediction, CARPe. It utilizes a
variation of Graph Isomorphism Networks in combination with an agile
convolutional neural network design to form a fast and accurate path prediction
approach. Notable results in both inference speed and prediction accuracy are
achieved, improving FPS considerably in comparison to current state-of-the-art
methods while delivering competitive accuracy on well-known path prediction
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1"&gt;Mat&amp;#xed;as Mendieta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1"&gt;Hamed Tabkhi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it Enough to Optimize CNN Architectures on ImageNet?. (arXiv:2103.09108v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09108</id>
        <link href="http://arxiv.org/abs/2103.09108"/>
        <updated>2021-06-10T01:56:46.594Z</updated>
        <summary type="html"><![CDATA[An implicit but pervasive hypothesis of modern computer vision research is
that convolutional neural network (CNN) architectures that perform better on
ImageNet will also perform better on other vision datasets. We challenge this
hypothesis through an extensive empirical study for which we train 500 sampled
CNN architectures on ImageNet as well as 8 other image classification datasets
from a wide array of application domains. The relationship between architecture
and performance varies wildly, depending on the datasets. For some of them, the
performance correlation with ImageNet is even negative. Clearly, it is not
enough to optimize architectures solely for ImageNet when aiming for progress
that is relevant for all applications. Therefore, we identify two
dataset-specific performance indicators: the cumulative width across layers as
well as the total depth of the network. Lastly, we show that the range of
dataset variability covered by ImageNet can be significantly extended by adding
ImageNet subsets restricted to few classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuggener_L/0/1/0/all/0/1"&gt;Lukas Tuggener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1"&gt;Thilo Stadelmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Adversarial Attacks on Neural Networks with Better Optimizer. (arXiv:2012.00567v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00567</id>
        <link href="http://arxiv.org/abs/2012.00567"/>
        <updated>2021-06-10T01:56:46.577Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks have outperformed humans in image recognition
tasks, but they remain vulnerable to attacks from adversarial examples. Since
these data are crafted by adding imperceptible noise to normal images, their
existence poses potential security threats to deep learning systems.
Sophisticated adversarial examples with strong attack performance can also be
used as a tool to evaluate the robustness of a model. However, the success rate
of adversarial attacks can be further improved in black-box environments.
Therefore, this study combines a modified Adam gradient descent algorithm with
the iterative gradient-based attack method. The proposed Adam Iterative Fast
Gradient Method is then used to improve the transferability of adversarial
examples. Extensive experiments on ImageNet showed that the proposed method
offers a higher attack success rate than existing iterative methods. By
extending our method, we achieved a state-of-the-art attack success rate of
95.0% on defense models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Heng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hengwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_R/0/1/0/all/0/1"&gt;Ruiyu Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time. (arXiv:2106.05266v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05266</id>
        <link href="http://arxiv.org/abs/2106.05266"/>
        <updated>2021-06-10T01:56:46.570Z</updated>
        <summary type="html"><![CDATA[Estimating 3D hand and object pose from a single image is an extremely
challenging problem: hands and objects are often self-occluded during
interactions, and the 3D annotations are scarce as even humans cannot directly
label the ground-truths from a single image perfectly. To tackle these
challenges, we propose a unified framework for estimating the 3D hand and
object poses with semi-supervised learning. We build a joint learning framework
where we perform explicit contextual reasoning between hand and object
representations by a Transformer. Going beyond limited 3D annotations in a
single image, we leverage the spatial-temporal consistency in large-scale
hand-object videos as a constraint for generating pseudo labels in
semi-supervised learning. Our method not only improves hand pose estimation in
challenging real-world dataset, but also substantially improve the object pose
which has fewer ground-truths per instance. By training with large-scale
diverse videos, our model also generalizes better across multiple out-of-domain
datasets. Project page and code: https://stevenlsw.github.io/Semi-Hand-Object]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shaowei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hanwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiarui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sifei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Models as a Data Source for Multiview Representation Learning. (arXiv:2106.05258v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05258</id>
        <link href="http://arxiv.org/abs/2106.05258"/>
        <updated>2021-06-10T01:56:46.564Z</updated>
        <summary type="html"><![CDATA[Generative models are now capable of producing highly realistic images that
look nearly indistinguishable from the data on which they are trained. This
raises the question: if we have good enough generative models, do we still need
datasets? We investigate this question in the setting of learning
general-purpose visual representations from a black-box generative model rather
than directly from data. Given an off-the-shelf image generator without any
access to its training data, we train representations from the samples output
by this generator. We compare several representation learning methods that can
be applied to this setting, using the latent space of the generator to generate
multiple "views" of the same semantic content. We show that for contrastive
methods, this multiview data can naturally be used to identify positive pairs
(nearby in latent space) and negative pairs (far apart in latent space). We
find that the resulting representations rival those learned directly from real
data, but that good performance requires care in the sampling strategy applied
and the training method. Generative models can be viewed as a compressed and
organized copy of a dataset, and we envision a future where more and more
"model zoos" proliferate while datasets become increasingly unwieldy, missing,
or private. This paper suggests several techniques for dealing with visual
representation learning in such a future. Code is released on our project page:
https://ali-design.github.io/GenRep/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jahanian_A/0/1/0/all/0/1"&gt;Ali Jahanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puig_X/0/1/0/all/0/1"&gt;Xavier Puig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonglong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Evaluation of Multimodal Models under Realistic Gray Box Assumption. (arXiv:2011.12902v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12902</id>
        <link href="http://arxiv.org/abs/2011.12902"/>
        <updated>2021-06-10T01:56:46.559Z</updated>
        <summary type="html"><![CDATA[This work examines the vulnerability of multimodal (image + text) models to
adversarial threats similar to those discussed in previous literature on
unimodal (image- or text-only) models. We introduce realistic assumptions of
partial model knowledge and access, and discuss how these assumptions differ
from the standard "black-box"/"white-box" dichotomy common in current
literature on adversarial attacks. Working under various levels of these
"gray-box" assumptions, we develop new attack methodologies unique to
multimodal classification and evaluate them on the Hateful Memes Challenge
classification task. We find that attacking multiple modalities yields stronger
attacks than unimodal attacks alone (inducing errors in up to 73% of cases),
and that the unimodal image attacks on multimodal classifiers we explored were
stronger than character-based text augmentation attacks (inducing errors on
average in 45% and 30% of cases, respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1"&gt;Ivan Evtimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howes_R/0/1/0/all/0/1"&gt;Russel Howes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolhansky_B/0/1/0/all/0/1"&gt;Brian Dolhansky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1"&gt;Hamed Firooz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1"&gt;Cristian Canton Ferrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Point of Gaze Estimator for Low-Resolution Imaging Systems Using Extracted Ocular Features Based Neural Architecture. (arXiv:2106.05106v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05106</id>
        <link href="http://arxiv.org/abs/2106.05106"/>
        <updated>2021-06-10T01:56:46.553Z</updated>
        <summary type="html"><![CDATA[A user's eyes provide means for Human Computer Interaction (HCI) research as
an important modal. The time to time scientific explorations of the eye has
already seen an upsurge of the benefits in HCI applications from gaze
estimation to the measure of attentiveness of a user looking at a screen for a
given time period. The eye tracking system as an assisting, interactive tool
can be incorporated by physically disabled individuals, fitted best for those
who have eyes as only a limited set of communication. The threefold objective
of this paper is - 1. To introduce a neural network based architecture to
predict users' gaze at 9 positions displayed in the 11.31{\deg} visual range on
the screen, through a low resolution based system such as a webcam in real time
by learning various aspects of eyes as an ocular feature set. 2.A collection of
coarsely supervised feature set obtained in real time which is also validated
through the user case study presented in the paper for 21 individuals ( 17 men
and 4 women ) from whom a 35k set of instances was derived with an accuracy
score of 82.36% and f1_score of 82.2% and 3.A detailed study over applicability
and underlying challenges of such systems. The experimental results verify the
feasibility and validity of the proposed eye gaze tracking model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sahay_A/0/1/0/all/0/1"&gt;Atul Sahay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_I/0/1/0/all/0/1"&gt;Imon Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arya_K/0/1/0/all/0/1"&gt;Kavi Arya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Neural Networks and Individual Neuron Importance via Information-Ordered Cumulative Ablation. (arXiv:1804.06679v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1804.06679</id>
        <link href="http://arxiv.org/abs/1804.06679"/>
        <updated>2021-06-10T01:56:46.536Z</updated>
        <summary type="html"><![CDATA[In this work, we investigate the use of three information-theoretic
quantities -- entropy, mutual information with the class variable, and a class
selectivity measure based on Kullback-Leibler divergence -- to understand and
study the behavior of already trained fully-connected feed-forward neural
networks. We analyze the connection between these information-theoretic
quantities and classification performance on the test set by cumulatively
ablating neurons in networks trained on MNIST, FashionMNIST, and CIFAR-10. Our
results parallel those recently published by Morcos et al., indicating that
class selectivity is not a good indicator for classification performance.
However, looking at individual layers separately, both mutual information and
class selectivity are positively correlated with classification performance, at
least for networks with ReLU activation functions. We provide explanations for
this phenomenon and conclude that it is ill-advised to compare the proposed
information-theoretic quantities across layers. Furthermore, we show that
cumulative ablation of neurons with ascending or descending
information-theoretic quantities can be used to formulate hypotheses regarding
the joint behavior of multiple neurons, such as redundancy and synergy, with
comparably low computational cost. We also draw connections to the information
bottleneck theory for neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1"&gt;Rana Ali Amjad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kairen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1"&gt;Bernhard C. Geiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images. (arXiv:2106.00116v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00116</id>
        <link href="http://arxiv.org/abs/2106.00116"/>
        <updated>2021-06-10T01:56:46.530Z</updated>
        <summary type="html"><![CDATA[Transfer learning aims to exploit pre-trained models for more efficient
follow-up training on wide range of downstream tasks and datasets, enabling
successful training also on small data. Recent line of work posits strong
benefits for model generalization and transfer when model size, data size, and
compute budget are increased for the pre-training. It remains however still
largely unclear whether the observed transfer improvement due to increase in
scale also holds when source and target data distributions are far apart from
each other. In this work we conduct large-scale pre-training on large source
datasets of either natural (ImageNet-21k/1k) or medical chest X-Ray images and
compare full and few-shot transfer using different target datasets from both
natural and medical imaging domains. Our observations provide evidence that
while pre-training and transfer on closely related datasets do show clear
benefit of increasing model and data size during pre-training, such benefits
are not clearly visible when source and target datasets are further apart.
These observations hold across both full and few-shot transfer and indicate
that scaling laws pointing to improvement of generalization and transfer with
increasing model and data size are incomplete and should be revised by taking
into account the type and proximity of the source and target data, to correctly
predict the effect of model and data scale during pre-training on transfer.
Remarkably, in full shot transfer to a large X-Ray chest imaging target
(PadChest), the largest model pre-trained on ImageNet-21k slightly outperforms
best models pre-trained on large X-Ray chest imaging data. This indicates
possibility to obtain high quality models for domain-specific transfer even
without access to large domain-specific data, by pre-training instead on
comparably very large, generic source data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1"&gt;Mehdi Cherti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1"&gt;Jenia Jitsev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for Investigating Learned Representations. (arXiv:1905.13308v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.13308</id>
        <link href="http://arxiv.org/abs/1905.13308"/>
        <updated>2021-06-10T01:56:46.523Z</updated>
        <summary type="html"><![CDATA[Hierarchy and compositionality are common latent properties in many natural
and scientific datasets. Determining when a deep network's hidden activations
represent hierarchy and compositionality is important both for understanding
deep representation learning and for applying deep networks in domains where
interpretability is crucial. However, current benchmark machine learning
datasets either have little hierarchical or compositional structure, or the
structure is not known. This gap impedes precise analysis of a network's
representations and thus hinders development of new methods that can learn such
properties. To address this gap, we developed a new benchmark dataset with
known hierarchical and compositional structure. The Hangul Fonts Dataset (HFD)
is comprised of 35 fonts from the Korean writing system (Hangul), each with
11,172 blocks (syllables) composed from the product of initial consonant,
medial vowel, and final consonant glyphs. All blocks can be grouped into a few
geometric types which induces a hierarchy across blocks. In addition, each
block is composed of individual glyphs with rotations, translations, scalings,
and naturalistic style variation across fonts. We find that both shallow and
deep unsupervised methods only show modest evidence of hierarchy and
compositionality in their representations of the HFD compared to supervised
deep networks. Supervised deep network representations contain structure
related to the geometrical hierarchy of the characters, but the compositional
structure of the data is not evident. Thus, HFD enables the identification of
shortcomings in existing methods, a critical first step toward developing new
machine learning algorithms to extract hierarchical and compositional structure
in the context of naturalistic variability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Livezey_J/0/1/0/all/0/1"&gt;Jesse A. Livezey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_A/0/1/0/all/0/1"&gt;Ahyeon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_J/0/1/0/all/0/1"&gt;Jacob Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1"&gt;Kristofer E. Bouchard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Space-Time Networks with Improved Memory Coverage for Efficient Video Object Segmentation. (arXiv:2106.05210v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05210</id>
        <link href="http://arxiv.org/abs/2106.05210"/>
        <updated>2021-06-10T01:56:46.517Z</updated>
        <summary type="html"><![CDATA[This paper presents a simple yet effective approach to modeling space-time
correspondences in the context of video object segmentation. Unlike most
existing approaches, we establish correspondences directly between frames
without re-encoding the mask features for every object, leading to a highly
efficient and robust framework. With the correspondences, every node in the
current query frame is inferred by aggregating features from the past in an
associative fashion. We cast the aggregation process as a voting problem and
find that the existing inner-product affinity leads to poor use of memory with
a small (fixed) subset of memory nodes dominating the votes, regardless of the
query. In light of this phenomenon, we propose using the negative squared
Euclidean distance instead to compute the affinities. We validated that every
memory node now has a chance to contribute, and experimentally showed that such
diversified voting is beneficial to both memory efficiency and inference
accuracy. The synergy of correspondence networks and diversified voting works
exceedingly well, achieves new state-of-the-art results on both DAVIS and
YouTubeVOS datasets while running significantly faster at 20+ FPS for multiple
objects without bells and whistles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Ho Kei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chi-Keung Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All Tokens Matter: Token Labeling for Training Better Vision Transformers. (arXiv:2104.10858v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10858</id>
        <link href="http://arxiv.org/abs/2104.10858"/>
        <updated>2021-06-10T01:56:46.511Z</updated>
        <summary type="html"><![CDATA[In this paper, we present token labeling -- a new training objective for
training high-performance vision transformers (ViTs). Different from the
standard training objective of ViTs that computes the classification loss on an
additional trainable class token, our proposed one takes advantage of all the
image patch tokens to compute the training loss in a dense manner.
Specifically, token labeling reformulates the image classification problem into
multiple token-level recognition problems and assigns each patch token with an
individual location-specific supervision generated by a machine annotator.
Experiments show that token labeling can clearly and consistently improve the
performance of various ViT models across a wide spectrum. For a vision
transformer with 26M learnable parameters serving as an example, with token
labeling, the model can achieve 84.4% Top-1 accuracy on ImageNet. The result
can be further increased to 86.4% by slightly scaling the model size up to
150M, delivering the minimal-sized model among previous models (250M+) reaching
86%. We also show that token labeling can clearly improve the generalization
capability of the pre-trained models on downstream tasks with dense prediction,
such as semantic segmentation. Our code and all the training details will be
made publicly available at https://github.com/zihangJiang/TokenLabeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zihang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qibin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Li Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Daquan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yujun Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xiaojie Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Anran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More than meets the eye: Self-supervised depth reconstruction from brain activity. (arXiv:2106.05113v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05113</id>
        <link href="http://arxiv.org/abs/2106.05113"/>
        <updated>2021-06-10T01:56:46.494Z</updated>
        <summary type="html"><![CDATA[In the past few years, significant advancements were made in reconstruction
of observed natural images from fMRI brain recordings using deep-learning
tools. Here, for the first time, we show that dense 3D depth maps of observed
2D natural images can also be recovered directly from fMRI brain recordings. We
use an off-the-shelf method to estimate the unknown depth maps of natural
images. This is applied to both: (i) the small number of images presented to
subjects in an fMRI scanner (images for which we have fMRI recordings -
referred to as "paired" data), and (ii) a very large number of natural images
with no fMRI recordings ("unpaired data"). The estimated depth maps are then
used as an auxiliary reconstruction criterion to train for depth reconstruction
directly from fMRI. We propose two main approaches: Depth-only recovery and
joint image-depth RGBD recovery. Because the number of available "paired"
training data (images with fMRI) is small, we enrich the training data via
self-supervised cycle-consistent training on many "unpaired" data (natural
images & depth maps without fMRI). This is achieved using our newly defined and
trained Depth-based Perceptual Similarity metric as a reconstruction criterion.
We show that predicting the depth map directly from fMRI outperforms its
indirect sequential recovery from the reconstructed images. We further show
that activations from early cortical visual areas dominate our depth
reconstruction results, and propose means to characterize fMRI voxels by their
degree of depth-information tuning. This work adds an important layer of
decoded information, extending the current envelope of visual brain decoding
capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gaziv_G/0/1/0/all/0/1"&gt;Guy Gaziv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1"&gt;Michal Irani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Application of Deep Learning in Generating Desired Design Options: Experiments Using Synthetic Training Dataset. (arXiv:2001.05849v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.05849</id>
        <link href="http://arxiv.org/abs/2001.05849"/>
        <updated>2021-06-10T01:56:46.487Z</updated>
        <summary type="html"><![CDATA[Most design methods contain a forward framework, asking for primary
specifications of a building to generate an output or assess its performance.
However, architects urge for specific objectives though uncertain of the proper
design parameters. Deep Learning (DL) algorithms provide an intelligent
workflow in which the system can learn from sequential training experiments.
This study applies a method using DL algorithms towards generating demanded
design options. In this study, an object recognition problem is investigated to
initially predict the label of unseen sample images based on training dataset
consisting of different types of synthetic 2D shapes; later, a generative DL
algorithm is applied to be trained and generate new shapes for given labels. In
the next step, the algorithm is trained to generate a window/wall pattern for
desired light/shadow performance based on the spatial daylight autonomy (sDA)
metrics. The experiments show promising results both in predicting unseen
sample shapes and generating new design options.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaghaghian_Z/0/1/0/all/0/1"&gt;Zohreh Shaghaghian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1"&gt;Wei Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Modal Contrastive Learning for Text-to-Image Generation. (arXiv:2101.04702v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04702</id>
        <link href="http://arxiv.org/abs/2101.04702"/>
        <updated>2021-06-10T01:56:46.481Z</updated>
        <summary type="html"><![CDATA[The output of text-to-image synthesis systems should be coherent, clear,
photo-realistic scenes with high semantic fidelity to their conditioned text
descriptions. Our Cross-Modal Contrastive Generative Adversarial Network
(XMC-GAN) addresses this challenge by maximizing the mutual information between
image and text. It does this via multiple contrastive losses which capture
inter-modality and intra-modality correspondences. XMC-GAN uses an attentional
self-modulation generator, which enforces strong text-image correspondence, and
a contrastive discriminator, which acts as a critic as well as a feature
encoder for contrastive learning. The quality of XMC-GAN's output is a major
step up from previous models, as we show on three challenging datasets. On
MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33,
but--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1
for image-text alignment, compared to three other recent models. XMC-GAN also
generalizes to the challenging Localized Narratives dataset (which has longer,
more detailed descriptions), improving state-of-the-art FID from 48.70 to
14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images
data, establishing a strong benchmark FID score of 26.91.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1"&gt;Jing Yu Koh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1"&gt;Jason Baldridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinfei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Salient Object Ranking with Position-Preserved Attention. (arXiv:2106.05047v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05047</id>
        <link href="http://arxiv.org/abs/2106.05047"/>
        <updated>2021-06-10T01:56:46.473Z</updated>
        <summary type="html"><![CDATA[Instance segmentation can detect where the objects are in an image, but hard
to understand the relationship between them. We pay attention to a typical
relationship, relative saliency. A closely related task, salient object
detection, predicts a binary map highlighting a visually salient region while
hard to distinguish multiple objects. Directly combining two tasks by
post-processing also leads to poor performance. There is a lack of research on
relative saliency at present, limiting the practical applications such as
content-aware image cropping, video summary, and image labeling.

In this paper, we study the Salient Object Ranking (SOR) task, which manages
to assign a ranking order of each detected object according to its visual
saliency. We propose the first end-to-end framework of the SOR task and solve
it in a multi-task learning fashion. The framework handles instance
segmentation and salient object ranking simultaneously. In this framework, the
SOR branch is independent and flexible to cooperate with different detection
methods, so that easy to use as a plugin. We also introduce a
Position-Preserved Attention (PPA) module tailored for the SOR branch. It
consists of the position embedding stage and feature interaction stage.
Considering the importance of position in saliency comparison, we preserve
absolute coordinates of objects in ROI pooling operation and then fuse
positional information with semantic features in the first stage. In the
feature interaction stage, we apply the attention mechanism to obtain
proposals' contextualized representations to predict their relative ranking
orders. Extensive experiments have been conducted on the ASR dataset. Without
bells and whistles, our proposed method outperforms the former state-of-the-art
method significantly. The code will be released publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1"&gt;Hao Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Daoxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiawei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofei He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer in Convolutional Neural Networks. (arXiv:2106.03180v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03180</id>
        <link href="http://arxiv.org/abs/2106.03180"/>
        <updated>2021-06-10T01:56:46.466Z</updated>
        <summary type="html"><![CDATA[We tackle the low-efficiency flaw of vision transformer caused by the high
computational/space complexity in Multi-Head Self-Attention (MHSA). To this
end, we propose the Hierarchical MHSA (H-MHSA), whose representation is
computed in a hierarchical manner. Specifically, our H-MHSA first learns
feature relationships within small grids by viewing image patches as tokens.
Then, small grids are merged into larger ones, within which feature
relationship is learned by viewing each small grid at the preceding step as a
token. This process is iterated to gradually reduce the number of tokens. The
H-MHSA module is readily pluggable into any CNN architectures and amenable to
training via backpropagation. We call this new backbone TransCNN, and it
essentially inherits the advantages of both transformer and CNN. Experiments
demonstrate that TransCNN achieves state-of-the-art accuracy for image
recognition. Code and pretrained models are available at
https://github.com/yun-liu/TransCNN. This technical report will keep updating
by adding more experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guolei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yu Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Le Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1"&gt;Ajad Chhatkuli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XIRL: Cross-embodiment Inverse Reinforcement Learning. (arXiv:2106.03911v1 [cs.RO] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.03911</id>
        <link href="http://arxiv.org/abs/2106.03911"/>
        <updated>2021-06-10T01:56:46.449Z</updated>
        <summary type="html"><![CDATA[We investigate the visual cross-embodiment imitation setting, in which agents
learn policies from videos of other agents (such as humans) demonstrating the
same task, but with stark differences in their embodiments -- shape, actions,
end-effector dynamics, etc. In this work, we demonstrate that it is possible to
automatically discover and learn vision-based reward functions from
cross-embodiment demonstration videos that are robust to these differences.
Specifically, we present a self-supervised method for Cross-embodiment Inverse
Reinforcement Learning (XIRL) that leverages temporal cycle-consistency
constraints to learn deep visual embeddings that capture task progression from
offline videos of demonstrations across multiple expert agents, each performing
the same task differently due to embodiment differences. Prior to our work,
producing rewards from self-supervised embeddings has typically required
alignment with a reference trajectory, which may be difficult to acquire. We
show empirically that if the embeddings are aware of task-progress, simply
taking the negative distance between the current state and goal state in the
learned embedding space is useful as a reward for training policies with
reinforcement learning. We find our learned reward function not only works for
embodiments seen during training, but also generalizes to entirely new
embodiments. We also find that XIRL policies are more sample efficient than
baselines, and in some cases exceed the sample efficiency of the same agent
trained with ground truth sparse rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zakka_K/0/1/0/all/0/1"&gt;Kevin Zakka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Andy Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1"&gt;Pete Florence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1"&gt;Jonathan Tompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1"&gt;Jeannette Bohg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwibedi_D/0/1/0/all/0/1"&gt;Debidatta Dwibedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Programmable 3D snapshot microscopy with Fourier convolutional networks. (arXiv:2104.10611v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10611</id>
        <link href="http://arxiv.org/abs/2104.10611"/>
        <updated>2021-06-10T01:56:46.444Z</updated>
        <summary type="html"><![CDATA[3D snapshot microscopy enables fast volumetric imaging by capturing a 3D
volume in a single 2D camera image, and has found a variety of biological
applications such as whole brain imaging of fast neural activity in larval
zebrafish. The optimal microscope design for this optical 3D-to-2D encoding is
both sample- and task-dependent, with no general solution known. Highly
programmable optical elements create new possibilities for sample-specific
computational optimization of microscope parameters, e.g. tuning the collection
of light for a given sample structure. We perform such optimization with deep
learning, using a differentiable wave-optics simulation of light propagation
through a programmable microscope and a neural network to reconstruct volumes
from the microscope image. We introduce a class of global kernel Fourier
convolutional neural networks which can efficiently decode information from
multiple depths in the volume, globally encoded across a 3D snapshot image. We
show that our proposed networks succeed in large field of view volume
reconstruction and microscope parameter optimization where traditional networks
fail. We also show that our networks outperform the state-of-the-art learned
reconstruction algorithms for lensless computational photography.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Deb_D/0/1/0/all/0/1"&gt;Diptodip Deb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiao_Z/0/1/0/all/0/1"&gt;Zhenfei Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1"&gt;Alex B. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahrens_M/0/1/0/all/0/1"&gt;Misha B. Ahrens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Podgorski_K/0/1/0/all/0/1"&gt;Kaspar Podgorski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turaga_S/0/1/0/all/0/1"&gt;Srinivas C. Turaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Agile wide-field imaging with selective high resolution. (arXiv:2106.05082v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05082</id>
        <link href="http://arxiv.org/abs/2106.05082"/>
        <updated>2021-06-10T01:56:46.438Z</updated>
        <summary type="html"><![CDATA[Wide-field and high-resolution (HR) imaging is essential for various
applications such as aviation reconnaissance, topographic mapping and safety
monitoring. The existing techniques require a large-scale detector array to
capture HR images of the whole field, resulting in high complexity and heavy
cost. In this work, we report an agile wide-field imaging framework with
selective high resolution that requires only two detectors. It builds on the
statistical sparsity prior of natural scenes that the important targets locate
only at small regions of interests (ROI), instead of the whole field. Under
this assumption, we use a short-focal camera to image wide field with a certain
low resolution, and use a long-focal camera to acquire the HR images of ROI. To
automatically locate ROI in the wide field in real time, we propose an
efficient deep-learning based multiscale registration method that is robust and
blind to the large setting differences (focal, white balance, etc) between the
two cameras. Using the registered location, the long-focal camera mounted on a
gimbal enables real-time tracking of the ROI for continuous HR imaging. We
demonstrated the novel imaging framework by building a proof-of-concept setup
with only 1181 gram weight, and assembled it on an unmanned aerial vehicle for
air-to-ground monitoring. Experiments show that the setup maintains
120$^{\circ}$ wide field-of-view (FOV) with selective 0.45$mrad$ instantaneous
FOV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1"&gt;Lintao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_L/0/1/0/all/0/1"&gt;Liheng Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tiexin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Modality Vehicle Anomaly Detection via Bilateral Trajectory Tracing. (arXiv:2106.05003v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05003</id>
        <link href="http://arxiv.org/abs/2106.05003"/>
        <updated>2021-06-10T01:56:46.432Z</updated>
        <summary type="html"><![CDATA[Traffic anomaly detection has played a crucial role in Intelligent
Transportation System (ITS). The main challenges of this task lie in the highly
diversified anomaly scenes and variational lighting conditions. Although much
work has managed to identify the anomaly in homogenous weather and scene, few
resolved to cope with complex ones. In this paper, we proposed a dual-modality
modularized methodology for the robust detection of abnormal vehicles. We
introduced an integrated anomaly detection framework comprising the following
modules: background modeling, vehicle tracking with detection, mask
construction, Region of Interest (ROI) backtracking, and dual-modality tracing.
Concretely, we employed background modeling to filter the motion information
and left the static information for later vehicle detection. For the vehicle
detection and tracking module, we adopted YOLOv5 and multi-scale tracking to
localize the anomalies. Besides, we utilized the frame difference and tracking
results to identify the road and obtain the mask. In addition, we introduced
multiple similarity estimation metrics to refine the anomaly period via
backtracking. Finally, we proposed a dual-modality bilateral tracing module to
refine the time further. The experiments conducted on the Track 4 testset of
the NVIDIA 2021 AI City Challenge yielded a result of 0.9302 F1-Score and
3.4039 root mean square error (RMSE), indicating the effectiveness of our
framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Guanchen Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuchen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1"&gt;Wenwei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kangmin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tianyi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanping Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenzhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Feature Enhancement: Applying Internal Pretext Task to Supervised Learning. (arXiv:2106.04921v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04921</id>
        <link href="http://arxiv.org/abs/2106.04921"/>
        <updated>2021-06-10T01:56:46.426Z</updated>
        <summary type="html"><![CDATA[Traditional self-supervised learning requires CNNs using external pretext
tasks (i.e., image- or video-based tasks) to encode high-level semantic visual
representations. In this paper, we show that feature transformations within
CNNs can also be regarded as supervisory signals to construct the
self-supervised task, called \emph{internal pretext task}. And such a task can
be applied for the enhancement of supervised learning. Specifically, we first
transform the internal feature maps by discarding different channels, and then
define an additional internal pretext task to identify the discarded channels.
CNNs are trained to predict the joint labels generated by the combination of
self-supervised labels and original labels. By doing so, we let CNNs know which
channels are missing while classifying in the hope to mine richer feature
information. Extensive experiments show that our approach is effective on
various models and datasets. And it's worth noting that we only incur
negligible computational overhead. Furthermore, our approach can also be
compatible with other methods to get better results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuhang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zilin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaomin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A machine learning pipeline for aiding school identification from child trafficking images. (arXiv:2106.05215v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05215</id>
        <link href="http://arxiv.org/abs/2106.05215"/>
        <updated>2021-06-10T01:56:46.410Z</updated>
        <summary type="html"><![CDATA[Child trafficking in a serious problem around the world. Every year there are
more than 4 million victims of child trafficking around the world, many of them
for the purposes of child sexual exploitation. In collaboration with UK Police
and a non-profit focused on child abuse prevention, Global Emancipation
Network, we developed a proof-of-concept machine learning pipeline to aid the
identification of children from intercepted images. In this work, we focus on
images that contain children wearing school uniforms to identify the school of
origin. In the absence of a machine learning pipeline, this hugely time
consuming and labor intensive task is manually conducted by law enforcement
personnel. Thus, by automating aspects of the school identification process, we
hope to significantly impact the speed of this portion of child identification.
Our proposed pipeline consists of two machine learning models: i) to identify
whether an image of a child contains a school uniform in it, and ii)
identification of attributes of different school uniform items (such as
color/texture of shirts, sweaters, blazers etc.). We describe the data
collection, labeling, model development and validation process, along with
strategies for efficient searching of schools using the model predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Sumit Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sederholm_T/0/1/0/all/0/1"&gt;Tina Sederholm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roman_A/0/1/0/all/0/1"&gt;Anthony C. Roman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankar_R/0/1/0/all/0/1"&gt;Ria Sankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caltagirone_S/0/1/0/all/0/1"&gt;Sherrie Caltagirone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real Time Egocentric Object Segmentation: THU-READ Labeling and Benchmarking Results. (arXiv:2106.04957v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04957</id>
        <link href="http://arxiv.org/abs/2106.04957"/>
        <updated>2021-06-10T01:56:46.405Z</updated>
        <summary type="html"><![CDATA[Egocentric segmentation has attracted recent interest in the computer vision
community due to their potential in Mixed Reality (MR) applications. While most
previous works have been focused on segmenting egocentric human body parts
(mainly hands), little attention has been given to egocentric objects. Due to
the lack of datasets of pixel-wise annotations of egocentric objects, in this
paper we contribute with a semantic-wise labeling of a subset of 2124 images
from the RGB-D THU-READ Dataset. We also report benchmarking results using
Thundernet, a real-time semantic segmentation network, that could allow future
integration with end-to-end MR applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Sosa_E/0/1/0/all/0/1"&gt;E. Gonzalez-Sosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robledo_G/0/1/0/all/0/1"&gt;G. Robledo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Morin_D/0/1/0/all/0/1"&gt;D. Gonzalez-Morin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Garcia_P/0/1/0/all/0/1"&gt;P. Perez-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_A/0/1/0/all/0/1"&gt;A. Villegas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A multi-stage GAN for multi-organ chest X-ray image generation and segmentation. (arXiv:2106.05132v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.05132</id>
        <link href="http://arxiv.org/abs/2106.05132"/>
        <updated>2021-06-10T01:56:46.399Z</updated>
        <summary type="html"><![CDATA[Multi-organ segmentation of X-ray images is of fundamental importance for
computer aided diagnosis systems. However, the most advanced semantic
segmentation methods rely on deep learning and require a huge amount of labeled
images, which are rarely available due to both the high cost of human resources
and the time required for labeling. In this paper, we present a novel
multi-stage generation algorithm based on Generative Adversarial Networks
(GANs) that can produce synthetic images along with their semantic labels and
can be used for data augmentation. The main feature of the method is that,
unlike other approaches, generation occurs in several stages, which simplifies
the procedure and allows it to be used on very small datasets. The method has
been evaluated on the segmentation of chest radiographic images, showing
promising results. The multistage approach achieves state-of-the-art and, when
very few images are used to train the GANs, outperforms the corresponding
single-stage approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ciano_G/0/1/0/all/0/1"&gt;Giorgio Ciano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Andreini_P/0/1/0/all/0/1"&gt;Paolo Andreini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mazzierli_T/0/1/0/all/0/1"&gt;Tommaso Mazzierli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bianchini_M/0/1/0/all/0/1"&gt;Monica Bianchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scarselli_F/0/1/0/all/0/1"&gt;Franco Scarselli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ST++: Make Self-training Work Better for Semi-supervised Semantic Segmentation. (arXiv:2106.05095v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05095</id>
        <link href="http://arxiv.org/abs/2106.05095"/>
        <updated>2021-06-10T01:56:46.394Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate if we could make the self-training -- a simple
but popular framework -- work better for semi-supervised segmentation. Since
the core issue in semi-supervised setting lies in effective and efficient
utilization of unlabeled data, we notice that increasing the diversity and
hardness of unlabeled data is crucial to performance improvement. Being aware
of this fact, we propose to adopt the most plain self-training scheme coupled
with appropriate strong data augmentations on unlabeled data (namely ST) for
this task, which surprisingly outperforms previous methods under various
settings without any bells and whistles. Moreover, to alleviate the negative
impact of the wrongly pseudo labeled images, we further propose an advanced
self-training framework (namely ST++), that performs selective re-training via
selecting and prioritizing the more reliable unlabeled images. As a result, the
proposed ST++ boosts the performance of semi-supervised model significantly and
surpasses existing methods by a large margin on the Pascal VOC 2012 and
Cityscapes benchmark. Overall, we hope this straightforward and simple
framework will serve as a strong baseline or competitor for future works. Code
is available at https://github.com/LiheYoung/ST-PlusPlus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lihe Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1"&gt;Wei Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yinghuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of convolutional neural network image classifiers in a hierarchical max-pooling model with additional local pooling. (arXiv:2106.05233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05233</id>
        <link href="http://arxiv.org/abs/2106.05233"/>
        <updated>2021-06-10T01:56:46.388Z</updated>
        <summary type="html"><![CDATA[Image classification is considered, and a hierarchical max-pooling model with
additional local pooling is introduced. Here the additional local pooling
enables the hierachical model to combine parts of the image which have a
variable relative distance towards each other. Various convolutional neural
network image classifiers are introduced and compared in view of their rate of
convergence. The finite sample size performance of the estimates is analyzed by
applying them to simulated and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Walter_B/0/1/0/all/0/1"&gt;Benjamin Walter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Rank Words: Optimizing Ranking Metrics for Word Spotting. (arXiv:2106.05144v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05144</id>
        <link href="http://arxiv.org/abs/2106.05144"/>
        <updated>2021-06-10T01:56:46.373Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore and evaluate the use of ranking-based objective
functions for learning simultaneously a word string and a word image encoder.
We consider retrieval frameworks in which the user expects a retrieval list
ranked according to a defined relevance score. In the context of a word
spotting problem, the relevance score has been set according to the string edit
distance from the query string. We experimentally demonstrate the competitive
performance of the proposed model on query-by-string word spotting for both,
handwritten and real scene word images. We also provide the results for
query-by-example word spotting, although it is not the main focus of this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1"&gt;Pau Riba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1"&gt;Adri&amp;#xe0; Molina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1"&gt;Lluis Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramos_Terrades_O/0/1/0/all/0/1"&gt;Oriol Ramos-Terrades&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1"&gt;Josep Llad&amp;#xf3;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling Image Classifiers in Object Detectors. (arXiv:2106.05209v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05209</id>
        <link href="http://arxiv.org/abs/2106.05209"/>
        <updated>2021-06-10T01:56:46.366Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation constitutes a simple yet effective way to improve the
performance of a compact student network by exploiting the knowledge of a more
powerful teacher. Nevertheless, the knowledge distillation literature remains
limited to the scenario where the student and the teacher tackle the same task.
Here, we investigate the problem of transferring knowledge not only across
architectures but also across tasks. To this end, we study the case of object
detection and, instead of following the standard detector-to-detector
distillation approach, introduce a classifier-to-detector knowledge transfer
framework. In particular, we propose strategies to exploit the classification
teacher to improve both the detector's recognition accuracy and localization
performance. Our experiments on several detectors with different backbones
demonstrate the effectiveness of our approach, allowing us to outperform the
state-of-the-art detector-to-detector distillation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shuxuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Defending against Adversarial Examples via Attack-Invariant Features. (arXiv:2106.05036v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05036</id>
        <link href="http://arxiv.org/abs/2106.05036"/>
        <updated>2021-06-10T01:56:46.358Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are vulnerable to adversarial noise. Their
adversarial robustness can be improved by exploiting adversarial examples.
However, given the continuously evolving attacks, models trained on seen types
of adversarial examples generally cannot generalize well to unseen types of
adversarial examples. To solve this problem, in this paper, we propose to
remove adversarial noise by learning generalizable invariant features across
attacks which maintain semantic classification information. Specifically, we
introduce an adversarial feature learning mechanism to disentangle invariant
features from adversarial noise. A normalization term has been proposed in the
encoded space of the attack-invariant features to address the bias issue
between the seen and unseen types of attacks. Empirical evaluations demonstrate
that our method could provide better protection in comparison to previous
state-of-the-art approaches, especially against unseen types of attacks and
adaptive attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dawei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Nannan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chunlei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xinbo Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised lane detection with Deep Hough Transform. (arXiv:2106.05094v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05094</id>
        <link href="http://arxiv.org/abs/2106.05094"/>
        <updated>2021-06-10T01:56:46.352Z</updated>
        <summary type="html"><![CDATA[Current work on lane detection relies on large manually annotated datasets.
We reduce the dependency on annotations by leveraging massive cheaply available
unlabelled data. We propose a novel loss function exploiting geometric
knowledge of lanes in Hough space, where a lane can be identified as a local
maximum. By splitting lanes into separate channels, we can localize each lane
via simple global max-pooling. The location of the maximum encodes the layout
of a lane, while the intensity indicates the the probability of a lane being
present. Maximizing the log-probability of the maximal bins helps neural
networks find lanes without labels. On the CULane and TuSimple datasets, we
show that the proposed Hough Transform loss improves performance significantly
by learning from large amounts of unlabelled images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yancong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1"&gt;Silvia-Laura Pintea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic CT Segmentation from Bounding Box Annotations using Convolutional Neural Networks. (arXiv:2105.14314v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14314</id>
        <link href="http://arxiv.org/abs/2105.14314"/>
        <updated>2021-06-10T01:56:46.347Z</updated>
        <summary type="html"><![CDATA[Accurate segmentation for medical images is important for clinical diagnosis.
Existing automatic segmentation methods are mainly based on fully supervised
learning and have an extremely high demand for precise annotations, which are
very costly and time-consuming to obtain. To address this problem, we proposed
an automatic CT segmentation method based on weakly supervised learning, by
which one could train an accurate segmentation model only with weak annotations
in the form of bounding boxes. The proposed method is composed of two steps: 1)
generating pseudo masks with bounding box annotations by k-means clustering,
and 2) iteratively training a 3D U-Net convolutional neural network as a
segmentation model. Some data pre-processing methods are used to improve
performance. The method was validated on four datasets containing three types
of organs with a total of 627 CT volumes. For liver, spleen and kidney
segmentation, it achieved an accuracy of 95.19%, 92.11%, and 91.45%,
respectively. Experimental results demonstrate that our method is accurate,
efficient, and suitable for clinical use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanpeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_Q/0/1/0/all/0/1"&gt;Qinglei Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhiyi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1"&gt;Shaolin Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1"&gt;Dexing Kong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethink Transfer Learning in Medical Image Classification. (arXiv:2106.05152v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.05152</id>
        <link href="http://arxiv.org/abs/2106.05152"/>
        <updated>2021-06-10T01:56:46.341Z</updated>
        <summary type="html"><![CDATA[Transfer learning (TL) with deep convolutional neural networks (DCNNs) has
proved successful in medical image classification (MIC). However, the current
practice is puzzling, as MIC typically relies only on low- and/or mid-level
features that are learned in the bottom layers of DCNNs. Following this
intuition, we question the current strategies of TL in MIC. In this paper, we
perform careful experimental comparisons between shallow and deep networks for
classification on two chest x-ray datasets, using different TL strategies. We
find that deep models are not always favorable, and finetuning truncated deep
models almost always yields the best performance, especially in data-poor
regimes.

Project webpage:
https://github.com/sun-umn/Transfer-Learning-in-Medical-Imaging

Keywords: Transfer learning, Medical image classification, Feature hierarchy,
Medical imaging, Evaluation metrics, Imbalanced data]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1"&gt;Le Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hengyue Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Taihui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An ordinal CNN approach for the assessment of neurological damage in Parkinson's disease patients. (arXiv:2106.05230v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05230</id>
        <link href="http://arxiv.org/abs/2106.05230"/>
        <updated>2021-06-10T01:56:46.324Z</updated>
        <summary type="html"><![CDATA[3D image scans are an assessment tool for neurological damage in Parkinson's
disease (PD) patients. This diagnosis process can be automatized to help
medical staff through Decision Support Systems (DSSs), and Convolutional Neural
Networks (CNNs) are good candidates, because they are effective when applied to
spatial data. This paper proposes a 3D CNN ordinal model for assessing the
level or neurological damage in PD patients. Given that CNNs need large
datasets to achieve acceptable performance, a data augmentation method is
adapted to work with spatial data. We consider the Ordinal Graph-based
Oversampling via Shortest Paths (OGO-SP) method, which applies a gamma
probability distribution for inter-class data generation. A modification of
OGO-SP is proposed, the OGO-SP-$\beta$ algorithm, which applies the beta
distribution for generating synthetic samples in the inter-class region, a
better suited distribution when compared to gamma. The evaluation of the
different methods is based on a novel 3D image dataset provided by the Hospital
Universitario 'Reina Sof\'ia' (C\'ordoba, Spain). We show how the ordinal
methodology improves the performance with respect to the nominal one, and how
OGO-SP-$\beta$ yields better performance than OGO-SP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbero_Gomez_J/0/1/0/all/0/1"&gt;Javier Barbero-G&amp;#xf3;mez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_P/0/1/0/all/0/1"&gt;Pedro-Antonio Guti&amp;#xe9;rrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vargas_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor-Manuel Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vallejo_Casas_J/0/1/0/all/0/1"&gt;Juan-Antonio Vallejo-Casas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hervas_Martinez_C/0/1/0/all/0/1"&gt;C&amp;#xe9;sar Herv&amp;#xe1;s-Mart&amp;#xed;nez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge distillation: A good teacher is patient and consistent. (arXiv:2106.05237v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05237</id>
        <link href="http://arxiv.org/abs/2106.05237"/>
        <updated>2021-06-10T01:56:46.293Z</updated>
        <summary type="html"><![CDATA[There is a growing discrepancy in computer vision between large-scale models
that achieve state-of-the-art performance and models that are affordable in
practical applications. In this paper we address this issue and significantly
bridge the gap between these two types of models. Throughout our empirical
investigation we do not aim to necessarily propose a new method, but strive to
identify a robust and effective recipe for making state-of-the-art large scale
models affordable in practice. We demonstrate that, when performed correctly,
knowledge distillation can be a powerful tool for reducing the size of large
models without compromising their performance. In particular, we uncover that
there are certain implicit design choices, which may drastically affect the
effectiveness of distillation. Our key contribution is the explicit
identification of these design choices, which were not previously articulated
in the literature. We back up our findings by a comprehensive empirical study,
demonstrate compelling results on a wide range of vision datasets and, in
particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which
achieves 82.8\% top-1 accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Royer_A/0/1/0/all/0/1"&gt;Am&amp;#xe9;lie Royer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1"&gt;Larisa Markeeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1"&gt;Rohan Anil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Salient Positions based Attention Network for Image Classification. (arXiv:2106.04996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04996</id>
        <link href="http://arxiv.org/abs/2106.04996"/>
        <updated>2021-06-10T01:56:46.285Z</updated>
        <summary type="html"><![CDATA[The self-attention mechanism has attracted wide publicity for its most
important advantage of modeling long dependency, and its variations in computer
vision tasks, the non-local block tries to model the global dependency of the
input feature maps. Gathering global contextual information will inevitably
need a tremendous amount of memory and computing resources, which has been
extensively studied in the past several years. However, there is a further
problem with the self-attention scheme: is all information gathered from the
global scope helpful for the contextual modelling? To our knowledge, few
studies have focused on the problem. Aimed at both questions this paper
proposes the salient positions-based attention scheme SPANet, which is inspired
by some interesting observations on the attention maps and affinity matrices
generated in self-attention scheme. We believe these observations are
beneficial for better understanding of the self-attention. SPANet uses the
salient positions selection algorithm to select only a limited amount of
salient points to attend in the attention map computing. This approach will not
only spare a lot of memory and computing resources, but also try to distill the
positive information from the transformation of the input feature maps. In the
implementation, considering the feature maps with channel high dimensions,
which are completely different from the general visual image, we take the
squared power of the feature maps along the channel dimension as the saliency
metric of the positions. In general, different from the non-local block method,
SPANet models the contextual information using only the selected positions
instead of all, along the channel dimension instead of space dimension. Our
source code is available at https://github.com/likyoo/SPANet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Sheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kaiyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhe Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding inductive biases in natural images:invariance stems from variations in data. (arXiv:2106.05121v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05121</id>
        <link href="http://arxiv.org/abs/2106.05121"/>
        <updated>2021-06-10T01:56:46.277Z</updated>
        <summary type="html"><![CDATA[To perform well on unseen and potentially out-of-distribution samples, it is
desirable for machine learning models to have a predictable response with
respect to transformations affecting the factors of variation of the input.
Invariance is commonly achieved through hand-engineered data augmentation, but
do standard data augmentations address transformations that explain variations
in real data? While prior work has focused on synthetic data, we attempt here
to characterize the factors of variation in a real dataset, ImageNet, and study
the invariance of both standard residual networks and the recently proposed
vision transformer with respect to changes in these factors. We show standard
augmentation relies on a precise combination of translation and scale, with
translation recapturing most of the performance improvement -- despite the
(approximate) translation invariance built in to convolutional architectures,
such as residual networks. In fact, we found that scale and translation
invariance was similar across residual networks and vision transformer models
despite their markedly different inductive biases. We show the training data
itself is the main source of invariance, and that data augmentation only
further increases the learned invariances. Interestingly, the invariances
brought from the training process align with the ImageNet factors of variation
we found. Finally, we find that the main factors of variation in ImageNet
mostly relate to appearance and are specific to each class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bouchacourt_D/0/1/0/all/0/1"&gt;Diane Bouchacourt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1"&gt;Mark Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1"&gt;Ari S. Morcos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data. (arXiv:2106.05001v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05001</id>
        <link href="http://arxiv.org/abs/2106.05001"/>
        <updated>2021-06-10T01:56:46.259Z</updated>
        <summary type="html"><![CDATA[A central challenge in training classification models in the real-world
federated system is learning with non-IID data. To cope with this, most of the
existing works involve enforcing regularization in local optimization or
improving the model aggregation scheme at the server. Other works also share
public datasets or synthesized samples to supplement the training of
under-represented classes or introduce a certain level of personalization.
Though effective, they lack a deep understanding of how the data heterogeneity
affects each layer of a deep classification model. In this paper, we bridge
this gap by performing an experimental analysis of the representations learned
by different layers. Our observations are surprising: (1) there exists a
greater bias in the classifier than other layers, and (2) the classification
performance can be significantly improved by post-calibrating the classifier
after federated training. Motivated by the above findings, we propose a novel
and simple algorithm called Classifier Calibration with Virtual Representations
(CCVR), which adjusts the classifier using virtual representations sampled from
an approximated gaussian mixture model. Experimental results demonstrate that
CCVR achieves state-of-the-art performance on popular federated learning
benchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple
yet effective method can shed some light on the future research of federated
learning with non-IID data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Mi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dapeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation. (arXiv:2106.04732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04732</id>
        <link href="http://arxiv.org/abs/2106.04732"/>
        <updated>2021-06-10T01:56:46.254Z</updated>
        <summary type="html"><![CDATA[We extend semi-supervised learning to the problem of domain adaptation to
learn significantly higher-accuracy models that train on one data distribution
and test on a different one. With the goal of generality, we introduce
AdaMatch, a method that unifies the tasks of unsupervised domain adaptation
(UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation
(SSDA). In an extensive experimental study, we compare its behavior with
respective state-of-the-art techniques from SSL, SSDA, and UDA on vision
classification tasks. We find AdaMatch either matches or significantly exceeds
the state-of-the-art in each case using the same hyper-parameters regardless of
the dataset or task. For example, AdaMatch nearly doubles the accuracy compared
to that of the prior state-of-the-art on the UDA task for DomainNet and even
exceeds the accuracy of the prior state-of-the-art obtained with pre-training
by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by
providing AdaMatch with just one labeled example per class from the target
domain (i.e., the SSDA setting), we increase the target accuracy by an
additional 6.1%, and with 5 labeled examples, by 13.6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berthelot_D/0/1/0/all/0/1"&gt;David Berthelot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1"&gt;Rebecca Roelofs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1"&gt;Alex Kurakin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracking by Joint Local and Global Search: A Target-aware Attention based Approach. (arXiv:2106.04840v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04840</id>
        <link href="http://arxiv.org/abs/2106.04840"/>
        <updated>2021-06-10T01:56:46.238Z</updated>
        <summary type="html"><![CDATA[Tracking-by-detection is a very popular framework for single object tracking
which attempts to search the target object within a local search window for
each frame. Although such local search mechanism works well on simple videos,
however, it makes the trackers sensitive to extremely challenging scenarios,
such as heavy occlusion and fast motion. In this paper, we propose a novel and
general target-aware attention mechanism (termed TANet) and integrate it with
tracking-by-detection framework to conduct joint local and global search for
robust tracking. Specifically, we extract the features of target object patch
and continuous video frames, then we concatenate and feed them into a decoder
network to generate target-aware global attention maps. More importantly, we
resort to adversarial training for better attention prediction. The appearance
and motion discriminator networks are designed to ensure its consistency in
spatial and temporal views. In the tracking procedure, we integrate the
target-aware attention with multiple trackers by exploring candidate search
regions for robust tracking. Extensive experiments on both short-term and
long-term tracking benchmark datasets all validated the effectiveness of our
algorithm. The project page of this paper can be found at
\url{https://sites.google.com/view/globalattentiontracking/home/extend}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1"&gt;Bin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaowei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-discrete multiple target tracking with out-of-sequence measurements. (arXiv:2106.04898v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.04898</id>
        <link href="http://arxiv.org/abs/2106.04898"/>
        <updated>2021-06-10T01:56:46.228Z</updated>
        <summary type="html"><![CDATA[This paper derives the optimal Bayesian processing of an out-of-sequence
(OOS) set of measurements in continuous-time for multiple target tracking. We
consider a multi-target system modelled in continuous time that is discretised
at the time steps when we receive the measurements, which are distributed
according to the standard point target model. All information about this system
at the sampled time steps is provided by the posterior density on the set of
all trajectories. This density can be computed via the continuous-discrete
trajectory Poisson multi-Bernoulli mixture (TPMBM) filter. When we receive an
OOS measurement, the optimal Bayesian processing performs a retrodiction step
that adds trajectory information at the OOS measurement time stamp followed by
an update step. After the OOS measurement update, the posterior remains in
TPMBM form. We also provide a computationally lighter alternative based on a
trajectory Poisson multi-Bernoulli filter. The effectiveness of the two
approaches to handle OOS measurements is evaluated via simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_Fernandez_A/0/1/0/all/0/1"&gt;&amp;#xc1;ngel F. Garc&amp;#xed;a-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yi_W/0/1/0/all/0/1"&gt;Wei Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It Takes Two to Tango: Mixup for Deep Metric Learning. (arXiv:2106.04990v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04990</id>
        <link href="http://arxiv.org/abs/2106.04990"/>
        <updated>2021-06-10T01:56:46.222Z</updated>
        <summary type="html"><![CDATA[Metric learning involves learning a discriminative representation such that
embeddings of similar classes are encouraged to be close, while embeddings of
dissimilar classes are pushed far apart. State-of-the-art methods focus mostly
on sophisticated loss functions or mining strategies. On the one hand, metric
learning losses consider two or more examples at a time. On the other hand,
modern data augmentation methods for classification consider two or more
examples at a time. The combination of the two ideas is under-studied.

In this work, we aim to bridge this gap and improve representations using
mixup, which is a powerful data augmentation approach interpolating two or more
examples and corresponding target labels at a time. This task is challenging
because, unlike classification, the loss functions used in metric learning are
not additive over examples, so the idea of interpolating target labels is not
straightforward. To the best of our knowledge, we are the first to investigate
mixing examples and target labels for deep metric learning. We develop a
generalized formulation that encompasses existing metric learning loss
functions and modify it to accommodate for mixup, introducing Metric Mix, or
Metrix. We show that mixing inputs, intermediate representations or embeddings
along with target labels significantly improves representations and outperforms
state-of-the-art metric learning methods on four benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venkataramanan_S/0/1/0/all/0/1"&gt;Shashanka Venkataramanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Psomas_B/0/1/0/all/0/1"&gt;Bill Psomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1"&gt;Yannis Avrithis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1"&gt;Ewa Kijak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amsaleg_L/0/1/0/all/0/1"&gt;Laurent Amsaleg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karantzalos_K/0/1/0/all/0/1"&gt;Konstantinos Karantzalos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Learned Symmetries in Group Equivariant Convolutions. (arXiv:2106.04914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04914</id>
        <link href="http://arxiv.org/abs/2106.04914"/>
        <updated>2021-06-10T01:56:46.215Z</updated>
        <summary type="html"><![CDATA[Group Equivariant Convolutions (GConvs) enable convolutional neural networks
to be equivariant to various transformation groups, but at an additional
parameter and compute cost. We investigate the filter parameters learned by
GConvs and find certain conditions under which they become highly redundant. We
show that GConvs can be efficiently decomposed into depthwise separable
convolutions while preserving equivariance properties and demonstrate improved
performance and data efficiency on two datasets. All code is publicly available
at github.com/Attila94/SepGrouPy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lengyel_A/0/1/0/all/0/1"&gt;Attila Lengyel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan C. van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHARP: Shape-Aware Reconstruction of People In Loose Clothing. (arXiv:2106.04778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04778</id>
        <link href="http://arxiv.org/abs/2106.04778"/>
        <updated>2021-06-10T01:56:46.198Z</updated>
        <summary type="html"><![CDATA[3D human body reconstruction from monocular images is an interesting and
ill-posed problem in computer vision with wider applications in multiple
domains. In this paper, we propose SHARP, a novel end-to-end trainable network
that accurately recovers the detailed geometry and appearance of 3D people in
loose clothing from a monocular image. We propose a sparse and efficient fusion
of a parametric body prior with a non-parametric peeled depth map
representation of clothed models. The parametric body prior constraints our
model in two ways: first, the network retains geometrically consistent body
parts that are not occluded by clothing, and second, it provides a body shape
context that improves prediction of the peeled depth maps. This enables SHARP
to recover fine-grained 3D geometrical details with just L1 losses on the 2D
maps, given an input image. We evaluate SHARP on publicly available Cloth3D and
THuman datasets and report superior performance to state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jinka_S/0/1/0/all/0/1"&gt;Sai Sagar Jinka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chacko_R/0/1/0/all/0/1"&gt;Rohan Chacko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Astitva Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Avinash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1"&gt;P.J. Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Dual-Stream Neural Network for Sequential Whole-Body PET Segmentation. (arXiv:2106.04961v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04961</id>
        <link href="http://arxiv.org/abs/2106.04961"/>
        <updated>2021-06-10T01:56:46.192Z</updated>
        <summary type="html"><![CDATA[Sequential whole-body 18F-Fluorodeoxyglucose (FDG) positron emission
tomography (PET) scans are regarded as the imaging modality of choice for the
assessment of treatment response in the lymphomas because they detect treatment
response when there may not be changes on anatomical imaging. Any computerized
analysis of lymphomas in whole-body PET requires automatic segmentation of the
studies so that sites of disease can be quantitatively monitored over time.
State-of-the-art PET image segmentation methods are based on convolutional
neural networks (CNNs) given their ability to leverage annotated datasets to
derive high-level features about the disease process. Such methods, however,
focus on PET images from a single time-point and discard information from other
scans or are targeted towards specific organs and cannot cater for the multiple
structures in whole-body PET images. In this study, we propose a
spatio-temporal 'dual-stream' neural network (ST-DSNN) to segment sequential
whole-body PET scans. Our ST-DSNN learns and accumulates image features from
the PET images done over time. The accumulated image features are used to
enhance the organs / structures that are consistent over time to allow easier
identification of sites of active lymphoma. Our results show that our method
outperforms the state-of-the-art PET image segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kai-Chieh Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1"&gt;Lei Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ashnil Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fulham_M/0/1/0/all/0/1"&gt;Michael Fulham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinman Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoAtNet: Marrying Convolution and Attention for All Data Sizes. (arXiv:2106.04803v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04803</id>
        <link href="http://arxiv.org/abs/2106.04803"/>
        <updated>2021-06-10T01:56:46.027Z</updated>
        <summary type="html"><![CDATA[Transformers have attracted increasing interests in computer vision, but they
still fall behind state-of-the-art convolutional networks. In this work, we
show that while Transformers tend to have larger model capacity, their
generalization can be worse than convolutional networks due to the lack of the
right inductive bias. To effectively combine the strengths from both
architectures, we present CoAtNets(pronounced "coat" nets), a family of hybrid
models built from two key insights:(1) depthwise Convolution and self-Attention
can be naturally unified via simple relative attention; (2) vertically stacking
convolution layers and attention layers in a principled way is surprisingly
effective in improving generalization, capacity and efficiency. Experiments
show that our CoAtNets achieve state-of-the-art performance under different
resource constraints across various datasets. For example, CoAtNet achieves
86.0% ImageNet top-1 accuracy without extra data, and 89.77% with extra JFT
data, outperforming prior arts of both convolutional networks and Transformers.
Notably, when pre-trained with 13M images fromImageNet-21K, our CoAtNet
achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images
from JFT while using 23x less data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zihang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hanxiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingxing Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Interaction Networks with Rethinking Mechanism for Document-level Sentiment Analysis. (arXiv:2007.08445v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08445</id>
        <link href="http://arxiv.org/abs/2007.08445"/>
        <updated>2021-06-10T01:56:46.022Z</updated>
        <summary type="html"><![CDATA[Document-level Sentiment Analysis (DSA) is more challenging due to vague
semantic links and complicate sentiment information. Recent works have been
devoted to leveraging text summarization and have achieved promising results.
However, these summarization-based methods did not take full advantage of the
summary including ignoring the inherent interactions between the summary and
document. As a result, they limited the representation to express major points
in the document, which is highly indicative of the key sentiment. In this
paper, we study how to effectively generate a discriminative representation
with explicit subject patterns and sentiment contexts for DSA. A Hierarchical
Interaction Networks (HIN) is proposed to explore bidirectional interactions
between the summary and document at multiple granularities and learn
subject-oriented document representations for sentiment classification.
Furthermore, we design a Sentiment-based Rethinking mechanism (SR) by refining
the HIN with sentiment label information to learn a more sentiment-aware
document representation. We extensively evaluate our proposed models on three
public datasets. The experimental results consistently demonstrate the
effectiveness of our proposed models and show that HIN-SR outperforms various
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Lingwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dou Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xuehai Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaodan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jizhong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Songlin Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intent Detection and Slot Filling for Vietnamese. (arXiv:2104.02021v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02021</id>
        <link href="http://arxiv.org/abs/2104.02021"/>
        <updated>2021-06-10T01:56:46.005Z</updated>
        <summary type="html"><![CDATA[Intent detection and slot filling are important tasks in spoken and natural
language understanding. However, Vietnamese is a low-resource language in these
research topics. In this paper, we present the first public intent detection
and slot filling dataset for Vietnamese. In addition, we also propose a joint
model for intent detection and slot filling, that extends the recent
state-of-the-art JointBERT+CRF model with an intent-slot attention layer to
explicitly incorporate intent context information into slot filling via "soft"
intent label embedding. Experimental results on our Vietnamese dataset show
that our proposed model significantly outperforms JointBERT+CRF. We publicly
release our dataset and the implementation of our model at:
https://github.com/VinAIResearch/JointIDSF]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1"&gt;Mai Hoang Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Thinh Hung Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dat Quoc Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09632</id>
        <link href="http://arxiv.org/abs/2105.09632"/>
        <updated>2021-06-10T01:56:45.999Z</updated>
        <summary type="html"><![CDATA[Today, we are seeing an ever-increasing number of clinical notes that contain
clinical results, images, and textual descriptions of patient's health state.
All these data can be analyzed and employed to cater novel services that can
help people and domain experts with their common healthcare tasks. However,
many technologies such as Deep Learning and tools like Word Embeddings have
started to be investigated only recently, and many challenges remain open when
it comes to healthcare domain applications. To address these challenges, we
propose the use of Deep Learning and Word Embeddings for identifying sixteen
morbidity types within textual descriptions of clinical records. For this
purpose, we have used a Deep Learning model based on Bidirectional Long-Short
Term Memory (LSTM) layers which can exploit state-of-the-art vector
representations of data such as Word Embeddings. We have employed pre-trained
Word Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained
on the target domain. Furthermore, we have compared the performances of the
deep learning approaches against the traditional tf-idf using Support Vector
Machine and Multilayer perceptron (our baselines). From the obtained results it
seems that the latter outperforms the combination of Deep Learning approaches
using any word embeddings. Our preliminary results indicate that there are
specific features that make the dataset biased in favour of traditional machine
learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1"&gt;Danilo Dessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1"&gt;Rim Helaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vivek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1"&gt;Diego Reforgiato Recupero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1"&gt;Daniele Riboni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp. (arXiv:2106.04726v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.04726</id>
        <link href="http://arxiv.org/abs/2106.04726"/>
        <updated>2021-06-10T01:56:45.993Z</updated>
        <summary type="html"><![CDATA[WhatsApp is a popular chat application used by over 2 billion users
worldwide. However, due to end-to-end encryption, there is currently no easy
way to fact-check content on WhatsApp at scale. In this paper, we analyze the
usefulness of a crowd-sourced system on WhatsApp through which users can submit
"tips" containing messages they want fact-checked. We compare the tips sent to
a WhatsApp tipline run during the 2019 Indian national elections with the
messages circulating in large, public groups on WhatsApp and other social media
platforms during the same period. We find that tiplines are a very useful lens
into WhatsApp conversations: a significant fraction of messages and images sent
to the tipline match with the content being shared on public WhatsApp groups
and other social media. Our analysis also shows that tiplines cover the most
popular content well, and a majority of such content is often shared to the
tipline before appearing in large, public WhatsApp groups. Overall, the
analysis suggests tiplines can be an effective source for discovering content
to fact-check.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1"&gt;Ashkan Kazemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1"&gt;Kiran Garimella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1"&gt;Gautam Kishore Shahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1"&gt;Devin Gaffney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1"&gt;Scott A. Hale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DefSent: Sentence Embeddings using Definition Sentences. (arXiv:2105.04339v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04339</id>
        <link href="http://arxiv.org/abs/2105.04339"/>
        <updated>2021-06-10T01:56:45.987Z</updated>
        <summary type="html"><![CDATA[Sentence embedding methods using natural language inference (NLI) datasets
have been successfully applied to various tasks. However, these methods are
only available for limited languages due to relying heavily on the large NLI
datasets. In this paper, we propose DefSent, a sentence embedding method that
uses definition sentences from a word dictionary, which performs comparably on
unsupervised semantics textual similarity (STS) tasks and slightly better on
SentEval tasks than conventional methods. Since dictionaries are available for
many languages, DefSent is more broadly applicable than methods using NLI
datasets without constructing additional datasets. We demonstrate that DefSent
performs comparably on unsupervised semantics textual similarity (STS) tasks
and slightly better on SentEval tasks to the methods using large NLI datasets.
Our code is publicly available at https://github.com/hpprc/defsent .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsukagoshi_H/0/1/0/all/0/1"&gt;Hayato Tsukagoshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasano_R/0/1/0/all/0/1"&gt;Ryohei Sasano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1"&gt;Koichi Takeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vocabulary Learning via Optimal Transport for Machine Translation. (arXiv:2012.15671v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15671</id>
        <link href="http://arxiv.org/abs/2012.15671"/>
        <updated>2021-06-10T01:56:45.981Z</updated>
        <summary type="html"><![CDATA[The choice of token vocabulary affects the performance of machine
translation. This paper aims to figure out what is a good vocabulary and
whether one can find the optimal vocabulary without trial training. To answer
these questions, we first provide an alternative understanding of the role of
vocabulary from the perspective of information theory. Motivated by this, we
formulate the quest of vocabularization -- finding the best token dictionary
with a proper size -- as an optimal transport (OT) problem.We We propose VOLT,
a simple and efficient solution without trial training. Empirical results show
that VOLT outperforms widely-used vocabularies in diverse scenarios, including
WMT-14 English-German and TED's 52 translation directions. For example, VOLT
achieves 70% vocabulary size reduction and 0.5 BLEU gain on English-German
translation. Also, compared to BPE-search, VOLT reduces the search time from
384 GPU hours to 30 GPU hours on English-German translation. Codes are
available at https://github.com/Jingjing-NLP/VOLT .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jingjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chun Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zaixiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Case Studies on using Natural Language Processing Techniques in Customer Relationship Management Software. (arXiv:2106.05160v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.05160</id>
        <link href="http://arxiv.org/abs/2106.05160"/>
        <updated>2021-06-10T01:56:45.974Z</updated>
        <summary type="html"><![CDATA[How can a text corpus stored in a customer relationship management (CRM)
database be used for data mining and segmentation? In order to answer this
question we inherited the state of the art methods commonly used in natural
language processing (NLP) literature, such as word embeddings, and deep
learning literature, such as recurrent neural networks (RNN). We used the text
notes from a CRM system which are taken by customer representatives of an
internet ads consultancy agency between years 2009 and 2020. We trained word
embeddings by using the corresponding text corpus and showed that these word
embeddings can not only be used directly for data mining but also be used in
RNN architectures, which are deep learning frameworks built with long short
term memory (LSTM) units, for more comprehensive segmentation objectives. The
results prove that structured text data in a CRM can be used to mine out very
valuable information and any CRM can be equipped with useful NLP features once
the problem definitions are properly built and the solution methods are
conveniently implemented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Densely connected normalizing flows. (arXiv:2106.04627v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04627</id>
        <link href="http://arxiv.org/abs/2106.04627"/>
        <updated>2021-06-10T01:56:45.957Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are bijective mappings between inputs and latent
representations with a fully factorized distribution. They are very attractive
due to exact likelihood evaluation and efficient sampling. However, their
effective capacity is often insufficient since the bijectivity constraint
limits the model width. We address this issue by incrementally padding
intermediate representations with noise. We precondition the noise in
accordance with previous invertible units, which we describe as cross-unit
coupling. Our invertible glow-like modules express intra-unit affine coupling
as a fusion of a densely connected block and Nystr\"om self-attention. We refer
to our architecture as DenseFlow since both cross-unit and intra-unit couplings
rely on dense connectivity. Experiments show significant improvements due to
the proposed contributions, and reveal state-of-the-art density estimation
among all generative models under moderate computing budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grcic_M/0/1/0/all/0/1"&gt;Matej Grci&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1"&gt;Ivan Grubi&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1"&gt;Sini&amp;#x161;a &amp;#x160;egvi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Tiny Network for Recognition-Oriented Face Image Quality Assessment. (arXiv:2106.04852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04852</id>
        <link href="http://arxiv.org/abs/2106.04852"/>
        <updated>2021-06-10T01:56:45.951Z</updated>
        <summary type="html"><![CDATA[Face recognition has made significant progress in recent years due to deep
convolutional neural networks (CNN). In many face recognition (FR) scenarios,
face images are acquired from a sequence with huge intra-variations. These
intra-variations, which are mainly affected by the low-quality face images,
cause instability of recognition performance. Previous works have focused on
ad-hoc methods to select frames from a video or use face image quality
assessment (FIQA) methods, which consider only a particular or combination of
several distortions.

In this work, we present an efficient non-reference image quality assessment
for FR that directly links image quality assessment (IQA) and FR. More
specifically, we propose a new measurement to evaluate image quality without
any reference. Based on the proposed quality measurement, we propose a deep
Tiny Face Quality network (tinyFQnet) to learn a quality prediction function
from data.

We evaluate the proposed method for different powerful FR models on two
classical video-based (or template-based) benchmark: IJB-B and YTF. Extensive
experiments show that, although the tinyFQnet is much smaller than the others,
the proposed method outperforms state-of-the-art quality assessment methods in
terms of effectiveness and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Baoyun Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Min Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Heng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhaoning Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks. (arXiv:2011.02511v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02511</id>
        <link href="http://arxiv.org/abs/2011.02511"/>
        <updated>2021-06-10T01:56:45.944Z</updated>
        <summary type="html"><![CDATA[Large volumes of interaction logs can be collected from NLP systems that are
deployed in the real world. How can this wealth of information be leveraged?
Using such interaction logs in an offline reinforcement learning (RL) setting
is a promising approach. However, due to the nature of NLP tasks and the
constraints of production systems, a series of challenges arise. We present a
concise overview of these challenges and discuss possible solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1"&gt;Julia Kreutzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1"&gt;Stefan Riezler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1"&gt;Carolin Lawrence&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Computational Ghost Imaging using Unpaired Deep Learning and a Constrained Generative Adversarial Network. (arXiv:2106.04822v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04822</id>
        <link href="http://arxiv.org/abs/2106.04822"/>
        <updated>2021-06-10T01:56:45.938Z</updated>
        <summary type="html"><![CDATA[The unpaired training can be the only option available for fast deep
learning-based ghost imaging, where obtaining a high signal-to-noise ratio
(SNR) image copy of each low SNR ghost image could be practically
time-consuming and challenging. This paper explores the capabilities of deep
learning to leverage computational ghost imaging when there is a lack of paired
training images. The deep learning approach proposed here enables fast ghost
imaging through reconstruction of high SNR images from faint and hastily shot
ghost images using a constrained Wasserstein generative adversarial network. In
the proposed approach, the objective function is regularized to enforce the
generation of faithful and relevant high SNR images to the ghost copies. This
regularization measures the distance between reconstructed images and the faint
ghost images in a low-noise manifold generated by a shadow network. The
performance of the constrained network is shown to be particularly important
for ghost images with low SNR. The proposed pipeline is able to reconstruct
high-quality images from the ghost images with SNR values not necessarily equal
to the SNR of the training set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alishahi_F/0/1/0/all/0/1"&gt;Fatemeh Alishahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mohajerin_Ariaei_A/0/1/0/all/0/1"&gt;Amirhossein Mohajerin-Ariaei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Check It Again: Progressive Visual Question Answering via Visual Entailment. (arXiv:2106.04605v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04605</id>
        <link href="http://arxiv.org/abs/2106.04605"/>
        <updated>2021-06-10T01:56:45.923Z</updated>
        <summary type="html"><![CDATA[While sophisticated Visual Question Answering models have achieved remarkable
success, they tend to answer questions only according to superficial
correlations between question and answer. Several recent approaches have been
developed to address this language priors problem. However, most of them
predict the correct answer according to one best output without checking the
authenticity of answers. Besides, they only explore the interaction between
image and question, ignoring the semantics of candidate answers. In this paper,
we propose a select-and-rerank (SAR) progressive framework based on Visual
Entailment. Specifically, we first select the candidate answers relevant to the
question or the image, then we rerank the candidate answers by a visual
entailment task, which verifies whether the image semantically entails the
synthetic statement of the question and each candidate answer. Experimental
results show the effectiveness of our proposed framework, which establishes a
new state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1"&gt;Qingyi Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingyu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1"&gt;Peng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ex uno plures: Splitting One Model into an Ensemble of Subnetworks. (arXiv:2106.04767v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04767</id>
        <link href="http://arxiv.org/abs/2106.04767"/>
        <updated>2021-06-10T01:56:45.918Z</updated>
        <summary type="html"><![CDATA[Monte Carlo (MC) dropout is a simple and efficient ensembling method that can
improve the accuracy and confidence calibration of high-capacity deep neural
network models. However, MC dropout is not as effective as more
compute-intensive methods such as deep ensembles. This performance gap can be
attributed to the relatively poor quality of individual models in the MC
dropout ensemble and their lack of diversity. These issues can in turn be
traced back to the coupled training and substantial parameter sharing of the
dropout models. Motivated by this perspective, we propose a strategy to compute
an ensemble of subnetworks, each corresponding to a non-overlapping dropout
mask computed via a pruning strategy and trained independently. We show that
the proposed subnetwork ensembling method can perform as well as standard deep
ensembles in both accuracy and uncertainty estimates, yet with a computational
efficiency similar to MC dropout. Lastly, using several computer vision
datasets like CIFAR10/100, CUB200, and Tiny-Imagenet, we experimentally
demonstrate that subnetwork ensembling also consistently outperforms recently
proposed approaches that efficiently ensemble neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhilu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1"&gt;Vianne R. Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabuncu_M/0/1/0/all/0/1"&gt;Mert R. Sabuncu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-stage Sequence-to-Sequence Training. (arXiv:2103.16809v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16809</id>
        <link href="http://arxiv.org/abs/2103.16809"/>
        <updated>2021-06-10T01:56:45.912Z</updated>
        <summary type="html"><![CDATA[Emotional voice conversion (EVC) aims to change the emotional state of an
utterance while preserving the linguistic content and speaker identity. In this
paper, we propose a novel 2-stage training strategy for sequence-to-sequence
emotional voice conversion with a limited amount of emotional speech data. We
note that the proposed EVC framework leverages text-to-speech (TTS) as they
share a common goal that is to generate high-quality expressive voice. In stage
1, we perform style initialization with a multi-speaker TTS corpus, to
disentangle speaking style and linguistic content. In stage 2, we perform
emotion training with a limited amount of emotional speech data, to learn how
to disentangle emotional style and linguistic information from the speech. The
proposed framework can perform both spectrum and prosody conversion and
achieves significant improvement over the state-of-the-art baselines in both
objective and subjective evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1"&gt;Berrak Sisman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haizhou Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transient Chaos in BERT. (arXiv:2106.03181v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03181</id>
        <link href="http://arxiv.org/abs/2106.03181"/>
        <updated>2021-06-10T01:56:45.906Z</updated>
        <summary type="html"><![CDATA[Language is an outcome of our complex and dynamic human-interactions and the
technique of natural language processing (NLP) is hence built on human
linguistic activities. Bidirectional Encoder Representations from Transformers
(BERT) has recently gained its popularity by establishing the state-of-the-art
scores in several NLP benchmarks. A Lite BERT (ALBERT) is literally
characterized as a lightweight version of BERT, in which the number of BERT
parameters is reduced by repeatedly applying the same neural network called
Transformer's encoder layer. By pre-training the parameters with a massive
amount of natural language data, ALBERT can convert input sentences into
versatile high-dimensional vectors potentially capable of solving multiple NLP
tasks. In that sense, ALBERT can be regarded as a well-designed
high-dimensional dynamical system whose operator is the Transformer's encoder,
and essential structures of human language are thus expected to be encapsulated
in its dynamics. In this study, we investigated the embedded properties of
ALBERT to reveal how NLP tasks are effectively solved by exploiting its
dynamics. We thereby aimed to explore the nature of human language from the
dynamical expressions of the NLP model. Our short-term analysis clarified that
the pre-trained model stably yields trajectories with higher dimensionality,
which would enhance the expressive capacity required for NLP tasks. Also, our
long-term analysis revealed that ALBERT intrinsically shows transient chaos, a
typical nonlinear phenomenon showing chaotic dynamics only in its transient,
and the pre-trained ALBERT model tends to produce the chaotic trajectory for a
significantly longer time period compared to a randomly-initialized one. Our
results imply that local chaoticity would contribute to improving NLP
performance, uncovering a novel aspect in the role of chaotic dynamics in human
language behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1"&gt;Katsuma Inoue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohara_S/0/1/0/all/0/1"&gt;Soh Ohara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1"&gt;Yasuo Kuniyoshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakajima_K/0/1/0/all/0/1"&gt;Kohei Nakajima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Upsampling via Disentangled Refinement. (arXiv:2106.04779v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04779</id>
        <link href="http://arxiv.org/abs/2106.04779"/>
        <updated>2021-06-10T01:56:45.900Z</updated>
        <summary type="html"><![CDATA[Point clouds produced by 3D scanning are often sparse, non-uniform, and
noisy. Recent upsampling approaches aim to generate a dense point set, while
achieving both distribution uniformity and proximity-to-surface, and possibly
amending small holes, all in a single network. After revisiting the task, we
propose to disentangle the task based on its multi-objective nature and
formulate two cascaded sub-networks, a dense generator and a spatial refiner.
The dense generator infers a coarse but dense output that roughly describes the
underlying surface, while the spatial refiner further fine-tunes the coarse
output by adjusting the location of each point. Specifically, we design a pair
of local and global refinement units in the spatial refiner to evolve a coarse
feature map. Also, in the spatial refiner, we regress a per-point offset vector
to further adjust the coarse outputs in fine-scale. Extensive qualitative and
quantitative results on both synthetic and real-scanned datasets demonstrate
the superiority of our method over the state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruihui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xianzhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng-Ann Heng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Chi-Wing Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncovering Closed-form Governing Equations of Nonlinear Dynamics from Videos. (arXiv:2106.04776v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04776</id>
        <link href="http://arxiv.org/abs/2106.04776"/>
        <updated>2021-06-10T01:56:45.895Z</updated>
        <summary type="html"><![CDATA[Distilling analytical models from data has the potential to advance our
understanding and prediction of nonlinear dynamics. Although discovery of
governing equations based on observed system states (e.g., trajectory time
series) has revealed success in a wide range of nonlinear dynamics, uncovering
the closed-form equations directly from raw videos still remains an open
challenge. To this end, we introduce a novel end-to-end unsupervised deep
learning framework to uncover the mathematical structure of equations that
governs the dynamics of moving objects in videos. Such an architecture consists
of (1) an encoder-decoder network that learns low-dimensional spatial/pixel
coordinates of the moving object, (2) a learnable Spatial-Physical
Transformation component that creates mapping between the extracted
spatial/pixel coordinates and the latent physical states of dynamics, and (3) a
numerical integrator-based sparse regression module that uncovers the
parsimonious closed-form governing equations of learned physical states and,
meanwhile, serves as a constraint to the autoencoder. The efficacy of the
proposed method is demonstrated by uncovering the governing equations of a
variety of nonlinear dynamical systems depicted by moving objects in videos.
The resulting computational framework enables discovery of parsimonious
interpretable model in a flexible and accessible sensing environment where only
videos are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luan_L/0/1/0/all/0/1"&gt;Lele Luan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Quality Diversification for Task-Oriented Dialogue Systems. (arXiv:2106.00891v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00891</id>
        <link href="http://arxiv.org/abs/2106.00891"/>
        <updated>2021-06-10T01:56:45.889Z</updated>
        <summary type="html"><![CDATA[Many task-oriented dialogue systems use deep reinforcement learning (DRL) to
learn policies that respond to the user appropriately and complete the tasks
successfully. Training DRL agents with diverse dialogue trajectories prepare
them well for rare user requests and unseen situations. One effective
diversification method is to let the agent interact with a diverse set of
learned user models. However, trajectories created by these artificial user
models may contain generation errors, which can quickly propagate into the
agent's policy. It is thus important to control the quality of the
diversification and resist the noise. In this paper, we propose a novel
dialogue diversification method for task-oriented dialogue systems trained in
simulators. Our method, Intermittent Short Extension Ensemble (I-SEE),
constrains the intensity to interact with an ensemble of diverse user models
and effectively controls the quality of the diversification. Evaluations on the
Multiwoz dataset show that I-SEE successfully boosts the performance of several
state-of-the-art DRL dialogue agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhiwen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1"&gt;Hrishikesh Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Grace Hui Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10042</id>
        <link href="http://arxiv.org/abs/2105.10042"/>
        <updated>2021-06-10T01:56:45.884Z</updated>
        <summary type="html"><![CDATA[End-to-end spoken language understanding (SLU) has recently attracted
increasing interest. Compared to the conventional tandem-based approach that
combines speech recognition and language understanding as separate modules, the
new approach extracts users' intentions directly from the speech signals,
resulting in joint optimization and low latency. Such an approach, however, is
typically designed to process one intention at a time, which leads users to
take multiple rounds to fulfill their requirements while interacting with a
dialogue system. In this paper, we propose a streaming end-to-end framework
that can process multiple intentions in an online and incremental way. The
backbone of our framework is a unidirectional RNN trained with the
connectionist temporal classification (CTC) criterion. By this design, an
intention can be identified when sufficient evidence has been accumulated, and
multiple intentions can be identified sequentially. We evaluate our solution on
the Fluent Speech Commands (FSC) dataset and the intent detection accuracy is
about 97 % on all multi-intent settings. This result is comparable to the
performance of the state-of-the-art non-streaming models, but is achieved in an
online and incremental way. We also employ our model to a keyword spotting task
using the Google Speech Commands dataset and the results are also highly
promising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1"&gt;Nihal Potdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1"&gt;Anderson R. Avila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1"&gt;Chao Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yiran Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Neural Architecture Search via Proxy Data. (arXiv:2106.04784v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04784</id>
        <link href="http://arxiv.org/abs/2106.04784"/>
        <updated>2021-06-10T01:56:45.857Z</updated>
        <summary type="html"><![CDATA[Despite the increasing interest in neural architecture search (NAS), the
significant computational cost of NAS is a hindrance to researchers. Hence, we
propose to reduce the cost of NAS using proxy data, i.e., a representative
subset of the target data, without sacrificing search performance. Even though
data selection has been used across various fields, our evaluation of existing
selection methods for NAS algorithms offered by NAS-Bench-1shot1 reveals that
they are not always appropriate for NAS and a new selection method is
necessary. By analyzing proxy data constructed using various selection methods
through data entropy, we propose a novel proxy data selection method tailored
for NAS. To empirically demonstrate the effectiveness, we conduct thorough
experiments across diverse datasets, search spaces, and NAS algorithms.
Consequently, NAS algorithms with the proposed selection discover architectures
that are competitive with those obtained using the entire dataset. It
significantly reduces the search cost: executing DARTS with the proposed
selection requires only 40 minutes on CIFAR-10 and 7.5 hours on ImageNet with a
single GPU. Additionally, when the architecture searched on ImageNet using the
proposed selection is inversely transferred to CIFAR-10, a state-of-the-art
test error of 2.4\% is yielded. Our code is available at
https://github.com/nabk89/NAS-with-Proxy-data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1"&gt;Byunggook Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1"&gt;Jisoo Mok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1"&gt;Hyeokjun Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OODIn: An Optimised On-Device Inference Framework for Heterogeneous Mobile Devices. (arXiv:2106.04723v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04723</id>
        <link href="http://arxiv.org/abs/2106.04723"/>
        <updated>2021-06-10T01:56:45.840Z</updated>
        <summary type="html"><![CDATA[Radical progress in the field of deep learning (DL) has led to unprecedented
accuracy in diverse inference tasks. As such, deploying DL models across mobile
platforms is vital to enable the development and broad availability of the
next-generation intelligent apps. Nevertheless, the wide and optimised
deployment of DL models is currently hindered by the vast system heterogeneity
of mobile devices, the varying computational cost of different DL models and
the variability of performance needs across DL applications. This paper
proposes OODIn, a framework for the optimised deployment of DL apps across
heterogeneous mobile devices. OODIn comprises a novel DL-specific software
architecture together with an analytical framework for modelling DL
applications that: (1) counteract the variability in device resources and DL
models by means of a highly parametrised multi-layer design; and (2) perform a
principled optimisation of both model- and system-level parameters through a
multi-objective formulation, designed for DL inference apps, in order to adapt
the deployment to the user-specified performance requirements and device
capabilities. Quantitative evaluation shows that the proposed framework
consistently outperforms status-quo designs across heterogeneous devices and
delivers up to 4.3x and 3.5x performance gain over highly optimised platform-
and model-aware designs respectively, while effectively adapting execution to
dynamic changes in resource availability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1"&gt;Stylianos I. Venieris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panopoulos_I/0/1/0/all/0/1"&gt;Ioannis Panopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_I/0/1/0/all/0/1"&gt;Iakovos S. Venieris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes. (arXiv:2012.14210v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14210</id>
        <link href="http://arxiv.org/abs/2012.14210"/>
        <updated>2021-06-10T01:56:45.826Z</updated>
        <summary type="html"><![CDATA[Information Retrieval using dense low-dimensional representations recently
became popular and showed out-performance to traditional sparse-representations
like BM25. However, no previous work investigated how dense representations
perform with large index sizes. We show theoretically and empirically that the
performance for dense representations decreases quicker than sparse
representations for increasing index sizes. In extreme cases, this can even
lead to a tipping point where at a certain index size sparse representations
outperform dense representations. We show that this behavior is tightly
connected to the number of dimensions of the representations: The lower the
dimension, the higher the chance for false positives, i.e. returning irrelevant
documents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1"&gt;Nils Reimers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TED-net: Convolution-free T2T Vision Transformer-based Encoder-decoder Dilation network for Low-dose CT Denoising. (arXiv:2106.04650v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04650</id>
        <link href="http://arxiv.org/abs/2106.04650"/>
        <updated>2021-06-10T01:56:45.809Z</updated>
        <summary type="html"><![CDATA[Low dose computed tomography is a mainstream for clinical applications.
How-ever, compared to normal dose CT, in the low dose CT (LDCT) images, there
are stronger noise and more artifacts which are obstacles for practical
applications. In the last few years, convolution-based end-to-end deep learning
methods have been widely used for LDCT image denoising. Recently, transformer
has shown superior performance over convolution with more feature interactions.
Yet its ap-plications in LDCT denoising have not been fully cultivated. Here,
we propose a convolution-free T2T vision transformer-based Encoder-decoder
Dilation net-work (TED-net) to enrich the family of LDCT denoising algorithms.
The model is free of convolution blocks and consists of a symmetric
encoder-decoder block with sole transformer. Our model is evaluated on the
AAPM-Mayo clinic LDCT Grand Challenge dataset, and results show outperformance
over the state-of-the-art denoising methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dayang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hengyong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Domain Question Answering over Tables via Dense Retrieval. (arXiv:2103.12011v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12011</id>
        <link href="http://arxiv.org/abs/2103.12011"/>
        <updated>2021-06-10T01:56:45.803Z</updated>
        <summary type="html"><![CDATA[Recent advances in open-domain QA have led to strong models based on dense
retrieval, but only focused on retrieving textual passages. In this work, we
tackle open-domain QA over tables for the first time, and show that retrieval
can be improved by a retriever designed to handle tabular context. We present
an effective pre-training procedure for our retriever and improve retrieval
quality with mined hard negatives. As relevant datasets are missing, we extract
a subset of Natural Questions (Kwiatkowski et al., 2019) into a Table QA
dataset. We find that our retriever improves retrieval results from 72.0 to
81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a
BERT based retriever.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1"&gt;Jonathan Herzig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1"&gt;Thomas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krichene_S/0/1/0/all/0/1"&gt;Syrine Krichene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1"&gt;Julian Martin Eisenschlos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparative Study on Neural Architectures and Training Methods for Japanese Speech Recognition. (arXiv:2106.05111v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.05111</id>
        <link href="http://arxiv.org/abs/2106.05111"/>
        <updated>2021-06-10T01:56:45.785Z</updated>
        <summary type="html"><![CDATA[End-to-end (E2E) modeling is advantageous for automatic speech recognition
(ASR) especially for Japanese since word-based tokenization of Japanese is not
trivial, and E2E modeling is able to model character sequences directly. This
paper focuses on the latest E2E modeling techniques, and investigates their
performances on character-based Japanese ASR by conducting comparative
experiments. The results are analyzed and discussed in order to understand the
relative advantages of long short-term memory (LSTM), and Conformer models in
combination with connectionist temporal classification, transducer, and
attention-based loss functions. Furthermore, the paper investigates on
effectivity of the recent training techniques such as data augmentation
(SpecAugment), variational noise injection, and exponential moving average. The
best configuration found in the paper achieved the state-of-the-art character
error rates of 4.1%, 3.2%, and 3.5% for Corpus of Spontaneous Japanese (CSJ)
eval1, eval2, and eval3 tasks, respectively. The system is also shown to be
computationally efficient thanks to the efficiency of Conformer transducers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karita_S/0/1/0/all/0/1"&gt;Shigeki Karita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kubo_Y/0/1/0/all/0/1"&gt;Yotaro Kubo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacchiani_M/0/1/0/all/0/1"&gt;Michiel Adriaan Unico Bacchiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1"&gt;Llion Jones&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Complex Knowledge Graph Embeddings. (arXiv:2008.03130v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03130</id>
        <link href="http://arxiv.org/abs/2008.03130"/>
        <updated>2021-06-10T01:56:45.779Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of learning continuous vector
representations of knowledge graphs for predicting missing links. We present a
new approach called ConEx, which infers missing links by leveraging the
composition of a 2D convolution with a Hermitian inner product of
complex-valued embedding vectors. We evaluate ConEx against state-of-the-art
approaches on the WN18RR, FB15K-237, KINSHIP and UMLS benchmark datasets. Our
experimental results show that ConEx achieves a performance superior to that of
state-of-the-art approaches such as RotatE, QuatE and TuckER on the link
prediction task on all datasets while requiring at least 8 times fewer
parameters. We ensure the reproducibility of our results by providing an
open-source implementation which includes the training, evaluation scripts
along with pre-trained models at https://github.com/conex-kge/ConEx.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1"&gt;Caglar Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1"&gt;Axel-Cyrille Ngonga Ngomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coreference Reasoning in Machine Reading Comprehension. (arXiv:2012.15573v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15573</id>
        <link href="http://arxiv.org/abs/2012.15573"/>
        <updated>2021-06-10T01:56:45.772Z</updated>
        <summary type="html"><![CDATA[Coreference resolution is essential for natural language understanding and
has been long studied in NLP. In recent years, as the format of Question
Answering (QA) became a standard for machine reading comprehension (MRC), there
have been data collection efforts, e.g., Dasigi et al. (2019), that attempt to
evaluate the ability of MRC models to reason about coreference. However, as we
show, coreference reasoning in MRC is a greater challenge than earlier thought;
MRC datasets do not reflect the natural distribution and, consequently, the
challenges of coreference reasoning. Specifically, success on these datasets
does not reflect a model's proficiency in coreference reasoning. We propose a
methodology for creating MRC datasets that better reflect the challenges of
coreference reasoning and use it to create a sample evaluation set. The results
on our dataset show that state-of-the-art models still struggle with these
phenomena. Furthermore, we develop an effective way to use naturally occurring
coreference phenomena from existing coreference resolution datasets when
training MRC models. This allows us to show an improvement in the coreference
reasoning abilities of state-of-the-art models. The code and the resulting
dataset are available at https://github.com/UKPLab/coref-reasoning-in-qa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mingzhu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1"&gt;Nafise Sadat Moosavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1"&gt;Dan Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04632</id>
        <link href="http://arxiv.org/abs/2106.04632"/>
        <updated>2021-06-10T01:56:45.767Z</updated>
        <summary type="html"><![CDATA[Most existing video-and-language (VidL) research focuses on a single dataset,
or multiple datasets of a single task. In reality, a truly useful VidL system
is expected to be easily generalizable to diverse tasks, domains, and datasets.
To facilitate the evaluation of such systems, we introduce Video-And-Language
Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets
over 3 popular tasks: (i) text-to-video retrieval; (ii) video question
answering; and (iii) video captioning. VALUE benchmark aims to cover a broad
range of video genres, video lengths, data volumes, and task difficulty levels.
Rather than focusing on single-channel videos with visual information only,
VALUE promotes models that leverage information from both video frames and
their associated subtitles, as well as models that share knowledge across
multiple tasks. We evaluate various baseline methods with and without
large-scale VidL pre-training, and systematically investigate the impact of
video input channels, fusion methods, and different video representations. We
also study the transferability between tasks, and conduct multi-task learning
under different settings. The significant gap between our best model and human
performance calls for future study for advanced VidL models. VALUE is available
at https://value-leaderboard.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Licheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillai_R/0/1/0/all/0/1"&gt;Rohit Pillai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luowei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Eric Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;William Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara Lee Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Attention Belief Networks. (arXiv:2106.05251v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.05251</id>
        <link href="http://arxiv.org/abs/2106.05251"/>
        <updated>2021-06-10T01:56:45.755Z</updated>
        <summary type="html"><![CDATA[Attention-based neural networks have achieved state-of-the-art results on a
wide range of tasks. Most such models use deterministic attention while
stochastic attention is less explored due to the optimization difficulties or
complicated model design. This paper introduces Bayesian attention belief
networks, which construct a decoder network by modeling unnormalized attention
weights with a hierarchy of gamma distributions, and an encoder network by
stacking Weibull distributions with a deterministic-upward-stochastic-downward
structure to approximate the posterior. The resulting auto-encoding networks
can be optimized in a differentiable way with a variational lower bound. It is
simple to convert any models with deterministic attention, including pretrained
ones, to the proposed Bayesian attention belief networks. On a variety of
language understanding tasks, we show that our method outperforms deterministic
attention and state-of-the-art stochastic attention in accuracy, uncertainty
estimation, generalization across domains, and robustness to adversarial
attacks. We further demonstrate the general applicability of our method on
neural machine translation and visual question answering, showing great
potential of incorporating our method into various attention-related tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shujian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xinjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Would a Teacher Do? Predicting Future Talk Moves. (arXiv:2106.05249v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.05249</id>
        <link href="http://arxiv.org/abs/2106.05249"/>
        <updated>2021-06-10T01:56:45.738Z</updated>
        <summary type="html"><![CDATA[Recent advances in natural language processing (NLP) have the ability to
transform how classroom learning takes place. Combined with the increasing
integration of technology in today's classrooms, NLP systems leveraging
question answering and dialog processing techniques can serve as private tutors
or participants in classroom discussions to increase student engagement and
learning. To progress towards this goal, we use the classroom discourse
framework of academically productive talk (APT) to learn strategies that make
for the best learning experience. In this paper, we introduce a new task,
called future talk move prediction (FTMP): it consists of predicting the next
talk move -- an utterance strategy from APT -- given a conversation history
with its corresponding talk moves. We further introduce a neural network model
for this task, which outperforms multiple baselines by a large margin. Finally,
we compare our model's performance on FTMP to human performance and show
several similarities between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_A/0/1/0/all/0/1"&gt;Ananya Ganesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1"&gt;Martha Palmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1"&gt;Katharina Kann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Class-Transductive Intent Representations for Zero-shot Intent Detection. (arXiv:2012.01721v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01721</id>
        <link href="http://arxiv.org/abs/2012.01721"/>
        <updated>2021-06-10T01:56:45.732Z</updated>
        <summary type="html"><![CDATA[Zero-shot intent detection (ZSID) aims to deal with the continuously emerging
intents without annotated training data. However, existing ZSID systems suffer
from two limitations: 1) They are not good at modeling the relationship between
seen and unseen intents. 2) They cannot effectively recognize unseen intents
under the generalized intent detection (GZSID) setting. A critical problem
behind these limitations is that the representations of unseen intents cannot
be learned in the training stage. To address this problem, we propose a novel
framework that utilizes unseen class labels to learn Class-Transductive Intent
Representations (CTIR). Specifically, we allow the model to predict unseen
intents during training, with the corresponding label names serving as input
utterances. On this basis, we introduce a multi-task learning objective, which
encourages the model to learn the distinctions among intents, and a similarity
scorer, which estimates the connections among intents more accurately. CTIR is
easy to implement and can be integrated with existing methods. Experiments on
two real-world datasets show that CTIR brings considerable improvement to the
baseline systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1"&gt;Qingyi Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanxin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1"&gt;Peng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiangnan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phraseformer: Multimodal Key-phrase Extraction using Transformer and Graph Embedding. (arXiv:2106.04939v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04939</id>
        <link href="http://arxiv.org/abs/2106.04939"/>
        <updated>2021-06-10T01:56:45.726Z</updated>
        <summary type="html"><![CDATA[Background: Keyword extraction is a popular research topic in the field of
natural language processing. Keywords are terms that describe the most relevant
information in a document. The main problem that researchers are facing is how
to efficiently and accurately extract the core keywords from a document.
However, previous keyword extraction approaches have utilized the text and
graph features, there is the lack of models that can properly learn and combine
these features in a best way.

Methods: In this paper, we develop a multimodal Key-phrase extraction
approach, namely Phraseformer, using transformer and graph embedding
techniques. In Phraseformer, each keyword candidate is presented by a vector
which is the concatenation of the text and structure learning representations.
Phraseformer takes the advantages of recent researches such as BERT and ExEm to
preserve both representations. Also, the Phraseformer treats the key-phrase
extraction task as a sequence labeling problem solved using classification
task.

Results: We analyze the performance of Phraseformer on three datasets
including Inspec, SemEval2010 and SemEval 2017 by F1-score. Also, we
investigate the performance of different classifiers on Phraseformer method
over Inspec dataset. Experimental results demonstrate the effectiveness of
Phraseformer method over the three datasets used. Additionally, the Random
Forest classifier gain the highest F1-score among all classifiers.

Conclusions: Due to the fact that the combination of BERT and ExEm is more
meaningful and can better represent the semantic of words. Hence, Phraseformer
significantly outperforms single-modality methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikzad_Khasmakhi_N/0/1/0/all/0/1"&gt;Narjes Nikzad-Khasmakhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1"&gt;Mohammad-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1"&gt;Meysam Asgari-Chenaghlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1"&gt;Mohammad-Ali Balafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1"&gt;Ali-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahkar_Farshi_T/0/1/0/all/0/1"&gt;Taymaz Rahkar-Farshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1"&gt;Majid Ramezani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jahanbakhsh_Nagadeh_Z/0/1/0/all/0/1"&gt;Zoleikha Jahanbakhsh-Nagadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafarani_Moattar_E/0/1/0/all/0/1"&gt;Elnaz Zafarani-Moattar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1"&gt;Mehrdad Ranjbar-Khadivi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which transformer architecture fits my data? A vocabulary bottleneck in self-attention. (arXiv:2105.03928v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03928</id>
        <link href="http://arxiv.org/abs/2105.03928"/>
        <updated>2021-06-10T01:56:45.720Z</updated>
        <summary type="html"><![CDATA[After their successful debut in natural language processing, Transformer
architectures are now becoming the de-facto standard in many domains. An
obstacle for their deployment over new modalities is the architectural
configuration: the optimal depth-to-width ratio has been shown to dramatically
vary across data types (e.g., $10$x larger over images than over language). We
theoretically predict the existence of an embedding rank bottleneck that limits
the contribution of self-attention width to the Transformer expressivity. We
thus directly tie the input vocabulary size and rank to the optimal
depth-to-width ratio, since a small vocabulary size or rank dictates an added
advantage of depth over width. We empirically demonstrate the existence of this
bottleneck and its implications on the depth-to-width interplay of Transformer
architectures, linking the architecture variability across domains to the often
glossed-over usage of different vocabulary sizes or embedding ranks in
different domains. As an additional benefit, our rank bottlenecking framework
allows us to identify size redundancies of $25\%-50\%$ in leading NLP models
such as ALBERT and T5.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1"&gt;Noam Wies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1"&gt;Yoav Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jannai_D/0/1/0/all/0/1"&gt;Daniel Jannai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1"&gt;Amnon Shashua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers. (arXiv:2103.14465v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14465</id>
        <link href="http://arxiv.org/abs/2103.14465"/>
        <updated>2021-06-10T01:56:45.713Z</updated>
        <summary type="html"><![CDATA[We investigate how sentence-level transformers can be modified into effective
sequence labelers at the token level without any direct supervision. Existing
approaches to zero-shot sequence labeling do not perform well when applied on
transformer-based architectures. As transformers contain multiple layers of
multi-head self-attention, information in the sentence gets distributed between
many tokens, negatively affecting zero-shot token-level performance. We find
that a soft attention module which explicitly encourages sharpness of attention
weights can significantly outperform existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bujel_K/0/1/0/all/0/1"&gt;Kamil Bujel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1"&gt;Helen Yannakoudakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1"&gt;Marek Rei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter-Efficient Transfer Learning with Diff Pruning. (arXiv:2012.07463v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07463</id>
        <link href="http://arxiv.org/abs/2012.07463"/>
        <updated>2021-06-10T01:56:45.695Z</updated>
        <summary type="html"><![CDATA[While task-specific finetuning of pretrained networks has led to significant
empirical advances in NLP, the large size of networks makes finetuning
difficult to deploy in multi-task, memory-constrained settings. We propose diff
pruning as a simple approach to enable parameter-efficient transfer learning
within the pretrain-finetune framework. This approach views finetuning as
learning a task-specific diff vector that is applied on top of the pretrained
parameter vector, which remains fixed and is shared across different tasks. The
diff vector is adaptively pruned during training with a differentiable
approximation to the L0-norm penalty to encourage sparsity. Diff pruning
becomes parameter-efficient as the number of tasks increases, as it requires
storing only the nonzero positions and weights of the diff vector for each
task, while the cost of storing the shared pretrained model remains constant.
It further does not require access to all tasks during training, which makes it
attractive in settings where tasks arrive in stream or the set of tasks is
unknown. We find that models finetuned with diff pruning can match the
performance of fully finetuned baselines on the GLUE benchmark while only
modifying 0.5% of the pretrained model's parameters per task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1"&gt;Demi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1"&gt;Alexander M. Rush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yoon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multilingual Representation for Natural Language Understanding with Enhanced Cross-Lingual Supervision. (arXiv:2106.05166v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.05166</id>
        <link href="http://arxiv.org/abs/2106.05166"/>
        <updated>2021-06-10T01:56:45.650Z</updated>
        <summary type="html"><![CDATA[Recently, pre-training multilingual language models has shown great potential
in learning multilingual representation, a crucial topic of natural language
processing. Prior works generally use a single mixed attention (MA) module,
following TLM (Conneau and Lample, 2019), for attending to intra-lingual and
cross-lingual contexts equivalently and simultaneously. In this paper, we
propose a network named decomposed attention (DA) as a replacement of MA. The
DA consists of an intra-lingual attention (IA) and a cross-lingual attention
(CA), which model intralingual and cross-lingual supervisions respectively. In
addition, we introduce a language-adaptive re-weighting strategy during
training to further boost the model's performance. Experiments on various
cross-lingual natural language understanding (NLU) tasks show that the proposed
architecture and learning strategy significantly improve the model's
cross-lingual transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yinpeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liangyou Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation. (arXiv:2004.08694v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08694</id>
        <link href="http://arxiv.org/abs/2004.08694"/>
        <updated>2021-06-10T01:56:45.642Z</updated>
        <summary type="html"><![CDATA[Question Generation (QG) is fundamentally a simple syntactic transformation;
however, many aspects of semantics influence what questions are good to form.
We implement this observation by developing Syn-QG, a set of transparent
syntactic rules leveraging universal dependencies, shallow semantic parsing,
lexical resources, and custom rules which transform declarative sentences into
question-answer pairs. We utilize PropBank argument descriptions and VerbNet
state predicates to incorporate shallow semantic content, which helps generate
questions of a descriptive nature and produce inferential and semantically
richer questions than existing systems. In order to improve syntactic fluency
and eliminate grammatically incorrect questions, we employ back-translation
over the output of these syntactic rules. A set of crowd-sourced evaluations
shows that our system can generate a larger number of highly grammatical and
relevant questions than previous QG systems and that back-translation
drastically improves grammaticality at a slight cost of generating irrelevant
questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1"&gt;Kaustubh D. Dhole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1"&gt;Christopher D. Manning&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Supervised Domain Adaptation by Augmenting Pre-trained Models with Random Units. (arXiv:2106.04935v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04935</id>
        <link href="http://arxiv.org/abs/2106.04935"/>
        <updated>2021-06-10T01:56:45.636Z</updated>
        <summary type="html"><![CDATA[Neural Transfer Learning (TL) is becoming ubiquitous in Natural Language
Processing (NLP), thanks to its high performance on many tasks, especially in
low-resourced scenarios. Notably, TL is widely used for neural domain
adaptation to transfer valuable knowledge from high-resource to low-resource
domains. In the standard fine-tuning scheme of TL, a model is initially
pre-trained on a source domain and subsequently fine-tuned on a target domain
and, therefore, source and target domains are trained using the same
architecture. In this paper, we show through interpretation methods that such
scheme, despite its efficiency, is suffering from a main limitation. Indeed,
although capable of adapting to new domains, pre-trained neurons struggle with
learning certain patterns that are specific to the target domain. Moreover, we
shed light on the hidden negative transfer occurring despite the high
relatedness between source and target domains, which may mitigate the final
gain brought by transfer learning. To address these problems, we propose to
augment the pre-trained model with normalised, weighted and randomly
initialised units that foster a better adaptation while maintaining the
valuable source knowledge. We show that our approach exhibits significant
improvements to the standard fine-tuning scheme for neural domain adaptation
from the news domain to the social media domain on four NLP tasks:
part-of-speech tagging, chunking, named entity recognition and morphosyntactic
tagging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meftah_S/0/1/0/all/0/1"&gt;Sara Meftah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semmar_N/0/1/0/all/0/1"&gt;Nasredine Semmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamaazousti_Y/0/1/0/all/0/1"&gt;Youssef Tamaazousti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Essafi_H/0/1/0/all/0/1"&gt;Hassane Essafi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadat_F/0/1/0/all/0/1"&gt;Fatiha Sadat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Memorization of Conspiracy Theories in Text Generation. (arXiv:2101.00379v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00379</id>
        <link href="http://arxiv.org/abs/2101.00379"/>
        <updated>2021-06-10T01:56:45.630Z</updated>
        <summary type="html"><![CDATA[The adoption of natural language generation (NLG) models can leave
individuals vulnerable to the generation of harmful information memorized by
the models, such as conspiracy theories. While previous studies examine
conspiracy theories in the context of social media, they have not evaluated
their presence in the new space of generative language models. In this work, we
investigate the capability of language models to generate conspiracy theory
text. Specifically, we aim to answer: can we test pretrained generative
language models for the memorization and elicitation of conspiracy theories
without access to the model's training data? We highlight the difficulties of
this task and discuss it in the context of memorization, generalization, and
hallucination. Utilizing a new dataset consisting of conspiracy theory topics
and machine-generated conspiracy theories helps us discover that many
conspiracy theories are deeply rooted in the pretrained language models. Our
experiments demonstrate a relationship between model parameters such as size
and temperature and their propensity to generate conspiracy theory text. These
results indicate the need for a more thorough review of NLG applications before
release and an in-depth discussion of the drawbacks of memorization in
generative language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1"&gt;Sharon Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1"&gt;Michael Saxon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;William Yang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09501</id>
        <link href="http://arxiv.org/abs/2105.09501"/>
        <updated>2021-06-10T01:56:45.612Z</updated>
        <summary type="html"><![CDATA[Existing multilingual machine translation approaches mainly focus on
English-centric directions, while the non-English directions still lag behind.
In this work, we aim to build a many-to-many translation system with an
emphasis on the quality of non-English language directions. Our intuition is
based on the hypothesis that a universal cross-language representation leads to
better multilingual translation performance. To this end, we propose mRASP2, a
training method to obtain a single unified multilingual translation model.
mRASP2 is empowered by two techniques: a) a contrastive learning scheme to
close the gap among representations of different languages, and b) data
augmentation on both multiple parallel and monolingual data to further align
token representations. For English-centric directions, mRASP2 outperforms
existing best unified model and achieves competitive or even better performance
than the pre-trained and fine-tuned model mBART on tens of WMT's translation
directions. For non-English directions, mRASP2 achieves an improvement of
average 10+ BLEU compared with the multilingual Transformer baseline. Code,
data and trained models are available at https://github.com/PANXiao1994/mRASP2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Lack of Robust Interpretability of Neural Text Classifiers. (arXiv:2106.04631v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04631</id>
        <link href="http://arxiv.org/abs/2106.04631"/>
        <updated>2021-06-10T01:56:45.596Z</updated>
        <summary type="html"><![CDATA[With the ever-increasing complexity of neural language models, practitioners
have turned to methods for understanding the predictions of these models. One
of the most well-adopted approaches for model interpretability is feature-based
interpretability, i.e., ranking the features in terms of their impact on model
predictions. Several prior studies have focused on assessing the fidelity of
feature-based interpretability methods, i.e., measuring the impact of dropping
the top-ranked features on the model output. However, relatively little work
has been conducted on quantifying the robustness of interpretations. In this
work, we assess the robustness of interpretations of neural text classifiers,
specifically, those based on pretrained Transformer encoders, using two
randomization tests. The first compares the interpretations of two models that
are identical except for their initializations. The second measures whether the
interpretations differ between a model with trained parameters and a model with
random parameters. Both tests show surprising deviations from expected
behavior, raising questions about the extent of insights that practitioners may
draw from interpretations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1"&gt;Muhammad Bilal Zafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donini_M/0/1/0/all/0/1"&gt;Michele Donini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1"&gt;Dylan Slack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Archambeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sanjiv Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1"&gt;Krishnaram Kenthapadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Extractive Search. (arXiv:2106.04612v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04612</id>
        <link href="http://arxiv.org/abs/2106.04612"/>
        <updated>2021-06-10T01:56:45.590Z</updated>
        <summary type="html"><![CDATA[Domain experts often need to extract structured information from large
corpora. We advocate for a search paradigm called ``extractive search'', in
which a search query is enriched with capture-slots, to allow for such rapid
extraction. Such an extractive search system can be built around syntactic
structures, resulting in high-precision, low-recall results. We show how the
recall can be improved using neural retrieval and alignment. The goals of this
paper are to concisely introduce the extractive-search paradigm; and to
demonstrate a prototype neural retrieval system for extractive search and its
benefits and potential. Our prototype is available at
\url{https://spike.neural-sim.apps.allenai.org/} and a video demonstration is
available at \url{https://vimeo.com/559586687}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1"&gt;Shauli Ravfogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taub_Tabib_H/0/1/0/all/0/1"&gt;Hillel Taub-Tabib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech to Standard German Text Corpus. (arXiv:2010.02810v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02810</id>
        <link href="http://arxiv.org/abs/2010.02810"/>
        <updated>2021-06-10T01:56:45.584Z</updated>
        <summary type="html"><![CDATA[We present the Swiss Parliaments Corpus (SPC), an automatically aligned Swiss
German speech to Standard German text corpus. This first version of the corpus
is based on publicly available data of the Bernese cantonal parliament and
consists of 293 hours of data. It was created using a novel forced sentence
alignment procedure and an alignment quality estimator, which can be used to
trade off corpus size and quality. We trained Automatic Speech Recognition
(ASR) models as baselines on different subsets of the data and achieved a Word
Error Rate (WER) of 0.278 and a BLEU score of 0.586 on the SPC test set. The
corpus is freely available for download.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pluss_M/0/1/0/all/0/1"&gt;Michel Pl&amp;#xfc;ss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neukom_L/0/1/0/all/0/1"&gt;Lukas Neukom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheller_C/0/1/0/all/0/1"&gt;Christian Scheller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogel_M/0/1/0/all/0/1"&gt;Manfred Vogel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding. (arXiv:2106.04970v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04970</id>
        <link href="http://arxiv.org/abs/2106.04970"/>
        <updated>2021-06-10T01:56:45.577Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the
online inference efficiency of the Transformer for instantaneous Grammatical
Error Correction (GEC). SAD optimizes the online inference efficiency for GEC
by two innovations: 1) it aggressively decodes as many tokens as possible in
parallel instead of always decoding only one token in each step to improve
computational parallelism; 2) it uses a shallow decoder instead of the
conventional Transformer architecture with balanced encoder-decoder depth to
reduce the computational cost during inference. Experiments in both English and
Chinese GEC benchmarks show that aggressive decoding could yield the same
predictions as greedy decoding but with a significant speedup for online
inference. Its combination with the shallow decoder could offer an even higher
online inference speedup over the powerful Transformer baseline without quality
loss. Not only does our approach allow a single model to achieve the
state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14
and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference
speedup over the Transformer-big model, but also it is easily adapted to other
languages. Our code is available at
https://github.com/AutoTemp/Shallow-Aggressive-Decoding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1"&gt;Tao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Houfeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation. (arXiv:2106.05093v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.05093</id>
        <link href="http://arxiv.org/abs/2106.05093"/>
        <updated>2021-06-10T01:56:45.570Z</updated>
        <summary type="html"><![CDATA[We propose a new training objective named order-agnostic cross entropy (OaXE)
for fully non-autoregressive translation (NAT) models. OaXE improves the
standard cross-entropy loss to ameliorate the effect of word reordering, which
is a common source of the critical multimodality problem in NAT. Concretely,
OaXE removes the penalty for word order errors, and computes the cross entropy
loss based on the best possible alignment between model predictions and target
tokens. Since the log loss is very sensitive to invalid references, we leverage
cross entropy initialization and loss truncation to ensure the model focuses on
a good part of the search space. Extensive experiments on major WMT benchmarks
show that OaXE substantially improves translation performance, setting new
state of the art for fully NAT models. Further analyses show that OaXE
alleviates the multimodality problem by reducing token repetitions and
increasing prediction confidence. Our code, data, and trained models are
available at https://github.com/tencent-ailab/ICML21_OAXE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1"&gt;Cunxiao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint System-Wise Optimization for Pipeline Goal-Oriented Dialog System. (arXiv:2106.04835v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04835</id>
        <link href="http://arxiv.org/abs/2106.04835"/>
        <updated>2021-06-10T01:56:45.553Z</updated>
        <summary type="html"><![CDATA[Recent work (Takanobu et al., 2020) proposed the system-wise evaluation on
dialog systems and found that improvement on individual components (e.g., NLU,
policy) in prior work may not necessarily bring benefit to pipeline systems in
system-wise evaluation. To improve the system-wise performance, in this paper,
we propose new joint system-wise optimization techniques for the pipeline
dialog system. First, we propose a new data augmentation approach which
automates the labeling process for NLU training. Second, we propose a novel
stochastic policy parameterization with Poisson distribution that enables
better exploration and offers a principled way to compute policy gradient.
Third, we propose a reward bonus to help policy explore successful dialogs. Our
approaches outperform the competitive pipeline systems from Takanobu et al.
(2020) by big margins of 12% success rate in automatic system-wise evaluation
and of 16% success rate in human evaluation on the standard multi-domain
benchmark dataset MultiWOZ 2.1, and also outperform the recent state-of-the-art
end-to-end trained model from DSTC9.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zichuan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bowen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaodong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-tagging of Short Conversational Sentences using Natural Language Processing Methods. (arXiv:2106.04959v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04959</id>
        <link href="http://arxiv.org/abs/2106.04959"/>
        <updated>2021-06-10T01:56:45.531Z</updated>
        <summary type="html"><![CDATA[In this study, we aim to find a method to auto-tag sentences specific to a
domain. Our training data comprises short conversational sentences extracted
from chat conversations between company's customer representatives and web site
visitors. We manually tagged approximately 14 thousand visitor inputs into ten
basic categories, which will later be used in a transformer-based language
model with attention mechanisms for the ultimate goal of developing a chatbot
application that can produce meaningful dialogue. We considered three different
state-of-the-art models and reported their auto-tagging capabilities. We
achieved the best performance with the bidirectional encoder representation
from transformers (BERT) model. Implementation of the models used in these
experiments can be cloned from our GitHub repository and tested for similar
auto-tagging problems without much effort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1"&gt;D. Emre Ta&amp;#x15f;ar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Better Use of Bilingual Information for Cross-Lingual AMR Parsing. (arXiv:2106.04814v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04814</id>
        <link href="http://arxiv.org/abs/2106.04814"/>
        <updated>2021-06-10T01:56:45.525Z</updated>
        <summary type="html"><![CDATA[Abstract Meaning Representation (AMR) is a rooted, labeled, acyclic graph
representing the semantics of natural language. As previous works show,
although AMR is designed for English at first, it can also represent semantics
in other languages. However, they find that concepts in their predicted AMR
graphs are less specific. We argue that the misprediction of concepts is due to
the high relevance between English tokens and AMR concepts. In this work, we
introduce bilingual input, namely the translated texts as well as non-English
texts, in order to enable the model to predict more accurate concepts. Besides,
we also introduce an auxiliary task, requiring the decoder to predict the
English sequences at the same time. The auxiliary task can help the decoder
understand what exactly the corresponding English tokens are. Our proposed
cross-lingual AMR parser surpasses previous state-of-the-art parser by 10.6
points on Smatch F1 score. The ablation study also demonstrates the efficacy of
our proposed modules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yitao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhe Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xiaojun Wan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing Multilingual Language Models for Discourse. (arXiv:2106.04832v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04832</id>
        <link href="http://arxiv.org/abs/2106.04832"/>
        <updated>2021-06-10T01:56:45.505Z</updated>
        <summary type="html"><![CDATA[Pre-trained multilingual language models have become an important building
block in multilingual natural language processing. In the present paper, we
investigate a range of such models to find out how well they transfer
discourse-level knowledge across languages. This is done with a systematic
evaluation on a broader set of discourse-level tasks than has been previously
been assembled. We find that the XLM-RoBERTa family of models consistently show
the best performance, by simultaneously being good monolingual models and
degrading relatively little in a zero-shot setting. Our results also indicate
that model distillation may hurt the ability of cross-lingual transfer of
sentence representations, while language dissimilarity at most has a modest
effect. We hope that our test suite, covering 5 tasks with a total of 22
languages in 10 distinct families, will serve as a useful evaluation platform
for multilingual performance at and beyond the sentence level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurfali_M/0/1/0/all/0/1"&gt;Murathan Kurfal&amp;#x131;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostling_R/0/1/0/all/0/1"&gt;Robert &amp;#xd6;stling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Psycholinguistic Tripartite Graph Network for Personality Detection. (arXiv:2106.04963v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04963</id>
        <link href="http://arxiv.org/abs/2106.04963"/>
        <updated>2021-06-10T01:56:45.498Z</updated>
        <summary type="html"><![CDATA[Most of the recent work on personality detection from online posts adopts
multifarious deep neural networks to represent the posts and builds predictive
models in a data-driven manner, without the exploitation of psycholinguistic
knowledge that may unveil the connections between one's language usage and his
psychological traits. In this paper, we propose a psycholinguistic
knowledge-based tripartite graph network, TrigNet, which consists of a
tripartite graph network and a BERT-based graph initializer. The graph network
injects structural psycholinguistic knowledge from LIWC, a computerized
instrument for psycholinguistic analysis, by constructing a heterogeneous
tripartite graph. The graph initializer is employed to provide initial
embeddings for the graph nodes. To reduce the computational cost in graph
learning, we further propose a novel flow graph attention network (GAT) that
only transmits messages between neighboring parties in the tripartite graph.
Benefiting from the tripartite graph, TrigNet can aggregate post information
from a psychological perspective, which is a novel way of exploiting domain
knowledge. Extensive experiments on two datasets show that TrigNet outperforms
the existing state-of-art model by 3.47 and 2.10 points in average F1.
Moreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%,
respectively, in comparison to the original GAT in our setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Feifan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1"&gt;Haolan Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1"&gt;Xiaojun Quan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DGA-Net Dynamic Gaussian Attention Network for Sentence Semantic Matching. (arXiv:2106.04905v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04905</id>
        <link href="http://arxiv.org/abs/2106.04905"/>
        <updated>2021-06-10T01:56:45.477Z</updated>
        <summary type="html"><![CDATA[Sentence semantic matching requires an agent to determine the semantic
relation between two sentences, where much recent progress has been made by the
advancement of representation learning techniques and inspiration of human
behaviors. Among all these methods, attention mechanism plays an essential role
by selecting important parts effectively. However, current attention methods
either focus on all the important parts in a static way or only select one
important part at one attention step dynamically, which leaves a large space
for further improvement. To this end, in this paper, we design a novel Dynamic
Gaussian Attention Network (DGA-Net) to combine the advantages of current
static and dynamic attention methods. More specifically, we first leverage
pre-trained language model to encode the input sentences and construct semantic
representations from a global perspective. Then, we develop a Dynamic Gaussian
Attention (DGA) to dynamically capture the important parts and corresponding
local contexts from a detailed perspective. Finally, we combine the global
information and detailed local information together to decide the semantic
relation of sentences comprehensively and precisely. Extensive experiments on
two popular sentence semantic matching tasks demonstrate that our proposed
DGA-Net is effective in improving the ability of attention mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1"&gt;Guangyi Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Automatic Speech Recognition: A Review. (arXiv:2106.04897v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04897</id>
        <link href="http://arxiv.org/abs/2106.04897"/>
        <updated>2021-06-10T01:56:45.471Z</updated>
        <summary type="html"><![CDATA[Automatic Speech Recognition (ASR) systems can be trained to achieve
remarkable performance given large amounts of manually transcribed speech, but
large labeled data sets can be difficult or expensive to acquire for all
languages of interest. In this paper, we review the research literature to
identify models and ideas that could lead to fully unsupervised ASR, including
unsupervised segmentation of the speech signal, unsupervised mapping from
speech segments to text, and semi-supervised models with nominal amounts of
labeled examples. The objective of the study is to identify the limitations of
what can be learned from speech data alone and to understand the minimum
requirements for speech recognition. Identifying these limitations would help
optimize the resources and efforts in ASR development for low-resource
languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1"&gt;Hanan Aldarmaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullah_A/0/1/0/all/0/1"&gt;Asad Ullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaki_N/0/1/0/all/0/1"&gt;Nazar Zaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentence Embeddings using Supervised Contrastive Learning. (arXiv:2106.04791v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04791</id>
        <link href="http://arxiv.org/abs/2106.04791"/>
        <updated>2021-06-10T01:56:45.459Z</updated>
        <summary type="html"><![CDATA[Sentence embeddings encode sentences in fixed dense vectors and have played
an important role in various NLP tasks and systems. Methods for building
sentence embeddings include unsupervised learning such as Quick-Thoughts and
supervised learning such as InferSent. With the success of pretrained NLP
models, recent research shows that fine-tuning pretrained BERT on SNLI and
Multi-NLI data creates state-of-the-art sentence embeddings, outperforming
previous sentence embeddings methods on various evaluation benchmarks. In this
paper, we propose a new method to build sentence embeddings by doing supervised
contrastive learning. Specifically our method fine-tunes pretrained BERT on
SNLI data, incorporating both supervised crossentropy loss and supervised
contrastive loss. Compared with baseline where fine-tuning is only done with
supervised cross-entropy loss similar to current state-of-the-art method SBERT,
our supervised contrastive method improves 2.8% in average on Semantic Textual
Similarity (STS) benchmarks and 1.05% in average on various sentence transfer
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1"&gt;Danqi Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Sexism Detection with Multilingual Transformer Models. (arXiv:2106.04908v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04908</id>
        <link href="http://arxiv.org/abs/2106.04908"/>
        <updated>2021-06-10T01:56:45.452Z</updated>
        <summary type="html"><![CDATA[Sexism has become an increasingly major problem on social networks during the
last years. The first shared task on sEXism Identification in Social neTworks
(EXIST) at IberLEF 2021 is an international competition in the field of Natural
Language Processing (NLP) with the aim to automatically identify sexism in
social media content by applying machine learning methods. Thereby sexism
detection is formulated as a coarse (binary) classification problem and a
fine-grained classification task that distinguishes multiple types of sexist
content (e.g., dominance, stereotyping, and objectification). This paper
presents the contribution of the AIT_FHSTP team at the EXIST2021 benchmark for
both tasks. To solve the tasks we applied two multilingual transformer models,
one based on multilingual BERT and one based on XLM-R. Our approach uses two
different strategies to adapt the transformers to the detection of sexist
content: first, unsupervised pre-training with additional data and second,
supervised fine-tuning with additional and augmented data. For both tasks our
best model is XLM-R with unsupervised pre-training on the EXIST data and
additional datasets and fine-tuning on the provided dataset. The best run for
the binary classification (task 1) achieves a macro F1-score of 0.7752 and
scores 5th rank in the benchmark; for the multiclass classification (task 2)
our best submission scores 6th rank with a macro F1-score of 0.5589.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mina_S/0/1/0/all/0/1"&gt;Sch&amp;#xfc;tz Mina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaqueline_B/0/1/0/all/0/1"&gt;Boeck Jaqueline&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daria_L/0/1/0/all/0/1"&gt;Liakhovets Daria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djordje_S/0/1/0/all/0/1"&gt;Slijep&amp;#x10d;evi&amp;#x107; Djordje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armin_K/0/1/0/all/0/1"&gt;Kirchknopf Armin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manuel_H/0/1/0/all/0/1"&gt;Hecht Manuel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johannes_B/0/1/0/all/0/1"&gt;Bogensperger Johannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sven_S/0/1/0/all/0/1"&gt;Schlarb Sven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexander_S/0/1/0/all/0/1"&gt;Schindler Alexander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matthias_Z/0/1/0/all/0/1"&gt;Zeppelzauer Matthias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Based Models for Code Generation under Compilability Constraints. (arXiv:2106.04985v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04985</id>
        <link href="http://arxiv.org/abs/2106.04985"/>
        <updated>2021-06-10T01:56:45.445Z</updated>
        <summary type="html"><![CDATA[Neural language models can be successfully trained on source code, leading to
applications such as code completion. However, their versatile autoregressive
self-supervision objective overlooks important global sequence-level features
that are present in the data such as syntactic correctness or compilability. In
this work, we pose the problem of learning to generate compilable code as
constraint satisfaction. We define an Energy-Based Model (EBM) representing a
pre-trained generative model with an imposed constraint of generating only
compilable sequences. We then use the KL-Adaptive Distributional Policy
Gradient algorithm (Khalifa et al., 2021) to train a generative model
approximating the EBM. We conduct experiments showing that our proposed
approach is able to improve compilability rates without sacrificing diversity
and complexity of the generated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1"&gt;Tomasz Korbak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1"&gt;Hady Elsahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dymetman_M/0/1/0/all/0/1"&gt;Marc Dymetman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1"&gt;Germ&amp;#xe1;n Kruszewski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniKeyphrase: A Unified Extraction and Generation Framework for Keyphrase Prediction. (arXiv:2106.04847v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04847</id>
        <link href="http://arxiv.org/abs/2106.04847"/>
        <updated>2021-06-10T01:56:45.429Z</updated>
        <summary type="html"><![CDATA[Keyphrase Prediction (KP) task aims at predicting several keyphrases that can
summarize the main idea of the given document. Mainstream KP methods can be
categorized into purely generative approaches and integrated models with
extraction and generation. However, these methods either ignore the diversity
among keyphrases or only weakly capture the relation across tasks implicitly.
In this paper, we propose UniKeyphrase, a novel end-to-end learning framework
that jointly learns to extract and generate keyphrases. In UniKeyphrase,
stacked relation layer and bag-of-words constraint are proposed to fully
exploit the latent semantic relation between extraction and generation in the
view of model structure and training process, respectively. Experiments on KP
benchmarks demonstrate that our joint approach outperforms mainstream methods
by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Huanqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_D/0/1/0/all/0/1"&gt;Dan Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Feng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Catchphrase: Automatic Detection of Cultural References. (arXiv:2106.04830v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04830</id>
        <link href="http://arxiv.org/abs/2106.04830"/>
        <updated>2021-06-10T01:56:45.422Z</updated>
        <summary type="html"><![CDATA[A snowclone is a customizable phrasal template that can be realized in
multiple, instantly recognized variants. For example, ``* is the new *" (Orange
is the new black, 40 is the new 30). Snowclones are extensively used in social
media. In this paper, we study snowclones originating from pop-culture quotes;
our goal is to automatically detect cultural references in text. We introduce a
new, publicly available data set of pop-culture quotes and their corresponding
snowclone usages and train models on them. We publish code for Catchphrase, an
internet browser plugin to automatically detect and mark references in
real-time, and examine its performance via a user study. Aside from assisting
people to better comprehend cultural references, we hope that detecting
snowclones can complement work on paraphrasing and help to tackle long-standing
questions in social science about the dynamics of information propagation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sweed_N/0/1/0/all/0/1"&gt;Nir Sweed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1"&gt;Dafna Shahaf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fragmented and Valuable: Following Sentiment Changes in Food Tweets. (arXiv:2106.04903v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04903</id>
        <link href="http://arxiv.org/abs/2106.04903"/>
        <updated>2021-06-10T01:56:45.416Z</updated>
        <summary type="html"><![CDATA[We analysed sentiment and frequencies related to smell, taste and temperature
expressed by food tweets in the Latvian language. To get a better understanding
of the role of smell, taste and temperature in the mental map of food
associations, we looked at such categories as 'tasty' and 'healthy', which
turned out to be mutually exclusive. By analysing the occurrence frequency of
words associated with these categories, we discovered that food discourse
overall was permeated by `tasty' while the category of 'healthy' was relatively
small. Finally, we used the analysis of temporal dynamics to see if we can
trace seasonality or other temporal aspects in smell, taste and temperature as
reflected in food tweets. Understanding the composition of social media content
with relation to smell, taste and temperature in food tweets allows us to
develop our work further - on food culture/seasonality and its relation to
temperature, on our limited capacity to express smell-related sentiments, and
the lack of the paradigm of taste in discussing food healthiness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Maija K&amp;#x101;le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rikters_M/0/1/0/all/0/1"&gt;Mat&amp;#x12b;ss Rikters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Human Evaluation for Style Transfer. (arXiv:2106.04747v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04747</id>
        <link href="http://arxiv.org/abs/2106.04747"/>
        <updated>2021-06-10T01:56:45.258Z</updated>
        <summary type="html"><![CDATA[This paper reviews and summarizes human evaluation practices described in 97
style transfer papers with respect to three main evaluation aspects: style
transfer, meaning preservation, and fluency. In principle, evaluations by human
raters should be the most reliable. However, in style transfer papers, we find
that protocols for human evaluations are often underspecified and not
standardized, which hampers the reproducibility of research in this field and
progress toward better human and automatic evaluation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Briakou_E/0/1/0/all/0/1"&gt;Eleftheria Briakou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sweta Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Ke Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1"&gt;Joel Tetreault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1"&gt;Marine Carpuat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Sample Based Explanation Methods for NLP:Efficiency, Faithfulness, and Semantic Evaluation. (arXiv:2106.04753v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04753</id>
        <link href="http://arxiv.org/abs/2106.04753"/>
        <updated>2021-06-10T01:56:45.250Z</updated>
        <summary type="html"><![CDATA[In the recent advances of natural language processing, the scale of the
state-of-the-art models and datasets is usually extensive, which challenges the
application of sample-based explanation methods in many aspects, such as
explanation interpretability, efficiency, and faithfulness. In this work, for
the first time, we can improve the interpretability of explanations by allowing
arbitrary text sequences as the explanation unit. On top of this, we implement
a hessian-free method with a model faithfulness guarantee. Finally, to compare
our method with the others, we propose a semantic-based evaluation metric that
can better align with humans' judgment of explanations than the widely adopted
diagnostic or re-training measures. The empirical results on multiple real data
sets demonstrate the proposed method's superior performance to popular
explanation techniques such as Influence Function or TracIn on semantic
evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yada Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1"&gt;Guangnan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiaodong Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes. (arXiv:2012.14210v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14210</id>
        <link href="http://arxiv.org/abs/2012.14210"/>
        <updated>2021-06-10T01:56:45.233Z</updated>
        <summary type="html"><![CDATA[Information Retrieval using dense low-dimensional representations recently
became popular and showed out-performance to traditional sparse-representations
like BM25. However, no previous work investigated how dense representations
perform with large index sizes. We show theoretically and empirically that the
performance for dense representations decreases quicker than sparse
representations for increasing index sizes. In extreme cases, this can even
lead to a tipping point where at a certain index size sparse representations
outperform dense representations. We show that this behavior is tightly
connected to the number of dimensions of the representations: The lower the
dimension, the higher the chance for false positives, i.e. returning irrelevant
documents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1"&gt;Nils Reimers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Expansion using Back Translation and Paraphrasing for Hate Speech Detection. (arXiv:2106.04681v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04681</id>
        <link href="http://arxiv.org/abs/2106.04681"/>
        <updated>2021-06-10T01:56:45.226Z</updated>
        <summary type="html"><![CDATA[With proliferation of user generated contents in social media platforms,
establishing mechanisms to automatically identify toxic and abusive content
becomes a prime concern for regulators, researchers, and society. Keeping the
balance between freedom of speech and respecting each other dignity is a major
concern of social media platform regulators. Although, automatic detection of
offensive content using deep learning approaches seems to provide encouraging
results, training deep learning-based models requires large amounts of
high-quality labeled data, which is often missing. In this regard, we present
in this paper a new deep learning-based method that fuses a Back Translation
method, and a Paraphrasing technique for data augmentation. Our pipeline
investigates different word-embedding-based architectures for classification of
hate speech. The back translation technique relies on an encoder-decoder
architecture pre-trained on a large corpus and mostly used for machine
translation. In addition, paraphrasing exploits the transformer model and the
mixture of experts to generate diverse paraphrases. Finally, LSTM, and CNN are
compared to seek enhanced classification results. We evaluate our proposal on
five publicly available datasets; namely, AskFm corpus, Formspring dataset,
Warner and Waseem dataset, Olid, and Wikipedia toxic comments dataset. The
performance of the proposal together with comparison to some related
state-of-art results demonstrate the effectiveness and soundness of our
proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beddiar_D/0/1/0/all/0/1"&gt;Djamila Romaissa Beddiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jahan_M/0/1/0/all/0/1"&gt;Md Saroar Jahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oussalah_M/0/1/0/all/0/1"&gt;Mourad Oussalah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DravidianMultiModality: A Dataset for Multi-modal Sentiment Analysis in Tamil and Malayalam. (arXiv:2106.04853v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04853</id>
        <link href="http://arxiv.org/abs/2106.04853"/>
        <updated>2021-06-10T01:56:45.214Z</updated>
        <summary type="html"><![CDATA[Human communication is inherently multimodal and asynchronous. Analyzing
human emotions and sentiment is an emerging field of artificial intelligence.
We are witnessing an increasing amount of multimodal content in local languages
on social media about products and other topics. However, there are not many
multimodal resources available for under-resourced Dravidian languages. Our
study aims to create a multimodal sentiment analysis dataset for the
under-resourced Tamil and Malayalam languages. First, we downloaded product or
movies review videos from YouTube for Tamil and Malayalam. Next, we created
captions for the videos with the help of annotators. Then we labelled the
videos for sentiment, and verified the inter-annotator agreement using Fleiss's
Kappa. This is the first multimodal sentiment analysis dataset for Tamil and
Malayalam by volunteer annotators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1"&gt;Bharathi Raja Chakravarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_J/0/1/0/all/0/1"&gt;Jishnu Parameswaran P.K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+B_P/0/1/0/all/0/1"&gt;Premjith B&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soman_K/0/1/0/all/0/1"&gt;K.P Soman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponnusamy_R/0/1/0/all/0/1"&gt;Rahul Ponnusamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumaresan_P/0/1/0/all/0/1"&gt;Prasanna Kumar Kumaresan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1"&gt;Kingston Pal Thamburaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1"&gt;John P. McCrae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastSeq: Make Sequence Generation Faster. (arXiv:2106.04718v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04718</id>
        <link href="http://arxiv.org/abs/2106.04718"/>
        <updated>2021-06-10T01:56:45.207Z</updated>
        <summary type="html"><![CDATA[Transformer-based models have made tremendous impacts in natural language
generation. However the inference speed is a bottleneck due to large model size
and intensive computing involved in auto-regressive decoding process. We
develop FastSeq framework to accelerate sequence generation without accuracy
loss. The proposed optimization techniques include an attention cache
optimization, an efficient algorithm for detecting repeated n-grams, and an
asynchronous generation pipeline with parallel I/O. These optimizations are
general enough to be applicable to Transformer-based models (e.g., T5, GPT2,
and UniLM). Our benchmark results on a set of widely used and diverse models
demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use
with a simple one-line code change. The source code is available at
https://github.com/microsoft/fastseq.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1"&gt;Fei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1"&gt;Nikhil Bhendawade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1"&gt;Ting Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_D/0/1/0/all/0/1"&gt;Desheng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1"&gt;Bingyu Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruifei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp. (arXiv:2106.04726v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.04726</id>
        <link href="http://arxiv.org/abs/2106.04726"/>
        <updated>2021-06-10T01:56:45.187Z</updated>
        <summary type="html"><![CDATA[WhatsApp is a popular chat application used by over 2 billion users
worldwide. However, due to end-to-end encryption, there is currently no easy
way to fact-check content on WhatsApp at scale. In this paper, we analyze the
usefulness of a crowd-sourced system on WhatsApp through which users can submit
"tips" containing messages they want fact-checked. We compare the tips sent to
a WhatsApp tipline run during the 2019 Indian national elections with the
messages circulating in large, public groups on WhatsApp and other social media
platforms during the same period. We find that tiplines are a very useful lens
into WhatsApp conversations: a significant fraction of messages and images sent
to the tipline match with the content being shared on public WhatsApp groups
and other social media. Our analysis also shows that tiplines cover the most
popular content well, and a majority of such content is often shared to the
tipline before appearing in large, public WhatsApp groups. Overall, the
analysis suggests tiplines can be an effective source for discovering content
to fact-check.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1"&gt;Ashkan Kazemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1"&gt;Kiran Garimella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1"&gt;Gautam Kishore Shahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1"&gt;Devin Gaffney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1"&gt;Scott A. Hale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Open-World Recommendation: An Inductive Model-based Collaborative Filtering Approach. (arXiv:2007.04833v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04833</id>
        <link href="http://arxiv.org/abs/2007.04833"/>
        <updated>2021-06-10T01:56:45.180Z</updated>
        <summary type="html"><![CDATA[Recommendation models can effectively estimate underlying user interests and
predict one's future behaviors by factorizing an observed user-item rating
matrix into products of two sets of latent factors. However, the user-specific
embedding factors can only be learned in a transductive way, making it
difficult to handle new users on-the-fly. In this paper, we propose an
inductive collaborative filtering framework that contains two representation
models. The first model follows conventional matrix factorization which
factorizes a group of key users' rating matrix to obtain meta latents. The
second model resorts to attention-based structure learning that estimates
hidden relations from query to key users and learns to leverage meta latents to
inductively compute embeddings for query users via neural message passing. Our
model enables inductive representation learning for users and meanwhile
guarantees equivalent representation capacity as matrix factorization.
Experiments demonstrate that our model achieves promising results for
recommendation on few-shot users with limited training ratings and new unseen
users which are commonly encountered in open-world recommender systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qitian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hengrui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiaofeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Balancing Reinforcement Learning Training Experiences in Interactive Information Retrieval. (arXiv:2006.03185v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03185</id>
        <link href="http://arxiv.org/abs/2006.03185"/>
        <updated>2021-06-10T01:56:45.173Z</updated>
        <summary type="html"><![CDATA[Interactive Information Retrieval (IIR) and Reinforcement Learning (RL) share
many commonalities, including an agent who learns while interacts, a long-term
and complex goal, and an algorithm that explores and adapts. To successfully
apply RL methods to IIR, one challenge is to obtain sufficient relevance labels
to train the RL agents, which are infamously known as sample inefficient.
However, in a text corpus annotated for a given query, it is not the relevant
documents but the irrelevant documents that predominate. This would cause very
unbalanced training experiences for the agent and prevent it from learning any
policy that is effective. Our paper addresses this issue by using domain
randomization to synthesize more relevant documents for the training. Our
experimental results on the Text REtrieval Conference (TREC) Dynamic Domain
(DD) 2017 Track show that the proposed method is able to boost an RL agent's
learning effectiveness by 22\% in dealing with unseen situations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Limin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhiwen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Grace Hui Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepTileBars: Visualizing Term Distribution for Neural Information Retrieval. (arXiv:1811.00606v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.00606</id>
        <link href="http://arxiv.org/abs/1811.00606"/>
        <updated>2021-06-10T01:56:45.165Z</updated>
        <summary type="html"><![CDATA[Most neural Information Retrieval (Neu-IR) models derive query-to-document
ranking scores based on term-level matching. Inspired by TileBars, a classical
term distribution visualization method, in this paper, we propose a novel
Neu-IR model that handles query-to-document matching at the subtopic and higher
levels. Our system first splits the documents into topical segments,
"visualizes" the matchings between the query and the segments, and then feeds
an interaction matrix into a Neu-IR model, DeepTileBars, to obtain the final
ranking scores. DeepTileBars models the relevance signals occurring at
different granularities in a document's topic hierarchy. It better captures the
discourse structure of a document and thus the matching patterns. Although its
design and implementation are light-weight, DeepTileBars outperforms other
state-of-the-art Neu-IR models on benchmark datasets including the Text
REtrieval Conference (TREC) 2010-2012 Web Tracks and LETOR 4.0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhiwen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Grace Hui Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Context Enhanced Graph Neural Networks for Session-based Recommendation. (arXiv:2106.05081v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.05081</id>
        <link href="http://arxiv.org/abs/2106.05081"/>
        <updated>2021-06-10T01:56:45.154Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation (SBR) is a challenging task, which aims at
recommending items based on anonymous behavior sequences. Almost all the
existing solutions for SBR model user preference only based on the current
session without exploiting the other sessions, which may contain both relevant
and irrelevant item-transitions to the current session. This paper proposes a
novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN)
to exploit item transitions over all sessions in a more subtle manner for
better inferring the user preference of the current session. Specifically,
GCE-GNN learns two levels of item embeddings from session graph and global
graph, respectively: (i) Session graph, which is to learn the session-level
item embedding by modeling pairwise item-transitions within the current
session; and (ii) Global graph, which is to learn the global-level item
embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN,
we propose a novel global-level item representation learning layer, which
employs a session-aware attention mechanism to recursively incorporate the
neighbors' embeddings of each node on the global graph. We also design a
session-level item representation learning layer, which employs a GNN on the
session graph to learn session-level item embeddings within the current
session. Moreover, GCE-GNN aggregates the learnt item representations in the
two levels with a soft attention mechanism. Experiments on three benchmark
datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods
consistently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1"&gt;Gao Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiao-Li Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xian-Ling Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1"&gt;Minghui Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoFT: Automatic Fine-Tune for Parameters Transfer Learning in Click-Through Rate Prediction. (arXiv:2106.04873v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04873</id>
        <link href="http://arxiv.org/abs/2106.04873"/>
        <updated>2021-06-10T01:56:45.129Z</updated>
        <summary type="html"><![CDATA[Recommender systems are often asked to serve multiple recommendation
scenarios or domains. Fine-tuning a pre-trained CTR model from source domains
and adapting it to a target domain allows knowledge transferring. However,
optimizing all the parameters of the pre-trained network may result in
over-fitting if the target dataset is small and the number of parameters is
large. This leads us to think of directly reusing parameters in the pre-trained
model which represent more general features learned from multiple domains.
However, the design of freezing or fine-tuning layers of parameters requires
much manual effort since the decision highly depends on the pre-trained model
and target instances. In this work, we propose an end-to-end transfer learning
framework, called Automatic Fine-Tuning (AutoFT), for CTR prediction. AutoFT
consists of a field-wise transfer policy and a layer-wise transfer policy. The
field-wise transfer policy decides how the pre-trained embedding
representations are frozen or fine-tuned based on the given instance from the
target domain. The layer-wise transfer policy decides how the high?order
feature representations are transferred layer by layer. Extensive experiments
on two public benchmark datasets and one private industrial dataset demonstrate
that AutoFT can significantly improve the performance of CTR prediction
compared with state-of-the-art transferring approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiangli Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1"&gt;Rong Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhirong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Corpus-Level End-to-End Exploration for Interactive Systems. (arXiv:1912.00753v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.00753</id>
        <link href="http://arxiv.org/abs/1912.00753"/>
        <updated>2021-06-10T01:56:45.121Z</updated>
        <summary type="html"><![CDATA[A core interest in building Artificial Intelligence (AI) agents is to let
them interact with and assist humans. One example is Dynamic Search (DS), which
models the process that a human works with a search engine agent to accomplish
a complex and goal-oriented task. Early DS agents using Reinforcement Learning
(RL) have only achieved limited success for (1) their lack of direct control
over which documents to return and (2) the difficulty to recover from wrong
search trajectories. In this paper, we present a novel corpus-level end-to-end
exploration (CE3) method to address these issues. In our method, an entire text
corpus is compressed into a global low-dimensional representation, which
enables the agent to gain access to the full state and action spaces, including
the under-explored areas. We also propose a new form of retrieval function,
whose linear approximation allows end-to-end manipulation of documents.
Experiments on the Text REtrieval Conference (TREC) Dynamic Domain (DD) Track
show that CE3 outperforms the state-of-the-art DS systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhiwen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Grace Hui Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAM: Understanding Product Images in Cross Product Category Attribute Extraction. (arXiv:2106.04630v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04630</id>
        <link href="http://arxiv.org/abs/2106.04630"/>
        <updated>2021-06-10T01:56:45.112Z</updated>
        <summary type="html"><![CDATA[Understanding product attributes plays an important role in improving online
shopping experience for customers and serves as an integral part for
constructing a product knowledge graph. Most existing methods focus on
attribute extraction from text description or utilize visual information from
product images such as shape and color. Compared to the inputs considered in
prior works, a product image in fact contains more information, represented by
a rich mixture of words and visual clues with a layout carefully designed to
impress customers. This work proposes a more inclusive framework that fully
utilizes these different modalities for attribute extraction. Inspired by
recent works in visual question answering, we use a transformer based sequence
to sequence model to fuse representations of product text, Optical Character
Recognition (OCR) tokens and visual objects detected in the product image. The
framework is further extended with the capability to extract attribute value
across multiple product categories with a single model, by training the decoder
to predict both product category and attribute value and conditioning its
output on product category. The model provides a unified attribute extraction
solution desirable at an e-commerce platform that offers numerous product
categories with a diverse body of product attributes. We evaluated the model on
two product attributes, one with many possible values and one with a small set
of possible values, over 14 product categories and found the model could
achieve 15% gain on the Recall and 10% gain on the F1 score compared to
existing methods using text-only features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jie Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1"&gt;Nasser Zalmout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Luna Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-Server Private Linear Transformation: The Individual Privacy Case. (arXiv:2106.05222v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.05222</id>
        <link href="http://arxiv.org/abs/2106.05222"/>
        <updated>2021-06-10T01:56:45.102Z</updated>
        <summary type="html"><![CDATA[This paper considers the single-server Private Linear Transformation (PLT)
problem with individual privacy guarantees. In this problem, there is a user
that wishes to obtain $L$ independent linear combinations of a $D$-subset of
messages belonging to a dataset of $K$ messages stored on a single server. The
goal is to minimize the download cost while keeping the identity of each
message required for the computation individually private. The individual
privacy requirement ensures that the identity of each individual message
required for the computation is kept private. This is in contrast to the
stricter notion of joint privacy that protects the entire set of identities of
all messages used for the computation, including the correlations between these
identities. The notion of individual privacy captures a broad set of practical
applications. For example, such notion is relevant when the dataset contains
information about individuals, each of them requires privacy guarantees for
their data access patterns. We focus on the setting in which the required
linear transformation is associated with a maximum distance separable (MDS)
matrix. In particular, we require that the matrix of coefficients pertaining to
the required linear combinations is the generator matrix of an MDS code. We
establish lower and upper bounds on the capacity of PLT with individual
privacy, where the capacity is defined as the supremum of all achievable
download rates. We show that our bounds are tight under certain conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1"&gt;Anoosheh Heidarzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmati_N/0/1/0/all/0/1"&gt;Nahid Esmati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sprintson_A/0/1/0/all/0/1"&gt;Alex Sprintson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIGRAC: Digraph Clustering with Flow Imbalance. (arXiv:2106.05194v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.05194</id>
        <link href="http://arxiv.org/abs/2106.05194"/>
        <updated>2021-06-10T01:56:45.093Z</updated>
        <summary type="html"><![CDATA[Node clustering is a powerful tool in the analysis of networks. Here, we
introduce a graph neural network framework with a novel scalable Directed Mixed
Path Aggregation(DIMPA) scheme to obtain node embeddings for directed networks
in a self-supervised manner, including a novel probabilistic imbalance loss.
The method is end-to-end in combining embedding generation and clustering
without an intermediate step. In contrast to standard approaches in the
literature, in this paper, directionality is not treated as a nuisance, but
rather contains the main signal. In particular, we leverage the recently
introduced cut flow imbalance measure, which is tightly related to
directionality; cut flow imbalance is optimized without resorting to spectral
methods or cluster labels. Experimental results on synthetic data, in the form
of directed stochastic block models and real-world data at different scales,
demonstrate that our method attains state-of-the-art results on directed
clustering, for a wide range of noise and sparsity levels, as well as graph
structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+He_Y/0/1/0/all/0/1"&gt;Yixuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Reinert_G/0/1/0/all/0/1"&gt;Gesine Reinert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cucuringu_M/0/1/0/all/0/1"&gt;Mihai Cucuringu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04641</id>
        <link href="http://arxiv.org/abs/2106.04641"/>
        <updated>2021-06-10T01:56:45.067Z</updated>
        <summary type="html"><![CDATA[Transfer learning methods, and in particular domain adaptation, help exploit
labeled data in one domain to improve the performance of a certain task in
another domain. However, it is still not clear what factors affect the success
of domain adaptation. This paper models adaptation success and selection of the
most suitable source domains among several candidates in text similarity. We
use descriptive domain information and cross-domain similarity metrics as
predictive features. While mostly positive, the results also point to some
domains where adaptation success was difficult to predict.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1"&gt;Nicolai Pogrebnyakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1"&gt;Shohreh Shaghaghian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Helping results assessment by adding explainable elements to the deep relevance matching model. (arXiv:2106.05147v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.05147</id>
        <link href="http://arxiv.org/abs/2106.05147"/>
        <updated>2021-06-10T01:56:45.056Z</updated>
        <summary type="html"><![CDATA[In this paper we address the explainability of web search engines. We propose
two explainable elements on the search engine result page: a visualization of
query term weights and a visualization of passage relevance. The idea is that
search engines that indicate to the user why results are retrieved are valued
higher by users and gain user trust. We deduce the query term weights from the
term gating network in the Deep Relevance Matching Model (DRMM) and visualize
them as a doughnut chart. In addition, we train a passage-level ranker with
DRMM that selects the most relevant passage from each document and shows it as
snippet on the result page. Next to the snippet we show a document thumbnail
with this passage highlighted. We evaluate the proposed interface in an online
user study, asking users to judge the explainability and assessability of the
interface. We found that users judge our proposed interface significantly more
explainable and easier to assess than a regular search engine result page.
However, they are not significantly better in selecting the relevant documents
from the top-5. This indicates that the explainability of the search engine
result page leads to a better user experience. Thus, we conclude that the
proposed explainable elements are promising as visualization for search engine
users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chios_I/0/1/0/all/0/1"&gt;Ioannis Chios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1"&gt;Suzan Verberne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sirius: A Mutual Information Tool for Exploratory Visualization of Mixed Data. (arXiv:2106.05260v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2106.05260</id>
        <link href="http://arxiv.org/abs/2106.05260"/>
        <updated>2021-06-10T01:56:45.043Z</updated>
        <summary type="html"><![CDATA[Data scientists across disciplines are increasingly in need of exploratory
analysis tools for data sets with a high volume of features. We expand upon
graph mining approaches for exploratory analysis of high-dimensional data to
introduce Sirius, a visualization package for researchers to explore feature
relationships among mixed data types using mutual information and network
backbone sparsification. Visualizations of feature relationships aid data
scientists in finding meaningful dependence among features, which can engender
further analysis for feature selection, feature extraction, projection,
identification of proxy variables, or insight into temporal variation at the
macro scale. Graph mining approaches for feature analysis exist, such as
association networks of binary features, or correlation networks of
quantitative features, but mixed data types present a unique challenge for
developing comprehensive feature networks for exploratory analysis. Using an
information theoretic approach, Sirius supports heterogeneous data sets
consisting of binary, continuous quantitative, and discrete categorical data
types, and provides a user interface exploring feature pairs with high mutual
information scores. We leverage a backbone sparsification approach from network
theory as a dimensionality reduction technique, which probabilistically trims
edges according to the local network context. Sirius is an open source Python
package and Django web application for exploratory visualization, which can be
deployed in data analysis pipelines. The Sirius codebase and exemplary data
sets can be found at: https://github.com/compstorylab/sirius]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Adams_J/0/1/0/all/0/1"&gt;Jane L. Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deluca_T/0/1/0/all/0/1"&gt;Todd F. Deluca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Danforth_C/0/1/0/all/0/1"&gt;Christopher M. Danforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dodds_P/0/1/0/all/0/1"&gt;Peter S. Dodds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuhang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Anastasakis_K/0/1/0/all/0/1"&gt;Konstantinos Anastasakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Choi_B/0/1/0/all/0/1"&gt;Boyoon Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Min_A/0/1/0/all/0/1"&gt;Allison Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bessey_M/0/1/0/all/0/1"&gt;Michael M. Bessey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compacter: Efficient Low-Rank Hypercomplex Adapter Layers. (arXiv:2106.04647v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04647</id>
        <link href="http://arxiv.org/abs/2106.04647"/>
        <updated>2021-06-10T01:56:45.020Z</updated>
        <summary type="html"><![CDATA[Adapting large-scale pretrained language models to downstream tasks via
fine-tuning is the standard method for achieving state-of-the-art performance
on NLP benchmarks. However, fine-tuning all weights of models with millions or
billions of parameters is sample-inefficient, unstable in low-resource
settings, and wasteful as it requires storing a separate copy of the model for
each task. Recent work has developed parameter-efficient fine-tuning methods,
but these approaches either still require a relatively large number of
parameters or underperform standard fine-tuning. In this work, we propose
Compacter, a method for fine-tuning large-scale language models with a better
trade-off between task performance and the number of trainable parameters than
prior work. Compacter accomplishes this by building on top of ideas from
adapters, low-rank optimization, and parameterized hypercomplex multiplication
layers.

Specifically, Compacter inserts task-specific weight matrices into a
pretrained model's weights, which are computed efficiently as a sum of
Kronecker products between shared ``slow'' weights and ``fast'' rank-one
matrices defined per Compacter layer. By only training 0.047% of a pretrained
model's parameters, Compacter performs on par with standard fine-tuning on GLUE
and outperforms fine-tuning in low-resource settings. Our code is publicly
available in https://github.com/rabeehk/compacter/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahabadi_R/0/1/0/all/0/1"&gt;Rabeeh Karimi Mahabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1"&gt;James Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1"&gt;Sebastian Ruder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential End-to-End Intent and Slot Label Classification and Localization. (arXiv:2106.04660v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04660</id>
        <link href="http://arxiv.org/abs/2106.04660"/>
        <updated>2021-06-10T01:56:44.974Z</updated>
        <summary type="html"><![CDATA[Human-computer interaction (HCI) is significantly impacted by delayed
responses from a spoken dialogue system. Hence, end-to-end (e2e) spoken
language understanding (SLU) solutions have recently been proposed to decrease
latency. Such approaches allow for the extraction of semantic information
directly from the speech signal, thus bypassing the need for a transcript from
an automatic speech recognition (ASR) system. In this paper, we propose a
compact e2e SLU architecture for streaming scenarios, where chunks of the
speech signal are processed continuously to predict intent and slot values. Our
model is based on a 3D convolutional neural network (3D-CNN) and a
unidirectional long short-term memory (LSTM). We compare the performance of two
alignment-free losses: the connectionist temporal classification (CTC) method
and its adapted version, namely connectionist temporal localization (CTL). The
latter performs not only the classification but also localization of sequential
audio events. The proposed solution is evaluated on the Fluent Speech Command
dataset and results show our model ability to process incoming speech signal,
reaching accuracy as high as 98.97 % for CTC and 98.78 % for CTL on
single-label classification, and as high as 95.69 % for CTC and 95.28 % for CTL
on two-label prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yiran Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1"&gt;Nihal Potdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1"&gt;Anderson R. Avila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Rank Words: Optimizing Ranking Metrics for Word Spotting. (arXiv:2106.05144v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.05144</id>
        <link href="http://arxiv.org/abs/2106.05144"/>
        <updated>2021-06-10T01:56:44.964Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore and evaluate the use of ranking-based objective
functions for learning simultaneously a word string and a word image encoder.
We consider retrieval frameworks in which the user expects a retrieval list
ranked according to a defined relevance score. In the context of a word
spotting problem, the relevance score has been set according to the string edit
distance from the query string. We experimentally demonstrate the competitive
performance of the proposed model on query-by-string word spotting for both,
handwritten and real scene word images. We also provide the results for
query-by-example word spotting, although it is not the main focus of this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1"&gt;Pau Riba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1"&gt;Adri&amp;#xe0; Molina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1"&gt;Lluis Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramos_Terrades_O/0/1/0/all/0/1"&gt;Oriol Ramos-Terrades&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1"&gt;Josep Llad&amp;#xf3;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comprehension Based Question Answering using Bloom's Taxonomy. (arXiv:2106.04653v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04653</id>
        <link href="http://arxiv.org/abs/2106.04653"/>
        <updated>2021-06-10T01:56:44.947Z</updated>
        <summary type="html"><![CDATA[Current pre-trained language models have lots of knowledge, but a more
limited ability to use that knowledge. Bloom's Taxonomy helps educators teach
children how to use knowledge by categorizing comprehension skills, so we use
it to analyze and improve the comprehension skills of large pre-trained
language models. Our experiments focus on zero-shot question answering, using
the taxonomy to provide proximal context that helps the model answer questions
by being relevant to those questions. We show targeting context in this manner
improves performance across 4 popular common sense question answer datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_P/0/1/0/all/0/1"&gt;Pritish Sahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1"&gt;Michael Cogswell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rutherford_Quach_S/0/1/0/all/0/1"&gt;Sara Rutherford-Quach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1"&gt;Ajay Divakaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-Server Private Linear Transformation: The Joint Privacy Case. (arXiv:2106.05220v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.05220</id>
        <link href="http://arxiv.org/abs/2106.05220"/>
        <updated>2021-06-10T01:56:44.923Z</updated>
        <summary type="html"><![CDATA[This paper introduces the problem of Private Linear Transformation (PLT)
which generalizes the problems of private information retrieval and private
linear computation. The PLT problem includes one or more remote server(s)
storing (identical copies of) $K$ messages and a user who wants to compute $L$
independent linear combinations of a $D$-subset of messages. The objective of
the user is to perform the computation by downloading minimum possible amount
of information from the server(s), while protecting the identities of the $D$
messages required for the computation. In this work, we focus on the
single-server setting of the PLT problem when the identities of the $D$
messages required for the computation must be protected jointly. We consider
two different models, depending on whether the coefficient matrix of the
required $L$ linear combinations generates a Maximum Distance Separable (MDS)
code. We prove that the capacity for both models is given by $L/(K-D+L)$, where
the capacity is defined as the supremum of all achievable download rates. Our
converse proofs are based on linear-algebraic and information-theoretic
arguments that establish connections between PLT schemes and linear codes. We
also present an achievability scheme for each of the models being considered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1"&gt;Anoosheh Heidarzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmati_N/0/1/0/all/0/1"&gt;Nahid Esmati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sprintson_A/0/1/0/all/0/1"&gt;Alex Sprintson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Extractive Search. (arXiv:2106.04612v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04612</id>
        <link href="http://arxiv.org/abs/2106.04612"/>
        <updated>2021-06-10T01:56:44.908Z</updated>
        <summary type="html"><![CDATA[Domain experts often need to extract structured information from large
corpora. We advocate for a search paradigm called ``extractive search'', in
which a search query is enriched with capture-slots, to allow for such rapid
extraction. Such an extractive search system can be built around syntactic
structures, resulting in high-precision, low-recall results. We show how the
recall can be improved using neural retrieval and alignment. The goals of this
paper are to concisely introduce the extractive-search paradigm; and to
demonstrate a prototype neural retrieval system for extractive search and its
benefits and potential. Our prototype is available at
\url{https://spike.neural-sim.apps.allenai.org/} and a video demonstration is
available at \url{https://vimeo.com/559586687}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1"&gt;Shauli Ravfogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taub_Tabib_H/0/1/0/all/0/1"&gt;Hillel Taub-Tabib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Initialization Matters: Regularizing Manifold-informed Initialization for Neural Recommendation Systems. (arXiv:2106.04993v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04993</id>
        <link href="http://arxiv.org/abs/2106.04993"/>
        <updated>2021-06-10T01:56:44.870Z</updated>
        <summary type="html"><![CDATA[Proper initialization is crucial to the optimization and the generalization
of neural networks. However, most existing neural recommendation systems
initialize the user and item embeddings randomly. In this work, we propose a
new initialization scheme for user and item embeddings called Laplacian
Eigenmaps with Popularity-based Regularization for Isolated Data (LEPORID).
LEPORID endows the embeddings with information regarding multi-scale
neighborhood structures on the data manifold and performs adaptive
regularization to compensate for high embedding variance on the tail of the
data distribution. Exploiting matrix sparsity, LEPORID embeddings can be
computed efficiently. We evaluate LEPORID in a wide range of neural
recommendation models. In contrast to the recent surprising finding that the
simple K-nearest-neighbor (KNN) method often outperforms neural recommendation
systems, we show that existing neural systems initialized with LEPORID often
perform on par or better than KNN. To maximize the effects of the
initialization, we propose the Dual-Loss Residual Recommendation (DLR2)
network, which, when initialized with LEPORID, substantially outperforms both
traditional and state-of-the-art neural recommender systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yinan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization for Document Authentication against Practical Recapturing Attacks. (arXiv:2101.01404v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01404</id>
        <link href="http://arxiv.org/abs/2101.01404"/>
        <updated>2021-06-10T01:56:44.423Z</updated>
        <summary type="html"><![CDATA[Recapturing attack can be employed as a simple but effective anti-forensic
tool for digital document images. Inspired by the document inspection process
that compares a questioned document against a reference sample, we proposed a
document recapture detection scheme by employing Siamese network to compare and
extract distinct features in a recapture document image. The proposed algorithm
takes advantages of both metric learning and image forensic techniques. Instead
of adopting Euclidean distance-based loss function, we integrate the forensic
similarity function with a triplet loss and a normalized softmax loss. After
training with the proposed triplet selection strategy, the resulting feature
embedding clusters the genuine samples near the reference while pushes the
recaptured samples apart. In the experiment, we consider practical domain
generalization problems, such as the variations in printing/imaging devices,
substrates, recapturing channels, and document types. To evaluate the
robustness of different approaches, we benchmark some popular off-the-shelf
machine learning-based approaches, a state-of-the-art document image detection
scheme, and the proposed schemes with different network backbones under various
experimental protocols. Experimental results show that the proposed schemes
with different network backbones have consistently outperformed the
state-of-the-art approaches under different experimental settings.
Specifically, under the most challenging scenario in our experiment, i.e.,
evaluation across different types of documents that produced by different
devices, we have achieved less than 5.00% APCER (Attack Presentation
Classification Error Rate) and 5.56% BPCER (Bona Fide Presentation
Classification Error Rate) by the proposed network with ResNeXt101 backbone at
5% BPCER decision threshold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changsheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuzheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_F/0/1/0/all/0/1"&gt;Fengbo Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiwu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Launchpad: A Programming Model for Distributed Machine Learning Research. (arXiv:2106.04516v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.04516</id>
        <link href="http://arxiv.org/abs/2106.04516"/>
        <updated>2021-06-10T00:27:39.761Z</updated>
        <summary type="html"><![CDATA[A major driver behind the success of modern machine learning algorithms has
been their ability to process ever-larger amounts of data. As a result, the use
of distributed systems in both research and production has become increasingly
prevalent as a means to scale to this growing data. At the same time, however,
distributing the learning process can drastically complicate the implementation
of even simple algorithms. This is especially problematic as many machine
learning practitioners are not well-versed in the design of distributed
systems, let alone those that have complicated communication topologies. In
this work we introduce Launchpad, a programming model that simplifies the
process of defining and launching distributed systems that is specifically
tailored towards a machine learning audience. We describe our framework, its
design philosophy and implementation, and give a number of examples of common
learning algorithms whose designs are greatly simplified by this approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barth_Maron_G/0/1/0/all/0/1"&gt;Gabriel Barth-Maron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanczyk_P/0/1/0/all/0/1"&gt;Piotr Sta&amp;#x144;czyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1"&gt;Matthew Hoffman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kroiss_M/0/1/0/all/0/1"&gt;Manuel Kroiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pope_A/0/1/0/all/0/1"&gt;Aedan Pope&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rrustemi_A/0/1/0/all/0/1"&gt;Alban Rrustemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Theoretic Measures for Fairness-aware Feature Selection. (arXiv:2106.00772v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00772</id>
        <link href="http://arxiv.org/abs/2106.00772"/>
        <updated>2021-06-09T22:43:50.610Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms are increasingly used for consequential decision
making regarding individuals based on their relevant features. Features that
are relevant for accurate decisions may however lead to either explicit or
implicit forms of discrimination against unprivileged groups, such as those of
certain race or gender. This happens due to existing biases in the training
data, which are often replicated or even exacerbated by the learning algorithm.
Identifying and measuring these biases at the data level is a challenging
problem due to the interdependence among the features, and the decision
outcome. In this work, we develop a framework for fairness-aware feature
selection which takes into account the correlation among the features and the
decision outcome, and is based on information theoretic measures for the
accuracy and discriminatory impacts of features. In particular, we first
propose information theoretic measures which quantify the impact of different
subsets of features on the accuracy and discrimination of the decision
outcomes. We then deduce the marginal impact of each feature using Shapley
value function; a solution concept in cooperative game theory used to estimate
marginal contributions of players in a coalitional game. Finally, we design a
fairness utility score for each feature (for feature selection) which
quantifies how this feature influences accurate as well as nondiscriminatory
decisions. Our framework depends on the joint statistics of the data rather
than a particular classifier design. We examine our proposed framework on real
and synthetic data to evaluate its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khodadadian_S/0/1/0/all/0/1"&gt;Sajad Khodadadian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nafea_M/0/1/0/all/0/1"&gt;Mohamed Nafea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1"&gt;AmirEmad Ghassami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1"&gt;Negar Kiyavash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Active Surface Models. (arXiv:2011.08826v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08826</id>
        <link href="http://arxiv.org/abs/2011.08826"/>
        <updated>2021-06-09T22:43:50.598Z</updated>
        <summary type="html"><![CDATA[Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1"&gt;Udaranga Wickramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knott_G/0/1/0/all/0/1"&gt;Graham Knott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalizable Approach to Learning Optimizers. (arXiv:2106.00958v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00958</id>
        <link href="http://arxiv.org/abs/2106.00958"/>
        <updated>2021-06-09T22:43:50.588Z</updated>
        <summary type="html"><![CDATA[A core issue with learning to optimize neural networks has been the lack of
generalization to real world problems. To address this, we describe a system
designed from a generalization-first perspective, learning to update optimizer
hyperparameters instead of model parameters directly using novel features,
actions, and a reward function. This system outperforms Adam at all neural
network tasks including on modalities not seen during training. We achieve 2x
speedups on ImageNet, and a 2.5x speedup on a language modeling task using over
5 orders of magnitude more compute than the training tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Almeida_D/0/1/0/all/0/1"&gt;Diogo Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1"&gt;Clemens Winter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaremba_W/0/1/0/all/0/1"&gt;Wojciech Zaremba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrinsic Bias Metrics Do Not Correlate with Application Bias. (arXiv:2012.15859v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15859</id>
        <link href="http://arxiv.org/abs/2012.15859"/>
        <updated>2021-06-09T22:43:50.385Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing (NLP) systems learn harmful societal biases that
cause them to amplify inequality as they are deployed in more and more
situations. To guide efforts at debiasing these systems, the NLP community
relies on a variety of metrics that quantify bias in models. Some of these
metrics are intrinsic, measuring bias in word embedding spaces, and some are
extrinsic, measuring bias in downstream tasks that the word embeddings enable.
Do these intrinsic and extrinsic metrics correlate with each other? We compare
intrinsic and extrinsic metrics across hundreds of trained models covering
different tasks and experimental conditions. Our results show no reliable
correlation between these metrics that holds in all scenarios across tasks and
languages. We urge researchers working on debiasing to focus on extrinsic
measures of bias, and to make using these measures more feasible via creation
of new challenge sets and annotated test data. To aid this effort, we release
code, a new intrinsic metric, and an annotated test set focused on gender bias
in hate speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goldfarb_Tarrant_S/0/1/0/all/0/1"&gt;Seraphina Goldfarb-Tarrant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marchant_R/0/1/0/all/0/1"&gt;Rebecca Marchant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1"&gt;Ricardo Mu&amp;#xf1;oz Sanchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandya_M/0/1/0/all/0/1"&gt;Mugdha Pandya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1"&gt;Adam Lopez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Active Surface Models. (arXiv:2011.08826v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08826</id>
        <link href="http://arxiv.org/abs/2011.08826"/>
        <updated>2021-06-09T22:43:50.359Z</updated>
        <summary type="html"><![CDATA[Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1"&gt;Udaranga Wickramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knott_G/0/1/0/all/0/1"&gt;Graham Knott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic. (arXiv:2101.01785v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01785</id>
        <link href="http://arxiv.org/abs/2101.01785"/>
        <updated>2021-06-09T22:43:50.339Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (LMs) are currently integral to many natural
language processing systems. Although multilingual LMs were also introduced to
serve many languages, these have limitations such as being costly at inference
time and the size and diversity of non-English data involved in their
pre-training. We remedy these issues for a collection of diverse Arabic
varieties by introducing two powerful deep bidirectional transformer-based
models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a
new benchmark for multi-dialectal Arabic language understanding evaluation.
ARLUE is built using 42 datasets targeting six different task clusters,
allowing us to offer a series of standardized experiments under rich
conditions. When fine-tuned on ARLUE, our models collectively achieve new
state-of-the-art results across the majority of tasks (37 out of 48
classification tasks, on the 42 datasets). Our best model acquires the highest
ARLUE score (77.40) across all six task clusters, outperforming all other
models including XLM-R Large (~ 3.4 x larger size). Our models are publicly
available at https://github.com/UBC-NLP/marbert and ARLUE will be released
through the same repository.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1"&gt;Muhammad Abdul-Mageed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1"&gt;AbdelRahim Elmadany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1"&gt;El Moatez Billah Nagoudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Sparse Attention more Interpretable?. (arXiv:2106.01087v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01087</id>
        <link href="http://arxiv.org/abs/2106.01087"/>
        <updated>2021-06-09T22:43:50.188Z</updated>
        <summary type="html"><![CDATA[Sparse attention has been claimed to increase model interpretability under
the assumption that it highlights influential inputs. Yet the attention
distribution is typically over representations internal to the model rather
than the inputs themselves, suggesting this assumption may not have merit. We
build on the recent work exploring the interpretability of attention; we design
a set of experiments to help us understand how sparsity affects our ability to
use attention as an explainability tool. On three text classification tasks, we
verify that only a weak relationship between inputs and co-indexed intermediate
representations exists -- under sparse attention and otherwise. Further, we do
not find any plausible mappings from sparse attention distributions to a sparse
set of influential inputs through other avenues. Rather, we observe in this
setting that inducing sparsity may make it less plausible that attention can be
used as a tool for understanding model behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1"&gt;Clara Meister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazov_S/0/1/0/all/0/1"&gt;Stefan Lazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification. (arXiv:2104.14528v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14528</id>
        <link href="http://arxiv.org/abs/2104.14528"/>
        <updated>2021-06-09T02:01:52.019Z</updated>
        <summary type="html"><![CDATA[Existing deep learning methods for diagnosis of gastric cancer commonly use
convolutional neural network. Recently, the Visual Transformer has attracted
great attention because of its performance and efficiency, but its applications
are mostly in the field of computer vision. In this paper, a multi-scale visual
transformer model, referred to as GasHis-Transformer, is proposed for Gastric
Histopathological Image Classification (GHIC), which enables the automatic
classification of microscopic gastric images into abnormal and normal cases.
The GasHis-Transformer model consists of two key modules: A global information
module and a local information module to extract histopathological features
effectively. In our experiments, a public hematoxylin and eosin (H&E) stained
gastric histopathological dataset with 280 abnormal and normal images are
divided into training, validation and test sets by a ratio of 1 : 1 : 2. The
GasHis-Transformer model is applied to estimate precision, recall, F1-score and
accuracy on the test set of gastric histopathological dataset as 98.0%, 100.0%,
96.0% and 98.0%, respectively. Furthermore, a critical study is conducted to
evaluate the robustness of GasHis-Transformer, where ten different noises
including four adversarial attack and six conventional image noises are added.
In addition, a clinically meaningful study is executed to test the
gastrointestinal cancer identification performance of GasHis-Transformer with
620 abnormal images and achieves 96.8% accuracy. Finally, a comparative study
is performed to test the generalizability with both H&E and immunohistochemical
stained images on a lymphoma image dataset and a breast cancer dataset,
producing comparable F1-scores (85.6% and 82.8%) and accuracies (83.9% and
89.4%), respectively. In conclusion, GasHisTransformer demonstrates high
classification performance and shows its significant potential in the GHIC
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changhao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yudong Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1"&gt;Yueyang Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph Clustering. (arXiv:2105.05320v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05320</id>
        <link href="http://arxiv.org/abs/2105.05320"/>
        <updated>2021-06-09T02:01:52.013Z</updated>
        <summary type="html"><![CDATA[Recently, there has been considerable research interest in graph clustering
aimed at data partition using the graph information. However, one limitation of
the most of graph-based methods is that they assume the graph structure to
operate is fixed and reliable. And there are inevitably some edges in the graph
that are not conducive to graph clustering, which we call spurious edges. This
paper is the first attempt to employ graph pooling technique for node
clustering and we propose a novel dual graph embedding network (DGEN), which is
designed as a two-step graph encoder connected by a graph pooling layer to
learn the graph embedding. In our model, it is assumed that if a node and its
nearest neighboring node are close to the same clustering center, this node is
an informative node and this edge can be considered as a cluster-friendly edge.
Based on this assumption, the neighbor cluster pooling (NCPool) is devised to
select the most informative subset of nodes and the corresponding edges based
on the distance of nodes and their nearest neighbors to the cluster centers.
This can effectively alleviate the impact of the spurious edges on the
clustering. Finally, to obtain the clustering assignment of all nodes, a
classifier is trained using the clustering results of the selected nodes.
Experiments on five benchmark graph datasets demonstrate the superiority of the
proposed method over state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Dongxia Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhiqian Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyTr^2: Unbiased Image Style Transfer with Transformers. (arXiv:2105.14576v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14576</id>
        <link href="http://arxiv.org/abs/2105.14576"/>
        <updated>2021-06-09T02:01:52.007Z</updated>
        <summary type="html"><![CDATA[The goal of image style transfer is to render an image with artistic features
guided by a style reference while maintaining the original content. Due to the
locality and spatial invariance in CNNs, it is difficult to extract and
maintain the global information of input images. Therefore, traditional neural
style transfer methods are usually biased and content leak can be observed by
running several times of the style transfer process with the same reference
style image. To address this critical issue, we take long-range dependencies of
input images into account for unbiased style transfer by proposing a
transformer-based approach, namely StyTr^2. In contrast with visual
transformers for other vision tasks, our StyTr^2 contains two different
transformer encoders to generate domain-specific sequences for content and
style, respectively. Following the encoders, a multi-layer transformer decoder
is adopted to stylize the content sequence according to the style sequence. In
addition, we analyze the deficiency of existing positional encoding methods and
propose the content-aware positional encoding (CAPE) which is scale-invariant
and more suitable for image style transfer task. Qualitative and quantitative
experiments demonstrate the effectiveness of the proposed StyTr^2 compared to
state-of-the-art CNN-based and flow-based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yingying Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1"&gt;Fan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xingjia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Weiming Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chongyang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Changsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SWAD: Domain Generalization by Seeking Flat Minima. (arXiv:2102.08604v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08604</id>
        <link href="http://arxiv.org/abs/2102.08604"/>
        <updated>2021-06-09T02:01:52.002Z</updated>
        <summary type="html"><![CDATA[Domain generalization (DG) methods aim to achieve generalizability to an
unseen target domain by using only training data from the source domains.
Although a variety of DG methods have been proposed, a recent study shows that
under a fair evaluation protocol, called DomainBed, the simple empirical risk
minimization (ERM) approach works comparable to or even outperforms previous
methods. Unfortunately, simply solving ERM on a complex, non-convex loss
function can easily lead to sub-optimal generalizability by seeking sharp
minima. In this paper, we theoretically show that finding flat minima results
in a smaller domain generalization gap. We also propose a simple yet effective
method, named Stochastic Weight Averaging Densely (SWAD), to find flat minima.
SWAD finds flatter minima and suffers less from overfitting than does the
vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy.
SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS,
VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large
margins of +1.6% averagely on out-of-domain accuracy. We also compare SWAD with
conventional generalization methods, such as data augmentation and consistency
regularization methods, to verify that the remarkable performance improvements
are originated from by seeking flat minima, not from better in-domain
generalizability. Last but not least, SWAD is readily adaptable to existing DG
methods without modification; the combination of SWAD and an existing DG method
further improves DG performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1"&gt;Junbum Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1"&gt;Sanghyuk Chun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyungjae Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Han-Cheol Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seunghyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yunsung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungrae Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Synchronized Reprojection-based Model for 3D Human Pose Estimation. (arXiv:2106.04274v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04274</id>
        <link href="http://arxiv.org/abs/2106.04274"/>
        <updated>2021-06-09T02:01:51.996Z</updated>
        <summary type="html"><![CDATA[3D human pose estimation is still a challenging problem despite the large
amount of work that has been done in this field. Generally, most methods
directly use neural networks and ignore certain constraints (e.g., reprojection
constraints and joint angle and bone length constraints). This paper proposes a
weakly supervised GAN-based model for 3D human pose estimation that considers
3D information along with 2D information simultaneously, in which a
reprojection network is employed to learn the mapping of the distribution from
3D poses to 2D poses. In particular, we train the reprojection network and the
generative adversarial network synchronously. Furthermore, inspired by the
typical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix,
which is added into the discriminator's input to impose joint angle and bone
length constraints. The experimental results on Human3.6M show that our method
outperforms state-of-the-art methods by approximately 5.1\%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yicheng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Cheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yongqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiahui Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interaction-GCN: A Graph Convolutional Network based framework for social interaction recognition in egocentric videos. (arXiv:2104.14007v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14007</id>
        <link href="http://arxiv.org/abs/2104.14007"/>
        <updated>2021-06-09T02:01:51.979Z</updated>
        <summary type="html"><![CDATA[In this paper we propose a new framework to categorize social interactions in
egocentric videos, we named InteractionGCN. Our method extracts patterns of
relational and non-relational cues at the frame level and uses them to build a
relational graph from which the interactional context at the frame level is
estimated via a Graph Convolutional Network based approach. Then it propagates
this context over time, together with first-person motion information, through
a Gated Recurrent Unit architecture. Ablation studies and experimental
evaluation on two publicly available datasets validate the proposed approach
and establish state of the art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Felicioni_S/0/1/0/all/0/1"&gt;Simone Felicioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1"&gt;Mariella Dimiccoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean-Shifted Contrastive Loss for Anomaly Detection. (arXiv:2106.03844v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.03844</id>
        <link href="http://arxiv.org/abs/2106.03844"/>
        <updated>2021-06-09T02:01:51.967Z</updated>
        <summary type="html"><![CDATA[Deep anomaly detection methods learn representations that separate between
normal and anomalous samples. Very effective representations are obtained when
powerful externally trained feature extractors (e.g. ResNets pre-trained on
ImageNet) are fine-tuned on the training data which consists of normal samples
and no anomalies. However, this is a difficult task that can suffer from
catastrophic collapse, i.e. it is prone to learning trivial and non-specific
features. In this paper, we propose a new loss function which can overcome
failure modes of both center-loss and contrastive-loss methods. Furthermore, we
combine it with a confidence-invariant angular center loss, which replaces the
Euclidean distance used in previous work, that was sensitive to prediction
confidence. Our improvements yield a new anomaly detection approach, based on
$\textit{Mean-Shifted Contrastive Loss}$, which is both more accurate and less
sensitive to catastrophic collapse than previous methods. Our method achieves
state-of-the-art anomaly detection performance on multiple benchmarks including
$97.5\%$ ROC-AUC on the CIFAR-10 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiss_T/0/1/0/all/0/1"&gt;Tal Reiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1"&gt;Yedid Hoshen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NaturalProofs: Mathematical Theorem Proving in Natural Language. (arXiv:2104.01112v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01112</id>
        <link href="http://arxiv.org/abs/2104.01112"/>
        <updated>2021-06-09T02:01:51.890Z</updated>
        <summary type="html"><![CDATA[Understanding and creating mathematics using natural mathematical language -
the mixture of symbolic and natural language used by humans - is a challenging
and important problem for driving progress in machine learning. As a step in
this direction, we develop NaturalProofs, a multi-domain corpus of mathematical
statements and their proofs, written in natural mathematical language.
NaturalProofs unifies broad coverage, deep coverage, and low-resource
mathematical sources, allowing for evaluating both in-distribution and
zero-shot generalization. Using NaturalProofs, we benchmark strong neural
methods on mathematical reference retrieval and generation tasks which test a
system's ability to determine key results that appear in a proof. Large-scale
sequence models show promise compared to classical information retrieval
methods, yet their performance and out-of-domain generalization leave
substantial room for improvement. NaturalProofs opens many avenues for research
on challenging mathematical tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1"&gt;Sean Welleck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiacheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1"&gt;Ronan Le Bras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC Best Arm Identification Under a Deadline. (arXiv:2106.03221v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03221</id>
        <link href="http://arxiv.org/abs/2106.03221"/>
        <updated>2021-06-09T02:01:51.875Z</updated>
        <summary type="html"><![CDATA[We study $(\epsilon, \delta)$-PAC best arm identification, where a
decision-maker must identify an $\epsilon$-optimal arm with probability at
least $1 - \delta$, while minimizing the number of arm pulls (samples). Most of
the work on this topic is in the sequential setting, where there is no
constraint on the time taken to identify such an arm; this allows the
decision-maker to pull one arm at a time. In this work, the decision-maker is
given a deadline of $T$ rounds, where, on each round, it can adaptively choose
which arms to pull and how many times to pull them; this distinguishes the
number of decisions made (i.e., time or number of rounds) from the number of
samples acquired (cost). Such situations occur in clinical trials, where one
may need to identify a promising treatment under a deadline while minimizing
the number of test subjects, or in simulation-based studies run on the cloud,
where we can elastically scale up or down the number of virtual machines to
conduct as many experiments as we wish, but need to pay for the resource-time
used. As the decision-maker can only make $T$ decisions, she may need to pull
some arms excessively relative to a sequential algorithm in order to perform
well on all possible problems. We formalize this added difficulty with two
hardness results that indicate that unlike sequential settings, the ability to
adapt to the problem difficulty is constrained by the finite deadline. We
propose Elastic Batch Racing (EBR), a novel algorithm for this setting and
bound its sample complexity, showing that EBR is optimal with respect to both
hardness results. We present simulations evaluating EBR in this setting, where
it outperforms baselines by several orders of magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1"&gt;Kirthevasan Kandasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1"&gt;Ion Stoica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Whitening Batch Normalization. (arXiv:2106.04413v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04413</id>
        <link href="http://arxiv.org/abs/2106.04413"/>
        <updated>2021-06-09T02:01:51.869Z</updated>
        <summary type="html"><![CDATA[Batch Normalization (BN) is a popular technique for training Deep Neural
Networks (DNNs). BN uses scaling and shifting to normalize activations of
mini-batches to accelerate convergence and improve generalization. The recently
proposed Iterative Normalization (IterNorm) method improves these properties by
whitening the activations iteratively using Newton's method. However, since
Newton's method initializes the whitening matrix independently at each training
step, no information is shared between consecutive steps. In this work, instead
of exact computation of whitening matrix at each time step, we estimate it
gradually during training in an online fashion, using our proposed Stochastic
Whitening Batch Normalization (SWBN) algorithm. We show that while SWBN
improves the convergence rate and generalization of DNNs, its computational
overhead is less than that of IterNorm. Due to the high efficiency of the
proposed method, it can be easily employed in most DNN architectures with a
large number of layers. We provide comprehensive experiments and comparisons
between BN, IterNorm, and SWBN layers to demonstrate the effectiveness of the
proposed technique in conventional (many-shot) image classification and
few-shot classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nezhadarya_E/0/1/0/all/0/1"&gt;Ehsan Nezhadarya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fashandi_H/0/1/0/all/0/1"&gt;Homa Fashandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiayi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graham_D/0/1/0/all/0/1"&gt;Darin Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mohak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03954</id>
        <link href="http://arxiv.org/abs/2106.03954"/>
        <updated>2021-06-09T02:01:51.864Z</updated>
        <summary type="html"><![CDATA[With the popularity of Machine Learning (ML) solutions, algorithms and data
have been released faster than the capacity of processing them. In this
context, the problem of Algorithm Recommendation (AR) is receiving a
significant deal of attention recently. This problem has been addressed in the
literature as a learning task, often as a Meta-Learning problem where the aim
is to recommend the best alternative for a specific dataset. For such, datasets
encoded by meta-features are explored by ML algorithms that try to learn the
mapping between meta-representations and the best technique to be used. One of
the challenges for the successful use of ML is to define which features are the
most valuable for a specific dataset since several meta-features can be used,
which increases the meta-feature dimension. This paper presents an empirical
analysis of Feature Selection and Feature Extraction in the meta-level for the
AR problem. The present study was focused on three criteria: predictive
performance, dimensionality reduction, and pipeline runtime. As we verified,
applying Dimensionality Reduction (DR) methods did not improve predictive
performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed
that various datasets have many non-informative meta-features and that it is
possible to obtain high predictive performance using around 20% of the original
meta-features. Therefore, due to their natural trend for high dimensionality,
DR methods should be used for Meta-Feature Selection and Meta-Feature
Extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1"&gt;Geand Trindade Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1"&gt;Moises Rocha dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1"&gt;Andre Carlos Ponce de Leon Ferreira de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering. (arXiv:2106.03798v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03798</id>
        <link href="http://arxiv.org/abs/2106.03798"/>
        <updated>2021-06-09T02:01:51.859Z</updated>
        <summary type="html"><![CDATA[We introduce DoubleField, a novel representation combining the merits of both
surface field and radiance field for high-fidelity human rendering. Within
DoubleField, the surface field and radiance field are associated together by a
shared feature embedding and a surface-guided sampling strategy. In this way,
DoubleField has a continuous but disentangled learning space for geometry and
appearance modeling, which supports fast training, inference, and finetuning.
To achieve high-fidelity free-viewpoint rendering, DoubleField is further
augmented to leverage ultra-high-resolution inputs, where a view-to-view
transformer and a transfer learning scheme are introduced for more efficient
learning and finetuning from sparse-view inputs at original resolutions. The
efficacy of DoubleField is validated by the quantitative evaluations on several
datasets and the qualitative results in a real-world sparse multi-view system,
showing its superior capability for photo-realistic free-viewpoint human
rendering. For code and demo video, please refer to our project page:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1"&gt;Ruizhi Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;He Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yanpei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yebin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PolypGen: A multi-center polyp detection and segmentation dataset for generalisability assessment. (arXiv:2106.04463v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04463</id>
        <link href="http://arxiv.org/abs/2106.04463"/>
        <updated>2021-06-09T02:01:51.853Z</updated>
        <summary type="html"><![CDATA[Polyps in the colon are widely known as cancer precursors identified by
colonoscopy either related to diagnostic work-up for symptoms, colorectal
cancer screening or systematic surveillance of certain diseases. Whilst most
polyps are benign, the number, size and the surface structure of the polyp are
tightly linked to the risk of colon cancer. There exists a high missed
detection rate and incomplete removal of colon polyps due to the variable
nature, difficulties to delineate the abnormality, high recurrence rates and
the anatomical topography of the colon. In the past, several methods have been
built to automate polyp detection and segmentation. However, the key issue of
most methods is that they have not been tested rigorously on a large
multi-center purpose-built dataset. Thus, these methods may not generalise to
different population datasets as they overfit to a specific population and
endoscopic surveillance. To this extent, we have curated a dataset from 6
different centers incorporating more than 300 patients. The dataset includes
both single frame and sequence data with 3446 annotated polyp labels with
precise delineation of polyp boundaries verified by six senior
gastroenterologists. To our knowledge, this is the most comprehensive detection
and pixel-level segmentation dataset curated by a team of computational
scientists and expert gastroenterologists. This dataset has been originated as
the part of the Endocv2021 challenge aimed at addressing generalisability in
polyp detection and segmentation. In this paper, we provide comprehensive
insight into data construction and annotation strategies, annotation quality
assurance and technical validation for our extended EndoCV2021 dataset which we
refer to as PolypGen.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sharib Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1"&gt;Debesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghatwary_N/0/1/0/all/0/1"&gt;Noha Ghatwary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Realdon_S/0/1/0/all/0/1"&gt;Stefano Realdon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cannizzaro_R/0/1/0/all/0/1"&gt;Renato Cannizzaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salem_O/0/1/0/all/0/1"&gt;Osama E. Salem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lamarque_D/0/1/0/all/0/1"&gt;Dominique Lamarque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Daul_C/0/1/0/all/0/1"&gt;Christian Daul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anonsen_K/0/1/0/all/0/1"&gt;Kim V. Anonsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1"&gt;Michael A. Riegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1"&gt;P&amp;#xe5;l Halvorsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1"&gt;Jens Rittscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1"&gt;Thomas de Lange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+East_J/0/1/0/all/0/1"&gt;James E. East&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DisTop: Discovering a Topological representation to learn diverse and rewarding skills. (arXiv:2106.03853v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03853</id>
        <link href="http://arxiv.org/abs/2106.03853"/>
        <updated>2021-06-09T02:01:51.847Z</updated>
        <summary type="html"><![CDATA[The optimal way for a deep reinforcement learning (DRL) agent to explore is
to learn a set of skills that achieves a uniform distribution of states.
Following this,we introduce DisTop, a new model that simultaneously learns
diverse skills and focuses on improving rewarding skills. DisTop progressively
builds a discrete topology of the environment using an unsupervised contrastive
loss, a growing network and a goal-conditioned policy. Using this topology, a
state-independent hierarchical policy can select where the agent has to keep
discovering skills in the state space. In turn, the newly visited states allows
an improved learnt representation and the learning loop continues. Our
experiments emphasize that DisTop is agnostic to the ground state
representation and that the agent can discover the topology of its environment
whether the states are high-dimensional binary data, images, or proprioceptive
inputs. We demonstrate that this paradigm is competitiveon MuJoCo benchmarks
with state-of-the-art algorithms on both single-task dense rewards and diverse
skill discovery. By combining these two aspects, we showthat DisTop achieves
state-of-the-art performance in comparison with hierarchical reinforcement
learning (HRL) when rewards are sparse. We believe DisTop opens new
perspectives by showing that bottom-up skill discovery combined with
representation learning can unlock the exploration challenge in DRL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aubret_A/0/1/0/all/0/1"&gt;Arthur Aubret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+matignon_L/0/1/0/all/0/1"&gt;Laetitia matignon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassas_S/0/1/0/all/0/1"&gt;Salima Hassas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Practical Credit Assignment for Deep Reinforcement Learning. (arXiv:2106.04499v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04499</id>
        <link href="http://arxiv.org/abs/2106.04499"/>
        <updated>2021-06-09T02:01:51.840Z</updated>
        <summary type="html"><![CDATA[Credit assignment is a fundamental problem in reinforcement learning, the
problem of measuring an action's influence on future rewards. Improvements in
credit assignment methods have the potential to boost the performance of RL
algorithms on many tasks, but thus far have not seen widespread adoption.
Recently, a family of methods called Hindsight Credit Assignment (HCA) was
proposed, which explicitly assign credit to actions in hindsight based on the
probability of the action having led to an observed outcome. This approach is
appealing as a means to more efficient data usage, but remains a largely
theoretical idea applicable to a limited set of tabular RL tasks, and it is
unclear how to extend HCA to Deep RL environments. In this work, we explore the
use of HCA-style credit in a deep RL context. We first describe the limitations
of existing HCA algorithms in deep RL, then propose several
theoretically-justified modifications to overcome them. Based on this
exploration, we present a new algorithm, Credit-Constrained Advantage
Actor-Critic (C2A2C), which ignores policy updates for actions which don't
affect future outcomes based on credit in hindsight, while updating the policy
as normal for those that do. We find that C2A2C outperforms Advantage
Actor-Critic (A2C) on the Arcade Learning Environment (ALE) benchmark, showing
broad improvements over A2C and motivating further work on credit-constrained
update rules for deep RL methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alipov_V/0/1/0/all/0/1"&gt;Vyacheslav Alipov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simmons_Edler_R/0/1/0/all/0/1"&gt;Riley Simmons-Edler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Putintsev_N/0/1/0/all/0/1"&gt;Nikita Putintsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinin_P/0/1/0/all/0/1"&gt;Pavel Kalinin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04408</id>
        <link href="http://arxiv.org/abs/2011.04408"/>
        <updated>2021-06-09T02:01:51.835Z</updated>
        <summary type="html"><![CDATA[Changing environments poses a great challenge on the outdoor visual
perception and scene understanding for robust long-term autonomous driving and
mobile robots, where depth-auxiliary geometric information plays an essential
role to the robustness under challenging scenes. Although monocular depth
prediction has been well studied recently, there are few work focusing on the
depth prediction across multiple environmental conditions, e.g. changing
illumination and seasons, owing to the lack of such a real-world dataset and
benchmark. In this work, a new cross-season monocular depth prediction dataset
SeasonDepth (available on https://seasondepth.github.io) is derived from CMU
Visual Localization dataset through structure from motion. To benchmark the
depth estimation performance under different environments, we investigate
representative and recent state-of-the-art open-source supervised,
self-supervised and domain adaptation depth prediction methods from KITTI
benchmark using several newly-formulated metrics. Through extensive
experimental evaluation on the proposed dataset without fine-tuning, the
influence of multiple environments on performance and robustness is analyzed
both qualitatively and quantitatively, showing that the long-term monocular
depth prediction is far from solved. We further give promising solutions
especially with stereo geometry and multi-task sequential self-supervised
training to enhance the robustness to changing environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hanjiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Baoquan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhijian Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hesheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Loss Networks. (arXiv:2106.03722v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03722</id>
        <link href="http://arxiv.org/abs/2106.03722"/>
        <updated>2021-06-09T02:01:51.829Z</updated>
        <summary type="html"><![CDATA[A novel model called error loss network (ELN) is proposed to build an error
loss function for supervised learning. The ELN is in structure similar to a
radial basis function (RBF) neural network, but its input is an error sample
and output is a loss corresponding to that error sample. That means the
nonlinear input-output mapper of ELN creates an error loss function. The
proposed ELN provides a unified model for a large class of error loss
functions, which includes some information theoretic learning (ITL) loss
functions as special cases. The activation function, weight parameters and
network size of the ELN can be predetermined or learned from the error samples.
On this basis, we propose a new machine learning paradigm where the learning
process is divided into two stages: first, learning a loss function using an
ELN; second, using the learned loss function to continue to perform the
learning. Experimental results are presented to demonstrate the desirable
performance of the new method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Badong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yunfei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pengju Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04484</id>
        <link href="http://arxiv.org/abs/2106.04484"/>
        <updated>2021-06-09T02:01:51.810Z</updated>
        <summary type="html"><![CDATA[Deep learning algorithms have shown promising results in visual question
answering (VQA) tasks, but a more careful look reveals that they often do not
understand the rich signal they are being fed with. To understand and better
measure the generalization capabilities of VQA systems, we look at their
robustness to counterfactually augmented data. Our proposed augmentations are
designed to make a focused intervention on a specific property of the question
such that the answer changes. Using these augmentations, we propose a new
robustness measure, Robustness to Augmented Data (RAD), which measures the
consistency of model predictions between original and augmented examples.
Through extensive experimentation, we show that RAD, unlike classical accuracy
measures, can quantify when state-of-the-art systems are not robust to
counterfactuals. We find substantial failure cases which reveal that current
VQA systems are still brittle. Finally, we connect between robustness and
generalization, demonstrating the predictive power of RAD for performance on
unseen augmentations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1"&gt;Daniel Rosenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1"&gt;Itai Gat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1"&gt;Amir Feder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1"&gt;Roi Reichart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The best of both worlds: stochastic and adversarial episodic MDPs with unknown transition. (arXiv:2106.04117v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04117</id>
        <link href="http://arxiv.org/abs/2106.04117"/>
        <updated>2021-06-09T02:01:51.800Z</updated>
        <summary type="html"><![CDATA[We consider the best-of-both-worlds problem for learning an episodic Markov
Decision Process through $T$ episodes, with the goal of achieving
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret when the losses are adversarial and
simultaneously $\mathcal{O}(\text{polylog}(T))$ regret when the losses are
(almost) stochastic. Recent work by [Jin and Luo, 2020] achieves this goal when
the fixed transition is known, and leaves the case of unknown transition as a
major open question. In this work, we resolve this open problem by using the
same Follow-the-Regularized-Leader ($\text{FTRL}$) framework together with a
set of new techniques. Specifically, we first propose a loss-shifting trick in
the $\text{FTRL}$ analysis, which greatly simplifies the approach of [Jin and
Luo, 2020] and already improves their results for the known transition case.
Then, we extend this idea to the unknown transition case and develop a novel
analysis which upper bounds the transition estimation error by (a fraction of)
the regret itself in the stochastic setting, a key property to ensure
$\mathcal{O}(\text{polylog}(T))$ regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1"&gt;Tiancheng Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longbo Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Hybrid Automata: Learning Dynamics with Multiple Modes and Stochastic Transitions. (arXiv:2106.04165v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04165</id>
        <link href="http://arxiv.org/abs/2106.04165"/>
        <updated>2021-06-09T02:01:51.793Z</updated>
        <summary type="html"><![CDATA[Effective control and prediction of dynamical systems often require
appropriate handling of continuous-time and discrete, event-triggered
processes. Stochastic hybrid systems (SHSs), common across engineering domains,
provide a formalism for dynamical systems subject to discrete, possibly
stochastic, state jumps and multi-modal continuous-time flows. Despite the
versatility and importance of SHSs across applications, a general procedure for
the explicit learning of both discrete events and multi-mode continuous
dynamics remains an open problem. This work introduces Neural Hybrid Automata
(NHAs), a recipe for learning SHS dynamics without a priori knowledge on the
number of modes and inter-modal transition dynamics. NHAs provide a systematic
inference method based on normalizing flows, neural differential equations and
self-supervision. We showcase NHAs on several tasks, including mode recovery
and flow learning in systems with stochastic transitions, and end-to-end
learning of hierarchical robot controllers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1"&gt;Michael Poli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1"&gt;Stefano Massaroli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scimeca_L/0/1/0/all/0/1"&gt;Luca Scimeca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Seong Joon Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1"&gt;Sanghyuk Chun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1"&gt;Atsushi Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1"&gt;Hajime Asama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinkyoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1"&gt;Animesh Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighted Sparse Subspace Representation: A Unified Framework for Subspace Clustering, Constrained Clustering, and Active Learning. (arXiv:2106.04330v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04330</id>
        <link href="http://arxiv.org/abs/2106.04330"/>
        <updated>2021-06-09T02:01:51.771Z</updated>
        <summary type="html"><![CDATA[Spectral-based subspace clustering methods have proved successful in many
challenging applications such as gene sequencing, image recognition, and motion
segmentation. In this work, we first propose a novel spectral-based subspace
clustering algorithm that seeks to represent each point as a sparse convex
combination of a few nearby points. We then extend the algorithm to constrained
clustering and active learning settings. Our motivation for developing such a
framework stems from the fact that typically either a small amount of labelled
data is available in advance; or it is possible to label some points at a cost.
The latter scenario is typically encountered in the process of validating a
cluster assignment. Extensive experiments on simulated and real data sets show
that the proposed approach is effective and competitive with state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hankui Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pavlidis_N/0/1/0/all/0/1"&gt;Nicos G. Pavlidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonsmooth Implicit Differentiation for Machine Learning and Optimization. (arXiv:2106.04350v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04350</id>
        <link href="http://arxiv.org/abs/2106.04350"/>
        <updated>2021-06-09T02:01:51.760Z</updated>
        <summary type="html"><![CDATA[In view of training increasingly complex learning architectures, we establish
a nonsmooth implicit function theorem with an operational calculus. Our result
applies to most practical problems (i.e., definable problems) provided that a
nonsmooth form of the classical invertibility condition is fulfilled. This
approach allows for formal subdifferentiation: for instance, replacing
derivatives by Clarke Jacobians in the usual differentiation formulas is fully
justified for a wide class of nonsmooth problems. Moreover this calculus is
entirely compatible with algorithmic differentiation (e.g., backpropagation).
We provide several applications such as training deep equilibrium networks,
training neural nets with conic optimization layers, or hyperparameter-tuning
for nonsmooth Lasso-type models. To show the sharpness of our assumptions, we
present numerical experiments showcasing the extremely pathological gradient
dynamics one can encounter when applying implicit algorithmic differentiation
without any hypothesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Bolte&lt;/a&gt; (TSE), &lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Tam Le&lt;/a&gt; (TSE), &lt;a href="http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1"&gt;Edouard Pauwels&lt;/a&gt; (IRIT), &lt;a href="http://arxiv.org/find/cs/1/au:+Silveti_Falls_A/0/1/0/all/0/1"&gt;Antonio Silveti-Falls&lt;/a&gt; (TSE)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture of Robust Experts (MoRE): A Flexible Defense Against Multiple Perturbations. (arXiv:2104.10586v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10586</id>
        <link href="http://arxiv.org/abs/2104.10586"/>
        <updated>2021-06-09T02:01:51.747Z</updated>
        <summary type="html"><![CDATA[To tackle the susceptibility of deep neural networks to adversarial examples,
the adversarial training has been proposed which provides a notion of security
through an inner maximization problem presenting the first-order adversaries
embedded within the outer minimization of the training loss. To generalize the
adversarial robustness over different perturbation types, the adversarial
training method has been augmented with the improved inner maximization
presenting a union of multiple perturbations e.g., various $\ell_p$
norm-bounded perturbations. However, the improved inner maximization only
enjoys limited flexibility in terms of the allowable perturbation types. In
this work, through a gating mechanism, we assemble a set of expert networks,
each one either adversarially trained to deal with a particular perturbation
type or normally trained for boosting accuracy on clean data. The gating module
assigns weights dynamically to each expert to achieve superior accuracy under
various data types e.g., adversarial examples, adverse weather perturbations,
and clean input. In order to deal with the obfuscated gradients issue, the
training of the gating module is conducted together with fine-tuning of the
last fully connected layers of expert networks through adversarial training
approach. Using extensive experiments, we show that our Mixture of Robust
Experts (MoRE) approach enables flexible integration of a broad range of robust
experts with superior performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldhahn_R/0/1/0/all/0/1"&gt;Ryan Goldhahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Expressive Power of Self-Attention Matrices. (arXiv:2106.03764v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03764</id>
        <link href="http://arxiv.org/abs/2106.03764"/>
        <updated>2021-06-09T02:01:51.734Z</updated>
        <summary type="html"><![CDATA[Transformer networks are able to capture patterns in data coming from many
domains (text, images, videos, proteins, etc.) with little or no change to
architecture components. We perform a theoretical analysis of the core
component responsible for signal propagation between elements, i.e. the
self-attention matrix. In practice, this matrix typically exhibits two
properties: (1) it is sparse, meaning that each token only attends to a small
subset of other tokens; and (2) it changes dynamically depending on the input
to the module. With these considerations in mind, we ask the following
question: Can a fixed self-attention module approximate arbitrary sparse
patterns depending on the input? How small is the hidden size $d$ required for
such approximation? We make progress in answering this question and show that
the self-attention matrix can provably approximate sparse matrices, where
sparsity is in terms of a bounded number of nonzero elements in each row and
column. While the parameters of self-attention are fixed, various sparse
matrices can be approximated by only modifying the inputs. Our proof is based
on the random projection technique and uses the seminal Johnson-Lindenstrauss
lemma. Our proof is constructive, enabling us to propose an algorithm for
finding adaptive inputs and fixed self-attention parameters in order to
approximate a given matrix. In particular, we show that, in order to
approximate any sparse matrix up to a given precision defined in terms of
preserving matrix element ratios, $d$ grows only logarithmically with the
sequence length $L$ (i.e. $d = O(\log L)$).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1"&gt;Valerii Likhosherstov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks. (arXiv:2106.04469v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.04469</id>
        <link href="http://arxiv.org/abs/2106.04469"/>
        <updated>2021-06-09T02:01:51.729Z</updated>
        <summary type="html"><![CDATA[We consider the task of minimizing the sum of smooth and strongly convex
functions stored in a decentralized manner across the nodes of a communication
network whose links are allowed to change in time. We solve two fundamental
problems for this task. First, we establish the first lower bounds on the
number of decentralized communication rounds and the number of local
computations required to find an $\epsilon$-accurate solution. Second, we
design two optimal algorithms that attain these lower bounds: (i) a variant of
the recently proposed algorithm ADOM (Kovalev et al., 2021) enhanced via a
multi-consensus subroutine, which is optimal in the case when access to the
dual gradients is assumed, and (ii) a novel algorithm, called ADOM+, which is
optimal in the case when access to the primal gradients is assumed. We
corroborate the theoretical efficiency of these algorithms by performing an
experimental comparison with existing state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kovalev_D/0/1/0/all/0/1"&gt;Dmitry Kovalev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gasanov_E/0/1/0/all/0/1"&gt;Elnur Gasanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1"&gt;Alexander Gasnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doing Natural Language Processing in A Natural Way: An NLP toolkit based on object-oriented knowledge base and multi-level grammar base. (arXiv:2105.05227v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05227</id>
        <link href="http://arxiv.org/abs/2105.05227"/>
        <updated>2021-06-09T02:01:51.713Z</updated>
        <summary type="html"><![CDATA[We introduce an NLP toolkit based on object-oriented knowledge base and
multi-level grammar base. This toolkit focuses on semantic parsing, it also has
abilities to discover new knowledge and grammar automatically, new discovered
knowledge and grammar will be identified by human, and will be used to update
the knowledge base and grammar base. This process can be iterated many times to
improve the toolkit continuously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stability and Generalization of Bilevel Programming in Hyperparameter Optimization. (arXiv:2106.04188v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04188</id>
        <link href="http://arxiv.org/abs/2106.04188"/>
        <updated>2021-06-09T02:01:51.708Z</updated>
        <summary type="html"><![CDATA[Recently, the (gradient-based) bilevel programming framework is widely used
in hyperparameter optimization and has achieved excellent performance
empirically. Previous theoretical work mainly focuses on its optimization
properties, while leaving the analysis on generalization largely open. This
paper attempts to address the issue by presenting an expectation bound w.r.t.
the validation set based on uniform stability. Our results can explain some
mysterious behaviours of the bilevel programming in practice, for instance,
overfitting to the validation set. We also present an expectation bound for the
classical cross-validation algorithm. Our results suggest that gradient-based
algorithms can be better than cross-validation under certain conditions in a
theoretical perspective. Furthermore, we prove that regularization terms in
both the outer and inner levels can relieve the overfitting problem in
gradient-based algorithms. In experiments on feature learning and data
reweighting for noisy labels, we corroborate our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1"&gt;Fan Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Guoqiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chongxuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconciling Rewards with Predictive State Representations. (arXiv:2106.03926v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.03926</id>
        <link href="http://arxiv.org/abs/2106.03926"/>
        <updated>2021-06-09T02:01:51.703Z</updated>
        <summary type="html"><![CDATA[Predictive state representations (PSRs) are models of controlled non-Markov
observation sequences which exhibit the same generative process governing POMDP
observations without relying on an underlying latent state. In that respect, a
PSR is indistinguishable from the corresponding POMDP. However, PSRs
notoriously ignore the notion of rewards, which undermines the general utility
of PSR models for control, planning, or reinforcement learning. Therefore, we
describe a sufficient and necessary accuracy condition which determines whether
a PSR is able to accurately model POMDP rewards, we show that rewards can be
approximated even when the accuracy condition is not satisfied, and we find
that a non-trivial number of POMDPs taken from a well-known third-party
repository do not satisfy the accuracy condition. We propose reward-predictive
state representations (R-PSRs), a generalization of PSRs which accurately
models both observations and rewards, and develop value iteration for R-PSRs.
We show that there is a mismatch between optimal POMDP policies and the optimal
PSR policies derived from approximate rewards. On the other hand, optimal R-PSR
policies perfectly match optimal POMDP policies, reconfirming R-PSRs as
accurate state-less generative models of observations and rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baisero_A/0/1/0/all/0/1"&gt;Andrea Baisero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1"&gt;Christopher Amato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Householder-Absolute Neural Layers For High Variability and Deep Trainability. (arXiv:2106.04088v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04088</id>
        <link href="http://arxiv.org/abs/2106.04088"/>
        <updated>2021-06-09T02:01:51.697Z</updated>
        <summary type="html"><![CDATA[We propose a new architecture for artificial neural networks called
Householder-absolute neural layers, or Han-layers for short, that use
Householder reflectors as weight matrices and the absolute-value function for
activation. Han-layers, functioning as fully connected layers, are motivated by
recent results on neural-network variability and are designed to increase
activation ratio and reduce the chance of Collapse to Constants. Neural
networks constructed chiefly from Han-layers are called HanNets. By
construction, HanNets enjoy a theoretical guarantee that vanishing or exploding
gradient never occurs. We conduct several proof-of-concept experiments. Some
surprising results obtained on styled test problems suggest that, under certain
conditions, HanNets exhibit an unusual ability to produce nearly perfect
solutions unattainable by fully connected networks. Experiments on regression
datasets show that HanNets can significantly reduce the number of model
parameters while maintaining or improving the level of generalization accuracy.
In addition, by adding a few Han-layers into the pre-classification FC-layer of
a convolutional neural network, we are able to quickly improve a
state-of-the-art result on CIFAR10 dataset. These proof-of-concept results are
sufficient to necessitate further studies on HanNets to understand their
capacities and limits, and to exploit their potentials in real-world
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yueyao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StutterNet: Stuttering Detection Using Time Delay Neural Network. (arXiv:2105.05599v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05599</id>
        <link href="http://arxiv.org/abs/2105.05599"/>
        <updated>2021-06-09T02:01:51.692Z</updated>
        <summary type="html"><![CDATA[This paper introduces StutterNet, a novel deep learning based stuttering
detection capable of detecting and identifying various types of disfluencies.
Most of the existing work in this domain uses automatic speech recognition
(ASR) combined with language models for stuttering detection. Compared to the
existing work, which depends on the ASR module, our method relies solely on the
acoustic signal. We use a time-delay neural network (TDNN) suitable for
capturing contextual aspects of the disfluent utterances. We evaluate our
system on the UCLASS stuttering dataset consisting of more than 100 speakers.
Our method achieves promising results and outperforms the state-of-the-art
residual neural network based method. The number of trainable parameters of the
proposed method is also substantially less due to the parameter sharing scheme
of TDNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sheikh_S/0/1/0/all/0/1"&gt;Shakeel A. Sheikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1"&gt;Md Sahidullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hirsch_F/0/1/0/all/0/1"&gt;Fabrice Hirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ouni_S/0/1/0/all/0/1"&gt;Slim Ouni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bangla Natural Language Processing: A Comprehensive Review of Classical, Machine Learning, and Deep Learning Based Methods. (arXiv:2105.14875v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14875</id>
        <link href="http://arxiv.org/abs/2105.14875"/>
        <updated>2021-06-09T02:01:51.675Z</updated>
        <summary type="html"><![CDATA[The Bangla language is the seventh most spoken language, with 265 million
native and non-native speakers worldwide. However, English is the predominant
language for online resources and technical knowledge, journals, and
documentation. Consequently, many Bangla-speaking people, who have limited
command of English, face hurdles to utilize English resources. To bridge the
gap between limited support and increasing demand, researchers conducted many
experiments and developed valuable tools and techniques to create and process
Bangla language materials. Many efforts are also ongoing to make it easy to use
the Bangla language in the online and technical domains. There are some review
papers to understand the past, previous, and future Bangla Natural Language
Processing (BNLP) trends. The studies are mainly concentrated on the specific
domains of BNLP, such as sentiment analysis, speech recognition, optical
character recognition, and text summarization. There is an apparent scarcity of
resources that contain a comprehensive study of the recent BNLP tools and
methods. Therefore, in this paper, we present a thorough review of 71 BNLP
research papers and categorize them into 11 categories, namely Information
Extraction, Machine Translation, Named Entity Recognition, Parsing, Parts of
Speech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake
Detection, Text Summarization, Word Sense Disambiguation, and Speech Processing
and Recognition. We study articles published between 1999 to 2021, and 50% of
the papers were published after 2015. We discuss Classical, Machine Learning
and Deep Learning approaches with different datasets while addressing the
limitations and current and future trends of the BNLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sen_O/0/1/0/all/0/1"&gt;Ovishake Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Mohtasim Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;MD. Nazrul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbi_J/0/1/0/all/0/1"&gt;Jakaria Rabbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;MD. Kamrul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baz_M/0/1/0/all/0/1"&gt;Mohammed Baz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masud_M/0/1/0/all/0/1"&gt;Mehedi Masud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awal_M/0/1/0/all/0/1"&gt;Md. Abdul Awal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fime_A/0/1/0/all/0/1"&gt;Awal Ahmed Fime&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Md. Tahmid Hasan Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sikder_D/0/1/0/all/0/1"&gt;Delowar Sikder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iftee_M/0/1/0/all/0/1"&gt;MD. Akil Raihan Iftee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14188</id>
        <link href="http://arxiv.org/abs/2105.14188"/>
        <updated>2021-06-09T02:01:51.669Z</updated>
        <summary type="html"><![CDATA[Advertising expenditures have become the major source of revenue for
e-commerce platforms. Providing good advertising experiences for advertisers
through reducing their costs of trial and error for discovering the optimal
advertising strategies is crucial for the long-term prosperity of online
advertising. To achieve this goal, the advertising platform needs to identify
the advertisers' marketing objectives, and then recommend the corresponding
strategies to fulfill this objective. In this work, we first deploy a prototype
of strategy recommender system on Taobao display advertising platform,
recommending bid prices and targeted users to advertisers. We further augment
this prototype system by directly revealing the advertising performance, and
then infer the advertisers' marketing objectives through their adoptions of
different recommending advertising performance. We use the techniques from
context bandit to jointly learn the advertisers' marketing objectives and the
recommending strategies. Online evaluations show that the designed advertising
strategy recommender system can optimize the advertisers' advertising
performance and increase the platform's revenue. Simulation experiments based
on Taobao online bidding data show that the designed contextual bandit
algorithm can effectively optimize the strategy adoption rate of advertisers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Liyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junqi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhenzhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zhizhuang Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Fei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1"&gt;Lvyin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuning Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Channel Dimensions for Efficient Model Design. (arXiv:2007.00992v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00992</id>
        <link href="http://arxiv.org/abs/2007.00992"/>
        <updated>2021-06-09T02:01:51.663Z</updated>
        <summary type="html"><![CDATA[Designing an efficient model within the limited computational cost is
challenging. We argue the accuracy of a lightweight model has been further
limited by the design convention: a stage-wise configuration of the channel
dimensions, which looks like a piecewise linear function of the network stage.
In this paper, we study an effective channel dimension configuration towards
better performance than the convention. To this end, we empirically study how
to design a single layer properly by analyzing the rank of the output feature.
We then investigate the channel configuration of a model by searching network
architectures concerning the channel configuration under the computational cost
restriction. Based on the investigation, we propose a simple yet effective
channel configuration that can be parameterized by the layer index. As a
result, our proposed model following the channel parameterization achieves
remarkable performance on ImageNet classification and transfer learning tasks
including COCO object detection, COCO instance segmentation, and fine-grained
classifications. Code and ImageNet pretrained models are available at
https://github.com/clovaai/rexnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dongyoon Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1"&gt;Byeongho Heo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1"&gt;YoungJoon Yoo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Medkit-Learn(ing) Environment: Medical Decision Modelling through Simulation. (arXiv:2106.04240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04240</id>
        <link href="http://arxiv.org/abs/2106.04240"/>
        <updated>2021-06-09T02:01:51.657Z</updated>
        <summary type="html"><![CDATA[Understanding decision-making in clinical environments is of paramount
importance if we are to bring the strengths of machine learning to ultimately
improve patient outcomes. Several factors including the availability of public
data, the intrinsically offline nature of the problem, and the complexity of
human decision making, has meant that the mainstream development of algorithms
is often geared towards optimal performance in tasks that do not necessarily
translate well into the medical regime; often overlooking more niche issues
commonly associated with the area. We therefore present a new benchmarking
suite designed specifically for medical sequential decision making: the
Medkit-Learn(ing) Environment, a publicly available Python package providing
simple and easy access to high-fidelity synthetic medical data. While providing
a standardised way to compare algorithms in a realistic medical setting we
employ a generating process that disentangles the policy and environment
dynamics to allow for a range of customisations, thus enabling systematic
evaluation of algorithms' robustness against specific challenges prevalent in
healthcare.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Alex J. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bica_I/0/1/0/all/0/1"&gt;Ioana Bica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1"&gt;Alihan Huyuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jarrett_D/0/1/0/all/0/1"&gt;Daniel Jarrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable agent communication from scratch(with a generic visual processor emerging on the side). (arXiv:2106.04258v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04258</id>
        <link href="http://arxiv.org/abs/2106.04258"/>
        <updated>2021-06-09T02:01:51.652Z</updated>
        <summary type="html"><![CDATA[As deep networks begin to be deployed as autonomous agents, the issue of how
they can communicate with each other becomes important. Here, we train two deep
nets from scratch to perform realistic referent identification through
unsupervised emergent communication. We show that the largely interpretable
emergent protocol allows the nets to successfully communicate even about object
types they did not see at training time. The visual representations induced as
a by-product of our training regime, moreover, show comparable quality, when
re-used as generic visual features, to a recent self-supervised learning model.
Our results provide concrete evidence of the viability of (interpretable)
emergent deep net communication in a more realistic scenario than previously
considered, as well as establishing an intriguing link between this field and
self-supervised visual learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1"&gt;Roberto Dess&amp;#xec;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1"&gt;Eugene Kharitonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1"&gt;Marco Baroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models. (arXiv:2106.03969v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03969</id>
        <link href="http://arxiv.org/abs/2106.03969"/>
        <updated>2021-06-09T02:01:51.635Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning a tree-structured Ising model from data,
such that subsequent predictions computed using the model are accurate.
Concretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small
sets of variables $S$ are accurate. Since its introduction more than 50 years
ago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood
tree, has been the benchmark algorithm for learning tree-structured graphical
models. A bound on the sample complexity of the Chow-Liu algorithm with respect
to the prediction-centric local total variation loss was shown in [BK19]. While
those results demonstrated that it is possible to learn a useful model even
when recovering the true underlying graph is impossible, their bound depends on
the maximum strength of interactions and thus does not achieve the
information-theoretic optimum. In this paper, we introduce a new algorithm that
carefully combines elements of the Chow-Liu algorithm with tree metric
reconstruction methods to efficiently and optimally learn tree Ising models
under a prediction-centric loss. Our algorithm is robust to model
misspecification and adversarial corruptions. In contrast, we show that the
celebrated Chow-Liu algorithm can be arbitrarily suboptimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boix_Adsera_E/0/1/0/all/0/1"&gt;Enric Boix-Adsera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bresler_G/0/1/0/all/0/1"&gt;Guy Bresler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1"&gt;Frederic Koehler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Data Augmentation Do We Need for Deep-Learning-Based Finance?. (arXiv:2106.04114v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04114</id>
        <link href="http://arxiv.org/abs/2106.04114"/>
        <updated>2021-06-09T02:01:51.629Z</updated>
        <summary type="html"><![CDATA[The main task we consider is portfolio construction in a speculative market,
a fundamental problem in modern finance. While various empirical works now
exist to explore deep learning in finance, the theory side is almost
non-existent. In this work, we focus on developing a theoretical framework for
understanding the use of data augmentation for deep-learning-based approaches
to quantitative finance. The proposed theory clarifies the role and necessity
of data augmentation for finance; moreover, our theory motivates a simple
algorithm of injecting a random noise of strength $\sqrt{|r_{t-1}|}$ to the
observed return $r_{t}$. This algorithm is shown to work well in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1"&gt;Liu Ziyin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minami_K/0/1/0/all/0/1"&gt;Kentaro Minami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imajo_K/0/1/0/all/0/1"&gt;Kentaro Imajo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[E(n) Equivariant Normalizing Flows. (arXiv:2105.09016v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09016</id>
        <link href="http://arxiv.org/abs/2105.09016"/>
        <updated>2021-06-09T02:01:51.623Z</updated>
        <summary type="html"><![CDATA[This paper introduces a generative model equivariant to Euclidean symmetries:
E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the
discriminative E(n) graph neural networks and integrate them as a differential
equation to obtain an invertible equivariant function: a continuous-time
normalizing flow. We demonstrate that E-NFs considerably outperform baselines
and existing methods from the literature on particle systems such as DW4 and
LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our
knowledge, this is the first flow that jointly generates molecule features and
positions in 3D.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Satorras_V/0/1/0/all/0/1"&gt;Victor Garcia Satorras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1"&gt;Emiel Hoogeboom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuchs_F/0/1/0/all/0/1"&gt;Fabian B. Fuchs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1"&gt;Ingmar Posner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Makes Multimodal Learning Better than Single (Provably). (arXiv:2106.04538v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04538</id>
        <link href="http://arxiv.org/abs/2106.04538"/>
        <updated>2021-06-09T02:01:51.618Z</updated>
        <summary type="html"><![CDATA[The world provides us with data of multiple modalities. Intuitively, models
fusingdata from different modalities outperform unimodal models, since more
informationis aggregated. Recently, joining the success of deep learning, there
is an influentialline of work on deep multimodal learning, which has remarkable
empirical resultson various applications. However, theoretical justifications
in this field are notablylacking.Can multimodal provably perform better than
unimodal? In this paper, we answer this question under a most popular
multimodal learningframework, which firstly encodes features from different
modalities into a commonlatent space and seamlessly maps the latent
representations into the task space. Weprove that learning with multiple
modalities achieves a smaller population risk thanonly using its subset of
modalities. The main intuition is that the former has moreaccurate estimate of
the latent space representation. To the best of our knowledge,this is the first
theoretical treatment to capture important qualitative phenomenaobserved in
real multimodal applications. Combining with experiment results, weshow that
multimodal learning does possess an appealing formal guarantee.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1"&gt;Chenzhuang Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuanyao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longbo Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incentive Mechanism for Privacy-Preserving Federated Learning. (arXiv:2106.04384v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04384</id>
        <link href="http://arxiv.org/abs/2106.04384"/>
        <updated>2021-06-09T02:01:51.613Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is an emerging paradigm for machine learning, in
which data owners can collaboratively train a model by sharing gradients
instead of their raw data. Two fundamental research problems in FL are
incentive mechanism and privacy protection. The former focuses on how to
incentivize data owners to participate in FL. The latter studies how to protect
data owners' privacy while maintaining high utility of trained models. However,
incentive mechanism and privacy protection in FL have been studied separately
and no work solves both problems at the same time. In this work, we address the
two problems simultaneously by an FL-Market that incentivizes data owners'
participation by providing appropriate payments and privacy protection.
FL-Market enables data owners to obtain compensation according to their privacy
loss quantified by local differential privacy (LDP). Our insight is that, by
meeting data owners' personalized privacy preferences and providing appropriate
payments, we can (1) incentivize privacy risk-tolerant data owners to set
larger privacy parameters (i.e., gradients with less noise) and (2) provide
preferred privacy protection for privacy risk-averse data owners. To achieve
this, we design a personalized LDP-based FL framework with a deep
learning-empowered auction mechanism for incentivizing trading gradients with
less noise and optimal aggregation mechanisms for model updates. Our
experiments verify the effectiveness of the proposed framework and mechanisms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuyuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1"&gt;Masatoshi Yoshikawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Policy Comparison under Limited Historical Agent-Environment Interactions. (arXiv:2106.03934v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03934</id>
        <link href="http://arxiv.org/abs/2106.03934"/>
        <updated>2021-06-09T02:01:51.606Z</updated>
        <summary type="html"><![CDATA[We address the challenge of policy evaluation in real-world applications of
reinforcement learning systems where the available historical data is limited
due to ethical, practical, or security considerations. This constrained
distribution of data samples often leads to biased policy evaluation estimates.
To remedy this, we propose that instead of policy evaluation, one should
perform policy comparison, i.e. to rank the policies of interest in terms of
their value based on available historical data. In addition we present the
Limited Data Estimator (LDE) as a simple method for evaluating and comparing
policies from a small number of interactions with the environment. According to
our theoretical analysis, the LDE is shown to be statistically reliable on
policy comparison tasks under mild assumptions on the distribution of the
historical data. Additionally, our numerical experiments compare the LDE to
other policy evaluation methods on the task of policy ranking and demonstrate
its advantage in various settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dereventsov_A/0/1/0/all/0/1"&gt;Anton Dereventsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daws_J/0/1/0/all/0/1"&gt;Joseph D. Daws Jr.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Webster_C/0/1/0/all/0/1"&gt;Clayton Webster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Online Learning for Dynamic k-Clustering. (arXiv:2106.04336v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04336</id>
        <link href="http://arxiv.org/abs/2106.04336"/>
        <updated>2021-06-09T02:01:51.590Z</updated>
        <summary type="html"><![CDATA[We study dynamic clustering problems from the perspective of online learning.
We consider an online learning problem, called \textit{Dynamic $k$-Clustering},
in which $k$ centers are maintained in a metric space over time (centers may
change positions) such as a dynamically changing set of $r$ clients is served
in the best possible way. The connection cost at round $t$ is given by the
\textit{$p$-norm} of the vector consisting of the distance of each client to
its closest center at round $t$, for some $p\geq 1$ or $p = \infty$. We present
a \textit{$\Theta\left( \min(k,r) \right)$-regret} polynomial-time online
learning algorithm and show that, under some well-established computational
complexity conjectures, \textit{constant-regret} cannot be achieved in
polynomial-time. In addition to the efficient solution of Dynamic
$k$-Clustering, our work contributes to the long line of research on
combinatorial online learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1"&gt;Dimitris Fotakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1"&gt;Georgios Piliouras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skoulakis_S/0/1/0/all/0/1"&gt;Stratis Skoulakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Topology Divergence: a Framework for Comparing Data Manifolds. (arXiv:2106.04024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04024</id>
        <link href="http://arxiv.org/abs/2106.04024"/>
        <updated>2021-06-09T02:01:51.583Z</updated>
        <summary type="html"><![CDATA[We develop a framework for comparing data manifolds, aimed, in particular,
towards the evaluation of deep generative models. We describe a novel tool,
Cross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional
space, tracks multiscale topology spacial discrepancies between manifolds on
which the distributions are concentrated. Based on the Cross-Barcode, we
introduce the Manifold Topology Divergence score (MTop-Divergence) and apply it
to assess the performance of deep generative models in various domains: images,
3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN,
CIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate
that the MTop-Divergence accurately detects various degrees of mode-dropping,
intra-mode collapse, mode invention, and image disturbance. Our algorithm
scales well (essentially linearly) with the increase of the dimension of the
ambient high-dimensional space. It is one of the first TDA-based practical
methodologies that can be applied universally to datasets of different sizes
and dimensions, including the ones on which the most recent GANs in the visual
domain are trained. The proposed method is domain agnostic and does not rely on
pre-trained networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1"&gt;Serguei Barannikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1"&gt;Ilya Trofimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotnikov_G/0/1/0/all/0/1"&gt;Grigorii Sotnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trimbach_E/0/1/0/all/0/1"&gt;Ekaterina Trimbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1"&gt;Alexander Korotin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1"&gt;Alexander Filippov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Anomalous Event Sequences with Temporal Point Processes. (arXiv:2106.04465v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04465</id>
        <link href="http://arxiv.org/abs/2106.04465"/>
        <updated>2021-06-09T02:01:51.578Z</updated>
        <summary type="html"><![CDATA[Automatically detecting anomalies in event data can provide substantial value
in domains such as healthcare, DevOps, and information security. In this paper,
we frame the problem of detecting anomalous continuous-time event sequences as
out-of-distribution (OoD) detection for temporal point processes (TPPs). First,
we show how this problem can be approached using goodness-of-fit (GoF) tests.
We then demonstrate the limitations of popular GoF statistics for TPPs and
propose a new test that addresses these shortcomings. The proposed method can
be combined with various TPP models, such as neural TPPs, and is easy to
implement. In our experiments, we show that the proposed statistic excels at
both traditional GoF testing, as well as at detecting anomalies in simulated
and real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shchur_O/0/1/0/all/0/1"&gt;Oleksandr Shchur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turkmen_A/0/1/0/all/0/1"&gt;Ali Caner T&amp;#xfc;rkmen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Januschowski_T/0/1/0/all/0/1"&gt;Tim Januschowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasthaus_J/0/1/0/all/0/1"&gt;Jan Gasthaus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GSVNet: Guided Spatially-Varying Convolution for Fast Semantic Segmentation on Video. (arXiv:2103.08834v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08834</id>
        <link href="http://arxiv.org/abs/2103.08834"/>
        <updated>2021-06-09T02:01:51.572Z</updated>
        <summary type="html"><![CDATA[This paper addresses fast semantic segmentation on video.Video segmentation
often calls for real-time, or even fasterthan real-time, processing. One common
recipe for conserving computation arising from feature extraction is to
propagate features of few selected keyframes. However, recent advances in fast
image segmentation make these solutions less attractive. To leverage fast image
segmentation for furthering video segmentation, we propose a simple yet
efficient propagation framework. Specifically, we perform lightweight flow
estimation in 1/8-downscaled image space for temporal warping in segmentation
outpace space. Moreover, we introduce a guided spatially-varying convolution
for fusing segmentations derived from the previous and current frames, to
mitigate propagation error and enable lightweight feature extraction on
non-keyframes. Experimental results on Cityscapes and CamVid show that our
scheme achieves the state-of-the-art accuracy-throughput trade-off on video
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Shih-Po Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Si-Cun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wen-Hsiao Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of data-splits on generalization: Identifying COVID-19 from cough and context. (arXiv:2106.03851v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.03851</id>
        <link href="http://arxiv.org/abs/2106.03851"/>
        <updated>2021-06-09T02:01:51.567Z</updated>
        <summary type="html"><![CDATA[Rapidly scaling screening, testing and quarantine has shown to be an
effective strategy to combat the COVID-19 pandemic. We consider the application
of deep learning techniques to distinguish individuals with COVID from
non-COVID by using data acquirable from a phone. Using cough and context
(symptoms and meta-data) represent such a promising approach. Several
independent works in this direction have shown promising results. However, none
of them report performance across clinically relevant data splits.
Specifically, the performance where the development and test sets are split in
time (retrospective validation) and across sites (broad validation). Although
there is meaningful generalization across these splits the performance
significantly varies (up to 0.1 AUC score). In addition, we study the
performance of symptomatic and asymptomatic individuals across these three
splits. Finally, we show that our model focuses on meaningful features of the
input, cough bouts for cough and relevant symptoms for context. The code and
checkpoints are available at https://github.com/WadhwaniAI/cough-against-covid]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1"&gt;Makkunda Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_N/0/1/0/all/0/1"&gt;Nikhil Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_J/0/1/0/all/0/1"&gt;Jigar Doshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagad_P/0/1/0/all/0/1"&gt;Piyush Bagad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalmia_A/0/1/0/all/0/1"&gt;Aman Dalmia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhamare_P/0/1/0/all/0/1"&gt;Parag Bhamare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahale_A/0/1/0/all/0/1"&gt;Amrita Mahale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rane_S/0/1/0/all/0/1"&gt;Saurabh Rane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_N/0/1/0/all/0/1"&gt;Neeraj Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panicker_R/0/1/0/all/0/1"&gt;Rahul Panicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning. (arXiv:2106.04480v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04480</id>
        <link href="http://arxiv.org/abs/2106.04480"/>
        <updated>2021-06-09T02:01:51.552Z</updated>
        <summary type="html"><![CDATA[We propose to learn to distinguish reversible from irreversible actions for
better informed decision-making in Reinforcement Learning (RL). From
theoretical considerations, we show that approximate reversibility can be
learned through a simple surrogate task: ranking randomly sampled trajectory
events in chronological order. Intuitively, pairs of events that are always
observed in the same order are likely to be separated by an irreversible
sequence of actions. Conveniently, learning the temporal order of events can be
done in a fully self-supervised way, which we use to estimate the reversibility
of actions from experience, without any priors. We propose two different
strategies that incorporate reversibility in RL agents, one strategy for
exploration (RAE) and one strategy for control (RAC). We demonstrate the
potential of reversibility-aware agents in several environments, including the
challenging Sokoban game. In synthetic tasks, we show that we can learn control
policies that never fail and reduce to zero the side-effects of interactions,
even without access to the reward function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grinsztajn_N/0/1/0/all/0/1"&gt;Nathan Grinsztajn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferret_J/0/1/0/all/0/1"&gt;Johan Ferret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1"&gt;Olivier Pietquin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1"&gt;Philippe Preux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1"&gt;Matthieu Geist&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Properties of Vision Transformers. (arXiv:2105.10497v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10497</id>
        <link href="http://arxiv.org/abs/2105.10497"/>
        <updated>2021-06-09T02:01:51.547Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViT) have demonstrated impressive performance across
various machine vision problems. These models are based on multi-head
self-attention mechanisms that can flexibly attend to a sequence of image
patches to encode contextual cues. An important question is how such
flexibility in attending image-wide context conditioned on a given patch can
facilitate handling nuisances in natural images e.g., severe occlusions, domain
shifts, spatial permutations, adversarial and natural perturbations. We
systematically study this question via an extensive set of experiments
encompassing three ViT families and comparisons with a high-performing
convolutional neural network (CNN). We show and analyze the following
intriguing properties of ViT: (a) Transformers are highly robust to severe
occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1
accuracy on ImageNet even after randomly occluding 80% of the image content.
(b) The robust performance to occlusions is not due to a bias towards local
textures, and ViTs are significantly less biased towards textures compared to
CNNs. When properly trained to encode shape-based features, ViTs demonstrate
shape recognition capability comparable to that of human visual system,
previously unmatched in the literature. (c) Using ViTs to encode shape
representation leads to an interesting consequence of accurate semantic
segmentation without pixel-level supervision. (d) Off-the-shelf features from a
single ViT model can be combined to create a feature ensemble, leading to high
accuracy rates across a range of classification datasets in both traditional
and few-shot learning paradigms. We show effective features of ViTs are due to
flexible and dynamic receptive fields possible via the self-attention
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1"&gt;Munawar Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive transfer learning. (arXiv:2106.04455v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04455</id>
        <link href="http://arxiv.org/abs/2106.04455"/>
        <updated>2021-06-09T02:01:51.541Z</updated>
        <summary type="html"><![CDATA[In transfer learning, we wish to make inference about a target population
when we have access to data both from the distribution itself, and from a
different but related source distribution. We introduce a flexible framework
for transfer learning in the context of binary classification, allowing for
covariate-dependent relationships between the source and target distributions
that are not required to preserve the Bayes decision boundary. Our main
contributions are to derive the minimax optimal rates of convergence (up to
poly-logarithmic factors) in this problem, and show that the optimal rate can
be achieved by an algorithm that adapts to key aspects of the unknown transfer
relationship, as well as the smoothness and tail parameters of our
distributional classes. This optimal rate turns out to have several regimes,
depending on the interplay between the relative sample sizes and the strength
of the transfer relationship, and our algorithm achieves optimality by careful,
decision tree-based calibration of local nearest-neighbour procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Reeve_H/0/1/0/all/0/1"&gt;Henry W. J. Reeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cannings_T/0/1/0/all/0/1"&gt;Timothy I. Cannings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Samworth_R/0/1/0/all/0/1"&gt;Richard J. Samworth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Elastic Lottery Ticket Hypothesis. (arXiv:2103.16547v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16547</id>
        <link href="http://arxiv.org/abs/2103.16547"/>
        <updated>2021-06-09T02:01:51.536Z</updated>
        <summary type="html"><![CDATA[Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse
trainable subnetworks, or winning tickets, of training, which can be trained in
isolation to achieve similar or even better performance compared to the full
models. Despite many efforts being made, the most effective method to identify
such winning tickets is still Iterative Magnitude-based Pruning (IMP), which is
computationally expensive and has to be run thoroughly for every different
network. A natural question that comes in is: can we "transform" the winning
ticket found in one network to another with a different architecture, yielding
a winning ticket for the latter at the beginning, without re-doing the
expensive IMP? Answering this question is not only practically relevant for
efficient "once-for-all" winning ticket finding, but also theoretically
appealing for uncovering inherently scalable sparse patterns in networks. We
conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety
of strategies to tweak the winning tickets found from different networks of the
same model family (e.g., ResNets). Based on these results, we articulate the
Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or
dropping) and re-ordering layers for one network, its corresponding winning
ticket could be stretched (or squeezed) into a subnetwork for another deeper
(or shallower) network from the same family, whose performance is nearly the
same competitive as the latter's winning ticket directly found by IMP. We have
also thoroughly compared E-LTH with pruning-at-initialization and dynamic
sparse training methods, and discuss the generalizability of E-LTH to different
model families, layer types, or across datasets. Code is available at
https://github.com/VITA-Group/ElasticLTH.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustifying $\ell_\infty$ Adversarial Training to the Union of Perturbation Models. (arXiv:2105.14710v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14710</id>
        <link href="http://arxiv.org/abs/2105.14710"/>
        <updated>2021-06-09T02:01:51.520Z</updated>
        <summary type="html"><![CDATA[Classical adversarial training (AT) frameworks are designed to achieve high
adversarial accuracy against a single attack type, typically $\ell_\infty$
norm-bounded perturbations. Recent extensions in AT have focused on defending
against the union of multiple perturbations but this benefit is obtained at the
expense of a significant (up to $10\times$) increase in training complexity
over single-attack $\ell_\infty$ AT. In this work, we expand the capabilities
of widely popular single-attack $\ell_\infty$ AT frameworks to provide
robustness to the union of ($\ell_\infty, \ell_2, \ell_1$) perturbations while
preserving their training efficiency. Our technique, referred to as Shaped
Noise Augmented Processing (SNAP), exploits a well-established byproduct of
single-attack AT frameworks -- the reduction in the curvature of the decision
boundary of networks. SNAP prepends a given deep net with a shaped noise
augmentation layer whose distribution is learned along with network parameters
using any standard single-attack AT. As a result, SNAP enhances adversarial
accuracy of ResNet-18 on CIFAR-10 against the union of ($\ell_\infty, \ell_2,
\ell_1$) perturbations by 14%-to-20% for four state-of-the-art (SOTA)
single-attack $\ell_\infty$ AT frameworks, and, for the first time, establishes
a benchmark for ResNet-50 and ResNet-101 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1"&gt;Ameya D. Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuttle_M/0/1/0/all/0/1"&gt;Michael Tuttle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander G. Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanbhag_N/0/1/0/all/0/1"&gt;Naresh R. Shanbhag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Feature Learning for Manipulation with Contrastive Domain Randomization. (arXiv:2103.11144v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11144</id>
        <link href="http://arxiv.org/abs/2103.11144"/>
        <updated>2021-06-09T02:01:51.515Z</updated>
        <summary type="html"><![CDATA[Robotic tasks such as manipulation with visual inputs require image features
that capture the physical properties of the scene, e.g., the position and
configuration of objects. Recently, it has been suggested to learn such
features in an unsupervised manner from simulated, self-supervised, robot
interaction; the idea being that high-level physical properties are well
captured by modern physical simulators, and their representation from visual
inputs may transfer well to the real world. In particular, learning methods
based on noise contrastive estimation have shown promising results. To
robustify the simulation-to-real transfer, domain randomization (DR) was
suggested for learning features that are invariant to irrelevant visual
properties such as textures or lighting. In this work, however, we show that a
naive application of DR to unsupervised learning based on contrastive
estimation does not promote invariance, as the loss function maximizes mutual
information between the features and both the relevant and irrelevant visual
properties. We propose a simple modification of the contrastive loss to fix
this, exploiting the fact that we can control the simulated randomization of
visual properties. Our approach learns physical features that are significantly
more robust to visual domain variation, as we demonstrate using both rigid and
non-rigid objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rabinovitz_C/0/1/0/all/0/1"&gt;Carmel Rabinovitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grupen_N/0/1/0/all/0/1"&gt;Niko Grupen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1"&gt;Aviv Tamar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling. (arXiv:2102.13156v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13156</id>
        <link href="http://arxiv.org/abs/2102.13156"/>
        <updated>2021-06-09T02:01:51.509Z</updated>
        <summary type="html"><![CDATA[Integrating physics models within machine learning models holds considerable
promise toward learning robust models with improved interpretability and
abilities to extrapolate. In this work, we focus on the integration of
incomplete physics models into deep generative models. In particular, we
introduce an architecture of variational autoencoders (VAEs) in which a part of
the latent space is grounded by physics. A key technical challenge is to strike
a balance between the incomplete physics and trainable components such as
neural networks for ensuring that the physics part is used in a meaningful
manner. To this end, we propose a regularized learning method that controls the
effect of the trainable components and preserves the semantics of the
physics-based latent variables as intended. We not only demonstrate generative
performance improvements over a set of synthetic and real-world datasets, but
we also show that we learn robust models that can consistently extrapolate
beyond the training distribution in a meaningful manner. Moreover, we show that
we can control the generative process in an interpretable manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takeishi_N/0/1/0/all/0/1"&gt;Naoya Takeishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1"&gt;Alexandros Kalousis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deeply-Debiased Off-Policy Interval Estimation. (arXiv:2105.04646v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04646</id>
        <link href="http://arxiv.org/abs/2105.04646"/>
        <updated>2021-06-09T02:01:51.504Z</updated>
        <summary type="html"><![CDATA[Off-policy evaluation learns a target policy's value with a historical
dataset generated by a different behavior policy. In addition to a point
estimate, many applications would benefit significantly from having a
confidence interval (CI) that quantifies the uncertainty of the point estimate.
In this paper, we propose a novel deeply-debiasing procedure to construct an
efficient, robust, and flexible CI on a target policy's value. Our method is
justified by theoretical results and numerical experiments. A Python
implementation of the proposed procedure is available at
https://github.com/RunzheStat/D2OPE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chengchun Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wan_R/0/1/0/all/0/1"&gt;Runzhe Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1"&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Song_R/0/1/0/all/0/1"&gt;Rui Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sublinear Least-Squares Value Iteration via Locality Sensitive Hashing. (arXiv:2105.08285v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08285</id>
        <link href="http://arxiv.org/abs/2105.08285"/>
        <updated>2021-06-09T02:01:51.499Z</updated>
        <summary type="html"><![CDATA[We present the first provable Least-Squares Value Iteration (LSVI) algorithms
that have runtime complexity sublinear in the number of actions. We formulate
the value function estimation procedure in value iteration as an approximate
maximum inner product search problem and propose a locality sensitive hashing
(LSH) [Indyk and Motwani STOC'98, Andoni and Razenshteyn STOC'15, Andoni,
Laarhoven, Razenshteyn and Waingarten SODA'17] type data structure to solve
this problem with sublinear time complexity. Moreover, we build the connections
between the theory of approximate maximum inner product search and the regret
analysis of reinforcement learning. We prove that, with our choice of
approximation factor, our Sublinear LSVI algorithms maintain the same regret as
the original LSVI algorithms while reducing the runtime complexity to sublinear
in the number of actions. To the best of our knowledge, this is the first work
that combines LSH with reinforcement learning resulting in provable
improvements. We hope that our novel way of combining data-structures and
iterative algorithm will open the door for further study into cost reduction in
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhaozhuo Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Learning in Online Queuing Systems. (arXiv:2106.04228v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04228</id>
        <link href="http://arxiv.org/abs/2106.04228"/>
        <updated>2021-06-09T02:01:51.484Z</updated>
        <summary type="html"><![CDATA[Motivated by packet routing in computer networks, online queuing systems are
composed of queues receiving packets at different rates. Repeatedly, they send
packets to servers, each of them treating only at most one packet at a time. In
the centralized case, the number of accumulated packets remains bounded (i.e.,
the system is \textit{stable}) as long as the ratio between service rates and
arrival rates is larger than $1$. In the decentralized case, individual
no-regret strategies ensures stability when this ratio is larger than $2$. Yet,
myopically minimizing regret disregards the long term effects due to the
carryover of packets to further rounds. On the other hand, minimizing long term
costs leads to stable Nash equilibria as soon as the ratio exceeds
$\frac{e}{e-1}$. Stability with decentralized learning strategies with a ratio
below $2$ was a major remaining question. We first argue that for ratios up to
$2$, cooperation is required for stability of learning strategies, as selfish
minimization of policy regret, a \textit{patient} notion of regret, might
indeed still be unstable in this case. We therefore consider cooperative queues
and propose the first learning decentralized algorithm guaranteeing stability
of the system as long as the ratio of rates is larger than $1$, thus reaching
performances comparable to centralized strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sentenac_F/0/1/0/all/0/1"&gt;Flore Sentenac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Boursier_E/0/1/0/all/0/1"&gt;Etienne Boursier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Perchet_V/0/1/0/all/0/1"&gt;Vianney Perchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cyberbullying Detection Using Deep Neural Network from Social Media Comments in Bangla Language. (arXiv:2106.04506v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04506</id>
        <link href="http://arxiv.org/abs/2106.04506"/>
        <updated>2021-06-09T02:01:51.468Z</updated>
        <summary type="html"><![CDATA[Cyberbullying or Online harassment detection on social media for various
major languages is currently being given a good amount of focus by researchers
worldwide. Being the seventh most speaking language in the world and increasing
usage of online platform among the Bengali speaking people urge to find
effective detection technique to handle the online harassment. In this paper,
we have proposed binary and multiclass classification model using hybrid neural
network for bully expression detection in Bengali language. We have used 44,001
users comments from popular public Facebook pages, which fall into five classes
- Non-bully, Sexual, Threat, Troll and Religious. We have examined the
performance of our proposed models from different perspective. Our binary
classification model gives 87.91% accuracy, whereas introducing ensemble
technique after neural network for multiclass classification, we got 85%
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1"&gt;Md Faisal Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmud_Z/0/1/0/all/0/1"&gt;Zalish Mahmud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biash_Z/0/1/0/all/0/1"&gt;Zarin Tasnim Biash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryen_A/0/1/0/all/0/1"&gt;Ahmed Ann Noor Ryen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_A/0/1/0/all/0/1"&gt;Arman Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashraf_F/0/1/0/all/0/1"&gt;Faisal Bin Ashraf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Struggle with Academic Plagiarism: Approaches based on Semantic Similarity. (arXiv:2106.04404v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04404</id>
        <link href="http://arxiv.org/abs/2106.04404"/>
        <updated>2021-06-09T02:01:51.443Z</updated>
        <summary type="html"><![CDATA[Academic plagiarism is a serious problem nowadays. Due to the existence of
inexhaustible sources of digital information, today it is easier to plagiarize
more than ever before. The good thing is that plagiarism detection techniques
have improved and are powerful enough to detect attempts of plagiarism in
education. We are now witnessing efficient plagiarism detection software in
action, such as Turnitin, iThenticate or SafeAssign. In the introduction we
explore software that is used within the Croatian academic community for
plagiarism detection in universities and/or in scientific journals. The
question is: is this enough? Current software has proven to be successful,
however the problem of identifying paraphrasing or obfuscation plagiarism
remains unresolved. In this paper we present a report of how semantic
similarity measures can be used in the plagiarism detection task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vrbanec_T/0/1/0/all/0/1"&gt;Tedo Vrbanec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mestrovic_A/0/1/0/all/0/1"&gt;Ana Mestrovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Property-Aware Robot Object Manipulation: a Generative Approach. (arXiv:2106.04385v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04385</id>
        <link href="http://arxiv.org/abs/2106.04385"/>
        <updated>2021-06-09T02:01:51.315Z</updated>
        <summary type="html"><![CDATA[When transporting an object, we unconsciously adapt our movement to its
properties, for instance by slowing down when the item is fragile. The most
relevant features of an object are immediately revealed to a human observer by
the way the handling occurs, without any need for verbal description. It would
greatly facilitate collaboration to enable humanoid robots to perform movements
that convey similar intuitive cues to the observers. In this work, we focus on
how to generate robot motion adapted to the hidden properties of the
manipulated objects, such as their weight and fragility. We explore the
possibility of leveraging Generative Adversarial Networks to synthesize new
actions coherent with the properties of the object. The use of a generative
approach allows us to create new and consistent motion patterns, without the
need of collecting a large number of recorded human-led demonstrations.
Besides, the informative content of the actions is preserved. Our results show
that Generative Adversarial Nets can be a powerful tool for the generation of
novel and meaningful transportation actions, which result effectively modulated
as a function of the object weight and the carefulness required in its
handling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garello_L/0/1/0/all/0/1"&gt;Luca Garello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lastrico_L/0/1/0/all/0/1"&gt;Linda Lastrico&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rea_F/0/1/0/all/0/1"&gt;Francesco Rea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mastrogiovanni_F/0/1/0/all/0/1"&gt;Fulvio Mastrogiovanni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noceti_N/0/1/0/all/0/1"&gt;Nicoletta Noceti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sciutti_A/0/1/0/all/0/1"&gt;Alessandra Sciutti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Generation of Machine Learning Synthetic Data Using ROS. (arXiv:2106.04547v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04547</id>
        <link href="http://arxiv.org/abs/2106.04547"/>
        <updated>2021-06-09T02:01:51.309Z</updated>
        <summary type="html"><![CDATA[Data labeling is a time intensive process. As such, many data scientists use
various tools to aid in the data generation and labeling process. While these
tools help automate labeling, many still require user interaction throughout
the process. Additionally, most target only a few network frameworks. Any
researchers exploring multiple frameworks must find additional tools orwrite
conversion scripts. This paper presents an automated tool for generating
synthetic data in arbitrary network formats. It uses Robot Operating System
(ROS) and Gazebo, which are common tools in the robotics community. Through ROS
paradigms, it allows extensive user customization of the simulation environment
and data generation process. Additionally, a plugin-like framework allows the
development of arbitrary data format writers without the need to change the
main body of code. Using this tool, the authors were able to generate an
arbitrarily large image dataset for three unique training formats using
approximately 15 min of user setup time and a variable amount of hands-off run
time, depending on the dataset size. The source code for this data generation
tool is available at https://github.com/Navy-RISE-Lab/nn_data_collection]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hart_K/0/1/0/all/0/1"&gt;Kyle M. Hart&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_A/0/1/0/all/0/1"&gt;Ari B. Goodman&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+OShea_R/0/1/0/all/0/1"&gt;Ryan P. O&amp;#x27;Shea&lt;/a&gt; (1) ((1) Naval Air Warfare Center - Aircraft Division - Lakehurst)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Control with Graph Neural Networks. (arXiv:2012.14906v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14906</id>
        <link href="http://arxiv.org/abs/2012.14906"/>
        <updated>2021-06-09T02:01:51.166Z</updated>
        <summary type="html"><![CDATA[Dynamical systems consisting of a set of autonomous agents face the challenge
of having to accomplish a global task, relying only on local information. While
centralized controllers are readily available, they face limitations in terms
of scalability and implementation, as they do not respect the distributed
information structure imposed by the network system of agents. Given the
difficulties in finding optimal decentralized controllers, we propose a novel
framework using graph neural networks (GNNs) to \emph{learn} these controllers.
GNNs are well-suited for the task since they are naturally distributed
architectures and exhibit good scalability and transferability properties. The
problems of flocking and multi-agent path planning are explored to illustrate
the potential of GNNs in learning decentralized controllers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1"&gt;Fernando Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingbiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolstaya_E/0/1/0/all/0/1"&gt;Ekaterina Tolstaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1"&gt;Amanda Prorok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Deep Inverse Rosenblatt Transports. (arXiv:2106.04170v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04170</id>
        <link href="http://arxiv.org/abs/2106.04170"/>
        <updated>2021-06-09T02:01:51.160Z</updated>
        <summary type="html"><![CDATA[We present a novel offline-online method to mitigate the computational burden
of the characterization of conditional beliefs in statistical learning. In the
offline phase, the proposed method learns the joint law of the belief random
variables and the observational random variables in the tensor-train (TT)
format. In the online phase, it utilizes the resulting order-preserving
conditional transport map to issue real-time characterization of the
conditional beliefs given new observed information. Compared with the
state-of-the-art normalizing flows techniques, the proposed method relies on
function approximation and is equipped with thorough performance analysis. This
also allows us to further extend the capability of transport maps in
challenging problems with high-dimensional observations and high-dimensional
belief variables. On the one hand, we present novel heuristics to reorder
and/or reparametrize the variables to enhance the approximation power of TT. On
the other, we integrate the TT-based transport maps and the parameter
reordering/reparametrization into layered compositions to further improve the
performance of the resulting transport maps. We demonstrate the efficiency of
the proposed method on various statistical learning tasks in ordinary
differential equations (ODEs) and partial differential equations (PDEs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cui_T/0/1/0/all/0/1"&gt;Tiangang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dolgov_S/0/1/0/all/0/1"&gt;Sergey Dolgov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zahm_O/0/1/0/all/0/1"&gt;Olivier Zahm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Loss Surfaces of Neural Networks with General Activation Functions. (arXiv:2004.03959v3 [math.PR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03959</id>
        <link href="http://arxiv.org/abs/2004.03959"/>
        <updated>2021-06-09T02:01:51.146Z</updated>
        <summary type="html"><![CDATA[The loss surfaces of deep neural networks have been the subject of several
studies, theoretical and experimental, over the last few years. One strand of
work considers the complexity, in the sense of local optima, of high
dimensional random functions with the aim of informing how local optimisation
methods may perform in such complicated settings. Prior work of Choromanska et
al (2015) established a direct link between the training loss surfaces of deep
multi-layer perceptron networks and spherical multi-spin glass models under
some very strong assumptions on the network and its data. In this work, we test
the validity of this approach by removing the undesirable restriction to ReLU
activation functions. In doing so, we chart a new path through the spin glass
complexity calculations using supersymmetric methods in Random Matrix Theory
which may prove useful in other contexts. Our results shed new light on both
the strengths and the weaknesses of spin glass models in this context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Baskerville_N/0/1/0/all/0/1"&gt;Nicholas P. Baskerville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Keating_J/0/1/0/all/0/1"&gt;Jonathan P. Keating&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mezzadri_F/0/1/0/all/0/1"&gt;Francesco Mezzadri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Najnudel_J/0/1/0/all/0/1"&gt;Joseph Najnudel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deterministic Neural Networks with Inductive Biases Capture Epistemic and Aleatoric Uncertainty. (arXiv:2102.11582v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11582</id>
        <link href="http://arxiv.org/abs/2102.11582"/>
        <updated>2021-06-09T02:01:51.140Z</updated>
        <summary type="html"><![CDATA[We show that a single softmax neural net with minimal changes can beat the
uncertainty predictions of Deep Ensembles and other more complex
single-forward-pass uncertainty approaches. Standard softmax neural nets suffer
from feature collapse and extrapolate arbitrarily for OoD points. This results
in arbitrary softmax entropies for OoD points which can have high entropy, low,
or anything in between, thus cannot capture epistemic uncertainty reliably. We
prove that this failure lies at the core of "why" Deep Ensemble Uncertainty
works well. Instead of using softmax entropy, we show that with appropriate
inductive biases softmax neural nets trained with maximum likelihood reliably
capture epistemic uncertainty through their feature-space density. This density
is obtained using simple Gaussian Discriminant Analysis, but it cannot
represent aleatoric uncertainty reliably. We show that it is necessary to
combine feature-space density with softmax entropy to disentangle uncertainties
well. We evaluate the epistemic uncertainty quality on active learning and OoD
detection, achieving SOTA ~98 AUROC on CIFAR-10 vs SVHN without fine-tuning on
OoD data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukhoti_J/0/1/0/all/0/1"&gt;Jishnu Mukhoti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1"&gt;Andreas Kirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amersfoort_J/0/1/0/all/0/1"&gt;Joost van Amersfoort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A low discrepancy sequence on graphs. (arXiv:2010.04227v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04227</id>
        <link href="http://arxiv.org/abs/2010.04227"/>
        <updated>2021-06-09T02:01:51.135Z</updated>
        <summary type="html"><![CDATA[Many applications such as election forecasting, environmental monitoring,
health policy, and graph based machine learning require taking expectation of
functions defined on the vertices of a graph. We describe a construction of a
sampling scheme analogous to the so called Leja points in complex potential
theory that can be proved to give low discrepancy estimates for the
approximation of the expected value by the impirical expected value based on
these points. In contrast to classical potential theory where the kernel is
fixed and the equilibrium distribution depends upon the kernel, we fix a
probability distribution and construct a kernel (which represents the graph
structure) for which the equilibrium distribution is the given probability
distribution. Our estimates do not depend upon the size of the graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cloninger_A/0/1/0/all/0/1"&gt;A. Cloninger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mhaskar_H/0/1/0/all/0/1"&gt;H. N. Mhaskar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the benefits of defining vicinal distributions in latent space. (arXiv:2003.06566v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.06566</id>
        <link href="http://arxiv.org/abs/2003.06566"/>
        <updated>2021-06-09T02:01:51.129Z</updated>
        <summary type="html"><![CDATA[The vicinal risk minimization (VRM) principle is an empirical risk
minimization (ERM) variant that replaces Dirac masses with vicinal functions.
There is strong numerical and theoretical evidence showing that VRM outperforms
ERM in terms of generalization if appropriate vicinal functions are chosen.
Mixup Training (MT), a popular choice of vicinal distribution, improves the
generalization performance of models by introducing globally linear behavior in
between training examples. Apart from generalization, recent works have shown
that mixup trained models are relatively robust to input
perturbations/corruptions and at the same time are calibrated better than their
non-mixup counterparts. In this work, we investigate the benefits of defining
these vicinal distributions like mixup in latent space of generative models
rather than in input space itself. We propose a new approach - \textit{VarMixup
(Variational Mixup)} - to better sample mixup images by using the latent
manifold underlying the data. Our empirical studies on CIFAR-10, CIFAR-100, and
Tiny-ImageNet demonstrate that models trained by performing mixup in the latent
manifold learned by VAEs are inherently more robust to various input
corruptions/perturbations, are significantly better calibrated, and exhibit
more local-linear loss landscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1"&gt;Puneet Mangla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1"&gt;Vedant Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1"&gt;Shreyas Jayant Havaldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications. (arXiv:1703.01610v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1703.01610</id>
        <link href="http://arxiv.org/abs/1703.01610"/>
        <updated>2021-06-09T02:01:51.124Z</updated>
        <summary type="html"><![CDATA[We study combinatorial multi-armed bandit with probabilistically triggered
arms (CMAB-T) and semi-bandit feedback. We resolve a serious issue in the prior
CMAB-T studies where the regret bounds contain a possibly exponentially large
factor of $1/p^*$, where $p^*$ is the minimum positive probability that an arm
is triggered by any action. We address this issue by introducing a triggering
probability modulated (TPM) bounded smoothness condition into the general
CMAB-T framework, and show that many applications such as influence
maximization bandit and combinatorial cascading bandit satisfy this TPM
condition. As a result, we completely remove the factor of $1/p^*$ from the
regret bounds, achieving significantly better regret bounds for influence
maximization and cascading bandits than before. Finally, we provide lower bound
results showing that the factor $1/p^*$ is unavoidable for general CMAB-T
problems, suggesting that the TPM condition is crucial in removing this factor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qinshi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of Assumptions in Bayesian Optimisation. (arXiv:2012.03826v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03826</id>
        <link href="http://arxiv.org/abs/2012.03826"/>
        <updated>2021-06-09T02:01:51.108Z</updated>
        <summary type="html"><![CDATA[Inspired by the increasing desire to efficiently tune machine learning
hyper-parameters, in this work we rigorously analyse conventional and
non-conventional assumptions inherent to Bayesian optimisation. Across an
extensive set of experiments we conclude that: 1) the majority of
hyper-parameter tuning tasks exhibit heteroscedasticity and non-stationarity,
2) multi-objective acquisition ensembles with Pareto-front solutions
significantly improve queried configurations, and 3) robust acquisition
maximisation affords empirical advantages relative to its non-robust
counterparts. We hope these findings may serve as guiding principles, both for
practitioners and for further research in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cowen_Rivers_A/0/1/0/all/0/1"&gt;Alexander I. Cowen-Rivers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1"&gt;Wenlong Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tutunov_R/0/1/0/all/0/1"&gt;Rasul Tutunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grosnit_A/0/1/0/all/0/1"&gt;Antoine Grosnit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1"&gt;Ryan Rhys Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jianye_H/0/1/0/all/0/1"&gt;Hao Jianye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ammar_H/0/1/0/all/0/1"&gt;Haitham Bou Ammar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization. (arXiv:2105.12002v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12002</id>
        <link href="http://arxiv.org/abs/2105.12002"/>
        <updated>2021-06-09T02:01:51.102Z</updated>
        <summary type="html"><![CDATA[The Lottery Ticket Hypothesis suggests that an over-parametrized network
consists of ``lottery tickets'', and training a certain collection of them
(i.e., a subnetwork) can match the performance of the full model. In this
paper, we study such a collection of tickets, which is referred to as ``winning
tickets'', in extremely over-parametrized models, e.g., pre-trained language
models. We observe that at certain compression ratios, the generalization
performance of the winning tickets can not only match but also exceed that of
the full model. In particular, we observe a phase transition phenomenon: As the
compression ratio increases, generalization performance of the winning tickets
first improves then deteriorates after a certain threshold. We refer to the
tickets on the threshold as ``super tickets''. We further show that the phase
transition is task and model dependent -- as the model size becomes larger and
the training data set becomes smaller, the transition becomes more pronounced.
Our experiments on the GLUE benchmark show that the super tickets improve
single task fine-tuning by $0.9$ points on BERT-base and $1.0$ points on
BERT-large, in terms of task-average score. We also demonstrate that adaptively
sharing the super tickets across tasks benefits multi-task learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1"&gt;Simiao Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minshuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus Spike Proteins. (arXiv:2105.00351v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00351</id>
        <link href="http://arxiv.org/abs/2105.00351"/>
        <updated>2021-06-09T02:01:51.070Z</updated>
        <summary type="html"><![CDATA[Topological data analysis, including persistent homology, has undergone
significant development in recent years. However, one outstanding challenge is
to build a coherent statistical inference procedure on persistent diagrams. The
paired dependent data structure, as birth and death in persistent diagrams,
adds additional complexity to the development. In this paper, we present a new
lattice path representation for persistent diagrams. A new exact statistical
inference procedure is developed for lattice paths via combinatorial
enumerations. The proposed lattice path method is applied to the topological
characterization of the protein structures of COVID-19 viruse. We demonstrate
that there are topological changes during the conformation change of spike
proteins that are needed to initiate the infection of host cells.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1"&gt;Moo K. Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1"&gt;Hernando Ombao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness Through Regularization for Learning to Rank. (arXiv:2102.05996v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05996</id>
        <link href="http://arxiv.org/abs/2102.05996"/>
        <updated>2021-06-09T02:01:51.064Z</updated>
        <summary type="html"><![CDATA[Given the abundance of applications of ranking in recent years, addressing
fairness concerns around automated ranking systems becomes necessary for
increasing the trust among end-users. Previous work on fair ranking has mostly
focused on application-specific fairness notions, often tailored to online
advertising, and it rarely considers learning as part of the process. In this
work, we show how to transfer numerous fairness notions from binary
classification to a learning to rank setting. Our formalism allows us to design
methods for incorporating fairness objectives with provable generalization
guarantees. An extensive experimental evaluation shows that our method can
improve ranking fairness substantially with no or only little loss of model
quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinov_N/0/1/0/all/0/1"&gt;Nikola Konstantinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1"&gt;Christoph H. Lampert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactuals and Causability in Explainable Artificial Intelligence: Theory, Algorithms, and Applications. (arXiv:2103.04244v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04244</id>
        <link href="http://arxiv.org/abs/2103.04244"/>
        <updated>2021-06-09T02:01:51.058Z</updated>
        <summary type="html"><![CDATA[There has been a growing interest in model-agnostic methods that can make
deep learning models more transparent and explainable to a user. Some
researchers recently argued that for a machine to achieve a certain degree of
human-level explainability, this machine needs to provide human causally
understandable explanations, also known as causability. A specific class of
algorithms that have the potential to provide causability are counterfactuals.
This paper presents an in-depth systematic review of the diverse existing body
of literature on counterfactuals and causability for explainable artificial
intelligence. We performed an LDA topic modelling analysis under a PRISMA
framework to find the most relevant literature articles. This analysis resulted
in a novel taxonomy that considers the grounding theories of the surveyed
algorithms, together with their underlying properties and applications in
real-world data. This research suggests that current model-agnostic
counterfactual algorithms for explainable AI are not grounded on a causal
theoretical formalism and, consequently, cannot promote causability to a human
decision-maker. Our findings suggest that the explanations derived from major
algorithms in the literature provide spurious correlations rather than
cause/effects relationships, leading to sub-optimal, erroneous or even biased
explanations. This paper also advances the literature with new directions and
challenges on promoting causability in model-agnostic approaches for
explainable artificial intelligence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yu-Liang Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1"&gt;Catarina Moreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruza_P/0/1/0/all/0/1"&gt;Peter Bruza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1"&gt;Chun Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorge_J/0/1/0/all/0/1"&gt;Joaquim Jorge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Universal Law of Robustness via Isoperimetry. (arXiv:2105.12806v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12806</id>
        <link href="http://arxiv.org/abs/2105.12806"/>
        <updated>2021-06-09T02:01:51.041Z</updated>
        <summary type="html"><![CDATA[Classically, data interpolation with a parametrized model class is possible
as long as the number of parameters is larger than the number of equations to
be satisfied. A puzzling phenomenon in deep learning is that models are trained
with many more parameters than what this classical theory would suggest. We
propose a theoretical explanation for this phenomenon. We prove that for a
broad class of data distributions and model classes, overparametrization is
necessary if one wants to interpolate the data smoothly. Namely we show that
smooth interpolation requires $d$ times more parameters than mere
interpolation, where $d$ is the ambient data dimension. We prove this universal
law of robustness for any smoothly parametrized function class with polynomial
size weights, and any covariate distribution verifying isoperimetry. In the
case of two-layers neural networks and Gaussian covariates, this law was
conjectured in prior work by Bubeck, Li and Nagaraj. We also give an
interpretation of our result as an improved generalization bound for model
classes consisting of smooth functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Bubeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1"&gt;Mark Sellke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Greedy-Step Bellman Optimality Equation for Efficient Value Propagation. (arXiv:2102.11717v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11717</id>
        <link href="http://arxiv.org/abs/2102.11717"/>
        <updated>2021-06-09T02:01:51.036Z</updated>
        <summary type="html"><![CDATA[Efficiently propagating credit to responsible actions is a central and
challenging task in reinforcement learning. To accelerate information
propagation, this paper presents a new method that bridges a highway that
allows unimpeded information to flow across long horizons. The key to our
method is a newly proposed Bellman equation, called Greedy-Step Bellman
Optimality Equation, through which the high-credit information can fast
propagate across a long horizon. We theoretically show that the solution of the
new equation is exactly the optimal value function and the corresponding
operator converges faster than the classical operator. Besides, it leads to a
new multi-step off-policy algorithm, which is capable of safely utilizing any
off-policy data collected by the arbitrary policy. Experiments reveal that the
proposed method is reliable, easy to implement. Moreover, without employing
additional components of Rainbow except Double DQN, our method achieves
competitive performance with Rainbow on the benchmark tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingyuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiaoyang Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Policy Gradient against Strong Data Corruption. (arXiv:2102.05800v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05800</id>
        <link href="http://arxiv.org/abs/2102.05800"/>
        <updated>2021-06-09T02:01:51.030Z</updated>
        <summary type="html"><![CDATA[We study the problem of robust reinforcement learning under adversarial
corruption on both rewards and transitions. Our attack model assumes an
\textit{adaptive} adversary who can arbitrarily corrupt the reward and
transition at every step within an episode, for at most $\epsilon$-fraction of
the learning episodes. Our attack model is strictly stronger than those
considered in prior works. Our first result shows that no algorithm can find a
better than $O(\epsilon)$-optimal policy under our attack model. Next, we show
that surprisingly the natural policy gradient (NPG) method retains a natural
robustness property if the reward corruption is bounded, and can find an
$O(\sqrt{\epsilon})$-optimal policy. Consequently, we develop a Filtered Policy
Gradient (FPG) algorithm that can tolerate even unbounded reward corruption and
can find an $O(\epsilon^{1/4})$-optimal policy. We emphasize that FPG is the
first that can achieve a meaningful learning guarantee when a constant fraction
of episodes are corrupted. Complimentary to the theoretical results, we show
that a neural implementation of FPG achieves strong robust learning performance
on the MuJoCo continuous control benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuezhou Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiding Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaojin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Local Pseudorandom Generators to Hardness of Learning. (arXiv:2101.08303v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08303</id>
        <link href="http://arxiv.org/abs/2101.08303"/>
        <updated>2021-06-09T02:01:51.024Z</updated>
        <summary type="html"><![CDATA[We prove hardness-of-learning results under a well-studied assumption on the
existence of local pseudorandom generators. As we show, this assumption allows
us to surpass the current state of the art, and prove hardness of various basic
problems, with no hardness results to date.

Our results include: hardness of learning shallow ReLU neural networks under
the Gaussian distribution and other distributions; hardness of learning
intersections of $\omega(1)$ halfspaces, DNF formulas with $\omega(1)$ terms,
and ReLU networks with $\omega(1)$ hidden neurons; hardness of weakly learning
deterministic finite automata under the uniform distribution; hardness of
weakly learning depth-$3$ Boolean circuits under the uniform distribution, as
well as distribution-specific hardness results for learning DNF formulas and
intersections of halfspaces. We also establish lower bounds on the complexity
of learning intersections of a constant number of halfspaces, and ReLU networks
with a constant number of hidden neurons. Moreover, our results imply the
hardness of virtually all improper PAC-learning problems (both
distribution-free and distribution-specific) that were previously shown hard
under other assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daniely_A/0/1/0/all/0/1"&gt;Amit Daniely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1"&gt;Gal Vardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing. (arXiv:2106.04502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04502</id>
        <link href="http://arxiv.org/abs/2106.04502"/>
        <updated>2021-06-09T02:01:51.018Z</updated>
        <summary type="html"><![CDATA[Tuning hyperparameters is a crucial but arduous part of the machine learning
pipeline. Hyperparameter optimization is even more challenging in federated
learning, where models are learned over a distributed network of heterogeneous
devices; here, the need to keep data on device and perform local training makes
it difficult to efficiently train and evaluate configurations. In this work, we
investigate the problem of federated hyperparameter tuning. We first identify
key challenges and show how standard approaches may be adapted to form
baselines for the federated setting. Then, by making a novel connection to the
neural architecture search technique of weight-sharing, we introduce a new
method, FedEx, to accelerate federated hyperparameter tuning that is applicable
to widely-used federated optimization methods such as FedAvg and recent
variants. Theoretically, we show that a FedEx variant correctly tunes the
on-device learning rate in the setting of online convex optimization across
devices. Empirically, we show that FedEx can outperform natural baselines for
federated hyperparameter tuning by several percentage points on the
Shakespeare, FEMNIST, and CIFAR-10 benchmarks, obtaining higher accuracy using
the same training budget.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1"&gt;Mikhail Khodak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1"&gt;Renbo Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liam Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1"&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1"&gt;Virginia Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Regularization in ReLU Networks with the Square Loss. (arXiv:2012.05156v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05156</id>
        <link href="http://arxiv.org/abs/2012.05156"/>
        <updated>2021-06-09T02:01:51.002Z</updated>
        <summary type="html"><![CDATA[Understanding the implicit regularization (or implicit bias) of gradient
descent has recently been a very active research area. However, the implicit
regularization in nonlinear neural networks is still poorly understood,
especially for regression losses such as the square loss. Perhaps surprisingly,
we prove that even for a single ReLU neuron, it is impossible to characterize
the implicit regularization with the square loss by any explicit function of
the model parameters (although on the positive side, we show it can be
characterized approximately). For one hidden-layer networks, we prove a similar
result, where in general it is impossible to characterize implicit
regularization properties in this manner, except for the "balancedness"
property identified in Du et al. [2018]. Our results suggest that a more
general framework than the one considered so far may be needed to understand
implicit regularization for nonlinear predictors, and provides some clues on
what this framework should be.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1"&gt;Gal Vardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-MLP: Node Classification without Message Passing in Graph. (arXiv:2106.04051v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04051</id>
        <link href="http://arxiv.org/abs/2106.04051"/>
        <updated>2021-06-09T02:01:50.997Z</updated>
        <summary type="html"><![CDATA[Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing
with non-Euclidean structural data. Both spatial-based and spectral-based GNNs
are relying on adjacency matrix to guide message passing among neighbors during
feature aggregation. Recent works have mainly focused on powerful message
passing modules, however, in this paper, we show that none of the message
passing modules is necessary. Instead, we propose a pure
multilayer-perceptron-based framework, Graph-MLP with the supervision signal
leveraging graph structure, which is sufficient for learning discriminative
node representation. In model-level, Graph-MLP only includes multi-layer
perceptrons, activation function, and layer normalization. In the loss level,
we design a neighboring contrastive (NContrast) loss to bridge the gap between
GNNs and MLPs by utilizing the adjacency information implicitly. This design
allows our model to be lighter and more robust when facing large-scale graph
data and corrupted adjacency information. Extensive experiments prove that even
without adjacency information in testing phase, our framework can still reach
comparable and even superior performance against the state-of-the-art models in
the graph node classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1"&gt;Haoxuan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhecan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1"&gt;Erjin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust R-Peak Detection in Low-Quality Holter ECGs using 1D Convolutional Neural Network. (arXiv:2101.01666v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01666</id>
        <link href="http://arxiv.org/abs/2101.01666"/>
        <updated>2021-06-09T02:01:50.991Z</updated>
        <summary type="html"><![CDATA[Noise and low quality of ECG signals acquired from Holter or wearable devices
deteriorate the accuracy and robustness of R-peak detection algorithms. This
paper presents a generic and robust system for R-peak detection in Holter ECG
signals. While many proposed algorithms have successfully addressed the problem
of ECG R-peak detection, there is still a notable gap in the performance of
these detectors on such low-quality ECG records. Therefore, in this study, a
novel implementation of the 1D Convolutional Neural Network (CNN) is used
integrated with a verification model to reduce the number of false alarms. This
CNN architecture consists of an encoder block and a corresponding decoder block
followed by a sample-wise classification layer to construct the 1D segmentation
map of R- peaks from the input ECG signal. Once the proposed model has been
trained, it can solely be used to detect R-peaks possibly in a single channel
ECG data stream quickly and accurately, or alternatively, such a solution can
be conveniently employed for real-time monitoring on a lightweight portable
device. The model is tested on two open-access ECG databases: The China
Physiological Signal Challenge (2020) database (CPSC-DB) with more than one
million beats, and the commonly used MIT-BIH Arrhythmia Database (MIT-DB).
Experimental results demonstrate that the proposed systematic approach achieves
99.30% F1-score, 99.69% recall, and 98.91% precision in CPSC-DB, which is the
best R-peak detection performance ever achieved. Compared to all competing
methods, the proposed approach can reduce the false-positives and
false-negatives in Holter ECG signals by more than 54% and 82%, respectively.
Results also demonstrate similar or better performance than most competing
algorithms on MIT-DB with 99.83% F1-score, 99.85% recall, and 99.82% precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zahid_M/0/1/0/all/0/1"&gt;Muhammad Uzair Zahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ince_T/0/1/0/all/0/1"&gt;Turker Ince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Devecioglu_O/0/1/0/all/0/1"&gt;Ozer Can Devecioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1"&gt;Amith Khandakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tahir_A/0/1/0/all/0/1"&gt;Anas Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1"&gt;Moncef Gabbouj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastAdaBelief: Improving Convergence Rate for Belief-based Adaptive Optimizers by Exploiting Strong Convexity. (arXiv:2104.13790v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13790</id>
        <link href="http://arxiv.org/abs/2104.13790"/>
        <updated>2021-06-09T02:01:50.986Z</updated>
        <summary type="html"><![CDATA[AdaBelief, one of the current best optimizers, demonstrates superior
generalization ability compared to the popular Adam algorithm by viewing the
exponential moving average of observed gradients. AdaBelief is theoretically
appealing in that it has a data-dependent $O(\sqrt{T})$ regret bound when
objective functions are convex, where $T$ is a time horizon. It remains however
an open problem whether the convergence rate can be further improved without
sacrificing its generalization ability. %on how to exploit strong convexity to
further improve the convergence rate of AdaBelief. To this end, we make a first
attempt in this work and design a novel optimization algorithm called
FastAdaBelief that aims to exploit its strong convexity in order to achieve an
even faster convergence rate. In particular, by adjusting the step size that
better considers strong convexity and prevents fluctuation, our proposed
FastAdaBelief demonstrates excellent generalization ability as well as superior
convergence. As an important theoretical contribution, we prove that
FastAdaBelief attains a data-dependant $O(\log T)$ regret bound, which is
substantially lower than AdaBelief. On the empirical side, we validate our
theoretical analysis with extensive experiments in both scenarios of strong and
non-strong convexity on three popular baseline models. Experimental results are
very encouraging: FastAdaBelief converges the quickest in comparison to all
mainstream algorithms while maintaining an excellent generalization ability, in
cases of both strong or non-strong convexity. FastAdaBelief is thus posited as
a new benchmark model for the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yangfan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Cheng Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1"&gt;Amir Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical VAEs Know What They Don't Know. (arXiv:2102.08248v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08248</id>
        <link href="http://arxiv.org/abs/2102.08248"/>
        <updated>2021-06-09T02:01:50.980Z</updated>
        <summary type="html"><![CDATA[Deep generative models have been demonstrated as state-of-the-art density
estimators. Yet, recent work has found that they often assign a higher
likelihood to data from outside the training distribution. This seemingly
paradoxical behavior has caused concerns over the quality of the attained
density estimates. In the context of hierarchical variational autoencoders, we
provide evidence to explain this behavior by out-of-distribution data having
in-distribution low-level features. We argue that this is both expected and
desirable behavior. With this insight in hand, we develop a fast, scalable and
fully unsupervised likelihood-ratio score for OOD detection that requires data
to be in-distribution across all feature-levels. We benchmark the method on a
vast set of data and model combinations and achieve state-of-the-art results on
out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1"&gt;Jakob D. Havtorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frellsen_J/0/1/0/all/0/1"&gt;Jes Frellsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maaloe_L/0/1/0/all/0/1"&gt;Lars Maal&amp;#xf8;e&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Vision Transformers. (arXiv:2106.04560v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04560</id>
        <link href="http://arxiv.org/abs/2106.04560"/>
        <updated>2021-06-09T02:01:50.974Z</updated>
        <summary type="html"><![CDATA[Attention-based neural networks such as the Vision Transformer (ViT) have
recently attained state-of-the-art results on many computer vision benchmarks.
Scale is a primary ingredient in attaining excellent results, therefore,
understanding a model's scaling properties is a key to designing future
generations effectively. While the laws for scaling Transformer language models
have been studied, it is unknown how Vision Transformers scale. To address
this, we scale ViT models and data, both up and down, and characterize the
relationships between error rate, data, and compute. Along the way, we refine
the architecture and training of ViT, reducing memory consumption and
increasing accuracy the resulting models. As a result, we successfully train a
ViT model with two billion parameters, which attains a new state-of-the-art on
ImageNet of 90.45% top-1 accuracy. The model also performs well on few-shot
learning, for example, attaining 84.86% top-1 accuracy on ImageNet with only 10
examples per class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Balancing Geometry and Density: Path Distances on High-Dimensional Data. (arXiv:2012.09385v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09385</id>
        <link href="http://arxiv.org/abs/2012.09385"/>
        <updated>2021-06-09T02:01:50.958Z</updated>
        <summary type="html"><![CDATA[New geometric and computational analyses of power-weighted shortest-path
distances (PWSPDs) are presented. By illuminating the way these metrics balance
density and geometry in the underlying data, we clarify their key parameters
and discuss how they may be chosen in practice. Comparisons are made with
related data-driven metrics, which illustrate the broader role of density in
kernel-based unsupervised and semi-supervised machine learning.
Computationally, we relate PWSPDs on complete weighted graphs to their
analogues on weighted nearest neighbor graphs, providing high probability
guarantees on their equivalence that are near-optimal. Connections with
percolation theory are developed to establish estimates on the bias and
variance of PWSPDs in the finite sample setting. The theoretical results are
bolstered by illustrative experiments, demonstrating the versatility of PWSPDs
for a wide range of data settings. Throughout the paper, our results require
only that the underlying data is sampled from a low-dimensional manifold, and
depend crucially on the intrinsic dimension of this manifold, rather than its
ambient dimension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Little_A/0/1/0/all/0/1"&gt;Anna Little&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+McKenzie_D/0/1/0/all/0/1"&gt;Daniel McKenzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Murphy_J/0/1/0/all/0/1"&gt;James Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoequivariant Network Search via Group Decomposition. (arXiv:2104.04848v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04848</id>
        <link href="http://arxiv.org/abs/2104.04848"/>
        <updated>2021-06-09T02:01:50.952Z</updated>
        <summary type="html"><![CDATA[Recent works show that group equivariance as an inductive bias improves
neural network performance for both classification and generation. However,
designing group-equivariant neural networks is challenging when the group of
interest is large and is unknown. Moreover, inducing equivariance can
significantly reduce the number of independent parameters in a network with
fixed feature size, affecting its overall performance. We address these
problems by proving a new group-theoretic result in the context of equivariant
neural networks that shows that a network is equivariant to a large group if
and only if it is equivariant to smaller groups from which it is constructed.
Using this result, we design a novel fast group equivariant construction
algorithm, and a deep Q-learning-based search algorithm in a reduced search
space, yielding what we call autoequivariant networks (AENs). AENs find the
right balance between equivariance and network size when tested on new
benchmark datasets, G-MNIST and G-Fashion-MNIST, obtained via group
transformations on MNIST and Fashion-MNIST respectively that we release.
Extending these results to group convolutional neural networks, where we
optimize between equivariances, augmentations, and network sizes, we find group
equivariance to be the most dominating factor in all high-performing GCNNs on
several datasets like CIFAR10, SVHN, RotMNIST, ASL, EMNIST, and KMNIST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1"&gt;Sourya Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magesh_A/0/1/0/all/0/1"&gt;Akshayaa Magesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_H/0/1/0/all/0/1"&gt;Harshit Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1"&gt;Lav R. Varshney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Properties of Vision Transformers. (arXiv:2105.10497v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10497</id>
        <link href="http://arxiv.org/abs/2105.10497"/>
        <updated>2021-06-09T02:01:50.946Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViT) have demonstrated impressive performance across
various machine vision problems. These models are based on multi-head
self-attention mechanisms that can flexibly attend to a sequence of image
patches to encode contextual cues. An important question is how such
flexibility in attending image-wide context conditioned on a given patch can
facilitate handling nuisances in natural images e.g., severe occlusions, domain
shifts, spatial permutations, adversarial and natural perturbations. We
systematically study this question via an extensive set of experiments
encompassing three ViT families and comparisons with a high-performing
convolutional neural network (CNN). We show and analyze the following
intriguing properties of ViT: (a) Transformers are highly robust to severe
occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1
accuracy on ImageNet even after randomly occluding 80% of the image content.
(b) The robust performance to occlusions is not due to a bias towards local
textures, and ViTs are significantly less biased towards textures compared to
CNNs. When properly trained to encode shape-based features, ViTs demonstrate
shape recognition capability comparable to that of human visual system,
previously unmatched in the literature. (c) Using ViTs to encode shape
representation leads to an interesting consequence of accurate semantic
segmentation without pixel-level supervision. (d) Off-the-shelf features from a
single ViT model can be combined to create a feature ensemble, leading to high
accuracy rates across a range of classification datasets in both traditional
and few-shot learning paradigms. We show effective features of ViTs are due to
flexible and dynamic receptive fields possible via the self-attention
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1"&gt;Munawar Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-paced ensemble learning for speech and audio classification. (arXiv:2103.11988v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11988</id>
        <link href="http://arxiv.org/abs/2103.11988"/>
        <updated>2021-06-09T02:01:50.931Z</updated>
        <summary type="html"><![CDATA[Combining multiple machine learning models into an ensemble is known to
provide superior performance levels compared to the individual components
forming the ensemble. This is because models can complement each other in
taking better decisions. Instead of just combining the models, we propose a
self-paced ensemble learning scheme in which models learn from each other over
several iterations. During the self-paced learning process based on
pseudo-labeling, in addition to improving the individual models, our ensemble
also gains knowledge about the target domain. To demonstrate the generality of
our self-paced ensemble learning (SPEL) scheme, we conduct experiments on three
audio tasks. Our empirical results indicate that SPEL significantly outperforms
the baseline ensemble models. We also show that applying self-paced learning on
individual models is less effective, illustrating the idea that models in the
ensemble actually learn from each other.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1"&gt;Nicolae-Catalin Ristea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch-wise++ Perturbation for Adversarial Targeted Attacks. (arXiv:2012.15503v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15503</id>
        <link href="http://arxiv.org/abs/2012.15503"/>
        <updated>2021-06-09T02:01:50.926Z</updated>
        <summary type="html"><![CDATA[Although great progress has been made on adversarial attacks for deep neural
networks (DNNs), their transferability is still unsatisfactory, especially for
targeted attacks. There are two problems behind that have been long overlooked:
1) the conventional setting of $T$ iterations with the step size of
$\epsilon/T$ to comply with the $\epsilon$-constraint. In this case, most of
the pixels are allowed to add very small noise, much less than $\epsilon$; and
2) usually manipulating pixel-wise noise. However, features of a pixel
extracted by DNNs are influenced by its surrounding regions, and different DNNs
generally focus on different discriminative regions in recognition. To tackle
these issues, our previous work proposes a patch-wise iterative method (PIM)
aimed at crafting adversarial examples with high transferability. Specifically,
we introduce an amplification factor to the step size in each iteration, and
one pixel's overall gradient overflowing the $\epsilon$-constraint is properly
assigned to its surrounding regions by a project kernel. But targeted attacks
aim to push the adversarial examples into the territory of a specific class,
and the amplification factor may lead to underfitting. Thus, we introduce the
temperature and propose a patch-wise++ iterative method (PIM++) to further
improve transferability without significantly sacrificing the performance of
the white-box attack. Our method can be generally integrated to any
gradient-based attack methods. Compared with the current state-of-the-art
attack methods, we significantly improve the success rate by 33.1\% for defense
models and 31.4\% for normally trained models on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianli Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qilong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jingkuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Heng Tao Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proactive and AoI-aware Failure Recovery for Stateful NFV-enabled Zero-Touch 6G Networks: Model-Free DRL Approach. (arXiv:2103.03817v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03817</id>
        <link href="http://arxiv.org/abs/2103.03817"/>
        <updated>2021-06-09T02:01:50.919Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Zero-Touch, deep reinforcement learning
(DRL)-based Proactive Failure Recovery framework called ZT-PFR for stateful
network function virtualization (NFV)-enabled networks. To this end, we
formulate a resource-efficient optimization problem minimizing the network cost
function including resource cost and wrong decision penalty. As a solution, we
propose state-of-the-art DRL-based methods such as soft-actor-critic (SAC) and
proximal-policy-optimization (PPO). In addition, to train and test our DRL
agents, we propose a novel impending failure model. Moreover, to keep network
status information at an acceptable freshness level for appropriate
decision-making, we apply the concept of age of information to strike a balance
between the event and scheduling-based monitoring. Several key systems and DRL
algorithm design insights for ZT-PFR are drawn from our analysis and simulation
results. For example, we use a hybrid neural network, consisting long
short-term memory layers in the DRL agents]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shaghaghi_A/0/1/0/all/0/1"&gt;Amirhossein Shaghaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zakeri_A/0/1/0/all/0/1"&gt;Abolfazl Zakeri&lt;/a&gt; (Student Member, IEEE), &lt;a href="http://arxiv.org/find/eess/1/au:+Mokari_N/0/1/0/all/0/1"&gt;Nader Mokari&lt;/a&gt; (Senior Member, IEEE), &lt;a href="http://arxiv.org/find/eess/1/au:+Javan_M/0/1/0/all/0/1"&gt;Mohammad Reza Javan&lt;/a&gt; (Senior Member, IEEE), &lt;a href="http://arxiv.org/find/eess/1/au:+Behdadfar_M/0/1/0/all/0/1"&gt;Mohammad Behdadfar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jorswieck_E/0/1/0/all/0/1"&gt;Eduard A Jorswieck&lt;/a&gt; (Fellow, IEEE)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Many-Speakers Single Channel Speech Separation with Optimal Permutation Training. (arXiv:2104.08955v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08955</id>
        <link href="http://arxiv.org/abs/2104.08955"/>
        <updated>2021-06-09T02:01:50.907Z</updated>
        <summary type="html"><![CDATA[Single channel speech separation has experienced great progress in the last
few years. However, training neural speech separation for a large number of
speakers (e.g., more than 10 speakers) is out of reach for the current methods,
which rely on the Permutation Invariant Loss (PIT). In this work, we present a
permutation invariant training that employs the Hungarian algorithm in order to
train with an $O(C^3)$ time complexity, where $C$ is the number of speakers, in
comparison to $O(C!)$ of PIT based methods. Furthermore, we present a modified
architecture that can handle the increased number of speakers. Our approach
separates up to $20$ speakers and improves the previous results for large $C$
by a wide margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dovrat_S/0/1/0/all/0/1"&gt;Shaked Dovrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1"&gt;Eliya Nachmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Too-Good-to-be-True Prior to Reduce Shortcut Reliance. (arXiv:2102.06406v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06406</id>
        <link href="http://arxiv.org/abs/2102.06406"/>
        <updated>2021-06-09T02:01:50.901Z</updated>
        <summary type="html"><![CDATA[Despite their impressive performance in object recognition and other tasks
under standard testing conditions, deep networks often fail to generalize to
out-of-distribution (o.o.d.) samples. One cause for this shortcoming is that
modern architectures tend to rely on "shortcuts" - superficial features that
correlate with categories without capturing deeper invariants that hold across
contexts. Real-world concepts often possess a complex structure that can vary
superficially across contexts, which can make the most intuitive and promising
solutions in one context not generalize to others. One potential way to improve
o.o.d. generalization is to assume simple solutions are unlikely to be valid
across contexts and avoid them, which we refer to as the too-good-to-be-true
prior. A low-capacity network (LCN) with a shallow architecture should only be
able to learn surface relationships, including shortcuts. We find that LCNs can
serve as shortcut detectors. Furthermore, an LCN's predictions can be used in a
two-stage approach to encourage a high-capacity network (HCN) to rely on deeper
invariant features that should generalize broadly. In particular, items that
the LCN can master are downweighted when training the HCN. Using a modified
version of the CIFAR-10 dataset in which we introduced shortcuts, we found that
the two-stage LCN-HCN approach reduced reliance on shortcuts and facilitated
o.o.d. generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dagaev_N/0/1/0/all/0/1"&gt;Nikolay Dagaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1"&gt;Brett D. Roads&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiaoliang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barry_D/0/1/0/all/0/1"&gt;Daniel N. Barry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_K/0/1/0/all/0/1"&gt;Kaustubh R. Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1"&gt;Bradley C. Love&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PILOT: Introducing Transformers for Probabilistic Sound Event Localization. (arXiv:2106.03903v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.03903</id>
        <link href="http://arxiv.org/abs/2106.03903"/>
        <updated>2021-06-09T02:01:50.895Z</updated>
        <summary type="html"><![CDATA[Sound event localization aims at estimating the positions of sound sources in
the environment with respect to an acoustic receiver (e.g. a microphone array).
Recent advances in this domain most prominently focused on utilizing deep
recurrent neural networks. Inspired by the success of transformer architectures
as a suitable alternative to classical recurrent neural networks, this paper
introduces a novel transformer-based sound event localization framework, where
temporal dependencies in the received multi-channel audio signals are captured
via self-attention mechanisms. Additionally, the estimated sound event
positions are represented as multivariate Gaussian variables, yielding an
additional notion of uncertainty, which many previously proposed deep
learning-based systems designed for this application do not provide. The
framework is evaluated on three publicly available multi-source sound event
localization datasets and compared against state-of-the-art methods in terms of
localization error and event detection accuracy. It outperforms all competing
systems on all datasets with statistical significant differences in
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schymura_C/0/1/0/all/0/1"&gt;Christopher Schymura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonninghoff_B/0/1/0/all/0/1"&gt;Benedikt B&amp;#xf6;nninghoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochiai_T/0/1/0/all/0/1"&gt;Tsubasa Ochiai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delcroix_M/0/1/0/all/0/1"&gt;Marc Delcroix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kinoshita_K/0/1/0/all/0/1"&gt;Keisuke Kinoshita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakatani_T/0/1/0/all/0/1"&gt;Tomohiro Nakatani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araki_S/0/1/0/all/0/1"&gt;Shoko Araki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1"&gt;Dorothea Kolossa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GSVNet: Guided Spatially-Varying Convolution for Fast Semantic Segmentation on Video. (arXiv:2103.08834v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08834</id>
        <link href="http://arxiv.org/abs/2103.08834"/>
        <updated>2021-06-09T02:01:50.889Z</updated>
        <summary type="html"><![CDATA[This paper addresses fast semantic segmentation on video.Video segmentation
often calls for real-time, or even fasterthan real-time, processing. One common
recipe for conserving computation arising from feature extraction is to
propagate features of few selected keyframes. However, recent advances in fast
image segmentation make these solutions less attractive. To leverage fast image
segmentation for furthering video segmentation, we propose a simple yet
efficient propagation framework. Specifically, we perform lightweight flow
estimation in 1/8-downscaled image space for temporal warping in segmentation
outpace space. Moreover, we introduce a guided spatially-varying convolution
for fusing segmentations derived from the previous and current frames, to
mitigate propagation error and enable lightweight feature extraction on
non-keyframes. Experimental results on Cityscapes and CamVid show that our
scheme achieves the state-of-the-art accuracy-throughput trade-off on video
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Shih-Po Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Si-Cun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wen-Hsiao Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Reinforcement Learning with a Nonparametric Off-Policy Policy Gradient. (arXiv:2010.14771v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14771</id>
        <link href="http://arxiv.org/abs/2010.14771"/>
        <updated>2021-06-09T02:01:50.884Z</updated>
        <summary type="html"><![CDATA[Off-policy Reinforcement Learning (RL) holds the promise of better data
efficiency as it allows sample reuse and potentially enables safe interaction
with the environment. Current off-policy policy gradient methods either suffer
from high bias or high variance, delivering often unreliable estimates. The
price of inefficiency becomes evident in real-world scenarios such as
interaction-driven robot learning, where the success of RL has been rather
limited, and a very high sample cost hinders straightforward application. In
this paper, we propose a nonparametric Bellman equation, which can be solved in
closed form. The solution is differentiable w.r.t the policy parameters and
gives access to an estimation of the policy gradient. In this way, we avoid the
high variance of importance sampling approaches, and the high bias of
semi-gradient methods. We empirically analyze the quality of our gradient
estimate against state-of-the-art methods, and show that it outperforms the
baselines in terms of sample efficiency on classical control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tosatto_S/0/1/0/all/0/1"&gt;Samuele Tosatto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Whitening Batch Normalization. (arXiv:2106.04413v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04413</id>
        <link href="http://arxiv.org/abs/2106.04413"/>
        <updated>2021-06-09T02:01:50.859Z</updated>
        <summary type="html"><![CDATA[Batch Normalization (BN) is a popular technique for training Deep Neural
Networks (DNNs). BN uses scaling and shifting to normalize activations of
mini-batches to accelerate convergence and improve generalization. The recently
proposed Iterative Normalization (IterNorm) method improves these properties by
whitening the activations iteratively using Newton's method. However, since
Newton's method initializes the whitening matrix independently at each training
step, no information is shared between consecutive steps. In this work, instead
of exact computation of whitening matrix at each time step, we estimate it
gradually during training in an online fashion, using our proposed Stochastic
Whitening Batch Normalization (SWBN) algorithm. We show that while SWBN
improves the convergence rate and generalization of DNNs, its computational
overhead is less than that of IterNorm. Due to the high efficiency of the
proposed method, it can be easily employed in most DNN architectures with a
large number of layers. We provide comprehensive experiments and comparisons
between BN, IterNorm, and SWBN layers to demonstrate the effectiveness of the
proposed technique in conventional (many-shot) image classification and
few-shot classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nezhadarya_E/0/1/0/all/0/1"&gt;Ehsan Nezhadarya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fashandi_H/0/1/0/all/0/1"&gt;Homa Fashandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiayi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graham_D/0/1/0/all/0/1"&gt;Darin Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mohak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of marine floating plastic using Sentinel-2 imagery and machine learning models. (arXiv:2106.03694v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03694</id>
        <link href="http://arxiv.org/abs/2106.03694"/>
        <updated>2021-06-09T02:01:50.835Z</updated>
        <summary type="html"><![CDATA[The increasing level of marine plastic pollution poses severe threats to the
marine ecosystem and biodiversity. The present study attempted to explore the
full functionality of open Sentinel satellite data and ML models for detecting
and classifying floating plastic debris in Mytilene (Greece), Limassol
(Cyprus), Calabria (Italy), and Beirut (Lebanon). Two ML models, i.e. Support
Vector Machine (SVM) and Random Forest (RF) were utilized to carry out the
classification analysis. In-situ plastic location data was collected from the
control experiment conducted in Mytilene, Greece and Limassol, Cyprus, and the
same was considered for training the models. Both remote sensing bands and
spectral indices were used for developing the ML models. A spectral signature
profile for plastic was created for discriminating the floating plastic from
other marine debris. A newly developed index, kernel Normalized Difference
Vegetation Index (kNDVI), was incorporated into the modelling to examine its
contribution to model performances. Both SVM and RF were performed well in five
models and test case combinations. Among the two ML models, the highest
performance was measured for the RF. The inclusion of kNDVI was found effective
and increased the model performances, reflected by high balanced accuracy
measured for model 2 (~80% to ~98 % for SVM and ~87% to ~97 % for RF). Using
the best-performed model, an automated floating plastic detection system was
developed and tested in Calabria and Beirut. For both sites, the trained model
had detected the floating plastic with ~99% accuracy. Among the six predictors,
the FDI was found the most important variable for detecting marine floating
plastic. These findings collectively suggest that high-resolution remote
sensing imagery and the automated ML models can be an effective alternative for
the cost-effective detection of marine floating plastic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sannigrahi_S/0/1/0/all/0/1"&gt;Srikanta Sannigrahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_B/0/1/0/all/0/1"&gt;Bidroha Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1"&gt;Arunima Sarkar Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilla_F/0/1/0/all/0/1"&gt;Francesco Pilla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample-Efficient Learning of Stackelberg Equilibria in General-Sum Games. (arXiv:2102.11494v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11494</id>
        <link href="http://arxiv.org/abs/2102.11494"/>
        <updated>2021-06-09T02:01:50.821Z</updated>
        <summary type="html"><![CDATA[Real world applications such as economics and policy making often involve
solving multi-agent games with two unique features: (1) The agents are
inherently asymmetric and partitioned into leaders and followers; (2) The
agents have different reward functions, thus the game is general-sum. The
majority of existing results in this field focuses on either symmetric solution
concepts (e.g. Nash equilibrium) or zero-sum games. It remains vastly open how
to learn the Stackelberg equilibrium -- an asymmetric analog of the Nash
equilibrium -- in general-sum games efficiently from samples.

This paper initiates the theoretical study of sample-efficient learning of
the Stackelberg equilibrium, in the bandit feedback setting where we only
observe noisy samples of the reward. We consider three representative
two-player general-sum games: bandit games, bandit-reinforcement learning
(bandit-RL) games, and linear bandit games. In all these games, we identify a
fundamental gap between the exact value of the Stackelberg equilibrium and its
estimated version using finitely many noisy samples, which can not be closed
information-theoretically regardless of the algorithm. We then establish sharp
positive results on sample-efficient learning of Stackelberg equilibrium with
value optimal up to the gap identified above, with matching lower bounds in the
dependency on the gap, error tolerance, and the size of the action spaces.
Overall, our results unveil unique challenges in learning Stackelberg
equilibria under noisy bandit feedback, which we hope could shed light on
future research on this topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yu Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Optimization to Train Neural Networks on Critical and Under-Represented Classes. (arXiv:2102.12894v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12894</id>
        <link href="http://arxiv.org/abs/2102.12894"/>
        <updated>2021-06-09T02:01:50.804Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are notorious for making more mistakes for the
classes that have substantially fewer samples than the others during training.
Such class imbalance is ubiquitous in clinical applications and very crucial to
handle because the classes with fewer samples most often correspond to critical
cases (e.g., cancer) where misclassifications can have severe consequences. Not
to miss such cases, binary classifiers need to be operated at high True
Positive Rates (TPR) by setting a higher threshold but this comes at the cost
of very high False Positive Rates (FPR) for problems with class imbalance.
Existing methods for learning under class imbalance most often do not take this
into account. We argue that prediction accuracy should be improved by
emphasizing reducing FPRs at high TPRs for problems where misclassification of
the positive, i.e., critical, class samples are associated with higher cost. To
this end, we pose the training of a DNN for binary classification as a
constrained optimization problem and introduce a novel constraint that can be
used with existing loss functions to enforce maximal area under the ROC curve
(AUC) through prioritizing FPR reduction at high TPR. We solve the resulting
constrained optimization problem using an Augmented Lagrangian method (ALM).
Going beyond binary, we also propose two possible extensions of the proposed
constraint for multi-class classification problems. We present experimental
results for image-based binary and multi-class classification applications
using an in-house medical imaging dataset, CIFAR10, and CIFAR100. Our results
demonstrate that the proposed method improves the baselines in majority of the
cases by attaining higher accuracy on critical classes while reducing the
misclassification rate for the non-critical class samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sangalli_S/0/1/0/all/0/1"&gt;Sara Sangalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdil_E/0/1/0/all/0/1"&gt;Ertunc Erdil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoetker_A/0/1/0/all/0/1"&gt;Andreas Hoetker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donati_O/0/1/0/all/0/1"&gt;Olivio Donati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1"&gt;Ender Konukoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration. (arXiv:2105.06411v1 [cs.RO] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.06411</id>
        <link href="http://arxiv.org/abs/2105.06411"/>
        <updated>2021-06-09T02:01:50.790Z</updated>
        <summary type="html"><![CDATA[We introduce a simple new method for visual imitation learning, which allows
a novel robot manipulation task to be learned from a single human
demonstration, without requiring any prior knowledge of the object being
interacted with. Our method models imitation learning as a state estimation
problem, with the state defined as the end-effector's pose at the point where
object interaction begins, as observed from the demonstration. By modelling a
manipulation task as a coarse, approach trajectory followed by a fine,
interaction trajectory, this state estimator can be trained in a
self-supervised manner, by automatically moving the end-effector's camera
around the object. At test time, the end-effector is moved to the estimated
state through a linear path, at which point the demonstration's end-effector
velocities are simply repeated, enabling convenient acquisition of a complex
interaction trajectory without actually needing to explicitly learn a policy.
Real-world experiments on 8 everyday tasks show that our method can learn a
diverse range of skills from just a single human demonstration, whilst also
yielding a stable and interpretable controller.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1"&gt;Edward Johns&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential- and Parallel- Constrained Max-value Entropy Search via Information Lower Bound. (arXiv:2102.09788v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09788</id>
        <link href="http://arxiv.org/abs/2102.09788"/>
        <updated>2021-06-09T02:01:50.783Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization (BO) is known as a powerful tool for optimizing an
unknown, expensive function through querying the function values sequentially.
On the other hand, in many practical problems, additional unknown constraints
also need to be considered. In this paper, we propose an information-theoretic
approach called Constrained Max-value Entropy Search via Information lower
BOund (CMES-IBO) for the constrained BO (CBO). Although information-theoretic
methods have been studied in CBO literature, they have not revealed any
relation between their acquisition functions and the original mutual
information. In contrast, our acquisition function is an unbiased consistent
estimator of a lower bound of mutual information. We show that our CMES-IBO has
several advantageous properties such as non-negativity, estimation error bounds
of the acquisition function, and well-definedness of the criterion, none of
which have been shown for the existing information-theoretic CBO. Furthermore,
by using conditional mutual information, we extend CMES-IBO to the parallel
setting in which multiple queries can be issued simultaneously. We demonstrate
the effectiveness of CMES-IBO by several benchmark functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takeno_S/0/1/0/all/0/1"&gt;Shion Takeno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamura_T/0/1/0/all/0/1"&gt;Tomoyuki Tamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shitara_K/0/1/0/all/0/1"&gt;Kazuki Shitara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karasuyama_M/0/1/0/all/0/1"&gt;Masayuki Karasuyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean-Shifted Contrastive Loss for Anomaly Detection. (arXiv:2106.03844v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.03844</id>
        <link href="http://arxiv.org/abs/2106.03844"/>
        <updated>2021-06-09T02:01:50.764Z</updated>
        <summary type="html"><![CDATA[Deep anomaly detection methods learn representations that separate between
normal and anomalous samples. Very effective representations are obtained when
powerful externally trained feature extractors (e.g. ResNets pre-trained on
ImageNet) are fine-tuned on the training data which consists of normal samples
and no anomalies. However, this is a difficult task that can suffer from
catastrophic collapse, i.e. it is prone to learning trivial and non-specific
features. In this paper, we propose a new loss function which can overcome
failure modes of both center-loss and contrastive-loss methods. Furthermore, we
combine it with a confidence-invariant angular center loss, which replaces the
Euclidean distance used in previous work, that was sensitive to prediction
confidence. Our improvements yield a new anomaly detection approach, based on
$\textit{Mean-Shifted Contrastive Loss}$, which is both more accurate and less
sensitive to catastrophic collapse than previous methods. Our method achieves
state-of-the-art anomaly detection performance on multiple benchmarks including
$97.5\%$ ROC-AUC on the CIFAR-10 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiss_T/0/1/0/all/0/1"&gt;Tal Reiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1"&gt;Yedid Hoshen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data. (arXiv:2005.00792v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00792</id>
        <link href="http://arxiv.org/abs/2005.00792"/>
        <updated>2021-06-09T02:01:50.758Z</updated>
        <summary type="html"><![CDATA[Event forecasting is a challenging, yet important task, as humans seek to
constantly plan for the future. Existing automated forecasting studies rely
mostly on structured data, such as time-series or event-based knowledge graphs,
to help predict future events. In this work, we aim to formulate a task,
construct a dataset, and provide benchmarks for developing methods for event
forecasting with large volumes of unstructured text data. To simulate the
forecasting scenario on temporal news documents, we formulate the problem as a
restricted-domain, multiple-choice, question-answering (QA) task. Unlike
existing QA tasks, our task limits accessible information, and thus a model has
to make a forecasting judgement. To showcase the usefulness of this task
formulation, we introduce ForecastQA, a question-answering dataset consisting
of 10,392 event forecasting questions, which have been collected and verified
via crowdsourcing efforts. We present our experiments on ForecastQA using
BERT-based models and find that our best model achieves 60.1% accuracy on the
dataset, which still lags behind human performance by about 19%. We hope
ForecastQA will support future research efforts in bridging this gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1"&gt;Woojeong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khanna_R/0/1/0/all/0/1"&gt;Rahul Khanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Suji Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dong-Ho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1"&gt;Fred Morstatter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1"&gt;Aram Galstyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Thompson Sampling using Sparse Gaussian Process Models. (arXiv:2006.05356v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05356</id>
        <link href="http://arxiv.org/abs/2006.05356"/>
        <updated>2021-06-09T02:01:50.752Z</updated>
        <summary type="html"><![CDATA[Thompson Sampling (TS) from Gaussian Process (GP) models is a powerful tool
for the optimization of black-box functions. Although TS enjoys strong
theoretical guarantees and convincing empirical performance, it incurs a large
computational overhead that scales polynomially with the optimization budget.
Recently, scalable TS methods based on sparse GP models have been proposed to
increase the scope of TS, enabling its application to problems that are
sufficiently multi-modal, noisy or combinatorial to require more than a few
hundred evaluations to be solved. However, the approximation error introduced
by sparse GPs invalidates all existing regret bounds. In this work, we perform
a theoretical and empirical analysis of scalable TS. We provide theoretical
guarantees and show that the drastic reduction in computational complexity of
scalable TS can be enjoyed without loss in the regret performance over the
standard TS. These conceptual claims are validated for practical
implementations of scalable TS on synthetic benchmarks and as part of a
real-world high-throughput molecular design task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vakili_S/0/1/0/all/0/1"&gt;Sattar Vakili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moss_H/0/1/0/all/0/1"&gt;Henry Moss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Artemev_A/0/1/0/all/0/1"&gt;Artem Artemev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dutordoir_V/0/1/0/all/0/1"&gt;Vincent Dutordoir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Picheny_V/0/1/0/all/0/1"&gt;Victor Picheny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Binary Neural Network Training on the Edge. (arXiv:2102.04270v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04270</id>
        <link href="http://arxiv.org/abs/2102.04270"/>
        <updated>2021-06-09T02:01:50.736Z</updated>
        <summary type="html"><![CDATA[The ever-growing computational demands of increasingly complex machine
learning models frequently necessitate the use of powerful cloud-based
infrastructure for their training. Binary neural networks are known to be
promising candidates for on-device inference due to their extreme compute and
memory savings over higher-precision alternatives. However, their existing
training methods require the concurrent storage of high-precision activations
for all layers, generally making learning on memory-constrained devices
infeasible. In this paper, we demonstrate that the backward propagation
operations needed for binary neural network training are strongly robust to
quantization, thereby making on-the-edge learning with modern models a
practical proposition. We introduce a low-cost binary neural network training
strategy exhibiting sizable memory footprint and energy reductions while
inducing little to no accuracy loss vs Courbariaux & Bengio's standard
approach. These resource decreases are primarily enabled through the retention
of activations exclusively in binary format. Against the latter algorithm, our
drop-in replacement sees coincident memory requirement and energy consumption
drops of 2--6$\times$, while reaching similar test accuracy in comparable time,
across a range of small-scale models trained to classify popular datasets. We
also demonstrate from-scratch ImageNet training of binarized ResNet-18,
achieving a 3.12$\times$ memory reduction. Such savings will allow for
unnecessary cloud offloading to be avoided, reducing latency, increasing energy
efficiency and safeguarding privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Erwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;James J. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moro_D/0/1/0/all/0/1"&gt;Daniele Moro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zielinski_P/0/1/0/all/0/1"&gt;Piotr Zielinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1"&gt;Claudionor Coelho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Satrajit Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_P/0/1/0/all/0/1"&gt;Peter Y. K. Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constantinides_G/0/1/0/all/0/1"&gt;George A. Constantinides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoCL: Contrastive Learning on Molecular Graphs with Multi-level Domain Knowledge. (arXiv:2106.04509v1 [physics.bio-ph])]]></title>
        <id>http://arxiv.org/abs/2106.04509</id>
        <link href="http://arxiv.org/abs/2106.04509"/>
        <updated>2021-06-09T02:01:50.730Z</updated>
        <summary type="html"><![CDATA[Recent years have seen a rapid growth of utilizing graph neural networks
(GNNs) in the biomedical domain for tackling drug-related problems. However,
like any other deep architectures, GNNs are data hungry. While requiring labels
in real world is often expensive, pretraining GNNs in an unsupervised manner
has been actively explored. Among them, graph contrastive learning, by
maximizing the mutual information between paired graph augmentations, has been
shown to be effective on various downstream tasks. However, the current graph
contrastive learning framework has two limitations. First, the augmentations
are designed for general graphs and thus may not be suitable or powerful enough
for certain domains. Second, the contrastive scheme only learns representations
that are invariant to local perturbations and thus does not consider the global
structure of the dataset, which may also be useful for downstream tasks.
Therefore, in this paper, we study graph contrastive learning in the context of
biomedical domain, where molecular graphs are present. We propose a novel
framework called MoCL, which utilizes domain knowledge at both local- and
global-level to assist representation learning. The local-level domain
knowledge guides the augmentation process such that variation is introduced
without changing graph semantics. The global-level knowledge encodes the
similarity information between graphs in the entire dataset and helps to learn
representations with richer semantics. The entire model is learned through a
double contrast objective. We evaluate MoCL on various molecular datasets under
both linear and semi-supervised settings and results show that MoCL achieves
state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mengying Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Xing_J/0/1/0/all/0/1"&gt;Jing Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huijun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiayu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Worst-Case Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:2010.12163v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12163</id>
        <link href="http://arxiv.org/abs/2010.12163"/>
        <updated>2021-06-09T02:01:50.723Z</updated>
        <summary type="html"><![CDATA[This paper studies regret minimization with randomized value functions in
reinforcement learning. In tabular finite-horizon Markov Decision Processes, we
introduce a clipping variant of one classical Thompson Sampling (TS)-like
algorithm, randomized least-squares value iteration (RLSVI). Our
$\tilde{\mathrm{O}}(H^2S\sqrt{AT})$ high-probability worst-case regret bound
improves the previous sharpest worst-case regret bounds for RLSVI and matches
the existing state-of-the-art worst-case TS-based regret bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1"&gt;Priyank Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jinglin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Softmax Policy Gradient Methods Can Take Exponential Time to Converge. (arXiv:2102.11270v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11270</id>
        <link href="http://arxiv.org/abs/2102.11270"/>
        <updated>2021-06-09T02:01:50.723Z</updated>
        <summary type="html"><![CDATA[The softmax policy gradient (PG) method, which performs gradient ascent under
softmax policy parameterization, is arguably one of the de facto
implementations of policy optimization in modern reinforcement learning. For
$\gamma$-discounted infinite-horizon tabular Markov decision processes (MDPs),
remarkable progress has recently been achieved towards establishing global
convergence of softmax PG methods in finding a near-optimal policy. However,
prior results fall short of delineating clear dependencies of convergence rates
on salient parameters such as the cardinality of the state space $\mathcal{S}$
and the effective horizon $\frac{1}{1-\gamma}$, both of which could be
excessively large. In this paper, we deliver a pessimistic message regarding
the iteration complexity of softmax PG methods, despite assuming access to
exact gradient computation. Specifically, we demonstrate that the softmax PG
method with stepsize $\eta$ can take \[

\frac{1}{\eta} |\mathcal{S}|^{2^{\Omega\big(\frac{1}{1-\gamma}\big)}}
~\text{iterations} \] to converge, even in the presence of a benign policy
initialization and an initial state distribution amenable to exploration (so
that the distribution mismatch coefficient is not exceedingly large). This is
accomplished by characterizing the algorithmic dynamics over a
carefully-constructed MDP containing only three actions. Our exponential lower
bound hints at the necessity of carefully adjusting update rules or enforcing
proper regularization in accelerating PG methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yuting Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1"&gt;Yuejie Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuantao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation and Learning with Deep Convolutional Models: a Kernel Perspective. (arXiv:2102.10032v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10032</id>
        <link href="http://arxiv.org/abs/2102.10032"/>
        <updated>2021-06-09T02:01:50.713Z</updated>
        <summary type="html"><![CDATA[The empirical success of deep convolutional networks on tasks involving
high-dimensional data such as images or audio suggests that they can
efficiently approximate certain functions that are well-suited for such tasks.
In this paper, we study this through the lens of kernel methods, by considering
simple hierarchical kernels with two or three convolution and pooling layers,
inspired by convolutional kernel networks. These achieve good empirical
performance on standard vision datasets, while providing a simple enough
description of the functional space to shed light on their inductive bias. We
show that the RKHS consists of additive models of interaction terms between
patches, and that its norm encourages structured spatial similarities between
these terms through pooling layers. We then provide generalization bounds which
illustrate how pooling yields improved sample complexity guarantees when the
target function presents such regularities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1"&gt;Alberto Bietti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Image Reconstruction using Deep Generative Models. (arXiv:2012.04567v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04567</id>
        <link href="http://arxiv.org/abs/2012.04567"/>
        <updated>2021-06-09T02:01:50.701Z</updated>
        <summary type="html"><![CDATA[Machine learning models are commonly trained end-to-end and in a supervised
setting, using paired (input, output) data. Examples include recent
super-resolution methods that train on pairs of (low-resolution,
high-resolution) images. However, these end-to-end approaches require
re-training every time there is a distribution shift in the inputs (e.g., night
images vs daylight) or relevant latent variables (e.g., camera blur or hand
motion). In this work, we leverage state-of-the-art (SOTA) generative models
(here StyleGAN2) for building powerful image priors, which enable application
of Bayes' theorem for many downstream reconstruction tasks. Our method,
Bayesian Reconstruction through Generative Models (BRGM), uses a single
pre-trained generator model to solve different image restoration tasks, i.e.,
super-resolution and in-painting, by combining it with different forward
corruption models. We keep the weights of the generator model fixed, and
reconstruct the image by estimating the Bayesian maximum a-posteriori (MAP)
estimate over the input latent vector that generated the reconstructed image.
We further use variational inference to approximate the posterior distribution
over the latent vectors, from which we sample multiple solutions. We
demonstrate BRGM on three large and diverse datasets: (i) 60,000 images from
the Flick Faces High Quality dataset (ii) 240,000 chest X-rays from MIMIC III
and (iii) a combined collection of 5 brain MRI datasets with 7,329 scans.
Across all three datasets and without any dataset-specific hyperparameter
tuning, our simple approach yields performance competitive with current
task-specific state-of-the-art methods on super-resolution and in-painting,
while being more generalisable and without requiring any training. Our source
code and pre-trained models are available online:
https://razvanmarinescu.github.io/brgm/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marinescu_R/0/1/0/all/0/1"&gt;Razvan V Marinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1"&gt;Daniel Moyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GSGP-CUDA -- a CUDA framework for Geometric Semantic Genetic Programming. (arXiv:2106.04034v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.04034</id>
        <link href="http://arxiv.org/abs/2106.04034"/>
        <updated>2021-06-09T02:01:50.700Z</updated>
        <summary type="html"><![CDATA[Geometric Semantic Genetic Programming (GSGP) is a state-of-the-art machine
learning method based on evolutionary computation. GSGP performs search
operations directly at the level of program semantics, which can be done more
efficiently then operating at the syntax level like most GP systems. Efficient
implementations of GSGP in C++ exploit this fact, but not to its full
potential. This paper presents GSGP-CUDA, the first CUDA implementation of GSGP
and the most efficient, which exploits the intrinsic parallelism of GSGP using
GPUs. Results show speedups greater than 1,000X relative to the
state-of-the-art sequential implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trujillo_L/0/1/0/all/0/1"&gt;Leonardo Trujillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Contreras_J/0/1/0/all/0/1"&gt;Jose Manuel Mu&amp;#xf1;oz Contreras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1"&gt;Daniel E Hernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castelli_M/0/1/0/all/0/1"&gt;Mauro Castelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1"&gt;Juan J Tapia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language-Mediated, Object-Centric Representation Learning. (arXiv:2012.15814v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15814</id>
        <link href="http://arxiv.org/abs/2012.15814"/>
        <updated>2021-06-09T02:01:50.700Z</updated>
        <summary type="html"><![CDATA[We present Language-mediated, Object-centric Representation Learning (LORL),
a paradigm for learning disentangled, object-centric scene representations from
vision and language. LORL builds upon recent advances in unsupervised object
discovery and segmentation, notably MONet and Slot Attention. While these
algorithms learn an object-centric representation just by reconstructing the
input image, LORL enables them to further learn to associate the learned
representations to concepts, i.e., words for object categories, properties, and
spatial relationships, from language input. These object-centric concepts
derived from language facilitate the learning of object-centric
representations. LORL can be integrated with various unsupervised object
discovery algorithms that are language-agnostic. Experiments show that the
integration of LORL consistently improves the performance of unsupervised
object discovery methods on two datasets via the help of language. We also show
that concepts learned by LORL, in conjunction with object discovery methods,
aid downstream tasks such as referring expression comprehension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruocheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiayuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1"&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Recombine and Resample Data for Compositional Generalization. (arXiv:2010.03706v6 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03706</id>
        <link href="http://arxiv.org/abs/2010.03706"/>
        <updated>2021-06-09T02:01:50.689Z</updated>
        <summary type="html"><![CDATA[Flexible neural sequence models outperform grammar- and automaton-based
counterparts on a variety of tasks. However, neural models perform poorly in
settings requiring compositional generalization beyond the training data --
particularly to rare or unseen subsequences. Past work has found symbolic
scaffolding (e.g. grammars or automata) essential in these settings. We
describe R&R, a learned data augmentation scheme that enables a large category
of compositional generalizations without appeal to latent symbolic structure.
R&R has two components: recombination of original training examples via a
prototype-based generative model and resampling of generated examples to
encourage extrapolation. Training an ordinary neural sequence model on a
dataset augmented with recombined and resampled examples significantly improves
generalization in two language processing problems -- instruction following
(SCAN) and morphological analysis (SIGMORPHON 2018) -- where R&R enables
learning of new constructions and tenses from as few as eight initial examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1"&gt;Ekin Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_A/0/1/0/all/0/1"&gt;Afra Feyza Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SWAD: Domain Generalization by Seeking Flat Minima. (arXiv:2102.08604v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08604</id>
        <link href="http://arxiv.org/abs/2102.08604"/>
        <updated>2021-06-09T02:01:50.686Z</updated>
        <summary type="html"><![CDATA[Domain generalization (DG) methods aim to achieve generalizability to an
unseen target domain by using only training data from the source domains.
Although a variety of DG methods have been proposed, a recent study shows that
under a fair evaluation protocol, called DomainBed, the simple empirical risk
minimization (ERM) approach works comparable to or even outperforms previous
methods. Unfortunately, simply solving ERM on a complex, non-convex loss
function can easily lead to sub-optimal generalizability by seeking sharp
minima. In this paper, we theoretically show that finding flat minima results
in a smaller domain generalization gap. We also propose a simple yet effective
method, named Stochastic Weight Averaging Densely (SWAD), to find flat minima.
SWAD finds flatter minima and suffers less from overfitting than does the
vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy.
SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS,
VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large
margins of +1.6% averagely on out-of-domain accuracy. We also compare SWAD with
conventional generalization methods, such as data augmentation and consistency
regularization methods, to verify that the remarkable performance improvements
are originated from by seeking flat minima, not from better in-domain
generalizability. Last but not least, SWAD is readily adaptable to existing DG
methods without modification; the combination of SWAD and an existing DG method
further improves DG performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1"&gt;Junbum Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1"&gt;Sanghyuk Chun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyungjae Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Han-Cheol Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seunghyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yunsung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungrae Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic selection of clustering algorithms using supervised graph embedding. (arXiv:2011.08225v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08225</id>
        <link href="http://arxiv.org/abs/2011.08225"/>
        <updated>2021-06-09T02:01:50.685Z</updated>
        <summary type="html"><![CDATA[The widespread adoption of machine learning (ML) techniques and the extensive
expertise required to apply them have led to increased interest in automated ML
solutions that reduce the need for human intervention. One of the main
challenges in applying ML to previously unseen problems is algorithm selection
- the identification of high-performing algorithm(s) for a given dataset, task,
and evaluation measure. This study addresses the algorithm selection challenge
for data clustering, a fundamental task in data mining that is aimed at
grouping similar objects. We present MARCO-GE, a novel meta-learning approach
for the automated recommendation of clustering algorithms. MARCO-GE first
transforms datasets into graphs and then utilizes a graph convolutional neural
network technique to extract their latent representation. Using the embedding
representations obtained, MARCO-GE trains a ranking meta-model capable of
accurately recommending top-performing algorithms for a new dataset and
clustering evaluation measure. Extensive evaluation on 210 datasets, 13
clustering algorithms, and 10 clustering measures demonstrates the
effectiveness of our approach and its superiority in terms of predictive and
generalization performance over state-of-the-art clustering meta-learning
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Shapira_N/0/1/0/all/0/1"&gt;Noy Cohen-Shapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rokach_L/0/1/0/all/0/1"&gt;Lior Rokach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Elastic Lottery Ticket Hypothesis. (arXiv:2103.16547v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16547</id>
        <link href="http://arxiv.org/abs/2103.16547"/>
        <updated>2021-06-09T02:01:50.684Z</updated>
        <summary type="html"><![CDATA[Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse
trainable subnetworks, or winning tickets, of training, which can be trained in
isolation to achieve similar or even better performance compared to the full
models. Despite many efforts being made, the most effective method to identify
such winning tickets is still Iterative Magnitude-based Pruning (IMP), which is
computationally expensive and has to be run thoroughly for every different
network. A natural question that comes in is: can we "transform" the winning
ticket found in one network to another with a different architecture, yielding
a winning ticket for the latter at the beginning, without re-doing the
expensive IMP? Answering this question is not only practically relevant for
efficient "once-for-all" winning ticket finding, but also theoretically
appealing for uncovering inherently scalable sparse patterns in networks. We
conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety
of strategies to tweak the winning tickets found from different networks of the
same model family (e.g., ResNets). Based on these results, we articulate the
Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or
dropping) and re-ordering layers for one network, its corresponding winning
ticket could be stretched (or squeezed) into a subnetwork for another deeper
(or shallower) network from the same family, whose performance is nearly the
same competitive as the latter's winning ticket directly found by IMP. We have
also thoroughly compared E-LTH with pruning-at-initialization and dynamic
sparse training methods, and discuss the generalizability of E-LTH to different
model families, layer types, or across datasets. Code is available at
https://github.com/VITA-Group/ElasticLTH.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Reordering for Modeling Latent Alignments in Sequence Transduction. (arXiv:2106.03257v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03257</id>
        <link href="http://arxiv.org/abs/2106.03257"/>
        <updated>2021-06-09T02:01:50.683Z</updated>
        <summary type="html"><![CDATA[Despite success in many domains, neural models struggle in settings where
train and test examples are drawn from different distributions. In particular,
in contrast to humans, conventional sequence-to-sequence (seq2seq) models fail
to generalize systematically, i.e., interpret sentences representing novel
combinations of concepts (e.g., text segments) seen in training. Traditional
grammar formalisms excel in such settings by implicitly encoding alignments
between input and output segments, but are hard to scale and maintain. Instead
of engineering a grammar, we directly model segment-to-segment alignments as
discrete structured latent variables within a neural seq2seq model. To
efficiently explore the large space of alignments, we introduce a reorder-first
align-later framework whose central component is a neural reordering module
producing {\it separable} permutations. We present an efficient dynamic
programming algorithm performing exact marginal inference of separable
permutations, and, thus, enabling end-to-end differentiable training of our
model. The resulting seq2seq model exhibits better systematic generalization
than standard models on synthetic problems and NLP tasks (i.e., semantic
parsing and machine translation).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bailin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1"&gt;Ivan Titov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Anomaly Detectors: Context Matters. (arXiv:2012.06260v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06260</id>
        <link href="http://arxiv.org/abs/2012.06260"/>
        <updated>2021-06-09T02:01:50.678Z</updated>
        <summary type="html"><![CDATA[Deep generative models are challenging the classical methods in the field of
anomaly detection nowadays. Every new method provides evidence of outperforming
its predecessors, often with contradictory results. The objective of this
comparison is twofold: to compare anomaly detection methods of various
paradigms with focus on deep generative models, and identification of sources
of variability that can yield different results. The methods were compared on
popular tabular and image datasets. We identified the main sources of
variability to be experimental conditions: i) the type data set (tabular or
image) and the nature of anomalies (statistical or semantic), and ii) strategy
of selection of hyperparameters, especially the number of available anomalies
in the validation set. Different methods perform the best in different
contexts, i.e. combination of experimental conditions together with
computational time. This explains the variability of the previous results and
highlights the importance of careful specification of the context in the
publication of a new method. All our code and results are available for
download.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Skvara_V/0/1/0/all/0/1"&gt;V&amp;#xed;t &amp;#x160;kv&amp;#xe1;ra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franc%5Cr%7Bu%7D_J/0/1/0/all/0/1"&gt;Jan Franc&amp;#x16f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorek_M/0/1/0/all/0/1"&gt;Mat&amp;#x11b;j Zorek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1"&gt;Tom&amp;#xe1;&amp;#x161; Pevn&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smidl_V/0/1/0/all/0/1"&gt;V&amp;#xe1;clav &amp;#x160;m&amp;#xed;dl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NWT: Towards natural audio-to-video generation with representation learning. (arXiv:2106.04283v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.04283</id>
        <link href="http://arxiv.org/abs/2106.04283"/>
        <updated>2021-06-09T02:01:50.667Z</updated>
        <summary type="html"><![CDATA[In this work we introduce NWT, an expressive speech-to-video model. Unlike
approaches that use domain-specific intermediate representations such as pose
keypoints, NWT learns its own latent representations, with minimal assumptions
about the audio and video content. To this end, we propose a novel discrete
variational autoencoder with adversarial loss, dVAE-Adv, which learns a new
discrete latent representation we call Memcodes. Memcodes are straightforward
to implement, require no additional loss terms, are stable to train compared
with other approaches, and show evidence of interpretability. To predict on the
Memcode space, we use an autoregressive encoder-decoder model conditioned on
audio. Additionally, our model can control latent attributes in the generated
video that are not annotated in the data. We train NWT on clips from HBO's Last
Week Tonight with John Oliver. NWT consistently scores above other approaches
in Mean Opinion Score (MOS) on tests of overall video naturalness, facial
naturalness and expressiveness, and lipsync quality. This work sets a strong
baseline for generalized audio-to-video synthesis. Samples are available at
https://next-week-tonight.github.io/NWT/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mama_R/0/1/0/all/0/1"&gt;Rayhane Mama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyndel_M/0/1/0/all/0/1"&gt;Marc S. Tyndel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadhim_H/0/1/0/all/0/1"&gt;Hashiam Kadhim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifford_C/0/1/0/all/0/1"&gt;Cole Clifford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thurairatnam_R/0/1/0/all/0/1"&gt;Ragavan Thurairatnam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Learning for Knowledge Distillation. (arXiv:2106.04570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04570</id>
        <link href="http://arxiv.org/abs/2106.04570"/>
        <updated>2021-06-09T02:01:50.666Z</updated>
        <summary type="html"><![CDATA[We present Meta Learning for Knowledge Distillation (MetaDistil), a simple
yet effective alternative to traditional knowledge distillation (KD) methods
where the teacher model is fixed during training. We show the teacher network
can learn to better transfer knowledge to the student network (i.e., learning
to teach) with the feedback from the performance of the distilled student
network in a meta learning framework. Moreover, we introduce a pilot update
mechanism to improve the alignment between the inner-learner and meta-learner
in meta learning algorithms that focus on an improved inner-learner.
Experiments on various benchmarks show that MetaDistil can yield significant
improvements compared with traditional KD algorithms and is less sensitive to
the choice of different student capacity and hyperparameters, facilitating the
use of KD on different tasks and models. The code is available at
https://github.com/JetRunner/MetaDistil]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Canwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Medical Image Alignment with Curriculum Learning. (arXiv:2102.10438v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10438</id>
        <link href="http://arxiv.org/abs/2102.10438"/>
        <updated>2021-06-09T02:01:50.665Z</updated>
        <summary type="html"><![CDATA[We explore different curriculum learning methods for training convolutional
neural networks on the task of deformable pairwise 3D medical image
registration. To the best of our knowledge, we are the first to attempt to
improve performance by training medical image registration models using
curriculum learning, starting from an easy training setup in the first training
stages, and gradually increasing the complexity of the setup. On the one hand,
we consider two existing curriculum learning approaches, namely curriculum
dropout and curriculum by smoothing. On the other hand, we propose a novel and
simple strategy to achieve curriculum, namely to use purposely blurred images
at the beginning, then gradually transit to sharper images in the later
training stages. Our experiments with an underlying state-of-the-art deep
learning model show that curriculum learning can lead to superior results
compared to conventional training. Additionally, we show that curriculum by
input blur has the best accuracy versus speed trade-off among the compared
curriculum learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burduja_M/0/1/0/all/0/1"&gt;Mihail Burduja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Too-Good-to-be-True Prior to Reduce Shortcut Reliance. (arXiv:2102.06406v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06406</id>
        <link href="http://arxiv.org/abs/2102.06406"/>
        <updated>2021-06-09T02:01:50.660Z</updated>
        <summary type="html"><![CDATA[Despite their impressive performance in object recognition and other tasks
under standard testing conditions, deep networks often fail to generalize to
out-of-distribution (o.o.d.) samples. One cause for this shortcoming is that
modern architectures tend to rely on "shortcuts" - superficial features that
correlate with categories without capturing deeper invariants that hold across
contexts. Real-world concepts often possess a complex structure that can vary
superficially across contexts, which can make the most intuitive and promising
solutions in one context not generalize to others. One potential way to improve
o.o.d. generalization is to assume simple solutions are unlikely to be valid
across contexts and avoid them, which we refer to as the too-good-to-be-true
prior. A low-capacity network (LCN) with a shallow architecture should only be
able to learn surface relationships, including shortcuts. We find that LCNs can
serve as shortcut detectors. Furthermore, an LCN's predictions can be used in a
two-stage approach to encourage a high-capacity network (HCN) to rely on deeper
invariant features that should generalize broadly. In particular, items that
the LCN can master are downweighted when training the HCN. Using a modified
version of the CIFAR-10 dataset in which we introduced shortcuts, we found that
the two-stage LCN-HCN approach reduced reliance on shortcuts and facilitated
o.o.d. generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dagaev_N/0/1/0/all/0/1"&gt;Nikolay Dagaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1"&gt;Brett D. Roads&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiaoliang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barry_D/0/1/0/all/0/1"&gt;Daniel N. Barry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_K/0/1/0/all/0/1"&gt;Kaustubh R. Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1"&gt;Bradley C. Love&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-aware Spatiotemporal Modules with Auxiliary Tasks for Meta-Learning. (arXiv:2006.08831v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08831</id>
        <link href="http://arxiv.org/abs/2006.08831"/>
        <updated>2021-06-09T02:01:50.650Z</updated>
        <summary type="html"><![CDATA[Modeling the dynamics of real-world physical systems is critical for
spatiotemporal prediction tasks, but challenging when data is limited. The
scarcity of real-world data and the difficulty in reproducing the data
distribution hinder directly applying meta-learning techniques. Although the
knowledge of governing partial differential equations (PDE) of data can be
helpful for the fast adaptation to few observations, it is mostly infeasible to
exactly find the equation for observations in real-world physical systems. In
this work, we propose a framework, physics-aware meta-learning with auxiliary
tasks, whose spatial modules incorporate PDE-independent knowledge and temporal
modules utilize the generalized features from the spatial modules to be adapted
to the limited data, respectively. The framework is inspired by a local
conservation law expressed mathematically as a continuity equation and does not
require the exact form of governing equation to model the spatiotemporal
observations. The proposed method mitigates the need for a large number of
real-world tasks for meta-learning by leveraging spatial information in
simulated data to meta-initialize the spatial modules. We apply the proposed
framework to both synthetic and real-world spatiotemporal prediction tasks and
demonstrate its superior performance with limited observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Sungyong Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chuizheng Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1"&gt;Sirisha Rambhatla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Representation Learning for Efficient Medical Image Analysis. (arXiv:2006.11223v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11223</id>
        <link href="http://arxiv.org/abs/2006.11223"/>
        <updated>2021-06-09T02:01:50.644Z</updated>
        <summary type="html"><![CDATA[Medical image analysis typically includes several tasks such as enhancement,
segmentation, and classification. Traditionally, these tasks are implemented
using separate deep learning models for separate tasks, which is not efficient
because it involves unnecessary training repetitions, demands greater
computational resources, and requires a relatively large amount of labeled
data. In this paper, we propose a multi-task training approach for medical
image analysis, where individual tasks are fine-tuned simultaneously through
relevant knowledge transfer using a unified modality-specific feature
representation (UMS-Rep). We explore different fine-tuning strategies to
demonstrate the impact of the strategy on the performance of target medical
image tasks. We experiment with different visual tasks (e.g., image denoising,
segmentation, and classification) to highlight the advantages offered with our
approach for two imaging modalities, chest X-ray and Doppler echocardiography.
Our results demonstrate that the proposed approach reduces the overall demand
for computational resources and improves target task generalization and
performance. Further, our results prove that the performance of target tasks in
medical images is highly influenced by the utilized fine-tuning strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajaraman_S/0/1/0/all/0/1"&gt;Sivaramakrishnan Rajaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antani_S/0/1/0/all/0/1"&gt;Sameer Antani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future. (arXiv:2106.04420v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04420</id>
        <link href="http://arxiv.org/abs/2106.04420"/>
        <updated>2021-06-09T02:01:50.639Z</updated>
        <summary type="html"><![CDATA[In real-time forecasting in public health, data collection is a non-trivial
and demanding task. Often after initially released, it undergoes several
revisions later (maybe due to human or technical constraints) - as a result, it
may take weeks until the data reaches to a stable value. This so-called
'backfill' phenomenon and its effect on model performance has been barely
studied in the prior literature. In this paper, we introduce the multi-variate
backfill problem using COVID-19 as the motivating example. We construct a
detailed dataset composed of relevant signals over the past year of the
pandemic. We then systematically characterize several patterns in backfill
dynamics and leverage our observations for formulating a novel problem and
neural framework Back2Future that aims to refines a given model's predictions
in real-time. Our extensive experiments demonstrate that our method refines the
performance of top models for COVID-19 forecasting, in contrast to non-trivial
baselines, yielding 18% improvement over baselines, enabling us obtain a new
SOTA performance. In addition, we show that our model improves model evaluation
too; hence policy-makers can better understand the true accuracy of forecasting
models in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamarthi_H/0/1/0/all/0/1"&gt;Harshavardhan Kamarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1"&gt;Alexander Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1"&gt;B. Aditya Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Fairness of Causal Algorithmic Recourse. (arXiv:2010.06529v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06529</id>
        <link href="http://arxiv.org/abs/2010.06529"/>
        <updated>2021-06-09T02:01:50.623Z</updated>
        <summary type="html"><![CDATA[Algorithmic fairness is typically studied from the perspective of
predictions. Instead, here we investigate fairness from the perspective of
recourse actions suggested to individuals to remedy an unfavourable
classification. We propose two new fairness criteria at the group and
individual level, which -- unlike prior work on equalising the average
group-wise distance from the decision boundary -- explicitly account for causal
relationships between features, thereby capturing downstream effects of
recourse actions performed in the physical world. We explore how our criteria
relate to others, such as counterfactual fairness, and show that fairness of
recourse is complementary to fairness of prediction. We study theoretically and
empirically how to enforce fair causal recourse by altering the classifier and
perform a case study on the Adult dataset. Finally, we discuss whether fairness
violations in the data generating process revealed by our criteria may be
better addressed by societal interventions as opposed to constraints on the
classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1"&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1"&gt;Amir-Hossein Karimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1"&gt;Umang Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valera_I/0/1/0/all/0/1"&gt;Isabel Valera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accommodating Picky Customers: Regret Bound and Exploration Complexity for Multi-Objective Reinforcement Learning. (arXiv:2011.13034v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13034</id>
        <link href="http://arxiv.org/abs/2011.13034"/>
        <updated>2021-06-09T02:01:50.617Z</updated>
        <summary type="html"><![CDATA[In this paper we consider multi-objective reinforcement learning where the
objectives are balanced using preferences. In practice, the preferences are
often given in an adversarial manner, e.g., customers can be picky in many
applications. We formalize this problem as an episodic learning problem on a
Markov decision process, where transitions are unknown and a reward function is
the inner product of a preference vector with pre-specified multi-objective
reward functions. We consider two settings. In the online setting, the agent
receives a (adversarial) preference every episode and proposes policies to
interact with the environment. We provide a model-based algorithm that achieves
a nearly minimax optimal regret bound
$\widetilde{\mathcal{O}}\bigl(\sqrt{\min\{d,S\}\cdot H^2 SAK}\bigr)$, where $d$
is the number of objectives, $S$ is the number of states, $A$ is the number of
actions, $H$ is the length of the horizon, and $K$ is the number of episodes.
Furthermore, we consider preference-free exploration, i.e., the agent first
interacts with the environment without specifying any preference and then is
able to accommodate arbitrary preference vector up to $\epsilon$ error. Our
proposed algorithm is provably efficient with a nearly optimal trajectory
complexity $\widetilde{\mathcal{O}}\bigl({\min\{d,S\}\cdot H^3
SA}/{\epsilon^2}\bigr)$. This result partly resolves an open problem raised by
\citet{jin2020reward}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jingfeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1"&gt;Vladimir Braverman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin F. Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Transformers. (arXiv:2106.04554v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04554</id>
        <link href="http://arxiv.org/abs/2106.04554"/>
        <updated>2021-06-09T02:01:50.552Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved great success in many artificial intelligence
fields, such as natural language processing, computer vision, and audio
processing. Therefore, it is natural to attract lots of interest from academic
and industry researchers. Up to the present, a great variety of Transformer
variants (a.k.a. X-formers) have been proposed, however, a systematic and
comprehensive literature review on these Transformer variants is still missing.
In this survey, we provide a comprehensive review of various X-formers. We
first briefly introduce the vanilla Transformer and then propose a new taxonomy
of X-formers. Next, we introduce the various X-formers from three perspectives:
architectural modification, pre-training, and applications. Finally, we outline
some potential directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Feature Distillation for Visual Recognition. (arXiv:2106.04411v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04411</id>
        <link href="http://arxiv.org/abs/2106.04411"/>
        <updated>2021-06-09T02:01:50.546Z</updated>
        <summary type="html"><![CDATA[Fairness is becoming an increasingly crucial issue for computer vision,
especially in the human-related decision systems. However, achieving
algorithmic fairness, which makes a model produce indiscriminative outcomes
against protected groups, is still an unresolved problem. In this paper, we
devise a systematic approach which reduces algorithmic biases via feature
distillation for visual recognition tasks, dubbed as MMD-based Fair
Distillation (MFD). While the distillation technique has been widely used in
general to improve the prediction accuracy, to the best of our knowledge, there
has been no explicit work that also tries to improve fairness via distillation.
Furthermore, We give a theoretical justification of our MFD on the effect of
knowledge distillation and fairness. Throughout the extensive experiments, we
show our MFD significantly mitigates the bias against specific minorities
without any loss of the accuracy on both synthetic and real-world face
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1"&gt;Sangwon Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Donggyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1"&gt;Taeeon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modular Deep Reinforcement Learning for Continuous Motion Planning with Temporal Logic. (arXiv:2102.12855v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12855</id>
        <link href="http://arxiv.org/abs/2102.12855"/>
        <updated>2021-06-09T02:01:50.431Z</updated>
        <summary type="html"><![CDATA[This paper investigates the motion planning of autonomous dynamical systems
modeled by Markov decision processes (MDP) with unknown transition
probabilities over continuous state and action spaces. Linear temporal logic
(LTL) is used to specify high-level tasks over infinite horizon, which can be
converted into a limit deterministic generalized B\"uchi automaton (LDGBA) with
several accepting sets. The novelty is to design an embedded product MDP
(EP-MDP) between the LDGBA and the MDP by incorporating a synchronous
tracking-frontier function to record unvisited accepting sets of the automaton,
and to facilitate the satisfaction of the accepting conditions. The proposed
LDGBA-based reward shaping and discounting schemes for the model-free
reinforcement learning (RL) only depend on the EP-MDP states and can overcome
the issues of sparse rewards. Rigorous analysis shows that any RL method that
optimizes the expected discounted return is guaranteed to find an optimal
policy whose traces maximize the satisfaction probability. A modular deep
deterministic policy gradient (DDPG) is then developed to generate such
policies over continuous state and action spaces. The performance of our
framework is evaluated via an array of OpenAI gym environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1"&gt;Mingyu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasanbeig_M/0/1/0/all/0/1"&gt;Mohammadhosein Hasanbeig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1"&gt;Shaoping Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1"&gt;Alessandro Abate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1"&gt;Zhen Kan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity and Overparameterization Bounds for Temporal Difference Learning with Neural Network Approximation. (arXiv:2103.01391v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01391</id>
        <link href="http://arxiv.org/abs/2103.01391"/>
        <updated>2021-06-09T02:01:50.403Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the dynamics of temporal difference learning with
neural network-based value function approximation over a general state space,
namely, \emph{Neural TD learning}. We consider two practically used algorithms,
projection-free and max-norm regularized Neural TD learning, and establish the
first convergence bounds for these algorithms. An interesting observation from
our results is that max-norm regularization can dramatically improve the
performance of TD learning algorithms, both in terms of sample complexity and
overparameterization. In particular, we prove that max-norm regularization
appears to be more effective than $\ell_2$-regularization, again both in terms
of sample complexity and overparameterization. The results in this work rely on
a novel Lyapunov drift analysis of the network parameters as a stopped and
controlled random process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1"&gt;Semih Cayci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satpathi_S/0/1/0/all/0/1"&gt;Siddhartha Satpathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1"&gt;Niao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Concise yet Effective model for Non-Aligned Incomplete Multi-view and Missing Multi-label Learning. (arXiv:2005.00976v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00976</id>
        <link href="http://arxiv.org/abs/2005.00976"/>
        <updated>2021-06-09T02:01:50.377Z</updated>
        <summary type="html"><![CDATA[In reality, learning from multi-view multi-label data inevitably confronts
three challenges: missing labels, incomplete views, and non-aligned views.
Existing methods mainly concern the first two and commonly need multiple
assumptions to attack them, making even state-of-the-arts involve at least two
explicit hyper-parameters such that model selection is quite difficult. More
roughly, they will fail in handling the third challenge, let alone addressing
the three jointly. In this paper, we aim at meeting these under the least
assumption by building a concise yet effective model with just one
hyper-parameter. To ease insufficiency of available labels, we exploit not only
the consensus of multiple views but also the global and local structures hidden
among multiple labels. Specifically, we introduce an indicator matrix to tackle
the first two challenges in a regression form while aligning the same
individual labels and all labels of different views in a common label space to
battle the third challenge. In aligning, we characterize the global and local
structures of multiple labels to be high-rank and low-rank, respectively.
Subsequently, an efficient algorithm with linear time complexity in the number
of samples is established. Finally, even without view-alignment, our method
substantially outperforms state-of-the-arts with view-alignment on five real
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Songcan Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning. (arXiv:1912.02631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.02631</id>
        <link href="http://arxiv.org/abs/1912.02631"/>
        <updated>2021-06-09T02:01:50.352Z</updated>
        <summary type="html"><![CDATA[Machine learning has started to be deployed in fields such as healthcare and
finance, which propelled the need for and growth of privacy-preserving machine
learning (PPML). We propose an actively secure four-party protocol (4PC), and a
framework for PPML, showcasing its applications on four of the most
widely-known machine learning algorithms -- Linear Regression, Logistic
Regression, Neural Networks, and Convolutional Neural Networks. Our 4PC
protocol tolerating at most one malicious corruption is practically efficient
as compared to the existing works. We use the protocol to build an efficient
mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and
Garbled worlds. Our framework operates in the offline-online paradigm over
rings and is instantiated in an outsourced setting for machine learning. Also,
we propose conversions especially relevant to privacy-preserving machine
learning. The highlights of our framework include using a minimal number of
expensive circuits overall as compared to ABY3. This can be seen in our
technique for truncation, which does not affect the online cost of
multiplication and removes the need for any circuits in the offline phase. Our
B2A conversion has an improvement of $\mathbf{7} \times$ in rounds and
$\mathbf{18} \times$ in the communication complexity. The practicality of our
framework is argued through improvements in the benchmarking of the
aforementioned algorithms when compared with ABY3. All the protocols are
implemented over a 64-bit ring in both LAN and WAN settings. Our improvements
go up to $\mathbf{187} \times$ for the training phase and $\mathbf{158} \times$
for the prediction phase when observed over LAN and WAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_H/0/1/0/all/0/1"&gt;Harsh Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rachuri_R/0/1/0/all/0/1"&gt;Rahul Rachuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ajith Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inference for Network Regression Models with Community Structure. (arXiv:2106.04271v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.04271</id>
        <link href="http://arxiv.org/abs/2106.04271"/>
        <updated>2021-06-09T02:01:50.340Z</updated>
        <summary type="html"><![CDATA[Network regression models, where the outcome comprises the valued edge in a
network and the predictors are actor or dyad-level covariates, are used
extensively in the social and biological sciences. Valid inference relies on
accurately modeling the residual dependencies among the relations. Frequently
homogeneity assumptions are placed on the errors which are commonly incorrect
and ignore critical, natural clustering of the actors. In this work, we present
a novel regression modeling framework that models the errors as resulting from
a community-based dependence structure and exploits the subsequent
exchangeability properties of the error distribution to obtain parsimonious
standard errors for regression parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pan_M/0/1/0/all/0/1"&gt;Mengjie Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+McCormick_T/0/1/0/all/0/1"&gt;Tyler H. McCormick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fosdick_B/0/1/0/all/0/1"&gt;Bailey K. Fosdick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Uncanny Similarity of Recurrence and Depth. (arXiv:2102.11011v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11011</id>
        <link href="http://arxiv.org/abs/2102.11011"/>
        <updated>2021-06-09T02:01:50.335Z</updated>
        <summary type="html"><![CDATA[It is widely believed that deep neural networks contain layer specialization,
wherein networks extract hierarchical features representing edges and patterns
in shallow layers and complete objects in deeper layers. Unlike common
feed-forward models that have distinct filters at each layer, recurrent
networks reuse the same parameters at various depths. In this work, we observe
that recurrent models exhibit the same hierarchical behaviors and the same
performance benefits as depth despite reusing the same filters at every
recurrence. By training models of various feed-forward and recurrent
architectures on several datasets for image classification as well as maze
solving, we show that recurrent networks have the ability to closely emulate
the behavior of non-recurrent deep models, often doing so with far fewer
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1"&gt;Avi Schwarzschild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Arjun Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiasi_A/0/1/0/all/0/1"&gt;Amin Ghiasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Obtaining Better Static Word Embeddings Using Contextual Embedding Models. (arXiv:2106.04302v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04302</id>
        <link href="http://arxiv.org/abs/2106.04302"/>
        <updated>2021-06-09T02:01:50.334Z</updated>
        <summary type="html"><![CDATA[The advent of contextual word embeddings -- representations of words which
incorporate semantic and syntactic information from their context -- has led to
tremendous improvements on a wide variety of NLP tasks. However, recent
contextual models have prohibitively high computational cost in many use-cases
and are often hard to interpret. In this work, we demonstrate that our proposed
distillation method, which is a simple extension of CBOW-based training, allows
to significantly improve computational efficiency of NLP applications, while
outperforming the quality of existing static embeddings trained from scratch as
well as those distilled from previously proposed methods. As a side-effect, our
approach also allows a fair comparison of both contextual and static embeddings
via standard lexical evaluation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Prakhar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Possibility in Algorithmic Fairness: Can Calibration and Equal Error Rates Be Reconciled?. (arXiv:2002.07676v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07676</id>
        <link href="http://arxiv.org/abs/2002.07676"/>
        <updated>2021-06-09T02:01:50.328Z</updated>
        <summary type="html"><![CDATA[Decision makers increasingly rely on algorithmic risk scores to determine
access to binary treatments including bail, loans, and medical interventions.
In these settings, we reconcile two fairness criteria that were previously
shown to be in conflict: calibration and error rate equality. In particular, we
derive necessary and sufficient conditions for the existence of calibrated
scores that yield classifications achieving equal error rates at any given
group-blind threshold. We then present an algorithm that searches for the most
accurate score subject to both calibration and minimal error rate disparity.
Applied to the COMPAS criminal risk assessment tool, we show that our method
can eliminate error disparities while maintaining calibration. In a separate
application to credit lending, we compare our procedure to the omission of
sensitive features and show that it raises both profit and the probability that
creditworthy individuals receive loans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1"&gt;Claire Lazar Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vijaykumar_S/0/1/0/all/0/1"&gt;Suhas Vijaykumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When in Doubt: Neural Non-Parametric Uncertainty Quantification for Epidemic Forecasting. (arXiv:2106.03904v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03904</id>
        <link href="http://arxiv.org/abs/2106.03904"/>
        <updated>2021-06-09T02:01:50.304Z</updated>
        <summary type="html"><![CDATA[Accurate and trustworthy epidemic forecasting is an important problem that
has impact on public health planning and disease mitigation. Most existing
epidemic forecasting models disregard uncertainty quantification, resulting in
mis-calibrated predictions. Recent works in deep neural models for
uncertainty-aware time-series forecasting also have several limitations; e.g.
it is difficult to specify meaningful priors in Bayesian NNs, while methods
like deep ensembling are computationally expensive in practice. In this paper,
we fill this important gap. We model the forecasting task as a probabilistic
generative process and propose a functional neural process model called EPIFNP,
which directly models the probability density of the forecast value. EPIFNP
leverages a dynamic stochastic correlation graph to model the correlations
between sequences in a non-parametric way, and designs different stochastic
latent variables to capture functional uncertainty from different perspectives.
Our extensive experiments in a real-time flu forecasting setting show that
EPIFNP significantly outperforms previous state-of-the-art models in both
accuracy and calibration metrics, up to 2.5x in accuracy and 2.4x in
calibration. Additionally, due to properties of its generative process,EPIFNP
learns the relations between the current season and similar patterns of
historical seasons,enabling interpretable forecasts. Beyond epidemic
forecasting, the EPIFNP can be of independent interest for advancing principled
uncertainty quantification in deep sequential models for predictive analytics]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamarthi_H/0/1/0/all/0/1"&gt;Harshavardhan Kamarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1"&gt;Lingkai Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1"&gt;Alexander Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1"&gt;B. Aditya Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust R-Peak Detection in Low-Quality Holter ECGs using 1D Convolutional Neural Network. (arXiv:2101.01666v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01666</id>
        <link href="http://arxiv.org/abs/2101.01666"/>
        <updated>2021-06-09T02:01:50.295Z</updated>
        <summary type="html"><![CDATA[Noise and low quality of ECG signals acquired from Holter or wearable devices
deteriorate the accuracy and robustness of R-peak detection algorithms. This
paper presents a generic and robust system for R-peak detection in Holter ECG
signals. While many proposed algorithms have successfully addressed the problem
of ECG R-peak detection, there is still a notable gap in the performance of
these detectors on such low-quality ECG records. Therefore, in this study, a
novel implementation of the 1D Convolutional Neural Network (CNN) is used
integrated with a verification model to reduce the number of false alarms. This
CNN architecture consists of an encoder block and a corresponding decoder block
followed by a sample-wise classification layer to construct the 1D segmentation
map of R- peaks from the input ECG signal. Once the proposed model has been
trained, it can solely be used to detect R-peaks possibly in a single channel
ECG data stream quickly and accurately, or alternatively, such a solution can
be conveniently employed for real-time monitoring on a lightweight portable
device. The model is tested on two open-access ECG databases: The China
Physiological Signal Challenge (2020) database (CPSC-DB) with more than one
million beats, and the commonly used MIT-BIH Arrhythmia Database (MIT-DB).
Experimental results demonstrate that the proposed systematic approach achieves
99.30% F1-score, 99.69% recall, and 98.91% precision in CPSC-DB, which is the
best R-peak detection performance ever achieved. Compared to all competing
methods, the proposed approach can reduce the false-positives and
false-negatives in Holter ECG signals by more than 54% and 82%, respectively.
Results also demonstrate similar or better performance than most competing
algorithms on MIT-DB with 99.83% F1-score, 99.85% recall, and 99.82% precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zahid_M/0/1/0/all/0/1"&gt;Muhammad Uzair Zahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ince_T/0/1/0/all/0/1"&gt;Turker Ince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Devecioglu_O/0/1/0/all/0/1"&gt;Ozer Can Devecioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1"&gt;Amith Khandakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tahir_A/0/1/0/all/0/1"&gt;Anas Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1"&gt;Moncef Gabbouj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Biomanufacturing Harvesting Decisions under Limited Historical Data. (arXiv:2101.03735v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03735</id>
        <link href="http://arxiv.org/abs/2101.03735"/>
        <updated>2021-06-09T02:01:50.295Z</updated>
        <summary type="html"><![CDATA[In the biopharmaceutical manufacturing, fermentation process plays a critical
role impacting on productivity and profit. Since biotherapeutics are
manufactured in living cells whose biological mechanisms are complex and have
highly variable outputs, in this paper, we introduce a model-based
reinforcement learning framework accounting for model risk to support
bioprocess online learning and guide the optimal reliable customized stopping
policy for fermentation process. Specifically, built on the dynamic mechanisms
of protein and impurity generation, we first construct a probabilistic model
characterizing the impact of underlying bioprocess stochastic uncertainty on
impurity and protein growth rates. Since biopharmaceutical manufacturing often
has very limited batch data during the development and early stage of
production, we derive the posterior distribution quantifying the process model
risk, and further develop the Bayesian rule based knowledge update to support
bioprocess online learning. With the prediction risk accounting for both
bioprocess stochastic uncertainty and model risk, the proposed reinforcement
learning framework can provide the optimal and reliable decision making. We
conduct the structural analysis of optimal policy and study the impact of model
risk on the policy selection. We can show that it asymptotically converges to
the optimal policy obtained under perfect information of underlying stochastic
process. Our case studies demonstrate that the proposed framework can greatly
improve the biomanufacturing industrial practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Martagan_T/0/1/0/all/0/1"&gt;Tugce Martagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akcay_A/0/1/0/all/0/1"&gt;Alp Akcay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ravenstein_B/0/1/0/all/0/1"&gt;Bram van Ravenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled Hyperedge Prediction with Structural Spectral Features and Neural Networks. (arXiv:2106.04292v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.04292</id>
        <link href="http://arxiv.org/abs/2106.04292"/>
        <updated>2021-06-09T02:01:50.291Z</updated>
        <summary type="html"><![CDATA[Hypergraph offers a framework to depict the multilateral relationships in
real-world complex data. Predicting higher-order relationships, i.e hyperedge,
becomes a fundamental problem for the full understanding of complicated
interactions. The development of graph neural network (GNN) has greatly
advanced the analysis of ordinary graphs with pair-wise relations. However,
these methods could not be easily extended to the case of hypergraph. In this
paper, we generalize the challenges of GNN in representing higher-order data in
principle, which are edge- and node-level ambiguities. To overcome the
challenges, we present \textbf{SNALS} that utilizes bipartite graph neural
network with structural features to collectively tackle the two ambiguity
issues. SNALS captures the joint interactions of a hyperedge by its local
environment, which is retrieved by collecting the spectrum information of their
connections. As a result, SNALS achieves nearly 30% performance increase
compared with most recent GNN-based models. In addition, we applied SNALS to
predict genetic higher-order interactions on 3D genome organization data. SNALS
showed consistently high prediction accuracy across different chromosomes, and
generated novel findings on 4-way gene interaction, which is further validated
by existing literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1"&gt;Changlin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Muhan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1"&gt;Wei Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Sha Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the stability properties of Gated Recurrent Units neural networks. (arXiv:2011.06806v4 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06806</id>
        <link href="http://arxiv.org/abs/2011.06806"/>
        <updated>2021-06-09T02:01:50.291Z</updated>
        <summary type="html"><![CDATA[The goal of this paper is to provide sufficient conditions for guaranteeing
the Input-to-State Stability (ISS) and the Incremental Input-to-State Stability
({\delta}ISS) of Gated Recurrent Units (GRUs) neural networks. These
conditions, devised for both single-layer and multi-layer architectures,
consist of nonlinear inequalities on network's weights. They can be employed to
check the stability of trained networks, or can be enforced as constraints
during the training procedure of a GRU. The resulting training procedure is
tested on a Quadruple Tank nonlinear benchmark system, showing satisfactory
modeling performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bonassi_F/0/1/0/all/0/1"&gt;Fabio Bonassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Farina_M/0/1/0/all/0/1"&gt;Marcello Farina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scattolini_R/0/1/0/all/0/1"&gt;Riccardo Scattolini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cooperative Stochastic Multi-agent Multi-armed Bandits Robust to Adversarial Corruptions. (arXiv:2106.04207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04207</id>
        <link href="http://arxiv.org/abs/2106.04207"/>
        <updated>2021-06-09T02:01:50.290Z</updated>
        <summary type="html"><![CDATA[We study the problem of stochastic bandits with adversarial corruptions in
the cooperative multi-agent setting, where $V$ agents interact with a common
$K$-armed bandit problem, and each pair of agents can communicate with each
other to expedite the learning process. In the problem, the rewards are
independently sampled from distributions across all agents and rounds, but they
may be corrupted by an adversary. Our goal is to minimize both the overall
regret and communication cost across all agents. We first show that an additive
term of corruption is unavoidable for any algorithm in this problem. Then, we
propose a new algorithm that is agnostic to the level of corruption. Our
algorithm not only achieves near-optimal regret in the stochastic setting, but
also obtains a regret with an additive term of corruption in the corrupted
setting, while maintaining efficient communication. The algorithm is also
applicable for the single-agent corruption problem, and achieves a high
probability regret that removes the multiplicative dependence of $K$ on
corruption level. Our result of the single-agent case resolves an open question
from Gupta et al. [2019].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dapeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question Generation for Adaptive Education. (arXiv:2106.04262v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04262</id>
        <link href="http://arxiv.org/abs/2106.04262"/>
        <updated>2021-06-09T02:01:50.290Z</updated>
        <summary type="html"><![CDATA[Intelligent and adaptive online education systems aim to make high-quality
education available for a diverse range of students. However, existing systems
usually depend on a pool of hand-made questions, limiting how fine-grained and
open-ended they can be in adapting to individual students. We explore targeted
question generation as a controllable sequence generation task. We first show
how to fine-tune pre-trained language models for deep knowledge tracing
(LM-KT). This model accurately predicts the probability of a student answering
a question correctly, and generalizes to questions not seen in training. We
then use LM-KT to specify the objective and data for training a model to
generate questions conditioned on the student and target difficulty. Our
results show we succeed at generating novel, well-calibrated language
translation questions for second language learners from a real online education
platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1"&gt;Megha Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session-Aware Query Auto-completion using Extreme Multi-label Ranking. (arXiv:2012.07654v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07654</id>
        <link href="http://arxiv.org/abs/2012.07654"/>
        <updated>2021-06-09T02:01:50.290Z</updated>
        <summary type="html"><![CDATA[Query auto-completion (QAC) is a fundamental feature in search engines where
the task is to suggest plausible completions of a prefix typed in the search
bar. Previous queries in the user session can provide useful context for the
user's intent and can be leveraged to suggest auto-completions that are more
relevant while adhering to the user's prefix. Such session-aware QACs can be
generated by recent sequence-to-sequence deep learning models; however, these
generative approaches often do not meet the stringent latency requirements of
responding to each user keystroke. Moreover, these generative approaches pose
the risk of showing nonsensical queries.

In this paper, we provide a solution to this problem: we take the novel
approach of modeling session-aware QAC as an eXtreme Multi-Label Ranking (XMR)
problem where the input is the previous query in the session and the user's
current prefix, while the output space is the set of tens of millions of
queries entered by users in the recent past. We adapt a popular XMR algorithm
for this purpose by proposing several modifications to the key steps in the
algorithm. The proposed modifications yield a 10x improvement in terms of Mean
Reciprocal Rank (MRR) over the baseline XMR approach on a public search logs
dataset. We are able to maintain an inference latency of less than 10 ms while
still using session context. When compared against baseline models of
acceptable latency, we observed a 33% improvement in MRR for short prefixes of
up to 3 characters. Moreover, our model yielded a statistically significant
improvement of 2.81% over a production QAC system in terms of suggestion
acceptance rate, when deployed on the search bar of an online shopping store as
part of an A/B test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1"&gt;Nishant Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1"&gt;Rajat Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_D/0/1/0/all/0/1"&gt;Daniel N. Hill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deciding What to Learn: A Rate-Distortion Approach. (arXiv:2101.06197v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06197</id>
        <link href="http://arxiv.org/abs/2101.06197"/>
        <updated>2021-06-09T02:01:50.290Z</updated>
        <summary type="html"><![CDATA[Agents that learn to select optimal actions represent a prominent focus of
the sequential decision-making literature. In the face of a complex environment
or constraints on time and resources, however, aiming to synthesize such an
optimal policy can become infeasible. These scenarios give rise to an important
trade-off between the information an agent must acquire to learn and the
sub-optimality of the resulting policy. While an agent designer has a
preference for how this trade-off is resolved, existing approaches further
require that the designer translate these preferences into a fixed learning
target for the agent. In this work, leveraging rate-distortion theory, we
automate this process such that the designer need only express their
preferences via a single hyperparameter and the agent is endowed with the
ability to compute its own learning targets that best achieve the desired
trade-off. We establish a general bound on expected discounted regret for an
agent that decides what to learn in this manner along with computational
experiments that illustrate the expressiveness of designer preferences and even
show improvements over Thompson sampling in identifying an optimal policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1"&gt;Dilip Arumugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Privacy-Preserving Text Classification based on Secure Multiparty Computation. (arXiv:2101.07365v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07365</id>
        <link href="http://arxiv.org/abs/2101.07365"/>
        <updated>2021-06-09T02:01:50.290Z</updated>
        <summary type="html"><![CDATA[We propose a privacy-preserving Naive Bayes classifier and apply it to the
problem of private text classification. In this setting, a party (Alice) holds
a text message, while another party (Bob) holds a classifier. At the end of the
protocol, Alice will only learn the result of the classifier applied to her
text input and Bob learns nothing. Our solution is based on Secure Multiparty
Computation (SMC). Our Rust implementation provides a fast and secure solution
for the classification of unstructured text. Applying our solution to the case
of spam detection (the solution is generic, and can be used in any other
scenario in which the Naive Bayes classifier can be employed), we can classify
an SMS as spam or ham in less than 340ms in the case where the dictionary size
of Bob's model includes all words (n = 5200) and Alice's SMS has at most m =
160 unigrams. In the case with n = 369 and m = 8 (the average of a spam SMS in
the database), our solution takes only 21ms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Resende_A/0/1/0/all/0/1"&gt;Amanda Resende&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Railsback_D/0/1/0/all/0/1"&gt;Davis Railsback&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dowsley_R/0/1/0/all/0/1"&gt;Rafael Dowsley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nascimento_A/0/1/0/all/0/1"&gt;Anderson C. A. Nascimento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aranha_D/0/1/0/all/0/1"&gt;Diego F. Aranha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RobustNav: Towards Benchmarking Robustness in Embodied Navigation. (arXiv:2106.04531v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04531</id>
        <link href="http://arxiv.org/abs/2106.04531"/>
        <updated>2021-06-09T02:01:50.289Z</updated>
        <summary type="html"><![CDATA[As an attempt towards assessing the robustness of embodied navigation agents,
we propose RobustNav, a framework to quantify the performance of embodied
navigation agents when exposed to a wide variety of visual - affecting RGB
inputs - and dynamics - affecting transition dynamics - corruptions. Most
recent efforts in visual navigation have typically focused on generalizing to
novel target environments with similar appearance and dynamics characteristics.
With RobustNav, we find that some standard embodied navigation agents
significantly underperform (or fail) in the presence of visual or dynamics
corruptions. We systematically analyze the kind of idiosyncrasies that emerge
in the behavior of such agents when operating under corruptions. Finally, for
visual corruptions in RobustNav, we show that while standard techniques to
improve robustness such as data-augmentation and self-supervised adaptation
offer some zero-shot resistance and improvements in navigation performance,
there is still a long way to go in terms of recovering lost performance
relative to clean "non-corrupt" settings, warranting more research in this
direction. Our code is available at https://github.com/allenai/robustnav]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_P/0/1/0/all/0/1"&gt;Prithvijit Chattopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1"&gt;Judy Hoffman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1"&gt;Aniruddha Kembhavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03932</id>
        <link href="http://arxiv.org/abs/2106.03932"/>
        <updated>2021-06-09T02:01:50.289Z</updated>
        <summary type="html"><![CDATA[Successful active speaker detection requires a three-stage pipeline: (i)
audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation
modeling between a reference speaker and the background speakers within each
frame, and (iii) temporal modeling for the reference speaker. Each stage of
this pipeline plays an important role for the final performance of the created
architecture. Based on a series of controlled experiments, this work presents
several practical guidelines for audio-visual active speaker detection.
Correspondingly, we present a new architecture called ASDNet, which achieves a
new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%
outperforming the second best with a large margin of 4.7%. Our code and
pretrained models are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1"&gt;Okan K&amp;#xf6;p&amp;#xfc;kl&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taseska_M/0/1/0/all/0/1"&gt;Maja Taseska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1"&gt;Gerhard Rigoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NWT: Towards natural audio-to-video generation with representation learning. (arXiv:2106.04283v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.04283</id>
        <link href="http://arxiv.org/abs/2106.04283"/>
        <updated>2021-06-09T02:01:50.289Z</updated>
        <summary type="html"><![CDATA[In this work we introduce NWT, an expressive speech-to-video model. Unlike
approaches that use domain-specific intermediate representations such as pose
keypoints, NWT learns its own latent representations, with minimal assumptions
about the audio and video content. To this end, we propose a novel discrete
variational autoencoder with adversarial loss, dVAE-Adv, which learns a new
discrete latent representation we call Memcodes. Memcodes are straightforward
to implement, require no additional loss terms, are stable to train compared
with other approaches, and show evidence of interpretability. To predict on the
Memcode space, we use an autoregressive encoder-decoder model conditioned on
audio. Additionally, our model can control latent attributes in the generated
video that are not annotated in the data. We train NWT on clips from HBO's Last
Week Tonight with John Oliver. NWT consistently scores above other approaches
in Mean Opinion Score (MOS) on tests of overall video naturalness, facial
naturalness and expressiveness, and lipsync quality. This work sets a strong
baseline for generalized audio-to-video synthesis. Samples are available at
https://next-week-tonight.github.io/NWT/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mama_R/0/1/0/all/0/1"&gt;Rayhane Mama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyndel_M/0/1/0/all/0/1"&gt;Marc S. Tyndel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadhim_H/0/1/0/all/0/1"&gt;Hashiam Kadhim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifford_C/0/1/0/all/0/1"&gt;Cole Clifford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thurairatnam_R/0/1/0/all/0/1"&gt;Ragavan Thurairatnam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Fairness in Classification with a Model-Agnostic Multi-Objective Algorithm. (arXiv:2009.04441v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04441</id>
        <link href="http://arxiv.org/abs/2009.04441"/>
        <updated>2021-06-09T02:01:50.289Z</updated>
        <summary type="html"><![CDATA[The goal of fairness in classification is to learn a classifier that does not
discriminate against groups of individuals based on sensitive attributes, such
as race and gender. One approach to designing fair algorithms is to use
relaxations of fairness notions as regularization terms or in a constrained
optimization problem. We observe that the hyperbolic tangent function can
approximate the indicator function. We leverage this property to define a
differentiable relaxation that approximates fairness notions provably better
than existing relaxations. In addition, we propose a model-agnostic
multi-objective architecture that can simultaneously optimize for multiple
fairness notions and multiple sensitive attributes and supports all statistical
parity-based notions of fairness. We use our relaxation with the
multi-objective architecture to learn fair classifiers. Experiments on public
datasets show that our method suffers a significantly lower loss of accuracy
than current debiasing algorithms relative to the unconstrained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Padh_K/0/1/0/all/0/1"&gt;Kirtan Padh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation. (arXiv:2009.07526v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07526</id>
        <link href="http://arxiv.org/abs/2009.07526"/>
        <updated>2021-06-09T02:01:50.288Z</updated>
        <summary type="html"><![CDATA[Scene graphs are semantic abstraction of images that encourage visual
understanding and reasoning. However, the performance of Scene Graph Generation
(SGG) is unsatisfactory when faced with biased data in real-world scenarios.
Conventional debiasing research mainly studies from the view of balancing data
distribution or learning unbiased models and representations, ignoring the
correlations among the biased classes. In this work, we analyze this problem
from a novel cognition perspective: automatically building a hierarchical
cognitive structure from the biased predictions and navigating that hierarchy
to locate the relationships, making the tail relationships receive more
attention in a coarse-to-fine mode. To this end, we propose a novel debiasing
Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive
structure CogTree to organize the relationships based on the prediction of a
biased SGG model. The CogTree distinguishes remarkably different relationships
at first and then focuses on a small portion of easily confused ones. Then, we
propose a debiasing loss specially for this cognitive structure, which supports
coarse-to-fine distinction for the correct relationships. The loss is
model-agnostic and consistently boosting the performance of several
state-of-the-art models. The code is available at:
https://github.com/CYVincent/Scene-Graph-Transformer-CogTree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuan Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yue Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stochastic Subgradient Method for Distributionally Robust Non-Convex Learning. (arXiv:2006.04873v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04873</id>
        <link href="http://arxiv.org/abs/2006.04873"/>
        <updated>2021-06-09T02:01:50.280Z</updated>
        <summary type="html"><![CDATA[We consider a distributionally robust formulation of stochastic optimization
problems arising in statistical learning, where robustness is with respect to
uncertainty in the underlying data distribution. Our formulation builds on
risk-averse optimization techniques and the theory of coherent risk measures.
It uses semi-deviation risk for quantifying uncertainty, allowing us to compute
solutions that are robust against perturbations in the population data
distribution. We consider a large family of loss functions that can be
non-convex and non-smooth and develop an efficient stochastic subgradient
method. We prove that it converges to a point satisfying the optimality
conditions. To our knowledge, this is the first method with rigorous
convergence guarantees in the context of non-convex non-smooth distributionally
robust stochastic optimization. Our method can achieve any desired level of
robustness with little extra computational cost compared to population risk
minimization. We also illustrate the performance of our algorithm on real
datasets arising in convex and non-convex supervised learning problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert G&amp;#xfc;rb&amp;#xfc;zbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ruszczynski_A/0/1/0/all/0/1"&gt;Andrzej Ruszczy&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Landi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Multiple Noisy Partial Labelers. (arXiv:2106.04530v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04530</id>
        <link href="http://arxiv.org/abs/2106.04530"/>
        <updated>2021-06-09T02:01:50.279Z</updated>
        <summary type="html"><![CDATA[Programmatic weak supervision creates models without hand-labeled training
data by combining the outputs of noisy, user-written rules and other heuristic
labelers. Existing frameworks make the restrictive assumption that labelers
output a single class label. Enabling users to create partial labelers that
output subsets of possible class labels would greatly expand the expressivity
of programmatic weak supervision. We introduce this capability by defining a
probabilistic generative model that can estimate the underlying accuracies of
multiple noisy partial labelers without ground truth labels. We prove that this
class of models is generically identifiable up to label swapping under mild
conditions. We also show how to scale up learning to 100k examples in one
minute, a 300X speed up compared to a naive implementation. We evaluate our
framework on three text classification and six object classification tasks. On
text tasks, adding partial labels increases average accuracy by 9.6 percentage
points. On image tasks, we show that partial labels allow us to approach some
zero-shot object classification problems with programmatic weak supervision by
using class attributes as partial labelers. Our framework is able to achieve
accuracy comparable to recent embedding-based zero-shot learning methods using
only pre-trained attribute detectors]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Peilin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1"&gt;Tiffany Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1"&gt;Stephen H. Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Representation Learning for Efficient Medical Image Analysis. (arXiv:2006.11223v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11223</id>
        <link href="http://arxiv.org/abs/2006.11223"/>
        <updated>2021-06-09T02:01:50.278Z</updated>
        <summary type="html"><![CDATA[Medical image analysis typically includes several tasks such as enhancement,
segmentation, and classification. Traditionally, these tasks are implemented
using separate deep learning models for separate tasks, which is not efficient
because it involves unnecessary training repetitions, demands greater
computational resources, and requires a relatively large amount of labeled
data. In this paper, we propose a multi-task training approach for medical
image analysis, where individual tasks are fine-tuned simultaneously through
relevant knowledge transfer using a unified modality-specific feature
representation (UMS-Rep). We explore different fine-tuning strategies to
demonstrate the impact of the strategy on the performance of target medical
image tasks. We experiment with different visual tasks (e.g., image denoising,
segmentation, and classification) to highlight the advantages offered with our
approach for two imaging modalities, chest X-ray and Doppler echocardiography.
Our results demonstrate that the proposed approach reduces the overall demand
for computational resources and improves target task generalization and
performance. Further, our results prove that the performance of target tasks in
medical images is highly influenced by the utilized fine-tuning strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajaraman_S/0/1/0/all/0/1"&gt;Sivaramakrishnan Rajaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antani_S/0/1/0/all/0/1"&gt;Sameer Antani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Graph Transformers with Spectral Attention. (arXiv:2106.03893v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03893</id>
        <link href="http://arxiv.org/abs/2106.03893"/>
        <updated>2021-06-09T02:01:50.277Z</updated>
        <summary type="html"><![CDATA[In recent years, the Transformer architecture has proven to be very
successful in sequence processing, but its application to other data
structures, such as graphs, has remained limited due to the difficulty of
properly defining positions. Here, we present the $\textit{Spectral Attention
Network}$ (SAN), which uses a learned positional encoding (LPE) that can take
advantage of the full Laplacian spectrum to learn the position of each node in
a given graph. This LPE is then added to the node features of the graph and
passed to a fully-connected Transformer. By leveraging the full spectrum of the
Laplacian, our model is theoretically powerful in distinguishing graphs, and
can better detect similar sub-structures from their resonance. Further, by
fully connecting the graph, the Transformer does not suffer from
over-squashing, an information bottleneck of most GNNs, and enables better
modeling of physical phenomenons such as heat transfer and electric
interaction. When tested empirically on a set of 4 standard datasets, our model
performs on par or better than state-of-the-art GNNs, and outperforms any
attention-based model by a wide margin, becoming the first fully-connected
architecture to perform well on graph benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kreuzer_D/0/1/0/all/0/1"&gt;Devin Kreuzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beaini_D/0/1/0/all/0/1"&gt;Dominique Beaini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Letourneau_V/0/1/0/all/0/1"&gt;Vincent L&amp;#xe9;tourneau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tossou_P/0/1/0/all/0/1"&gt;Prudencio Tossou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00250</id>
        <link href="http://arxiv.org/abs/2106.00250"/>
        <updated>2021-06-09T02:01:50.276Z</updated>
        <summary type="html"><![CDATA[Multimodal Machine Translation (MMT) enriches the source text with visual
information for translation. It has gained popularity in recent years, and
several pipelines have been proposed in the same direction. Yet, the task lacks
quality datasets to illustrate the contribution of visual modality in the
translation systems. In this paper, we propose our system under the team name
Volta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We
also participate in the textual-only subtask of the same language pair for
which we use mBART, a pretrained multilingual sequence-to-sequence model. For
multimodal translation, we propose to enhance the textual input by bringing the
visual information to a textual domain by extracting object tags from the
image. We also explore the robustness of our system by systematically degrading
the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test
set and challenge set of the multimodal task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1"&gt;Radhika Mamidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Improving BERT's Mathematical Abilities by Predicting the Order of Reasoning. (arXiv:2106.03921v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03921</id>
        <link href="http://arxiv.org/abs/2106.03921"/>
        <updated>2021-06-09T02:01:50.276Z</updated>
        <summary type="html"><![CDATA[Imagine you are in a supermarket. You have two bananas in your basket and
want to buy four apples. How many fruits do you have in total? This seemingly
straightforward question can be challenging for data-driven language models,
even if trained at scale. However, we would expect such generic language models
to possess some mathematical abilities in addition to typical linguistic
competence. Towards this goal, we investigate if a commonly used language
model, BERT, possesses such mathematical abilities and, if so, to what degree.
For that, we fine-tune BERT on a popular dataset for word math problems,
AQuA-RAT, and conduct several tests to understand learned representations
better. Since we teach models trained on natural language to do formal
mathematics, we hypothesize that such models would benefit from training on
semi-formal steps that explain how math results are derived. To better
accommodate such training, we also propose new pretext tasks for learning
mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or
NROP). With this new model, we achieve significantly better outcomes than
data-driven baselines and even on-par with more tailored models. We also show
how to reduce positional bias in such models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Piekos_P/0/1/0/all/0/1"&gt;Piotr Pi&amp;#x119;kos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1"&gt;Henryk Michalewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correcting Momentum in Temporal Difference Learning. (arXiv:2106.03955v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03955</id>
        <link href="http://arxiv.org/abs/2106.03955"/>
        <updated>2021-06-09T02:01:50.275Z</updated>
        <summary type="html"><![CDATA[A common optimization tool used in deep reinforcement learning is momentum,
which consists in accumulating and discounting past gradients, reapplying them
at each iteration. We argue that, unlike in supervised learning, momentum in
Temporal Difference (TD) learning accumulates gradients that become doubly
stale: not only does the gradient of the loss change due to parameter updates,
the loss itself changes due to bootstrapping. We first show that this
phenomenon exists, and then propose a first-order correction term to momentum.
We show that this correction term improves sample efficiency in policy
evaluation by correcting target value drift. An important insight of this work
is that deep RL methods are not always best served by directly importing
techniques from the supervised setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1"&gt;Emmanuel Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Virtual Screening of Pharmaceutical Compounds with hERG Inhibitory Activity (Cardiotoxicity) using Ensemble Learning. (arXiv:2106.04377v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.04377</id>
        <link href="http://arxiv.org/abs/2106.04377"/>
        <updated>2021-06-09T02:01:50.271Z</updated>
        <summary type="html"><![CDATA[In silico prediction of cardiotoxicity with high sensitivity and specificity
for potential drug molecules can be of immense value. Hence, building machine
learning classification models, based on some features extracted from the
molecular structure of drugs, which are capable of efficiently predicting
cardiotoxicity is critical. In this paper, we consider the application of
various machine learning approaches, and then propose an ensemble classifier
for the prediction of molecular activity on a Drug Discovery Hackathon (DDH)
(1st reference) dataset. We have used only 2-D descriptors of SMILE notations
for our prediction. Our ensemble classification uses 5 classifiers (2 Random
Forest Classifiers, 2 Support Vector Machines and a Dense Neural Network) and
uses Max-Voting technique and Weighted-Average technique for final decision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Aditya Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bhavsar_A/0/1/0/all/0/1"&gt;Arnav Bhavsar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation. (arXiv:2106.04151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04151</id>
        <link href="http://arxiv.org/abs/2106.04151"/>
        <updated>2021-06-09T02:01:50.266Z</updated>
        <summary type="html"><![CDATA[Unsupervised Domain Adaptation (UDA) aims to generalize the knowledge learned
from a well-labeled source domain to an unlabeled target domain. Recently,
adversarial domain adaptation with two distinct classifiers (bi-classifier) has
been introduced into UDA which is effective to align distributions between
different domains. Previous bi-classifier adversarial learning methods only
focus on the similarity between the outputs of two distinct classifiers.
However, the similarity of the outputs cannot guarantee the accuracy of target
samples, i.e., target samples may match to wrong categories even if the
discrepancy between two classifiers is small. To challenge this issue, in this
paper, we propose a cross-domain gradient discrepancy minimization (CGDM)
method which explicitly minimizes the discrepancy of gradients generated by
source samples and target samples. Specifically, the gradient gives a cue for
the semantic information of target samples so it can be used as a good
supervision to improve the accuracy of target samples. In order to compute the
gradient signal of target samples, we further obtain target pseudo labels
through a clustering-based self-supervised learning. Extensive experiments on
three widely used UDA datasets show that our method surpasses many previous
state-of-the-arts. Codes are available at https://github.com/lijin118/CGDM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhekai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hongzu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear MPC for Offset-Free Tracking of systems learned by GRU Neural Networks. (arXiv:2103.02383v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02383</id>
        <link href="http://arxiv.org/abs/2103.02383"/>
        <updated>2021-06-09T02:01:50.260Z</updated>
        <summary type="html"><![CDATA[The use of Recurrent Neural Networks (RNNs) for system identification has
recently gathered increasing attention, thanks to their black-box modeling
capabilities.Albeit RNNs have been fruitfully adopted in many applications,
only few works are devoted to provide rigorous theoretical foundations that
justify their use for control purposes. The aim of this paper is to describe
how stable Gated Recurrent Units (GRUs), a particular RNN architecture, can be
trained and employed in a Nonlinear MPC framework to perform offset-free
tracking of constant references with guaranteed closed-loop stability. The
proposed approach is tested on a pH neutralization process benchmark, showing
remarkable performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bonassi_F/0/1/0/all/0/1"&gt;Fabio Bonassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Silva_C/0/1/0/all/0/1"&gt;Caio Fabio Oliveira da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scattolini_R/0/1/0/all/0/1"&gt;Riccardo Scattolini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Federated Learning in the Presence of Arbitrary Device Unavailability. (arXiv:2106.04159v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04159</id>
        <link href="http://arxiv.org/abs/2106.04159"/>
        <updated>2021-06-09T02:01:50.254Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) coordinates with numerous heterogeneous devices to
collaboratively train a shared model while preserving user privacy. Despite its
multiple advantages, FL faces new challenges. One challenge arises when devices
drop out of the training process beyond the control of the central server. In
this case, the convergence of popular FL algorithms such as FedAvg is severely
influenced by the straggling devices. To tackle this challenge, we study
federated learning algorithms under arbitrary device unavailability and propose
an algorithm named Memory-augmented Impatient Federated Averaging (MIFA). Our
algorithm efficiently avoids excessive latency induced by inactive devices, and
corrects the gradient bias using the memorized latest updates from the devices.
We prove that MIFA achieves minimax optimal convergence rates on non-i.i.d.
data for both strongly convex and non-convex smooth functions. We also provide
an explicit characterization of the improvement over baseline algorithms
through a case study, and validate the results by numerical experiments on
real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xinran Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longbo Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-dataset Pretraining: A Unified Model for Semantic Segmentation. (arXiv:2106.04121v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04121</id>
        <link href="http://arxiv.org/abs/2106.04121"/>
        <updated>2021-06-09T02:01:50.234Z</updated>
        <summary type="html"><![CDATA[Collecting annotated data for semantic segmentation is time-consuming and
hard to scale up. In this paper, we for the first time propose a unified
framework, termed as Multi-Dataset Pretraining, to take full advantage of the
fragmented annotations of different datasets. The highlight is that the
annotations from different domains can be efficiently reused and consistently
boost performance for each specific domain. This is achieved by first
pretraining the network via the proposed pixel-to-prototype contrastive loss
over multiple datasets regardless of their taxonomy labels, and followed by
fine-tuning the pretrained model over specific dataset as usual. In order to
better model the relationship among images and classes from different datasets,
we extend the pixel level embeddings via cross dataset mixing and propose a
pixel-to-class sparse coding strategy that explicitly models the pixel-class
similarity over the manifold embedding space. In this way, we are able to
increase intra-class compactness and inter-class separability, as well as
considering inter-class similarity across different datasets for better
transferability. Experiments conducted on several benchmarks demonstrate its
superior performance. Notably, MDP consistently outperforms the pretrained
models over ImageNet by a considerable margin, while only using less than 10%
samples for pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Bowen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaopeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haohang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wenrui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Junni Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Broadcasted Residual Learning for Efficient Keyword Spotting. (arXiv:2106.04140v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.04140</id>
        <link href="http://arxiv.org/abs/2106.04140"/>
        <updated>2021-06-09T02:01:50.211Z</updated>
        <summary type="html"><![CDATA[Keyword spotting is an important research field because it plays a key role
in device wake-up and user interaction on smart devices. However, it is
challenging to minimize errors while operating efficiently in devices with
limited resources such as mobile phones. We present a broadcasted residual
learning method to achieve high accuracy with small model size and
computational load. Our method configures most of the residual functions as 1D
temporal convolution while still allows 2D convolution together using a
broadcasted-residual connection that expands temporal output to
frequency-temporal dimension. This residual mapping enables the network to
effectively represent useful audio features with much less computation than
conventional convolutional neural networks. We also propose a novel network
architecture, Broadcasting-residual network (BC-ResNet), based on broadcasted
residual learning and describe how to scale up the model according to the
target device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%
top-1 accuracy on Google speech command datasets v1 and v2, respectively, and
consistently outperform previous approaches, using fewer computations and
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Byeonggeun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Simyung Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jinkyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_D/0/1/0/all/0/1"&gt;Dooyong Sung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Normalization Orthogonalizes Representations in Deep Random Networks. (arXiv:2106.03970v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.03970</id>
        <link href="http://arxiv.org/abs/2106.03970"/>
        <updated>2021-06-09T02:01:50.210Z</updated>
        <summary type="html"><![CDATA[This paper underlines a subtle property of batch-normalization (BN):
Successive batch normalizations with random linear transformations make hidden
representations increasingly orthogonal across layers of a deep neural network.
We establish a non-asymptotic characterization of the interplay between depth,
width, and the orthogonality of deep representations. More precisely, under a
mild assumption, we prove that the deviation of the representations from
orthogonality rapidly decays with depth up to a term inversely proportional to
the network width. This result has two main implications: 1) Theoretically, as
the depth grows, the distribution of the representation -- after the linear
layers -- contracts to a Wasserstein-2 ball around an isotropic Gaussian
distribution. Furthermore, the radius of this Wasserstein ball shrinks with the
width of the network. 2) In practice, the orthogonality of the representations
directly influences the performance of stochastic gradient descent (SGD). When
representations are initially aligned, we observe SGD wastes many iterations to
orthogonalize representations before the classification. Nevertheless, we
experimentally show that starting optimization from orthogonal representations
is sufficient to accelerate SGD, with no need for BN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Daneshmand_H/0/1/0/all/0/1"&gt;Hadi Daneshmand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Joudaki_A/0/1/0/all/0/1"&gt;Amir Joudaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulated Adversarial Testing of Face Recognition Models. (arXiv:2106.04569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04569</id>
        <link href="http://arxiv.org/abs/2106.04569"/>
        <updated>2021-06-09T02:01:50.208Z</updated>
        <summary type="html"><![CDATA[Most machine learning models are validated and tested on fixed datasets. This
can give an incomplete picture of the capabilities and weaknesses of the model.
Such weaknesses can be revealed at test time in the real world. The risks
involved in such failures can be loss of profits, loss of time or even loss of
life in certain critical applications. In order to alleviate this issue,
simulators can be controlled in a fine-grained manner using interpretable
parameters to explore the semantic image manifold. In this work, we propose a
framework for learning how to test machine learning algorithms using simulators
in an adversarial manner in order to find weaknesses in the model before
deploying it in critical scenarios. We apply this model in a face recognition
scenario. We are the first to show that weaknesses of models trained on real
data can be discovered using simulated samples. Using our proposed method, we
can find adversarial synthetic faces that fool contemporary face recognition
models. This demonstrates the fact that these models have weaknesses that are
not measured by commonly used validation datasets. We hypothesize that this
type of adversarial examples are not isolated, but usually lie in connected
components in the latent space of the simulator. We present a method to find
these adversarial regions as opposed to the typical adversarial points found in
the adversarial example literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1"&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1"&gt;Adam Kortylewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1"&gt;Weichao Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cihang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1"&gt;Sarah Adel Bargal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1"&gt;Stan Sclaroff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slot Machines: Discovering Winning Combinations of Random Weights in Neural Networks. (arXiv:2101.06475v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06475</id>
        <link href="http://arxiv.org/abs/2101.06475"/>
        <updated>2021-06-09T02:01:50.208Z</updated>
        <summary type="html"><![CDATA[In contrast to traditional weight optimization in a continuous space, we
demonstrate the existence of effective random networks whose weights are never
updated. By selecting a weight among a fixed set of random values for each
individual connection, our method uncovers combinations of random weights that
match the performance of traditionally-trained networks of the same capacity.
We refer to our networks as "slot machines" where each reel (connection)
contains a fixed set of symbols (random values). Our backpropagation algorithm
"spins" the reels to seek "winning" combinations, i.e., selections of random
weight values that minimize the given loss. Quite surprisingly, we find that
allocating just a few random values to each connection (e.g., 8 values per
connection) yields highly competitive combinations despite being dramatically
more constrained compared to traditionally learned weights. Moreover,
finetuning these combinations often improves performance over the trained
baselines. A randomly initialized VGG-19 with 8 values per connection contains
a combination that achieves 91% test accuracy on CIFAR-10. Our method also
achieves an impressive performance of 98.2% on MNIST for neural networks
containing only random weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aladago_M/0/1/0/all/0/1"&gt;Maxwell Mbabilla Aladago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1"&gt;Lorenzo Torresani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Distributional Treatment Effect with Kernel Conditional Mean Embeddings and U-Statistic Regression. (arXiv:2102.08208v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08208</id>
        <link href="http://arxiv.org/abs/2102.08208"/>
        <updated>2021-06-09T02:01:50.207Z</updated>
        <summary type="html"><![CDATA[We propose to analyse the conditional distributional treatment effect
(CoDiTE), which, in contrast to the more common conditional average treatment
effect (CATE), is designed to encode a treatment's distributional aspects
beyond the mean. We first introduce a formal definition of the CoDiTE
associated with a distance function between probability measures. Then we
discuss the CoDiTE associated with the maximum mean discrepancy via kernel
conditional mean embeddings, which, coupled with a hypothesis test, tells us
whether there is any conditional distributional effect of the treatment.
Finally, we investigate what kind of conditional distributional effect the
treatment has, both in an exploratory manner via the conditional witness
function, and in a quantitative manner via U-statistic regression, generalising
the CATE to higher-order moments. Experiments on synthetic, semi-synthetic and
real datasets demonstrate the merits of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Park_J/0/1/0/all/0/1"&gt;Junhyung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shalit_U/0/1/0/all/0/1"&gt;Uri Shalit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Muandet_K/0/1/0/all/0/1"&gt;Krikamol Muandet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight. (arXiv:2106.04263v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04263</id>
        <link href="http://arxiv.org/abs/2106.04263"/>
        <updated>2021-06-09T02:01:50.205Z</updated>
        <summary type="html"><![CDATA[Vision Transformer (ViT) attains state-of-the-art performance in visual
recognition, and the variant, Local Vision Transformer, makes further
improvements. The major component in Local Vision Transformer, local attention,
performs the attention separately over small local windows. We rephrase local
attention as a channel-wise locally-connected layer and analyze it from two
network regularization manners, sparse connectivity and weight sharing, as well
as weight computation. Sparse connectivity: there is no connection across
channels, and each position is connected to the positions within a small local
window. Weight sharing: the connection weights for one position are shared
across channels or within each group of channels. Dynamic weight: the
connection weights are dynamically predicted according to each image instance.
We point out that local attention resembles depth-wise convolution and its
dynamic version in sparse connectivity. The main difference lies in weight
sharing - depth-wise convolution shares connection weights (kernel weights)
across spatial positions. We empirically observe that the models based on
depth-wise convolution and the dynamic variant with lower computation
complexity perform on-par with or sometimes slightly better than Swin
Transformer, an instance of Local Vision Transformer, for ImageNet
classification, COCO object detection and ADE semantic segmentation. These
observations suggest that Local Vision Transformer takes advantage of two
regularization forms and dynamic weight to increase the network capacity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1"&gt;Qi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zejia Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qi Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingdong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Machine Learning with Plausible Deniability. (arXiv:2106.04267v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04267</id>
        <link href="http://arxiv.org/abs/2106.04267"/>
        <updated>2021-06-09T02:01:50.205Z</updated>
        <summary type="html"><![CDATA[We study the question of how well machine learning (ML) models trained on a
certain data set provide privacy for the training data, or equivalently,
whether it is possible to reverse-engineer the training data from a given ML
model. While this is easy to answer negatively in the most general case, it is
interesting to note that the protection extends over non-recoverability towards
plausible deniability: Given an ML model $f$, we show that one can take a set
of purely random training data, and from this define a suitable ``learning
rule'' that will produce a ML model that is exactly $f$. Thus, any speculation
about which data has been used to train $f$ is deniable upon the claim that any
other data could have led to the same results. We corroborate our theoretical
finding with practical examples, and open source implementations of how to find
the learning rules for a chosen set of raining data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rass_S/0/1/0/all/0/1"&gt;Stefan Rass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konig_S/0/1/0/all/0/1"&gt;Sandra K&amp;#xf6;nig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wachter_J/0/1/0/all/0/1"&gt;Jasmin Wachter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Egger_M/0/1/0/all/0/1"&gt;Manuel Egger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hobisch_M/0/1/0/all/0/1"&gt;Manuel Hobisch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Noisy Labels with Deep Neural Networks: A Survey. (arXiv:2007.08199v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08199</id>
        <link href="http://arxiv.org/abs/2007.08199"/>
        <updated>2021-06-09T02:01:50.204Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved remarkable success in numerous domains with help
from large amounts of big data. However, the quality of data labels is a
concern because of the lack of high-quality labels in many real-world
scenarios. As noisy labels severely degrade the generalization performance of
deep neural networks, learning from noisy labels (robust training) is becoming
an important task in modern deep learning applications. In this survey, we
first describe the problem of learning with label noise from a supervised
learning perspective. Next, we provide a comprehensive review of 57
state-of-the-art robust training methods, all of which are categorized into
five groups according to their methodological difference, followed by a
systematic comparison of six properties used to evaluate their superiority.
Subsequently, we perform an in-depth analysis of noise rate estimation and
summarize the typically used evaluation methodology, including public noisy
datasets and evaluation metrics. Finally, we present several promising research
directions that can serve as a guideline for future studies. All the contents
will be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hwanjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yooju Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae-Gil Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NISQ Algorithm for Semidefinite Programming. (arXiv:2106.03891v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.03891</id>
        <link href="http://arxiv.org/abs/2106.03891"/>
        <updated>2021-06-09T02:01:50.203Z</updated>
        <summary type="html"><![CDATA[Semidefinite Programming (SDP) is a class of convex optimization programs
with vast applications in control theory, quantum information, combinatorial
optimization and operational research. Noisy intermediate-scale quantum (NISQ)
algorithms aim to make an efficient use of the current generation of quantum
hardware. However, optimizing variational quantum algorithms is a challenge as
it is an NP-hard problem that in general requires an exponential time to solve
and can contain many far from optimal local minima. Here, we present a current
term NISQ algorithm for SDP. The classical optimization program of our NISQ
solver is another SDP over a smaller dimensional ansatz space. We harness the
SDP based formulation of the Hamiltonian ground state problem to design a NISQ
eigensolver. Unlike variational quantum eigensolvers, the classical
optimization program of our eigensolver is convex, can be solved in polynomial
time with the number of ansatz parameters and every local minimum is a global
minimum. Further, we demonstrate the potential of our NISQ SDP solver by
finding the largest eigenvalue of up to $2^{1000}$ dimensional matrices and
solving graph problems related to quantum contextuality. We also discuss NISQ
algorithms for rank-constrained SDPs. Our work extends the application of NISQ
computers onto one of the most successful algorithmic frameworks of the past
few decades.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bharti_K/0/1/0/all/0/1"&gt;Kishor Bharti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Haug_T/0/1/0/all/0/1"&gt;Tobias Haug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Vedral_V/0/1/0/all/0/1"&gt;Vlatko Vedral&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kwek_L/0/1/0/all/0/1"&gt;Leong-Chuan Kwek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast rates in structured prediction. (arXiv:2102.00760v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00760</id>
        <link href="http://arxiv.org/abs/2102.00760"/>
        <updated>2021-06-09T02:01:50.201Z</updated>
        <summary type="html"><![CDATA[Discrete supervised learning problems such as classification are often
tackled by introducing a continuous surrogate problem akin to regression.
Bounding the original error, between estimate and solution, by the surrogate
error endows discrete problems with convergence rates already shown for
continuous instances. Yet, current approaches do not leverage the fact that
discrete problems are essentially predicting a discrete output when continuous
problems are predicting a continuous value. In this paper, we tackle this issue
for general structured prediction problems, opening the way to "super fast"
rates, that is, convergence rates for the excess risk faster than $n^{-1}$,
where $n$ is the number of observations, with even exponential rates with the
strongest assumptions. We first illustrate it for predictors based on nearest
neighbors, generalizing rates known for binary classification to any discrete
problem within the framework of structured prediction. We then consider kernel
ridge regression where we improve known rates in $n^{-1/4}$ to arbitrarily fast
rates, depending on a parameter characterizing the hardness of the problem,
thus allowing, under smoothness assumptions, to bypass the curse of
dimensionality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1"&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Bin Packing with Predictions. (arXiv:2102.03311v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03311</id>
        <link href="http://arxiv.org/abs/2102.03311"/>
        <updated>2021-06-09T02:01:50.201Z</updated>
        <summary type="html"><![CDATA[Bin packing is a classic optimization problem with a wide range of
applications from load balancing in networks to supply chain management. In
this work we study the online variant of the problem, in which a sequence of
items of various sizes must be placed into a minimum number of bins of uniform
capacity. The online algorithm is enhanced with a (potentially erroneous)
prediction concerning the frequency of item sizes in the sequence. We design
and analyze online algorithms with efficient tradeoffs between consistency
(i.e., the competitive ratio assuming no prediction error) and robustness
(i.e., the competitive ratio under adversarial error), and whose performance
degrades gently as a function of the prediction error. This is the first
theoretical study of online bin packing in the realistic setting of erroneous
predictions, as well as the first experimental study in the setting in which
the input is generated according to both static and evolving distributions.
Previous work on this problem has only addressed the extreme cases with respect
to the prediction error, has relied on overly powerful and error-free
prediction oracles, and has focused on experimental evaluation based on static
input distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Angelopoulos_S/0/1/0/all/0/1"&gt;Spyros Angelopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamali_S/0/1/0/all/0/1"&gt;Shahin Kamali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shadkami_K/0/1/0/all/0/1"&gt;Kimia Shadkami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super-Human Performance in Online Low-latency Recognition of Conversational Speech. (arXiv:2010.03449v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03449</id>
        <link href="http://arxiv.org/abs/2010.03449"/>
        <updated>2021-06-09T02:01:50.200Z</updated>
        <summary type="html"><![CDATA[Achieving super-human performance in recognizing human speech has been a goal
for several decades, as researchers have worked on increasingly challenging
tasks. In the 1990's it was discovered, that conversational speech between two
humans turns out to be considerably more difficult than read speech as
hesitations, disfluencies, false starts and sloppy articulation complicate
acoustic processing and require robust handling of acoustic, lexical and
language context, jointly. Early attempts with statistical models could only
reach error rates over 50% and far from human performance (WER of around 5.5%).
Neural hybrid models and recent attention-based encoder-decoder models have
considerably improved performance as such contexts can now be learned in an
integral fashion. However, processing such contexts requires an entire
utterance presentation and thus introduces unwanted delays before a recognition
result can be output. In this paper, we address performance as well as latency.
We present results for a system that can achieve super-human performance (at a
WER of 5.0%, over the Switchboard conversational benchmark) at a word based
latency of only 1 second behind a speaker's speech. The system uses multiple
attention-based encoder-decoder networks integrated within a novel low latency
incremental inference approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thai-Son Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stueker_S/0/1/0/all/0/1"&gt;Sebastian Stueker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1"&gt;Alex Waibel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the use of automatically generated synthetic image datasets for benchmarking face recognition. (arXiv:2106.04215v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04215</id>
        <link href="http://arxiv.org/abs/2106.04215"/>
        <updated>2021-06-09T02:01:50.198Z</updated>
        <summary type="html"><![CDATA[The availability of large-scale face datasets has been key in the progress of
face recognition. However, due to licensing issues or copyright infringement,
some datasets are not available anymore (e.g. MS-Celeb-1M). Recent advances in
Generative Adversarial Networks (GANs), to synthesize realistic face images,
provide a pathway to replace real datasets by synthetic datasets, both to train
and benchmark face recognition (FR) systems. The work presented in this paper
provides a study on benchmarking FR systems using a synthetic dataset. First,
we introduce the proposed methodology to generate a synthetic dataset, without
the need for human intervention, by exploiting the latent structure of a
StyleGAN2 model with multiple controlled factors of variation. Then, we confirm
that (i) the generated synthetic identities are not data subjects from the
GAN's training dataset, which is verified on a synthetic dataset with 10K+
identities; (ii) benchmarking results on the synthetic dataset are a good
substitution, often providing error rates and system ranking similar to the
benchmarking on the real dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colbois_L/0/1/0/all/0/1"&gt;Laurent Colbois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_T/0/1/0/all/0/1"&gt;Tiago de Freitas Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Marcel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Baselines: Benchmarks for Uncertainty & Robustness in Deep Learning. (arXiv:2106.04015v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04015</id>
        <link href="http://arxiv.org/abs/2106.04015"/>
        <updated>2021-06-09T02:01:50.198Z</updated>
        <summary type="html"><![CDATA[High-quality estimates of uncertainty and robustness are crucial for numerous
real-world applications, especially for deep learning which underlies many
deployed ML systems. The ability to compare techniques for improving these
estimates is therefore very important for research and practice alike. Yet,
competitive comparisons of methods are often lacking due to a range of reasons,
including: compute availability for extensive tuning, incorporation of
sufficiently many baselines, and concrete documentation for reproducibility. In
this paper we introduce Uncertainty Baselines: high-quality implementations of
standard and state-of-the-art deep learning methods on a variety of tasks. As
of this writing, the collection spans 19 methods across 9 tasks, each with at
least 5 metrics. Each baseline is a self-contained experiment pipeline with
easily reusable and extendable components. Our goal is to provide immediate
starting points for experimentation with new methods or applications.
Additionally we provide model checkpoints, experiment outputs as Python
notebooks, and leaderboards for comparing results. Code available at
https://github.com/google/uncertainty-baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nado_Z/0/1/0/all/0/1"&gt;Zachary Nado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1"&gt;Neil Band&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collier_M/0/1/0/all/0/1"&gt;Mark Collier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1"&gt;Josip Djolonga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dusenberry_M/0/1/0/all/0/1"&gt;Michael W. Dusenberry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1"&gt;Sebastian Farquhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filos_A/0/1/0/all/0/1"&gt;Angelos Filos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1"&gt;Marton Havasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jerfel_G/0/1/0/all/0/1"&gt;Ghassen Jerfel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mariet_Z/0/1/0/all/0/1"&gt;Zelda Mariet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nixon_J/0/1/0/all/0/1"&gt;Jeremy Nixon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padhy_S/0/1/0/all/0/1"&gt;Shreyas Padhy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1"&gt;Tim G. J. Rudner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yeming Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1"&gt;Florian Wenzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1"&gt;Kevin Murphy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sculley_D/0/1/0/all/0/1"&gt;D. Sculley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1"&gt;Jasper Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph Clustering. (arXiv:2105.05320v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05320</id>
        <link href="http://arxiv.org/abs/2105.05320"/>
        <updated>2021-06-09T02:01:50.198Z</updated>
        <summary type="html"><![CDATA[Recently, there has been considerable research interest in graph clustering
aimed at data partition using the graph information. However, one limitation of
the most of graph-based methods is that they assume the graph structure to
operate is fixed and reliable. And there are inevitably some edges in the graph
that are not conducive to graph clustering, which we call spurious edges. This
paper is the first attempt to employ graph pooling technique for node
clustering and we propose a novel dual graph embedding network (DGEN), which is
designed as a two-step graph encoder connected by a graph pooling layer to
learn the graph embedding. In our model, it is assumed that if a node and its
nearest neighboring node are close to the same clustering center, this node is
an informative node and this edge can be considered as a cluster-friendly edge.
Based on this assumption, the neighbor cluster pooling (NCPool) is devised to
select the most informative subset of nodes and the corresponding edges based
on the distance of nodes and their nearest neighbors to the cluster centers.
This can effectively alleviate the impact of the spurious edges on the
clustering. Finally, to obtain the clustering assignment of all nodes, a
classifier is trained using the clustering results of the selected nodes.
Experiments on five benchmark graph datasets demonstrate the superiority of the
proposed method over state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Dongxia Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhiqian Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotating spiders and reflecting dogs: a class conditional approach to learning data augmentation distributions. (arXiv:2106.04009v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04009</id>
        <link href="http://arxiv.org/abs/2106.04009"/>
        <updated>2021-06-09T02:01:50.197Z</updated>
        <summary type="html"><![CDATA[Building invariance to non-meaningful transformations is essential to
building efficient and generalizable machine learning models. In practice, the
most common way to learn invariance is through data augmentation. There has
been recent interest in the development of methods that learn distributions on
augmentation transformations from the training data itself. While such
approaches are beneficial since they are responsive to the data, they ignore
the fact that in many situations the range of transformations to which a model
needs to be invariant changes depending on the particular class input belongs
to. For example, if a model needs to be able to predict whether an image
contains a starfish or a dog, we may want to apply random rotations to starfish
images during training (since these do not have a preferred orientation), but
we would not want to do this to images of dogs. In this work we introduce a
method by which we can learn class conditional distributions on augmentation
transformations. We give a number of examples where our methods learn different
non-meaningful transformations depending on class and further show how our
method can be used as a tool to probe the symmetries intrinsic to a potentially
complex dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahan_S/0/1/0/all/0/1"&gt;Scott Mahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doster_T/0/1/0/all/0/1"&gt;Tim Doster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LEADS: Learning Dynamical Systems that Generalize Across Environments. (arXiv:2106.04546v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04546</id>
        <link href="http://arxiv.org/abs/2106.04546"/>
        <updated>2021-06-09T02:01:50.197Z</updated>
        <summary type="html"><![CDATA[When modeling dynamical systems from real-world data samples, the
distribution of data often changes according to the environment in which they
are captured, and the dynamics of the system itself vary from one environment
to another. Generalizing across environments thus challenges the conventional
frameworks. The classical settings suggest either considering data as i.i.d.
and learning a single model to cover all situations or learning
environment-specific models. Both are sub-optimal: the former disregards the
discrepancies between environments leading to biased solutions, while the
latter does not exploit their potential commonalities and is prone to scarcity
problems. We propose LEADS, a novel framework that leverages the commonalities
and discrepancies among known environments to improve model generalization.
This is achieved with a tailored training formulation aiming at capturing
common dynamics within a shared model while additional terms capture
environment-specific dynamics. We ground our approach in theory, exhibiting a
decrease in sample complexity with our approach and corroborate these results
empirically, instantiating it for linear dynamics. Moreover, we concretize this
framework for neural networks and evaluate it experimentally on representative
families of nonlinear dynamics. We show that this new setting can exploit
knowledge extracted from environment-dependent data and improves generalization
for both known and novel environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yuan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ibrahim Ayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bezenac_E/0/1/0/all/0/1"&gt;Emmanuel de B&amp;#xe9;zenac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baskiotis_N/0/1/0/all/0/1"&gt;Nicolas Baskiotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Directional Bias Amplification. (arXiv:2102.12594v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12594</id>
        <link href="http://arxiv.org/abs/2102.12594"/>
        <updated>2021-06-09T02:01:50.197Z</updated>
        <summary type="html"><![CDATA[Mitigating bias in machine learning systems requires refining our
understanding of bias propagation pathways: from societal structures to
large-scale data to trained models to impact on society. In this work, we focus
on one aspect of the problem, namely bias amplification: the tendency of models
to amplify the biases present in the data they are trained on. A metric for
measuring bias amplification was introduced in the seminal work by Zhao et al.
(2017); however, as we demonstrate, this metric suffers from a number of
shortcomings including conflating different types of bias amplification and
failing to account for varying base rates of protected attributes. We introduce
and analyze a new, decoupled metric for measuring bias amplification,
$\text{BiasAmp}_{\rightarrow}$ (Directional Bias Amplification). We thoroughly
analyze and discuss both the technical assumptions and normative implications
of this metric. We provide suggestions about its measurement by cautioning
against predicting sensitive attributes, encouraging the use of confidence
intervals due to fluctuations in the fairness of models across runs, and
discussing the limitations of what this metric captures. Throughout this paper,
we work to provide an interrogative look at the technical measurement of bias
amplification, guided by our normative ideas of what we want it to encompass.
Code is located at https://github.com/princetonvisualai/directional-bias-amp]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Angelina Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1"&gt;Olga Russakovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Sparse Training for Deep Reinforcement Learning. (arXiv:2106.04217v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04217</id>
        <link href="http://arxiv.org/abs/2106.04217"/>
        <updated>2021-06-09T02:01:50.196Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning has achieved significant success in many
decision-making tasks in various fields. However, it requires a large training
time of dense neural networks to obtain a good performance. This hinders its
applicability on low-resource devices where memory and computation are strictly
constrained. In a step towards enabling deep reinforcement learning agents to
be applied to low-resource devices, in this work, we propose for the first time
to dynamically train deep reinforcement learning agents with sparse neural
networks from scratch. We adopt the evolution principles of dynamic sparse
training in the reinforcement learning paradigm and introduce a training
algorithm that optimizes the sparse topology and the weight values jointly to
dynamically fit the incoming data. Our approach is easy to be integrated into
existing deep reinforcement learning algorithms and has many favorable
advantages. First, it allows for significant compression of the network size
which reduces the memory and computation costs substantially. This would
accelerate not only the agent inference but also its training process. Second,
it speeds up the agent learning process and allows for reducing the number of
required training steps. Third, it can achieve higher performance than training
the dense counterpart network. We evaluate our approach on OpenAI gym
continuous control tasks. The experimental results show the effectiveness of
our approach in achieving higher performance than one of the state-of-art
baselines with a 50\% reduction in the network size and floating-point
operations (FLOPs). Moreover, our proposed approach can reach the same
performance achieved by the dense network with a 40-50\% reduction in the
number of training steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sokar_G/0/1/0/all/0/1"&gt;Ghada Sokar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_E/0/1/0/all/0/1"&gt;Elena Mocanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1"&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1"&gt;Peter Stone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HPRNet: Hierarchical Point Regression for Whole-Body Human Pose Estimation. (arXiv:2106.04269v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04269</id>
        <link href="http://arxiv.org/abs/2106.04269"/>
        <updated>2021-06-09T02:01:50.195Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a new bottom-up one-stage method for whole-body
pose estimation, which we name "hierarchical point regression," or HPRNet for
short, referring to the network that implements this method. To handle the
scale variance among different body parts, we build a hierarchical point
representation of body parts and jointly regress them. Unlike the existing
two-stage methods, our method predicts whole-body pose in a constant time
independent of the number of people in an image. On the COCO WholeBody dataset,
HPRNet significantly outperforms all previous bottom-up methods on the keypoint
detection of all whole-body parts (i.e. body, foot, face and hand); it also
achieves state-of-the-art results in the face (75.4 AP) and hand (50.4 AP)
keypoint detection. Code and models are available at
https://github.com/nerminsamet/HPRNet.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samet_N/0/1/0/all/0/1"&gt;Nermin Samet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1"&gt;Emre Akbas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Graph-level Representation Learning with Local and Global Structure. (arXiv:2106.04113v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04113</id>
        <link href="http://arxiv.org/abs/2106.04113"/>
        <updated>2021-06-09T02:01:50.195Z</updated>
        <summary type="html"><![CDATA[This paper studies unsupervised/self-supervised whole-graph representation
learning, which is critical in many tasks such as molecule properties
prediction in drug and material discovery. Existing methods mainly focus on
preserving the local similarity structure between different graph instances but
fail to discover the global semantic structure of the entire data set. In this
paper, we propose a unified framework called Local-instance and Global-semantic
Learning (GraphLoG) for self-supervised whole-graph representation learning.
Specifically, besides preserving the local similarities, GraphLoG introduces
the hierarchical prototypes to capture the global semantic clusters. An
efficient online expectation-maximization (EM) algorithm is further developed
for learning the model. We evaluate GraphLoG by pre-training it on massive
unlabeled graphs followed by fine-tuning on downstream tasks. Extensive
experiments on both chemical and biological benchmark data sets demonstrate the
effectiveness of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minghao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time-series Imputation of Temporally-occluded Multiagent Trajectories. (arXiv:2106.04219v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04219</id>
        <link href="http://arxiv.org/abs/2106.04219"/>
        <updated>2021-06-09T02:01:50.193Z</updated>
        <summary type="html"><![CDATA[In multiagent environments, several decision-making individuals interact
while adhering to the dynamics constraints imposed by the environment. These
interactions, combined with the potential stochasticity of the agents'
decision-making processes, make such systems complex and interesting to study
from a dynamical perspective. Significant research has been conducted on
learning models for forward-direction estimation of agent behaviors, for
example, pedestrian predictions used for collision-avoidance in self-driving
cars. However, in many settings, only sporadic observations of agents may be
available in a given trajectory sequence. For instance, in football, subsets of
players may come in and out of view of broadcast video footage, while
unobserved players continue to interact off-screen. In this paper, we study the
problem of multiagent time-series imputation, where available past and future
observations of subsets of agents are used to estimate missing observations for
other agents. Our approach, called the Graph Imputer, uses forward- and
backward-information in combination with graph networks and variational
autoencoders to enable learning of a distribution of imputed trajectories. We
evaluate our approach on a dataset of football matches, using a projective
camera module to train and evaluate our model for the off-screen player state
estimation setting. We illustrate that our method outperforms several
state-of-the-art approaches, including those hand-crafted for football.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Omidshafiei_S/0/1/0/all/0/1"&gt;Shayegan Omidshafiei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennes_D/0/1/0/all/0/1"&gt;Daniel Hennes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garnelo_M/0/1/0/all/0/1"&gt;Marta Garnelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarassov_E/0/1/0/all/0/1"&gt;Eugene Tarassov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elie_R/0/1/0/all/0/1"&gt;Romuald Elie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Connor_J/0/1/0/all/0/1"&gt;Jerome T. Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1"&gt;Paul Muller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graham_I/0/1/0/all/0/1"&gt;Ian Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spearman_W/0/1/0/all/0/1"&gt;William Spearman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuyls_K/0/1/0/all/0/1"&gt;Karl Tuyls&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Mixture Density Networks. (arXiv:2012.03085v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03085</id>
        <link href="http://arxiv.org/abs/2012.03085"/>
        <updated>2021-06-09T02:01:50.193Z</updated>
        <summary type="html"><![CDATA[We introduce the Graph Mixture Density Networks, a new family of machine
learning models that can fit multimodal output distributions conditioned on
graphs of arbitrary topology. By combining ideas from mixture models and graph
representation learning, we address a broader class of challenging conditional
density estimation problems that rely on structured data. In this respect, we
evaluate our method on a new benchmark application that leverages random graphs
for stochastic epidemic simulations. We show a significant improvement in the
likelihood of epidemic outcomes when taking into account both multimodality and
structure. The empirical analysis is complemented by two real-world regression
tasks showing the effectiveness of our approach in modeling the output
prediction uncertainty. Graph Mixture Density Networks open appealing research
opportunities in the study of structure-dependent phenomena that exhibit
non-trivial conditional output distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Errica_F/0/1/0/all/0/1"&gt;Federico Errica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1"&gt;Alessio Micheli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XIRL: Cross-embodiment Inverse Reinforcement Learning. (arXiv:2106.03911v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.03911</id>
        <link href="http://arxiv.org/abs/2106.03911"/>
        <updated>2021-06-09T02:01:50.192Z</updated>
        <summary type="html"><![CDATA[We investigate the visual cross-embodiment imitation setting, in which agents
learn policies from videos of other agents (such as humans) demonstrating the
same task, but with stark differences in their embodiments -- shape, actions,
end-effector dynamics, etc. In this work, we demonstrate that it is possible to
automatically discover and learn vision-based reward functions from
cross-embodiment demonstration videos that are robust to these differences.
Specifically, we present a self-supervised method for Cross-embodiment Inverse
Reinforcement Learning (XIRL) that leverages temporal cycle-consistency
constraints to learn deep visual embeddings that capture task progression from
offline videos of demonstrations across multiple expert agents, each performing
the same task differently due to embodiment differences. Prior to our work,
producing rewards from self-supervised embeddings has typically required
alignment with a reference trajectory, which may be difficult to acquire. We
show empirically that if the embeddings are aware of task-progress, simply
taking the negative distance between the current state and goal state in the
learned embedding space is useful as a reward for training policies with
reinforcement learning. We find our learned reward function not only works for
embodiments seen during training, but also generalizes to entirely new
embodiments. We also find that XIRL policies are more sample efficient than
baselines, and in some cases exceed the sample efficiency of the same agent
trained with ground truth sparse rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zakka_K/0/1/0/all/0/1"&gt;Kevin Zakka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Andy Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1"&gt;Pete Florence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1"&gt;Jonathan Tompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1"&gt;Jeannette Bohg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwibedi_D/0/1/0/all/0/1"&gt;Debidatta Dwibedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BIGDML: Towards Exact Machine Learning Force Fields for Materials. (arXiv:2106.04229v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2106.04229</id>
        <link href="http://arxiv.org/abs/2106.04229"/>
        <updated>2021-06-09T02:01:50.192Z</updated>
        <summary type="html"><![CDATA[Machine-learning force fields (MLFF) should be accurate, computationally and
data efficient, and applicable to molecules, materials, and interfaces thereof.
Currently, MLFFs often introduce tradeoffs that restrict their practical
applicability to small subsets of chemical space or require exhaustive datasets
for training. Here, we introduce the Bravais-Inspired Gradient-Domain Machine
Learning (BIGDML) approach and demonstrate its ability to construct reliable
force fields using a training set with just 10-200 geometries for materials
including pristine and defect-containing 2D and 3D semiconductors and metals,
as well as chemisorbed and physisorbed atomic and molecular adsorbates on
surfaces. The BIGDML model employs the full relevant symmetry group for a given
material, does not assume artificial atom types or localization of atomic
interactions and exhibits high data efficiency and state-of-the-art energy
accuracies (errors substantially below 1 meV per atom) for an extended set of
materials. Extensive path-integral molecular dynamics carried out with BIGDML
models demonstrate the counterintuitive localization of benzene--graphene
dynamics induced by nuclear quantum effects and allow to rationalize the
Arrhenius behavior of hydrogen diffusion coefficient in a Pd crystal for a wide
range of temperatures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sauceda_H/0/1/0/all/0/1"&gt;Huziel E. Sauceda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Galvez_Gonzalez_L/0/1/0/all/0/1"&gt;Luis E. G&amp;#xe1;lvez-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Chmiela_S/0/1/0/all/0/1"&gt;Stefan Chmiela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Paz_Borbon_L/0/1/0/all/0/1"&gt;Lauro Oliver Paz-Borb&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Tkatchenko_A/0/1/0/all/0/1"&gt;Alexandre Tkatchenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Robust Detection of Out-of-distribution Data (almost) for free. (arXiv:2106.04260v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04260</id>
        <link href="http://arxiv.org/abs/2106.04260"/>
        <updated>2021-06-09T02:01:50.192Z</updated>
        <summary type="html"><![CDATA[When applying machine learning in safety-critical systems, a reliable
assessment of the uncertainy of a classifier is required. However, deep neural
networks are known to produce highly overconfident predictions on
out-of-distribution (OOD) data and even if trained to be non-confident on OOD
data one can still adversarially manipulate OOD data so that the classifer
again assigns high confidence to the manipulated samples. In this paper we
propose a novel method where from first principles we combine a certifiable OOD
detector with a standard classifier into an OOD aware classifier. In this way
we achieve the best of two worlds: certifiably adversarially robust OOD
detection, even for OOD samples close to the in-distribution, without loss in
prediction accuracy and close to state-of-the-art OOD detection performance for
non-manipulated OOD data. Moreover, due to the particular construction our
classifier provably avoids the asymptotic overconfidence problem of standard
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meinke_A/0/1/0/all/0/1"&gt;Alexander Meinke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bitterwolf_J/0/1/0/all/0/1"&gt;Julian Bitterwolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Sampling in POMDPs with Lipschitz Bandits for Motion Planning in Continuous Spaces. (arXiv:2106.04206v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04206</id>
        <link href="http://arxiv.org/abs/2106.04206"/>
        <updated>2021-06-09T02:01:50.191Z</updated>
        <summary type="html"><![CDATA[Decision making under uncertainty can be framed as a partially observable
Markov decision process (POMDP). Finding exact solutions of POMDPs is generally
computationally intractable, but the solution can be approximated by
sampling-based approaches. These sampling-based POMDP solvers rely on
multi-armed bandit (MAB) heuristics, which assume the outcomes of different
actions to be uncorrelated. In some applications, like motion planning in
continuous spaces, similar actions yield similar outcomes. In this paper, we
utilize variants of MAB heuristics that make Lipschitz continuity assumptions
on the outcomes of actions to improve the efficiency of sampling-based planning
approaches. We demonstrate the effectiveness of this approach in the context of
motion planning for automated driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tas_O/0/1/0/all/0/1"&gt;&amp;#xd6;mer &amp;#x15e;ahin Ta&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauser_F/0/1/0/all/0/1"&gt;Felix Hauser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lauer_M/0/1/0/all/0/1"&gt;Martin Lauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Byakto Speech: Real-time long speech synthesis with convolutional neural network: Transfer learning from English to Bangla. (arXiv:2106.03937v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.03937</id>
        <link href="http://arxiv.org/abs/2106.03937"/>
        <updated>2021-06-09T02:01:50.184Z</updated>
        <summary type="html"><![CDATA[Speech synthesis is one of the challenging tasks to automate by deep
learning, also being a low-resource language there are very few attempts at
Bangla speech synthesis. Most of the existing works can't work with anything
other than simple Bangla characters script, very short sentences, etc. This
work attempts to solve these problems by introducing Byakta, the first-ever
open-source deep learning-based bilingual (Bangla and English) text to a speech
synthesis system. A speech recognition model-based automated scoring metric was
also proposed to evaluate the performance of a TTS model. We also introduce a
test benchmark dataset for Bangla speech synthesis models for evaluating speech
quality. The TTS is available at https://github.com/zabir-nabil/bangla-tts]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nazi_Z/0/1/0/all/0/1"&gt;Zabir Al Nazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huda_S/0/1/0/all/0/1"&gt;Sayed Mohammed Tasmimul Huda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal Transformer: Complex-valued Attention and Meta-Learning for Signal Recognition. (arXiv:2106.04392v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04392</id>
        <link href="http://arxiv.org/abs/2106.04392"/>
        <updated>2021-06-09T02:01:50.184Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been shown as a class of useful tools for
addressing signal recognition issues in recent years, especially for
identifying the nonlinear feature structures of signals. However, this power of
most deep learning techniques heavily relies on an abundant amount of training
data, so the performance of classic neural nets decreases sharply when the
number of training data samples is small or unseen data are presented in the
testing phase. This calls for an advanced strategy, i.e., model-agnostic
meta-learning (MAML), which is able to capture the invariant representation of
the data samples or signals. In this paper, inspired by the special structure
of the signal, i.e., real and imaginary parts consisted in practical
time-series signals, we propose a Complex-valued Attentional MEta Learner
(CAMEL) for the problem of few-shot signal recognition by leveraging attention
and meta-learning in the complex domain. To the best of our knowledge, this is
also the first complex-valued MAML that can find the first-order stationary
points of general nonconvex problems with theoretical convergence guarantees.
Extensive experiments results showcase the superiority of the proposed CAMEL
compared with the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yihong Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Ying Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Songtao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1"&gt;Qingjiang Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MindReader: Recommendation over Knowledge Graph Entities with Explicit User Ratings. (arXiv:2106.04209v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04209</id>
        <link href="http://arxiv.org/abs/2106.04209"/>
        <updated>2021-06-09T02:01:50.183Z</updated>
        <summary type="html"><![CDATA[Knowledge Graphs (KGs) have been integrated in several models of
recommendation to augment the informational value of an item by means of its
related entities in the graph. Yet, existing datasets only provide explicit
ratings on items and no information is provided about user opinions of other
(non-recommendable) entities. To overcome this limitation, we introduce a new
dataset, called the MindReader, providing explicit user ratings both for items
and for KG entities. In this first version, the MindReader dataset provides
more than 102 thousands explicit ratings collected from 1,174 real users on
both items and entities from a KG in the movie domain. This dataset has been
collected through an online interview application that we also release open
source. As a demonstration of the importance of this new dataset, we present a
comparative study of the effect of the inclusion of ratings on non-item KG
entities in a variety of state-of-the-art recommendation models. In particular,
we show that most models, whether designed specifically for graph data or not,
see improvements in recommendation quality when trained on explicit non-item
ratings. Moreover, for some models, we show that non-item ratings can
effectively replace item ratings without loss of recommendation quality. This
finding, thanks also to an observed greater familiarity of users towards common
KG entities than towards long-tail items, motivates the use of KG entities for
both warm and cold-start recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brams_A/0/1/0/all/0/1"&gt;Anders H. Brams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakobsen_A/0/1/0/all/0/1"&gt;Anders L. Jakobsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jendal_T/0/1/0/all/0/1"&gt;Theis E. Jendal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lissandrini_M/0/1/0/all/0/1"&gt;Matteo Lissandrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolog_P/0/1/0/all/0/1"&gt;Peter Dolog&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hose_K/0/1/0/all/0/1"&gt;Katja Hose&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning. (arXiv:2008.03606v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03606</id>
        <link href="http://arxiv.org/abs/2008.03606"/>
        <updated>2021-06-09T02:01:50.183Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a challenging setting for optimization due to the
heterogeneity of the data across different clients which gives rise to the
client drift phenomenon. In fact, obtaining an algorithm for FL which is
uniformly better than simple centralized training has been a major open problem
thus far. In this work, we propose a general algorithmic framework, Mime, which
i) mitigates client drift and ii) adapts arbitrary centralized optimization
algorithms such as momentum and Adam to the cross-device federated learning
setting. Mime uses a combination of control-variates and server-level
statistics (e.g. momentum) at every client-update step to ensure that each
local update mimics that of the centralized method run on iid data. We prove a
reduction result showing that Mime can translate the convergence of a generic
algorithm in the centralized setting into convergence in the federated setting.
Further, we show that when combined with momentum based variance reduction,
Mime is provably faster than any centralized method--the first such result. We
also perform a thorough experimental exploration of Mime's performance on real
world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1"&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1"&gt;Satyen Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1"&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1"&gt;Sashank J. Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ananda Theertha Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation. (arXiv:2106.04403v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04403</id>
        <link href="http://arxiv.org/abs/2106.04403"/>
        <updated>2021-06-09T02:01:50.182Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have brought significant progress in visual
grounding tasks such as language-guided video object segmentation. However,
collecting large datasets for these tasks is expensive in terms of annotation
time, which represents a bottleneck. To this end, we propose a novel method,
namely SynthRef, for generating synthetic referring expressions for target
objects in an image (or video frame), and we also present and disseminate the
first large-scale dataset with synthetic referring expressions for video object
segmentation. Our experiments demonstrate that by training with our synthetic
referring expressions one can improve the ability of a model to generalize
across different datasets, without any additional annotation cost. Moreover,
our formulation allows its application to any object detection or segmentation
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazakos_I/0/1/0/all/0/1"&gt;Ioannis Kazakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_C/0/1/0/all/0/1"&gt;Carles Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellver_M/0/1/0/all/0/1"&gt;Miriam Bellver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1"&gt;Carina Silberer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Giro-i-Nieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breaking the Limits of Message Passing Graph Neural Networks. (arXiv:2106.04319v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04319</id>
        <link href="http://arxiv.org/abs/2106.04319"/>
        <updated>2021-06-09T02:01:50.182Z</updated>
        <summary type="html"><![CDATA[Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear
complexity with respect to the number of nodes when applied to sparse graphs,
they have been widely implemented and still raise a lot of interest even though
their theoretical expressive power is limited to the first order
Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph
convolution supports are designed in spectral-domain by a non-linear custom
function of eigenvalues and masked with an arbitrary large receptive field, the
MPNN is theoretically more powerful than the 1-WL test and experimentally as
powerful as a 3-WL existing models, while remaining spatially localized.
Moreover, by designing custom filter functions, outputs can have various
frequency components that allow the convolution process to learn different
relationships between a given input graph signal and its associated properties.
So far, the best 3-WL equivalent graph neural networks have a computational
complexity in $\mathcal{O}(n^3)$ with memory usage in $\mathcal{O}(n^2)$,
consider non-local update mechanism and do not provide the spectral richness of
output profile. The proposed method overcomes all these aforementioned problems
and reaches state-of-the-art results in many downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balcilar_M/0/1/0/all/0/1"&gt;Muhammet Balcilar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heroux_P/0/1/0/all/0/1"&gt;Pierre H&amp;#xe9;roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gauzere_B/0/1/0/all/0/1"&gt;Benoit Ga&amp;#xfc;z&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasseur_P/0/1/0/all/0/1"&gt;Pascal Vasseur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honeine_P/0/1/0/all/0/1"&gt;Paul Honeine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Hierarchical Learning Based Network Traffic Analytics. (arXiv:2106.03850v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03850</id>
        <link href="http://arxiv.org/abs/2106.03850"/>
        <updated>2021-06-09T02:01:50.181Z</updated>
        <summary type="html"><![CDATA[Classifying network traffic is the basis for important network applications.
Prior research in this area has faced challenges on the availability of
representative datasets, and many of the results cannot be readily reproduced.
Such a problem is exacerbated by emerging data-driven machine learning based
approaches. To address this issue, we present(N et)2databasewith three open
datasets containing nearly 1.3M labeled flows in total, with a comprehensive
list of flow features, for there search community1. We focus on broad aspects
in network traffic analysis, including both malware detection and application
classification. As we continue to grow them, we expect the datasets to serve as
a common ground for AI driven, reproducible research on network flow analytics.
We release the datasets publicly and also introduce a Multi-Task Hierarchical
Learning (MTHL)model to perform all tasks in a single model. Our results show
that MTHL is capable of accurately performing multiple tasks with hierarchical
labeling with a dramatic reduction in training time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barut_O/0/1/0/all/0/1"&gt;Onur Barut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weigang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peilong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Out-of-Distribution Detection with Mixup Outlier Exposure. (arXiv:2106.03917v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03917</id>
        <link href="http://arxiv.org/abs/2106.03917"/>
        <updated>2021-06-09T02:01:50.181Z</updated>
        <summary type="html"><![CDATA[Enabling out-of-distribution (OOD) detection for DNNs is critical for their
safe and reliable operation in the "open world". Unfortunately, current works
in both methodology and evaluation focus on rather contrived detection
problems, and only consider a coarse level of granularity w.r.t.: 1) the
in-distribution (ID) classes, and 2) the OOD data's "closeness" to the ID data.
We posit that such settings may be poor approximations of many real-world tasks
that are naturally fine-grained (e.g., bird species classification), and thus
the reported detection abilities may be over-estimates. Differently, in this
work we make granularity a top priority and focus on fine-grained OOD
detection. We start by carefully constructing five novel fine-grained test
environments in which existing methods are shown to have difficulties. We then
propose a new DNN training algorithm, Mixup Outlier Exposure (MixupOE), which
leverages an outlier distribution and principles from vicinal risk
minimization. Finally, we perform extensive experiments and analyses in our
custom test environments and demonstrate that MixupOE can consistently improve
fine-grained detection performance, establishing a strong baseline in these
more realistic and challenging OOD detection settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1"&gt;Nathan Inkawhich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiran Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hai Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Targeted Active Learning for Bayesian Decision-Making. (arXiv:2106.04193v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04193</id>
        <link href="http://arxiv.org/abs/2106.04193"/>
        <updated>2021-06-09T02:01:50.180Z</updated>
        <summary type="html"><![CDATA[Active learning is usually applied to acquire labels of informative data
points in supervised learning, to maximize accuracy in a sample-efficient way.
However, maximizing the accuracy is not the end goal when the results are used
for decision-making, for example in personalized medicine or economics. We
argue that when acquiring samples sequentially, separating learning and
decision-making is sub-optimal, and we introduce a novel active learning
strategy which takes the down-the-line decision problem into account.
Specifically, we introduce a novel active learning criterion which maximizes
the expected information gain on the posterior distribution of the optimal
decision. We compare our decision-making-aware active learning strategy to
existing alternatives on both simulated and real data, and show improved
performance in decision-making accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Filstroff_L/0/1/0/all/0/1"&gt;Louis Filstroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sundin_I/0/1/0/all/0/1"&gt;Iiris Sundin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mikkola_P/0/1/0/all/0/1"&gt;Petrus Mikkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tiulpin_A/0/1/0/all/0/1"&gt;Aleksei Tiulpin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kylmaoja_J/0/1/0/all/0/1"&gt;Juuso Kylm&amp;#xe4;oja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TENGraD: Time-Efficient Natural Gradient Descent with Exact Fisher-Block Inversion. (arXiv:2106.03947v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03947</id>
        <link href="http://arxiv.org/abs/2106.03947"/>
        <updated>2021-06-09T02:01:50.177Z</updated>
        <summary type="html"><![CDATA[This work proposes a time-efficient Natural Gradient Descent method, called
TENGraD, with linear convergence guarantees. Computing the inverse of the
neural network's Fisher information matrix is expensive in NGD because the
Fisher matrix is large. Approximate NGD methods such as KFAC attempt to improve
NGD's running time and practical application by reducing the Fisher matrix
inversion cost with approximation. However, the approximations do not reduce
the overall time significantly and lead to less accurate parameter updates and
loss of curvature information. TENGraD improves the time efficiency of NGD by
computing Fisher block inverses with a computationally efficient covariance
factorization and reuse method. It computes the inverse of each block exactly
using the Woodbury matrix identity to preserve curvature information while
admitting (linear) fast convergence rates. Our experiments on image
classification tasks for state-of-the-art deep neural architecture on CIFAR-10,
CIFAR-100, and Fashion-MNIST show that TENGraD significantly outperforms
state-of-the-art NGD methods and often stochastic gradient descent in
wall-clock time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soori_S/0/1/0/all/0/1"&gt;Saeed Soori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Can_B/0/1/0/all/0/1"&gt;Bugra Can&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_B/0/1/0/all/0/1"&gt;Baourun Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert G&amp;#xfc;rb&amp;#xfc;zbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehnavi_M/0/1/0/all/0/1"&gt;Maryam Mehri Dehnavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Vehicles Trajectories in Urban Scenarios with Transformer Networks and Augmented Information. (arXiv:2106.00559v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00559</id>
        <link href="http://arxiv.org/abs/2106.00559"/>
        <updated>2021-06-09T02:01:50.176Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior of road users is of vital importance for the
development of trajectory prediction systems. In this context, the latest
advances have focused on recurrent structures, establishing the social
interaction between the agents involved in the scene. More recently, simpler
structures have also been introduced for predicting pedestrian trajectories,
based on Transformer Networks, and using positional information. They allow the
individual modelling of each agent's trajectory separately without any complex
interaction terms. Our model exploits these simple structures by adding
augmented data (position and heading), and adapting their use to the problem of
vehicle trajectory prediction in urban scenarios in prediction horizons up to 5
seconds. In addition, a cross-performance analysis is performed between
different types of scenarios, including highways, intersections and
roundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our
model achieves state-of-the-art results and proves to be flexible and adaptable
to different types of urban contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quintanar_A/0/1/0/all/0/1"&gt;A. Quintanar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Llorca_D/0/1/0/all/0/1"&gt;D. Fern&amp;#xe1;ndez-Llorca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parra_I/0/1/0/all/0/1"&gt;I. Parra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Izquierdo_R/0/1/0/all/0/1"&gt;R. Izquierdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotelo_M/0/1/0/all/0/1"&gt;M. A. Sotelo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Theoretical Framework of Out-of-Distribution Generalization. (arXiv:2106.04496v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04496</id>
        <link href="http://arxiv.org/abs/2106.04496"/>
        <updated>2021-06-09T02:01:50.176Z</updated>
        <summary type="html"><![CDATA[Generalization to out-of-distribution (OOD) data, or domain generalization,
is one of the central problems in modern machine learning. Recently, there is a
surge of attempts to propose algorithms for OOD that mainly build upon the idea
of extracting invariant features. Although intuitively reasonable, theoretical
understanding of what kind of invariance can guarantee OOD generalization is
still limited, and generalization to arbitrary out-of-distribution is clearly
impossible. In this work, we take the first step towards rigorous and
quantitative definitions of 1) what is OOD; and 2) what does it mean by saying
an OOD problem is learnable. We also introduce a new concept of expansion
function, which characterizes to what extent the variance is amplified in the
test domains over the training domains, and therefore give a quantitative
meaning of invariant features. Based on these, we prove OOD generalization
error bounds. It turns out that OOD generalization largely depends on the
expansion function. As recently pointed out by Gulrajani and Lopez-Paz (2020),
any OOD learning algorithm without a model selection module is incomplete. Our
theory naturally induces a model selection criterion. Extensive experiments on
benchmark OOD datasets demonstrate that our model selection criterion has a
significant advantage over baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Haotian Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chuanlong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Spatio-Temporal Bilinear Network with Monte Carlo Dropout for Landmark-based Facial Expression Recognition with Uncertainty Estimation. (arXiv:2106.04332v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04332</id>
        <link href="http://arxiv.org/abs/2106.04332"/>
        <updated>2021-06-09T02:01:50.173Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been widely used for feature learning in facial
expression recognition systems. However, small datasets and large intra-class
variability can lead to overfitting. In this paper, we propose a method which
learns an optimized compact network topology for real-time facial expression
recognition utilizing localized facial landmark features. Our method employs a
spatio-temporal bilinear layer as backbone to capture the motion of facial
landmarks during the execution of a facial expression effectively. Besides, it
takes advantage of Monte Carlo Dropout to capture the model's uncertainty which
is of great importance to analyze and treat uncertain cases. The performance of
our method is evaluated on three widely used datasets and it is comparable to
that of video-based state-of-the-art methods while it has much less complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidari_N/0/1/0/all/0/1"&gt;Negar Heidari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coresets for Classification -- Simplified and Strengthened. (arXiv:2106.04254v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04254</id>
        <link href="http://arxiv.org/abs/2106.04254"/>
        <updated>2021-06-09T02:01:50.173Z</updated>
        <summary type="html"><![CDATA[We give relative error coresets for training linear classifiers with a broad
class of loss functions, including the logistic loss and hinge loss. Our
construction achieves $(1\pm \epsilon)$ relative error with $\tilde O(d \cdot
\mu_y(X)^2/\epsilon^2)$ points, where $\mu_y(X)$ is a natural complexity
measure of the data matrix $X \in \mathbb{R}^{n \times d}$ and label vector $y
\in \{-1,1\}^n$, introduced in by Munteanu et al. 2018. Our result is based on
subsampling data points with probabilities proportional to their $\ell_1$
$Lewis$ $weights$. It significantly improves on existing theoretical bounds and
performs well in practice, outperforming uniform subsampling along with other
importance sampling methods. Our sampling distribution does not depend on the
labels, so can be used for active learning. It also does not depend on the
specific loss function, so a single coreset can be used in multiple training
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1"&gt;Tung Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1"&gt;Anup B. Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Cameron Musco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Representation Learning for Hand Shape Estimation. (arXiv:2106.04324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04324</id>
        <link href="http://arxiv.org/abs/2106.04324"/>
        <updated>2021-06-09T02:01:50.172Z</updated>
        <summary type="html"><![CDATA[This work presents improvements in monocular hand shape estimation by
building on top of recent advances in unsupervised learning. We extend momentum
contrastive learning and contribute a structured collection of hand images,
well suited for visual representation learning, which we call HanCo. We find
that the representation learned by established contrastive learning methods can
be improved significantly by exploiting advanced background removal techniques
and multi-view information. These allow us to generate more diverse instance
pairs than those obtained by augmentations commonly used in exemplar based
approaches. Our method leads to a more suitable representation for the hand
shape estimation task and shows a 4.7% reduction in mesh error and a 3.6%
improvement in F-score compared to an ImageNet pretrained baseline. We make our
benchmark dataset publicly available, to encourage further research into this
direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_C/0/1/0/all/0/1"&gt;Christian Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Argus_M/0/1/0/all/0/1"&gt;Max Argus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1"&gt;Thomas Brox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Deep Learning based Cerebral Palsy Prediction with Channel Attention. (arXiv:2106.04471v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04471</id>
        <link href="http://arxiv.org/abs/2106.04471"/>
        <updated>2021-06-09T02:01:50.171Z</updated>
        <summary type="html"><![CDATA[Early prediction of cerebral palsy is essential as it leads to early
treatment and monitoring. Deep learning has shown promising results in
biomedical engineering thanks to its capacity of modelling complicated data
with its non-linear architecture. However, due to their complex structure, deep
learning models are generally not interpretable by humans, making it difficult
for clinicians to rely on the findings. In this paper, we propose a channel
attention module for deep learning models to predict cerebral palsy from
infants' body movements, which highlights the key features (i.e. body joints)
the model identifies as important, thereby indicating why certain diagnostic
results are found. To highlight the capacity of the deep network in modelling
input features, we utilize raw joint positions instead of hand-crafted
features. We validate our system with a real-world infant movement dataset. Our
proposed channel attention module enables the visualization of the vital joints
to this disease that the network considers. Our system achieves 91.67%
accuracy, suppressing other state-of-the-art deep learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Manli Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1"&gt;Qianhui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1"&gt;Edmond S. L. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leung_H/0/1/0/all/0/1"&gt;Howard Leung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1"&gt;Hubert P. H. Shum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Improving Adversarial Transferability of Vision Transformers. (arXiv:2106.04169v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04169</id>
        <link href="http://arxiv.org/abs/2106.04169"/>
        <updated>2021-06-09T02:01:50.157Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViTs) process input images as sequences of patches via
self-attention; a radically different architecture than convolutional neural
networks (CNNs). This makes it interesting to study the adversarial feature
space of ViT models and their transferability. In particular, we observe that
adversarial patterns found via conventional adversarial attacks show very low
black-box transferability even for large ViT models. However, we show that this
phenomenon is only due to the sub-optimal attack procedures that do not
leverage the true representation potential of ViTs. A deep ViT is composed of
multiple blocks, with a consistent architecture comprising of self-attention
and feed-forward layers, where each block is capable of independently producing
a class token. Formulating an attack using only the last class token
(conventional approach) does not directly leverage the discriminative
information stored in the earlier tokens, leading to poor adversarial
transferability of ViTs. Using the compositional nature of ViT models, we
enhance the transferability of existing attacks by introducing two novel
strategies specific to the architecture of ViT models. (i) Self-Ensemble: We
propose a method to find multiple discriminative pathways by dissecting a
single ViT model into an ensemble of networks. This allows explicitly utilizing
class-specific information at each ViT block. (ii) Token Refinement: We then
propose to refine the tokens to further enhance the discriminative capacity at
each block of ViT. Our token refinement systematically combines the class
tokens with structural information preserved within the patch tokens. An
adversarial attack, when applied to such refined tokens within the ensemble of
classifiers found in a single vision transformer, has significantly higher
transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1"&gt;Fatih Porikli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter Inference with Bifurcation Diagrams. (arXiv:2106.04243v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04243</id>
        <link href="http://arxiv.org/abs/2106.04243"/>
        <updated>2021-06-09T02:01:50.151Z</updated>
        <summary type="html"><![CDATA[Estimation of parameters in differential equation models can be achieved by
applying learning algorithms to quantitative time-series data. However,
sometimes it is only possible to measure qualitative changes of a system in
response to a controlled condition. In dynamical systems theory, such change
points are known as \textit{bifurcations} and lie on a function of the
controlled condition called the \textit{bifurcation diagram}. In this work, we
propose a gradient-based semi-supervised approach for inferring the parameters
of differential equations that produce a user-specified bifurcation diagram.
The cost function contains a supervised error term that is minimal when the
model bifurcations match the specified targets and an unsupervised bifurcation
measure which has gradients that push optimisers towards bifurcating parameter
regimes. The gradients can be computed without the need to differentiate
through the operations of the solver that was used to compute the diagram. We
demonstrate parameter inference with minimal models which explore the space of
saddle-node and pitchfork diagrams and the genetic toggle switch from synthetic
biology. Furthermore, the cost landscape allows us to organise models in terms
of topological and geometric equivalence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Szep_G/0/1/0/all/0/1"&gt;Gregory Szep&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalchau_N/0/1/0/all/0/1"&gt;Neil Dalchau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Csikasz_Nagy_A/0/1/0/all/0/1"&gt;Attila Csikasz-Nagy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Limited Memory Neural-Linear Bandits with Likelihood Matching. (arXiv:2102.03799v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03799</id>
        <link href="http://arxiv.org/abs/2102.03799"/>
        <updated>2021-06-09T02:01:50.144Z</updated>
        <summary type="html"><![CDATA[We study neural-linear bandits for solving problems where {\em both}
exploration and representation learning play an important role. Neural-linear
bandits harnesses the representation power of Deep Neural Networks (DNNs) and
combines it with efficient exploration mechanisms by leveraging uncertainty
estimation of the model, designed for linear contextual bandits on top of the
last hidden layer. In order to mitigate the problem of representation change
during the process, new uncertainty estimations are computed using stored data
from an unlimited buffer. Nevertheless, when the amount of stored data is
limited, a phenomenon called catastrophic forgetting emerges. To alleviate
this, we propose a likelihood matching algorithm that is resilient to
catastrophic forgetting and is completely online. We applied our algorithm,
Limited Memory Neural-Linear with Likelihood Matching (NeuralLinear-LiM2) on a
variety of datasets and observed that our algorithm achieves comparable
performance to the unlimited memory approach while exhibits resilience to
catastrophic forgetting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nabati_O/0/1/0/all/0/1"&gt;Ofir Nabati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1"&gt;Tom Zahavy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[White Paper Assistance: A Step Forward Beyond the Shortcut Learning. (arXiv:2106.04178v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04178</id>
        <link href="http://arxiv.org/abs/2106.04178"/>
        <updated>2021-06-09T02:01:50.138Z</updated>
        <summary type="html"><![CDATA[The promising performances of CNNs often overshadow the need to examine
whether they are doing in the way we are actually interested. We show through
experiments that even over-parameterized models would still solve a dataset by
recklessly leveraging spurious correlations, or so-called 'shortcuts'. To
combat with this unintended propensity, we borrow the idea of printer test page
and propose a novel approach called White Paper Assistance. Our proposed method
involves the white paper to detect the extent to which the model has preference
for certain characterized patterns and alleviates it by forcing the model to
make a random guess on the white paper. We show the consistent accuracy
improvements that are manifest in various architectures, datasets and
combinations with other techniques. Experiments have also demonstrated the
versatility of our approach on fine-grained recognition, imbalanced
classification and robustness to corruptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tianshu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaomin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiali Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minghui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras. (arXiv:2106.04477v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04477</id>
        <link href="http://arxiv.org/abs/2106.04477"/>
        <updated>2021-06-09T02:01:50.137Z</updated>
        <summary type="html"><![CDATA[Synthesizing novel views of dynamic humans from stationary monocular cameras
is a popular scenario. This is particularly attractive as it does not require
static scenes, controlled environments, or specialized hardware. In contrast to
techniques that exploit multi-view observations to constrain the modeling,
given a single fixed viewpoint only, the problem of modeling the dynamic scene
is significantly more under-constrained and ill-posed. In this paper, we
introduce Neural Motion Consensus Flow (MoCo-Flow), a representation that
models the dynamic scene using a 4D continuous time-variant function. The
proposed representation is learned by an optimization which models a dynamic
scene that minimizes the error of rendering all observation images. At the
heart of our work lies a novel optimization formulation, which is constrained
by a motion consensus regularization on the motion flow. We extensively
evaluate MoCo-Flow on several datasets that contain human motions of varying
complexity, and compare, both qualitatively and quantitatively, to several
baseline methods and variants of our methods. Pretrained model, code, and data
will be released for research purposes upon paper acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuelin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weiyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Baoquan Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical VAEs Know What They Don't Know. (arXiv:2102.08248v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08248</id>
        <link href="http://arxiv.org/abs/2102.08248"/>
        <updated>2021-06-09T02:01:50.133Z</updated>
        <summary type="html"><![CDATA[Deep generative models have been demonstrated as state-of-the-art density
estimators. Yet, recent work has found that they often assign a higher
likelihood to data from outside the training distribution. This seemingly
paradoxical behavior has caused concerns over the quality of the attained
density estimates. In the context of hierarchical variational autoencoders, we
provide evidence to explain this behavior by out-of-distribution data having
in-distribution low-level features. We argue that this is both expected and
desirable behavior. With this insight in hand, we develop a fast, scalable and
fully unsupervised likelihood-ratio score for OOD detection that requires data
to be in-distribution across all feature-levels. We benchmark the method on a
vast set of data and model combinations and achieve state-of-the-art results on
out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1"&gt;Jakob D. Havtorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frellsen_J/0/1/0/all/0/1"&gt;Jes Frellsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maaloe_L/0/1/0/all/0/1"&gt;Lars Maal&amp;#xf8;e&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Runtime-Based Computational Performance Predictor for Deep Neural Network Training. (arXiv:2102.00527v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00527</id>
        <link href="http://arxiv.org/abs/2102.00527"/>
        <updated>2021-06-09T02:01:50.128Z</updated>
        <summary type="html"><![CDATA[Deep learning researchers and practitioners usually leverage GPUs to help
train their deep neural networks (DNNs) faster. However, choosing which GPU to
use is challenging both because (i) there are many options, and (ii) users
grapple with competing concerns: maximizing compute performance while
minimizing costs. In this work, we present a new practical technique to help
users make informed and cost-efficient GPU selections: make performance
predictions with the help of a GPU that the user already has. Our technique
exploits the observation that, because DNN training consists of repetitive
compute steps, predicting the execution time of a single iteration is usually
enough to characterize the performance of an entire training process. We make
predictions by scaling the execution time of each operation in a training
iteration from one GPU to another using either (i) wave scaling, a technique
based on a GPU's execution model, or (ii) pre-trained multilayer perceptrons.
We implement our technique into a Python library called Habitat and find that
it makes accurate iteration execution time predictions (with an average error
of 11.8%) on ResNet-50, Inception v3, the Transformer, GNMT, and DCGAN across
six different GPU architectures. Habitat supports PyTorch, is easy to use, and
is open source.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Geoffrey X. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yubo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golikov_P/0/1/0/all/0/1"&gt;Pavel Golikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1"&gt;Gennady Pekhimenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-dataset Pretraining: A Unified Model for Semantic Segmentation. (arXiv:2106.04121v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04121</id>
        <link href="http://arxiv.org/abs/2106.04121"/>
        <updated>2021-06-09T02:01:50.112Z</updated>
        <summary type="html"><![CDATA[Collecting annotated data for semantic segmentation is time-consuming and
hard to scale up. In this paper, we for the first time propose a unified
framework, termed as Multi-Dataset Pretraining, to take full advantage of the
fragmented annotations of different datasets. The highlight is that the
annotations from different domains can be efficiently reused and consistently
boost performance for each specific domain. This is achieved by first
pretraining the network via the proposed pixel-to-prototype contrastive loss
over multiple datasets regardless of their taxonomy labels, and followed by
fine-tuning the pretrained model over specific dataset as usual. In order to
better model the relationship among images and classes from different datasets,
we extend the pixel level embeddings via cross dataset mixing and propose a
pixel-to-class sparse coding strategy that explicitly models the pixel-class
similarity over the manifold embedding space. In this way, we are able to
increase intra-class compactness and inter-class separability, as well as
considering inter-class similarity across different datasets for better
transferability. Experiments conducted on several benchmarks demonstrate its
superior performance. Notably, MDP consistently outperforms the pretrained
models over ImageNet by a considerable margin, while only using less than 10%
samples for pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Bowen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaopeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haohang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wenrui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Junni Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Convergence of Entropy-Regularized Natural Policy Gradient with Linear Function Approximation. (arXiv:2106.04096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04096</id>
        <link href="http://arxiv.org/abs/2106.04096"/>
        <updated>2021-06-09T02:01:50.106Z</updated>
        <summary type="html"><![CDATA[Natural policy gradient (NPG) methods with function approximation achieve
impressive empirical success in reinforcement learning problems with large
state-action spaces. However, theoretical understanding of their convergence
behaviors remains limited in the function approximation setting. In this paper,
we perform a finite-time analysis of NPG with linear function approximation and
softmax parameterization, and prove for the first time that widely used entropy
regularization method, which encourages exploration, leads to linear
convergence rate. We adopt a Lyapunov drift analysis to prove the convergence
results and explain the effectiveness of entropy regularization in improving
the convergence rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1"&gt;Semih Cayci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1"&gt;Niao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Easy-GT: Open-Source Software to Facilitate Making the Ground Truth for White Blood Cells Nucleus. (arXiv:2101.11654v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11654</id>
        <link href="http://arxiv.org/abs/2101.11654"/>
        <updated>2021-06-09T02:01:50.101Z</updated>
        <summary type="html"><![CDATA[The nucleus of white blood cells (WBCs) plays a significant role in their
detection and classification. Appropriate feature extraction of the nucleus is
necessary to fit a suitable artificial intelligence model to classify WBCs.
Therefore, designing a method is needed to segment the nucleus accurately.
There should be a comparison between the ground truths distinguished by a
hematologist and the detected nuclei to evaluate the performance of the nucleus
segmentation method accurately. It is a time-consuming and tedious task for
experts to establish the ground truth manually. This paper presents an
intelligent open-source software called Easy-GT to create the ground truth of
WBCs' nucleus faster and easier. This software first detects the nucleus by
employing a new Otsu's thresholding-based method with a dice similarity
coefficient (DSC) of 95.42 %; the hematologist can then create a more accurate
ground truth, using the designed buttons to modify the threshold value. This
software can speed up ground truth's forming process more than six times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kouzehkanan_Z/0/1/0/all/0/1"&gt;Zahra Mousavi Kouzehkanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tavakoli_S/0/1/0/all/0/1"&gt;Sajad Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alipanah_A/0/1/0/all/0/1"&gt;Arezoo Alipanah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Deep Q-Network for Autonomous Vehicles at Unsignalized Intersection. (arXiv:2106.04561v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04561</id>
        <link href="http://arxiv.org/abs/2106.04561"/>
        <updated>2021-06-09T02:01:50.096Z</updated>
        <summary type="html"><![CDATA[We propose a safe DRL approach for autonomous vehicle (AV) navigation through
crowds of pedestrians while making a left turn at an unsignalized intersection.
Our method uses two long-short term memory (LSTM) models that are trained to
generate the perceived state of the environment and the future trajectories of
pedestrians given noisy observations of their movement. A future collision
prediction algorithm based on the future trajectories of the ego vehicle and
pedestrians is used to mask unsafe actions if the system predicts a collision.
The performance of our approach is evaluated in two experiments using the
high-fidelity CARLA simulation environment. The first experiment tests the
performance of our method at intersections that are similar to the training
intersection and the second experiment tests our method at intersections with a
different topology. For both experiments, our methods do not result in a
collision with a pedestrian while still navigating the intersection at a
reasonable speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mokhtari_K/0/1/0/all/0/1"&gt;Kasra Mokhtari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1"&gt;Alan R. Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI and Adoption of Financial Algorithmic Advisors: an Experimental Study. (arXiv:2101.02555v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02555</id>
        <link href="http://arxiv.org/abs/2101.02555"/>
        <updated>2021-06-09T02:01:50.090Z</updated>
        <summary type="html"><![CDATA[We study whether receiving advice from either a human or algorithmic advisor,
accompanied by five types of Local and Global explanation labelings, has an
effect on the readiness to adopt, willingness to pay, and trust in a financial
AI consultant. We compare the differences over time and in various key
situations using a unique experimental framework where participants play a
web-based game with real monetary consequences. We observed that accuracy-based
explanations of the model in initial phases leads to higher adoption rates.
When the performance of the model is immaculate, there is less importance
associated with the kind of explanation for adoption. Using more elaborate
feature-based or accuracy-based explanations helps substantially in reducing
the adoption drop upon model failure. Furthermore, using an autopilot increases
adoption significantly. Participants assigned to the AI-labeled advice with
explanations were willing to pay more for the advice than the AI-labeled advice
with a No-explanation alternative. These results add to the literature on the
importance of XAI for algorithmic adoption and trust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1"&gt;Daniel Ben David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resheff_Y/0/1/0/all/0/1"&gt;Yehezkel S. Resheff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tron_T/0/1/0/all/0/1"&gt;Talia Tron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Description and Discussion on DCASE 2021 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring under Domain Shifted Conditions. (arXiv:2106.04492v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.04492</id>
        <link href="http://arxiv.org/abs/2106.04492"/>
        <updated>2021-06-09T02:01:50.076Z</updated>
        <summary type="html"><![CDATA[We present the task description and discussion on the results of the DCASE
2021 Challenge Task 2. Last year, we organized unsupervised anomalous sound
detection (ASD) task; identifying whether the given sound is normal or
anomalous without anomalous training data. In this year, we organize an
advanced unsupervised ASD task under domain-shift conditions which focuses on
the inevitable problem for the practical use of ASD systems. The main challenge
of this task is to detect unknown anomalous sounds where the acoustic
characteristics of the training and testing samples are different, i.e.
domain-shifted. This problem is frequently occurs due to changes in seasons,
manufactured products, and/or environmental noise. After the challenge
submission deadline, we will add challenge results and analysis of the
submissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kawaguchi_Y/0/1/0/all/0/1"&gt;Yohei Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Imoto_K/0/1/0/all/0/1"&gt;Keisuke Imoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koizumi_Y/0/1/0/all/0/1"&gt;Yuma Koizumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harada_N/0/1/0/all/0/1"&gt;Noboru Harada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niizumi_D/0/1/0/all/0/1"&gt;Daisuke Niizumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dohi_K/0/1/0/all/0/1"&gt;Kota Dohi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tanabe_R/0/1/0/all/0/1"&gt;Ryo Tanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Purohit_H/0/1/0/all/0/1"&gt;Harsh Purohit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Endo_T/0/1/0/all/0/1"&gt;Takashi Endo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\ell_0$-based Sparse Canonical Correlation Analysis. (arXiv:2010.05620v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05620</id>
        <link href="http://arxiv.org/abs/2010.05620"/>
        <updated>2021-06-09T02:01:50.070Z</updated>
        <summary type="html"><![CDATA[Canonical Correlation Analysis (CCA) models are powerful for studying the
associations between two sets of variables. The canonically correlated
representations, termed \textit{canonical variates} are widely used in
unsupervised learning to analyze unlabeled multi-modal registered datasets.
Despite their success, CCA models may break (or overfit) if the number of
variables in either of the modalities exceeds the number of samples. Moreover,
often a significant fraction of the variables measures modality-specific
information, and thus removing them is beneficial for identifying the
\textit{canonically correlated variates}. Here, we propose $\ell_0$-CCA, a
method for learning correlated representations based on sparse subsets of
variables from two observed modalities. Sparsity is obtained by multiplying the
input variables by stochastic gates, whose parameters are learned together with
the CCA weights via an $\ell_0$-regularized correlation loss. We further
propose $\ell_0$-Deep CCA for solving the problem of non-linear sparse CCA by
modeling the correlated representations using deep nets. We demonstrate the
efficacy of the method using several synthetic and real examples. Most notably,
by gating nuisance input variables, our approach improves the extracted
representations compared to other linear, non-linear and sparse CCA-based
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lindenbaum_O/0/1/0/all/0/1"&gt;Ofir Lindenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salhov_M/0/1/0/all/0/1"&gt;Moshe Salhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Averbuch_A/0/1/0/all/0/1"&gt;Amir Averbuch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kluger_Y/0/1/0/all/0/1"&gt;Yuval Kluger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04156</id>
        <link href="http://arxiv.org/abs/2106.04156"/>
        <updated>2021-06-09T02:01:50.064Z</updated>
        <summary type="html"><![CDATA[Recent works in self-supervised learning have advanced the state-of-the-art
by relying on the contrastive learning paradigm, which learns representations
by pushing positive pairs, or similar examples from the same class, closer
together while keeping negative pairs far apart. Despite the empirical
successes, theoretical foundations are limited -- prior analyses assume
conditional independence of the positive pairs given the same class label, but
recent empirical applications use heavily correlated positive pairs (i.e., data
augmentations of the same image). Our work analyzes contrastive learning
without assuming conditional independence of positive pairs using a novel
concept of the augmentation graph on data. Edges in this graph connect
augmentations of the same data, and ground-truth classes naturally form
connected sub-graphs. We propose a loss that performs spectral decomposition on
the population augmentation graph and can be succinctly written as a
contrastive learning objective on neural net representations. Minimizing this
objective leads to features with provable accuracy guarantees under linear
probe evaluation. By standard generalization bounds, these accuracy guarantees
also hold when minimizing the training contrastive loss. Empirically, the
features learned by our objective can match or outperform several strong
baselines on benchmark vision datasets. In all, this work provides the first
provable analysis for contrastive learning where guarantees for linear probe
evaluation can apply to realistic empirical settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1"&gt;Jeff Z. HaoChen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1"&gt;Adrien Gaidon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors. (arXiv:2005.07519v4 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07519</id>
        <link href="http://arxiv.org/abs/2005.07519"/>
        <updated>2021-06-09T02:01:50.058Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML), especially deep learning (DL) techniques have been
increasingly used in anomaly-based network intrusion detection systems (NIDS).
However, ML/DL has shown to be extremely vulnerable to adversarial attacks,
especially in such security-sensitive systems. Many adversarial attacks have
been proposed to evaluate the robustness of ML-based NIDSs. Unfortunately,
existing attacks mostly focused on feature-space and/or white-box attacks,
which make impractical assumptions in real-world scenarios, leaving the study
on practical gray/black-box attacks largely unexplored.

To bridge this gap, we conduct the first systematic study of the
gray/black-box traffic-space adversarial attacks to evaluate the robustness of
ML-based NIDSs. Our work outperforms previous ones in the following aspects:
(i) practical-the proposed attack can automatically mutate original traffic
with extremely limited knowledge and affordable overhead while preserving its
functionality; (ii) generic-the proposed attack is effective for evaluating the
robustness of various NIDSs using diverse ML/DL models and non-payload-based
features; (iii) explainable-we propose an explanation method for the fragile
robustness of ML-based NIDSs. Based on this, we also propose a defense scheme
against adversarial attacks to improve system robustness. We extensively
evaluate the robustness of various NIDSs using diverse feature sets and ML/DL
models. Experimental results show our attack is effective (e.g., >97% evasion
rate in half cases for Kitsune, a state-of-the-art NIDS) with affordable
execution cost and the proposed defense method can effectively mitigate such
attacks (evasion rate is reduced by >50% in most cases).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dongqi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Ying Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiahai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shuqiang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xingang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xia Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Heavy-Tail Phenomenon in SGD. (arXiv:2006.04740v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04740</id>
        <link href="http://arxiv.org/abs/2006.04740"/>
        <updated>2021-06-09T02:01:50.053Z</updated>
        <summary type="html"><![CDATA[In recent years, various notions of capacity and complexity have been
proposed for characterizing the generalization properties of stochastic
gradient descent (SGD) in deep learning. Some of the popular notions that
correlate well with the performance on unseen data are (i) the `flatness' of
the local minimum found by SGD, which is related to the eigenvalues of the
Hessian, (ii) the ratio of the stepsize $\eta$ to the batch-size $b$, which
essentially controls the magnitude of the stochastic gradient noise, and (iii)
the `tail-index', which measures the heaviness of the tails of the network
weights at convergence. In this paper, we argue that these three seemingly
unrelated perspectives for generalization are deeply linked to each other. We
claim that depending on the structure of the Hessian of the loss at the
minimum, and the choices of the algorithm parameters $\eta$ and $b$, the SGD
iterates will converge to a \emph{heavy-tailed} stationary distribution. We
rigorously prove this claim in the setting of quadratic optimization: we show
that even in a simple linear regression problem with independent and
identically distributed data whose distribution has finite moments of all
order, the iterates can be heavy-tailed with infinite variance. We further
characterize the behavior of the tails with respect to algorithm parameters,
the dimension, and the curvature. We then translate our results into insights
about the behavior of SGD in deep learning. We support our theory with
experiments conducted on synthetic data, fully connected, and convolutional
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1"&gt;Mert Gurbuzbalaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Simsekli_U/0/1/0/all/0/1"&gt;Umut Simsekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingjiong Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding (Generalized) Label Smoothing whenLearning with Noisy Labels. (arXiv:2106.04149v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04149</id>
        <link href="http://arxiv.org/abs/2106.04149"/>
        <updated>2021-06-09T02:01:50.047Z</updated>
        <summary type="html"><![CDATA[Label smoothing (LS) is an arising learning paradigm that uses the positively
weighted average of both the hard training labels and uniformly distributed
soft labels. It was shown that LS serves as a regularizer for training data
with hard labels and therefore improves the generalization of the model. Later
it was reported LS even helps with improving robustness when learning with
noisy labels. However, we observe that the advantage of LS vanishes when we
operate in a high label noise regime. Puzzled by the observation, we proceeded
to discover that several proposed learning-with-noisy-labels solutions in the
literature instead relate more closely to negative label smoothing (NLS), which
defines as using a negative weight to combine the hard and soft labels! We show
that NLS functions substantially differently from LS in their achieved model
confidence. To differentiate the two cases, we will call LS the positive label
smoothing (PLS), and this paper unifies PLS and NLS into generalized label
smoothing (GLS). We provide understandings for the properties of GLS when
learning with noisy labels. Among other established properties, we
theoretically show NLS is considered more beneficial when the label noise rates
are high. We provide experimental results to support our findings too.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiaheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hangyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Deep Learning based Cerebral Palsy Prediction with Channel Attention. (arXiv:2106.04471v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04471</id>
        <link href="http://arxiv.org/abs/2106.04471"/>
        <updated>2021-06-09T02:01:50.029Z</updated>
        <summary type="html"><![CDATA[Early prediction of cerebral palsy is essential as it leads to early
treatment and monitoring. Deep learning has shown promising results in
biomedical engineering thanks to its capacity of modelling complicated data
with its non-linear architecture. However, due to their complex structure, deep
learning models are generally not interpretable by humans, making it difficult
for clinicians to rely on the findings. In this paper, we propose a channel
attention module for deep learning models to predict cerebral palsy from
infants' body movements, which highlights the key features (i.e. body joints)
the model identifies as important, thereby indicating why certain diagnostic
results are found. To highlight the capacity of the deep network in modelling
input features, we utilize raw joint positions instead of hand-crafted
features. We validate our system with a real-world infant movement dataset. Our
proposed channel attention module enables the visualization of the vital joints
to this disease that the network considers. Our system achieves 91.67%
accuracy, suppressing other state-of-the-art deep learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Manli Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1"&gt;Qianhui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1"&gt;Edmond S. L. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leung_H/0/1/0/all/0/1"&gt;Howard Leung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1"&gt;Hubert P. H. Shum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Generalization despite Distribution Shift via Minimum Discriminating Information. (arXiv:2106.04443v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04443</id>
        <link href="http://arxiv.org/abs/2106.04443"/>
        <updated>2021-06-09T02:01:50.024Z</updated>
        <summary type="html"><![CDATA[Training models that perform well under distribution shifts is a central
challenge in machine learning. In this paper, we introduce a modeling framework
where, in addition to training data, we have partial structural knowledge of
the shifted test distribution. We employ the principle of minimum
discriminating information to embed the available prior knowledge, and use
distributionally robust optimization to account for uncertainty due to the
limited samples. By leveraging large deviation results, we obtain explicit
generalization bounds with respect to the unknown shifted distribution. Lastly,
we demonstrate the versatility of our framework by demonstrating it on two
rather distinct applications: (1) training classifiers on systematically biased
data and (2) off-policy evaluation in Markov Decision Processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sutter_T/0/1/0/all/0/1"&gt;Tobias Sutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuhn_D/0/1/0/all/0/1"&gt;Daniel Kuhn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04563</id>
        <link href="http://arxiv.org/abs/2106.04563"/>
        <updated>2021-06-09T02:01:50.019Z</updated>
        <summary type="html"><![CDATA[While deep and large pre-trained models are the state-of-the-art for various
natural language processing tasks, their huge size poses significant challenges
for practical uses in resource constrained settings. Recent works in knowledge
distillation propose task-agnostic as well as task-specific methods to compress
these models, with task-specific ones often yielding higher compression rate.
In this work, we develop a new task-agnostic distillation framework
XtremeDistilTransformers that leverages the advantage of task-specific methods
for learning a small universal model that can be applied to arbitrary tasks and
languages. To this end, we study the transferability of several source tasks,
augmentation resources and model architecture for distillation. We evaluate our
model performance on multiple tasks, including the General Language
Understanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and
a massive multi-lingual NER dataset with 41 languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhabrata Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks. (arXiv:2106.04537v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04537</id>
        <link href="http://arxiv.org/abs/2106.04537"/>
        <updated>2021-06-09T02:01:50.013Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are powerful machines for visual pattern recognition,
but reasoning tasks that are easy for humans may still be difficult for neural
models. Humans possess the ability to extrapolate reasoning strategies learned
on simple problems to solve harder examples, often by thinking for longer. For
example, a person who has learned to solve small mazes can easily extend the
very same search techniques to solve much larger mazes by spending more time.
In computers, this behavior is often achieved through the use of algorithms,
which scale to arbitrarily hard problem instances at the cost of more
computation. In contrast, the sequential computing budget of feed-forward
neural networks is limited by their depth, and networks trained on simple
problems have no way of extending their reasoning to accommodate harder
problems. In this work, we show that recurrent networks trained to solve simple
problems with few recurrent steps can indeed solve much more complex problems
simply by performing additional recurrences during inference. We demonstrate
this algorithmic behavior of recurrent networks on prefix sum computation,
mazes, and chess. In all three domains, networks trained on simple problem
instances are able to extend their reasoning abilities at test time simply by
"thinking for longer."]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1"&gt;Avi Schwarzschild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgnia_E/0/1/0/all/0/1"&gt;Eitan Borgnia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Arjun Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vishkin_U/0/1/0/all/0/1"&gt;Uzi Vishkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Giving Commands to a Self-Driving Car: How to Deal with Uncertain Situations?. (arXiv:2106.04232v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.04232</id>
        <link href="http://arxiv.org/abs/2106.04232"/>
        <updated>2021-06-09T02:01:50.008Z</updated>
        <summary type="html"><![CDATA[Current technology for autonomous cars primarily focuses on getting the
passenger from point A to B. Nevertheless, it has been shown that passengers
are afraid of taking a ride in self-driving cars. One way to alleviate this
problem is by allowing the passenger to give natural language commands to the
car. However, the car can misunderstand the issued command or the visual
surroundings which could lead to uncertain situations. It is desirable that the
self-driving car detects these situations and interacts with the passenger to
solve them. This paper proposes a model that detects uncertain situations when
a command is given and finds the visual objects causing it. Optionally, a
question generated by the system describing the uncertain objects is included.
We argue that if the car could explain the objects in a human-like way,
passengers could gain more confidence in the car's abilities. Thus, we
investigate how to (1) detect uncertain situations and their underlying causes,
and (2) how to generate clarifying questions for the passenger. When evaluating
on the Talk2Car dataset, we show that the proposed model, \acrfull{pipeline},
improves \gls{m:ambiguous-absolute-increase} in terms of $IoU_{.5}$ compared to
not using \gls{pipeline}. Furthermore, we designed a referring expression
generator (REG) \acrfull{reg_model} tailored to a self-driving car setting
which yields a relative improvement of \gls{m:meteor-relative} METEOR and
\gls{m:rouge-relative} ROUGE-l compared with state-of-the-art REG models, and
is three times faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deruyttere_T/0/1/0/all/0/1"&gt;Thierry Deruyttere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milewski_V/0/1/0/all/0/1"&gt;Victor Milewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1"&gt;Marie-Francine Moens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What training reveals about neural network complexity. (arXiv:2106.04186v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04186</id>
        <link href="http://arxiv.org/abs/2106.04186"/>
        <updated>2021-06-09T02:01:49.992Z</updated>
        <summary type="html"><![CDATA[This work explores the hypothesis that the complexity of the function a deep
neural network (NN) is learning can be deduced by how fast its weights change
during training. Our analysis provides evidence for this supposition by
relating the network's distribution of Lipschitz constants (i.e., the norm of
the gradient at different regions of the input space) during different training
intervals with the behavior of the stochastic training procedure. We first
observe that the average Lipschitz constant close to the training data affects
various aspects of the parameter trajectory, with more complex networks having
a longer trajectory, bigger variance, and often veering further from their
initialization. We then show that NNs whose biases are trained more steadily
have bounded complexity even in regions of the input space that are far from
any training point. Finally, we find that steady training with Dropout implies
a training- and data-dependent generalization bound that grows
poly-logarithmically with the number of parameters. Overall, our results
support the hypothesis that good training behavior can be a useful bias towards
good generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1"&gt;Andreas Loukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poiitis_M/0/1/0/all/0/1"&gt;Marinos Poiitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1"&gt;Stefanie Jegelka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Feedback Learning for Contact-Rich Manipulation Tasks with Uncertainty. (arXiv:2106.04306v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04306</id>
        <link href="http://arxiv.org/abs/2106.04306"/>
        <updated>2021-06-09T02:01:49.981Z</updated>
        <summary type="html"><![CDATA[While classic control theory offers state of the art solutions in many
problem scenarios, it is often desired to improve beyond the structure of such
solutions and surpass their limitations. To this end, \emph{\gls{rpl}} offers a
formulation to improve existing controllers with reinforcement learning (RL) by
learning an additive "residual" to the output of a given controller. However,
the applicability of such an approach highly depends on the structure of the
controller. Often, internal feedback signals of the controller limit an RL
algorithm to adequately change the policy and, hence, learn the task. We
propose a new formulation that addresses these limitations by also modifying
the feedback signals to the controller with an RL policy and show superior
performance of our approach on a contact-rich peg-insertion task under position
and orientation uncertainty. In addition, we use a recent impedance control
architecture as control framework and show the difficulties of standard RPL.
Furthermore, we introduce an adaptive curriculum for the given task to
gradually increase the task difficulty in terms of position and orientation
uncertainty. A video showing the results can be found at
https://youtu.be/SAZm_Krze7U .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranjbar_A/0/1/0/all/0/1"&gt;Alireza Ranjbar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1"&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1"&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1"&gt;Joschka Boedecker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1"&gt;Gerhard Neumann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Rank Subspaces in GANs. (arXiv:2106.04488v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04488</id>
        <link href="http://arxiv.org/abs/2106.04488"/>
        <updated>2021-06-09T02:01:49.979Z</updated>
        <summary type="html"><![CDATA[The latent space of a Generative Adversarial Network (GAN) has been shown to
encode rich semantics within some subspaces. To identify these subspaces,
researchers typically analyze the statistical information from a collection of
synthesized data, and the identified subspaces tend to control image attributes
globally (i.e., manipulating an attribute causes the change of an entire
image). By contrast, this work introduces low-rank subspaces that enable more
precise control of GAN generation. Concretely, given an arbitrary image and a
region of interest (e.g., eyes of face images), we manage to relate the latent
space to the image region with the Jacobian matrix and then use low-rank
factorization to discover steerable latent subspaces. There are three
distinguishable strengths of our approach that can be aptly called LowRankGAN.
First, compared to analytic algorithms in prior work, our low-rank
factorization of Jacobians is able to find the low-dimensional representation
of attribute manifold, making image editing more precise and controllable.
Second, low-rank factorization naturally yields a null space of attributes such
that moving the latent code within it only affects the outer region of
interest. Therefore, local image editing can be simply achieved by projecting
an attribute vector into the null space without relying on a spatial mask as
existing methods do. Third, our method can robustly work with a local region
from one image for analysis yet well generalize to other images, making it much
easy to use in practice. Extensive experiments on state-of-the-art GAN models
(including StyleGAN2 and BigGAN) trained on various datasets demonstrate the
effectiveness of our LowRankGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiapeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ruili Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yujun Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Deli Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhengjun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Staircase Attention for Recurrent Processing of Sequences. (arXiv:2106.04279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04279</id>
        <link href="http://arxiv.org/abs/2106.04279"/>
        <updated>2021-06-09T02:01:49.973Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms have become a standard tool for sequence modeling tasks,
in particular by stacking self-attention layers over the entire input sequence
as in the Transformer architecture. In this work we introduce a novel attention
procedure called staircase attention that, unlike self-attention, operates
across the sequence (in time) recurrently processing the input by adding
another step of processing. A step in the staircase comprises of backward
tokens (encoding the sequence so far seen) and forward tokens (ingesting a new
part of the sequence), or an extreme Ladder version with a forward step of zero
that simply repeats the Transformer on each step of the ladder, sharing the
weights. We thus describe a family of such models that can trade off
performance and compute, by either increasing the amount of recurrence through
time, the amount of sequential processing via recurrence in depth, or both.
Staircase attention is shown to be able to solve tasks that involve tracking
that conventional Transformers cannot, due to this recurrence. Further, it is
shown to provide improved modeling power for the same size model (number of
parameters) compared to self-attentive Transformers on large language modeling
and dialogue tasks, yielding significant perplexity gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1"&gt;Da Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intelligent Hybrid Model for Identity Document Classification. (arXiv:2106.04345v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04345</id>
        <link href="http://arxiv.org/abs/2106.04345"/>
        <updated>2021-06-09T02:01:49.968Z</updated>
        <summary type="html"><![CDATA[Digitization, i.e., the process of converting information into a digital
format, may provide various opportunities (e.g., increase in productivity,
disaster recovery, and environmentally friendly solutions) and challenges for
businesses. In this context, one of the main challenges would be to accurately
classify numerous scanned documents uploaded every day by customers as usual
business processes. For example, processes in banking (e.g., applying for
loans) or the Government Registry of BDM (Births, Deaths, and Marriages)
applications may involve uploading several documents such as a driver's license
and passport. There are not many studies available to address the challenge as
an application of image classification. Although some studies are available
which used various methods, a more accurate model is still required. The
current study has proposed a robust fusion model to define the type of identity
documents accurately. The proposed approach is based on two different methods
in which images are classified based on their visual features and text
features. A novel model based on statistics and regression has been proposed to
calculate the confidence level for the feature-based classifier. A fuzzy-mean
fusion model has been proposed to combine the classifier results based on their
confidence score. The proposed approach has been implemented using Python and
experimentally validated on synthetic and real-world datasets. The performance
of the proposed model is evaluated using the Receiver Operating Characteristic
(ROC) curve analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khandan_N/0/1/0/all/0/1"&gt;Nouna Khandan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RECOWNs: Probabilistic Circuits for Trustworthy Time Series Forecasting. (arXiv:2106.04148v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04148</id>
        <link href="http://arxiv.org/abs/2106.04148"/>
        <updated>2021-06-09T02:01:49.962Z</updated>
        <summary type="html"><![CDATA[Time series forecasting is a relevant task that is performed in several
real-world scenarios such as product sales analysis and prediction of energy
demand. Given their accuracy performance, currently, Recurrent Neural Networks
(RNNs) are the models of choice for this task. Despite their success in time
series forecasting, less attention has been paid to make the RNNs trustworthy.
For example, RNNs can not naturally provide an uncertainty measure to their
predictions. This could be extremely useful in practice in several cases e.g.
to detect when a prediction might be completely wrong due to an unusual pattern
in the time series. Whittle Sum-Product Networks (WSPNs), prominent deep
tractable probabilistic circuits (PCs) for time series, can assist an RNN with
providing meaningful probabilities as uncertainty measure. With this aim, we
propose RECOWN, a novel architecture that employs RNNs and a discriminant
variant of WSPNs called Conditional WSPNs (CWSPNs). We also formulate a
Log-Likelihood Ratio Score as better estimation of uncertainty that is tailored
to time series and Whittle likelihoods. In our experiments, we show that
RECOWNs are accurate and trustworthy time series predictors, able to "know when
they do not know".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thoma_N/0/1/0/all/0/1"&gt;Nils Thoma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhongjie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1"&gt;Fabrizio Ventola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Robust Detection of Out-of-distribution Data (almost) for free. (arXiv:2106.04260v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04260</id>
        <link href="http://arxiv.org/abs/2106.04260"/>
        <updated>2021-06-09T02:01:49.929Z</updated>
        <summary type="html"><![CDATA[When applying machine learning in safety-critical systems, a reliable
assessment of the uncertainy of a classifier is required. However, deep neural
networks are known to produce highly overconfident predictions on
out-of-distribution (OOD) data and even if trained to be non-confident on OOD
data one can still adversarially manipulate OOD data so that the classifer
again assigns high confidence to the manipulated samples. In this paper we
propose a novel method where from first principles we combine a certifiable OOD
detector with a standard classifier into an OOD aware classifier. In this way
we achieve the best of two worlds: certifiably adversarially robust OOD
detection, even for OOD samples close to the in-distribution, without loss in
prediction accuracy and close to state-of-the-art OOD detection performance for
non-manipulated OOD data. Moreover, due to the particular construction our
classifier provably avoids the asymptotic overconfidence problem of standard
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meinke_A/0/1/0/all/0/1"&gt;Alexander Meinke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bitterwolf_J/0/1/0/all/0/1"&gt;Julian Bitterwolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seamlessly Unifying Attributes and Items: Conversational Recommendation for Cold-Start Users. (arXiv:2005.12979v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12979</id>
        <link href="http://arxiv.org/abs/2005.12979"/>
        <updated>2021-06-09T02:01:49.921Z</updated>
        <summary type="html"><![CDATA[Static recommendation methods like collaborative filtering suffer from the
inherent limitation of performing real-time personalization for cold-start
users. Online recommendation, e.g., multi-armed bandit approach, addresses this
limitation by interactively exploring user preference online and pursuing the
exploration-exploitation (EE) trade-off. However, existing bandit-based methods
model recommendation actions homogeneously. Specifically, they only consider
the items as the arms, being incapable of handling the item attributes, which
naturally provide interpretable information of user's current demands and can
effectively filter out undesired items. In this work, we consider the
conversational recommendation for cold-start users, where a system can both ask
the attributes from and recommend items to a user interactively. This important
scenario was studied in a recent work. However, it employs a hand-crafted
function to decide when to ask attributes or make recommendations. Such
separate modeling of attributes and items makes the effectiveness of the system
highly rely on the choice of the hand-crafted function, thus introducing
fragility to the system. To address this limitation, we seamlessly unify
attributes and items in the same arm space and achieve their EE trade-offs
automatically using the framework of Thompson Sampling. Our Conversational
Thompson Sampling (ConTS) model holistically solves all questions in
conversational recommendation by choosing the arm with the maximal reward to
play. Extensive experiments on three benchmark datasets show that ConTS
outperforms the state-of-the-art methods Conversational UCB (ConUCB) and
Estimation-Action-Reflection model in both metrics of success rate and average
number of conversation turns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shijun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1"&gt;Wenqiang Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingyun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1"&gt;Peng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: A privacy-respecting Android malware classifier using Federated Learning. (arXiv:2007.08319v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08319</id>
        <link href="http://arxiv.org/abs/2007.08319"/>
        <updated>2021-06-09T02:01:49.913Z</updated>
        <summary type="html"><![CDATA[In this paper we present LiM ("Less is More"), a malware classification
framework that leverages Federated Learning to detect and classify malicious
apps in a privacy-respecting manner. Information about newly installed apps is
kept locally on users' devices, so that the provider cannot infer which apps
were installed by users. At the same time, input from all users is taken into
account in the federated learning process and they all benefit from better
classification performance. A key challenge of this setting is that users do
not have access to the ground truth (i.e. they cannot correctly identify
whether an app is malicious). To tackle this, LiM uses a safe semi-supervised
ensemble that maximizes classification accuracy with respect to a baseline
classifier trained by the service provider (i.e. the cloud). We implement LiM
and show that the cloud server has F1 score of 95%, while clients have perfect
recall with only 1 false positive in >100 apps, using a dataset of 25K clean
apps and 25K malicious apps, 200 users and 50 rounds of federation.
Furthermore, we conduct a security analysis and demonstrate that LiM is robust
against both poisoning attacks by adversaries who control half of the clients,
and inference attacks performed by an honest-but-curious cloud server. Further
experiments with MaMaDroid's dataset confirm resistance against poisoning
attacks and a performance improvement due to the federation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galvez_R/0/1/0/all/0/1"&gt;Rafa G&amp;#xe1;lvez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moonsamy_V/0/1/0/all/0/1"&gt;Veelasha Moonsamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_C/0/1/0/all/0/1"&gt;Claudia Diaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Isometric Gaussian Process Latent Variable Model for Dissimilarity Data. (arXiv:2006.11741v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11741</id>
        <link href="http://arxiv.org/abs/2006.11741"/>
        <updated>2021-06-09T02:01:49.907Z</updated>
        <summary type="html"><![CDATA[We present a probabilistic model where the latent variable respects both the
distances and the topology of the modeled data. The model leverages the
Riemannian geometry of the generated manifold to endow the latent space with a
well-defined stochastic distance measure, which is modeled locally as Nakagami
distributions. These stochastic distances are sought to be as similar as
possible to observed distances along a neighborhood graph through a censoring
process. The model is inferred by variational inference based on observations
of pairwise distances. We demonstrate how the new model can encode invariances
in the learned manifolds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jorgensen_M/0/1/0/all/0/1"&gt;Martin J&amp;#xf8;rgensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Multiple Shooting Layers. (arXiv:2106.03885v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03885</id>
        <link href="http://arxiv.org/abs/2106.03885"/>
        <updated>2021-06-09T02:01:49.902Z</updated>
        <summary type="html"><![CDATA[We detail a novel class of implicit neural models. Leveraging time-parallel
methods for differential equations, Multiple Shooting Layers (MSLs) seek
solutions of initial value problems via parallelizable root-finding algorithms.
MSLs broadly serve as drop-in replacements for neural ordinary differential
equations (Neural ODEs) with improved efficiency in number of function
evaluations (NFEs) and wall-clock inference time. We develop the algorithmic
framework of MSLs, analyzing the different choices of solution methods from a
theoretical and computational perspective. MSLs are showcased in long horizon
optimal control of ODEs and PDEs and as latent models for sequence generation.
Finally, we investigate the speedups obtained through application of MSL
inference in neural controlled differential equations (Neural CDEs) for time
series classification of medical data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1"&gt;Stefano Massaroli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1"&gt;Michael Poli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonoda_S/0/1/0/all/0/1"&gt;Sho Sonoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taji Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinkyoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1"&gt;Atsushi Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1"&gt;Hajime Asama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Suicidal Ideation and Mental Disorder Detection with Attentive Relation Networks. (arXiv:2004.07601v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07601</id>
        <link href="http://arxiv.org/abs/2004.07601"/>
        <updated>2021-06-09T02:01:49.887Z</updated>
        <summary type="html"><![CDATA[Mental health is a critical issue in modern society, and mental disorders
could sometimes turn to suicidal ideation without effective treatment. Early
detection of mental disorders and suicidal ideation from social content
provides a potential way for effective social intervention. However,
classifying suicidal ideation and other mental disorders is challenging as they
share similar patterns in language usage and sentimental polarity. This paper
enhances text representation with lexicon-based sentiment scores and latent
topics and proposes using relation networks to detect suicidal ideation and
mental disorders with related risk indicators. The relation module is further
equipped with the attention mechanism to prioritize more critical relational
features. Through experiments on three real-world datasets, our model
outperforms most of its counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shaoxiong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surveillance of COVID-19 Pandemic using Social Media: A Reddit Study in North Carolina. (arXiv:2106.04515v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.04515</id>
        <link href="http://arxiv.org/abs/2106.04515"/>
        <updated>2021-06-09T02:01:49.882Z</updated>
        <summary type="html"><![CDATA[Coronavirus disease (COVID-19) pandemic has changed various aspects of
people's lives and behaviors. At this stage, there are no other ways to control
the natural progression of the disease than adopting mitigation strategies such
as wearing masks, watching distance, and washing hands. Moreover, at this time
of social distancing, social media plays a key role in connecting people and
providing a platform for expressing their feelings. In this study, we tap into
social media to surveil the uptake of mitigation and detection strategies, and
capture issues and concerns about the pandemic. In particular, we explore the
research question, "how much can be learned regarding the public uptake of
mitigation strategies and concerns about COVID-19 pandemic by using natural
language processing on Reddit posts?" After extracting COVID-related posts from
the four largest subreddit communities of North Carolina over six months, we
performed NLP-based preprocessing to clean the noisy data. We employed a custom
Named-entity Recognition (NER) system and a Latent Dirichlet Allocation (LDA)
method for topic modeling on a Reddit corpus. We observed that 'mask', 'flu',
and 'testing' are the most prevalent named-entities for "Personal Protective
Equipment", "symptoms", and "testing" categories, respectively. We also
observed that the most discussed topics are related to testing, masks, and
employment. The mitigation measures are the most prevalent theme of discussion
across all subreddits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitfield_C/0/1/0/all/0/1"&gt;Christopher Whitfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_M/0/1/0/all/0/1"&gt;Mohad Anwar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk Ranked Recall: Collision Safety Metric for Object Detection Systems in Autonomous Vehicles. (arXiv:2106.04146v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04146</id>
        <link href="http://arxiv.org/abs/2106.04146"/>
        <updated>2021-06-09T02:01:49.876Z</updated>
        <summary type="html"><![CDATA[Commonly used metrics for evaluation of object detection systems (precision,
recall, mAP) do not give complete information about their suitability of use in
safety critical tasks, like obstacle detection for collision avoidance in
Autonomous Vehicles (AV). This work introduces the Risk Ranked Recall ($R^3$)
metrics for object detection systems. The $R^3$ metrics categorize objects
within three ranks. Ranks are assigned based on an objective cyber-physical
model for the risk of collision. Recall is measured for each rank.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1"&gt;Ayoosh Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jayati Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verucchi_M/0/1/0/all/0/1"&gt;Micaela Verucchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caccamo_M/0/1/0/all/0/1"&gt;Marco Caccamo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1"&gt;Lui Sha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Riemannian Manifolds for Geodesic Motion Skills. (arXiv:2106.04315v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.04315</id>
        <link href="http://arxiv.org/abs/2106.04315"/>
        <updated>2021-06-09T02:01:49.870Z</updated>
        <summary type="html"><![CDATA[For robots to work alongside humans and perform in unstructured environments,
they must learn new motion skills and adapt them to unseen situations on the
fly. This demands learning models that capture relevant motion patterns, while
offering enough flexibility to adapt the encoded skills to new requirements,
such as dynamic obstacle avoidance. We introduce a Riemannian manifold
perspective on this problem, and propose to learn a Riemannian manifold from
human demonstrations on which geodesics are natural motion skills. We realize
this with a variational autoencoder (VAE) over the space of position and
orientations of the robot end-effector. Geodesic motion skills let a robot plan
movements from and to arbitrary points on the data manifold. They also provide
a straightforward method to avoid obstacles by redefining the ambient metric in
an online fashion. Moreover, geodesics naturally exploit the manifold resulting
from multiple--mode tasks to design motions that were not explicitly
demonstrated previously. We test our learning framework using a 7-DoF robotic
manipulator, where the robot satisfactorily learns and reproduces realistic
skills featuring elaborated motion patterns, avoids previously unseen
obstacles, and generates novel movements in multiple-mode settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beik_Mohammadi_H/0/1/0/all/0/1"&gt;Hadi Beik-Mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arvanitidis_G/0/1/0/all/0/1"&gt;Georgios Arvanitidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1"&gt;Gerhard Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rozo_L/0/1/0/all/0/1"&gt;Leonel Rozo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Lov\'asz Embeddings for Proposal-free Panoptic Segmentation. (arXiv:2106.04555v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04555</id>
        <link href="http://arxiv.org/abs/2106.04555"/>
        <updated>2021-06-09T02:01:49.864Z</updated>
        <summary type="html"><![CDATA[Panoptic segmentation brings together two separate tasks: instance and
semantic segmentation. Although they are related, unifying them faces an
apparent paradox: how to learn simultaneously instance-specific and
category-specific (i.e. instance-agnostic) representations jointly. Hence,
state-of-the-art panoptic segmentation methods use complex models with a
distinct stream for each task. In contrast, we propose Hierarchical Lov\'asz
Embeddings, per pixel feature vectors that simultaneously encode instance- and
category-level discriminative information. We use a hierarchical Lov\'asz hinge
loss to learn a low-dimensional embedding space structured into a unified
semantic and instance hierarchy without requiring separate network branches or
object proposals. Besides modeling instances precisely in a proposal-free
manner, our Hierarchical Lov\'asz Embeddings generalize to categories by using
a simple Nearest-Class-Mean classifier, including for non-instance "stuff"
classes where instance segmentation methods are not applicable. Our simple
model achieves state-of-the-art results compared to existing proposal-free
panoptic segmentation methods on Cityscapes, COCO, and Mapillary Vistas.
Furthermore, our model demonstrates temporal stability between video frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kerola_T/0/1/0/all/0/1"&gt;Tommi Kerola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanehira_A/0/1/0/all/0/1"&gt;Atsushi Kanehira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kudo_Y/0/1/0/all/0/1"&gt;Yasunori Kudo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vallet_A/0/1/0/all/0/1"&gt;Alexis Vallet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1"&gt;Adrien Gaidon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Muddling Label Regularization: Deep Learning for Tabular Datasets. (arXiv:2106.04462v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04462</id>
        <link href="http://arxiv.org/abs/2106.04462"/>
        <updated>2021-06-09T02:01:49.857Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) is considered the state-of-the-art in computer vision,
speech recognition and natural language processing. Until recently, it was also
widely accepted that DL is irrelevant for learning tasks on tabular data,
especially in the small sample regime where ensemble methods are acknowledged
as the gold standard. We present a new end-to-end differentiable method to
train a standard FFNN. Our method, \textbf{Muddling labels for Regularization}
(\texttt{MLR}), penalizes memorization through the generation of uninformative
labels and the application of a differentiable close-form regularization scheme
on the last hidden layer during training. \texttt{MLR} outperforms classical NN
and the gold standard (GBDT, RF) for regression and classification tasks on
several datasets from the UCI database and Kaggle covering a large range of
sample sizes and feature to sample ratios. Researchers and practitioners can
use \texttt{MLR} on its own as an off-the-shelf \DL{} solution or integrate it
into the most advanced ML pipelines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lounici_K/0/1/0/all/0/1"&gt;Karim Lounici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meziani_K/0/1/0/all/0/1"&gt;Katia Meziani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riu_B/0/1/0/all/0/1"&gt;Benjamin Riu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-06-09T02:01:49.842Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Based Attention Through Internal Gating. (arXiv:2106.04540v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.04540</id>
        <link href="http://arxiv.org/abs/2106.04540"/>
        <updated>2021-06-09T02:01:49.837Z</updated>
        <summary type="html"><![CDATA[Object-based attention is a key component of the visual system, relevant for
perception, learning, and memory. Neurons tuned to features of attended objects
tend to be more active than those associated with non-attended objects. There
is a rich set of models of this phenomenon in computational neuroscience.
However, there is currently a divide between models that successfully match
physiological data but can only deal with extremely simple problems and models
of attention used in computer vision. For example, attention in the brain is
known to depend on top-down processing, whereas self-attention in deep learning
does not. Here, we propose an artificial neural network model of object-based
attention that captures the way in which attention is both top-down and
recurrent. Our attention model works well both on simple test stimuli, such as
those using images of handwritten digits, and on more complex stimuli, such as
natural images drawn from the COCO dataset. We find that our model replicates a
range of findings from neuroscience, including attention-invariant tuning,
inhibition of return, and attention-mediated scaling of activity. Understanding
object based attention is both computationally interesting and a key problem
for computational neuroscience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jordan Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Benjamin_A/0/1/0/all/0/1"&gt;Ari S. Benjamin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kording_K/0/1/0/all/0/1"&gt;Konrad P. Kording&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Image Reconstruction using Deep Generative Models. (arXiv:2012.04567v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04567</id>
        <link href="http://arxiv.org/abs/2012.04567"/>
        <updated>2021-06-09T02:01:49.831Z</updated>
        <summary type="html"><![CDATA[Machine learning models are commonly trained end-to-end and in a supervised
setting, using paired (input, output) data. Examples include recent
super-resolution methods that train on pairs of (low-resolution,
high-resolution) images. However, these end-to-end approaches require
re-training every time there is a distribution shift in the inputs (e.g., night
images vs daylight) or relevant latent variables (e.g., camera blur or hand
motion). In this work, we leverage state-of-the-art (SOTA) generative models
(here StyleGAN2) for building powerful image priors, which enable application
of Bayes' theorem for many downstream reconstruction tasks. Our method,
Bayesian Reconstruction through Generative Models (BRGM), uses a single
pre-trained generator model to solve different image restoration tasks, i.e.,
super-resolution and in-painting, by combining it with different forward
corruption models. We keep the weights of the generator model fixed, and
reconstruct the image by estimating the Bayesian maximum a-posteriori (MAP)
estimate over the input latent vector that generated the reconstructed image.
We further use variational inference to approximate the posterior distribution
over the latent vectors, from which we sample multiple solutions. We
demonstrate BRGM on three large and diverse datasets: (i) 60,000 images from
the Flick Faces High Quality dataset (ii) 240,000 chest X-rays from MIMIC III
and (iii) a combined collection of 5 brain MRI datasets with 7,329 scans.
Across all three datasets and without any dataset-specific hyperparameter
tuning, our simple approach yields performance competitive with current
task-specific state-of-the-art methods on super-resolution and in-painting,
while being more generalisable and without requiring any training. Our source
code and pre-trained models are available online:
https://razvanmarinescu.github.io/brgm/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marinescu_R/0/1/0/all/0/1"&gt;Razvan V Marinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1"&gt;Daniel Moyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Value-network Based Approach for Multi-Driver Order Dispatching. (arXiv:2106.04493v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04493</id>
        <link href="http://arxiv.org/abs/2106.04493"/>
        <updated>2021-06-09T02:01:49.825Z</updated>
        <summary type="html"><![CDATA[Recent works on ride-sharing order dispatching have highlighted the
importance of taking into account both the spatial and temporal dynamics in the
dispatching process for improving the transportation system efficiency. At the
same time, deep reinforcement learning has advanced to the point where it
achieves superhuman performance in a number of fields. In this work, we propose
a deep reinforcement learning based solution for order dispatching and we
conduct large scale online A/B tests on DiDi's ride-dispatching platform to
show that the proposed method achieves significant improvement on both total
driver income and user experience related metrics. In particular, we model the
ride dispatching problem as a Semi Markov Decision Process to account for the
temporal aspect of the dispatching actions. To improve the stability of the
value iteration with nonlinear function approximators like neural networks, we
propose Cerebellar Value Networks (CVNet) with a novel distributed state
representation layer. We further derive a regularized policy evaluation scheme
for CVNet that penalizes large Lipschitz constant of the value network for
additional robustness against adversarial perturbation and noises. Finally, we
adapt various transfer learning methods to CVNet for increased learning
adaptability and efficiency across multiple cities. We conduct extensive
offline simulations based on real dispatching data as well as online AB tests
through the DiDi's platform. Results show that CVNet consistently outperforms
other recently proposed dispatching methods. We finally show that the
performance can be further improved through the efficient use of transfer
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaodong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yintai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jieping Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Markov State Abstractions for Deep Reinforcement Learning. (arXiv:2106.04379v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04379</id>
        <link href="http://arxiv.org/abs/2106.04379"/>
        <updated>2021-06-09T02:01:49.808Z</updated>
        <summary type="html"><![CDATA[The fundamental assumption of reinforcement learning in Markov decision
processes (MDPs) is that the relevant decision process is, in fact, Markov.
However, when MDPs have rich observations, agents typically learn by way of an
abstract state representation, and such representations are not guaranteed to
preserve the Markov property. We introduce a novel set of conditions and prove
that they are sufficient for learning a Markov abstract state representation.
We then describe a practical training procedure that combines inverse model
estimation and temporal contrastive learning to learn an abstraction that
approximately satisfies these conditions. Our novel training objective is
compatible with both online and offline training: it does not require a reward
signal, but agents can capitalize on reward information when available. We
empirically evaluate our approach on a visual gridworld domain and a set of
continuous control benchmarks. Our approach learns representations that capture
the underlying structure of the domain and lead to improved sample efficiency
over state-of-the-art deep reinforcement learning with visual features -- often
matching or exceeding the performance achieved with hand-designed compact state
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1"&gt;Cameron Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_N/0/1/0/all/0/1"&gt;Neev Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottesman_O/0/1/0/all/0/1"&gt;Omer Gottesman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1"&gt;George Konidaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intelligent Hybrid Model for Identity Document Classification. (arXiv:2106.04345v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04345</id>
        <link href="http://arxiv.org/abs/2106.04345"/>
        <updated>2021-06-09T02:01:49.802Z</updated>
        <summary type="html"><![CDATA[Digitization, i.e., the process of converting information into a digital
format, may provide various opportunities (e.g., increase in productivity,
disaster recovery, and environmentally friendly solutions) and challenges for
businesses. In this context, one of the main challenges would be to accurately
classify numerous scanned documents uploaded every day by customers as usual
business processes. For example, processes in banking (e.g., applying for
loans) or the Government Registry of BDM (Births, Deaths, and Marriages)
applications may involve uploading several documents such as a driver's license
and passport. There are not many studies available to address the challenge as
an application of image classification. Although some studies are available
which used various methods, a more accurate model is still required. The
current study has proposed a robust fusion model to define the type of identity
documents accurately. The proposed approach is based on two different methods
in which images are classified based on their visual features and text
features. A novel model based on statistics and regression has been proposed to
calculate the confidence level for the feature-based classifier. A fuzzy-mean
fusion model has been proposed to combine the classifier results based on their
confidence score. The proposed approach has been implemented using Python and
experimentally validated on synthetic and real-world datasets. The performance
of the proposed model is evaluated using the Receiver Operating Characteristic
(ROC) curve analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khandan_N/0/1/0/all/0/1"&gt;Nouna Khandan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification. (arXiv:2106.04527v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04527</id>
        <link href="http://arxiv.org/abs/2106.04527"/>
        <updated>2021-06-09T02:01:49.796Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning has received a lot of recent attention as it
alleviates the need for large amounts of labelled data which can often be
expensive, requires expert knowledge and be time consuming to collect. Recent
developments in deep semi-supervised classification have reached unprecedented
performance and the gap between supervised and semi-supervised learning is
ever-decreasing. This improvement in performance has been based on the
inclusion of numerous technical tricks, strong augmentation techniques and
costly optimisation schemes with multi-term loss functions. We propose a new
framework, LaplaceNet, for deep semi-supervised classification that has a
greatly reduced model complexity. We utilise a hybrid energy-neural network
where graph based pseudo-labels, generated by minimising the graphical
Laplacian, are used to iteratively improve a neural-network backbone. Our model
outperforms state-of-the-art methods for deep semi-supervised classification,
over several benchmark datasets. Furthermore, we consider the application of
strong-augmentations to neural networks theoretically and justify the use of a
multi-sampling approach for semi-supervised learning. We demonstrate, through
rigorous experimentation, that a multi-sampling augmentation approach improves
generalisation and reduces the sensitivity of the network to augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sellars_P/0/1/0/all/0/1"&gt;Philip Sellars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1"&gt;Angelica I. Aviles-Rivero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Specific Causal Discovery for Categorical Data Using Staged Trees. (arXiv:2106.04416v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.04416</id>
        <link href="http://arxiv.org/abs/2106.04416"/>
        <updated>2021-06-09T02:01:49.790Z</updated>
        <summary type="html"><![CDATA[Causal discovery algorithms aims at untangling complex causal relationships
using observational data only. Here, we introduce new causal discovery
algorithms based on staged tree models, which can represent complex and
non-symmetric causal effects. To demonstrate the efficacy of our algorithms, we
introduce a new distance, inspired by the widely used structural interventional
distance, to quantify the closeness between two staged trees in terms of their
corresponding causal inference statements. A simulation study highlights the
efficacy of staged trees in uncovering complex, asymmetric causal relationship
from data and a real-world data application illustrates their use in a
practical causal analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Leonelli_M/0/1/0/all/0/1"&gt;Manuele Leonelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Varando_G/0/1/0/all/0/1"&gt;Gherardo Varando&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of marine floating plastic using Sentinel-2 imagery and machine learning models. (arXiv:2106.03694v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03694</id>
        <link href="http://arxiv.org/abs/2106.03694"/>
        <updated>2021-06-09T02:01:49.783Z</updated>
        <summary type="html"><![CDATA[The increasing level of marine plastic pollution poses severe threats to the
marine ecosystem and biodiversity. The present study attempted to explore the
full functionality of open Sentinel satellite data and ML models for detecting
and classifying floating plastic debris in Mytilene (Greece), Limassol
(Cyprus), Calabria (Italy), and Beirut (Lebanon). Two ML models, i.e. Support
Vector Machine (SVM) and Random Forest (RF) were utilized to carry out the
classification analysis. In-situ plastic location data was collected from the
control experiment conducted in Mytilene, Greece and Limassol, Cyprus, and the
same was considered for training the models. Both remote sensing bands and
spectral indices were used for developing the ML models. A spectral signature
profile for plastic was created for discriminating the floating plastic from
other marine debris. A newly developed index, kernel Normalized Difference
Vegetation Index (kNDVI), was incorporated into the modelling to examine its
contribution to model performances. Both SVM and RF were performed well in five
models and test case combinations. Among the two ML models, the highest
performance was measured for the RF. The inclusion of kNDVI was found effective
and increased the model performances, reflected by high balanced accuracy
measured for model 2 (~80% to ~98 % for SVM and ~87% to ~97 % for RF). Using
the best-performed model, an automated floating plastic detection system was
developed and tested in Calabria and Beirut. For both sites, the trained model
had detected the floating plastic with ~99% accuracy. Among the six predictors,
the FDI was found the most important variable for detecting marine floating
plastic. These findings collectively suggest that high-resolution remote
sensing imagery and the automated ML models can be an effective alternative for
the cost-effective detection of marine floating plastic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sannigrahi_S/0/1/0/all/0/1"&gt;Srikanta Sannigrahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_B/0/1/0/all/0/1"&gt;Bidroha Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1"&gt;Arunima Sarkar Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilla_F/0/1/0/all/0/1"&gt;Francesco Pilla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cheap and Good? Simple and Effective Data Augmentation for Low Resource Machine Reading. (arXiv:2106.04134v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04134</id>
        <link href="http://arxiv.org/abs/2106.04134"/>
        <updated>2021-06-09T02:01:49.777Z</updated>
        <summary type="html"><![CDATA[We propose a simple and effective strategy for data augmentation for
low-resource machine reading comprehension (MRC). Our approach first pretrains
the answer extraction components of a MRC system on the augmented data that
contains approximate context of the correct answers, before training it on the
exact answer spans. The approximate context helps the QA method components in
narrowing the location of the answers. We demonstrate that our simple strategy
substantially improves both document retrieval and answer extraction
performance by providing larger context of the answers and additional training
data. In particular, our method significantly improves the performance of BERT
based retriever (15.12\%), and answer extractor (4.33\% F1) on TechQA, a
complex, low-resource MRC task. Further, our data augmentation strategy yields
significant improvements of up to 3.9\% exact match (EM) and 2.7\% F1 for
answer extraction on PolicyQA, another practical but moderate sized QA dataset
that also contains long answer spans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1"&gt;Hoang Van&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_V/0/1/0/all/0/1"&gt;Vikas Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1"&gt;Mihai Surdeanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A critical look at the current train/test split in machine learning. (arXiv:2106.04525v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04525</id>
        <link href="http://arxiv.org/abs/2106.04525"/>
        <updated>2021-06-09T02:01:49.771Z</updated>
        <summary type="html"><![CDATA[The randomized or cross-validated split of training and testing sets has been
adopted as the gold standard of machine learning for decades. The establishment
of these split protocols are based on two assumptions: (i)-fixing the dataset
to be eternally static so we could evaluate different machine learning
algorithms or models; (ii)-there is a complete set of annotated data available
to researchers or industrial practitioners. However, in this article, we intend
to take a closer and critical look at the split protocol itself and point out
its weakness and limitation, especially for industrial applications. In many
real-world problems, we must acknowledge that there are numerous situations
where assumption (ii) does not hold. For instance, for interdisciplinary
applications like drug discovery, it often requires real lab experiments to
annotate data which poses huge costs in both time and financial considerations.
In other words, it can be very difficult or even impossible to satisfy
assumption (ii). In this article, we intend to access this problem and
reiterate the paradigm of active learning, and investigate its potential on
solving problems under unconventional train/test split protocols. We further
propose a new adaptive active learning architecture (AAL) which involves an
adaptation policy, in comparison with the traditional active learning that only
unidirectionally adds data points to the training pool. We primarily justify
our points by extensively investigating an interdisciplinary drug-protein
binding problem. We additionally evaluate AAL on more conventional machine
learning benchmarking datasets like CIFAR-10 to demonstrate the
generalizability and efficacy of the new framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1"&gt;Jimin Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Gang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jake Zhao&lt;/a&gt; (Junbo)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sketch-Based Streaming Anomaly Detection in Dynamic Graphs. (arXiv:2106.04486v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.04486</id>
        <link href="http://arxiv.org/abs/2106.04486"/>
        <updated>2021-06-09T02:01:49.765Z</updated>
        <summary type="html"><![CDATA[Given a stream of graph edges from a dynamic graph, how can we assign anomaly
scores to edges and subgraphs in an online manner, for the purpose of detecting
unusual behavior, using constant time and memory? For example, in intrusion
detection, existing work seeks to detect either anomalous edges or anomalous
subgraphs, but not both. In this paper, we first extend the count-min sketch
data structure to a higher-order sketch. This higher-order sketch has the
useful property of preserving the dense subgraph structure (dense subgraphs in
the input turn into dense submatrices in the data structure). We then propose
four online algorithms that utilize this enhanced data structure, which (a)
detect both edge and graph anomalies; (b) process each edge and graph in
constant memory and constant update time per newly arriving edge, and; (c)
outperform state-of-the-art baselines on four real-world datasets. Our method
is the first streaming approach that incorporates dense subgraph search to
detect graph anomalies in constant memory and time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1"&gt;Siddharth Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wadhwa_M/0/1/0/all/0/1"&gt;Mohit Wadhwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1"&gt;Bryan Hooi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Robustness of Neural Networks through Fourier Stabilization. (arXiv:2106.04435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04435</id>
        <link href="http://arxiv.org/abs/2106.04435"/>
        <updated>2021-06-09T02:01:49.728Z</updated>
        <summary type="html"><![CDATA[Despite the considerable success of neural networks in security settings such
as malware detection, such models have proved vulnerable to evasion attacks, in
which attackers make slight changes to inputs (e.g., malware) to bypass
detection. We propose a novel approach, \emph{Fourier stabilization}, for
designing evasion-robust neural networks with binary inputs. This approach,
which is complementary to other forms of defense, replaces the weights of
individual neurons with robust analogs derived using Fourier analytic tools.
The choice of which neurons to stabilize in a neural network is then a
combinatorial optimization problem, and we propose several methods for
approximately solving it. We provide a formal bound on the per-neuron drop in
accuracy due to Fourier stabilization, and experimentally demonstrate the
effectiveness of the proposed approach in boosting robustness of neural
networks in several detection settings. Moreover, we show that our approach
effectively composes with adversarial training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raviv_N/0/1/0/all/0/1"&gt;Netanel Raviv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelley_A/0/1/0/all/0/1"&gt;Aidan Kelley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Michael Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1"&gt;Yevgeny Vorobeychik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seismic Inverse Modeling Method based on Generative Adversarial Network. (arXiv:2106.04197v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04197</id>
        <link href="http://arxiv.org/abs/2106.04197"/>
        <updated>2021-06-09T02:01:49.714Z</updated>
        <summary type="html"><![CDATA[Seismic inverse modeling is a common method in reservoir prediction and it
plays a vital role in the exploration and development of oil and gas.
Conventional seismic inversion method is difficult to combine with complicated
and abstract knowledge on geological mode and its uncertainty is difficult to
be assessed. The paper proposes an inversion modeling method based on GAN
consistent with geology, well logs, seismic data. GAN is a the most promising
generation model algorithm that extracts spatial structure and abstract
features of training images. The trained GAN can reproduce the models with
specific mode. In our test, 1000 models were generated in 1 second. Based on
the trained GAN after assessment, the optimal result of models can be
calculated through Bayesian inversion frame. Results show that inversion models
conform to observation data and have a low uncertainty under the premise of
fast generation. This seismic inverse modeling method increases the efficiency
and quality of inversion iteration. It is worthy of studying and applying in
fusion of seismic data and geological knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengfei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yin_Y/0/1/0/all/0/1"&gt;YanShu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hou_J/0/1/0/all/0/1"&gt;JiaGen Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lixin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIPS-Plus: The Enhanced Database of Interacting Protein Structures for Interface Prediction. (arXiv:2106.04362v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.04362</id>
        <link href="http://arxiv.org/abs/2106.04362"/>
        <updated>2021-06-09T02:01:49.700Z</updated>
        <summary type="html"><![CDATA[How and where proteins interface with one another can ultimately impact the
proteins' functions along with a range of other biological processes. As such,
precise computational methods for protein interface prediction (PIP) come
highly sought after as they could yield significant advances in drug discovery
and design as well as protein function analysis. However, the traditional
benchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a
paltry 230 complexes for training, validating, and testing different machine
learning algorithms. In this work, we expand on a dataset recently introduced
for this task, the Database of Interacting Protein Structures (DIPS), to
present DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for
geometric deep learning of protein interfaces. The previous version of DIPS
contains only the Cartesian coordinates and types of the atoms comprising a
given protein complex, whereas DIPS-Plus now includes a plethora of new
residue-level features including protrusion indices, half-sphere amino acid
compositions, and new profile hidden Markov model (HMM)-based sequence features
for each amino acid, giving researchers a large, well-curated feature bank for
training protein interface prediction methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Morehead_A/0/1/0/all/0/1"&gt;Alex Morehead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sedova_A/0/1/0/all/0/1"&gt;Ada Sedova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jianlin Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chasing Sparsity in Vision Transformers:An End-to-End Exploration. (arXiv:2106.04533v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04533</id>
        <link href="http://arxiv.org/abs/2106.04533"/>
        <updated>2021-06-09T02:01:49.693Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViTs) have recently received explosive popularity, but
their enormous model sizes and training costs remain daunting. Conventional
post-training pruning often incurs higher training budgets. In contrast, this
paper aims to trim down both the training memory overhead and the inference
complexity, without scarifying the achievable accuracy. We launch and report
the first-of-its-kind comprehensive exploration, on taking a unified approach
of integrating sparsity in ViTs "from end to end". Specifically, instead of
training full ViTs, we dynamically extract and train sparse subnetworks, while
sticking to a fixed small parameter budget. Our approach jointly optimizes
model parameters and explores connectivity throughout training, ending up with
one sparse network as the final output. The approach is seamlessly extended
from unstructured to structured sparsity, the latter by considering to guide
the prune-and-grow of self-attention heads inside ViTs. For additional
efficiency gains, we further co-explore data and architecture sparsity, by
plugging in a novel learnable token selector to adaptively determine the
currently most vital patches. Extensive results validate the effectiveness of
our proposals on ImageNet with diverse ViT backbones. For instance, at 40%
structured sparsity, our sparsified DeiT-Base can achieve 0.42% accuracy gain,
at 33.13% and 24.70% running time} savings, compared to its dense counterpart.
Perhaps most surprisingly, we find that the proposed sparse (co-)training can
even improve the ViT accuracy rather than compromising it, making sparsity a
tantalizing "free lunch". For example, our sparsified DeiT-Small at 5%, 50%
sparsity for (data, architecture), improves 0.28% top-1 accuracy and meanwhile
enjoys 49.32% FLOPs and 4.40% running time savings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation. (arXiv:2106.04151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04151</id>
        <link href="http://arxiv.org/abs/2106.04151"/>
        <updated>2021-06-09T02:01:49.686Z</updated>
        <summary type="html"><![CDATA[Unsupervised Domain Adaptation (UDA) aims to generalize the knowledge learned
from a well-labeled source domain to an unlabeled target domain. Recently,
adversarial domain adaptation with two distinct classifiers (bi-classifier) has
been introduced into UDA which is effective to align distributions between
different domains. Previous bi-classifier adversarial learning methods only
focus on the similarity between the outputs of two distinct classifiers.
However, the similarity of the outputs cannot guarantee the accuracy of target
samples, i.e., target samples may match to wrong categories even if the
discrepancy between two classifiers is small. To challenge this issue, in this
paper, we propose a cross-domain gradient discrepancy minimization (CGDM)
method which explicitly minimizes the discrepancy of gradients generated by
source samples and target samples. Specifically, the gradient gives a cue for
the semantic information of target samples so it can be used as a good
supervision to improve the accuracy of target samples. In order to compute the
gradient signal of target samples, we further obtain target pseudo labels
through a clustering-based self-supervised learning. Extensive experiments on
three widely used UDA datasets show that our method surpasses many previous
state-of-the-arts. Codes are available at https://github.com/lijin118/CGDM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhekai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hongzu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation. (arXiv:2106.04399v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04399</id>
        <link href="http://arxiv.org/abs/2106.04399"/>
        <updated>2021-06-09T02:01:49.670Z</updated>
        <summary type="html"><![CDATA[This paper is about the problem of learning a stochastic policy for
generating an object (like a molecular graph) from a sequence of actions, such
that the probability of generating an object is proportional to a given
positive reward for that object. Whereas standard return maximization tends to
converge to a single return-maximizing sequence, there are cases where we would
like to sample a diverse set of high-return solutions. These arise, for
example, in black-box function optimization when few rounds are possible, each
with large batches of queries, where the batches should be diverse, e.g., in
the design of new molecules. One can also see this as a problem of
approximately converting an energy function to a generative distribution. While
MCMC methods can achieve that, they are expensive and generally only perform
local exploration. Instead, training a generative policy amortizes the cost of
search during training and yields to fast generation. Using insights from
Temporal Difference learning, we propose GFlowNet, based on a view of the
generative process as a flow network, making it possible to handle the tricky
case where different trajectories can yield the same final state, e.g., there
are many ways to sequentially add atoms to generate some molecular graph. We
cast the set of trajectories as a flow and convert the flow consistency
equations into a learning objective, akin to the casting of the Bellman
equations into Temporal Difference methods. We prove that any global minimum of
the proposed objectives yields a policy which samples from the desired
distribution, and demonstrate the improved performance and diversity of
GFlowNet on a simple domain where there are many modes to the reward function,
and on a molecule synthesis task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1"&gt;Emmanuel Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1"&gt;Moksh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korablyov_M/0/1/0/all/0/1"&gt;Maksym Korablyov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer-Assisted Analysis of Biomedical Images. (arXiv:2106.04381v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04381</id>
        <link href="http://arxiv.org/abs/2106.04381"/>
        <updated>2021-06-09T02:01:49.664Z</updated>
        <summary type="html"><![CDATA[Nowadays, the amount of heterogeneous biomedical data is increasing more and
more thanks to novel sensing techniques and high-throughput technologies. In
reference to biomedical image analysis, the advances in image acquisition
modalities and high-throughput imaging experiments are creating new challenges.
This huge information ensemble could overwhelm the analytic capabilities needed
by physicians in their daily decision-making tasks as well as by biologists
investigating complex biochemical systems. In particular, quantitative imaging
methods convey scientifically and clinically relevant information in
prediction, prognosis or treatment response assessment, by also considering
radiomics approaches. Therefore, the computational analysis of medical and
biological images plays a key role in radiology and laboratory applications. In
this regard, frameworks based on advanced Machine Learning and Computational
Intelligence can significantly improve traditional Image Processing and Pattern
Recognition approaches. However, conventional Artificial Intelligence
techniques must be tailored to address the unique challenges concerning
biomedical imaging data. This thesis aims at proposing novel and advanced
computer-assisted methods for biomedical image analysis, also as an instrument
in the development of Clinical Decision Support Systems, by always keeping in
mind the clinical feasibility of the developed solutions. In conclusion, the
ultimate goal of these research studies is to gain clinically and biologically
useful insights that can guide differential diagnosis and therapies, leading
towards biomedical data integration for personalized medicine. As a matter of
fact, the proposed computer-assisted bioimage analysis methods can be
beneficial for the definition of imaging biomarkers, as well as for
quantitative medicine and biology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1"&gt;Leonardo Rundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3KG: Contrastive Learning of 12-Lead Electrocardiograms using Physiologically-Inspired Augmentations. (arXiv:2106.04452v1 [physics.med-ph])]]></title>
        <id>http://arxiv.org/abs/2106.04452</id>
        <link href="http://arxiv.org/abs/2106.04452"/>
        <updated>2021-06-09T02:01:49.658Z</updated>
        <summary type="html"><![CDATA[Self-supervised contrastive learning approaches leverage modality-specific
context or invariances to pretrain models using unlabeled data. While
contrastive learning has demonstrated promising on results in the image domain,
there has been limited work on determining how to exploit modality-specific
invariances in biosignals such as the electrocardiogram. In this work, we
propose 3KG, a method to generate positive pairs for contrastive learning using
physiologically-inspired 3D augmentations of the 12-lead electrocardiogram. We
evaluate representation quality by fine-tuning a linear layer for the
downstream task of 24-class diagnosis on the PhysioNet 2020 challenge training
data, and find that models trained with physiologically-inspired augmentations
both outperform and complement standard time-series augmentations. Our best
performing strategy, which incorporates spatial rotation, spatial scaling, and
time masking, achieves a performance increase of 0.16, .086, and .046 in mean
AUROC over a randomly initialized baseline at 1%, 10%, and 100% label fractions
respectively. Additionally, we show that the strength of spatial augmentations
does not significantly affect the quality of the learned representations.
Finally, we investigate the clinical relevance of how physiologically-inspired
augmentations affect the performance of our classifier on different disease
subgroupings. As expert annotations are often expensive and scarce for medical
contexts, our approach highlights the potential of machine learning to tackle
medical problems with large quantities of unlabeled biosignal data by
exploiting their unique biological properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Gopal_B/0/1/0/all/0/1"&gt;Bryan Gopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Han_R/0/1/0/all/0/1"&gt;Ryan W. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Raghupathi_G/0/1/0/all/0/1"&gt;Gautham Raghupathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tison_G/0/1/0/all/0/1"&gt;Geoffrey H. Tison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating NODE with Pre-trained Neural Differential Operator for Learning Dynamics. (arXiv:2106.04166v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04166</id>
        <link href="http://arxiv.org/abs/2106.04166"/>
        <updated>2021-06-09T02:01:49.653Z</updated>
        <summary type="html"><![CDATA[Learning dynamics governed by differential equations is crucial for
predicting and controlling the systems in science and engineering. Neural
Ordinary Differential Equation (NODE), a deep learning model integrated with
differential equations, learns the dynamics directly from the samples on the
trajectory and shows great promise in the scientific field. However, the
training of NODE highly depends on the numerical solver, which can amplify
numerical noise and be unstable, especially for ill-conditioned dynamical
systems. In this paper, to reduce the reliance on the numerical solver, we
propose to enhance the supervised signal in learning dynamics. Specifically,
beyond learning directly from the trajectory samples, we pre-train a neural
differential operator (NDO) to output an estimation of the derivatives to serve
as an additional supervised signal. The NDO is pre-trained on a class of
symbolic functions, and it learns the mapping between the trajectory samples of
these functions to their derivatives. We provide theoretical guarantee on that
the output of NDO can well approximate the ground truth derivatives by proper
tuning the complexity of the library. To leverage both the trajectory signal
and the estimated derivatives from NDO, we propose an algorithm called
NDO-NODE, in which the loss function contains two terms: the fitness on the
true trajectory samples and the fitness on the estimated derivatives that are
output by the pre-trained NDO. Experiments on various of dynamics show that our
proposed NDO-NODE can consistently improve the forecasting accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1"&gt;Shiqi Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lijun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhi-Ming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer-Assisted Analysis of Biomedical Images. (arXiv:2106.04381v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04381</id>
        <link href="http://arxiv.org/abs/2106.04381"/>
        <updated>2021-06-09T02:01:49.645Z</updated>
        <summary type="html"><![CDATA[Nowadays, the amount of heterogeneous biomedical data is increasing more and
more thanks to novel sensing techniques and high-throughput technologies. In
reference to biomedical image analysis, the advances in image acquisition
modalities and high-throughput imaging experiments are creating new challenges.
This huge information ensemble could overwhelm the analytic capabilities needed
by physicians in their daily decision-making tasks as well as by biologists
investigating complex biochemical systems. In particular, quantitative imaging
methods convey scientifically and clinically relevant information in
prediction, prognosis or treatment response assessment, by also considering
radiomics approaches. Therefore, the computational analysis of medical and
biological images plays a key role in radiology and laboratory applications. In
this regard, frameworks based on advanced Machine Learning and Computational
Intelligence can significantly improve traditional Image Processing and Pattern
Recognition approaches. However, conventional Artificial Intelligence
techniques must be tailored to address the unique challenges concerning
biomedical imaging data. This thesis aims at proposing novel and advanced
computer-assisted methods for biomedical image analysis, also as an instrument
in the development of Clinical Decision Support Systems, by always keeping in
mind the clinical feasibility of the developed solutions. In conclusion, the
ultimate goal of these research studies is to gain clinically and biologically
useful insights that can guide differential diagnosis and therapies, leading
towards biomedical data integration for personalized medicine. As a matter of
fact, the proposed computer-assisted bioimage analysis methods can be
beneficial for the definition of imaging biomarkers, as well as for
quantitative medicine and biology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1"&gt;Leonardo Rundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Forest classifier for EEG-based seizure prediction. (arXiv:2106.04510v1 [physics.med-ph])]]></title>
        <id>http://arxiv.org/abs/2106.04510</id>
        <link href="http://arxiv.org/abs/2106.04510"/>
        <updated>2021-06-09T02:01:49.640Z</updated>
        <summary type="html"><![CDATA[Epileptic seizure prediction has gained considerable interest in the
computational Epilepsy research community. This paper presents a Machine
Learning based method for epileptic seizure prediction which outperforms
state-of-the art methods. We compute a probability for a given epoch, of being
pre-ictal against interictal using the Random Forest classifier and introduce
new concepts to enhance the robustness of the algorithm to false alarms. We
assessed our method on 20 patients of the benchmark scalp EEG CHB-MIT dataset
for a seizure prediction horizon (SPH) of 5 minutes and a seizure occurrence
period (SOP) of 30 minutes. Our approach achieves a sensitivity of 82.07 % and
a low false positive rate (FPR) of 0.0799 /h. We also tested our approach on
intracranial EEG recordings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Messaoud_R/0/1/0/all/0/1"&gt;Remy Ben Messaoud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chavez_M/0/1/0/all/0/1"&gt;Mario Chavez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient training for future video generation based on hierarchical disentangled representation of latent variables. (arXiv:2106.03502v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03502</id>
        <link href="http://arxiv.org/abs/2106.03502"/>
        <updated>2021-06-09T02:01:49.621Z</updated>
        <summary type="html"><![CDATA[Generating videos predicting the future of a given sequence has been an area
of active research in recent years. However, an essential problem remains
unsolved: most of the methods require large computational cost and memory usage
for training. In this paper, we propose a novel method for generating future
prediction videos with less memory usage than the conventional methods. This is
a critical stepping stone in the path towards generating videos with high image
quality, similar to that of generated images in the latest works in the field
of image generation. We achieve high-efficiency by training our method in two
stages: (1) image reconstruction to encode video frames into latent variables,
and (2) latent variable prediction to generate the future sequence. Our method
achieves an efficient compression of video into low-dimensional latent
variables by decomposing each frame according to its hierarchical structure.
That is, we consider that video can be separated into background and foreground
objects, and that each object holds time-varying and time-independent
information independently. Our experiments show that the proposed method can
efficiently generate future prediction videos, even for complex datasets that
cannot be handled by previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fushishita_N/0/1/0/all/0/1"&gt;Naoya Fushishita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tejero_de_Pablos_A/0/1/0/all/0/1"&gt;Antonio Tejero-de-Pablos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1"&gt;Yusuke Mukuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymmetrical Bi-RNN for pedestrian trajectory encoding. (arXiv:2106.04419v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04419</id>
        <link href="http://arxiv.org/abs/2106.04419"/>
        <updated>2021-06-09T02:01:49.615Z</updated>
        <summary type="html"><![CDATA[Pedestrian motion behavior involves a combination of individual goals and
social interactions with other agents. In this article, we present a
non-symmetrical bidirectional recurrent neural network architecture called
U-RNN as a sequence encoder and evaluate its relevance to replace LSTMs for
various forecasting models. Experimental results on the Trajnet++ benchmark
show that the U-LSTM variant can yield better results regarding every available
metric (ADE, FDE, Collision rate) than common LSTMs sequence encoders for a
variety of approaches and interaction modules.

Our implementation of the asymmetrical Bi-RNNs for the Trajnet++ benchmark is
available at:
github.com/JosephGesnouin/Asymmetrical-Bi-RNNs-to-encode-pedestrian-trajectories]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozenberg_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l Rozenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gesnouin_J/0/1/0/all/0/1"&gt;Joseph Gesnouin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1"&gt;Fabien Moutarde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harnessing Unrecognizable Faces for Face Recognition. (arXiv:2106.04112v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04112</id>
        <link href="http://arxiv.org/abs/2106.04112"/>
        <updated>2021-06-09T02:01:49.596Z</updated>
        <summary type="html"><![CDATA[The common implementation of face recognition systems as a cascade of a
detection stage and a recognition or verification stage can cause problems
beyond failures of the detector. When the detector succeeds, it can detect
faces that cannot be recognized, no matter how capable the recognition system.
Recognizability, a latent variable, should therefore be factored into the
design and implementation of face recognition systems. We propose a measure of
recognizability of a face image that leverages a key empirical observation: an
embedding of face images, implemented by a deep neural network trained using
mostly recognizable identities, induces a partition of the hypersphere whereby
unrecognizable identities cluster together. This occurs regardless of the
phenomenon that causes a face to be unrecognizable, it be optical or motion
blur, partial occlusion, spatial quantization, poor illumination. Therefore, we
use the distance from such an "unrecognizable identity" as a measure of
recognizability, and incorporate it in the design of the over-all system. We
show that accounting for recognizability reduces error rate of single-image
face recognition by 58% at FAR=1e-5 on the IJB-C Covariate Verification
benchmark, and reduces verification error rate by 24% at FAR=1e-5 in set-based
recognition on the IJB-C benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Siqi Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yuanjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Wei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closed-Form Analytical Results for Maximum Entropy Reinforcement Learning. (arXiv:2106.03931v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03931</id>
        <link href="http://arxiv.org/abs/2106.03931"/>
        <updated>2021-06-09T02:01:49.590Z</updated>
        <summary type="html"><![CDATA[We introduce a mapping between Maximum Entropy Reinforcement Learning (MaxEnt
RL) and Markovian processes conditioned on rare events. In the long time limit,
this mapping allows us to derive analytical expressions for the optimal policy,
dynamics and initial state distributions for the general case of stochastic
dynamics in MaxEnt RL. We find that soft-$\mathcal{Q}$ functions in MaxEnt RL
can be obtained from the Perron-Frobenius eigenvalue and the corresponding left
eigenvector of a regular, non-negative matrix derived from the underlying
Markov Decision Process (MDP). The results derived lead to novel algorithms for
model-based and model-free MaxEnt RL, which we validate by numerical
simulations. The mapping established in this work opens further avenues for the
application of novel analytical and computational approaches to problems in
MaxEnt RL. We make our code available at:
https://github.com/argearriojas/maxent-rl-mdp-scripts]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arriojas_A/0/1/0/all/0/1"&gt;Argenis Arriojas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiomkin_S/0/1/0/all/0/1"&gt;Stas Tiomkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1"&gt;Rahul V. Kulkarni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmenting Molecular Deep Generative Models with Topological Data Analysis Representations. (arXiv:2106.04464v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.04464</id>
        <link href="http://arxiv.org/abs/2106.04464"/>
        <updated>2021-06-09T02:01:49.584Z</updated>
        <summary type="html"><![CDATA[Deep generative models have emerged as a powerful tool for learning
informative molecular representations and designing novel molecules with
desired properties, with applications in drug discovery and material design.
Deep generative auto-encoders defined over molecular SMILES strings have been a
popular choice for that purpose. However, capturing salient molecular
properties like quantum-chemical energies remains challenging and requires
sophisticated neural net models of molecular graphs or geometry-based
information. As a simpler and more efficient alternative, we present a SMILES
Variational Auto-Encoder (VAE) augmented with topological data analysis (TDA)
representations of molecules, known as persistence images. Our experiments show
that this TDA augmentation enables a SMILES VAE to capture the complex relation
between 3D geometry and electronic properties, and allows generation of novel,
diverse, and valid molecules with geometric features consistent with the
training data, which exhibit a varying range of global electronic structural
properties, such as a small HOMO-LUMO gap - a critical property for designing
organic solar cells. We demonstrate that our TDA augmentation yields better
success in downstream tasks compared to models trained without these
representations and can assist in targeted molecule discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chenthamarakshan_V/0/1/0/all/0/1"&gt;Vijil Chenthamarakshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hoffman_S/0/1/0/all/0/1"&gt;Samuel Hoffman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ramamurthy_K/0/1/0/all/0/1"&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Tree Search Configuration: Cutting Planes and Beyond. (arXiv:2106.04033v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.04033</id>
        <link href="http://arxiv.org/abs/2106.04033"/>
        <updated>2021-06-09T02:01:49.568Z</updated>
        <summary type="html"><![CDATA[Cutting-plane methods have enabled remarkable successes in integer
programming over the last few decades. State-of-the-art solvers integrate a
myriad of cutting-plane techniques to speed up the underlying tree-search
algorithm used to find optimal solutions. In this paper we prove the first
guarantees for learning high-performing cut-selection policies tailored to the
instance distribution at hand using samples. We first bound the sample
complexity of learning cutting planes from the canonical family of
Chv\'atal-Gomory cuts. Our bounds handle any number of waves of any number of
cuts and are fine tuned to the magnitudes of the constraint coefficients. Next,
we prove sample complexity bounds for more sophisticated cut selection policies
that use a combination of scoring rules to choose from a family of cuts.
Finally, beyond the realm of cutting planes for integer programming, we develop
a general abstraction of tree search that captures key components such as node
selection and variable selection. For this abstraction, we bound the sample
complexity of learning a good policy for building the search tree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1"&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prasad_S/0/1/0/all/0/1"&gt;Siddharth Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandholm_T/0/1/0/all/0/1"&gt;Tuomas Sandholm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vitercik_E/0/1/0/all/0/1"&gt;Ellen Vitercik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discover the Unknown Biased Attribute of an Image Classifier. (arXiv:2104.14556v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14556</id>
        <link href="http://arxiv.org/abs/2104.14556"/>
        <updated>2021-06-09T02:01:49.563Z</updated>
        <summary type="html"><![CDATA[Recent works find that AI algorithms learn biases from data. Therefore, it is
urgent and vital to identify biases in AI algorithms. However, the previous
bias identification pipeline overly relies on human experts to conjecture
potential biases (e.g., gender), which may neglect other underlying biases not
realized by humans. To help human experts better find the AI algorithms'
biases, we study a new problem in this work -- for a classifier that predicts a
target attribute of the input image, discover its unknown biased attribute.

To solve this challenging problem, we use a hyperplane in the generative
model's latent space to represent an image attribute; thus, the original
problem is transformed to optimizing the hyperplane's normal vector and offset.
We propose a novel total-variation loss within this framework as the objective
function and a new orthogonalization penalty as a constraint. The latter
prevents trivial solutions in which the discovered biased attribute is
identical with the target or one of the known-biased attributes. Extensive
experiments on both disentanglement datasets and real-world datasets show that
our method can discover biased attributes and achieve better disentanglement
w.r.t. target attributes. Furthermore, the qualitative results show that our
method can discover unnoticeable biased attributes for various object and scene
classifiers, proving our method's generalizability for detecting biased
attributes in diverse domains of images. The code is available at
https://git.io/J3kMh.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenliang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Randomness of Input Data Spaces is an A Priori Predictor for Generalization. (arXiv:2106.04181v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04181</id>
        <link href="http://arxiv.org/abs/2106.04181"/>
        <updated>2021-06-09T02:01:49.558Z</updated>
        <summary type="html"><![CDATA[Over-parameterized models can perfectly learn various types of data
distributions, however, generalization error is usually lower for real data in
comparison to artificial data. This suggests that the properties of data
distributions have an impact on generalization capability. This work focuses on
the search space defined by the input data and assumes that the correlation
between labels of neighboring input values influences generalization. If
correlation is low, the randomness of the input data space is high leading to
high generalization error. We suggest to measure the randomness of an input
data space using Maurer's universal. Results for synthetic classification tasks
and common image classification benchmarks (MNIST, CIFAR10, and Microsoft's
cats vs. dogs data set) find a high correlation between the randomness of input
data spaces and the generalization error of deep neural networks for binary
classification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Briesch_M/0/1/0/all/0/1"&gt;Martin Briesch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sobania_D/0/1/0/all/0/1"&gt;Dominik Sobania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rothlauf_F/0/1/0/all/0/1"&gt;Franz Rothlauf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforced Few-Shot Acquisition Function Learning for Bayesian Optimization. (arXiv:2106.04335v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04335</id>
        <link href="http://arxiv.org/abs/2106.04335"/>
        <updated>2021-06-09T02:01:49.552Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization (BO) conventionally relies on handcrafted acquisition
functions (AFs) to sequentially determine the sample points. However, it has
been widely observed in practice that the best-performing AF in terms of regret
can vary significantly under different types of black-box functions. It has
remained a challenge to design one AF that can attain the best performance over
a wide variety of black-box functions. This paper aims to attack this challenge
through the perspective of reinforced few-shot AF learning (FSAF).
Specifically, we first connect the notion of AFs with Q-functions and view a
deep Q-network (DQN) as a surrogate differentiable AF. While it serves as a
natural idea to combine DQN and an existing few-shot learning method, we
identify that such a direct combination does not perform well due to severe
overfitting, which is particularly critical in BO due to the need of a
versatile sampling policy. To address this, we present a Bayesian variant of
DQN with the following three features: (i) It learns a distribution of
Q-networks as AFs based on the Kullback-Leibler regularization framework. This
inherently provides the uncertainty required in sampling for BO and mitigates
overfitting. (ii) For the prior of the Bayesian DQN, we propose to use a demo
policy induced by an off-the-shelf AF for better training stability. (iii) On
the meta-level, we leverage the meta-loss of Bayesian model-agnostic
meta-learning, which serves as a natural companion to the proposed FSAF.
Moreover, with the proper design of the Q-networks, FSAF is general-purpose in
that it is agnostic to the dimension and the cardinality of the input domain.
Through extensive experiments, we demonstrate that the FSAF achieves comparable
or better regrets than the state-of-the-art benchmarks on a wide variety of
synthetic and real-world test functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_B/0/1/0/all/0/1"&gt;Bing-Jing Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_P/0/1/0/all/0/1"&gt;Ping-Chun Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Fast Kernel Transform. (arXiv:2106.04487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04487</id>
        <link href="http://arxiv.org/abs/2106.04487"/>
        <updated>2021-06-09T02:01:49.541Z</updated>
        <summary type="html"><![CDATA[Kernel methods are a highly effective and widely used collection of modern
machine learning algorithms. A fundamental limitation of virtually all such
methods are computations involving the kernel matrix that naively scale
quadratically (e.g., constructing the kernel matrix and matrix-vector
multiplication) or cubically (solving linear systems) with the size of the data
set $N.$ We propose the Fast Kernel Transform (FKT), a general algorithm to
compute matrix-vector multiplications (MVMs) for datasets in moderate
dimensions with quasilinear complexity. Typically, analytically grounded fast
multiplication methods require specialized development for specific kernels. In
contrast, our scheme is based on auto-differentiation and automated symbolic
computations that leverage the analytical structure of the underlying kernel.
This allows the FKT to be easily applied to a broad class of kernels, including
Gaussian, Matern, and Rational Quadratic covariance functions and physically
motivated Green's functions, including those of the Laplace and Helmholtz
equations. Furthermore, the FKT maintains a high, quantifiable, and
controllable level of accuracy -- properties that many acceleration methods
lack. We illustrate the efficacy and versatility of the FKT by providing timing
and accuracy benchmarks and by applying it to scale the stochastic neighborhood
embedding (t-SNE) and Gaussian processes to large real-world data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ryan_J/0/1/0/all/0/1"&gt;John Paul Ryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ament_S/0/1/0/all/0/1"&gt;Sebastian Ament&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damle_A/0/1/0/all/0/1"&gt;Anil Damle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Neural Collaborative Filtering. (arXiv:2106.04405v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04405</id>
        <link href="http://arxiv.org/abs/2106.04405"/>
        <updated>2021-06-09T02:01:49.524Z</updated>
        <summary type="html"><![CDATA[In this work, we present a federated version of the state-of-the-art Neural
Collaborative Filtering (NCF) approach for item recommendations. The system,
named FedNCF, allows learning without requiring users to expose or transmit
their raw data. Experimental validation shows that FedNCF achieves comparable
recommendation quality to the original NCF system. Although federated learning
(FL) enables learning without raw data transmission, recent attacks showed that
FL alone does not eliminate privacy concerns. To overcome this challenge, we
integrate a privacy-preserving enhancement with a secure aggregation scheme
that satisfies the security requirements against an honest-but-curious (HBC)
entity, without affecting the quality of the original model. Finally, we
discuss the peculiarities observed in the application of FL in a collaborative
filtering (CF) task as well as we evaluate the privacy-preserving mechanism in
terms of computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perifanis_V/0/1/0/all/0/1"&gt;Vasileios Perifanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Efraimidis_P/0/1/0/all/0/1"&gt;Pavlos S. Efraimidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbalanced Optimal Transport through Non-negative Penalized Linear Regression. (arXiv:2106.04145v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.04145</id>
        <link href="http://arxiv.org/abs/2106.04145"/>
        <updated>2021-06-09T02:01:49.519Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of Unbalanced Optimal Transport (UOT) in
which the marginal conditions are relaxed (using weighted penalties in lieu of
equality) and no additional regularization is enforced on the OT plan. In this
context, we show that the corresponding optimization problem can be
reformulated as a non-negative penalized linear regression problem. This
reformulation allows us to propose novel algorithms inspired from inverse
problems and nonnegative matrix factorization. In particular, we consider
majorization-minimization which leads in our setting to efficient
multiplicative updates for a variety of penalties. Furthermore, we derive for
the first time an efficient algorithm to compute the regularization path of UOT
with quadratic penalties. The proposed algorithm provides a continuity of
piece-wise linear OT plans converging to the solution of balanced OT
(corresponding to infinite penalty weights). We perform several numerical
experiments on simulated and real data illustrating the new algorithms, and
provide a detailed discussion about more sophisticated optimization tools that
can further be used to solve OT problems thanks to our reformulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chapel_L/0/1/0/all/0/1"&gt;Laetitia Chapel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Flamary_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haoran Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fevotte_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric F&amp;#xe9;votte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PlayVirtual: Augmenting Cycle-Consistent Virtual Trajectories for Reinforcement Learning. (arXiv:2106.04152v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04152</id>
        <link href="http://arxiv.org/abs/2106.04152"/>
        <updated>2021-06-09T02:01:49.513Z</updated>
        <summary type="html"><![CDATA[Learning good feature representations is important for deep reinforcement
learning (RL). However, with limited experience, RL often suffers from data
inefficiency for training. For un-experienced or less-experienced trajectories
(i.e., state-action sequences), the lack of data limits the use of them for
better feature learning. In this work, we propose a novel method, dubbed
PlayVirtual, which augments cycle-consistent virtual trajectories to enhance
the data efficiency for RL feature representation learning. Specifically,
PlayVirtual predicts future states based on the current state and action by a
dynamics model and then predicts the previous states by a backward dynamics
model, which forms a trajectory cycle. Based on this, we augment the actions to
generate a large amount of virtual state-action trajectories. Being free of
groudtruth state supervision, we enforce a trajectory to meet the cycle
consistency constraint, which can significantly enhance the data efficiency. We
validate the effectiveness of our designs on the Atari and DeepMind Control
Suite benchmarks. Our method outperforms the current state-of-the-art methods
by a large margin on both benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1"&gt;Mingxiao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Machine Unlearning. (arXiv:2106.04378v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04378</id>
        <link href="http://arxiv.org/abs/2106.04378"/>
        <updated>2021-06-09T02:01:49.462Z</updated>
        <summary type="html"><![CDATA[Data deletion algorithms aim to remove the influence of deleted data points
from trained models at a cheaper computational cost than fully retraining those
models. However, for sequences of deletions, most prior work in the non-convex
setting gives valid guarantees only for sequences that are chosen independently
of the models that are published. If people choose to delete their data as a
function of the published models (because they don't like what the models
reveal about them, for example), then the update sequence is adaptive. In this
paper, we give a general reduction from deletion guarantees against adaptive
sequences to deletion guarantees against non-adaptive sequences, using
differential privacy and its connection to max information. Combined with ideas
from prior work which give guarantees for non-adaptive deletion sequences, this
leads to extremely flexible algorithms able to handle arbitrary model classes
and training methodologies, giving strong provable deletion guarantees for
adaptive deletion sequences. We show in theory how prior work for non-convex
models fails against adaptive deletion sequences, and use this intuition to
design a practical attack against the SISA algorithm of Bourtoule et al. [2021]
on CIFAR-10, MNIST, Fashion-MNIST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Varun Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1"&gt;Christopher Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neel_S/0/1/0/all/0/1"&gt;Seth Neel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharifi_Malvajerdi_S/0/1/0/all/0/1"&gt;Saeed Sharifi-Malvajerdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waites_C/0/1/0/all/0/1"&gt;Chris Waites&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation. (arXiv:2106.03907v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03907</id>
        <link href="http://arxiv.org/abs/2106.03907"/>
        <updated>2021-06-09T02:01:49.277Z</updated>
        <summary type="html"><![CDATA[Proxy causal learning (PCL) is a method for estimating the causal effect of
treatments on outcomes in the presence of unobserved confounding, using proxies
(structured side information) for the confounder. This is achieved via
two-stage regression: in the first stage, we model relations among the
treatment and proxies; in the second stage, we use this model to learn the
effect of treatment on the outcome, given the context provided by the proxies.
PCL guarantees recovery of the true causal effect, subject to identifiability
conditions. We propose a novel method for PCL, the deep feature proxy variable
method (DFPV), to address the case where the proxies, treatments, and outcomes
are high-dimensional and have nonlinear complex relationships, as represented
by deep neural network features. We show that DFPV outperforms recent
state-of-the-art PCL methods on challenging synthetic benchmarks, including
settings involving high dimensional image data. Furthermore, we show that PCL
can be applied to off-policy evaluation for the confounded bandit problem, in
which DFPV also exhibits competitive performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanagawa_H/0/1/0/all/0/1"&gt;Heishiro Kanagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuralFusion: Online Depth Fusion in Latent Space. (arXiv:2011.14791v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14791</id>
        <link href="http://arxiv.org/abs/2011.14791"/>
        <updated>2021-06-09T02:01:49.264Z</updated>
        <summary type="html"><![CDATA[We present a novel online depth map fusion approach that learns depth map
aggregation in a latent feature space. While previous fusion methods use an
explicit scene representation like signed distance functions (SDFs), we propose
a learned feature representation for the fusion. The key idea is a separation
between the scene representation used for the fusion and the output scene
representation, via an additional translator network. Our neural network
architecture consists of two main parts: a depth and feature fusion
sub-network, which is followed by a translator sub-network to produce the final
surface representation (e.g. TSDF) for visualization or other tasks. Our
approach is an online process, handles high noise levels, and is particularly
able to deal with gross outliers common for photometric stereo-based depth
maps. Experiments on real and synthetic data demonstrate improved results
compared to the state of the art, especially in challenging scenarios with
large amounts of noise and outliers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weder_S/0/1/0/all/0/1"&gt;Silvan Weder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonberger_J/0/1/0/all/0/1"&gt;Johannes L. Sch&amp;#xf6;nberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1"&gt;Martin R. Oswald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning by Distillation: A Self-Supervised Learning Framework for Optical Flow Estimation. (arXiv:2106.04195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04195</id>
        <link href="http://arxiv.org/abs/2106.04195"/>
        <updated>2021-06-09T02:01:49.262Z</updated>
        <summary type="html"><![CDATA[We present DistillFlow, a knowledge distillation approach to learning optical
flow. DistillFlow trains multiple teacher models and a student model, where
challenging transformations are applied to the input of the student model to
generate hallucinated occlusions as well as less confident predictions. Then, a
self-supervised learning framework is constructed: confident predictions from
teacher models are served as annotations to guide the student model to learn
optical flow for those less confident predictions. The self-supervised learning
framework enables us to effectively learn optical flow from unlabeled data, not
only for non-occluded pixels, but also for occluded pixels. DistillFlow
achieves state-of-the-art unsupervised learning performance on both KITTI and
Sintel datasets. Our self-supervised pre-trained model also provides an
excellent initialization for supervised fine-tuning, suggesting an alternate
training paradigm in contrast to current supervised learning methods that
highly rely on pre-training on synthetic data. At the time of writing, our
fine-tuned models ranked 1st among all monocular methods on the KITTI 2015
benchmark, and outperform all published methods on the Sintel Final benchmark.
More importantly, we demonstrate the generalization capability of DistillFlow
in three aspects: framework generalization, correspondence generalization and
cross-dataset generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengpeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael R. Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jia Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Space-time Video Super Resolution using Low-Resolution Flow and Mask Upsampling. (arXiv:2104.05778v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05778</id>
        <link href="http://arxiv.org/abs/2104.05778"/>
        <updated>2021-06-09T02:01:49.261Z</updated>
        <summary type="html"><![CDATA[This paper explores an efficient solution for Space-time Super-Resolution,
aiming to generate High-resolution Slow-motion videos from Low Resolution and
Low Frame rate videos. A simplistic solution is the sequential running of Video
Super Resolution and Video Frame interpolation models. However, this type of
solutions are memory inefficient, have high inference time, and could not make
the proper use of space-time relation property. To this extent, we first
interpolate in LR space using quadratic modeling. Input LR frames are
super-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps
and blending mask which are used to synthesize LR interpolated frame is reused
in HR space using bilinear upsampling. This leads to a coarse estimate of HR
intermediate frame which often contains artifacts along motion boundaries. We
use a refinement network to improve the quality of HR intermediate frame via
residual learning. Our model is lightweight and performs better than current
state-of-the-art models in REDS STSR Validation set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Saikat Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shah_N/0/1/0/all/0/1"&gt;Nisarg A. Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anurag Mittal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPANet: Generalized Permutationless Set Assignment for Particle Physics using Symmetry Preserving Attention. (arXiv:2106.03898v1 [hep-ex])]]></title>
        <id>http://arxiv.org/abs/2106.03898</id>
        <link href="http://arxiv.org/abs/2106.03898"/>
        <updated>2021-06-09T02:01:49.246Z</updated>
        <summary type="html"><![CDATA[The creation of unstable heavy particles at the Large Hadron Collider is the
most direct way to address some of the deepest open questions in physics.
Collisions typically produce variable-size sets of observed particles which
have inherent ambiguities complicating the assignment of observed particles to
the decay products of the heavy particles. Current strategies for tackling
these challenges in the physics community ignore the physical symmetries of the
decay products and consider all possible assignment permutations and do not
scale to complex configurations. Attention based deep learning methods for
sequence modelling have achieved state-of-the-art performance in natural
language processing, but they lack built-in mechanisms to deal with the unique
symmetries found in physical set-assignment problems. We introduce a novel
method for constructing symmetry-preserving attention networks which reflect
the problem's natural invariances to efficiently find assignments without
evaluating all permutations. This general approach is applicable to arbitrarily
complex configurations and significantly outperforms current methods, improving
reconstruction efficiency between 19\% - 35\% on typical benchmark problems
while decreasing inference time by two to five orders of magnitude on the most
complex events, making many important and previously intractable cases
tractable.

A full code repository containing a general library, the specific
configuration used, and a complete dataset release, are avaiable at
https://github.com/Alexanders101/SPANet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ex/1/au:+Shmakov_A/0/1/0/all/0/1"&gt;Alexander Shmakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Fenton_M/0/1/0/all/0/1"&gt;Michael James Fenton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Ho_T/0/1/0/all/0/1"&gt;Ta-Wei Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Hsu_S/0/1/0/all/0/1"&gt;Shih-Chieh Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Whiteson_D/0/1/0/all/0/1"&gt;Daniel Whiteson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Baldi_P/0/1/0/all/0/1"&gt;Pierre Baldi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On $w$-mixtures: Finite convex combinations of prescribed component distributions. (arXiv:1708.00568v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1708.00568</id>
        <link href="http://arxiv.org/abs/1708.00568"/>
        <updated>2021-06-09T02:01:49.225Z</updated>
        <summary type="html"><![CDATA[We consider the space of $w$-mixtures which is defined as the set of finite
statistical mixtures sharing the same prescribed component distributions closed
under convex combinations. The information geometry induced by the Bregman
generator set to the Shannon negentropy on this space yields a dually flat
space called the mixture family manifold. We show how the Kullback-Leibler (KL)
divergence can be recovered from the corresponding Bregman divergence for the
negentropy generator: That is, the KL divergence between two $w$-mixtures
amounts to a Bregman Divergence (BD) induced by the Shannon negentropy
generator. Thus the KL divergence between two Gaussian Mixture Models (GMMs)
sharing the same Gaussian components is equivalent to a Bregman divergence.
This KL-BD equivalence on a mixture family manifold implies that we can perform
optimal KL-averaging aggregation of $w$-mixtures without information loss. More
generally, we prove that the statistical skew Jensen-Shannon divergence between
$w$-mixtures is equivalent to a skew Jensen divergence between their
corresponding parameters. Finally, we state several properties, divergence
identities, and inequalities relating to $w$-mixtures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1"&gt;Richard Nock&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Occode: an end-to-end machine learning pipeline for transcription of historical population censuses. (arXiv:2106.03996v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03996</id>
        <link href="http://arxiv.org/abs/2106.03996"/>
        <updated>2021-06-09T02:01:49.197Z</updated>
        <summary type="html"><![CDATA[Machine learning approaches achieve high accuracy for text recognition and
are therefore increasingly used for the transcription of handwritten historical
sources. However, using machine learning in production requires a streamlined
end-to-end machine learning pipeline that scales to the dataset size, and a
model that achieves high accuracy with few manual transcriptions. In addition,
the correctness of the model results must be verified. This paper describes our
lessons learned developing, tuning, and using the Occode end-to-end machine
learning pipeline for transcribing 7,3 million rows with handwritten occupation
codes in the Norwegian 1950 population census. We achieve an accuracy of 97%
for the automatically transcribed codes, and we send 3% of the codes for manual
verification. We verify that the occupation code distribution found in our
result matches the distribution found in our training data which should be
representative for the census as a whole. We believe our approach and lessons
learned are useful for other transcription projects that plan to use machine
learning in production. The source code is available at:
https://github.com/uit-hdl/rhd-codes]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pedersen_B/0/1/0/all/0/1"&gt;Bj&amp;#xf8;rn-Richard Pedersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holsbo_E/0/1/0/all/0/1"&gt;Einar Holsb&amp;#xf8;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andersen_T/0/1/0/all/0/1"&gt;Trygve Andersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shvetsov_N/0/1/0/all/0/1"&gt;Nikita Shvetsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravn_J/0/1/0/all/0/1"&gt;Johan Ravn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommerseth_H/0/1/0/all/0/1"&gt;Hilde Leikny Sommerseth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bongo_L/0/1/0/all/0/1"&gt;Lars Ailo Bongo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speedy Performance Estimation for Neural Architecture Search. (arXiv:2006.04492v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04492</id>
        <link href="http://arxiv.org/abs/2006.04492"/>
        <updated>2021-06-09T02:01:49.141Z</updated>
        <summary type="html"><![CDATA[Reliable yet efficient evaluation of generalisation performance of a proposed
architecture is crucial to the success of neural architecture search (NAS).
Traditional approaches face a variety of limitations: training each
architecture to completion is prohibitively expensive, early stopped validation
accuracy may correlate poorly with fully trained performance, and model-based
estimators require large training sets. We instead propose to estimate the
final test performance based on a simple measure of training speed. Our
estimator is theoretically motivated by the connection between generalisation
and training speed, and is also inspired by the reformulation of a PAC-Bayes
bound under the Bayesian setting. Our model-free estimator is simple,
efficient, and cheap to implement, and does not require hyperparameter-tuning
or surrogate training before deployment. We demonstrate on various NAS search
spaces that our estimator consistently outperforms other alternatives in
achieving better correlation with the true test performance rankings. We
further show that our estimator can be easily incorporated into both
query-based and one-shot NAS methods to improve the speed or quality of the
search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ru_B/0/1/0/all/0/1"&gt;Binxin Ru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lyle_C/0/1/0/all/0/1"&gt;Clare Lyle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schut_L/0/1/0/all/0/1"&gt;Lisa Schut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fil_M/0/1/0/all/0/1"&gt;Miroslav Fil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1"&gt;Mark van der Wilk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-output Gaussian Processes for Uncertainty-aware Recommender Systems. (arXiv:2106.04221v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04221</id>
        <link href="http://arxiv.org/abs/2106.04221"/>
        <updated>2021-06-09T02:01:49.089Z</updated>
        <summary type="html"><![CDATA[Recommender systems are often designed based on a collaborative filtering
approach, where user preferences are predicted by modelling interactions
between users and items. Many common approaches to solve the collaborative
filtering task are based on learning representations of users and items,
including simple matrix factorization, Gaussian process latent variable models,
and neural-network based embeddings. While matrix factorization approaches fail
to model nonlinear relations, neural networks can potentially capture such
complex relations with unprecedented predictive power and are highly scalable.
However, neither of them is able to model predictive uncertainties. In
contrast, Gaussian Process based models can generate a predictive distribution,
but cannot scale to large amounts of data. In this manuscript, we propose a
novel approach combining the representation learning paradigm of collaborative
filtering with multi-output Gaussian processes in a joint framework to generate
uncertainty-aware recommendations. We introduce an efficient strategy for model
training and inference, resulting in a model that scales to very large and
sparse datasets and achieves competitive performance in terms of classical
metrics quantifying the reconstruction error. In addition to accurately
predicting user preferences, our model also provides meaningful uncertainty
estimates about that prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinchong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buettner_F/0/1/0/all/0/1"&gt;Florian Buettner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Statistical Arbitrage. (arXiv:2106.04028v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04028</id>
        <link href="http://arxiv.org/abs/2106.04028"/>
        <updated>2021-06-09T02:01:49.045Z</updated>
        <summary type="html"><![CDATA[Statistical arbitrage identifies and exploits temporal price differences
between similar assets. We propose a unifying conceptual framework for
statistical arbitrage and develop a novel deep learning solution, which finds
commonality and time-series patterns from large panels in a data-driven and
flexible way. First, we construct arbitrage portfolios of similar assets as
residual portfolios from conditional latent asset pricing factors. Second, we
extract the time series signals of these residual portfolios with one of the
most powerful machine learning time-series solutions, a convolutional
transformer. Last, we use these signals to form an optimal trading policy, that
maximizes risk-adjusted returns under constraints. We conduct a comprehensive
empirical comparison study with daily large cap U.S. stocks. Our optimal
trading strategy obtains a consistently high out-of-sample Sharpe ratio and
substantially outperforms all benchmark approaches. It is orthogonal to common
risk factors, and exploits asymmetric local trend and reversion patterns. Our
strategies remain profitable after taking into account trading frictions and
costs. Our findings suggest a high compensation for arbitrageurs to enforce the
law of one price.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guijarro_Ordonez_J/0/1/0/all/0/1"&gt;Jorge Guijarro-Ordonez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelger_M/0/1/0/all/0/1"&gt;Markus Pelger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanotti_G/0/1/0/all/0/1"&gt;Greg Zanotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Widening Access to Applied Machine Learning with TinyML. (arXiv:2106.04008v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04008</id>
        <link href="http://arxiv.org/abs/2106.04008"/>
        <updated>2021-06-09T02:01:49.039Z</updated>
        <summary type="html"><![CDATA[Broadening access to both computational and educational resources is critical
to diffusing machine-learning (ML) innovation. However, today, most ML
resources and experts are siloed in a few countries and organizations. In this
paper, we describe our pedagogical approach to increasing access to applied ML
through a massive open online course (MOOC) on Tiny Machine Learning (TinyML).
We suggest that TinyML, ML on resource-constrained embedded devices, is an
attractive means to widen access because TinyML both leverages low-cost and
globally accessible hardware, and encourages the development of complete,
self-contained applications, from data collection to deployment. To this end, a
collaboration between academia (Harvard University) and industry (Google)
produced a four-part MOOC that provides application-oriented instruction on how
to develop solutions using TinyML. The series is openly available on the edX
MOOC platform, has no prerequisites beyond basic programming, and is designed
for learners from a global variety of backgrounds. It introduces pupils to
real-world applications, ML algorithms, data-set engineering, and the ethical
considerations of these technologies via hands-on programming and deployment of
TinyML applications in both the cloud and their own microcontrollers. To
facilitate continued learning, community building, and collaboration beyond the
courses, we launched a standalone website, a forum, a chat, and an optional
course-project competition. We also released the course materials publicly,
hoping they will inspire the next generation of ML practitioners and educators
and further broaden access to cutting-edge ML technologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1"&gt;Vijay Janapa Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plancher_B/0/1/0/all/0/1"&gt;Brian Plancher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_S/0/1/0/all/0/1"&gt;Susan Kennedy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moroney_L/0/1/0/all/0/1"&gt;Laurence Moroney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1"&gt;Pete Warden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Anant Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1"&gt;Colby Banbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banzi_M/0/1/0/all/0/1"&gt;Massimo Banzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_M/0/1/0/all/0/1"&gt;Matthew Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_B/0/1/0/all/0/1"&gt;Benjamin Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitlangia_S/0/1/0/all/0/1"&gt;Sharad Chitlangia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_R/0/1/0/all/0/1"&gt;Radhika Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grafman_S/0/1/0/all/0/1"&gt;Sarah Grafman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaeger_R/0/1/0/all/0/1"&gt;Rupert Jaeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1"&gt;Srivatsan Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1"&gt;Maximilian Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leiker_D/0/1/0/all/0/1"&gt;Daniel Leiker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mann_C/0/1/0/all/0/1"&gt;Cara Mann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumder_M/0/1/0/all/0/1"&gt;Mark Mazumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajak_D/0/1/0/all/0/1"&gt;Dominic Pajak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramaprasad_D/0/1/0/all/0/1"&gt;Dhilan Ramaprasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1"&gt;J. Evan Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stewart_M/0/1/0/all/0/1"&gt;Matthew Stewart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tingley_D/0/1/0/all/0/1"&gt;Dustin Tingley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A self consistent theory of Gaussian Processes captures feature learning effects in finite CNNs. (arXiv:2106.04110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04110</id>
        <link href="http://arxiv.org/abs/2106.04110"/>
        <updated>2021-06-09T02:01:49.028Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) in the infinite width/channel limit have received
much attention recently, as they provide a clear analytical window to deep
learning via mappings to Gaussian Processes (GPs). Despite its theoretical
appeal, this viewpoint lacks a crucial ingredient of deep learning in finite
DNNs, laying at the heart of their success -- feature learning. Here we
consider DNNs trained with noisy gradient descent on a large training set and
derive a self consistent Gaussian Process theory accounting for strong
finite-DNN and feature learning effects. Applying this to a toy model of a
two-layer linear convolutional neural network (CNN) shows good agreement with
experiments. We further identify, both analytical and numerically, a sharp
transition between a feature learning regime and a lazy learning regime in this
model. Strong finite-DNN effects are also derived for a non-linear two-layer
fully connected network. Our self consistent theory provides a rich and
versatile analytical framework for studying feature learning and other non-lazy
effects in finite DNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naveh_G/0/1/0/all/0/1"&gt;Gadi Naveh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ringel_Z/0/1/0/all/0/1"&gt;Zohar Ringel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Label Cleaning with Example-based Explanations. (arXiv:2106.03922v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03922</id>
        <link href="http://arxiv.org/abs/2106.03922"/>
        <updated>2021-06-09T02:01:49.017Z</updated>
        <summary type="html"><![CDATA[We tackle sequential learning under label noise in applications where a human
supervisor can be queried to relabel suspicious examples. Existing approaches
are flawed, in that they only relabel incoming examples that look
``suspicious'' to the model. As a consequence, those mislabeled examples that
elude (or don't undergo) this cleaning step end up tainting the training data
and the model with no further chance of being cleaned. We propose Cincer, a
novel approach that cleans both new and past data by identifying pairs of
mutually incompatible examples. Whenever it detects a suspicious example,
Cincer identifies a counter-example in the training set that -- according to
the model -- is maximally incompatible with the suspicious example, and asks
the annotator to relabel either or both examples, resolving this possible
inconsistency. The counter-examples are chosen to be maximally incompatible, so
to serve as explanations of the model' suspicion, and highly influential, so to
convey as much information as possible if relabeled. Cincer achieves this by
leveraging an efficient and robust approximation of influence functions based
on the Fisher information matrix (FIM). Our extensive empirical evaluation
shows that clarifying the reasons behind the model's suspicions by cleaning the
counter-examples helps acquiring substantially better data and models,
especially when paired with our FIM approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teso_S/0/1/0/all/0/1"&gt;Stefano Teso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bontempelli_A/0/1/0/all/0/1"&gt;Andrea Bontempelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1"&gt;Fausto Giunchiglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1"&gt;Andrea Passerini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Curriculum Learning. (arXiv:2106.04072v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.04072</id>
        <link href="http://arxiv.org/abs/2106.04072"/>
        <updated>2021-06-09T02:01:49.010Z</updated>
        <summary type="html"><![CDATA[When faced with learning challenging new tasks, humans often follow sequences
of steps that allow them to incrementally build up the necessary skills for
performing these new tasks. However, in machine learning, models are most often
trained to solve the target tasks directly.Inspired by human learning, we
propose a novel curriculum learning approach which decomposes challenging tasks
into sequences of easier intermediate goals that are used to pre-train a model
before tackling the target task. We focus on classification tasks, and design
the intermediate tasks using an automatically constructed label hierarchy. We
train the model at each level of the hierarchy, from coarse labels to fine
labels, transferring acquired knowledge across these levels. For instance, the
model will first learn to distinguish animals from objects, and then use this
acquired knowledge when learning to classify among more fine-grained classes
such as cat, dog, car, and truck. Most existing curriculum learning algorithms
for supervised learning consist of scheduling the order in which the training
examples are presented to the model. In contrast, our approach focuses on the
output space of the model. We evaluate our method on several established
datasets and show significant performance gains especially on classification
problems with many labels. We also evaluate on a new synthetic dataset which
allows us to study multiple aspects of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stretcu_O/0/1/0/all/0/1"&gt;Otilia Stretcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Platanios_E/0/1/0/all/0/1"&gt;Emmanouil Antonios Platanios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1"&gt;Tom M. Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1"&gt;Barnab&amp;#xe1;s P&amp;#xf3;czos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amortized Generation of Sequential Counterfactual Explanations for Black-box Models. (arXiv:2106.03962v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03962</id>
        <link href="http://arxiv.org/abs/2106.03962"/>
        <updated>2021-06-09T02:01:48.994Z</updated>
        <summary type="html"><![CDATA[Explainable machine learning (ML) has gained traction in recent years due to
the increasing adoption of ML-based systems in many sectors. Counterfactual
explanations (CFEs) provide ``what if'' feedback of the form ``if an input
datapoint were $x'$ instead of $x$, then an ML-based system's output would be
$y'$ instead of $y$.'' CFEs are attractive due to their actionable feedback,
amenability to existing legal frameworks, and fidelity to the underlying ML
model. Yet, current CFE approaches are single shot -- that is, they assume $x$
can change to $x'$ in a single time period. We propose a novel
stochastic-control-based approach that generates sequential CFEs, that is, CFEs
that allow $x$ to move stochastically and sequentially across intermediate
states to a final state $x'$. Our approach is model agnostic and black box.
Furthermore, calculation of CFEs is amortized such that once trained, it
applies to multiple datapoints without the need for re-optimization. In
addition to these primary characteristics, our approach admits optional
desiderata such as adherence to the data manifold, respect for causal
relations, and sparsity -- identified by past research as desirable properties
of CFEs. We evaluate our approach using three real-world datasets and show
successful generation of sequential CFEs that respect other counterfactual
desiderata.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sahil Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hines_K/0/1/0/all/0/1"&gt;Keegan Hines&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1"&gt;John P. Dickerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double Descent and Other Interpolation Phenomena in GANs. (arXiv:2106.04003v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04003</id>
        <link href="http://arxiv.org/abs/2106.04003"/>
        <updated>2021-06-09T02:01:48.989Z</updated>
        <summary type="html"><![CDATA[We study overparameterization in generative adversarial networks (GANs) that
can interpolate the training data. We show that overparameterization can
improve generalization performance and accelerate the training process. We
study the generalization error as a function of latent space dimension and
identify two main behaviors, depending on the learning setting. First, we show
that overparameterized generative models that learn distributions by minimizing
a metric or $f$-divergence do not exhibit double descent in generalization
errors; specifically, all the interpolating solutions achieve the same
generalization error. Second, we develop a new pseudo-supervised learning
approach for GANs where the training utilizes pairs of fabricated (noise)
inputs in conjunction with real output samples. Our pseudo-supervised setting
exhibits double descent (and in some cases, triple descent) of generalization
errors. We combine pseudo-supervision with overparameterization (i.e., overly
large latent space dimension) to accelerate training while performing better,
or close to, the generalization performance without pseudo-supervision. While
our analysis focuses mostly on linear GANs, we also apply important insights
for improving generalization of nonlinear, multilayer GANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luzi_L/0/1/0/all/0/1"&gt;Lorenzo Luzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dar_Y/0/1/0/all/0/1"&gt;Yehuda Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Abstractive Unsupervised Summarization of Online News Discussions. (arXiv:2106.03953v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03953</id>
        <link href="http://arxiv.org/abs/2106.03953"/>
        <updated>2021-06-09T02:01:48.982Z</updated>
        <summary type="html"><![CDATA[Summarization has usually relied on gold standard summaries to train
extractive or abstractive models. Social media brings a hurdle to summarization
techniques since it requires addressing a multi-document multi-author approach.
We address this challenging task by introducing a novel method that generates
abstractive summaries of online news discussions. Our method extends a
BERT-based architecture, including an attention encoding that fed comments'
likes during the training stage. To train our model, we define a task which
consists of reconstructing high impact comments based on popularity (likes).
Accordingly, our model learns to summarize online discussions based on their
most relevant comments. Our novel approach provides a summary that represents
the most relevant aspects of a news item that users comment on, incorporating
the social context as a source of information to summarize texts in online
social networks. Our model is evaluated using ROUGE scores between the
generated summary and each comment on the thread. Our model, including the
social attention encoding, significantly outperforms both extractive and
abstractive summarization methods based on such evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palma_I/0/1/0/all/0/1"&gt;Ignacio Tampe Palma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1"&gt;Marcelo Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1"&gt;Evangelos Milios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention. (arXiv:2106.04133v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.04133</id>
        <link href="http://arxiv.org/abs/2106.04133"/>
        <updated>2021-06-09T02:01:48.970Z</updated>
        <summary type="html"><![CDATA[Emotion recognition from speech is a challenging task. Re-cent advances in
deep learning have led bi-directional recur-rent neural network (Bi-RNN) and
attention mechanism as astandard method for speech emotion recognition,
extractingand attending multi-modal features - audio and text, and thenfusing
them for downstream emotion classification tasks. Inthis paper, we propose a
simple yet efficient neural networkarchitecture to exploit both acoustic and
lexical informationfrom speech. The proposed framework using multi-scale
con-volutional layers (MSCNN) to obtain both audio and text hid-den
representations. Then, a statistical pooling unit (SPU)is used to further
extract the features in each modality. Be-sides, an attention module can be
built on top of the MSCNN-SPU (audio) and MSCNN (text) to further improve the
perfor-mance. Extensive experiments show that the proposed modeloutperforms
previous state-of-the-art methods on IEMOCAPdataset with four emotion
categories (i.e., angry, happy, sadand neutral) in both weighted accuracy (WA)
and unweightedaccuracy (UA), with an improvement of 5.0% and 5.2% respectively
under the ASR setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zixuan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shengfeng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yunfeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulated Adversarial Testing of Face Recognition Models. (arXiv:2106.04569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04569</id>
        <link href="http://arxiv.org/abs/2106.04569"/>
        <updated>2021-06-09T02:01:48.963Z</updated>
        <summary type="html"><![CDATA[Most machine learning models are validated and tested on fixed datasets. This
can give an incomplete picture of the capabilities and weaknesses of the model.
Such weaknesses can be revealed at test time in the real world. The risks
involved in such failures can be loss of profits, loss of time or even loss of
life in certain critical applications. In order to alleviate this issue,
simulators can be controlled in a fine-grained manner using interpretable
parameters to explore the semantic image manifold. In this work, we propose a
framework for learning how to test machine learning algorithms using simulators
in an adversarial manner in order to find weaknesses in the model before
deploying it in critical scenarios. We apply this model in a face recognition
scenario. We are the first to show that weaknesses of models trained on real
data can be discovered using simulated samples. Using our proposed method, we
can find adversarial synthetic faces that fool contemporary face recognition
models. This demonstrates the fact that these models have weaknesses that are
not measured by commonly used validation datasets. We hypothesize that this
type of adversarial examples are not isolated, but usually lie in connected
components in the latent space of the simulator. We present a method to find
these adversarial regions as opposed to the typical adversarial points found in
the adversarial example literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1"&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1"&gt;Adam Kortylewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1"&gt;Weichao Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cihang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1"&gt;Sarah Adel Bargal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1"&gt;Stan Sclaroff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoPtosis. (arXiv:2106.03905v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.03905</id>
        <link href="http://arxiv.org/abs/2106.03905"/>
        <updated>2021-06-09T02:01:48.945Z</updated>
        <summary type="html"><![CDATA[Blepharoptosis, or ptosis as it is more commonly referred to, is a condition
of the eyelid where the upper eyelid droops. The current diagnosis for ptosis
involves cumbersome manual measurements that are time-consuming and prone to
human error. In this paper, we present AutoPtosis, an artificial intelligence
based system with interpretable results for rapid diagnosis of ptosis. We
utilize a diverse dataset collected at the University of Illinois Hospital and
Health to successfully develop a robust deep learning model for prediction and
also develop a clinically inspired model that calculates the marginal reflex
distance and iris ratio. AutoPtosis achieved 95.5% accuracy on physician
verified data that had an equal class balance. The proposed algorithm can help
in the rapid and timely diagnosis of ptosis, significantly reduce the burden on
the healthcare system, and save the patients and clinics valuable resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Aleem_A/0/1/0/all/0/1"&gt;Abdullah Aleem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nallabothula_M/0/1/0/all/0/1"&gt;Manoj Prabhakar Nallabothula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Setabutr_P/0/1/0/all/0/1"&gt;Pete Setabutr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hallak_J/0/1/0/all/0/1"&gt;Joelle A. Hallak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yi_D/0/1/0/all/0/1"&gt;Darvin Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Flows with Invertible Attentions. (arXiv:2106.03959v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03959</id>
        <link href="http://arxiv.org/abs/2106.03959"/>
        <updated>2021-06-09T02:01:48.939Z</updated>
        <summary type="html"><![CDATA[Flow-based generative models have shown excellent ability to explicitly learn
the probability density function of data via a sequence of invertible
transformations. Yet, modeling long-range dependencies over normalizing flows
remains understudied. To fill the gap, in this paper, we introduce two types of
invertible attention mechanisms for generative flow models. To be precise, we
propose map-based and scaled dot-product attention for unconditional and
conditional generative flow models. The key idea is to exploit split-based
attention mechanisms to learn the attention weights and input representations
on every two splits of flow feature maps. Our method provides invertible
attention modules with tractable Jacobian determinants, enabling seamless
integration of it at any positions of the flow-based models. The proposed
attention mechanism can model the global data dependencies, leading to more
comprehensive flow models. Evaluation on multiple generation tasks demonstrates
that the introduced attention flow idea results in efficient flow models and
compares favorably against the state-of-the-art unconditional and conditional
generative flow methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1"&gt;Rhea Sanjay Sukthanker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Future is Log-Gaussian: ResNets and Their Infinite-Depth-and-Width Limit at Initialization. (arXiv:2106.04013v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04013</id>
        <link href="http://arxiv.org/abs/2106.04013"/>
        <updated>2021-06-09T02:01:48.932Z</updated>
        <summary type="html"><![CDATA[Theoretical results show that neural networks can be approximated by Gaussian
processes in the infinite-width limit. However, for fully connected networks,
it has been previously shown that for any fixed network width, $n$, the
Gaussian approximation gets worse as the network depth, $d$, increases. Given
that modern networks are deep, this raises the question of how well modern
architectures, like ResNets, are captured by the infinite-width limit. To
provide a better approximation, we study ReLU ResNets in the
infinite-depth-and-width limit, where both depth and width tend to infinity as
their ratio, $d/n$, remains constant. In contrast to the Gaussian
infinite-width limit, we show theoretically that the network exhibits
log-Gaussian behaviour at initialization in the infinite-depth-and-width limit,
with parameters depending on the ratio $d/n$. Using Monte Carlo simulations, we
demonstrate that even basic properties of standard ResNet architectures are
poorly captured by the Gaussian limit, but remarkably well captured by our
log-Gaussian limit. Moreover, our analysis reveals that ReLU ResNets at
initialization are hypoactivated: fewer than half of the ReLUs are activated.
Additionally, we calculate the interlayer correlations, which have the effect
of exponentially increasing the variance of the network output. Based on our
analysis, we introduce Balanced ResNets, a simple architecture modification,
which eliminates hypoactivation and interlayer correlations and is more
amenable to theoretical analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1"&gt;Mufan Bill Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nica_M/0/1/0/all/0/1"&gt;Mihai Nica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roy_D/0/1/0/all/0/1"&gt;Daniel M. Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FEAR: A Simple Lightweight Method to Rank Architectures. (arXiv:2106.04010v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04010</id>
        <link href="http://arxiv.org/abs/2106.04010"/>
        <updated>2021-06-09T02:01:48.923Z</updated>
        <summary type="html"><![CDATA[The fundamental problem in Neural Architecture Search (NAS) is to efficiently
find high-performing architectures from a given search space. We propose a
simple but powerful method which we call FEAR, for ranking architectures in any
search space. FEAR leverages the viewpoint that neural networks are powerful
non-linear feature extractors. First, we train different architectures in the
search space to the same training or validation error. Then, we compare the
usefulness of the features extracted by each architecture. We do so with a
quick training keeping most of the architecture frozen. This gives fast
estimates of the relative performance. We validate FEAR on Natsbench topology
search space on three different datasets against competing baselines and show
strong ranking correlation especially compared to recently proposed zero-cost
methods. FEAR particularly excels at ranking high-performance architectures in
the search space. When used in the inner loop of discrete search algorithms
like random search, FEAR can cut down the search time by approximately 2.4X
without losing accuracy. We additionally empirically study very recently
proposed zero-cost measures for ranking and find that they breakdown in ranking
performance as training proceeds and also that data-agnostic ranking scores
which ignore the dataset do not generalize across dissimilar datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1"&gt;Debadeepta Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Shital Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1"&gt;Sebastien Bubeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lexicon Learning for Few-Shot Neural Sequence Modeling. (arXiv:2106.03993v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03993</id>
        <link href="http://arxiv.org/abs/2106.03993"/>
        <updated>2021-06-09T02:01:48.917Z</updated>
        <summary type="html"><![CDATA[Sequence-to-sequence transduction is the core problem in language processing
applications as diverse as semantic parsing, machine translation, and
instruction following. The neural network models that provide the dominant
solution to these problems are brittle, especially in low-resource settings:
they fail to generalize correctly or systematically from small datasets. Past
work has shown that many failures of systematic generalization arise from
neural models' inability to disentangle lexical phenomena from syntactic ones.
To address this, we augment neural decoders with a lexical translation
mechanism that generalizes existing copy mechanisms to incorporate learned,
decontextualized, token-level translation rules. We describe how to initialize
this mechanism using a variety of lexicon learning algorithms, and show that it
improves systematic generalization on a diverse set of sequence modeling tasks
drawn from cognitive science, formal semantics, and machine translation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1"&gt;Ekin Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrinsic Dimension Estimation. (arXiv:2106.04018v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.04018</id>
        <link href="http://arxiv.org/abs/2106.04018"/>
        <updated>2021-06-09T02:01:48.900Z</updated>
        <summary type="html"><![CDATA[It has long been thought that high-dimensional data encountered in many
practical machine learning tasks have low-dimensional structure, i.e., the
manifold hypothesis holds. A natural question, thus, is to estimate the
intrinsic dimension of a given population distribution from a finite sample. We
introduce a new estimator of the intrinsic dimension and provide finite sample,
non-asymptotic guarantees. We then apply our techniques to get new sample
complexity bounds for Generative Adversarial Networks (GANs) depending only on
the intrinsic dimension of the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Block_A/0/1/0/all/0/1"&gt;Adam Block&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zeyu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Polyanskiy_Y/0/1/0/all/0/1"&gt;Yury Polyanskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rakhlin_A/0/1/0/all/0/1"&gt;Alexander Rakhlin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the role of feedback in visual processing: a predictive coding perspective. (arXiv:2106.04225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04225</id>
        <link href="http://arxiv.org/abs/2106.04225"/>
        <updated>2021-06-09T02:01:48.879Z</updated>
        <summary type="html"><![CDATA[Brain-inspired machine learning is gaining increasing consideration,
particularly in computer vision. Several studies investigated the inclusion of
top-down feedback connections in convolutional networks; however, it remains
unclear how and when these connections are functionally helpful. Here we
address this question in the context of object recognition under noisy
conditions. We consider deep convolutional networks (CNNs) as models of
feed-forward visual processing and implement Predictive Coding (PC) dynamics
through feedback connections (predictive feedback) trained for reconstruction
or classification of clean images. To directly assess the computational role of
predictive feedback in various experimental situations, we optimize and
interpret the hyper-parameters controlling the network's recurrent dynamics.
That is, we let the optimization process determine whether top-down connections
and predictive coding dynamics are functionally beneficial. Across different
model depths and architectures (3-layer CNN, ResNet18, and EfficientNetB0) and
against various types of noise (CIFAR100-C), we find that the network
increasingly relies on top-down predictions as the noise level increases; in
deeper networks, this effect is most prominent at lower layers. In addition,
the accuracy of the network implementing PC dynamics significantly increases
over time-steps, compared to its equivalent forward network. All in all, our
results provide novel insights relevant to Neuroscience by confirming the
computational role of feedback connections in sensory systems, and to Machine
Learning by revealing how these can improve the robustness of current vision
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1"&gt;Andrea Alamia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozafari_M/0/1/0/all/0/1"&gt;Milad Mozafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1"&gt;Bhavin Choksi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1"&gt;Rufin VanRullen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Person Re-Identification with a Locally Aware Transformer. (arXiv:2106.03720v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03720</id>
        <link href="http://arxiv.org/abs/2106.03720"/>
        <updated>2021-06-09T02:01:48.879Z</updated>
        <summary type="html"><![CDATA[Person Re-Identification is an important problem in computer vision-based
surveillance applications, in which the same person is attempted to be
identified from surveillance photographs in a variety of nearby zones. At
present, the majority of Person re-ID techniques are based on Convolutional
Neural Networks (CNNs), but Vision Transformers are beginning to displace pure
CNNs for a variety of object recognition tasks. The primary output of a vision
transformer is a global classification token, but vision transformers also
yield local tokens which contain additional information about local regions of
the image. Techniques to make use of these local tokens to improve
classification accuracy are an active area of research. We propose a novel
Locally Aware Transformer (LA-Transformer) that employs a Parts-based
Convolution Baseline (PCB)-inspired strategy for aggregating globally enhanced
local classification tokens into an ensemble of $\sqrt{N}$ classifiers, where
$N$ is the number of patches. An additional novelty is that we incorporate
blockwise fine-tuning which further improves re-ID accuracy. LA-Transformer
with blockwise fine-tuning achieves rank-1 accuracy of $98.27 \%$ with standard
deviation of $0.13$ on the Market-1501 and $98.7\%$ with standard deviation of
$0.2$ on the CUHK03 dataset respectively, outperforming all other
state-of-the-art published methods at the time of writing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_C/0/1/0/all/0/1"&gt;Charu Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapil_S/0/1/0/all/0/1"&gt;Siddhant R. Kapil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_D/0/1/0/all/0/1"&gt;David Chapman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Social Welfare While Preserving Autonomy via a Pareto Mediator. (arXiv:2106.03927v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.03927</id>
        <link href="http://arxiv.org/abs/2106.03927"/>
        <updated>2021-06-09T02:01:48.874Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms often make decisions on behalf of agents with
varied and sometimes conflicting interests. In domains where agents can choose
to take their own action or delegate their action to a central mediator, an
open question is how mediators should take actions on behalf of delegating
agents. The main existing approach uses delegating agents to punish
non-delegating agents in an attempt to get all agents to delegate, which tends
to be costly for all. We introduce a Pareto Mediator which aims to improve
outcomes for delegating agents without making any of them worse off. Our
experiments in random normal form games, a restaurant recommendation game, and
a reinforcement learning sequential social dilemma show that the Pareto
Mediator greatly increases social welfare. Also, even when the Pareto Mediator
is based on an incorrect model of agent utility, performance gracefully
degrades to the pre-intervention level, due to the individual autonomy
preserved by the voluntary mediator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McAleer_S/0/1/0/all/0/1"&gt;Stephen McAleer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lanier_J/0/1/0/all/0/1"&gt;John Lanier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1"&gt;Michael Dennis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1"&gt;Pierre Baldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_R/0/1/0/all/0/1"&gt;Roy Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Method Based on NARX models and Machine Learning for Pattern Recognition. (arXiv:2106.04021v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04021</id>
        <link href="http://arxiv.org/abs/2106.04021"/>
        <updated>2021-06-09T02:01:48.868Z</updated>
        <summary type="html"><![CDATA[This work presents a novel technique that integrates the methodologies of
machine learning and system identification to solve multiclass problems. Such
an approach allows to extract and select sets of representative features with
reduced dimensionality, as well as predicts categorical outputs. The efficiency
of the method was tested by running case studies investigated in machine
learning, obtaining better absolute results when compared with classical
classification algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_P/0/1/0/all/0/1"&gt;P. H. O. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerqueira_A/0/1/0/all/0/1"&gt;A. S. Cerqueira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nepomuceno_E/0/1/0/all/0/1"&gt;E. G. Nepomuceno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Vision Transformers. (arXiv:2106.04560v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04560</id>
        <link href="http://arxiv.org/abs/2106.04560"/>
        <updated>2021-06-09T02:01:48.863Z</updated>
        <summary type="html"><![CDATA[Attention-based neural networks such as the Vision Transformer (ViT) have
recently attained state-of-the-art results on many computer vision benchmarks.
Scale is a primary ingredient in attaining excellent results, therefore,
understanding a model's scaling properties is a key to designing future
generations effectively. While the laws for scaling Transformer language models
have been studied, it is unknown how Vision Transformers scale. To address
this, we scale ViT models and data, both up and down, and characterize the
relationships between error rate, data, and compute. Along the way, we refine
the architecture and training of ViT, reducing memory consumption and
increasing accuracy the resulting models. As a result, we successfully train a
ViT model with two billion parameters, which attains a new state-of-the-art on
ImageNet of 90.45% top-1 accuracy. The model also performs well on few-shot
learning, for example, attaining 84.86% top-1 accuracy on ImageNet with only 10
examples per class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Learning for Knowledge Distillation. (arXiv:2106.04570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04570</id>
        <link href="http://arxiv.org/abs/2106.04570"/>
        <updated>2021-06-09T02:01:48.833Z</updated>
        <summary type="html"><![CDATA[We present Meta Learning for Knowledge Distillation (MetaDistil), a simple
yet effective alternative to traditional knowledge distillation (KD) methods
where the teacher model is fixed during training. We show the teacher network
can learn to better transfer knowledge to the student network (i.e., learning
to teach) with the feedback from the performance of the distilled student
network in a meta learning framework. Moreover, we introduce a pilot update
mechanism to improve the alignment between the inner-learner and meta-learner
in meta learning algorithms that focus on an improved inner-learner.
Experiments on various benchmarks show that MetaDistil can yield significant
improvements compared with traditional KD algorithms and is less sensitive to
the choice of different student capacity and hyperparameters, facilitating the
use of KD on different tasks and models. The code is available at
https://github.com/JetRunner/MetaDistil]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Canwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language-Mediated, Object-Centric Representation Learning. (arXiv:2012.15814v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15814</id>
        <link href="http://arxiv.org/abs/2012.15814"/>
        <updated>2021-06-09T02:01:48.822Z</updated>
        <summary type="html"><![CDATA[We present Language-mediated, Object-centric Representation Learning (LORL),
a paradigm for learning disentangled, object-centric scene representations from
vision and language. LORL builds upon recent advances in unsupervised object
discovery and segmentation, notably MONet and Slot Attention. While these
algorithms learn an object-centric representation just by reconstructing the
input image, LORL enables them to further learn to associate the learned
representations to concepts, i.e., words for object categories, properties, and
spatial relationships, from language input. These object-centric concepts
derived from language facilitate the learning of object-centric
representations. LORL can be integrated with various unsupervised object
discovery algorithms that are language-agnostic. Experiments show that the
integration of LORL consistently improves the performance of unsupervised
object discovery methods on two datasets via the help of language. We also show
that concepts learned by LORL, in conjunction with object discovery methods,
aid downstream tasks such as referring expression comprehension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruocheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiayuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1"&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[f-CNN$^{\text{x}}$: A Toolflow for Mapping Multi-CNN Applications on FPGAs. (arXiv:1805.10174v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1805.10174</id>
        <link href="http://arxiv.org/abs/1805.10174"/>
        <updated>2021-06-09T02:01:48.816Z</updated>
        <summary type="html"><![CDATA[The predictive power of Convolutional Neural Networks (CNNs) has been an
integral factor for emerging latency-sensitive applications, such as autonomous
drones and vehicles. Such systems employ multiple CNNs, each one trained for a
particular task. The efficient mapping of multiple CNNs on a single FPGA device
is a challenging task as the allocation of compute resources and external
memory bandwidth needs to be optimised at design time. This paper proposes
f-CNN$^{\text{x}}$, an automated toolflow for the optimised mapping of multiple
CNNs on FPGAs, comprising a novel multi-CNN hardware architecture together with
an automated design space exploration method that considers the user-specified
performance requirements for each model to allocate compute resources and
generate a synthesisable accelerator. Moreover, f-CNN$^{\text{x}}$ employs a
novel scheduling algorithm that alleviates the limitations of the memory
bandwidth contention between CNNs and sustains the high utilisation of the
architecture. Experimental evaluation shows that f-CNN$^{\text{x}}$'s designs
outperform contention-unaware FPGA mappings by up to 50% and deliver up to 6.8x
higher performance-per-Watt over highly optimised GPU designs for multi-CNN
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1"&gt;Stylianos I. Venieris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouganis_C/0/1/0/all/0/1"&gt;Christos-Savvas Bouganis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DETReg: Unsupervised Pretraining with Region Priors for Object Detection. (arXiv:2106.04550v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04550</id>
        <link href="http://arxiv.org/abs/2106.04550"/>
        <updated>2021-06-09T02:01:48.797Z</updated>
        <summary type="html"><![CDATA[Unsupervised pretraining has recently proven beneficial for computer vision
tasks, including object detection. However, previous self-supervised approaches
are not designed to handle a key aspect of detection: localizing objects. Here,
we present DETReg, an unsupervised pretraining approach for object DEtection
with TRansformers using Region priors. Motivated by the two tasks underlying
object detection: localization and categorization, we combine two complementary
signals for self-supervision. For an object localization signal, we use pseudo
ground truth object bounding boxes from an off-the-shelf unsupervised region
proposal method, Selective Search, which does not require training data and can
detect objects at a high recall rate and very low precision. The categorization
signal comes from an object embedding loss that encourages invariant object
representations, from which the object category can be inferred. We show how to
combine these two signals to train the Deformable DETR detection architecture
from large amounts of unlabeled data. DETReg improves the performance over
competitive baselines and previous self-supervised methods on standard
benchmarks like MS COCO and PASCAL VOC. DETReg also outperforms previous
supervised and unsupervised baseline approaches on low-data regime when trained
with only 1%, 2%, 5%, and 10% of the labeled data on MS COCO. For code and
pretrained models, visit the project page at https://amirbar.net/detreg]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1"&gt;Amir Bar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kantorov_V/0/1/0/all/0/1"&gt;Vadim Kantorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1"&gt;Colorado J Reed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1"&gt;Roei Herzig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1"&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1"&gt;Amir Globerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Based Attention Through Internal Gating. (arXiv:2106.04540v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.04540</id>
        <link href="http://arxiv.org/abs/2106.04540"/>
        <updated>2021-06-09T02:01:48.791Z</updated>
        <summary type="html"><![CDATA[Object-based attention is a key component of the visual system, relevant for
perception, learning, and memory. Neurons tuned to features of attended objects
tend to be more active than those associated with non-attended objects. There
is a rich set of models of this phenomenon in computational neuroscience.
However, there is currently a divide between models that successfully match
physiological data but can only deal with extremely simple problems and models
of attention used in computer vision. For example, attention in the brain is
known to depend on top-down processing, whereas self-attention in deep learning
does not. Here, we propose an artificial neural network model of object-based
attention that captures the way in which attention is both top-down and
recurrent. Our attention model works well both on simple test stimuli, such as
those using images of handwritten digits, and on more complex stimuli, such as
natural images drawn from the COCO dataset. We find that our model replicates a
range of findings from neuroscience, including attention-invariant tuning,
inhibition of return, and attention-mediated scaling of activity. Understanding
object based attention is both computationally interesting and a key problem
for computational neuroscience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jordan Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Benjamin_A/0/1/0/all/0/1"&gt;Ari S. Benjamin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kording_K/0/1/0/all/0/1"&gt;Konrad P. Kording&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MViT: Mask Vision Transformer for Facial Expression Recognition in the wild. (arXiv:2106.04520v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04520</id>
        <link href="http://arxiv.org/abs/2106.04520"/>
        <updated>2021-06-09T02:01:48.775Z</updated>
        <summary type="html"><![CDATA[Facial Expression Recognition (FER) in the wild is an extremely challenging
task in computer vision due to variant backgrounds, low-quality facial images,
and the subjectiveness of annotators. These uncertainties make it difficult for
neural networks to learn robust features on limited-scale datasets. Moreover,
the networks can be easily distributed by the above factors and perform
incorrect decisions. Recently, vision transformer (ViT) and data-efficient
image transformers (DeiT) present their significant performance in traditional
classification tasks. The self-attention mechanism makes transformers obtain a
global receptive field in the first layer which dramatically enhances the
feature extraction capability. In this work, we first propose a novel pure
transformer-based mask vision transformer (MViT) for FER in the wild, which
consists of two modules: a transformer-based mask generation network (MGN) to
generate a mask that can filter out complex backgrounds and occlusion of face
images, and a dynamic relabeling module to rectify incorrect labels in FER
datasets in the wild. Extensive experimental results demonstrate that our MViT
outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with
89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable
result on AffectNet-8 with 61.40%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hanting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_M/0/1/0/all/0/1"&gt;Mingzhe Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Feng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhengjun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Conditional Flow Model for Learning the Super-Resolution Space. (arXiv:2106.04428v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04428</id>
        <link href="http://arxiv.org/abs/2106.04428"/>
        <updated>2021-06-09T02:01:48.769Z</updated>
        <summary type="html"><![CDATA[Fundamentally, super-resolution is ill-posed problem because a low-resolution
image can be obtained from many high-resolution images. Recent studies for
super-resolution cannot create diverse super-resolution images. Although SRFlow
tried to account for ill-posed nature of the super-resolution by predicting
multiple high-resolution images given a low-resolution image, there is room to
improve the diversity and visual quality. In this paper, we propose Noise
Conditional flow model for Super-Resolution, NCSR, which increases the visual
quality and diversity of images through noise conditional layer. To learn more
diverse data distribution, we add noise to training data. However, low-quality
images are resulted from adding noise. We propose the noise conditional layer
to overcome this phenomenon. The noise conditional layer makes our model
generate more diverse images with higher visual quality than other works.
Furthermore, we show that this layer can overcome data distribution mismatch, a
problem that arises in normalizing flow models. With these benefits, NCSR
outperforms baseline in diversity and visual quality and achieves better visual
quality than traditional GAN-based models. We also get outperformed scores at
NTIRE 2021 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Younggeun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Son_D/0/1/0/all/0/1"&gt;Donghee Son&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Noisy Labels with Deep Neural Networks: A Survey. (arXiv:2007.08199v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08199</id>
        <link href="http://arxiv.org/abs/2007.08199"/>
        <updated>2021-06-09T02:01:48.764Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved remarkable success in numerous domains with help
from large amounts of big data. However, the quality of data labels is a
concern because of the lack of high-quality labels in many real-world
scenarios. As noisy labels severely degrade the generalization performance of
deep neural networks, learning from noisy labels (robust training) is becoming
an important task in modern deep learning applications. In this survey, we
first describe the problem of learning with label noise from a supervised
learning perspective. Next, we provide a comprehensive review of 57
state-of-the-art robust training methods, all of which are categorized into
five groups according to their methodological difference, followed by a
systematic comparison of six properties used to evaluate their superiority.
Subsequently, we perform an in-depth analysis of noise rate estimation and
summarize the typically used evaluation methodology, including public noisy
datasets and evaluation metrics. Finally, we present several promising research
directions that can serve as a guideline for future studies. All the contents
will be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hwanjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yooju Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae-Gil Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The PREVENTION Challenge: How Good Are Humans Predicting Lane Changes?. (arXiv:2009.05331v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05331</id>
        <link href="http://arxiv.org/abs/2009.05331"/>
        <updated>2021-06-09T02:01:48.758Z</updated>
        <summary type="html"><![CDATA[While driving on highways, every driver tries to be aware of the behavior of
surrounding vehicles, including possible emergency braking, evasive maneuvers
trying to avoid obstacles, unexpected lane changes, or other emergencies that
could lead to an accident. In this paper, human's ability to predict lane
changes in highway scenarios is analyzed through the use of video sequences
extracted from the PREVENTION dataset, a database focused on the development of
research on vehicle intention and trajectory prediction. Thus, users had to
indicate the moment at which they considered that a lane change maneuver was
taking place in a target vehicle, subsequently indicating its direction: left
or right. The results retrieved have been carefully analyzed and compared to
ground truth labels, evaluating statistical models to understand whether humans
can actually predict. The study has revealed that most participants are unable
to anticipate lane-change maneuvers, detecting them after they have started.
These results might serve as a baseline for AI's prediction ability evaluation,
grading if those systems can outperform human skills by analyzing hidden cues
that seem unnoticed, improving the detection time, and even anticipating
maneuvers in some cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quintanar_A/0/1/0/all/0/1"&gt;A. Quintanar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Izquierdo_R/0/1/0/all/0/1"&gt;R. Izquierdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parra_I/0/1/0/all/0/1"&gt;I. Parra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Llorca_D/0/1/0/all/0/1"&gt;D. Fern&amp;#xe1;ndez-Llorca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotelo_M/0/1/0/all/0/1"&gt;M. A. Sotelo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Efficient Instance Generation from Instance Discrimination. (arXiv:2106.04566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04566</id>
        <link href="http://arxiv.org/abs/2106.04566"/>
        <updated>2021-06-09T02:01:48.752Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have significantly advanced image
synthesis, however, the synthesis quality drops significantly given a limited
amount of training data. To improve the data efficiency of GAN training, prior
work typically employs data augmentation to mitigate the overfitting of the
discriminator yet still learn the discriminator with a bi-classification (i.e.,
real vs. fake) task. In this work, we propose a data-efficient Instance
Generation (InsGen) method based on instance discrimination. Concretely,
besides differentiating the real domain from the fake domain, the discriminator
is required to distinguish every individual image, no matter it comes from the
training set or from the generator. In this way, the discriminator can benefit
from the infinite synthesized samples for training, alleviating the overfitting
problem caused by insufficient training data. A noise perturbation strategy is
further introduced to improve its discriminative power. Meanwhile, the learned
instance discrimination capability from the discriminator is in turn exploited
to encourage the generator for diverse generation. Extensive experiments
demonstrate the effectiveness of our method on a variety of datasets and
training settings. Noticeably, on the setting of 2K training images from the
FFHQ dataset, we outperform the state-of-the-art approach with 23.5% FID
improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Ceyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yujun Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yinghao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04484</id>
        <link href="http://arxiv.org/abs/2106.04484"/>
        <updated>2021-06-09T02:01:48.737Z</updated>
        <summary type="html"><![CDATA[Deep learning algorithms have shown promising results in visual question
answering (VQA) tasks, but a more careful look reveals that they often do not
understand the rich signal they are being fed with. To understand and better
measure the generalization capabilities of VQA systems, we look at their
robustness to counterfactually augmented data. Our proposed augmentations are
designed to make a focused intervention on a specific property of the question
such that the answer changes. Using these augmentations, we propose a new
robustness measure, Robustness to Augmented Data (RAD), which measures the
consistency of model predictions between original and augmented examples.
Through extensive experimentation, we show that RAD, unlike classical accuracy
measures, can quantify when state-of-the-art systems are not robust to
counterfactuals. We find substantial failure cases which reveal that current
VQA systems are still brittle. Finally, we connect between robustness and
generalization, demonstrating the predictive power of RAD for performance on
unseen augmentations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1"&gt;Daniel Rosenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1"&gt;Itai Gat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1"&gt;Amir Feder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1"&gt;Roi Reichart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the benefits of defining vicinal distributions in latent space. (arXiv:2003.06566v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.06566</id>
        <link href="http://arxiv.org/abs/2003.06566"/>
        <updated>2021-06-09T02:01:48.714Z</updated>
        <summary type="html"><![CDATA[The vicinal risk minimization (VRM) principle is an empirical risk
minimization (ERM) variant that replaces Dirac masses with vicinal functions.
There is strong numerical and theoretical evidence showing that VRM outperforms
ERM in terms of generalization if appropriate vicinal functions are chosen.
Mixup Training (MT), a popular choice of vicinal distribution, improves the
generalization performance of models by introducing globally linear behavior in
between training examples. Apart from generalization, recent works have shown
that mixup trained models are relatively robust to input
perturbations/corruptions and at the same time are calibrated better than their
non-mixup counterparts. In this work, we investigate the benefits of defining
these vicinal distributions like mixup in latent space of generative models
rather than in input space itself. We propose a new approach - \textit{VarMixup
(Variational Mixup)} - to better sample mixup images by using the latent
manifold underlying the data. Our empirical studies on CIFAR-10, CIFAR-100, and
Tiny-ImageNet demonstrate that models trained by performing mixup in the latent
manifold learned by VAEs are inherently more robust to various input
corruptions/perturbations, are significantly better calibrated, and exhibit
more local-linear loss landscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1"&gt;Puneet Mangla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1"&gt;Vedant Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1"&gt;Shreyas Jayant Havaldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the relation between statistical learning and perceptual distances. (arXiv:2106.04427v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04427</id>
        <link href="http://arxiv.org/abs/2106.04427"/>
        <updated>2021-06-09T02:01:48.707Z</updated>
        <summary type="html"><![CDATA[It has been demonstrated many times that the behavior of the human visual
system is connected to the statistics of natural images. Since machine learning
relies on the statistics of training data as well, the above connection has
interesting implications when using perceptual distances (which mimic the
behavior of the human visual system) as a loss function. In this paper, we aim
to unravel the non-trivial relationship between the probability distribution of
the data, perceptual distances, and unsupervised machine learning. To this end,
we show that perceptual sensitivity is correlated with the probability of an
image in its close neighborhood. We also explore the relation between distances
induced by autoencoders and the probability distribution of the data used for
training them, as well as how these induced distances are correlated with human
perception. Finally, we discuss why perceptual distances might not lead to
noticeable gains in performance over standard Euclidean distances in common
image processing tasks except when data is scarce and the perceptual distance
provides regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1"&gt;Alexander Hepburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1"&gt;Valero Laparra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1"&gt;Raul Santos-Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1"&gt;Johannes Ball&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Malo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PolypGen: A multi-center polyp detection and segmentation dataset for generalisability assessment. (arXiv:2106.04463v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04463</id>
        <link href="http://arxiv.org/abs/2106.04463"/>
        <updated>2021-06-09T02:01:48.702Z</updated>
        <summary type="html"><![CDATA[Polyps in the colon are widely known as cancer precursors identified by
colonoscopy either related to diagnostic work-up for symptoms, colorectal
cancer screening or systematic surveillance of certain diseases. Whilst most
polyps are benign, the number, size and the surface structure of the polyp are
tightly linked to the risk of colon cancer. There exists a high missed
detection rate and incomplete removal of colon polyps due to the variable
nature, difficulties to delineate the abnormality, high recurrence rates and
the anatomical topography of the colon. In the past, several methods have been
built to automate polyp detection and segmentation. However, the key issue of
most methods is that they have not been tested rigorously on a large
multi-center purpose-built dataset. Thus, these methods may not generalise to
different population datasets as they overfit to a specific population and
endoscopic surveillance. To this extent, we have curated a dataset from 6
different centers incorporating more than 300 patients. The dataset includes
both single frame and sequence data with 3446 annotated polyp labels with
precise delineation of polyp boundaries verified by six senior
gastroenterologists. To our knowledge, this is the most comprehensive detection
and pixel-level segmentation dataset curated by a team of computational
scientists and expert gastroenterologists. This dataset has been originated as
the part of the Endocv2021 challenge aimed at addressing generalisability in
polyp detection and segmentation. In this paper, we provide comprehensive
insight into data construction and annotation strategies, annotation quality
assurance and technical validation for our extended EndoCV2021 dataset which we
refer to as PolypGen.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sharib Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1"&gt;Debesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghatwary_N/0/1/0/all/0/1"&gt;Noha Ghatwary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Realdon_S/0/1/0/all/0/1"&gt;Stefano Realdon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cannizzaro_R/0/1/0/all/0/1"&gt;Renato Cannizzaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salem_O/0/1/0/all/0/1"&gt;Osama E. Salem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lamarque_D/0/1/0/all/0/1"&gt;Dominique Lamarque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Daul_C/0/1/0/all/0/1"&gt;Christian Daul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anonsen_K/0/1/0/all/0/1"&gt;Kim V. Anonsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1"&gt;Michael A. Riegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1"&gt;P&amp;#xe5;l Halvorsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1"&gt;Jens Rittscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1"&gt;Thomas de Lange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+East_J/0/1/0/all/0/1"&gt;James E. East&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grapevine Winter Pruning Automation: On Potential Pruning Points Detection through 2D Plant Modeling using Grapevine Segmentation. (arXiv:2106.04208v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04208</id>
        <link href="http://arxiv.org/abs/2106.04208"/>
        <updated>2021-06-09T02:01:48.691Z</updated>
        <summary type="html"><![CDATA[Grapevine winter pruning is a complex task, that requires skilled workers to
execute it correctly. The complexity of this task is also the reason why it is
time consuming. Considering that this operation takes about 80-120 hours/ha to
be completed, and therefore is even more crucial in large-size vineyards, an
automated system can help to speed up the process. To this end, this paper
presents a novel multidisciplinary approach that tackles this challenging task
by performing object segmentation on grapevine images, used to create a
representative model of the grapevine plants. Second, a set of potential
pruning points is generated from this plant representation. We will describe
(a) a methodology for data acquisition and annotation, (b) a neural network
fine-tuning for grapevine segmentation, (c) an image processing based method
for creating the representative model of grapevines, starting from the inferred
segmentation and (d) potential pruning points detection and localization, based
on the plant model which is a simplification of the grapevine structure. With
this approach, we are able to identify a significant set of potential pruning
points on the canes, that can be used, with further selection, to derive the
final set of the real pruning points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_M/0/1/0/all/0/1"&gt;Miguel Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scaldaferri_A/0/1/0/all/0/1"&gt;Antonello Scaldaferri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1"&gt;Giuseppe Fiameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_T/0/1/0/all/0/1"&gt;Tao Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatti_M/0/1/0/all/0/1"&gt;Matteo Gatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poni_S/0/1/0/all/0/1"&gt;Stefano Poni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semini_C/0/1/0/all/0/1"&gt;Claudio Semini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caldwell_D/0/1/0/all/0/1"&gt;Darwin Caldwell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image2Point: 3D Point-Cloud Understanding with Pretrained 2D ConvNets. (arXiv:2106.04180v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04180</id>
        <link href="http://arxiv.org/abs/2106.04180"/>
        <updated>2021-06-09T02:01:48.663Z</updated>
        <summary type="html"><![CDATA[3D point-clouds and 2D images are different visual representations of the
physical world. While human vision can understand both representations,
computer vision models designed for 2D image and 3D point-cloud understanding
are quite different. Our paper investigates the potential for transferability
between these two representations by empirically investigating whether this
approach works, what factors affect the transfer performance, and how to make
it work even better. We discovered that we can indeed use the same neural net
model architectures to understand both images and point-clouds. Moreover, we
can transfer pretrained weights from image models to point-cloud models with
minimal effort. Specifically, based on a 2D ConvNet pretrained on an image
dataset, we can transfer the image model to a point-cloud model by
\textit{inflating} 2D convolutional filters to 3D then finetuning its input,
output, and optionally normalization layers. The transferred model can achieve
competitive performance on 3D point-cloud classification, indoor and driving
scene segmentation, even beating a wide range of point-cloud models that adopt
task-specific architectures and use a variety of tricks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shijia Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1"&gt;Bohan Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bichen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1"&gt;Xiangyu Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1"&gt;Wei Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1"&gt;Peter Vajda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSRNet: Cascaded Selective Resolution Network for Real-time Semantic Segmentation. (arXiv:2106.04400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04400</id>
        <link href="http://arxiv.org/abs/2106.04400"/>
        <updated>2021-06-09T02:01:48.641Z</updated>
        <summary type="html"><![CDATA[Real-time semantic segmentation has received considerable attention due to
growing demands in many practical applications, such as autonomous vehicles,
robotics, etc. Existing real-time segmentation approaches often utilize feature
fusion to improve segmentation accuracy. However, they fail to fully consider
the feature information at different resolutions and the receptive fields of
the networks are relatively limited, thereby compromising the performance. To
tackle this problem, we propose a light Cascaded Selective Resolution Network
(CSRNet) to improve the performance of real-time segmentation through multiple
context information embedding and enhanced feature aggregation. The proposed
network builds a three-stage segmentation system, which integrates feature
information from low resolution to high resolution and achieves feature
refinement progressively. CSRNet contains two critical modules: the Shorted
Pyramid Fusion Module (SPFM) and the Selective Resolution Module (SRM). The
SPFM is a computationally efficient module to incorporate the global context
information and significantly enlarge the receptive field at each stage. The
SRM is designed to fuse multi-resolution feature maps with various receptive
fields, which assigns soft channel attentions across the feature maps and helps
to remedy the problem caused by multi-scale objects. Comprehensive experiments
on two well-known datasets demonstrate that the proposed CSRNet effectively
improves the performance for real-time segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jingjing Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1"&gt;Lai-Man Po&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wing-Yin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xian_P/0/1/0/all/0/1"&gt;Pengfei Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1"&gt;Weifeng Ou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-frame sequence generator of 4D human body motion. (arXiv:2106.04387v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04387</id>
        <link href="http://arxiv.org/abs/2106.04387"/>
        <updated>2021-06-09T02:01:48.582Z</updated>
        <summary type="html"><![CDATA[We examine the problem of generating temporally and spatially dense 4D human
body motion. On the one hand generative modeling has been extensively studied
as a per time-frame static fitting problem for dense 3D models such as mesh
representations, where the temporal aspect is left out of the generative model.
On the other hand, temporal generative models exist for sparse human models
such as marker-based capture representations, but have not to our knowledge
been extended to dense 3D shapes. We propose to bridge this gap with a
generative auto-encoder-based framework, which encodes morphology, global
locomotion including translation and rotation, and multi-frame temporal motion
as a single latent space vector. To assess its generalization and factorization
abilities, we train our model on a cyclic locomotion subset of AMASS,
leveraging the dense surface models it provides for an extensive set of motion
captures. Our results validate the ability of the model to reconstruct 4D
sequences of human locomotions within a low error bound, and the meaningfulness
of latent space interpolation between latent vectors representing different
multi-frame sequences and locomotion types. We also illustrate the benefits of
the approach for 4D human motion prediction of future frames from initial human
locomotion frames, showing promising abilities of our model to learn realistic
spatio-temporal features of human motion. We show that our model allows for
data completion of both spatially and temporally sparse data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathieu_M/0/1/0/all/0/1"&gt;Marsot Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanie_W/0/1/0/all/0/1"&gt;Wuhrer Stefanie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jean_Sebastien_F/0/1/0/all/0/1"&gt;Franco Jean-Sebastien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stephane_D/0/1/0/all/0/1"&gt;Durocher Stephane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Improving Adversarial Transferability of Vision Transformers. (arXiv:2106.04169v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04169</id>
        <link href="http://arxiv.org/abs/2106.04169"/>
        <updated>2021-06-09T02:01:48.566Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViTs) process input images as sequences of patches via
self-attention; a radically different architecture than convolutional neural
networks (CNNs). This makes it interesting to study the adversarial feature
space of ViT models and their transferability. In particular, we observe that
adversarial patterns found via conventional adversarial attacks show very low
black-box transferability even for large ViT models. However, we show that this
phenomenon is only due to the sub-optimal attack procedures that do not
leverage the true representation potential of ViTs. A deep ViT is composed of
multiple blocks, with a consistent architecture comprising of self-attention
and feed-forward layers, where each block is capable of independently producing
a class token. Formulating an attack using only the last class token
(conventional approach) does not directly leverage the discriminative
information stored in the earlier tokens, leading to poor adversarial
transferability of ViTs. Using the compositional nature of ViT models, we
enhance the transferability of existing attacks by introducing two novel
strategies specific to the architecture of ViT models. (i) Self-Ensemble: We
propose a method to find multiple discriminative pathways by dissecting a
single ViT model into an ensemble of networks. This allows explicitly utilizing
class-specific information at each ViT block. (ii) Token Refinement: We then
propose to refine the tokens to further enhance the discriminative capacity at
each block of ViT. Our token refinement systematically combines the class
tokens with structural information preserved within the patch tokens. An
adversarial attack, when applied to such refined tokens within the ensemble of
classifiers found in a single vision transformer, has significantly higher
transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1"&gt;Fatih Porikli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Concise yet Effective model for Non-Aligned Incomplete Multi-view and Missing Multi-label Learning. (arXiv:2005.00976v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00976</id>
        <link href="http://arxiv.org/abs/2005.00976"/>
        <updated>2021-06-09T02:01:48.559Z</updated>
        <summary type="html"><![CDATA[In reality, learning from multi-view multi-label data inevitably confronts
three challenges: missing labels, incomplete views, and non-aligned views.
Existing methods mainly concern the first two and commonly need multiple
assumptions to attack them, making even state-of-the-arts involve at least two
explicit hyper-parameters such that model selection is quite difficult. More
roughly, they will fail in handling the third challenge, let alone addressing
the three jointly. In this paper, we aim at meeting these under the least
assumption by building a concise yet effective model with just one
hyper-parameter. To ease insufficiency of available labels, we exploit not only
the consensus of multiple views but also the global and local structures hidden
among multiple labels. Specifically, we introduce an indicator matrix to tackle
the first two challenges in a regression form while aligning the same
individual labels and all labels of different views in a common label space to
battle the third challenge. In aligning, we characterize the global and local
structures of multiple labels to be high-rank and low-rank, respectively.
Subsequently, an efficient algorithm with linear time complexity in the number
of samples is established. Finally, even without view-alignment, our method
substantially outperforms state-of-the-arts with view-alignment on five real
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Songcan Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SDGMNet: Statistic-based Dynamic Gradient Modulation for Local Descriptor Learning. (arXiv:2106.04434v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04434</id>
        <link href="http://arxiv.org/abs/2106.04434"/>
        <updated>2021-06-09T02:01:48.553Z</updated>
        <summary type="html"><![CDATA[Modifications on triplet loss that rescale the back-propagated gradients of
special pairs have made significant progress on local descriptor learning.
However, current gradient modulation strategies are mainly static so that they
would suffer from changes of training phases or datasets. In this paper, we
propose a dynamic gradient modulation, named SDGMNet, to improve triplet loss
for local descriptor learning. The core of our method is formulating modulation
functions with statistical characteristics which are estimated dynamically.
Firstly, we perform deep analysis on back propagation of general triplet-based
loss and introduce included angle for distance measure. On this basis,
auto-focus modulation is employed to moderate the impact of statistically
uncommon individual pairs in stochastic gradient descent optimization;
probabilistic margin cuts off the gradients of proportional Siamese pairs that
are believed to reach the optimum; power adjustment balances the total weights
of negative pairs and positive pairs. Extensive experiments demonstrate that
our novel descriptor surpasses previous state-of-the-arts on standard
benchmarks including patch verification, matching and retrieval tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiayi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yuxin Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative adversarial network with object detector discriminator for enhanced defect detection on ultrasonic B-scans. (arXiv:2106.04281v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04281</id>
        <link href="http://arxiv.org/abs/2106.04281"/>
        <updated>2021-06-09T02:01:48.546Z</updated>
        <summary type="html"><![CDATA[Non-destructive testing is a set of techniques for defect detection in
materials. While the set of imaging techniques are manifold, ultrasonic imaging
is the one used the most. The analysis is mainly performed by human inspectors
manually analyzing recorded images. The low number of defects in real
ultrasonic inspections and legal issues considering data from such inspections
make it difficult to obtain proper results from automatic ultrasonic image
(B-scan) analysis. In this paper, we present a novel deep learning Generative
Adversarial Network model for generating ultrasonic B-scans with defects in
distinct locations. Furthermore, we show that generated B-scans can be used for
synthetic data augmentation, and can improve the performance of deep
convolutional neural object detection networks. Our novel method is
demonstrated on a dataset of almost 4000 B-scans with more than 6000 annotated
defects. Defect detection performance when training on real data yielded
average precision of 71%. By training only on generated data the results
increased to 72.1%, and by mixing generated and real data we achieve 75.7%
average precision. We believe that synthetic data generation can generalize to
other challenges with limited datasets and could be used for training human
personnel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Posilovic_L/0/1/0/all/0/1"&gt;Luka Posilovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Medak_D/0/1/0/all/0/1"&gt;Duje Medak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Subasic_M/0/1/0/all/0/1"&gt;Marko Subasic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Budimir_M/0/1/0/all/0/1"&gt;Marko Budimir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Loncaric_S/0/1/0/all/0/1"&gt;Sven Loncaric&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation and ABCD rule extraction for skin tumors classification. (arXiv:2106.04372v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04372</id>
        <link href="http://arxiv.org/abs/2106.04372"/>
        <updated>2021-06-09T02:01:48.537Z</updated>
        <summary type="html"><![CDATA[During the last years, computer vision-based diagnosis systems have been
widely used in several hospitals and dermatology clinics, aiming at the early
detection of malignant melanoma tumor, which is among the most frequent types
of skin cancer. In this work, we present an automated diagnosis system based on
the ABCD rule used in clinical diagnosis in order to discriminate benign from
malignant skin lesions. First, to reduce the influence of small structures, a
preprocessing step based on morphological and fast marching schemes is used. In
the second step, an unsupervised approach for lesion segmentation is proposed.
Iterative thresholding is applied to initialize level set automatically. As the
detection of an automated border is an important step for the correctness of
subsequent phases in the computerized melanoma recognition systems, we compare
its accuracy with growcut and mean shift algorithms, and discuss how these
results may influence in the following steps: the feature extraction and the
final lesion classification. Relying on visual diagnosis four features:
Asymmetry (A), Border (B), Color (C) and Diversity (D) are computed and used to
construct a classification module based on artificial neural network for the
recognition of malignant melanoma. This framework has been tested on a
dermoscopic database [16] of 320 images. The classification results show an
increasing true detection rate and a decreasing false positive rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Messadi_M/0/1/0/all/0/1"&gt;Mahammed Messadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cherifi_H/0/1/0/all/0/1"&gt;Hocine Cherifi&lt;/a&gt; (Le2i), &lt;a href="http://arxiv.org/find/cs/1/au:+Bessaid_A/0/1/0/all/0/1"&gt;Abdelhafid Bessaid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Action Localization without Knowing Boundaries. (arXiv:2106.04150v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04150</id>
        <link href="http://arxiv.org/abs/2106.04150"/>
        <updated>2021-06-09T02:01:48.531Z</updated>
        <summary type="html"><![CDATA[Learning to localize actions in long, cluttered, and untrimmed videos is a
hard task, that in the literature has typically been addressed assuming the
availability of large amounts of annotated training samples for each class --
either in a fully-supervised setting, where action boundaries are known, or in
a weakly-supervised setting, where only class labels are known for each video.
In this paper, we go a step further and show that it is possible to learn to
localize actions in untrimmed videos when a) only one/few trimmed examples of
the target action are available at test time, and b) when a large collection of
videos with only class label annotation (some trimmed and some weakly annotated
untrimmed ones) are available for training; with no overlap between the classes
used during training and testing. To do so, we propose a network that learns to
estimate Temporal Similarity Matrices (TSMs) that model a fine-grained
similarity pattern between pairs of videos (trimmed or untrimmed), and uses
them to generate Temporal Class Activation Maps (TCAMs) for seen or unseen
classes. The TCAMs serve as temporal attention mechanisms to extract
video-level representations of untrimmed videos, and to temporally localize
actions at test time. To the best of our knowledge, we are the first to propose
a weakly-supervised, one/few-shot action localization network that can be
trained in an end-to-end fashion. Experimental results on THUMOS14 and
ActivityNet1.2 datasets, show that our method achieves performance comparable
or better to state-of-the-art fully-supervised, few-shot learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Ting-Ting Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1"&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_F/0/1/0/all/0/1"&gt;Fan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1"&gt;Ioannis Patras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04144</id>
        <link href="http://arxiv.org/abs/2106.04144"/>
        <updated>2021-06-09T02:01:48.514Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks may perform poorly when the test and train data
are from different domains. While this problem can be mitigated by using the
target domain data to align the source and target domain feature
representations, the target domain data may be unavailable due to privacy
concerns. Consequently, there is a need for methods that generalize well
without access to target domain data during training. In this work, we propose
an adversarial hallucination approach, which combines a class-wise
hallucination module and a semantic segmentation module. Since the segmentation
performance varies across different classes, we design a semantic-conditioned
style hallucination layer to adaptively stylize each class. The classwise
stylization parameters are generated from the semantic knowledge in the
segmentation probability maps of the source domain image. Both modules compete
adversarially, with the hallucination module generating increasingly
'difficult' style images to challenge the segmentation module. In response, the
segmentation module improves its performance as it is trained with generated
samples at an appropriate class-wise difficulty level. Experiments on state of
the art domain adaptation work demonstrate the efficacy of our proposed method
when no target domain data are available for training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tjio_G/0/1/0/all/0/1"&gt;Gabriel Tjio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Ping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1"&gt;Rick Siow Mong Goh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Left Ventricle Contouring in Cardiac Images Based on Deep Reinforcement Learning. (arXiv:2106.04127v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04127</id>
        <link href="http://arxiv.org/abs/2106.04127"/>
        <updated>2021-06-09T02:01:48.508Z</updated>
        <summary type="html"><![CDATA[Medical image segmentation is one of the important tasks of computer-aided
diagnosis in medical image analysis. Since most medical images have the
characteristics of blurred boundaries and uneven intensity distribution,
through existing segmentation methods, the discontinuity within the target area
and the discontinuity of the target boundary are likely to lead to rough or
even erroneous boundary delineation. In this paper, we propose a new iterative
refined interactive segmentation method for medical images based on agent
reinforcement learning, which focuses on the problem of target segmentation
boundaries. We model the dynamic process of drawing the target contour in a
certain order as a Markov Decision Process (MDP) based on a deep reinforcement
learning method. In the dynamic process of continuous interaction between the
agent and the image, the agent tracks the boundary point by point in order
within a limited length range until the contour of the target is completely
drawn. In this process, the agent can quickly improve the segmentation
performance by exploring an interactive policy in the image. The method we
proposed is simple and effective. At the same time, we evaluate our method on
the cardiac MRI scan data set. Experimental results show that our method has a
better segmentation effect on the left ventricle in a small number of medical
image data sets, especially in terms of segmentation boundaries, this method is
better than existing methods. Based on our proposed method, the dynamic
generation process of the predicted contour trajectory of the left ventricle
will be displayed online at https://github.com/H1997ym/LV-contour-trajectory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1"&gt;Sixing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yameng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shufang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EnMcGAN: Adversarial Ensemble Learning for 3D Complete Renal Structures Segmentation. (arXiv:2106.04130v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.04130</id>
        <link href="http://arxiv.org/abs/2106.04130"/>
        <updated>2021-06-09T02:01:48.502Z</updated>
        <summary type="html"><![CDATA[3D complete renal structures(CRS) segmentation targets on segmenting the
kidneys, tumors, renal arteries and veins in one inference. Once successful, it
will provide preoperative plans and intraoperative guidance for laparoscopic
partial nephrectomy(LPN), playing a key role in the renal cancer treatment.
However, no success has been reported in 3D CRS segmentation due to the complex
shapes of renal structures, low contrast and large anatomical variation. In
this study, we utilize the adversarial ensemble learning and propose Ensemble
Multi-condition GAN(EnMcGAN) for 3D CRS segmentation for the first time. Its
contribution is three-fold. 1)Inspired by windowing, we propose the
multi-windowing committee which divides CTA image into multiple narrow windows
with different window centers and widths enhancing the contrast for salient
boundaries and soft tissues. And then, it builds an ensemble segmentation model
on these narrow windows to fuse the segmentation superiorities and improve
whole segmentation quality. 2)We propose the multi-condition GAN which equips
the segmentation model with multiple discriminators to encourage the segmented
structures meeting their real shape conditions, thus improving the shape
feature extraction ability. 3)We propose the adversarial weighted ensemble
module which uses the trained discriminators to evaluate the quality of
segmented structures, and normalizes these evaluation scores for the ensemble
weights directed at the input image, thus enhancing the ensemble results. 122
patients are enrolled in this study and the mean Dice coefficient of the renal
structures achieves 84.6%. Extensive experiments with promising results on
renal structures reveal powerful segmentation accuracy and great clinical
significance in renal cancer treatment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuting He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ge_R/0/1/0/all/0/1"&gt;Rongjun Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xiaoming Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guanyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kong_Y/0/1/0/all/0/1"&gt;Youyong Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shu_H/0/1/0/all/0/1"&gt;Huazhong Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Coatrieux_J/0/1/0/all/0/1"&gt;Jean-Louis Coatrieux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Deformation Estimation via Multi-Objective Optimization. (arXiv:2106.04139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04139</id>
        <link href="http://arxiv.org/abs/2106.04139"/>
        <updated>2021-06-09T02:01:48.495Z</updated>
        <summary type="html"><![CDATA[The free-form deformation model can represent a wide range of non-rigid
deformations by manipulating a control point lattice over the image. However,
due to a large number of parameters, it is challenging to fit the free-form
deformation model directly to the deformed image for deformation estimation
because of the complexity of the fitness landscape. In this paper, we cast the
registration task as a multi-objective optimization problem (MOP) according to
the fact that regions affected by each control point overlap with each other.
Specifically, by partitioning the template image into several regions and
measuring the similarity of each region independently, multiple objectives are
built and deformation estimation can thus be realized by solving the MOP with
off-the-shelf multi-objective evolutionary algorithms (MOEAs). In addition, a
coarse-to-fine strategy is realized by image pyramid combined with control
point mesh subdivision. Specifically, the optimized candidate solutions of the
current image level are inherited by the next level, which increases the
ability to deal with large deformation. Also, a post-processing procedure is
proposed to generate a single output utilizing the Pareto optimal solutions.
Comparative experiments on both synthetic and real-world images show the
effectiveness and usefulness of our deformation estimation method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakane_T/0/1/0/all/0/1"&gt;Takumi Nakane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xuequan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Haoran Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization. (arXiv:2106.04185v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04185</id>
        <link href="http://arxiv.org/abs/2106.04185"/>
        <updated>2021-06-09T02:01:48.481Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a video-based learning framework for animating
personalized 3D talking faces from audio. We introduce two training-time data
normalizations that significantly improve data sample efficiency. First, we
isolate and represent faces in a normalized space that decouples 3D geometry,
head pose, and texture. This decomposes the prediction problem into regressions
over the 3D face shape and the corresponding 2D texture atlas. Second, we
leverage facial symmetry and approximate albedo constancy of skin to isolate
and remove spatio-temporal lighting variations. Together, these normalizations
allow simple networks to generate high fidelity lip-sync videos under novel
ambient illumination while training with just a single speaker-specific video.
Further, to stabilize temporal dynamics, we introduce an auto-regressive
approach that conditions the model on its previous visual state. Human ratings
and objective metrics demonstrate that our method outperforms contemporary
state-of-the-art audio-driven video reenactment benchmarks in terms of realism,
lip-sync and visual quality scores. We illustrate several applications enabled
by our framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1"&gt;Avisek Lahiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwatra_V/0/1/0/all/0/1"&gt;Vivek Kwatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frueh_C/0/1/0/all/0/1"&gt;Christian Frueh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1"&gt;John Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bregler_C/0/1/0/all/0/1"&gt;Chris Bregler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Highly accurate digital traffic recording as a basis for future mobility research: Methods and concepts of the research project HDV-Mess. (arXiv:2106.04175v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04175</id>
        <link href="http://arxiv.org/abs/2106.04175"/>
        <updated>2021-06-09T02:01:48.472Z</updated>
        <summary type="html"><![CDATA[The research project HDV-Mess aims at a currently missing, but very crucial
component for addressing important challenges in the field of connected and
automated driving on public roads. The goal is to record traffic events at
various relevant locations with high accuracy and to collect real traffic data
as a basis for the development and validation of current and future sensor
technologies as well as automated driving functions. For this purpose, it is
necessary to develop a concept for a mobile modular system of measuring
stations for highly accurate traffic data acquisition, which enables a
temporary installation of a sensor and communication infrastructure at
different locations. Within this paper, we first discuss the project goals
before we present our traffic detection concept using mobile modular
intelligent transport systems stations (ITS-Ss). We then explain the approaches
for data processing of sensor raw data to refined trajectories, data
communication, and data validation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kloeker_L/0/1/0/all/0/1"&gt;Laurent Kloeker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomsen_F/0/1/0/all/0/1"&gt;Fabian Thomsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1"&gt;Lutz Eckstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trettner_P/0/1/0/all/0/1"&gt;Philip Trettner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsner_T/0/1/0/all/0/1"&gt;Tim Elsner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nehring_Wirxel_J/0/1/0/all/0/1"&gt;Julius Nehring-Wirxel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_K/0/1/0/all/0/1"&gt;Kersten Schuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1"&gt;Leif Kobbelt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoesch_M/0/1/0/all/0/1"&gt;Michael Hoesch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversational Fashion Image Retrieval via Multiturn Natural Language Feedback. (arXiv:2106.04128v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04128</id>
        <link href="http://arxiv.org/abs/2106.04128"/>
        <updated>2021-06-09T02:01:48.467Z</updated>
        <summary type="html"><![CDATA[We study the task of conversational fashion image retrieval via multiturn
natural language feedback. Most previous studies are based on single-turn
settings. Existing models on multiturn conversational fashion image retrieval
have limitations, such as employing traditional models, and leading to
ineffective performance. We propose a novel framework that can effectively
handle conversational fashion image retrieval with multiturn natural language
feedback texts. One characteristic of the framework is that it searches for
candidate images based on exploitation of the encoded reference image and
feedback text information together with the conversation history. Furthermore,
the image fashion attribute information is leveraged via a mutual attention
strategy. Since there is no existing fashion dataset suitable for the multiturn
setting of our task, we derive a large-scale multiturn fashion dataset via
additional manual annotation efforts on an existing single-turn dataset. The
experiments show that our proposed model significantly outperforms existing
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yifei Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Transformer Networks for Semantic ImageSegmentation. (arXiv:2106.04108v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04108</id>
        <link href="http://arxiv.org/abs/2106.04108"/>
        <updated>2021-06-09T02:01:48.460Z</updated>
        <summary type="html"><![CDATA[Transformers have shown impressive performance in various natural language
processing and computer vision tasks, due to the capability of modeling
long-range dependencies. Recent progress has demonstrated to combine such
transformers with CNN-based semantic image segmentation models is very
promising. However, it is not well studied yet on how well a pure transformer
based approach can achieve for image segmentation. In this work, we explore a
novel framework for semantic image segmentation, which is encoder-decoder based
Fully Transformer Networks (FTN). Specifically, we first propose a Pyramid
Group Transformer (PGT) as the encoder for progressively learning hierarchical
features, while reducing the computation complexity of the standard visual
transformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse
semantic-level and spatial-level information from multiple levels of the PGT
encoder for semantic image segmentation. Surprisingly, this simple baseline can
achieve new state-of-the-art results on multiple challenging semantic
segmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. The
source code will be released upon the publication of this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sitong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianyi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fangjian Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1"&gt;Shengwei Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Contextualized Word Embeddings. (arXiv:2010.12684v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12684</id>
        <link href="http://arxiv.org/abs/2010.12684"/>
        <updated>2021-06-09T02:01:48.454Z</updated>
        <summary type="html"><![CDATA[Static word embeddings that represent words by a single vector cannot capture
the variability of word meaning in different linguistic and extralinguistic
contexts. Building on prior work on contextualized and dynamic word embeddings,
we introduce dynamic contextualized word embeddings that represent words as a
function of both linguistic and extralinguistic context. Based on a pretrained
language model (PLM), dynamic contextualized word embeddings model time and
social space jointly, which makes them attractive for a range of NLP tasks
involving semantic variability. We highlight potential application scenarios by
means of qualitative and quantitative analyses on four English datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1"&gt;Valentin Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1"&gt;Janet B. Pierrehumbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1"&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design of Low-Artifact Interpolation Kernels by Means of Computer Algebra. (arXiv:2106.04104v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04104</id>
        <link href="http://arxiv.org/abs/2106.04104"/>
        <updated>2021-06-09T02:01:48.449Z</updated>
        <summary type="html"><![CDATA[We present a number of new piecewise-polynomial kernels for image
interpolation. The kernels are constructed by optimizing a measure of
interpolation quality based on the magnitude of anisotropic artifacts. The
kernel design process is performed symbolically using Mathematica computer
algebra system. Experimental evaluation involving 14 image quality assessment
methods demonstrates that our results compare favorably with the existing
linear interpolators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karpov_P/0/1/0/all/0/1"&gt;Peter Karpov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Generic Hierarchical Human Motion Prior using VAEs. (arXiv:2106.04004v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04004</id>
        <link href="http://arxiv.org/abs/2106.04004"/>
        <updated>2021-06-09T02:01:48.443Z</updated>
        <summary type="html"><![CDATA[A deep generative model that describes human motions can benefit a wide range
of fundamental computer vision and graphics tasks, such as providing robustness
to video-based human pose estimation, predicting complete body movements for
motion capture systems during occlusions, and assisting key frame animation
with plausible movements. In this paper, we present a method for learning
complex human motions independent of specific tasks using a combined global and
local latent space to facilitate coarse and fine-grained modeling.
Specifically, we propose a hierarchical motion variational autoencoder (HM-VAE)
that consists of a 2-level hierarchical latent space. While the global latent
space captures the overall global body motion, the local latent space enables
to capture the refined poses of the different body parts. We demonstrate the
effectiveness of our hierarchical motion variational autoencoder in a variety
of tasks including video-based human pose estimation, motion completion from
partial observations, and motion synthesis from sparse key-frames. Even though,
our model has not been trained for any of these tasks specifically, it provides
superior performance than task-specific alternatives. Our general-purpose human
motion prior model can fix corrupted human body animations and generate
complete movements from incomplete observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiaman Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1"&gt;Ruben Villegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1"&gt;Duygu Ceylan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jimei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1"&gt;Zhengfei Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yajie Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affinity Attention Graph Neural Network for Weakly Supervised Semantic Segmentation. (arXiv:2106.04054v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04054</id>
        <link href="http://arxiv.org/abs/2106.04054"/>
        <updated>2021-06-09T02:01:48.436Z</updated>
        <summary type="html"><![CDATA[Weakly supervised semantic segmentation is receiving great attention due to
its low human annotation cost. In this paper, we aim to tackle bounding box
supervised semantic segmentation, i.e., training accurate semantic segmentation
models using bounding box annotations as supervision. To this end, we propose
Affinity Attention Graph Neural Network ($A^2$GNN). Following previous
practices, we first generate pseudo semantic-aware seeds, which are then formed
into semantic graphs based on our newly proposed affinity Convolutional Neural
Network (CNN). Then the built graphs are input to our $A^2$GNN, in which an
affinity attention layer is designed to acquire the short- and long- distance
information from soft graph edges to accurately propagate semantic labels from
the confident seeds to the unlabeled pixels. However, to guarantee the
precision of the seeds, we only adopt a limited number of confident pixel seed
labels for $A^2$GNN, which may lead to insufficient supervision for training.
To alleviate this issue, we further introduce a new loss function and a
consistency-checking mechanism to leverage the bounding box constraint, so that
more reliable guidance can be included for the model optimization. Experiments
show that our approach achieves new state-of-the-art performances on Pascal VOC
2012 datasets (val: 76.5\%, test: 75.2\%). More importantly, our approach can
be readily applied to bounding box supervised instance segmentation task or
other weakly supervised semantic segmentation tasks, with state-of-the-art or
comparable performance among almot all weakly supervised tasks on PASCAL VOC or
COCO dataset. Our source code will be available at
https://github.com/zbf1991/A2GNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jimin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jianbo Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation. (arXiv:2106.04067v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04067</id>
        <link href="http://arxiv.org/abs/2106.04067"/>
        <updated>2021-06-09T02:01:48.391Z</updated>
        <summary type="html"><![CDATA[Cross-resolution image alignment is a key problem in multiscale gigapixel
photography, which requires to estimate homography matrix using images with
large resolution gap. Existing deep homography methods concatenate the input
images or features, neglecting the explicit formulation of correspondences
between them, which leads to degraded accuracy in cross-resolution challenges.
In this paper, we consider the cross-resolution homography estimation as a
multimodal problem, and propose a local transformer network embedded within a
multiscale structure to explicitly learn correspondences between the multimodal
inputs, namely, input images with different resolutions. The proposed local
transformer adopts a local attention map specifically for each position in the
feature. By combining the local transformer with the multiscale structure, the
network is able to capture long-short range correspondences efficiently and
accurately. Experiments on both the MS-COCO dataset and the real-captured
cross-resolution dataset show that the proposed network outperforms existing
state-of-the-art feature-based and deep-learning-based homography estimation
methods, and is able to accurately align images under $10\times$ resolution
gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1"&gt;Ruizhi Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gaochang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuemei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Ying Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1"&gt;Lu Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yebin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoPtosis. (arXiv:2106.03905v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.03905</id>
        <link href="http://arxiv.org/abs/2106.03905"/>
        <updated>2021-06-09T02:01:48.373Z</updated>
        <summary type="html"><![CDATA[Blepharoptosis, or ptosis as it is more commonly referred to, is a condition
of the eyelid where the upper eyelid droops. The current diagnosis for ptosis
involves cumbersome manual measurements that are time-consuming and prone to
human error. In this paper, we present AutoPtosis, an artificial intelligence
based system with interpretable results for rapid diagnosis of ptosis. We
utilize a diverse dataset collected at the University of Illinois Hospital and
Health to successfully develop a robust deep learning model for prediction and
also develop a clinically inspired model that calculates the marginal reflex
distance and iris ratio. AutoPtosis achieved 95.5% accuracy on physician
verified data that had an equal class balance. The proposed algorithm can help
in the rapid and timely diagnosis of ptosis, significantly reduce the burden on
the healthcare system, and save the patients and clinics valuable resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Aleem_A/0/1/0/all/0/1"&gt;Abdullah Aleem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nallabothula_M/0/1/0/all/0/1"&gt;Manoj Prabhakar Nallabothula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Setabutr_P/0/1/0/all/0/1"&gt;Pete Setabutr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hallak_J/0/1/0/all/0/1"&gt;Joelle A. Hallak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yi_D/0/1/0/all/0/1"&gt;Darvin Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Schema2QA: High-Quality and Low-Cost Q&A Agents for the Structured Web. (arXiv:2001.05609v6 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.05609</id>
        <link href="http://arxiv.org/abs/2001.05609"/>
        <updated>2021-06-09T02:01:48.354Z</updated>
        <summary type="html"><![CDATA[Building a question-answering agent currently requires large annotated
datasets, which are prohibitively expensive. This paper proposes Schema2QA, an
open-source toolkit that can generate a Q&A system from a database schema
augmented with a few annotations for each field. The key concept is to cover
the space of possible compound queries on the database with a large number of
in-domain questions synthesized with the help of a corpus of generic query
templates. The synthesized data and a small paraphrase set are used to train a
novel neural network based on the BERT pretrained model. We use Schema2QA to
generate Q&A systems for five Schema.org domains, restaurants, people, movies,
books and music, and obtain an overall accuracy between 64% and 75% on
crowdsourced questions for these domains. Once annotations and paraphrases are
obtained for a Schema.org schema, no additional manual effort is needed to
create a Q&A agent for any website that uses the same schema. Furthermore, we
demonstrate that learning can be transferred from the restaurant to the hotel
domain, obtaining a 64% accuracy on crowdsourced questions with no manual
effort. Schema2QA achieves an accuracy of 60% on popular restaurant questions
that can be answered using Schema.org. Its performance is comparable to Google
Assistant, 7% lower than Siri, and 15% higher than Alexa. It outperforms all
these assistants by at least 18% on more complex, long-tail questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Silei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campagna_G/0/1/0/all/0/1"&gt;Giovanni Campagna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1"&gt;Monica S. Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Triad Matching and Reconstruction for Weakly Referring Expression Grounding. (arXiv:2106.04053v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04053</id>
        <link href="http://arxiv.org/abs/2106.04053"/>
        <updated>2021-06-09T02:01:48.348Z</updated>
        <summary type="html"><![CDATA[In this paper, we are tackling the weakly-supervised referring expression
grounding task, for the localization of a referent object in an image according
to a query sentence, where the mapping between image regions and queries are
not available during the training stage. In traditional methods, an object
region that best matches the referring expression is picked out, and then the
query sentence is reconstructed from the selected region, where the
reconstruction difference serves as the loss for back-propagation. The existing
methods, however, conduct both the matching and the reconstruction
approximately as they ignore the fact that the matching correctness is unknown.
To overcome this limitation, a discriminative triad is designed here as the
basis to the solution, through which a query can be converted into one or
multiple discriminative triads in a very scalable way. Based on the
discriminative triad, we further propose the triad-level matching and
reconstruction modules which are lightweight yet effective for the
weakly-supervised training, making it three times lighter and faster than the
previous state-of-the-art methods. One important merit of our work is its
superior performance despite the simple and neat design. Specifically, the
proposed method achieves a new state-of-the-art accuracy when evaluated on
RefCOCO (39.21%), RefCOCO+ (39.18%) and RefCOCOg (43.24%) datasets, that is
4.17%, 4.08% and 7.8% higher than the previous one, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mingjie Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jimin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1"&gt;Eng Gee Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Si Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goulermas_J/0/1/0/all/0/1"&gt;John Y. Goulermas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAiRE in DialDoc21: Data Augmentation for Information-Seeking Dialogue System. (arXiv:2106.03530v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03530</id>
        <link href="http://arxiv.org/abs/2106.03530"/>
        <updated>2021-06-09T02:01:48.343Z</updated>
        <summary type="html"><![CDATA[Information-seeking dialogue systems, including knowledge identification and
response generation, aim to respond to users with fluent, coherent, and
informative responses based on users' needs, which. To tackle this challenge,
we utilize data augmentation methods and several training techniques with the
pre-trained language models to learn a general pattern of the task and thus
achieve promising performance. In DialDoc21 competition, our system achieved
74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU
score in subtask 2. Empirical analysis is provided to explain the effectiveness
of our approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1"&gt;Etsuko Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1"&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhaojiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1"&gt;Andrea Madotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zihan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PANDORA Talks: Personality and Demographics on Reddit. (arXiv:2004.04460v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.04460</id>
        <link href="http://arxiv.org/abs/2004.04460"/>
        <updated>2021-06-09T02:01:48.326Z</updated>
        <summary type="html"><![CDATA[Personality and demographics are important variables in social sciences,
while in NLP they can aid in interpretability and removal of societal biases.
However, datasets with both personality and demographic labels are scarce. To
address this, we present PANDORA, the first large-scale dataset of Reddit
comments labeled with three personality models (including the well-established
Big 5 model) and demographics (age, gender, and location) for more than 10k
users. We showcase the usefulness of this dataset on three experiments, where
we leverage the more readily available data from other personality models to
predict the Big 5 traits, analyze gender classification biases arising from
psycho-demographic variables, and carry out a confirmatory and exploratory
analysis based on psychological theories. Finally, we present benchmark
prediction models for all personality and demographic variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gjurkovic_M/0/1/0/all/0/1"&gt;Matej Gjurkovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karan_M/0/1/0/all/0/1"&gt;Mladen Karan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vukojevic_I/0/1/0/all/0/1"&gt;Iva Vukojevi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1"&gt;Mihaela Bo&amp;#x161;njak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1"&gt;Jan &amp;#x160;najder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Recombine and Resample Data for Compositional Generalization. (arXiv:2010.03706v6 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03706</id>
        <link href="http://arxiv.org/abs/2010.03706"/>
        <updated>2021-06-09T02:01:48.320Z</updated>
        <summary type="html"><![CDATA[Flexible neural sequence models outperform grammar- and automaton-based
counterparts on a variety of tasks. However, neural models perform poorly in
settings requiring compositional generalization beyond the training data --
particularly to rare or unseen subsequences. Past work has found symbolic
scaffolding (e.g. grammars or automata) essential in these settings. We
describe R&R, a learned data augmentation scheme that enables a large category
of compositional generalizations without appeal to latent symbolic structure.
R&R has two components: recombination of original training examples via a
prototype-based generative model and resampling of generated examples to
encourage extrapolation. Training an ordinary neural sequence model on a
dataset augmented with recombined and resampled examples significantly improves
generalization in two language processing problems -- instruction following
(SCAN) and morphological analysis (SIGMORPHON 2018) -- where R&R enables
learning of new constructions and tenses from as few as eight initial examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1"&gt;Ekin Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_A/0/1/0/all/0/1"&gt;Afra Feyza Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting the Unknown from Long Math Problems. (arXiv:2103.12048v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12048</id>
        <link href="http://arxiv.org/abs/2103.12048"/>
        <updated>2021-06-09T02:01:48.314Z</updated>
        <summary type="html"><![CDATA[In problem solving, understanding the problem that one seeks to solve is an
essential initial step. In this paper, we propose computational methods for
facilitating problem understanding through the task of recognizing the unknown
in specifications of long Math problems. We focus on the topic of Probability.
Our experimental results show that learning models yield strong results on the
task, a promising first step towards human interpretable, modular approaches to
understanding long Math problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakashole_N/0/1/0/all/0/1"&gt;Ndapa Nakashole&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating label suggestions for opinion mining in German Covid-19 social media. (arXiv:2105.12980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12980</id>
        <link href="http://arxiv.org/abs/2105.12980"/>
        <updated>2021-06-09T02:01:48.308Z</updated>
        <summary type="html"><![CDATA[This work investigates the use of interactively updated label suggestions to
improve upon the efficiency of gathering annotations on the task of opinion
mining in German Covid-19 social media data. We develop guidelines to conduct a
controlled annotation study with social science students and find that
suggestions from a model trained on a small, expert-annotated dataset already
lead to a substantial improvement - in terms of inter-annotator agreement(+.14
Fleiss' $\kappa$) and annotation quality - compared to students that do not
receive any label suggestions. We further find that label suggestions from
interactively trained models do not lead to an improvement over suggestions
from a static model. Nonetheless, our analysis of suggestion bias shows that
annotators remain capable of reflecting upon the suggested label in general.
Finally, we confirm the quality of the annotated data in transfer learning
experiments between different annotator groups. To facilitate further research
in opinion mining on social media data, we release our collected data
consisting of 200 expert and 2,785 student annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1"&gt;Tilman Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Ji-Ung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viehmann_C/0/1/0/all/0/1"&gt;Christina Viehmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maurer_M/0/1/0/all/0/1"&gt;Marcus Maurer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quiring_O/0/1/0/all/0/1"&gt;Oliver Quiring&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novel View Video Prediction Using a Dual Representation. (arXiv:2106.03956v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03956</id>
        <link href="http://arxiv.org/abs/2106.03956"/>
        <updated>2021-06-09T02:01:48.302Z</updated>
        <summary type="html"><![CDATA[We address the problem of novel view video prediction; given a set of input
video clips from a single/multiple views, our network is able to predict the
video from a novel view. The proposed approach does not require any priors and
is able to predict the video from wider angular distances, upto 45 degree, as
compared to the recent studies predicting small variations in viewpoint.
Moreover, our method relies only onRGB frames to learn a dual representation
which is used to generate the video from a novel viewpoint. The dual
representation encompasses a view-dependent and a global representation which
incorporates complementary details to enable novel view video prediction. We
demonstrate the effectiveness of our framework on two real world datasets:
NTU-RGB+D and CMU Panoptic. A comparison with the State-of-the-art novel view
video prediction methods shows an improvement of 26.1% in SSIM, 13.6% in PSNR,
and 60% inFVD scores without using explicit priors from target views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shiraz_S/0/1/0/all/0/1"&gt;Sarah Shiraz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Regmi_K/0/1/0/all/0/1"&gt;Krishna Regmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1"&gt;Shruti Vyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1"&gt;Yogesh S. Rawat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Volumetric Image Segmentation with Deformed Templates. (arXiv:2106.03987v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03987</id>
        <link href="http://arxiv.org/abs/2106.03987"/>
        <updated>2021-06-09T02:01:48.297Z</updated>
        <summary type="html"><![CDATA[There are many approaches that use weak-supervision to train networks to
segment 2D images. By contrast, existing 3D approaches rely on full-supervision
of a subset of 2D slices of the 3D image volume. In this paper, we propose an
approach that is truly weakly-supervised in the sense that we only need to
provide a sparse set of 3D point on the surface of target objects, an easy task
that can be quickly done. We use the 3D points to deform a 3D template so that
it roughly matches the target object outlines and we introduce an architecture
that exploits the supervision provided by coarse template to train a network to
find accurate boundaries.

We evaluate the performance of our approach on Computed Tomography (CT),
Magnetic Resonance Imagery (MRI) and Electron Microscopy (EM) image datasets.
We will show that it outperforms a more traditional approach to
weak-supervision in 3D at a reduced supervision cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1"&gt;Udaranga Wickramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Reordering for Modeling Latent Alignments in Sequence Transduction. (arXiv:2106.03257v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03257</id>
        <link href="http://arxiv.org/abs/2106.03257"/>
        <updated>2021-06-09T02:01:48.280Z</updated>
        <summary type="html"><![CDATA[Despite success in many domains, neural models struggle in settings where
train and test examples are drawn from different distributions. In particular,
in contrast to humans, conventional sequence-to-sequence (seq2seq) models fail
to generalize systematically, i.e., interpret sentences representing novel
combinations of concepts (e.g., text segments) seen in training. Traditional
grammar formalisms excel in such settings by implicitly encoding alignments
between input and output segments, but are hard to scale and maintain. Instead
of engineering a grammar, we directly model segment-to-segment alignments as
discrete structured latent variables within a neural seq2seq model. To
efficiently explore the large space of alignments, we introduce a reorder-first
align-later framework whose central component is a neural reordering module
producing {\it separable} permutations. We present an efficient dynamic
programming algorithm performing exact marginal inference of separable
permutations, and, thus, enabling end-to-end differentiable training of our
model. The resulting seq2seq model exhibits better systematic generalization
than standard models on synthetic problems and NLP tasks (i.e., semantic
parsing and machine translation).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bailin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1"&gt;Ivan Titov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Translate, then Parse! A strong baseline for Cross-Lingual AMR Parsing. (arXiv:2106.04565v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04565</id>
        <link href="http://arxiv.org/abs/2106.04565"/>
        <updated>2021-06-09T02:01:48.275Z</updated>
        <summary type="html"><![CDATA[In cross-lingual Abstract Meaning Representation (AMR) parsing, researchers
develop models that project sentences from various languages onto their AMRs to
capture their essential semantic structures: given a sentence in any language,
we aim to capture its core semantic content through concepts connected by
manifold types of semantic relations. Methods typically leverage large silver
training data to learn a single model that is able to project non-English
sentences to AMRs. However, we find that a simple baseline tends to be
over-looked: translating the sentences to English and projecting their AMR with
a monolingual AMR parser (translate+parse,T+P). In this paper, we revisit this
simple two-step base-line, and enhance it with a strong NMT system and a strong
AMR parser. Our experiments show that T+P outperforms a recent state-of-the-art
system across all tested languages: German, Italian, Spanish and Mandarin with
+14.6, +12.6, +14.3 and +16.0 Smatch points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uhrig_S/0/1/0/all/0/1"&gt;Sarah Uhrig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Y/0/1/0/all/0/1"&gt;Yoalli Rezepka Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1"&gt;Juri Opitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational AutoEncoder for Reference based Image Super-Resolution. (arXiv:2106.04090v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04090</id>
        <link href="http://arxiv.org/abs/2106.04090"/>
        <updated>2021-06-09T02:01:48.268Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel reference based image super-resolution
approach via Variational AutoEncoder (RefVAE). Existing state-of-the-art
methods mainly focus on single image super-resolution which cannot perform well
on large upsampling factors, e.g., 8$\times$. We propose a reference based
image super-resolution, for which any arbitrary image can act as a reference
for super-resolution. Even using random map or low-resolution image itself, the
proposed RefVAE can transfer the knowledge from the reference to the
super-resolved images. Depending upon different references, the proposed method
can generate different versions of super-resolved images from a hidden
super-resolution space. Besides using different datasets for some standard
evaluations with PSNR and SSIM, we also took part in the NTIRE2021 SR Space
challenge and have provided results of the randomness evaluation of our
approach. Compared to other state-of-the-art methods, our approach achieves
higher diverse scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhi-Song Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siu_W/0/1/0/all/0/1"&gt;Wan-Chi Siu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li-Wen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpaceMeshLab: Spatial Context Memoization and Meshgrid Atrous Convolution Consensus for Semantic Segmentation. (arXiv:2106.04025v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04025</id>
        <link href="http://arxiv.org/abs/2106.04025"/>
        <updated>2021-06-09T02:01:48.263Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation networks adopt transfer learning from image
classification networks which occurs a shortage of spatial context information.
For this reason, we propose Spatial Context Memoization (SpaM), a bypassing
branch for spatial context by retaining the input dimension and constantly
communicating its spatial context and rich semantic information mutually with
the backbone network. Multi-scale context information for semantic segmentation
is crucial for dealing with diverse sizes and shapes of target objects in the
given scene. Conventional multi-scale context scheme adopts multiple effective
receptive fields by multiple dilation rates or pooling operations, but often
suffer from misalignment problem with respect to the target pixel. To this end,
we propose Meshgrid Atrous Convolution Consensus (MetroCon^2) which brings
multi-scale scheme into fine-grained multi-scale object context using
convolutions with meshgrid-like scattered dilation rates. SpaceMeshLab
(ResNet-101 + SpaM + MetroCon^2) achieves 82.0% mIoU in Cityscapes test and
53.5% mIoU on Pascal-Context validation set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinseong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daijin Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-MLP: Node Classification without Message Passing in Graph. (arXiv:2106.04051v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04051</id>
        <link href="http://arxiv.org/abs/2106.04051"/>
        <updated>2021-06-09T02:01:48.257Z</updated>
        <summary type="html"><![CDATA[Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing
with non-Euclidean structural data. Both spatial-based and spectral-based GNNs
are relying on adjacency matrix to guide message passing among neighbors during
feature aggregation. Recent works have mainly focused on powerful message
passing modules, however, in this paper, we show that none of the message
passing modules is necessary. Instead, we propose a pure
multilayer-perceptron-based framework, Graph-MLP with the supervision signal
leveraging graph structure, which is sufficient for learning discriminative
node representation. In model-level, Graph-MLP only includes multi-layer
perceptrons, activation function, and layer normalization. In the loss level,
we design a neighboring contrastive (NContrast) loss to bridge the gap between
GNNs and MLPs by utilizing the adjacency information implicitly. This design
allows our model to be lighter and more robust when facing large-scale graph
data and corrupted adjacency information. Extensive experiments prove that even
without adjacency information in testing phase, our framework can still reach
comparable and even superior performance against the state-of-the-art models in
the graph node classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1"&gt;Haoxuan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhecan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1"&gt;Erjin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer. (arXiv:2106.04095v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04095</id>
        <link href="http://arxiv.org/abs/2106.04095"/>
        <updated>2021-06-09T02:01:48.241Z</updated>
        <summary type="html"><![CDATA[Occluded person re-identification (Re-ID) is a challenging task as persons
are frequently occluded by various obstacles or other persons, especially in
the crowd scenario. To address these issues, we propose a novel end-to-end
Part-Aware Transformer (PAT) for occluded person Re-ID through diverse part
discovery via a transformer encoderdecoder architecture, including a pixel
context based transformer encoder and a part prototype based transformer
decoder. The proposed PAT model enjoys several merits. First, to the best of
our knowledge, this is the first work to exploit the transformer
encoder-decoder architecture for occluded person Re-ID in a unified deep model.
Second, to learn part prototypes well with only identity labels, we design two
effective mechanisms including part diversity and part discriminability.
Consequently, we can achieve diverse part discovery for occluded person Re-ID
in a weakly supervised manner. Extensive experimental results on six
challenging benchmarks for three tasks (occluded, partial and holistic Re-ID)
demonstrate that our proposed PAT performs favorably against stat-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yulin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianzhu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Salvage of Supervision in Weakly Supervised Detection. (arXiv:2106.04073v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04073</id>
        <link href="http://arxiv.org/abs/2106.04073"/>
        <updated>2021-06-09T02:01:48.235Z</updated>
        <summary type="html"><![CDATA[Weakly supervised object detection (WSOD) has recently attracted much
attention. However, the method, performance and speed gaps between WSOD and
fully supervised detection prevent WSOD from being applied in real-world tasks.
To bridge the gaps, this paper proposes a new framework, Salvage of Supervision
(SoS), with the key idea being to harness every potentially useful supervisory
signal in WSOD: the weak image-level labels, the pseudo-labels, and the power
of semi-supervised object detection. This paper shows that each type of
supervisory signal brings in notable improvements, outperforms existing WSOD
methods (which mainly use only the weak labels) by large margins. The proposed
SoS-WSOD method achieves 64.4 $m\text{AP}_{50}$ on VOC2007, 61.9
$m\text{AP}_{50}$ on VOC2012 and 16.4 $m\text{AP}_{50:95}$ on MS-COCO, and also
has fast inference speed. Ablations and visualization further verify the
effectiveness of SoS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sui_L/0/1/0/all/0/1"&gt;Lin Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen-Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianxin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Learning for Knowledge Distillation. (arXiv:2106.04570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04570</id>
        <link href="http://arxiv.org/abs/2106.04570"/>
        <updated>2021-06-09T02:01:48.229Z</updated>
        <summary type="html"><![CDATA[We present Meta Learning for Knowledge Distillation (MetaDistil), a simple
yet effective alternative to traditional knowledge distillation (KD) methods
where the teacher model is fixed during training. We show the teacher network
can learn to better transfer knowledge to the student network (i.e., learning
to teach) with the feedback from the performance of the distilled student
network in a meta learning framework. Moreover, we introduce a pilot update
mechanism to improve the alignment between the inner-learner and meta-learner
in meta learning algorithms that focus on an improved inner-learner.
Experiments on various benchmarks show that MetaDistil can yield significant
improvements compared with traditional KD algorithms and is less sensitive to
the choice of different student capacity and hyperparameters, facilitating the
use of KD on different tasks and models. The code is available at
https://github.com/JetRunner/MetaDistil]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Canwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Structure-from-Motion through Tightly-Coupled Depth and Egomotion Networks. (arXiv:2106.04007v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04007</id>
        <link href="http://arxiv.org/abs/2106.04007"/>
        <updated>2021-06-09T02:01:48.222Z</updated>
        <summary type="html"><![CDATA[Much recent literature has formulated structure-from-motion (SfM) as a
self-supervised learning problem where the goal is to jointly learn neural
network models of depth and egomotion through view synthesis. Herein, we
address the open problem of how to optimally couple the depth and egomotion
network components. Toward this end, we introduce several notions of coupling,
categorize existing approaches, and present a novel tightly-coupled approach
that leverages the interdependence of depth and egomotion at training and at
inference time. Our approach uses iterative view synthesis to recursively
update the egomotion network input, permitting contextual information to be
passed between the components without explicit weight sharing. Through
substantial experiments, we demonstrate that our approach promotes consistency
between the depth and egomotion predictions at test time, improves
generalization on new data, and leads to state-of-the-art accuracy on indoor
and outdoor depth and egomotion evaluation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagstaff_B/0/1/0/all/0/1"&gt;Brandon Wagstaff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peretroukhin_V/0/1/0/all/0/1"&gt;Valentin Peretroukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1"&gt;Jonathan Kelly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge. (arXiv:2104.02704v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02704</id>
        <link href="http://arxiv.org/abs/2104.02704"/>
        <updated>2021-06-09T02:01:48.216Z</updated>
        <summary type="html"><![CDATA[Cant is important for understanding advertising, comedies and dog-whistle
politics. However, computational research on cant is hindered by a lack of
available datasets. In this paper, we propose a large and diverse Chinese
dataset for creating and understanding cant from a computational linguistics
perspective. We formulate a task for cant understanding and provide both
quantitative and qualitative analysis for tested word embedding similarity and
pretrained language models. Experiments suggest that such a task requires deep
language understanding, common sense, and world knowledge and thus can be a
good testbed for pretrained language models and help models perform better on
other tasks. The code is available at https://github.com/JetRunner/dogwhistle.
The data and leaderboard are available at
https://competitions.codalab.org/competitions/30451.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Canwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1"&gt;Tao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00250</id>
        <link href="http://arxiv.org/abs/2106.00250"/>
        <updated>2021-06-09T02:01:48.210Z</updated>
        <summary type="html"><![CDATA[Multimodal Machine Translation (MMT) enriches the source text with visual
information for translation. It has gained popularity in recent years, and
several pipelines have been proposed in the same direction. Yet, the task lacks
quality datasets to illustrate the contribution of visual modality in the
translation systems. In this paper, we propose our system under the team name
Volta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We
also participate in the textual-only subtask of the same language pair for
which we use mBART, a pretrained multilingual sequence-to-sequence model. For
multimodal translation, we propose to enhance the textual input by bringing the
visual information to a textual domain by extracting object tags from the
image. We also explore the robustness of our system by systematically degrading
the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test
set and challenge set of the multimodal task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1"&gt;Radhika Mamidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Transformers with Gradient Boosted Decision Trees for NLI Fine-Tuning. (arXiv:2105.03791v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03791</id>
        <link href="http://arxiv.org/abs/2105.03791"/>
        <updated>2021-06-09T02:01:48.193Z</updated>
        <summary type="html"><![CDATA[Transfer learning has become the dominant paradigm for many natural language
processing tasks. In addition to models being pretrained on large datasets,
they can be further trained on intermediate (supervised) tasks that are similar
to the target task. For small Natural Language Inference (NLI) datasets,
language modelling is typically followed by pretraining on a large (labelled)
NLI dataset before fine-tuning with each NLI subtask. In this work, we explore
Gradient Boosted Decision Trees (GBDTs) as an alternative to the commonly used
Multi-Layer Perceptron (MLP) classification head. GBDTs have desirable
properties such as good performance on dense, numerical features and are
effective where the ratio of the number of samples w.r.t the number of features
is low. We then introduce FreeGBDT, a method of fitting a GBDT head on the
features computed during fine-tuning to increase performance without additional
computation by the neural network. We demonstrate the effectiveness of our
method on several NLI datasets using a strong baseline model (RoBERTa-large
with MNLI pretraining). The FreeGBDT shows a consistent improvement over the
MLP classification head.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Minixhofer_B/0/1/0/all/0/1"&gt;Benjamin Minixhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gritta_M/0/1/0/all/0/1"&gt;Milan Gritta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1"&gt;Ignacio Iacobacci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subject-Independent Brain-Computer Interface for Decoding High-Level Visual Imagery Tasks. (arXiv:2106.04026v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04026</id>
        <link href="http://arxiv.org/abs/2106.04026"/>
        <updated>2021-06-09T02:01:48.187Z</updated>
        <summary type="html"><![CDATA[Brain-computer interface (BCI) is used for communication between humans and
devices by recognizing status and intention of humans. Communication between
humans and a drone using electroencephalogram (EEG) signals is one of the most
challenging issues in the BCI domain. In particular, the control of drone
swarms (the direction and formation) has more advantages compared to the
control of a drone. The visual imagery (VI) paradigm is that subjects visually
imagine specific objects or scenes. Reduction of the variability among EEG
signals of subjects is essential for practical BCI-based systems. In this
study, we proposed the subepoch-wise feature encoder (SEFE) to improve the
performances in the subject-independent tasks by using the VI dataset. This
study is the first attempt to demonstrate the possibility of generalization
among subjects in the VI-based BCI. We used the leave-one-subject-out
cross-validation for evaluating the performances. We obtained higher
performances when including our proposed module than excluding our proposed
module. The DeepConvNet with SEFE showed the highest performance of 0.72 among
six different decoding models. Hence, we demonstrated the feasibility of
decoding the VI dataset in the subject-independent task with robust
performances by using our proposed module.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dae-Hyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dong-Kyun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sung-Jin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1"&gt;Ji-Hoon Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03932</id>
        <link href="http://arxiv.org/abs/2106.03932"/>
        <updated>2021-06-09T02:01:48.181Z</updated>
        <summary type="html"><![CDATA[Successful active speaker detection requires a three-stage pipeline: (i)
audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation
modeling between a reference speaker and the background speakers within each
frame, and (iii) temporal modeling for the reference speaker. Each stage of
this pipeline plays an important role for the final performance of the created
architecture. Based on a series of controlled experiments, this work presents
several practical guidelines for audio-visual active speaker detection.
Correspondingly, we present a new architecture called ASDNet, which achieves a
new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%
outperforming the second best with a large margin of 4.7%. Our code and
pretrained models are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1"&gt;Okan K&amp;#xf6;p&amp;#xfc;kl&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taseska_M/0/1/0/all/0/1"&gt;Maja Taseska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1"&gt;Gerhard Rigoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bh\=a$\unicode{x1E63}$\=acitra: Visualising the dialect geography of South Asia. (arXiv:2105.14082v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14082</id>
        <link href="http://arxiv.org/abs/2105.14082"/>
        <updated>2021-06-09T02:01:48.176Z</updated>
        <summary type="html"><![CDATA[We present Bh\=a$\unicode{x1E63}$\=acitra, a dialect mapping system for South
Asia built on a database of linguistic studies of languages of the region
annotated for topic and location data. We analyse language coverage and look
towards applications to typology by visualising example datasets. The
application is not only meant to be useful for feature mapping, but also serves
as a new kind of interactive bibliography for linguists of South Asian
languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1"&gt;Aryaman Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farris_A/0/1/0/all/0/1"&gt;Adam Farris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+R_G/0/1/0/all/0/1"&gt;Gopalakrishnan R&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1"&gt;Samopriya Basu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FEAR: A Simple Lightweight Method to Rank Architectures. (arXiv:2106.04010v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04010</id>
        <link href="http://arxiv.org/abs/2106.04010"/>
        <updated>2021-06-09T02:01:48.160Z</updated>
        <summary type="html"><![CDATA[The fundamental problem in Neural Architecture Search (NAS) is to efficiently
find high-performing architectures from a given search space. We propose a
simple but powerful method which we call FEAR, for ranking architectures in any
search space. FEAR leverages the viewpoint that neural networks are powerful
non-linear feature extractors. First, we train different architectures in the
search space to the same training or validation error. Then, we compare the
usefulness of the features extracted by each architecture. We do so with a
quick training keeping most of the architecture frozen. This gives fast
estimates of the relative performance. We validate FEAR on Natsbench topology
search space on three different datasets against competing baselines and show
strong ranking correlation especially compared to recently proposed zero-cost
methods. FEAR particularly excels at ranking high-performance architectures in
the search space. When used in the inner loop of discrete search algorithms
like random search, FEAR can cut down the search time by approximately 2.4X
without losing accuracy. We additionally empirically study very recently
proposed zero-cost measures for ranking and find that they breakdown in ranking
performance as training proceeds and also that data-agnostic ranking scores
which ignore the dataset do not generalize across dissimilar datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1"&gt;Debadeepta Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Shital Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1"&gt;Sebastien Bubeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language-Mediated, Object-Centric Representation Learning. (arXiv:2012.15814v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15814</id>
        <link href="http://arxiv.org/abs/2012.15814"/>
        <updated>2021-06-09T02:01:48.144Z</updated>
        <summary type="html"><![CDATA[We present Language-mediated, Object-centric Representation Learning (LORL),
a paradigm for learning disentangled, object-centric scene representations from
vision and language. LORL builds upon recent advances in unsupervised object
discovery and segmentation, notably MONet and Slot Attention. While these
algorithms learn an object-centric representation just by reconstructing the
input image, LORL enables them to further learn to associate the learned
representations to concepts, i.e., words for object categories, properties, and
spatial relationships, from language input. These object-centric concepts
derived from language facilitate the learning of object-centric
representations. LORL can be integrated with various unsupervised object
discovery algorithms that are language-agnostic. Experiments show that the
integration of LORL consistently improves the performance of unsupervised
object discovery methods on two datasets via the help of language. We also show
that concepts learned by LORL, in conjunction with object discovery methods,
aid downstream tasks such as referring expression comprehension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruocheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiayuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1"&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantically Controllable Scene Generation with Guidance of Explicit Knowledge. (arXiv:2106.04066v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04066</id>
        <link href="http://arxiv.org/abs/2106.04066"/>
        <updated>2021-06-09T02:01:48.139Z</updated>
        <summary type="html"><![CDATA[Deep Generative Models (DGMs) are known for their superior capability in
generating realistic data. Extending purely data-driven approaches, recent
specialized DGMs may satisfy additional controllable requirements such as
embedding a traffic sign in a driving scene, by manipulating patterns
\textit{implicitly} in the neuron or feature level. In this paper, we introduce
a novel method to incorporate domain knowledge \textit{explicitly} in the
generation process to achieve semantically controllable scene generation. We
categorize our knowledge into two types to be consistent with the composition
of natural scenes, where the first type represents the property of objects and
the second type represents the relationship among objects. We then propose a
tree-structured generative model to learn complex scene representation, whose
nodes and edges are naturally corresponding to the two types of knowledge
respectively. Knowledge can be explicitly integrated to enable semantically
controllable scene generation by imposing semantic rules on properties of nodes
and edges in the tree structure. We construct a synthetic example to illustrate
the controllability and explainability of our method in a clean setting. We
further extend the synthetic example to realistic autonomous vehicle driving
environments and conduct extensive experiments to show that our method
efficiently identifies adversarial traffic scenes against different
state-of-the-art 3D point cloud segmentation models satisfying the traffic
rules specified as the explicit knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenhao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1"&gt;Kim Ji Eun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Topology Divergence: a Framework for Comparing Data Manifolds. (arXiv:2106.04024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04024</id>
        <link href="http://arxiv.org/abs/2106.04024"/>
        <updated>2021-06-09T02:01:48.133Z</updated>
        <summary type="html"><![CDATA[We develop a framework for comparing data manifolds, aimed, in particular,
towards the evaluation of deep generative models. We describe a novel tool,
Cross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional
space, tracks multiscale topology spacial discrepancies between manifolds on
which the distributions are concentrated. Based on the Cross-Barcode, we
introduce the Manifold Topology Divergence score (MTop-Divergence) and apply it
to assess the performance of deep generative models in various domains: images,
3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN,
CIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate
that the MTop-Divergence accurately detects various degrees of mode-dropping,
intra-mode collapse, mode invention, and image disturbance. Our algorithm
scales well (essentially linearly) with the increase of the dimension of the
ambient high-dimensional space. It is one of the first TDA-based practical
methodologies that can be applied universally to datasets of different sizes
and dimensions, including the ones on which the most recent GANs in the visual
domain are trained. The proposed method is domain agnostic and does not rely on
pre-trained networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1"&gt;Serguei Barannikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1"&gt;Ilya Trofimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotnikov_G/0/1/0/all/0/1"&gt;Grigorii Sotnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trimbach_E/0/1/0/all/0/1"&gt;Ekaterina Trimbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1"&gt;Alexander Korotin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1"&gt;Alexander Filippov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bangla Natural Language Processing: A Comprehensive Review of Classical, Machine Learning, and Deep Learning Based Methods. (arXiv:2105.14875v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14875</id>
        <link href="http://arxiv.org/abs/2105.14875"/>
        <updated>2021-06-09T02:01:48.127Z</updated>
        <summary type="html"><![CDATA[The Bangla language is the seventh most spoken language, with 265 million
native and non-native speakers worldwide. However, English is the predominant
language for online resources and technical knowledge, journals, and
documentation. Consequently, many Bangla-speaking people, who have limited
command of English, face hurdles to utilize English resources. To bridge the
gap between limited support and increasing demand, researchers conducted many
experiments and developed valuable tools and techniques to create and process
Bangla language materials. Many efforts are also ongoing to make it easy to use
the Bangla language in the online and technical domains. There are some review
papers to understand the past, previous, and future Bangla Natural Language
Processing (BNLP) trends. The studies are mainly concentrated on the specific
domains of BNLP, such as sentiment analysis, speech recognition, optical
character recognition, and text summarization. There is an apparent scarcity of
resources that contain a comprehensive study of the recent BNLP tools and
methods. Therefore, in this paper, we present a thorough review of 71 BNLP
research papers and categorize them into 11 categories, namely Information
Extraction, Machine Translation, Named Entity Recognition, Parsing, Parts of
Speech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake
Detection, Text Summarization, Word Sense Disambiguation, and Speech Processing
and Recognition. We study articles published between 1999 to 2021, and 50% of
the papers were published after 2015. We discuss Classical, Machine Learning
and Deep Learning approaches with different datasets while addressing the
limitations and current and future trends of the BNLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sen_O/0/1/0/all/0/1"&gt;Ovishake Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Mohtasim Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;MD. Nazrul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbi_J/0/1/0/all/0/1"&gt;Jakaria Rabbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;MD. Kamrul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baz_M/0/1/0/all/0/1"&gt;Mohammed Baz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masud_M/0/1/0/all/0/1"&gt;Mehedi Masud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awal_M/0/1/0/all/0/1"&gt;Md. Abdul Awal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fime_A/0/1/0/all/0/1"&gt;Awal Ahmed Fime&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1"&gt;Md. Tahmid Hasan Fuad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sikder_D/0/1/0/all/0/1"&gt;Delowar Sikder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iftee_M/0/1/0/all/0/1"&gt;MD. Akil Raihan Iftee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I-BERT: Integer-only BERT Quantization. (arXiv:2101.01321v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01321</id>
        <link href="http://arxiv.org/abs/2101.01321"/>
        <updated>2021-06-09T02:01:48.121Z</updated>
        <summary type="html"><![CDATA[Transformer based models, like BERT and RoBERTa, have achieved
state-of-the-art results in many Natural Language Processing tasks. However,
their memory footprint, inference latency, and power consumption are
prohibitive efficient inference at the edge, and even at the data center. While
quantization can be a viable solution for this, previous work on quantizing
Transformer based models use floating-point arithmetic during inference, which
cannot efficiently utilize integer-only logical units such as the recent Turing
Tensor Cores, or traditional integer-only ARM processors. In this work, we
propose I-BERT, a novel quantization scheme for Transformer based models that
quantizes the entire inference with integer-only arithmetic. Based on
lightweight integer-only approximation methods for nonlinear operations, e.g.,
GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end
integer-only BERT inference without any floating point calculation. We evaluate
our approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that
for both cases, I-BERT achieves similar (and slightly higher) accuracy as
compared to the full-precision baseline. Furthermore, our preliminary
implementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4
GPU system as compared to FP32 inference. The framework has been developed in
PyTorch and has been open-sourced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1"&gt;Amir Gholami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lexical Semantic Recognition. (arXiv:2004.15008v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.15008</id>
        <link href="http://arxiv.org/abs/2004.15008"/>
        <updated>2021-06-09T02:01:48.103Z</updated>
        <summary type="html"><![CDATA[In lexical semantics, full-sentence segmentation and segment labeling of
various phenomena are generally treated separately, despite their
interdependence. We hypothesize that a unified lexical semantic recognition
task is an effective way to encapsulate previously disparate styles of
annotation, including multiword expression identification / classification and
supersense tagging. Using the STREUSLE corpus, we train a neural CRF sequence
tagger and evaluate its performance along various axes of annotation. As the
label set generalizes that of previous tasks (PARSEME, DiMSUM), we additionally
evaluate how well the model generalizes to those test sets, finding that it
approaches or surpasses existing models despite training only on STREUSLE. Our
work also establishes baseline models and evaluation metrics for integrated and
accurate modeling of lexical semantics, facilitating future work in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nelson F. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1"&gt;Daniel Hershcovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kranzlein_M/0/1/0/all/0/1"&gt;Michael Kranzlein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1"&gt;Nathan Schneider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Itihasa: A large-scale corpus for Sanskrit to English translation. (arXiv:2106.03269v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03269</id>
        <link href="http://arxiv.org/abs/2106.03269"/>
        <updated>2021-06-09T02:01:48.097Z</updated>
        <summary type="html"><![CDATA[This work introduces Itihasa, a large-scale translation dataset containing
93,000 pairs of Sanskrit shlokas and their English translations. The shlokas
are extracted from two Indian epics viz., The Ramayana and The Mahabharata. We
first describe the motivation behind the curation of such a dataset and follow
up with empirical analysis to bring out its nuances. We then benchmark the
performance of standard translation models on this corpus and show that even
state-of-the-art transformer architectures perform poorly, emphasizing the
complexity of the dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1"&gt;Rahul Aralikatte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1"&gt;Miryam de Lhoneux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1"&gt;Anoop Kunchukuttan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1"&gt;Anders S&amp;#xf8;gaard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Multi-scale Fusion Network for RGB-D Salient Object Detection. (arXiv:2106.03941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03941</id>
        <link href="http://arxiv.org/abs/2106.03941"/>
        <updated>2021-06-09T02:01:48.092Z</updated>
        <summary type="html"><![CDATA[Salient object detection(SOD) aims at locating the most significant object
within a given image. In recent years, great progress has been made in applying
SOD on many vision tasks. The depth map could provide additional spatial prior
and boundary cues to boost the performance. Combining the depth information
with image data obtained from standard visual cameras has been widely used in
recent SOD works, however, introducing depth information in a suboptimal fusion
strategy may have negative influence in the performance of SOD. In this paper,
we discuss about the advantages of the so-called progressive multi-scale fusion
method and propose a mask-guided feature aggregation module(MGFA). The proposed
framework can effectively combine the two features of different modalities and,
furthermore, alleviate the impact of erroneous depth features, which are
inevitably caused by the variation of depth quality. We further introduce a
mask-guided refinement module(MGRM) to complement the high-level semantic
features and reduce the irrelevant features from multi-scale fusion, leading to
an overall refinement of detection. Experiments on five challenging benchmarks
demonstrate that the proposed method outperforms 11 state-of-the-art methods
under different evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1"&gt;Guangyu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yanchu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1"&gt;Tianhong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stathaki_T/0/1/0/all/0/1"&gt;Tania Stathaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization. (arXiv:2105.12002v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12002</id>
        <link href="http://arxiv.org/abs/2105.12002"/>
        <updated>2021-06-09T02:01:48.084Z</updated>
        <summary type="html"><![CDATA[The Lottery Ticket Hypothesis suggests that an over-parametrized network
consists of ``lottery tickets'', and training a certain collection of them
(i.e., a subnetwork) can match the performance of the full model. In this
paper, we study such a collection of tickets, which is referred to as ``winning
tickets'', in extremely over-parametrized models, e.g., pre-trained language
models. We observe that at certain compression ratios, the generalization
performance of the winning tickets can not only match but also exceed that of
the full model. In particular, we observe a phase transition phenomenon: As the
compression ratio increases, generalization performance of the winning tickets
first improves then deteriorates after a certain threshold. We refer to the
tickets on the threshold as ``super tickets''. We further show that the phase
transition is task and model dependent -- as the model size becomes larger and
the training data set becomes smaller, the transition becomes more pronounced.
Our experiments on the GLUE benchmark show that the super tickets improve
single task fine-tuning by $0.9$ points on BERT-base and $1.0$ points on
BERT-large, in terms of task-average score. We also demonstrate that adaptively
sharing the super tickets across tasks benefits multi-task learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1"&gt;Simiao Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minshuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction. (arXiv:2106.03518v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03518</id>
        <link href="http://arxiv.org/abs/2106.03518"/>
        <updated>2021-06-09T02:01:48.078Z</updated>
        <summary type="html"><![CDATA[The Emotion Cause Extraction (ECE)} task aims to identify clauses which
contain emotion-evoking information for a particular emotion expressed in text.
We observe that a widely-used ECE dataset exhibits a bias that the majority of
annotated cause clauses are either directly before their associated emotion
clauses or are the emotion clauses themselves. Existing models for ECE tend to
explore such relative position information and suffer from the dataset bias. To
investigate the degree of reliance of existing ECE models on clause relative
positions, we propose a novel strategy to generate adversarial examples in
which the relative position information is no longer the indicative feature of
cause clauses. We test the performance of existing models on such adversarial
examples and observe a significant performance drop. To address the dataset
bias, we propose a novel graph-based method to explicitly model the emotion
triggering paths by leveraging the commonsense knowledge to enhance the
semantic dependencies between a candidate clause and an emotion clause.
Experimental results show that our proposed approach performs on par with the
existing state-of-the-art methods on the original ECE dataset, and is more
robust against adversarial attacks compared to existing models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hanqi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1"&gt;Lin Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1"&gt;Gabriele Pergola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Flows with Invertible Attentions. (arXiv:2106.03959v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03959</id>
        <link href="http://arxiv.org/abs/2106.03959"/>
        <updated>2021-06-09T02:01:48.072Z</updated>
        <summary type="html"><![CDATA[Flow-based generative models have shown excellent ability to explicitly learn
the probability density function of data via a sequence of invertible
transformations. Yet, modeling long-range dependencies over normalizing flows
remains understudied. To fill the gap, in this paper, we introduce two types of
invertible attention mechanisms for generative flow models. To be precise, we
propose map-based and scaled dot-product attention for unconditional and
conditional generative flow models. The key idea is to exploit split-based
attention mechanisms to learn the attention weights and input representations
on every two splits of flow feature maps. Our method provides invertible
attention modules with tractable Jacobian determinants, enabling seamless
integration of it at any positions of the flow-based models. The proposed
attention mechanism can model the global data dependencies, leading to more
comprehensive flow models. Evaluation on multiple generation tasks demonstrates
that the introduced attention flow idea results in efficient flow models and
compares favorably against the state-of-the-art unconditional and conditional
generative flow methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1"&gt;Rhea Sanjay Sukthanker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention Temperature Matters in Abstractive Summarization Distillation. (arXiv:2106.03441v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03441</id>
        <link href="http://arxiv.org/abs/2106.03441"/>
        <updated>2021-06-09T02:01:48.048Z</updated>
        <summary type="html"><![CDATA[Recent progress of abstractive text summarization largely relies on large
pre-trained sequence-to-sequence Transformer models, which are computationally
expensive. This paper aims to distill these large models into smaller ones for
faster inference and minimal performance loss. Pseudo-labeling based methods
are popular in sequence-to-sequence model distillation. In this paper, we find
simply manipulating attention temperatures in Transformers can make pseudo
labels easier to learn for student models. Our experiments on three
summarization datasets show our proposed method consistently improves over
vanilla pseudo-labeling based methods. We also find that both the pseudo labels
and summaries produced by our students are shorter and more abstractive. We
will make our code and models publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengqiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingxing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hangbo Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation. (arXiv:2009.07526v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07526</id>
        <link href="http://arxiv.org/abs/2009.07526"/>
        <updated>2021-06-09T02:01:48.042Z</updated>
        <summary type="html"><![CDATA[Scene graphs are semantic abstraction of images that encourage visual
understanding and reasoning. However, the performance of Scene Graph Generation
(SGG) is unsatisfactory when faced with biased data in real-world scenarios.
Conventional debiasing research mainly studies from the view of balancing data
distribution or learning unbiased models and representations, ignoring the
correlations among the biased classes. In this work, we analyze this problem
from a novel cognition perspective: automatically building a hierarchical
cognitive structure from the biased predictions and navigating that hierarchy
to locate the relationships, making the tail relationships receive more
attention in a coarse-to-fine mode. To this end, we propose a novel debiasing
Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive
structure CogTree to organize the relationships based on the prediction of a
biased SGG model. The CogTree distinguishes remarkably different relationships
at first and then focuses on a small portion of easily confused ones. Then, we
propose a debiasing loss specially for this cognitive structure, which supports
coarse-to-fine distinction for the correct relationships. The loss is
model-agnostic and consistently boosting the performance of several
state-of-the-art models. The code is available at:
https://github.com/CYVincent/Scene-Graph-Transformer-CogTree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuan Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yue Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation with GPT2. (arXiv:2004.02251v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.02251</id>
        <link href="http://arxiv.org/abs/2004.02251"/>
        <updated>2021-06-09T02:01:48.036Z</updated>
        <summary type="html"><![CDATA[The semantics of a text is manifested not only by what is read, but also by
what is not read. In this article, we will study how the implicit "not read"
information such as end-of-paragraph (\eop) and end-of-sequence (\eos) affect
the quality of text generation. Specifically, we find that the pre-trained
language model GPT2 can generate better continuations by learning to generate
the \eop in the fine-tuning stage. Experimental results on English story
generation show that \eop can lead to higher BLEU score and lower \eos
perplexity. We also conduct experiments on a self-collected Chinese essay
dataset with Chinese-GPT2, a character level LM without \eop or \eos during
pre-training. Experimental results show that the Chinese GPT2 can generate
better essay endings with \eop.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;He Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1"&gt;Peng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1"&gt;Luchen Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1"&gt;Kun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Ming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Suicidal Ideation and Mental Disorder Detection with Attentive Relation Networks. (arXiv:2004.07601v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07601</id>
        <link href="http://arxiv.org/abs/2004.07601"/>
        <updated>2021-06-09T02:01:48.030Z</updated>
        <summary type="html"><![CDATA[Mental health is a critical issue in modern society, and mental disorders
could sometimes turn to suicidal ideation without effective treatment. Early
detection of mental disorders and suicidal ideation from social content
provides a potential way for effective social intervention. However,
classifying suicidal ideation and other mental disorders is challenging as they
share similar patterns in language usage and sentimental polarity. This paper
enhances text representation with lexicon-based sentiment scores and latent
topics and proposes using relation networks to detect suicidal ideation and
mental disorders with related risk indicators. The relation module is further
equipped with the attention mechanism to prioritize more critical relational
features. Through experiments on three real-world datasets, our model
outperforms most of its counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shaoxiong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoQA: From Databases To QA Semantic Parsers With Only Synthetic Training Data. (arXiv:2010.04806v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04806</id>
        <link href="http://arxiv.org/abs/2010.04806"/>
        <updated>2021-06-09T02:01:48.024Z</updated>
        <summary type="html"><![CDATA[We propose AutoQA, a methodology and toolkit to generate semantic parsers
that answer questions on databases, with no manual effort. Given a database
schema and its data, AutoQA automatically generates a large set of high-quality
questions for training that covers different database operations. It uses
automatic paraphrasing combined with template-based parsing to find alternative
expressions of an attribute in different parts of speech. It also uses a novel
filtered auto-paraphraser to generate correct paraphrases of entire sentences.
We apply AutoQA to the Schema2QA dataset and obtain an average logical form
accuracy of 62.9% when tested on natural questions, which is only 6.4% lower
than a model trained with expert natural language annotations and paraphrase
data collected from crowdworkers. To demonstrate the generality of AutoQA, we
also apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy,
16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower
than the same model trained with human data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Silei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semnani_S/0/1/0/all/0/1"&gt;Sina J. Semnani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campagna_G/0/1/0/all/0/1"&gt;Giovanni Campagna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1"&gt;Monica S. Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Transformers. (arXiv:2106.04554v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04554</id>
        <link href="http://arxiv.org/abs/2106.04554"/>
        <updated>2021-06-09T02:01:48.008Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved great success in many artificial intelligence
fields, such as natural language processing, computer vision, and audio
processing. Therefore, it is natural to attract lots of interest from academic
and industry researchers. Up to the present, a great variety of Transformer
variants (a.k.a. X-formers) have been proposed, however, a systematic and
comprehensive literature review on these Transformer variants is still missing.
In this survey, we provide a comprehensive review of various X-formers. We
first briefly introduce the vanilla Transformer and then propose a new taxonomy
of X-formers. Next, we introduce the various X-formers from three perspectives:
architectural modification, pre-training, and applications. Finally, we outline
some potential directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task Transformation Learning for Robust Out-of-Distribution Detection. (arXiv:2106.03899v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.03899</id>
        <link href="http://arxiv.org/abs/2106.03899"/>
        <updated>2021-06-09T02:01:48.003Z</updated>
        <summary type="html"><![CDATA[Detecting out-of-distribution (OOD) samples plays a key role in open-world
and safety-critical applications such as autonomous systems and healthcare.
Self-supervised representation learning techniques (e.g., contrastive learning
and pretext learning) are well suited for learning representation that can
identify OOD samples. In this paper, we propose a simple framework that
leverages multi-task transformation learning for training effective
representation for OOD detection which outperforms state-of-the-art OOD
detection performance and robustness on several image datasets. We empirically
observe that the OOD performance depends on the choice of data transformations
which itself depends on the in-domain training set. To address this problem, we
propose a simple mechanism for selecting the transformations automatically and
modulate their effect on representation learning without requiring any OOD
training samples. We characterize the criteria for a desirable OOD detector for
real-world applications and demonstrate the efficacy of our proposed technique
against a diverse range of the state-of-the-art OOD detection techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohseni_S/0/1/0/all/0/1"&gt;Sina Mohseni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1"&gt;Arash Vahdat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadawa_J/0/1/0/all/0/1"&gt;Jay Yadawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets. (arXiv:2101.00063v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00063</id>
        <link href="http://arxiv.org/abs/2101.00063"/>
        <updated>2021-06-09T02:01:47.997Z</updated>
        <summary type="html"><![CDATA[Heavily overparameterized language models such as BERT, XLNet and T5 have
achieved impressive success in many NLP tasks. However, their high model
complexity requires enormous computation resources and extremely long training
time for both pre-training and fine-tuning. Many works have studied model
compression on large NLP models, but only focusing on reducing inference time
while still requiring an expensive training process. Other works use extremely
large batch sizes to shorten the pre-training time, at the expense of higher
computational resource demands. In this paper, inspired by the Early-Bird
Lottery Tickets recently studied for computer vision tasks, we propose
EarlyBERT, a general computationally-efficient training algorithm applicable to
both pre-training and fine-tuning of large-scale language models. By slimming
the self-attention and fully-connected sub-layers inside a transformer, we are
the first to identify structured winning tickets in the early stage of BERT
training. We apply those tickets towards efficient BERT training, and conduct
comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD
downstream tasks. Our results show that EarlyBERT achieves comparable
performance to standard BERT, with 35~45% less training time. Code is available
at https://github.com/VITA-Group/EarlyBERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TIMEDIAL: Temporal Commonsense Reasoning in Dialog. (arXiv:2106.04571v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04571</id>
        <link href="http://arxiv.org/abs/2106.04571"/>
        <updated>2021-06-09T02:01:47.991Z</updated>
        <summary type="html"><![CDATA[Everyday conversations require understanding everyday events, which in turn,
requires understanding temporal commonsense concepts interwoven with those
events. Despite recent progress with massive pre-trained language models (LMs)
such as T5 and GPT-3, their capability of temporal reasoning in dialogs remains
largely under-explored. In this paper, we present the first study to
investigate pre-trained LMs for their temporal reasoning capabilities in
dialogs by introducing a new task and a crowd-sourced English challenge set,
TIMEDIAL. We formulate TIME-DIAL as a multiple-choice cloze task with over 1.1K
carefully curated dialogs. Empirical results demonstrate that even the best
performing models struggle on this task compared to humans, with 23 absolute
points of gap in accuracy. Furthermore, our analysis reveals that the models
fail to reason about dialog context correctly; instead, they rely on shallow
cues based on existing temporal patterns in context, motivating future research
for modeling temporal concepts in text and robust contextual reasoning about
them. The dataset is publicly available at:
https://github.com/google-research-datasets/timedial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1"&gt;Lianhui Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Aditya Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Shyam Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Luheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faruqui_M/0/1/0/all/0/1"&gt;Manaal Faruqui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doing Natural Language Processing in A Natural Way: An NLP toolkit based on object-oriented knowledge base and multi-level grammar base. (arXiv:2105.05227v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05227</id>
        <link href="http://arxiv.org/abs/2105.05227"/>
        <updated>2021-06-09T02:01:47.985Z</updated>
        <summary type="html"><![CDATA[We introduce an NLP toolkit based on object-oriented knowledge base and
multi-level grammar base. This toolkit focuses on semantic parsing, it also has
abilities to discover new knowledge and grammar automatically, new discovered
knowledge and grammar will be identified by human, and will be used to update
the knowledge base and grammar base. This process can be iterated many times to
improve the toolkit continuously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stabilizing Label Assignment for Speech Separation by Self-supervised Pre-training. (arXiv:2010.15366v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15366</id>
        <link href="http://arxiv.org/abs/2010.15366"/>
        <updated>2021-06-09T02:01:47.966Z</updated>
        <summary type="html"><![CDATA[Speech separation has been well developed, with the very successful
permutation invariant training (PIT) approach, although the frequent label
assignment switching happening during PIT training remains to be a problem when
better convergence speed and achievable performance are desired. In this paper,
we propose to perform self-supervised pre-training to stabilize the label
assignment in training the speech separation model. Experiments over several
types of self-supervised approaches, several typical speech separation models
and two different datasets showed that very good improvements are achievable if
a proper self-supervised approach is chosen.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Sung-Feng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1"&gt;Shun-Po Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Da-Rong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Gene-Ping Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using a New Nonlinear Gradient Method for Solving Large Scale Convex Optimization Problems with an Application on Arabic Medical Text. (arXiv:2106.04383v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.04383</id>
        <link href="http://arxiv.org/abs/2106.04383"/>
        <updated>2021-06-09T02:01:47.960Z</updated>
        <summary type="html"><![CDATA[Gradient methods have applications in multiple fields, including signal
processing, image processing, and dynamic systems. In this paper, we present a
nonlinear gradient method for solving convex supra-quadratic functions by
developing the search direction, that done by hybridizing between the two
conjugate coefficients HRM [2] and NHS [1]. The numerical results proved the
effectiveness of the presented method by applying it to solve standard problems
and reaching the exact solution if the objective function is quadratic convex.
Also presented in this article, an application to the problem of named entities
in the Arabic medical language, as it proved the stability of the proposed
method and its efficiency in terms of execution time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hammoud_J/0/1/0/all/0/1"&gt;Jaafar Hammoud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Eisab_A/0/1/0/all/0/1"&gt;Ali Eisab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Dobrenkoa_N/0/1/0/all/0/1"&gt;Natalia Dobrenkoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gusarovaa_N/0/1/0/all/0/1"&gt;Natalia Gusarovaa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation. (arXiv:2106.04447v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04447</id>
        <link href="http://arxiv.org/abs/2106.04447"/>
        <updated>2021-06-09T02:01:47.955Z</updated>
        <summary type="html"><![CDATA[Answering a programming question using only its title is difficult as salient
contextual information is omitted. Based on this observation, we present a
corpus of over 40,000 StackOverflow question texts to be used in conjunction
with their corresponding intents from the CoNaLa dataset (Yin et al., 2018).
Using both the intent and question body, we use BART to establish a baseline
BLEU score of 34.35 for this new task. We find further improvements of $2.8\%$
by combining the mined CoNaLa data with the labeled data to achieve a 35.32
BLEU score. We evaluate prior state-of-the-art CoNaLa models with this
additional data and find that our proposed method of using the body and mined
data beats the BLEU score of the prior state-of-the-art by $71.96\%$. Finally,
we perform ablations to demonstrate that BART is an unsupervised multimodal
learner and examine its extractive behavior. The code and data can be found
https://github.com/gabeorlanski/stackoverflow-encourages-cheating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orlanski_G/0/1/0/all/0/1"&gt;Gabriel Orlanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1"&gt;Alex Gittens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection. (arXiv:2106.04564v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04564</id>
        <link href="http://arxiv.org/abs/2106.04564"/>
        <updated>2021-06-09T02:01:47.948Z</updated>
        <summary type="html"><![CDATA[Pretrained Transformer-based models were reported to be robust in intent
classification. In this work, we first point out the importance of in-domain
out-of-scope detection in few-shot intent recognition tasks and then illustrate
the vulnerability of pretrained Transformer-based models against samples that
are in-domain but out-of-scope (ID-OOS). We empirically show that pretrained
models do not perform well on both ID-OOS examples and general out-of-scope
examples, especially on fine-grained few-shot intent detection tasks. To figure
out how the models mistakenly classify ID-OOS intents as in-scope intents, we
further conduct analysis on confidence scores and the overlapping keywords and
provide several prospective directions for future work. We release the relevant
resources to facilitate future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian-Guo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1"&gt;Kazuma Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1"&gt;Yao Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ye Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04563</id>
        <link href="http://arxiv.org/abs/2106.04563"/>
        <updated>2021-06-09T02:01:47.932Z</updated>
        <summary type="html"><![CDATA[While deep and large pre-trained models are the state-of-the-art for various
natural language processing tasks, their huge size poses significant challenges
for practical uses in resource constrained settings. Recent works in knowledge
distillation propose task-agnostic as well as task-specific methods to compress
these models, with task-specific ones often yielding higher compression rate.
In this work, we develop a new task-agnostic distillation framework
XtremeDistilTransformers that leverages the advantage of task-specific methods
for learning a small universal model that can be applied to arbitrary tasks and
languages. To this end, we study the transferability of several source tasks,
augmentation resources and model architecture for distillation. We evaluate our
model performance on multiple tasks, including the General Language
Understanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and
a massive multi-lingual NER dataset with 41 languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhabrata Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Lifelong Learning of End-to-end ASR. (arXiv:2104.01616v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01616</id>
        <link href="http://arxiv.org/abs/2104.01616"/>
        <updated>2021-06-09T02:01:47.915Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition (ASR) technologies today are primarily optimized
for given datasets; thus, any changes in the application environment (e.g.,
acoustic conditions or topic domains) may inevitably degrade the performance.
We can collect new data describing the new environment and fine-tune the
system, but this naturally leads to higher error rates for the earlier
datasets, referred to as catastrophic forgetting. The concept of lifelong
learning (LLL) aiming to enable a machine to sequentially learn new tasks from
new datasets describing the changing real world without forgetting the
previously learned knowledge is thus brought to attention. This paper reports,
to our knowledge, the first effort to extensively consider and analyze the use
of various approaches of LLL in end-to-end (E2E) ASR, including proposing novel
methods in saving data for past domains to mitigate the catastrophic forgetting
problem. An overall relative reduction of 28.7% in WER was achieved compared to
the fine-tuning baseline when sequentially learning on three very different
benchmark corpora. This can be the first step toward the highly desired ASR
technologies capable of synchronizing with the continuously changing real
world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Heng-Jui Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1"&gt;Lin-shan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turing: an Accurate and Interpretable Multi-Hypothesis Cross-Domain Natural Language Database Interface. (arXiv:2106.04559v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04559</id>
        <link href="http://arxiv.org/abs/2106.04559"/>
        <updated>2021-06-09T02:01:47.909Z</updated>
        <summary type="html"><![CDATA[A natural language database interface (NLDB) can democratize data-driven
insights for non-technical users. However, existing Text-to-SQL semantic
parsers cannot achieve high enough accuracy in the cross-database setting to
allow good usability in practice. This work presents Turing, a NLDB system
toward bridging this gap. The cross-domain semantic parser of Turing with our
novel value prediction method achieves $75.1\%$ execution accuracy, and
$78.3\%$ top-5 beam execution accuracy on the Spider validation set. To benefit
from the higher beam accuracy, we design an interactive system where the SQL
hypotheses in the beam are explained step-by-step in natural language, with
their differences highlighted. The user can then compare and judge the
hypotheses to select which one reflects their intention if any. The English
explanations of SQL queries in Turing are produced by our high-precision
natural language generation system based on synchronous grammars.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zi_W/0/1/0/all/0/1"&gt;Wenjie Zi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahidi_H/0/1/0/all/0/1"&gt;Hamidreza Shahidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadar_A/0/1/0/all/0/1"&gt;&amp;#xc1;kos K&amp;#xe1;d&amp;#xe1;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Keyi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ateeq_J/0/1/0/all/0/1"&gt;Jawad Ateeq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barot_H/0/1/0/all/0/1"&gt;Harsh Barot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alon_M/0/1/0/all/0/1"&gt;Meidan Alon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yanshuai Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Obtaining Better Static Word Embeddings Using Contextual Embedding Models. (arXiv:2106.04302v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04302</id>
        <link href="http://arxiv.org/abs/2106.04302"/>
        <updated>2021-06-09T02:01:47.903Z</updated>
        <summary type="html"><![CDATA[The advent of contextual word embeddings -- representations of words which
incorporate semantic and syntactic information from their context -- has led to
tremendous improvements on a wide variety of NLP tasks. However, recent
contextual models have prohibitively high computational cost in many use-cases
and are often hard to interpret. In this work, we demonstrate that our proposed
distillation method, which is a simple extension of CBOW-based training, allows
to significantly improve computational efficiency of NLP applications, while
outperforming the quality of existing static embeddings trained from scratch as
well as those distilled from previously proposed methods. As a side-effect, our
approach also allows a fair comparison of both contextual and static embeddings
via standard lexical evaluation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Prakhar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLTR: An End-to-End, Transformer-Based System for Cell Level TableRetrieval and Table Question Answering. (arXiv:2106.04441v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04441</id>
        <link href="http://arxiv.org/abs/2106.04441"/>
        <updated>2021-06-09T02:01:47.855Z</updated>
        <summary type="html"><![CDATA[We present the first end-to-end, transformer-based table question answering
(QA) system that takes natural language questions and massive table corpus as
inputs to retrieve the most relevant tables and locate the correct table cells
to answer the question. Our system, CLTR, extends the current state-of-the-art
QA over tables model to build an end-to-end table QA architecture. This system
has successfully tackled many real-world table QA problems with a simple,
unified pipeline. Our proposed system can also generate a heatmap of candidate
columns and rows over complex tables and allow users to quickly identify the
correct cells to answer questions. In addition, we introduce two new
open-domain benchmarks, E2E_WTQ and E2E_GNQ, consisting of 2,005 natural
language questions over 76,242 tables. The benchmarks are designed to validate
CLTR as well as accommodate future table retrieval and end-to-end table QA
research and experiments. Our experiments demonstrate that our system is the
current state-of-the-art model on the table retrieval task and produces
promising results for end-to-end table QA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feifei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canim_M/0/1/0/all/0/1"&gt;Mustafa Canim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1"&gt;Michael Glass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1"&gt;Alfio Gliozzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_P/0/1/0/all/0/1"&gt;Peter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04484</id>
        <link href="http://arxiv.org/abs/2106.04484"/>
        <updated>2021-06-09T02:01:47.849Z</updated>
        <summary type="html"><![CDATA[Deep learning algorithms have shown promising results in visual question
answering (VQA) tasks, but a more careful look reveals that they often do not
understand the rich signal they are being fed with. To understand and better
measure the generalization capabilities of VQA systems, we look at their
robustness to counterfactually augmented data. Our proposed augmentations are
designed to make a focused intervention on a specific property of the question
such that the answer changes. Using these augmentations, we propose a new
robustness measure, Robustness to Augmented Data (RAD), which measures the
consistency of model predictions between original and augmented examples.
Through extensive experimentation, we show that RAD, unlike classical accuracy
measures, can quantify when state-of-the-art systems are not robust to
counterfactuals. We find substantial failure cases which reveal that current
VQA systems are still brittle. Finally, we connect between robustness and
generalization, demonstrating the predictive power of RAD for performance on
unseen augmentations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1"&gt;Daniel Rosenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1"&gt;Itai Gat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1"&gt;Amir Feder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1"&gt;Roi Reichart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks. (arXiv:2106.04489v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04489</id>
        <link href="http://arxiv.org/abs/2106.04489"/>
        <updated>2021-06-09T02:01:47.770Z</updated>
        <summary type="html"><![CDATA[State-of-the-art parameter-efficient fine-tuning methods rely on introducing
adapter modules between the layers of a pretrained language model. However,
such modules are trained separately for each task and thus do not enable
sharing information across tasks. In this paper, we show that we can learn
adapter parameters for all layers and tasks by generating them using shared
hypernetworks, which condition on task, adapter position, and layer id in a
transformer model. This parameter-efficient multi-task learning framework
allows us to achieve the best of both worlds by sharing knowledge across tasks
via hypernetworks while enabling the model to adapt to each individual task
through task-specific adapters. Experiments on the well-known GLUE benchmark
show improved performance in multi-task learning while adding only 0.29%
parameters per task. We additionally demonstrate substantial performance
improvements in few-shot domain generalization across a variety of tasks. Our
code is publicly available in https://github.com/rabeehk/hyperformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahabadi_R/0/1/0/all/0/1"&gt;Rabeeh Karimi Mahabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1"&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1"&gt;James Henderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Training for Machine Reading Comprehension with Virtual Embeddings. (arXiv:2106.04437v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04437</id>
        <link href="http://arxiv.org/abs/2106.04437"/>
        <updated>2021-06-09T02:01:47.677Z</updated>
        <summary type="html"><![CDATA[Adversarial training (AT) as a regularization method has proved its
effectiveness on various tasks. Though there are successful applications of AT
on some NLP tasks, the distinguishing characteristics of NLP tasks have not
been exploited. In this paper, we aim to apply AT on machine reading
comprehension (MRC) tasks. Furthermore, we adapt AT for MRC tasks by proposing
a novel adversarial training method called PQAT that perturbs the embedding
matrix instead of word vectors. To differentiate the roles of passages and
questions, PQAT uses additional virtual P/Q-embedding matrices to gather the
global perturbations of words from passages and questions separately. We test
the method on a wide range of MRC tasks, including span-based extractive RC and
multiple-choice RC. The results show that adversarial training is effective
universally, and PQAT further improves the performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziqing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yiming Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Chenglei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1"&gt;Wanxiang Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shijin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guoping Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question Generation for Adaptive Education. (arXiv:2106.04262v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04262</id>
        <link href="http://arxiv.org/abs/2106.04262"/>
        <updated>2021-06-09T02:01:47.656Z</updated>
        <summary type="html"><![CDATA[Intelligent and adaptive online education systems aim to make high-quality
education available for a diverse range of students. However, existing systems
usually depend on a pool of hand-made questions, limiting how fine-grained and
open-ended they can be in adapting to individual students. We explore targeted
question generation as a controllable sequence generation task. We first show
how to fine-tune pre-trained language models for deep knowledge tracing
(LM-KT). This model accurately predicts the probability of a student answering
a question correctly, and generalizes to questions not seen in training. We
then use LM-KT to specify the objective and data for training a model to
generate questions conditioned on the student and target difficulty. Our
results show we succeed at generating novel, well-calibrated language
translation questions for second language learners from a real online education
platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1"&gt;Megha Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Graph enhanced Embedding Neural Network for CTR Prediction. (arXiv:2106.00314v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00314</id>
        <link href="http://arxiv.org/abs/2106.00314"/>
        <updated>2021-06-09T02:01:47.582Z</updated>
        <summary type="html"><![CDATA[CTR prediction, which aims to estimate the probability that a user will click
an item, plays a crucial role in online advertising and recommender system.
Feature interaction modeling based and user interest mining based methods are
the two kinds of most popular techniques that have been extensively explored
for many years and have made great progress for CTR prediction. However, (1)
feature interaction based methods which rely heavily on the co-occurrence of
different features, may suffer from the feature sparsity problem (i.e., many
features appear few times); (2) user interest mining based methods which need
rich user behaviors to obtain user's diverse interests, are easy to encounter
the behavior sparsity problem (i.e., many users have very short behavior
sequences). To solve these problems, we propose a novel module named Dual Graph
enhanced Embedding, which is compatible with various CTR prediction models to
alleviate these two problems. We further propose a Dual Graph enhanced
Embedding Neural Network (DG-ENN) for CTR prediction. Dual Graph enhanced
Embedding exploits the strengths of graph representation with two carefully
designed learning strategies (divide-and-conquer, curriculum-learning-inspired
organized learning) to refine the embedding. We conduct comprehensive
experiments on three real-world industrial datasets. The experimental results
show that our proposed DG-ENN significantly outperforms state-of-the-art CTR
prediction models. Moreover, when applying to state-of-the-art CTR prediction
models, Dual graph enhanced embedding always obtains better performance.
Further case studies prove that our proposed dual graph enhanced embedding
could alleviate the feature sparsity and behavior sparsity problems. Our
framework will be open-source based on MindSpore in the near future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1"&gt;Rong Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1"&gt;Renhao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Huifeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yingxue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhirong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation. (arXiv:2106.04403v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04403</id>
        <link href="http://arxiv.org/abs/2106.04403"/>
        <updated>2021-06-09T02:01:47.564Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have brought significant progress in visual
grounding tasks such as language-guided video object segmentation. However,
collecting large datasets for these tasks is expensive in terms of annotation
time, which represents a bottleneck. To this end, we propose a novel method,
namely SynthRef, for generating synthetic referring expressions for target
objects in an image (or video frame), and we also present and disseminate the
first large-scale dataset with synthetic referring expressions for video object
segmentation. Our experiments demonstrate that by training with our synthetic
referring expressions one can improve the ability of a model to generalize
across different datasets, without any additional annotation cost. Moreover,
our formulation allows its application to any object detection or segmentation
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazakos_I/0/1/0/all/0/1"&gt;Ioannis Kazakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_C/0/1/0/all/0/1"&gt;Carles Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellver_M/0/1/0/all/0/1"&gt;Miriam Bellver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1"&gt;Carina Silberer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Giro-i-Nieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning compositional structures for semantic graph parsing. (arXiv:2106.04398v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04398</id>
        <link href="http://arxiv.org/abs/2106.04398"/>
        <updated>2021-06-09T02:01:47.559Z</updated>
        <summary type="html"><![CDATA[AM dependency parsing is a method for neural semantic graph parsing that
exploits the principle of compositionality. While AM dependency parsers have
been shown to be fast and accurate across several graphbanks, they require
explicit annotations of the compositional tree structures for training. In the
past, these were obtained using complex graphbank-specific heuristics written
by experts. Here we show how they can instead be trained directly on the graphs
with a neural latent-variable model, drastically reducing the amount and
complexity of manual heuristics. We demonstrate that our model picks up on
several linguistic phenomena on its own and achieves comparable accuracy to
supervised training, greatly facilitating the use of AM dependency parsing for
new sembanks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Groschwitz_J/0/1/0/all/0/1"&gt;Jonas Groschwitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowlie_M/0/1/0/all/0/1"&gt;Meaghan Fowlie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1"&gt;Alexander Koller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hash Layers For Large Sparse Models. (arXiv:2106.04426v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04426</id>
        <link href="http://arxiv.org/abs/2106.04426"/>
        <updated>2021-06-09T02:01:47.553Z</updated>
        <summary type="html"><![CDATA[We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cyberbullying Detection Using Deep Neural Network from Social Media Comments in Bangla Language. (arXiv:2106.04506v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04506</id>
        <link href="http://arxiv.org/abs/2106.04506"/>
        <updated>2021-06-09T02:01:47.548Z</updated>
        <summary type="html"><![CDATA[Cyberbullying or Online harassment detection on social media for various
major languages is currently being given a good amount of focus by researchers
worldwide. Being the seventh most speaking language in the world and increasing
usage of online platform among the Bengali speaking people urge to find
effective detection technique to handle the online harassment. In this paper,
we have proposed binary and multiclass classification model using hybrid neural
network for bully expression detection in Bengali language. We have used 44,001
users comments from popular public Facebook pages, which fall into five classes
- Non-bully, Sexual, Threat, Troll and Religious. We have examined the
performance of our proposed models from different perspective. Our binary
classification model gives 87.91% accuracy, whereas introducing ensemble
technique after neural network for multiclass classification, we got 85%
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1"&gt;Md Faisal Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmud_Z/0/1/0/all/0/1"&gt;Zalish Mahmud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biash_Z/0/1/0/all/0/1"&gt;Zarin Tasnim Biash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryen_A/0/1/0/all/0/1"&gt;Ahmed Ann Noor Ryen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_A/0/1/0/all/0/1"&gt;Arman Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashraf_F/0/1/0/all/0/1"&gt;Faisal Bin Ashraf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Generative Framework for Aspect-Based Sentiment Analysis. (arXiv:2106.04300v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04300</id>
        <link href="http://arxiv.org/abs/2106.04300"/>
        <updated>2021-06-09T02:01:47.541Z</updated>
        <summary type="html"><![CDATA[Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms,
their corresponding sentiment polarities, and the opinion terms. There exist
seven subtasks in ABSA. Most studies only focus on the subsets of these
subtasks, which leads to various complicated ABSA models while hard to solve
these subtasks in a unified framework. In this paper, we redefine every subtask
target as a sequence mixed by pointer indexes and sentiment class indexes,
which converts all ABSA subtasks into a unified generative formulation. Based
on the unified formulation, we exploit the pre-training sequence-to-sequence
model BART to solve all ABSA subtasks in an end-to-end framework. Extensive
experiments on four ABSA datasets for seven subtasks demonstrate that our
framework achieves substantial performance gain and provides a real unified
end-to-end solution for the whole ABSA subtasks, which could benefit multiple
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Junqi Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ji_T/0/1/0/all/0/1"&gt;Tuo ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets. (arXiv:2106.04476v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04476</id>
        <link href="http://arxiv.org/abs/2106.04476"/>
        <updated>2021-06-09T02:01:47.536Z</updated>
        <summary type="html"><![CDATA[Semantic parsers map natural language utterances to meaning representations.
The lack of a single standard for meaning representations led to the creation
of a plethora of semantic parsing datasets. To unify different datasets and
train a single model for them, we investigate the use of Multi-Task Learning
(MTL) architectures. We experiment with five datasets (Geoquery, NLMaps, TOP,
Overnight, AMR). We find that an MTL architecture that shares the entire
network across datasets yields competitive or better parsing accuracies than
the single-task baselines, while reducing the total number of parameters by
68%. We further provide evidence that MTL has also better compositional
generalization than single-task models. We also present a comparison of task
sampling methods and propose a competitive alternative to widespread
proportional sampling strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Damonte_M/0/1/0/all/0/1"&gt;Marco Damonte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monti_E/0/1/0/all/0/1"&gt;Emilio Monti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable agent communication from scratch(with a generic visual processor emerging on the side). (arXiv:2106.04258v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04258</id>
        <link href="http://arxiv.org/abs/2106.04258"/>
        <updated>2021-06-09T02:01:47.519Z</updated>
        <summary type="html"><![CDATA[As deep networks begin to be deployed as autonomous agents, the issue of how
they can communicate with each other becomes important. Here, we train two deep
nets from scratch to perform realistic referent identification through
unsupervised emergent communication. We show that the largely interpretable
emergent protocol allows the nets to successfully communicate even about object
types they did not see at training time. The visual representations induced as
a by-product of our training regime, moreover, show comparable quality, when
re-used as generic visual features, to a recent self-supervised learning model.
Our results provide concrete evidence of the viability of (interpretable)
emergent deep net communication in a more realistic scenario than previously
considered, as well as establishing an intriguing link between this field and
self-supervised visual learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1"&gt;Roberto Dess&amp;#xec;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1"&gt;Eugene Kharitonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1"&gt;Marco Baroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Staircase Attention for Recurrent Processing of Sequences. (arXiv:2106.04279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.04279</id>
        <link href="http://arxiv.org/abs/2106.04279"/>
        <updated>2021-06-09T02:01:47.513Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms have become a standard tool for sequence modeling tasks,
in particular by stacking self-attention layers over the entire input sequence
as in the Transformer architecture. In this work we introduce a novel attention
procedure called staircase attention that, unlike self-attention, operates
across the sequence (in time) recurrently processing the input by adding
another step of processing. A step in the staircase comprises of backward
tokens (encoding the sequence so far seen) and forward tokens (ingesting a new
part of the sequence), or an extreme Ladder version with a forward step of zero
that simply repeats the Transformer on each step of the ladder, sharing the
weights. We thus describe a family of such models that can trade off
performance and compute, by either increasing the amount of recurrence through
time, the amount of sequential processing via recurrence in depth, or both.
Staircase attention is shown to be able to solve tasks that involve tracking
that conventional Transformers cannot, due to this recurrence. Further, it is
shown to provide improved modeling power for the same size model (number of
parameters) compared to self-attentive Transformers on large language modeling
and dialogue tasks, yielding significant perplexity gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1"&gt;Da Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1"&gt;Stephen Roller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1"&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperbolic Temporal Knowledge Graph Embeddings with Relational and Time Curvatures. (arXiv:2106.04311v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04311</id>
        <link href="http://arxiv.org/abs/2106.04311"/>
        <updated>2021-06-09T02:01:47.508Z</updated>
        <summary type="html"><![CDATA[Knowledge Graph (KG) completion has been excessively studied with a massive
number of models proposed for the Link Prediction (LP) task. The main
limitation of such models is their insensitivity to time. Indeed, the temporal
aspect of stored facts is often ignored. To this end, more and more works
consider time as a parameter to complete KGs. In this paper, we first
demonstrate that, by simply increasing the number of negative samples, the
recent AttH model can achieve competitive or even better performance than the
state-of-the-art on Temporal KGs (TKGs), albeit its nontemporality. We further
propose Hercules, a time-aware extension of AttH model, which defines the
curvature of a Riemannian manifold as the product of both relation and time.
Our experiments show that both Hercules and AttH achieve competitive or new
state-of-the-art performances on ICEWS04 and ICEWS05-15 datasets. Therefore,
one should raise awareness when learning TKGs representations to identify
whether time truly boosts performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montella_S/0/1/0/all/0/1"&gt;Sebastien Montella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1"&gt;Lina Rojas-Barahona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinecke_J/0/1/0/all/0/1"&gt;Johannes Heinecke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realistic Evaluation Principles for Cross-document Coreference Resolution. (arXiv:2106.04192v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04192</id>
        <link href="http://arxiv.org/abs/2106.04192"/>
        <updated>2021-06-09T02:01:47.502Z</updated>
        <summary type="html"><![CDATA[We point out that common evaluation practices for cross-document coreference
resolution have been unrealistically permissive in their assumed settings,
yielding inflated results. We propose addressing this issue via two evaluation
methodology principles. First, as in other tasks, models should be evaluated on
predicted mentions rather than on gold mentions. Doing this raises a subtle
issue regarding singleton coreference clusters, which we address by decoupling
the evaluation of mention detection from that of coreference linking. Second,
we argue that models should not exploit the synthetic topic structure of the
standard ECB+ dataset, forcing models to confront the lexical ambiguity
challenge, as intended by the dataset creators. We demonstrate empirically the
drastic impact of our more realistic evaluation principles on a competitive
model, yielding a score which is 33 F1 lower compared to evaluating by prior
lenient practices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1"&gt;Arie Cattan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eirew_A/0/1/0/all/0/1"&gt;Alon Eirew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1"&gt;Gabriel Stanovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1"&gt;Mandar Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1"&gt;Ido Dagan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Word Segmentation from Discrete Speech Units in Low-Resource Settings. (arXiv:2106.04298v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04298</id>
        <link href="http://arxiv.org/abs/2106.04298"/>
        <updated>2021-06-09T02:01:47.496Z</updated>
        <summary type="html"><![CDATA[When documenting oral-languages, Unsupervised Word Segmentation (UWS) from
speech is a useful, yet challenging, task. It can be performed from phonetic
transcriptions, or in the absence of these, from the output of unsupervised
speech discretization models. These discretization models are trained using raw
speech only, producing discrete speech units which can be applied for
downstream (text-based) tasks. In this paper we compare five of these models:
three Bayesian and two neural approaches, with regards to the exploitability of
the produced units for UWS. Two UWS models are experimented with and we report
results for Finnish, Hungarian, Mboshi, Romanian and Russian in a low-resource
setting (using only 5k sentences). Our results suggest that neural models for
speech discretization are difficult to exploit in our setting, and that it
might be necessary to adapt them to limit sequence length. We obtain our best
UWS results by using the SHMM and H-SHMM Bayesian models, which produce high
quality, yet compressed, discrete representations of the input speech signal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1"&gt;Marcely Zanon Boito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yusuf_B/0/1/0/all/0/1"&gt;Bolaji Yusuf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ondel_L/0/1/0/all/0/1"&gt;Lucas Ondel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1"&gt;Aline Villavicencio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lexicon Learning for Few-Shot Neural Sequence Modeling. (arXiv:2106.03993v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03993</id>
        <link href="http://arxiv.org/abs/2106.03993"/>
        <updated>2021-06-09T02:01:47.480Z</updated>
        <summary type="html"><![CDATA[Sequence-to-sequence transduction is the core problem in language processing
applications as diverse as semantic parsing, machine translation, and
instruction following. The neural network models that provide the dominant
solution to these problems are brittle, especially in low-resource settings:
they fail to generalize correctly or systematically from small datasets. Past
work has shown that many failures of systematic generalization arise from
neural models' inability to disentangle lexical phenomena from syntactic ones.
To address this, we augment neural decoders with a lexical translation
mechanism that generalizes existing copy mechanisms to incorporate learned,
decontextualized, token-level translation rules. We describe how to initialize
this mechanism using a variety of lexicon learning algorithms, and show that it
improves systematic generalization on a diverse set of sequence modeling tasks
drawn from cognitive science, formal semantics, and machine translation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1"&gt;Ekin Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Falta de Pan, Buenas Son Tortas: The Efficacy of Predicted UPOS Tags for Low Resource UD Parsing. (arXiv:2106.04222v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04222</id>
        <link href="http://arxiv.org/abs/2106.04222"/>
        <updated>2021-06-09T02:01:47.474Z</updated>
        <summary type="html"><![CDATA[We evaluate the efficacy of predicted UPOS tags as input features for
dependency parsers in lower resource settings to evaluate how treebank size
affects the impact tagging accuracy has on parsing performance. We do this for
real low resource universal dependency treebanks, artificially low resource
data with varying treebank sizes, and for very small treebanks with varying
amounts of augmented data. We find that predicted UPOS tags are somewhat
helpful for low resource treebanks, especially when fewer fully-annotated trees
are available. We also find that this positive impact diminishes as the amount
of data increases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1"&gt;Mark Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehouck_M/0/1/0/all/0/1"&gt;Mathieu Dehouck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_C/0/1/0/all/0/1"&gt;Carlos G&amp;#xf3;mez Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disfl-QA: A Benchmark Dataset for Understanding Disfluencies in Question Answering. (arXiv:2106.04016v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04016</id>
        <link href="http://arxiv.org/abs/2106.04016"/>
        <updated>2021-06-09T02:01:47.468Z</updated>
        <summary type="html"><![CDATA[Disfluencies is an under-studied topic in NLP, even though it is ubiquitous
in human conversation. This is largely due to the lack of datasets containing
disfluencies. In this paper, we present a new challenge question answering
dataset, Disfl-QA, a derivative of SQuAD, where humans introduce contextual
disfluencies in previously fluent questions. Disfl-QA contains a variety of
challenging disfluencies that require a more comprehensive understanding of the
text than what was necessary in prior datasets. Experiments show that the
performance of existing state-of-the-art question answering models degrades
significantly when tested on Disfl-QA in a zero-shot setting.We show data
augmentation methods partially recover the loss in performance and also
demonstrate the efficacy of using gold data for fine-tuning. We argue that we
need large-scale disfluency datasets in order for NLP models to be robust to
them. The dataset is publicly available at:
https://github.com/google-research-datasets/disfl-qa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Aditya Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiacheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Shyam Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Diyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faruqui_M/0/1/0/all/0/1"&gt;Manaal Faruqui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making. (arXiv:2106.04174v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04174</id>
        <link href="http://arxiv.org/abs/2106.04174"/>
        <updated>2021-06-09T02:01:47.448Z</updated>
        <summary type="html"><![CDATA[Entity Matching (EM) aims at recognizing entity records that denote the same
real-world object. Neural EM models learn vector representation of entity
descriptions and match entities end-to-end. Though robust, these methods
require many resources for training, and lack of interpretability. In this
paper, we propose a novel EM framework that consists of Heterogeneous
Information Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple
feature representation from matching decision. Using self-supervised learning
and mask mechanism in pre-trained language modeling, HIF learns the embeddings
of noisy attribute values by inter-attribute attention with unlabeled data.
Using a set of comparison features and a limited amount of annotated data, KAT
Induction learns an efficient decision tree that can be interpreted by
generating entity matching rules whose structure is advocated by domain
experts. Experiments on 6 public datasets and 3 industrial datasets show that
our method is highly efficient and outperforms SOTA EM models in most cases.
Our codes and datasets can be obtained from https://github.com/THU-KEG/HIF-KAT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zijun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengjiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_T/0/1/0/all/0/1"&gt;Tiansi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xin Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jifan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Lei Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zelin Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modest Pareto Optimisation Analysis of Dependency Parsers in 2021. (arXiv:2106.04216v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04216</id>
        <link href="http://arxiv.org/abs/2106.04216"/>
        <updated>2021-06-09T02:01:47.442Z</updated>
        <summary type="html"><![CDATA[We evaluate three leading dependency parser systems from different paradigms
on a small yet diverse subset of languages in terms of their
accuracy-efficiency Pareto front. As we are interested in efficiency, we
evaluate core parsers without pretrained language models (as these are
typically huge networks and would constitute most of the compute time) or other
augmentations that can be transversally applied to any of them. Biaffine
parsing emerges as a well-balanced default choice, with sequence-labelling
parsing being preferable if inference speed (but not training energy cost) is
the priority.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1"&gt;Mar Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_C/0/1/0/all/0/1"&gt;Carlos G&amp;#xf3;mez Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Transfer Learning in Multilingual Pre-trained Language Models through Chinese Natural Language Inference. (arXiv:2106.03983v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03983</id>
        <link href="http://arxiv.org/abs/2106.03983"/>
        <updated>2021-06-09T02:01:47.437Z</updated>
        <summary type="html"><![CDATA[Multilingual transformers (XLM, mT5) have been shown to have remarkable
transfer skills in zero-shot settings. Most transfer studies, however, rely on
automatically translated resources (XNLI, XQuAD), making it hard to discern the
particular linguistic knowledge that is being transferred, and the role of
expert annotated monolingual datasets when developing task-specific models. We
investigate the cross-lingual transfer abilities of XLM-R for Chinese and
English natural language inference (NLI), with a focus on the recent
large-scale Chinese dataset OCNLI. To better understand linguistic transfer, we
created 4 categories of challenge and adversarial tasks (totaling 17 new
datasets) for Chinese that build on several well-known resources for English
(e.g., HANS, NLI stress-tests). We find that cross-lingual models trained on
English NLI do transfer well across our Chinese tasks (e.g., in 3/4 of our
challenge categories, they perform as well/better than the best monolingual
models, even on 3/5 uniquely Chinese linguistic phenomena such as idioms, pro
drop). These results, however, come with important caveats: cross-lingual
models often perform best when trained on a mixture of English and high-quality
monolingual NLI data (OCNLI), and are often hindered by automatically
translated resources (XNLI-zh). For many phenomena, all models continue to
struggle, highlighting the need for our new diagnostics to help benchmark
Chinese and cross-lingual models. All new datasets/code are released at
https://github.com/huhailinguist/ChineseNLIProbing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hai Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;He Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zuoyu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yiwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yina Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Yixin Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1"&gt;Kyle Richardson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised and Supervised Joint Training for Resource-rich Machine Translation. (arXiv:2106.04060v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04060</id>
        <link href="http://arxiv.org/abs/2106.04060"/>
        <updated>2021-06-09T02:01:47.420Z</updated>
        <summary type="html"><![CDATA[Self-supervised pre-training of text representations has been successfully
applied to low-resource Neural Machine Translation (NMT). However, it usually
fails to achieve notable gains on resource-rich NMT. In this paper, we propose
a joint training approach, $F_2$-XEnDec, to combine self-supervised and
supervised learning to optimize NMT models. To exploit complementary
self-supervised signals for supervised learning, NMT models are trained on
examples that are interbred from monolingual and parallel sentences through a
new process called crossover encoder-decoder. Experiments on two resource-rich
translation benchmarks, WMT'14 English-German and WMT'14 English-French,
demonstrate that our approach achieves substantial improvements over several
strong baseline methods and obtains a new state of the art of 46.19 BLEU on
English-French when incorporating back translation. Results also show that our
approach is capable of improving model robustness to input perturbations such
as code-switching noise which frequently appears on social media.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1"&gt;Wolfgang Macherey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness Through Regularization for Learning to Rank. (arXiv:2102.05996v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05996</id>
        <link href="http://arxiv.org/abs/2102.05996"/>
        <updated>2021-06-09T02:01:47.413Z</updated>
        <summary type="html"><![CDATA[Given the abundance of applications of ranking in recent years, addressing
fairness concerns around automated ranking systems becomes necessary for
increasing the trust among end-users. Previous work on fair ranking has mostly
focused on application-specific fairness notions, often tailored to online
advertising, and it rarely considers learning as part of the process. In this
work, we show how to transfer numerous fairness notions from binary
classification to a learning to rank setting. Our formalism allows us to design
methods for incorporating fairness objectives with provable generalization
guarantees. An extensive experimental evaluation shows that our method can
improve ranking fairness substantially with no or only little loss of model
quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinov_N/0/1/0/all/0/1"&gt;Nikola Konstantinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1"&gt;Christoph H. Lampert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Insight from NLP Analysis: COVID-19 Vaccines Sentiments on Social Media. (arXiv:2106.04081v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04081</id>
        <link href="http://arxiv.org/abs/2106.04081"/>
        <updated>2021-06-09T02:01:47.407Z</updated>
        <summary type="html"><![CDATA[Social media is an appropriate source for analyzing public attitudes towards
the COVID-19 vaccine and various brands. Nevertheless, there are few relevant
studies. In the research, we collected tweet posts by the UK and US residents
from the Twitter API during the pandemic and designed experiments to answer
three main questions concerning vaccination. To get the dominant sentiment of
the civics, we performed sentiment analysis by VADER and proposed a new method
that can count the individual's influence. This allows us to go a step further
in sentiment analysis and explain some of the fluctuations in the data
changing. The results indicated that celebrities could lead the opinion shift
on social media in vaccination progress. Moreover, at the peak, nearly 40\% of
the population in both countries have a negative attitude towards COVID-19
vaccines. Besides, we investigated how people's opinions toward different
vaccine brands are. We found that the Pfizer vaccine enjoys the most popular
among people. By applying the sentiment analysis tool, we discovered most
people hold positive views toward the COVID-19 vaccine manufactured by most
brands. In the end, we carried out topic modelling by using the LDA model. We
found residents in the two countries are willing to share their views and
feelings concerning the vaccine. Several death cases have occurred after
vaccination. Due to these negative events, US residents are more worried about
the side effects and safety of the vaccine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Na_T/0/1/0/all/0/1"&gt;Tao Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1"&gt;Wei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wanyu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongjiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning to Compositionally Generalize. (arXiv:2106.04252v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04252</id>
        <link href="http://arxiv.org/abs/2106.04252"/>
        <updated>2021-06-09T02:01:47.401Z</updated>
        <summary type="html"><![CDATA[Natural language is compositional; the meaning of a sentence is a function of
the meaning of its parts. This property allows humans to create and interpret
novel sentences, generalizing robustly outside their prior experience. Neural
networks have been shown to struggle with this kind of generalization, in
particular performing poorly on tasks designed to assess compositional
generalization (i.e. where training and testing distributions differ in ways
that would be trivial for a compositional strategy to resolve). Their poor
performance on these tasks may in part be due to the nature of supervised
learning which assumes training and testing data to be drawn from the same
distribution. We implement a meta-learning augmented version of supervised
learning whose objective directly optimizes for out-of-distribution
generalization. We construct pairs of tasks for meta-learning by sub-sampling
existing training data. Each pair of tasks is constructed to contain relevant
examples, as determined by a similarity metric, in an effort to inhibit models
from memorizing their input. Experimental results on the COGS and SCAN datasets
show that our similarity-driven meta-learning can improve generalization
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conklin_H/0/1/0/all/0/1"&gt;Henry Conklin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bailin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kenny Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1"&gt;Ivan Titov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Improving BERT's Mathematical Abilities by Predicting the Order of Reasoning. (arXiv:2106.03921v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03921</id>
        <link href="http://arxiv.org/abs/2106.03921"/>
        <updated>2021-06-09T02:01:47.387Z</updated>
        <summary type="html"><![CDATA[Imagine you are in a supermarket. You have two bananas in your basket and
want to buy four apples. How many fruits do you have in total? This seemingly
straightforward question can be challenging for data-driven language models,
even if trained at scale. However, we would expect such generic language models
to possess some mathematical abilities in addition to typical linguistic
competence. Towards this goal, we investigate if a commonly used language
model, BERT, possesses such mathematical abilities and, if so, to what degree.
For that, we fine-tune BERT on a popular dataset for word math problems,
AQuA-RAT, and conduct several tests to understand learned representations
better. Since we teach models trained on natural language to do formal
mathematics, we hypothesize that such models would benefit from training on
semi-formal steps that explain how math results are derived. To better
accommodate such training, we also propose new pretext tasks for learning
mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or
NROP). With this new model, we achieve significantly better outcomes than
data-driven baselines and even on-par with more tailored models. We also show
how to reduce positional bias in such models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Piekos_P/0/1/0/all/0/1"&gt;Piotr Pi&amp;#x119;kos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1"&gt;Henryk Michalewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study. (arXiv:2106.03958v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03958</id>
        <link href="http://arxiv.org/abs/2106.03958"/>
        <updated>2021-06-09T02:01:47.381Z</updated>
        <summary type="html"><![CDATA[Recent research in multilingual language models (LM) has demonstrated their
ability to effectively handle multiple languages in a single model. This holds
promise for low web-resource languages (LRL) as multilingual models can enable
transfer of supervision from high resource languages to LRLs. However,
incorporating a new language in an LM still remains a challenge, particularly
for languages with limited corpora and in unseen scripts. In this paper we
argue that relatedness among languages in a language family may be exploited to
overcome some of the corpora limitations of LRLs, and propose RelateLM. We
focus on Indian languages, and exploit relatedness along two dimensions: (1)
script (since many Indic scripts originated from the Brahmic script), and (2)
sentence structure. RelateLM uses transliteration to convert the unseen script
of limited LRL text into the script of a Related Prominent Language (RPL)
(Hindi in our case). While exploiting similar sentence structures, RelateLM
utilizes readily available bilingual dictionaries to pseudo translate RPL text
into LRL corpora. Experiments on multiple real-world benchmark datasets provide
validation to our hypothesis that using a related language as pivot, along with
transliteration and pseudo translation based data augmentation, can be an
effective way to adapt LMs for LRLs, rather than direct training or pivoting
through English.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khemchandani_Y/0/1/0/all/0/1"&gt;Yash Khemchandani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehtani_S/0/1/0/all/0/1"&gt;Sarvesh Mehtani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1"&gt;Vaidehi Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awasthi_A/0/1/0/all/0/1"&gt;Abhijeet Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Talukdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1"&gt;Sunita Sarawagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model. (arXiv:2106.04098v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04098</id>
        <link href="http://arxiv.org/abs/2106.04098"/>
        <updated>2021-06-09T02:01:47.374Z</updated>
        <summary type="html"><![CDATA[Recently, there is an effort to extend fine-grained entity typing by using a
richer and ultra-fine set of types, and labeling noun phrases including
pronouns and nominal nouns instead of just named entity mentions. A key
challenge for this ultra-fine entity typing task is that human annotated data
are extremely scarce, and the annotation ability of existing distant or weak
supervision approaches is very limited. To remedy this problem, in this paper,
we propose to obtain training data for ultra-fine entity typing by using a BERT
Masked Language Model (MLM). Given a mention in a sentence, our approach
constructs an input for the BERT MLM so that it predicts context dependent
hypernyms of the mention, which can be used as type labels. Experimental
results demonstrate that, with the help of these automatically generated
labels, the performance of an ultra-fine entity typing model can be improved
substantially. We also show that our approach can be applied to improve
traditional fine-grained entity typing after performing simple type mapping.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hongliang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yangqiu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haixun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation. (arXiv:2106.04080v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04080</id>
        <link href="http://arxiv.org/abs/2106.04080"/>
        <updated>2021-06-09T02:01:47.366Z</updated>
        <summary type="html"><![CDATA[To date, most abstractive summarisation models have relied on variants of the
negative log-likelihood (NLL) as their training objective. In some cases,
reinforcement learning has been added to train the models with an objective
that is closer to their evaluation measures (e.g. ROUGE). However, the reward
function to be used within the reinforcement learning approach can play a key
role for performance and is still partially unexplored. For this reason, in
this paper, we propose two reward functions for the task of abstractive
summarisation: the first function, referred to as RwB-Hinge, dynamically
selects the samples for the gradient update. The second function, nicknamed
RISK, leverages a small pool of strong candidates to inform the reward. In the
experiments, we probe the proposed approach by fine-tuning an NLL pre trained
model over nine summarisation datasets of diverse size and nature. The
experimental results show a consistent improvement over the negative
log-likelihood baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parnell_J/0/1/0/all/0/1"&gt;Jacob Parnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unanue_I/0/1/0/all/0/1"&gt;Inigo Jauregi Unanue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piccardi_M/0/1/0/all/0/1"&gt;Massimo Piccardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14188</id>
        <link href="http://arxiv.org/abs/2105.14188"/>
        <updated>2021-06-09T02:01:47.360Z</updated>
        <summary type="html"><![CDATA[Advertising expenditures have become the major source of revenue for
e-commerce platforms. Providing good advertising experiences for advertisers
through reducing their costs of trial and error for discovering the optimal
advertising strategies is crucial for the long-term prosperity of online
advertising. To achieve this goal, the advertising platform needs to identify
the advertisers' marketing objectives, and then recommend the corresponding
strategies to fulfill this objective. In this work, we first deploy a prototype
of strategy recommender system on Taobao display advertising platform,
recommending bid prices and targeted users to advertisers. We further augment
this prototype system by directly revealing the advertising performance, and
then infer the advertisers' marketing objectives through their adoptions of
different recommending advertising performance. We use the techniques from
context bandit to jointly learn the advertisers' marketing objectives and the
recommending strategies. Online evaluations show that the designed advertising
strategy recommender system can optimize the advertisers' advertising
performance and increase the platform's revenue. Simulation experiments based
on Taobao online bidding data show that the designed contextual bandit
algorithm can effectively optimize the strategy adoption rate of advertisers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Liyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junqi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhenzhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zhizhuang Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Fei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1"&gt;Lvyin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuning Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIGTYP 2021 Shared Task: Robust Spoken Language Identification. (arXiv:2106.03895v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03895</id>
        <link href="http://arxiv.org/abs/2106.03895"/>
        <updated>2021-06-09T02:01:47.353Z</updated>
        <summary type="html"><![CDATA[While language identification is a fundamental speech and language processing
task, for many languages and language families it remains a challenging task.
For many low-resource and endangered languages this is in part due to resource
availability: where larger datasets exist, they may be single-speaker or have
different domains than desired application scenarios, demanding a need for
domain and speaker-invariant language identification systems. This year's
shared task on robust spoken language identification sought to investigate just
this scenario: systems were to be trained on largely single-speaker speech from
one domain, but evaluated on data in other domains recorded from speakers under
different recording circumstances, mimicking realistic low-resource scenarios.
We see that domain and speaker mismatch proves very challenging for current
methods which can perform above 95% accuracy in-domain, which domain adaptation
can address to some degree, but that these conditions merit further
investigation to make spoken language identification accessible in many
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1"&gt;Elizabeth Salesky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1"&gt;Badr M. Abdullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mielke_S/0/1/0/all/0/1"&gt;Sabrina J. Mielke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klyachko_E/0/1/0/all/0/1"&gt;Elena Klyachko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serikov_O/0/1/0/all/0/1"&gt;Oleg Serikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1"&gt;Edoardo Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Ritesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vylomova_E/0/1/0/all/0/1"&gt;Ekaterina Vylomova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality. (arXiv:2106.04102v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04102</id>
        <link href="http://arxiv.org/abs/2106.04102"/>
        <updated>2021-06-09T02:01:47.346Z</updated>
        <summary type="html"><![CDATA[We release a new benchmark for lexical substitution, the task of finding
appropriate substitutes for a target word in a context. To assist humans with
writing, lexical substitution systems can suggest words that humans cannot
easily think of. However, existing benchmarks depend on human recall as the
only source of data, and therefore lack coverage of the substitutes that would
be most helpful to humans. Furthermore, annotators often provide substitutes of
low quality, which are not actually appropriate in the given context. We
collect higher-coverage and higher-quality data by framing lexical substitution
as a classification problem, guided by the intuition that it is easier for
humans to judge the appropriateness of candidate substitutes than conjure them
from memory. To this end, we use a context-free thesaurus to produce candidates
and rely on human judgement to determine contextual appropriateness. Compared
to the previous largest benchmark, our Swords benchmark has 4.1x more
substitutes per target word for the same level of quality, and its substitutes
are 1.5x more appropriate (based on human judgement) for the same number of
substitutes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mina Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1"&gt;Chris Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyabor_A/0/1/0/all/0/1"&gt;Alexander Iyabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Robin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cheap and Good? Simple and Effective Data Augmentation for Low Resource Machine Reading. (arXiv:2106.04134v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.04134</id>
        <link href="http://arxiv.org/abs/2106.04134"/>
        <updated>2021-06-09T02:01:47.340Z</updated>
        <summary type="html"><![CDATA[We propose a simple and effective strategy for data augmentation for
low-resource machine reading comprehension (MRC). Our approach first pretrains
the answer extraction components of a MRC system on the augmented data that
contains approximate context of the correct answers, before training it on the
exact answer spans. The approximate context helps the QA method components in
narrowing the location of the answers. We demonstrate that our simple strategy
substantially improves both document retrieval and answer extraction
performance by providing larger context of the answers and additional training
data. In particular, our method significantly improves the performance of BERT
based retriever (15.12\%), and answer extractor (4.33\% F1) on TechQA, a
complex, low-resource MRC task. Further, our data augmentation strategy yields
significant improvements of up to 3.9\% exact match (EM) and 2.7\% F1 for
answer extraction on PolicyQA, another practical but moderate sized QA dataset
that also contains long answer spans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1"&gt;Hoang Van&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_V/0/1/0/all/0/1"&gt;Vikas Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1"&gt;Mihai Surdeanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions. (arXiv:2106.03873v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03873</id>
        <link href="http://arxiv.org/abs/2106.03873"/>
        <updated>2021-06-09T02:01:47.302Z</updated>
        <summary type="html"><![CDATA[In conversation, uptake happens when a speaker builds on the contribution of
their interlocutor by, for example, acknowledging, repeating or reformulating
what they have said. In education, teachers' uptake of student contributions
has been linked to higher student achievement. Yet measuring and improving
teachers' uptake at scale is challenging, as existing methods require expensive
annotation by experts. We propose a framework for computationally measuring
uptake, by (1) releasing a dataset of student-teacher exchanges extracted from
US math classroom transcripts annotated for uptake by experts; (2) formalizing
uptake as pointwise Jensen-Shannon Divergence (pJSD), estimated via next
utterance classification; (3) conducting a linguistically-motivated comparison
of different unsupervised measures and (4) correlating these measures with
educational outcomes. We find that although repetition captures a significant
part of uptake, pJSD outperforms repetition-based baselines, as it is capable
of identifying a wider range of uptake phenomena like question answering and
reformulation. We apply our uptake measure to three different educational
datasets with outcome indicators. Unlike baseline measures, pJSD correlates
significantly with instruction quality in all three, providing evidence for its
generalizability and for its potential to serve as an automated professional
development tool for teachers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demszky_D/0/1/0/all/0/1"&gt;Dorottya Demszky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancenido_Z/0/1/0/all/0/1"&gt;Zid Mancenido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Julie Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_H/0/1/0/all/0/1"&gt;Heather Hill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1"&gt;Dan Jurafsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1"&gt;Tatsunori Hashimoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expressivity of Emergent Language is a Trade-off between Contextual Complexity and Unpredictability. (arXiv:2106.03982v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03982</id>
        <link href="http://arxiv.org/abs/2106.03982"/>
        <updated>2021-06-09T02:01:47.259Z</updated>
        <summary type="html"><![CDATA[Researchers are now using deep learning models to explore the emergence of
language in various language games, where simulated agents interact and develop
an emergent language to solve a task. Although it is quite intuitive that
different types of language games posing different communicative challenges
might require emergent languages which encode different levels of information,
there is no existing work exploring the expressivity of the emergent languages.
In this work, we propose a definition of partial order between expressivity
based on the generalisation performance across different language games. We
also validate the hypothesis that expressivity of emergent languages is a
trade-off between the complexity and unpredictability of the context those
languages are used in. Our second novel contribution is introducing contrastive
loss into the implementation of referential games. We show that using our
contrastive loss alleviates the collapse of message types seen using standard
referential loss functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shangmin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathewson_K/0/1/0/all/0/1"&gt;Kory Mathewson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirby_S/0/1/0/all/0/1"&gt;Simon Kirby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kenny Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Abstractive Unsupervised Summarization of Online News Discussions. (arXiv:2106.03953v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03953</id>
        <link href="http://arxiv.org/abs/2106.03953"/>
        <updated>2021-06-09T02:01:47.245Z</updated>
        <summary type="html"><![CDATA[Summarization has usually relied on gold standard summaries to train
extractive or abstractive models. Social media brings a hurdle to summarization
techniques since it requires addressing a multi-document multi-author approach.
We address this challenging task by introducing a novel method that generates
abstractive summaries of online news discussions. Our method extends a
BERT-based architecture, including an attention encoding that fed comments'
likes during the training stage. To train our model, we define a task which
consists of reconstructing high impact comments based on popularity (likes).
Accordingly, our model learns to summarize online discussions based on their
most relevant comments. Our novel approach provides a summary that represents
the most relevant aspects of a news item that users comment on, incorporating
the social context as a source of information to summarize texts in online
social networks. Our model is evaluated using ROUGE scores between the
generated summary and each comment on the thread. Our model, including the
social attention encoding, significantly outperforms both extractive and
abstractive summarization methods based on such evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palma_I/0/1/0/all/0/1"&gt;Ignacio Tampe Palma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1"&gt;Marcelo Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1"&gt;Evangelos Milios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Different Types of Subtle Toxicity in Unhealthy Online Conversations. (arXiv:2106.03952v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03952</id>
        <link href="http://arxiv.org/abs/2106.03952"/>
        <updated>2021-06-09T02:01:47.223Z</updated>
        <summary type="html"><![CDATA[This paper investigates the use of machine learning models for the
classification of unhealthy online conversations containing one or more forms
of subtler abuse, such as hostility, sarcasm, and generalization. We leveraged
a public dataset of 44K online comments containing healthy and unhealthy
comments labeled with seven forms of subtle toxicity. We were able to
distinguish between these comments with a top micro F1-score, macro F1-score,
and ROC-AUC of 88.76%, 67.98%, and 0.71, respectively. Hostile comments were
easier to detect than other types of unhealthy comments. We also conducted a
sentiment analysis which revealed that most types of unhealthy comments were
associated with a slight negative sentiment, with hostile comments being the
most negative ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gilda_S/0/1/0/all/0/1"&gt;Shlok Gilda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1"&gt;Mirela Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1"&gt;Luiz Giovanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1"&gt;Daniela Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic selection of clustering algorithms using supervised graph embedding. (arXiv:2011.08225v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08225</id>
        <link href="http://arxiv.org/abs/2011.08225"/>
        <updated>2021-06-09T02:01:47.217Z</updated>
        <summary type="html"><![CDATA[The widespread adoption of machine learning (ML) techniques and the extensive
expertise required to apply them have led to increased interest in automated ML
solutions that reduce the need for human intervention. One of the main
challenges in applying ML to previously unseen problems is algorithm selection
- the identification of high-performing algorithm(s) for a given dataset, task,
and evaluation measure. This study addresses the algorithm selection challenge
for data clustering, a fundamental task in data mining that is aimed at
grouping similar objects. We present MARCO-GE, a novel meta-learning approach
for the automated recommendation of clustering algorithms. MARCO-GE first
transforms datasets into graphs and then utilizes a graph convolutional neural
network technique to extract their latent representation. Using the embedding
representations obtained, MARCO-GE trains a ranking meta-model capable of
accurately recommending top-performing algorithms for a new dataset and
clustering evaluation measure. Extensive evaluation on 210 datasets, 13
clustering algorithms, and 10 clustering measures demonstrates the
effectiveness of our approach and its superiority in terms of predictive and
generalization performance over state-of-the-art clustering meta-learning
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Shapira_N/0/1/0/all/0/1"&gt;Noy Cohen-Shapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rokach_L/0/1/0/all/0/1"&gt;Lior Rokach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NaturalProofs: Mathematical Theorem Proving in Natural Language. (arXiv:2104.01112v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01112</id>
        <link href="http://arxiv.org/abs/2104.01112"/>
        <updated>2021-06-09T02:01:47.195Z</updated>
        <summary type="html"><![CDATA[Understanding and creating mathematics using natural mathematical language -
the mixture of symbolic and natural language used by humans - is a challenging
and important problem for driving progress in machine learning. As a step in
this direction, we develop NaturalProofs, a multi-domain corpus of mathematical
statements and their proofs, written in natural mathematical language.
NaturalProofs unifies broad coverage, deep coverage, and low-resource
mathematical sources, allowing for evaluating both in-distribution and
zero-shot generalization. Using NaturalProofs, we benchmark strong neural
methods on mathematical reference retrieval and generation tasks which test a
system's ability to determine key results that appear in a proof. Large-scale
sequence models show promise compared to classical information retrieval
methods, yet their performance and out-of-domain generalization leave
substantial room for improvement. NaturalProofs opens many avenues for research
on challenging mathematical tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1"&gt;Sean Welleck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiacheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1"&gt;Ronan Le Bras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session-Aware Query Auto-completion using Extreme Multi-label Ranking. (arXiv:2012.07654v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07654</id>
        <link href="http://arxiv.org/abs/2012.07654"/>
        <updated>2021-06-09T02:01:47.187Z</updated>
        <summary type="html"><![CDATA[Query auto-completion (QAC) is a fundamental feature in search engines where
the task is to suggest plausible completions of a prefix typed in the search
bar. Previous queries in the user session can provide useful context for the
user's intent and can be leveraged to suggest auto-completions that are more
relevant while adhering to the user's prefix. Such session-aware QACs can be
generated by recent sequence-to-sequence deep learning models; however, these
generative approaches often do not meet the stringent latency requirements of
responding to each user keystroke. Moreover, these generative approaches pose
the risk of showing nonsensical queries.

In this paper, we provide a solution to this problem: we take the novel
approach of modeling session-aware QAC as an eXtreme Multi-Label Ranking (XMR)
problem where the input is the previous query in the session and the user's
current prefix, while the output space is the set of tens of millions of
queries entered by users in the recent past. We adapt a popular XMR algorithm
for this purpose by proposing several modifications to the key steps in the
algorithm. The proposed modifications yield a 10x improvement in terms of Mean
Reciprocal Rank (MRR) over the baseline XMR approach on a public search logs
dataset. We are able to maintain an inference latency of less than 10 ms while
still using session context. When compared against baseline models of
acceptable latency, we observed a 33% improvement in MRR for short prefixes of
up to 3 characters. Moreover, our model yielded a statistically significant
improvement of 2.81% over a production QAC system in terms of suggestion
acceptance rate, when deployed on the search bar of an online shopping store as
part of an A/B test.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1"&gt;Nishant Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1"&gt;Rajat Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_D/0/1/0/all/0/1"&gt;Daniel N. Hill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Document Collection Visual Question Answering. (arXiv:2104.14336v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14336</id>
        <link href="http://arxiv.org/abs/2104.14336"/>
        <updated>2021-06-09T02:01:47.176Z</updated>
        <summary type="html"><![CDATA[Current tasks and methods in Document Understanding aims to process documents
as single elements. However, documents are usually organized in collections
(historical records, purchase invoices), that provide context useful for their
interpretation. To address this problem, we introduce Document Collection
Visual Question Answering (DocCVQA) a new dataset and related task, where
questions are posed over a whole collection of document images and the goal is
not only to provide the answer to the given question, but also to retrieve the
set of documents that contain the information needed to infer the answer. Along
with the dataset we propose a new evaluation metric and baselines which provide
further insights to the new dataset and task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1"&gt;Rub&amp;#xe8;n Tito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1"&gt;Dimosthenis Karatzas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1"&gt;Ernest Valveny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Large-Scale Analysis of Mixed Initiative in Information-Seeking Dialogues for Conversational Search. (arXiv:2104.07096v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07096</id>
        <link href="http://arxiv.org/abs/2104.07096"/>
        <updated>2021-06-09T02:01:47.169Z</updated>
        <summary type="html"><![CDATA[Conversational search is a relatively young area of research that aims at
automating an information-seeking dialogue. In this paper we help to position
it with respect to other research areas within conversational Artificial
Intelligence (AI) by analysing the structural properties of an
information-seeking dialogue. To this end, we perform a large-scale dialogue
analysis of more than 150K transcripts from 16 publicly available dialogue
datasets. These datasets were collected to inform different dialogue-based
tasks including conversational search. We extract different patterns of mixed
initiative from these dialogue transcripts and use them to compare dialogues of
different types. Moreover, we contrast the patterns found in
information-seeking dialogues that are being used for research purposes with
the patterns found in virtual reference interviews that were conducted by
professional librarians. The insights we provide (1) establish close relations
between conversational search and other conversational AI tasks; and (2)
uncover limitations of existing conversational datasets to inform future data
collection tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1"&gt;Svitlana Vakulenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1"&gt;Evangelos Kanoulas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Hypothetical Events for Abductive Inference. (arXiv:2106.03973v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.03973</id>
        <link href="http://arxiv.org/abs/2106.03973"/>
        <updated>2021-06-09T02:01:47.160Z</updated>
        <summary type="html"><![CDATA[Abductive reasoning starts from some observations and aims at finding the
most plausible explanation for these observations. To perform abduction, humans
often make use of temporal and causal inferences, and knowledge about how some
hypothetical situation can result in different outcomes. This work offers the
first study of how such knowledge impacts the Abductive NLI task -- which
consists in choosing the more likely explanation for given observations. We
train a specialized language model LMI that is tasked to generate what could
happen next from a hypothetical scenario that evolves from a given event. We
then propose a multi-task model MTL to solve the Abductive NLI task, which
predicts a plausible explanation by a) considering different possible events
emerging from candidate hypotheses -- events generated by LMI -- and b)
selecting the one that is most similar to the observed outcome. We show that
our MTL model improves over prior vanilla pre-trained LMs fine-tuned on
Abductive NLI. Our manual evaluation and analysis suggest that learning about
possible next events from different hypothetical scenarios supports abductive
inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1"&gt;Debjit Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Fairness in Classification with a Model-Agnostic Multi-Objective Algorithm. (arXiv:2009.04441v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04441</id>
        <link href="http://arxiv.org/abs/2009.04441"/>
        <updated>2021-06-09T02:01:47.142Z</updated>
        <summary type="html"><![CDATA[The goal of fairness in classification is to learn a classifier that does not
discriminate against groups of individuals based on sensitive attributes, such
as race and gender. One approach to designing fair algorithms is to use
relaxations of fairness notions as regularization terms or in a constrained
optimization problem. We observe that the hyperbolic tangent function can
approximate the indicator function. We leverage this property to define a
differentiable relaxation that approximates fairness notions provably better
than existing relaxations. In addition, we propose a model-agnostic
multi-objective architecture that can simultaneously optimize for multiple
fairness notions and multiple sensitive attributes and supports all statistical
parity-based notions of fairness. We use our relaxation with the
multi-objective architecture to learn fair classifiers. Experiments on public
datasets show that our method suffers a significantly lower loss of accuracy
than current debiasing algorithms relative to the unconstrained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Padh_K/0/1/0/all/0/1"&gt;Kirtan Padh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surveillance of COVID-19 Pandemic using Social Media: A Reddit Study in North Carolina. (arXiv:2106.04515v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.04515</id>
        <link href="http://arxiv.org/abs/2106.04515"/>
        <updated>2021-06-09T02:01:47.102Z</updated>
        <summary type="html"><![CDATA[Coronavirus disease (COVID-19) pandemic has changed various aspects of
people's lives and behaviors. At this stage, there are no other ways to control
the natural progression of the disease than adopting mitigation strategies such
as wearing masks, watching distance, and washing hands. Moreover, at this time
of social distancing, social media plays a key role in connecting people and
providing a platform for expressing their feelings. In this study, we tap into
social media to surveil the uptake of mitigation and detection strategies, and
capture issues and concerns about the pandemic. In particular, we explore the
research question, "how much can be learned regarding the public uptake of
mitigation strategies and concerns about COVID-19 pandemic by using natural
language processing on Reddit posts?" After extracting COVID-related posts from
the four largest subreddit communities of North Carolina over six months, we
performed NLP-based preprocessing to clean the noisy data. We employed a custom
Named-entity Recognition (NER) system and a Latent Dirichlet Allocation (LDA)
method for topic modeling on a Reddit corpus. We observed that 'mask', 'flu',
and 'testing' are the most prevalent named-entities for "Personal Protective
Equipment", "symptoms", and "testing" categories, respectively. We also
observed that the most discussed topics are related to testing, masks, and
employment. The mitigation measures are the most prevalent theme of discussion
across all subreddits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitfield_C/0/1/0/all/0/1"&gt;Christopher Whitfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_M/0/1/0/all/0/1"&gt;Mohad Anwar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A highly scalable repository of waveform and vital signs data from bedside monitoring devices. (arXiv:2106.03965v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.03965</id>
        <link href="http://arxiv.org/abs/2106.03965"/>
        <updated>2021-06-09T02:01:47.095Z</updated>
        <summary type="html"><![CDATA[The advent of cost effective cloud computing over the past decade and
ever-growing accumulation of high-fidelity clinical data in a modern hospital
setting is leading to new opportunities for translational medicine. Machine
learning is driving the appetite of the research community for various types of
signal data such as patient vitals. Health care systems, however, are ill
suited for massive processing of large volumes of data. In addition, due to the
sheer magnitude of the data being collected, it is not feasible to retain all
of the data in health care systems in perpetuity. This gold mine of information
gets purged periodically thereby losing invaluable future research
opportunities. We have developed a highly scalable solution that: a) siphons
off patient vital data on a nightly basis from on-premises bio-medical systems
to a cloud storage location as a permanent archive, b) reconstructs the
database in the cloud, c) generates waveforms, alarms and numeric data in a
research-ready format, and d) uploads the processed data to a storage location
in the cloud ready for research.

The data is de-identified and catalogued such that it can be joined with
Electronic Medical Records (EMR) and other ancillary data types such as
electroencephalogram (EEG), radiology, video monitoring etc. This technique
eliminates the research burden from health care systems. This highly scalable
solution is used to process high density patient monitoring data aggregated by
the Philips Patient Information Center iX (PIC iX) hospital surveillance system
for archival storage in the Philips Data Warehouse Connect enterprise-level
database. The solution is part of a broader platform that supports a secure
high performance clinical data science platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malunjkar_S/0/1/0/all/0/1"&gt;Sanjay Malunjkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_S/0/1/0/all/0/1"&gt;Susan Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1"&gt;Somalee Datta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Periodicity and Interactivity in Multi-Interest Framework for Sequential Recommendation. (arXiv:2106.04415v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04415</id>
        <link href="http://arxiv.org/abs/2106.04415"/>
        <updated>2021-06-09T02:01:47.082Z</updated>
        <summary type="html"><![CDATA[Sequential recommendation systems alleviate the problem of information
overload, and have attracted increasing attention in the literature. Most prior
works usually obtain an overall representation based on the user's behavior
sequence, which can not sufficiently reflect the multiple interests of the
user. To this end, we propose a novel method called PIMI to mitigate this
issue. PIMI can model the user's multi-interest representation effectively by
considering both the periodicity and interactivity in the item sequence.
Specifically, we design a periodicity-aware module to utilize the time interval
information between user's behaviors. Meanwhile, an ingenious graph is proposed
to enhance the interactivity between items in user's behavior sequence, which
can capture both global and local item features. Finally, a multi-interest
extraction module is applied to describe user's multiple interests based on the
obtained item representation. Extensive experiments on two real-world datasets
Amazon and Taobao show that PIMI outperforms state-of-the-art methods
consistently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Gaode Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinghua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yanyan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1"&gt;Cong Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1"&gt;Ji Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation. (arXiv:2106.04408v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04408</id>
        <link href="http://arxiv.org/abs/2106.04408"/>
        <updated>2021-06-09T02:01:47.074Z</updated>
        <summary type="html"><![CDATA[User interest modeling is critical for personalized news recommendation.
Existing news recommendation methods usually learn a single user embedding for
each user from their previous behaviors to represent their overall interest.
However, user interest is usually diverse and multi-grained, which is difficult
to be accurately modeled by a single user embedding. In this paper, we propose
a news recommendation method with hierarchical user interest modeling, named
HieRec. Instead of a single user embedding, in our method each user is
represented in a hierarchical interest tree to better capture their diverse and
multi-grained interest in news. We use a three-level hierarchy to represent 1)
overall user interest; 2) user interest in coarse-grained topics like sports;
and 3) user interest in fine-grained topics like football. Moreover, we propose
a hierarchical user interest matching framework to match candidate news with
different levels of user interest for more accurate user interest targeting.
Extensive experiments on two real-world datasets validate our method can
effectively improve the performance of user modeling for personalized news
recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1"&gt;Tao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Peiru Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversational Fashion Image Retrieval via Multiturn Natural Language Feedback. (arXiv:2106.04128v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04128</id>
        <link href="http://arxiv.org/abs/2106.04128"/>
        <updated>2021-06-09T02:01:47.055Z</updated>
        <summary type="html"><![CDATA[We study the task of conversational fashion image retrieval via multiturn
natural language feedback. Most previous studies are based on single-turn
settings. Existing models on multiturn conversational fashion image retrieval
have limitations, such as employing traditional models, and leading to
ineffective performance. We propose a novel framework that can effectively
handle conversational fashion image retrieval with multiturn natural language
feedback texts. One characteristic of the framework is that it searches for
candidate images based on exploitation of the encoded reference image and
feedback text information together with the conversation history. Furthermore,
the image fashion attribute information is leveraged via a mutual attention
strategy. Since there is no existing fashion dataset suitable for the multiturn
setting of our task, we derive a large-scale multiturn fashion dataset via
additional manual annotation efforts on an existing single-turn dataset. The
experiments show that our proposed model significantly outperforms existing
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yifei Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seamlessly Unifying Attributes and Items: Conversational Recommendation for Cold-Start Users. (arXiv:2005.12979v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12979</id>
        <link href="http://arxiv.org/abs/2005.12979"/>
        <updated>2021-06-09T02:01:47.041Z</updated>
        <summary type="html"><![CDATA[Static recommendation methods like collaborative filtering suffer from the
inherent limitation of performing real-time personalization for cold-start
users. Online recommendation, e.g., multi-armed bandit approach, addresses this
limitation by interactively exploring user preference online and pursuing the
exploration-exploitation (EE) trade-off. However, existing bandit-based methods
model recommendation actions homogeneously. Specifically, they only consider
the items as the arms, being incapable of handling the item attributes, which
naturally provide interpretable information of user's current demands and can
effectively filter out undesired items. In this work, we consider the
conversational recommendation for cold-start users, where a system can both ask
the attributes from and recommend items to a user interactively. This important
scenario was studied in a recent work. However, it employs a hand-crafted
function to decide when to ask attributes or make recommendations. Such
separate modeling of attributes and items makes the effectiveness of the system
highly rely on the choice of the hand-crafted function, thus introducing
fragility to the system. To address this limitation, we seamlessly unify
attributes and items in the same arm space and achieve their EE trade-offs
automatically using the framework of Thompson Sampling. Our Conversational
Thompson Sampling (ConTS) model holistically solves all questions in
conversational recommendation by choosing the arm with the maximal reward to
play. Extensive experiments on three benchmark datasets show that ConTS
outperforms the state-of-the-art methods Conversational UCB (ConUCB) and
Estimation-Action-Reflection model in both metrics of success rate and average
number of conversation turns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shijun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1"&gt;Wenqiang Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingyun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1"&gt;Peng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defining definition: a Text mining Approach to Define Innovative Technological Fields. (arXiv:2106.04210v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04210</id>
        <link href="http://arxiv.org/abs/2106.04210"/>
        <updated>2021-06-09T02:01:47.032Z</updated>
        <summary type="html"><![CDATA[One of the first task of an innovative project is delineating the scope of
the project itself or of the product/service to be developed. A wrong scope
definition can determine (in the worst case) project failure. A good scope
definition become even more relevant in technological intensive innovation
projects, nowadays characterized by a highly dynamic multidisciplinary,
turbulent and uncertain environment. In these cases, the boundaries of the
project are not easily detectable and it is difficult to decide what it is
in-scope and out-of-scope. The present work proposes a tool for the scope
delineation process, that automatically define an innovative technological
field or a new technology. The tool is based on Text Mining algorithm that
exploits Elsevier's Scopus abstracts in order to the extract relevant data to
define a technological scope. The automatic definition tool is then applied on
four case studies: Artificial Intelligence and Data Science. The results show
how the tool can provide many crucial information in the definition process of
a technological field. In particular for the target technological field (or
technology), it provides the definition and other elements related to the
target.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giordano_V/0/1/0/all/0/1"&gt;Vito Giordano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiarello_F/0/1/0/all/0/1"&gt;Filippo Chiarello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cervelli_E/0/1/0/all/0/1"&gt;Elena Cervelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MindReader: Recommendation over Knowledge Graph Entities with Explicit User Ratings. (arXiv:2106.04209v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04209</id>
        <link href="http://arxiv.org/abs/2106.04209"/>
        <updated>2021-06-09T02:01:47.017Z</updated>
        <summary type="html"><![CDATA[Knowledge Graphs (KGs) have been integrated in several models of
recommendation to augment the informational value of an item by means of its
related entities in the graph. Yet, existing datasets only provide explicit
ratings on items and no information is provided about user opinions of other
(non-recommendable) entities. To overcome this limitation, we introduce a new
dataset, called the MindReader, providing explicit user ratings both for items
and for KG entities. In this first version, the MindReader dataset provides
more than 102 thousands explicit ratings collected from 1,174 real users on
both items and entities from a KG in the movie domain. This dataset has been
collected through an online interview application that we also release open
source. As a demonstration of the importance of this new dataset, we present a
comparative study of the effect of the inclusion of ratings on non-item KG
entities in a variety of state-of-the-art recommendation models. In particular,
we show that most models, whether designed specifically for graph data or not,
see improvements in recommendation quality when trained on explicit non-item
ratings. Moreover, for some models, we show that non-item ratings can
effectively replace item ratings without loss of recommendation quality. This
finding, thanks also to an observed greater familiarity of users towards common
KG entities than towards long-tail items, motivates the use of KG entities for
both warm and cold-start recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brams_A/0/1/0/all/0/1"&gt;Anders H. Brams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakobsen_A/0/1/0/all/0/1"&gt;Anders L. Jakobsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jendal_T/0/1/0/all/0/1"&gt;Theis E. Jendal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lissandrini_M/0/1/0/all/0/1"&gt;Matteo Lissandrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolog_P/0/1/0/all/0/1"&gt;Peter Dolog&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hose_K/0/1/0/all/0/1"&gt;Katja Hose&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Neural Collaborative Filtering. (arXiv:2106.04405v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04405</id>
        <link href="http://arxiv.org/abs/2106.04405"/>
        <updated>2021-06-09T02:01:46.977Z</updated>
        <summary type="html"><![CDATA[In this work, we present a federated version of the state-of-the-art Neural
Collaborative Filtering (NCF) approach for item recommendations. The system,
named FedNCF, allows learning without requiring users to expose or transmit
their raw data. Experimental validation shows that FedNCF achieves comparable
recommendation quality to the original NCF system. Although federated learning
(FL) enables learning without raw data transmission, recent attacks showed that
FL alone does not eliminate privacy concerns. To overcome this challenge, we
integrate a privacy-preserving enhancement with a secure aggregation scheme
that satisfies the security requirements against an honest-but-curious (HBC)
entity, without affecting the quality of the original model. Finally, we
discuss the peculiarities observed in the application of FL in a collaborative
filtering (CF) task as well as we evaluate the privacy-preserving mechanism in
terms of computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perifanis_V/0/1/0/all/0/1"&gt;Vasileios Perifanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Efraimidis_P/0/1/0/all/0/1"&gt;Pavlos S. Efraimidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConSTR: A Contextual Search Term Recommender. (arXiv:2106.04376v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.04376</id>
        <link href="http://arxiv.org/abs/2106.04376"/>
        <updated>2021-06-09T02:01:46.968Z</updated>
        <summary type="html"><![CDATA[In this demo paper, we present ConSTR, a novel Contextual Search Term
Recommender that utilises the user's interaction context for search term
recommendation and literature retrieval. ConSTR integrates a two-layered
recommendation interface: the first layer suggests terms with respect to a
user's current search term, and the second layer suggests terms based on the
users' previous search activities (interaction context). For the demonstration,
ConSTR is built on the arXiv, an academic repository consisting of 1.8 million
documents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kramer_T/0/1/0/all/0/1"&gt;Thomas Kr&amp;#xe4;mer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carevic_Z/0/1/0/all/0/1"&gt;Zeljko Carevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1"&gt;Dwaipayan Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klas_C/0/1/0/all/0/1"&gt;Claus-Peter Klas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1"&gt;Philipp Mayr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Struggle with Academic Plagiarism: Approaches based on Semantic Similarity. (arXiv:2106.04404v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04404</id>
        <link href="http://arxiv.org/abs/2106.04404"/>
        <updated>2021-06-09T02:01:46.958Z</updated>
        <summary type="html"><![CDATA[Academic plagiarism is a serious problem nowadays. Due to the existence of
inexhaustible sources of digital information, today it is easier to plagiarize
more than ever before. The good thing is that plagiarism detection techniques
have improved and are powerful enough to detect attempts of plagiarism in
education. We are now witnessing efficient plagiarism detection software in
action, such as Turnitin, iThenticate or SafeAssign. In the introduction we
explore software that is used within the Croatian academic community for
plagiarism detection in universities and/or in scientific journals. The
question is: is this enough? Current software has proven to be successful,
however the problem of identifying paraphrasing or obfuscation plagiarism
remains unresolved. In this paper we present a report of how semantic
similarity measures can be used in the plagiarism detection task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vrbanec_T/0/1/0/all/0/1"&gt;Tedo Vrbanec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mestrovic_A/0/1/0/all/0/1"&gt;Ana Mestrovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.03954</id>
        <link href="http://arxiv.org/abs/2106.03954"/>
        <updated>2021-06-09T02:01:46.947Z</updated>
        <summary type="html"><![CDATA[With the popularity of Machine Learning (ML) solutions, algorithms and data
have been released faster than the capacity of processing them. In this
context, the problem of Algorithm Recommendation (AR) is receiving a
significant deal of attention recently. This problem has been addressed in the
literature as a learning task, often as a Meta-Learning problem where the aim
is to recommend the best alternative for a specific dataset. For such, datasets
encoded by meta-features are explored by ML algorithms that try to learn the
mapping between meta-representations and the best technique to be used. One of
the challenges for the successful use of ML is to define which features are the
most valuable for a specific dataset since several meta-features can be used,
which increases the meta-feature dimension. This paper presents an empirical
analysis of Feature Selection and Feature Extraction in the meta-level for the
AR problem. The present study was focused on three criteria: predictive
performance, dimensionality reduction, and pipeline runtime. As we verified,
applying Dimensionality Reduction (DR) methods did not improve predictive
performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed
that various datasets have many non-informative meta-features and that it is
possible to obtain high predictive performance using around 20% of the original
meta-features. Therefore, due to their natural trend for high dimensionality,
DR methods should be used for Meta-Feature Selection and Meta-Feature
Extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1"&gt;Geand Trindade Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1"&gt;Moises Rocha dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1"&gt;Andre Carlos Ponce de Leon Ferreira de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization of Service Addition in Multilevel Index Model for Edge Computing. (arXiv:2106.04494v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04494</id>
        <link href="http://arxiv.org/abs/2106.04494"/>
        <updated>2021-06-09T02:01:46.934Z</updated>
        <summary type="html"><![CDATA[With the development of Edge Computing and Artificial Intelligence (AI)
technologies, edge devices are witnessed to generate data at unprecedented
volume. The Edge Intelligence (EI) has led to the emergence of edge devices in
various application domains. The EI can provide efficient services to
delay-sensitive applications, where the edge devices are deployed as edge nodes
to host the majority of execution, which can effectively manage services and
improve service discovery efficiency. The multilevel index model is a
well-known model used for indexing service, such a model is being introduced
and optimized in the edge environments to efficiently services discovery whilst
managing large volumes of data. However, effectively updating the multilevel
index model by adding new services timely and precisely in the dynamic Edge
Computing environments is still a challenge. Addressing this issue, this paper
proposes a designated key selection method to improve the efficiency of adding
services in the multilevel index models. Our experimental results show that in
the partial index and the full index of multilevel index model, our method
reduces the service addition time by around 84% and 76%, respectively when
compared with the original key selection method and by around 78% and 66%,
respectively when compared with the random selection method. Our proposed
method significantly improves the service addition efficiency in the multilevel
index model, when compared with existing state-of-the-art key selection
methods, without compromising the service retrieval stability to any notable
level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiayan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anjum_A/0/1/0/all/0/1"&gt;Ashiq Anjum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panneerselvam_J/0/1/0/all/0/1"&gt;John Panneerselvam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Bo Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review Polarity-wise Recommender. (arXiv:2106.04155v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.04155</id>
        <link href="http://arxiv.org/abs/2106.04155"/>
        <updated>2021-06-09T02:01:46.754Z</updated>
        <summary type="html"><![CDATA[Utilizing review information to enhance recommendation, the de facto
review-involved recommender systems, have received increasing interests over
the past few years. Thereinto, one advanced branch is to extract salient
aspects from textual reviews (i.e., the item attributes that users express) and
combine them with the matrix factorization technique. However, existing
approaches all ignore the fact that semantically different reviews often
include opposite aspect information. In particular, positive reviews usually
express aspects that users prefer, while negative ones describe aspects that
users reject. As a result, it may mislead the recommender systems into making
incorrect decisions pertaining to user preference modeling. Towards this end,
in this paper, we propose a Review Polarity-wise Recommender model, dubbed as
RPR, to discriminately treat reviews with different polarities. To be specific,
in this model, positive and negative reviews are separately gathered and
utilized to model the user-preferred and user-rejected aspects, respectively.
Besides, in order to overcome the imbalance problem of semantically different
reviews, we also develop an aspect-aware importance weighting approach to align
the aspect importance for these two kinds of reviews. Extensive experiments
conducted on eight benchmark datasets have demonstrated the superiority of our
model as compared to a series of state-of-the-art review-involved baselines.
Moreover, our method can provide certain explanations to the real-world rating
prediction scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Han Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yangyang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianhua Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Liqiang Nie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation. (arXiv:2009.07526v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07526</id>
        <link href="http://arxiv.org/abs/2009.07526"/>
        <updated>2021-06-09T02:01:46.700Z</updated>
        <summary type="html"><![CDATA[Scene graphs are semantic abstraction of images that encourage visual
understanding and reasoning. However, the performance of Scene Graph Generation
(SGG) is unsatisfactory when faced with biased data in real-world scenarios.
Conventional debiasing research mainly studies from the view of balancing data
distribution or learning unbiased models and representations, ignoring the
correlations among the biased classes. In this work, we analyze this problem
from a novel cognition perspective: automatically building a hierarchical
cognitive structure from the biased predictions and navigating that hierarchy
to locate the relationships, making the tail relationships receive more
attention in a coarse-to-fine mode. To this end, we propose a novel debiasing
Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive
structure CogTree to organize the relationships based on the prediction of a
biased SGG model. The CogTree distinguishes remarkably different relationships
at first and then focuses on a small portion of easily confused ones. Then, we
propose a debiasing loss specially for this cognitive structure, which supports
coarse-to-fine distinction for the correct relationships. The loss is
model-agnostic and consistently boosting the performance of several
state-of-the-art models. The code is available at:
https://github.com/CYVincent/Scene-Graph-Transformer-CogTree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuan Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yue Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation. (arXiv:2106.04403v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04403</id>
        <link href="http://arxiv.org/abs/2106.04403"/>
        <updated>2021-06-09T02:01:46.689Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have brought significant progress in visual
grounding tasks such as language-guided video object segmentation. However,
collecting large datasets for these tasks is expensive in terms of annotation
time, which represents a bottleneck. To this end, we propose a novel method,
namely SynthRef, for generating synthetic referring expressions for target
objects in an image (or video frame), and we also present and disseminate the
first large-scale dataset with synthetic referring expressions for video object
segmentation. Our experiments demonstrate that by training with our synthetic
referring expressions one can improve the ability of a model to generalize
across different datasets, without any additional annotation cost. Moreover,
our formulation allows its application to any object detection or segmentation
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazakos_I/0/1/0/all/0/1"&gt;Ioannis Kazakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_C/0/1/0/all/0/1"&gt;Carles Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellver_M/0/1/0/all/0/1"&gt;Miriam Bellver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1"&gt;Carina Silberer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Giro-i-Nieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Video Configuration and Bitrate Allocation for Vehicles. (arXiv:2102.10898v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10898</id>
        <link href="http://arxiv.org/abs/2102.10898"/>
        <updated>2021-06-09T02:01:46.656Z</updated>
        <summary type="html"><![CDATA[Vehicles with autonomous driving capabilities are present on public streets.
However, edge cases remain that still require a human in-vehicle driver.
Assuming the vehicle manages to come to a safe state in an automated fashion,
teleoperated driving technology enables a human to resolve the situation
remotely by a control interface connected via a mobile network. While this is a
promising solution, it also introduces technical challenges, one of them being
the necessity to transmit video data of multiple cameras from the vehicle to
the human operator. In this paper, an adaptive video streaming framework
specifically designed for teleoperated vehicles is proposed and demonstrated.
The framework enables automatic reconfiguration of the video streams of the
multi-camera system at runtime. Predictions of variable transmission service
quality are taken into account. With the objective to improve visual quality,
the framework uses so-called rate-quality models to dynamically allocate
bitrates and select resolution scaling factors. Results from deploying the
proposed framework on an actual teleoperated driving system are presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schimpe_A/0/1/0/all/0/1"&gt;Andreas Schimpe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hoffmann_S/0/1/0/all/0/1"&gt;Simon Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diermeyer_F/0/1/0/all/0/1"&gt;Frank Diermeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Triad Matching and Reconstruction for Weakly Referring Expression Grounding. (arXiv:2106.04053v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04053</id>
        <link href="http://arxiv.org/abs/2106.04053"/>
        <updated>2021-06-09T02:01:46.640Z</updated>
        <summary type="html"><![CDATA[In this paper, we are tackling the weakly-supervised referring expression
grounding task, for the localization of a referent object in an image according
to a query sentence, where the mapping between image regions and queries are
not available during the training stage. In traditional methods, an object
region that best matches the referring expression is picked out, and then the
query sentence is reconstructed from the selected region, where the
reconstruction difference serves as the loss for back-propagation. The existing
methods, however, conduct both the matching and the reconstruction
approximately as they ignore the fact that the matching correctness is unknown.
To overcome this limitation, a discriminative triad is designed here as the
basis to the solution, through which a query can be converted into one or
multiple discriminative triads in a very scalable way. Based on the
discriminative triad, we further propose the triad-level matching and
reconstruction modules which are lightweight yet effective for the
weakly-supervised training, making it three times lighter and faster than the
previous state-of-the-art methods. One important merit of our work is its
superior performance despite the simple and neat design. Specifically, the
proposed method achieves a new state-of-the-art accuracy when evaluated on
RefCOCO (39.21%), RefCOCO+ (39.18%) and RefCOCOg (43.24%) datasets, that is
4.17%, 4.08% and 7.8% higher than the previous one, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mingjie Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jimin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1"&gt;Eng Gee Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Si Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goulermas_J/0/1/0/all/0/1"&gt;John Y. Goulermas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation. (arXiv:2106.04151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.04151</id>
        <link href="http://arxiv.org/abs/2106.04151"/>
        <updated>2021-06-09T02:01:46.600Z</updated>
        <summary type="html"><![CDATA[Unsupervised Domain Adaptation (UDA) aims to generalize the knowledge learned
from a well-labeled source domain to an unlabeled target domain. Recently,
adversarial domain adaptation with two distinct classifiers (bi-classifier) has
been introduced into UDA which is effective to align distributions between
different domains. Previous bi-classifier adversarial learning methods only
focus on the similarity between the outputs of two distinct classifiers.
However, the similarity of the outputs cannot guarantee the accuracy of target
samples, i.e., target samples may match to wrong categories even if the
discrepancy between two classifiers is small. To challenge this issue, in this
paper, we propose a cross-domain gradient discrepancy minimization (CGDM)
method which explicitly minimizes the discrepancy of gradients generated by
source samples and target samples. Specifically, the gradient gives a cue for
the semantic information of target samples so it can be used as a good
supervision to improve the accuracy of target samples. In order to compute the
gradient signal of target samples, we further obtain target pseudo labels
through a clustering-based self-supervised learning. Extensive experiments on
three widely used UDA datasets show that our method surpasses many previous
state-of-the-arts. Codes are available at https://github.com/lijin118/CGDM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhekai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hongzu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Transformer for Explainable Recommendation. (arXiv:2105.11601v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11601</id>
        <link href="http://arxiv.org/abs/2105.11601"/>
        <updated>2021-06-09T00:28:49.163Z</updated>
        <summary type="html"><![CDATA[Personalization of natural language generation plays a vital role in a large
spectrum of tasks, such as explainable recommendation, review summarization and
dialog systems. In these tasks, user and item IDs are important identifiers for
personalization. Transformer, which is demonstrated with strong language
modeling capability, however, is not personalized and fails to make use of the
user and item IDs since the ID tokens are not even in the same semantic space
as the words. To address this problem, we present a PErsonalized Transformer
for Explainable Recommendation (PETER), on which we design a simple and
effective learning objective that utilizes the IDs to predict the words in the
target explanation, so as to endow the IDs with linguistic meanings and to
achieve personalized Transformer. Besides generating explanations, PETER can
also make recommendations, which makes it a unified model for the whole
recommendation-explanation pipeline. Extensive experiments show that our small
unpretrained model outperforms fine-tuned BERT on the generation task, in terms
of both effectiveness and efficiency, which highlights the importance and the
nice utility of our design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Transformer for Explainable Recommendation. (arXiv:2105.11601v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11601</id>
        <link href="http://arxiv.org/abs/2105.11601"/>
        <updated>2021-06-09T00:28:49.118Z</updated>
        <summary type="html"><![CDATA[Personalization of natural language generation plays a vital role in a large
spectrum of tasks, such as explainable recommendation, review summarization and
dialog systems. In these tasks, user and item IDs are important identifiers for
personalization. Transformer, which is demonstrated with strong language
modeling capability, however, is not personalized and fails to make use of the
user and item IDs since the ID tokens are not even in the same semantic space
as the words. To address this problem, we present a PErsonalized Transformer
for Explainable Recommendation (PETER), on which we design a simple and
effective learning objective that utilizes the IDs to predict the words in the
target explanation, so as to endow the IDs with linguistic meanings and to
achieve personalized Transformer. Besides generating explanations, PETER can
also make recommendations, which makes it a unified model for the whole
recommendation-explanation pipeline. Extensive experiments show that our small
unpretrained model outperforms fine-tuned BERT on the generation task, in terms
of both effectiveness and efficiency, which highlights the importance and the
nice utility of our design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Compression-Compilation Framework for On-mobile Real-time BERT Applications. (arXiv:2106.00526v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00526</id>
        <link href="http://arxiv.org/abs/2106.00526"/>
        <updated>2021-06-08T22:44:25.107Z</updated>
        <summary type="html"><![CDATA[Transformer-based deep learning models have increasingly demonstrated high
accuracy on many natural language processing (NLP) tasks. In this paper, we
propose a compression-compilation co-design framework that can guarantee the
identified model to meet both resource and real-time specifications of mobile
devices. Our framework applies a compiler-aware neural architecture
optimization method (CANAO), which can generate the optimal compressed model
that balances both accuracy and latency. We are able to achieve up to 7.8x
speedup compared with TensorFlow-Lite with only minor accuracy loss. We present
two types of BERT applications on mobile devices: Question Answering (QA) and
Text Generation. Both can be executed in real-time with latency as low as 45ms.
Videos for demonstrating the framework can be found on
https://www.youtube.com/watch?v=_WIRvK_2PZI]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1"&gt;Wei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhenglun Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Weiwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1"&gt;Jiexiong Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Caiwen Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NewsEmbed: Modeling News through Pre-trained Document Representations. (arXiv:2106.00590v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00590</id>
        <link href="http://arxiv.org/abs/2106.00590"/>
        <updated>2021-06-08T22:44:25.008Z</updated>
        <summary type="html"><![CDATA[Effectively modeling text-rich fresh content such as news articles at
document-level is a challenging problem. To ensure a content-based model
generalize well to a broad range of applications, it is critical to have a
training dataset that is large beyond the scale of human labels while achieving
desired quality. In this work, we address those two challenges by proposing a
novel approach to mine semantically-relevant fresh documents, and their topic
labels, with little human supervision. Meanwhile, we design a multitask model
called NewsEmbed that alternatively trains a contrastive learning with a
multi-label classification to derive a universal document encoder. We show that
the proposed approach can provide billions of high quality organic training
examples and can be naturally extended to multilingual setting where texts in
different languages are encoded in the same semantic space. We experimentally
demonstrate NewsEmbed's competitive performance across multiple natural
language understanding tasks, both supervised and unsupervised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax Regret for Bandit Convex Optimisation of Ridge Functions. (arXiv:2106.00444v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00444</id>
        <link href="http://arxiv.org/abs/2106.00444"/>
        <updated>2021-06-08T22:44:24.996Z</updated>
        <summary type="html"><![CDATA[We analyse adversarial bandit convex optimisation with an adversary that is
restricted to playing functions of the form $f_t(x) = g_t(\langle x,
\theta\rangle)$ for convex $g_t : \mathbb R \to \mathbb R$ and unknown $\theta
\in \mathbb R^d$ that is homogeneous over time. We provide a short
information-theoretic proof that the minimax regret is at most $O(d \sqrt{n}
\log(n \operatorname{diam}(\mathcal K)))$ where $n$ is the number of
interactions, $d$ the dimension and $\operatorname{diam}(\mathcal K)$ is the
diameter of the constraint set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lattimore_T/0/1/0/all/0/1"&gt;Tor Lattimore&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenBox: A Generalized Black-box Optimization Service. (arXiv:2106.00421v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00421</id>
        <link href="http://arxiv.org/abs/2106.00421"/>
        <updated>2021-06-08T22:44:24.840Z</updated>
        <summary type="html"><![CDATA[Black-box optimization (BBO) has a broad range of applications, including
automatic machine learning, engineering, physics, and experimental design.
However, it remains a challenge for users to apply BBO methods to their
problems at hand with existing software packages, in terms of applicability,
performance, and efficiency. In this paper, we build OpenBox, an open-source
and general-purpose BBO service with improved usability. The modular design
behind OpenBox also facilitates flexible abstraction and optimization of basic
BBO components that are common in other existing systems. OpenBox is
distributed, fault-tolerant, and scalable. To improve efficiency, OpenBox
further utilizes "algorithm agnostic" parallelization and transfer learning.
Our experimental results demonstrate the effectiveness and efficiency of
OpenBox compared to existing systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wentao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huaijun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingchao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jinyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wentao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anti-Koopmanism. (arXiv:2106.00106v2 [math.FA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00106</id>
        <link href="http://arxiv.org/abs/2106.00106"/>
        <updated>2021-06-08T22:44:24.822Z</updated>
        <summary type="html"><![CDATA[This article addresses several longstanding misconceptions concerning Koopman
operators, including the existence of lattices of eigenfunctions, common
eigenfunctions between Koopman operators, and boundedness and compactness of
Koopman operators, among others. Counterexamples are provided for each
misconception. This manuscript also proves that the Gaussian RBF's native space
only supports bounded Koopman operator corresponding to affine dynamics, which
shows that the assumption of boundedness is very limiting. A framework for DMD
is presented that requires only densely defined Koopman operators over
reproducing kernel Hilbert spaces, and the effectiveness of this approach is
demonstrated through reconstruction examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gonzalez_E/0/1/0/all/0/1"&gt;Efrain Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Abudia_M/0/1/0/all/0/1"&gt;Moad Abudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jury_M/0/1/0/all/0/1"&gt;Michael Jury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kamalapurkar_R/0/1/0/all/0/1"&gt;Rushikesh Kamalapurkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rosenfeld_J/0/1/0/all/0/1"&gt;Joel A. Rosenfeld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Look Wide and Interpret Twice: Improving Performance on Interactive Instruction-following Tasks. (arXiv:2106.00596v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00596</id>
        <link href="http://arxiv.org/abs/2106.00596"/>
        <updated>2021-06-08T22:44:24.609Z</updated>
        <summary type="html"><![CDATA[There is a growing interest in the community in making an embodied AI agent
perform a complicated task while interacting with an environment following
natural language directives. Recent studies have tackled the problem using
ALFRED, a well-designed dataset for the task, but achieved only very low
accuracy. This paper proposes a new method, which outperforms the previous
methods by a large margin. It is based on a combination of several new ideas.
One is a two-stage interpretation of the provided instructions. The method
first selects and interprets an instruction without using visual information,
yielding a tentative action sequence prediction. It then integrates the
prediction with the visual information etc., yielding the final prediction of
an action and an object. As the object's class to interact is identified in the
first stage, it can accurately select the correct object from the input image.
Moreover, our method considers multiple egocentric views of the environment and
extracts essential information by applying hierarchical attention conditioned
on the current instruction. This contributes to the accurate prediction of
actions for navigation. A preliminary version of the method won the ALFRED
Challenge 2020. The current version achieves the unseen environment's success
rate of 4.45% with a single view, which is further improved to 8.37% with
multiple views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Van-Quang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1"&gt;Masanori Suganuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1"&gt;Takayuki Okatani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpanNER: Named Entity Re-/Recognition as Span Prediction. (arXiv:2106.00641v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00641</id>
        <link href="http://arxiv.org/abs/2106.00641"/>
        <updated>2021-06-08T22:44:24.222Z</updated>
        <summary type="html"><![CDATA[Recent years have seen the paradigm shift of Named Entity Recognition (NER)
systems from sequence labeling to span prediction. Despite its preliminary
effectiveness, the span prediction model's architectural bias has not been
fully understood. In this paper, we first investigate the strengths and
weaknesses when the span prediction model is used for named entity recognition
compared with the sequence labeling framework and how to further improve it,
which motivates us to make complementary advantages of systems based on
different paradigms. We then reveal that span prediction, simultaneously, can
serve as a system combiner to re-recognize named entities from different
systems' outputs. We experimentally implement 154 systems on 11 datasets,
covering three languages, comprehensive results show the effectiveness of span
prediction models that both serve as base NER systems and system combiners. We
make all code and datasets available: \url{https://github.com/neulab/spanner},
as well as an online system demo: \url{this http URL}. Our model also has
been deployed into the ExplainaBoard platform, which allows users to flexibly
perform a system combination of top-scoring systems in an interactive way:
\url{this http URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jinlan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NewsEmbed: Modeling News through Pre-trained Document Representations. (arXiv:2106.00590v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00590</id>
        <link href="http://arxiv.org/abs/2106.00590"/>
        <updated>2021-06-08T22:44:24.202Z</updated>
        <summary type="html"><![CDATA[Effectively modeling text-rich fresh content such as news articles at
document-level is a challenging problem. To ensure a content-based model
generalize well to a broad range of applications, it is critical to have a
training dataset that is large beyond the scale of human labels while achieving
desired quality. In this work, we address those two challenges by proposing a
novel approach to mine semantically-relevant fresh documents, and their topic
labels, with little human supervision. Meanwhile, we design a multitask model
called NewsEmbed that alternatively trains a contrastive learning with a
multi-label classification to derive a universal document encoder. We show that
the proposed approach can provide billions of high quality organic training
examples and can be naturally extended to multilingual setting where texts in
different languages are encoded in the same semantic space. We experimentally
demonstrate NewsEmbed's competitive performance across multiple natural
language understanding tasks, both supervised and unsupervised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KGPool: Dynamic Knowledge Graph Context Selection for Relation Extraction. (arXiv:2106.00459v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00459</id>
        <link href="http://arxiv.org/abs/2106.00459"/>
        <updated>2021-06-08T22:44:23.904Z</updated>
        <summary type="html"><![CDATA[We present a novel method for relation extraction (RE) from a single
sentence, mapping the sentence and two given entities to a canonical fact in a
knowledge graph (KG). Especially in this presumed sentential RE setting, the
context of a single sentence is often sparse. This paper introduces the KGPool
method to address this sparsity, dynamically expanding the context with
additional facts from the KG. It learns the representation of these facts
(entity alias, entity descriptions, etc.) using neural methods, supplementing
the sentential context. Unlike existing methods that statically use all
expanded facts, KGPool conditions this expansion on the sentence. We study the
efficacy of KGPool by evaluating it with different neural models and KGs
(Wikidata and NYT Freebase). Our experimental evaluation on standard datasets
shows that by feeding the KGPool representation into a Graph Neural Network,
the overall method is significantly more accurate than state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nadgeri_A/0/1/0/all/0/1"&gt;Abhishek Nadgeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1"&gt;Anson Bastos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kuldeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulang_I/0/1/0/all/0/1"&gt;Isaiah Onando Mulang&amp;#x27;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffart_J/0/1/0/all/0/1"&gt;Johannes Hoffart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shekarpour_S/0/1/0/all/0/1"&gt;Saeedeh Shekarpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraswat_V/0/1/0/all/0/1"&gt;Vijay Saraswat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NewsEmbed: Modeling News through Pre-trained Document Representations. (arXiv:2106.00590v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00590</id>
        <link href="http://arxiv.org/abs/2106.00590"/>
        <updated>2021-06-08T22:44:23.879Z</updated>
        <summary type="html"><![CDATA[Effectively modeling text-rich fresh content such as news articles at
document-level is a challenging problem. To ensure a content-based model
generalize well to a broad range of applications, it is critical to have a
training dataset that is large beyond the scale of human labels while achieving
desired quality. In this work, we address those two challenges by proposing a
novel approach to mine semantically-relevant fresh documents, and their topic
labels, with little human supervision. Meanwhile, we design a multitask model
called NewsEmbed that alternatively trains a contrastive learning with a
multi-label classification to derive a universal document encoder. We show that
the proposed approach can provide billions of high quality organic training
examples and can be naturally extended to multilingual setting where texts in
different languages are encoded in the same semantic space. We experimentally
demonstrate NewsEmbed's competitive performance across multiple natural
language understanding tasks, both supervised and unsupervised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Angular Contrastive Learning with Coarse Labels. (arXiv:2012.03515v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03515</id>
        <link href="http://arxiv.org/abs/2012.03515"/>
        <updated>2021-06-08T02:20:28.134Z</updated>
        <summary type="html"><![CDATA[Few-shot learning methods offer pre-training techniques optimized for easier
later adaptation of the model to new classes (unseen during training) using one
or a few examples. This adaptivity to unseen classes is especially important
for many practical applications where the pre-trained label space cannot remain
fixed for effective use and the model needs to be "specialized" to support new
categories on the fly. One particularly interesting scenario, essentially
overlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where
the training classes (e.g. animals) are of much `coarser granularity' than the
target (test) classes (e.g. breeds). A very practical example of C2FS is when
the target classes are sub-classes of the training classes. Intuitively, it is
especially challenging as (both regular and few-shot) supervised pre-training
tends to learn to ignore intra-class variability which is essential for
separating sub-classes. In this paper, we introduce a novel 'Angular
normalization' module that allows to effectively combine supervised and
self-supervised contrastive pre-training to approach the proposed C2FS task,
demonstrating significant gains in a broad study over multiple baselines and
datasets. We hope that this work will help to pave the way for future research
on this new, challenging, and very practical topic of C2FS classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bukchin_G/0/1/0/all/0/1"&gt;Guy Bukchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1"&gt;Eli Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahar_O/0/1/0/all/0/1"&gt;Ori Shahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1"&gt;Raja Giryes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1"&gt;Leonid Karlinsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Relation Alignment for Calibrated Cross-modal Retrieval. (arXiv:2105.13868v2 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.13868</id>
        <link href="http://arxiv.org/abs/2105.13868"/>
        <updated>2021-06-08T02:20:28.103Z</updated>
        <summary type="html"><![CDATA[Despite the achievements of large-scale multimodal pre-training approaches,
cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.
To bridge the semantic gap between the two modalities, previous studies mainly
focus on word-region alignment at the object level, lacking the matching
between the linguistic relation among the words and the visual relation among
the regions. The neglect of such relation consistency impairs the
contextualized representation of image-text pairs and hinders the model
performance and the interpretability. In this paper, we first propose a novel
metric, Intra-modal Self-attention Distance (ISD), to quantify the relation
consistency by measuring the semantic distance between linguistic and visual
relations. In response, we present Inter-modal Alignment on Intra-modal
Self-attentions (IAIS), a regularized training method to optimize the ISD and
calibrate intra-modal self-attentions from the two modalities mutually via
inter-modal alignment. The IAIS regularizer boosts the performance of
prevailing models on Flickr30k and MS COCO datasets by a considerable margin,
which demonstrates the superiority of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping Saddle Points Faster with Stochastic Momentum. (arXiv:2106.02985v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02985</id>
        <link href="http://arxiv.org/abs/2106.02985"/>
        <updated>2021-06-08T02:20:28.095Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) with stochastic momentum is popular in
nonconvex stochastic optimization and particularly for the training of deep
neural networks. In standard SGD, parameters are updated by improving along the
path of the gradient at the current iterate on a batch of examples, where the
addition of a ``momentum'' term biases the update in the direction of the
previous change in parameters. In non-stochastic convex optimization one can
show that a momentum adjustment provably reduces convergence time in many
settings, yet such results have been elusive in the stochastic and non-convex
settings. At the same time, a widely-observed empirical phenomenon is that in
training deep networks stochastic momentum appears to significantly improve
convergence time, variants of it have flourished in the development of other
popular update methods, e.g. ADAM [KB15], AMSGrad [RKK18], etc. Yet theoretical
justification for the use of stochastic momentum has remained a significant
open question. In this paper we propose an answer: stochastic momentum improves
deep network training because it modifies SGD to escape saddle points faster
and, consequently, to more quickly find a second order stationary point. Our
theoretical results also shed light on the related question of how to choose
the ideal momentum parameter--our analysis suggests that $\beta \in [0,1)$
should be large (close to 1), which comports with empirical findings. We also
provide experimental findings that further validate these conclusions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun-Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chi-Heng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1"&gt;Jacob Abernethy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manipulating SGD with Data Ordering Attacks. (arXiv:2104.09667v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09667</id>
        <link href="http://arxiv.org/abs/2104.09667"/>
        <updated>2021-06-08T02:20:28.089Z</updated>
        <summary type="html"><![CDATA[Machine learning is vulnerable to a wide variety of attacks. It is now well
understood that by changing the underlying data distribution, an adversary can
poison the model trained with it or introduce backdoors. In this paper we
present a novel class of training-time attacks that require no changes to the
underlying dataset or model architecture, but instead only change the order in
which data are supplied to the model. In particular, we find that the attacker
can either prevent the model from learning, or poison it to learn behaviours
specified by the attacker. Furthermore, we find that even a single
adversarially-ordered epoch can be enough to slow down model learning, or even
to reset all of the learning progress. Indeed, the attacks presented here are
not specific to the model or dataset, but rather target the stochastic nature
of modern learning procedures. We extensively evaluate our attacks on computer
vision and natural language benchmarks to find that the adversary can disrupt
model training and even introduce backdoors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumaylov_Z/0/1/0/all/0/1"&gt;Zakhar Shumaylov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1"&gt;Dmitry Kazhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiren Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks. (arXiv:2106.02804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02804</id>
        <link href="http://arxiv.org/abs/2106.02804"/>
        <updated>2021-06-08T02:20:28.082Z</updated>
        <summary type="html"><![CDATA[In applied image segmentation tasks, the ability to provide numerous and
precise labels for training is paramount to the accuracy of the model at
inference time. However, this overhead is often neglected, and recently
proposed segmentation architectures rely heavily on the availability and
fidelity of ground truth labels to achieve state-of-the-art accuracies. Failure
to acknowledge the difficulty in creating adequate ground truths can lead to an
over-reliance on pre-trained models or a lack of adoption in real-world
applications. We introduce Points2Polygons (P2P), a model which makes use of
contextual metric learning techniques that directly addresses this problem.
Points2Polygons performs well against existing fully-supervised segmentation
baselines with limited training data, despite using lightweight segmentation
models (U-Net with a ResNet18 backbone) and having access to only weak labels
in the form of object centroids and no pre-training. We demonstrate this on
several different small but non-trivial datasets. We show that metric learning
using contextual data provides key insights for self-supervised tasks in
general, and allow segmentation models to easily generalize across
traditionally label-intensive domains in computer vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1"&gt;Kuai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_H/0/1/0/all/0/1"&gt;Hakeem Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1"&gt;Daniel Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SketchGen: Generating Constrained CAD Sketches. (arXiv:2106.02711v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02711</id>
        <link href="http://arxiv.org/abs/2106.02711"/>
        <updated>2021-06-08T02:20:28.076Z</updated>
        <summary type="html"><![CDATA[Computer-aided design (CAD) is the most widely used modeling approach for
technical design. The typical starting point in these designs is 2D sketches
which can later be extruded and combined to obtain complex three-dimensional
assemblies. Such sketches are typically composed of parametric primitives, such
as points, lines, and circular arcs, augmented with geometric constraints
linking the primitives, such as coincidence, parallelism, or orthogonality.
Sketches can be represented as graphs, with the primitives as nodes and the
constraints as edges. Training a model to automatically generate CAD sketches
can enable several novel workflows, but is challenging due to the complexity of
the graphs and the heterogeneity of the primitives and constraints. In
particular, each type of primitive and constraint may require a record of
different size and parameter types. We propose SketchGen as a generative model
based on a transformer architecture to address the heterogeneity problem by
carefully designing a sequential language for the primitives and constraints
that allows distinguishing between different primitive or constraint types and
their parameters, while encouraging our model to re-use information across
related parameters, encoding shared structure. A particular highlight of our
work is the ability to produce primitives linked via constraints that enables
the final output to be further regularized via a constraint solver. We evaluate
our model by demonstrating constraint prediction for given sets of primitives
and full sketch generation from scratch, showing that our approach
significantly out performs the state-of-the-art in CAD sketch generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Para_W/0/1/0/all/0/1"&gt;Wamiq Reyaz Para&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1"&gt;Paul Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1"&gt;Tom Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Fewer Tasks through Task Interpolation. (arXiv:2106.02695v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02695</id>
        <link href="http://arxiv.org/abs/2106.02695"/>
        <updated>2021-06-08T02:20:28.057Z</updated>
        <summary type="html"><![CDATA[Meta-learning enables algorithms to quickly learn a newly encountered task
with just a few labeled examples by transferring previously learned knowledge.
However, the bottleneck of current meta-learning algorithms is the requirement
of a large number of meta-training tasks, which may not be accessible in
real-world scenarios. To address the challenge that available tasks may not
densely sample the space of tasks, we propose to augment the task set through
interpolation. By meta-learning with task interpolation (MLTI), our approach
effectively generates additional tasks by randomly sampling a pair of tasks and
interpolating the corresponding features and labels. Under both gradient-based
and metric-based meta-learning settings, our theoretical analysis shows MLTI
corresponds to a data-adaptive meta-regularization and further improves the
generalization. Empirically, in our experiments on eight datasets from diverse
domains including image recognition, pose prediction, molecule property
prediction, and medical image classification, we find that the proposed general
MLTI framework is compatible with representative meta-learning algorithms and
consistently outperforms other state-of-the-art strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Huaxiu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Classification of Very Large Images with Tiny Objects. (arXiv:2106.02694v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02694</id>
        <link href="http://arxiv.org/abs/2106.02694"/>
        <updated>2021-06-08T02:20:28.051Z</updated>
        <summary type="html"><![CDATA[An increasing number of applications in the computer vision domain,
specially, in medical imaging and remote sensing, are challenging when the goal
is to classify very large images with tiny objects. More specifically, these
type of classification tasks face two key challenges: $i$) the size of the
input image in the target dataset is usually in the order of megapixels,
however, existing deep architectures do not easily operate on such big images
due to memory constraints, consequently, we seek a memory-efficient method to
process these images; and $ii$) only a small fraction of the input images are
informative of the label of interest, resulting in low region of interest (ROI)
to image ratio. However, most of the current convolutional neural networks
(CNNs) are designed for image classification datasets that have relatively
large ROIs and small image size (sub-megapixel). Existing approaches have
addressed these two challenges in isolation. We present an end-to-end CNN model
termed Zoom-In network that leverages hierarchical attention sampling for
classification of large images with tiny objects using a single GPU. We
evaluate our method on two large-image datasets and one gigapixel dataset.
Experimental results show that our model achieves higher accuracy than existing
methods while requiring less computing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fanjie Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supercharging Imbalanced Data Learning With Energy-based Contrastive Representation Transfer. (arXiv:2011.12454v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12454</id>
        <link href="http://arxiv.org/abs/2011.12454"/>
        <updated>2021-06-08T02:20:28.044Z</updated>
        <summary type="html"><![CDATA[Dealing with severe class imbalance poses a major challenge for real-world
applications, especially when the accurate classification and generalization of
minority classes is of primary interest. In computer vision, learning from long
tailed datasets is a recurring theme, especially for natural image datasets.
While existing solutions mostly appeal to sampling or weighting adjustments to
alleviate the pathological imbalance, or imposing inductive bias to prioritize
non-spurious associations, we take novel perspectives to promote sample
efficiency and model generalization based on the invariance principles of
causality. Our proposal posits a meta-distributional scenario, where the data
generating mechanism is invariant across the label-conditional feature
distributions. Such causal assumption enables efficient knowledge transfer from
the dominant classes to their under-represented counterparts, even if the
respective feature distributions show apparent disparities. This allows us to
leverage a causal data inflation procedure to enlarge the representation of
minority classes. Our development is orthogonal to the existing extreme
classification techniques thus can be seamlessly integrated. The utility of our
proposal is validated with an extensive set of synthetic and real-world
computer vision tasks against SOTA solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiu_Z/0/1/0/all/0/1"&gt;Zidi Xiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_B/0/1/0/all/0/1"&gt;Benjamin Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chenyang Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02800</id>
        <link href="http://arxiv.org/abs/2106.02800"/>
        <updated>2021-06-08T02:20:28.038Z</updated>
        <summary type="html"><![CDATA[Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides real-time
retinal images with high resolution down to 2 $\mu m$. This technique enables
detection of the morphologies of individual microaneurysms (MAs), which are one
of the earliest signs of diabetic retinopathy (DR), a frequent complication of
diabetes that can lead to visual impairment and blindness. In contrast to
previous automatic models developed for MA detection on standard fundus
photographs, currently there is no high throughput image protocol available for
automatic analysis of AOSLO photographs. To address this urgency, we introduce
AOSLO-net, a deep neural network framework with customized training policy,
including preprocessing, data augmentation and transfer learning, to
automatically segment MAs from AOSLO images. We evaluate the performance of
AOSLO-net using 87 DR AOSLO images demonstrating very accurate MA detection and
segmentation, leading to correct MA morphological classification, while
outperforming the state-of-the-art both in accuracy and cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1"&gt;Konstantina Sampani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shengze Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yixiang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;He Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer K. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Machine Learning Classification Using Quantum Annealing for Real-world Applications. (arXiv:2106.02964v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02964</id>
        <link href="http://arxiv.org/abs/2106.02964"/>
        <updated>2021-06-08T02:20:28.031Z</updated>
        <summary type="html"><![CDATA[Optimizing the training of a machine learning pipeline helps in reducing
training costs and improving model performance. One such optimizing strategy is
quantum annealing, which is an emerging computing paradigm that has shown
potential in optimizing the training of a machine learning model. The
implementation of a physical quantum annealer has been realized by D-Wave
systems and is available to the research community for experiments. Recent
experimental results on a variety of machine learning applications using
quantum annealing have shown interesting results where the performance of
classical machine learning techniques is limited by limited training data and
high dimensional features. This article explores the application of D-Wave's
quantum annealer for optimizing machine learning pipelines for real-world
classification problems. We review the application domains on which a physical
quantum annealer has been used to train machine learning classifiers. We
discuss and analyze the experiments performed on the D-Wave quantum annealer
for applications such as image recognition, remote sensing imagery,
computational biology, and particle physics. We discuss the possible advantages
and the problems for which quantum annealing is likely to be advantageous over
classical computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Nath_R/0/1/0/all/0/1"&gt;Rajdeep Kumar Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Thapliyal_H/0/1/0/all/0/1"&gt;Himanshu Thapliyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Humble_T/0/1/0/all/0/1"&gt;Travis S. Humble&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning. (arXiv:2007.11622v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11622</id>
        <link href="http://arxiv.org/abs/2007.11622"/>
        <updated>2021-06-08T02:20:28.025Z</updated>
        <summary type="html"><![CDATA[On-device learning enables edge devices to continually adapt the AI models to
new data, which requires a small memory footprint to fit the tight memory
constraint of edge devices. Existing work solves this problem by reducing the
number of trainable parameters. However, this doesn't directly translate to
memory saving since the major bottleneck is the activations, not parameters. In
this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient
on-device learning. TinyTL freezes the weights while only learns the bias
modules, thus no need to store the intermediate activations. To maintain the
adaptation capacity, we introduce a new memory-efficient bias module, the lite
residual module, to refine the feature extractor by learning small residual
feature maps adding only 3.8% memory overhead. Extensive experiments show that
TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss
compared to fine-tuning the full network. Compared to fine-tuning the last
layer, TinyTL provides significant accuracy improvements (up to 34.1%) with
little memory overhead. Furthermore, combined with feature extractor
adaptation, TinyTL provides 7.3-12.9x memory saving without sacrificing
accuracy compared to fine-tuning the full Inception-V3.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Han Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Ligeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy. (arXiv:2104.09435v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09435</id>
        <link href="http://arxiv.org/abs/2104.09435"/>
        <updated>2021-06-08T02:20:28.005Z</updated>
        <summary type="html"><![CDATA[Volumetric imaging by fluorescence microscopy is often limited by anisotropic
spatial resolution from inferior axial resolution compared to the lateral
resolution. To address this problem, here we present a deep-learning-enabled
unsupervised super-resolution technique that enhances anisotropic images in
volumetric fluorescence microscopy. In contrast to the existing deep learning
approaches that require matched high-resolution target volume images, our
method greatly reduces the effort to put into practice as the training of a
network requires as little as a single 3D image stack, without a priori
knowledge of the image formation process, registration of training data, or
separate acquisition of target data. This is achieved based on the optimal
transport driven cycle-consistent generative adversarial network that learns
from an unpaired matching between high-resolution 2D images in lateral image
plane and low-resolution 2D images in the other planes. Using fluorescence
confocal microscopy and light-sheet microscopy, we demonstrate that the trained
network not only enhances axial resolution, but also restores suppressed visual
details between the imaging planes and removes imaging artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyoungjun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_M/0/1/0/all/0/1"&gt;Myeongsu Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Bumju Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Soohyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Ki Hean Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Sunghoe Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Learning of Hate Speech Classification on Social Media. (arXiv:2106.02821v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02821</id>
        <link href="http://arxiv.org/abs/2106.02821"/>
        <updated>2021-06-08T02:20:27.999Z</updated>
        <summary type="html"><![CDATA[Existing work on automated hate speech classification assumes that the
dataset is fixed and the classes are pre-defined. However, the amount of data
in social media increases every day, and the hot topics changes rapidly,
requiring the classifiers to be able to continuously adapt to new data without
forgetting the previously learned knowledge. This ability, referred to as
lifelong learning, is crucial for the real-word application of hate speech
classifiers in social media. In this work, we propose lifelong learning of hate
speech classification on social media. To alleviate catastrophic forgetting, we
propose to use Variational Representation Learning (VRL) along with a memory
module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural
Network). Experimentally, we show that combining variational representation
learning and the LB-SOINN memory module achieves better performance than the
commonly-used lifelong learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jing Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1"&gt;Mai ElSherief&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xifeng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPSfM: Global Projective SFM Using Algebraic Constraints on Multi-View Fundamental Matrices. (arXiv:1812.00426v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.00426</id>
        <link href="http://arxiv.org/abs/1812.00426"/>
        <updated>2021-06-08T02:20:27.993Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of recovering projective camera matrices
from collections of fundamental matrices in multiview settings. We make two
main contributions. First, given ${n \choose 2}$ fundamental matrices computed
for $n$ images, we provide a complete algebraic characterization in the form of
conditions that are both necessary and sufficient to enabling the recovery of
camera matrices. These conditions are based on arranging the fundamental
matrices as blocks in a single matrix, called the $n$-view fundamental matrix,
and characterizing this matrix in terms of the signs of its eigenvalues and
rank structures. Secondly, we propose a concrete algorithm for projective
structure-from-motion that utilizes this characterization. Given a complete or
partial collection of measured fundamental matrices, our method seeks camera
matrices that minimize a global algebraic error for the measured fundamental
matrices. In contrast to existing methods, our optimization, without any
initialization, produces a consistent set of fundamental matrices that
corresponds to a unique set of cameras (up to a choice of projective frame).
Our experiments indicate that our method achieves state of the art performance
in both accuracy and running time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1"&gt;Yoni Kasten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geifman_A/0/1/0/all/0/1"&gt;Amnon Geifman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galun_M/0/1/0/all/0/1"&gt;Meirav Galun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1"&gt;Ronen Basri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Contrastive Learning: Removing Undesirable Information in Self-Supervised Representations. (arXiv:2106.02866v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02866</id>
        <link href="http://arxiv.org/abs/2106.02866"/>
        <updated>2021-06-08T02:20:27.986Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning is a form of unsupervised learning that leverages
rich information in data to learn representations. However, data sometimes
contains certain information that may be undesirable for downstream tasks. For
instance, gender information may lead to biased decisions on many
gender-irrelevant tasks. In this paper, we develop conditional contrastive
learning to remove undesirable information in self-supervised representations.
To remove the effect of the undesirable variable, our proposed approach
conditions on the undesirable variable (i.e., by fixing the variations of it)
during the contrastive learning process. In particular, inspired by the
contrastive objective InfoNCE, we introduce Conditional InfoNCE (C-InfoNCE),
and its computationally efficient variant, Weak-Conditional InfoNCE
(WeaC-InfoNCE), for conditional contrastive learning. We demonstrate
empirically that our methods can successfully learn self-supervised
representations for downstream tasks while removing a great level of
information related to the undesirable variables. We study three scenarios,
each with a different type of undesirable variables: task-irrelevant
meta-information for self-supervised speech representation learning, sensitive
attributes for fair representation learning, and domain specification for
multi-domain visual representation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yao-Hung Hubert Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Martin Q. Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IPS300+: a Challenging Multimodal Dataset for Intersection Perception System. (arXiv:2106.02781v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02781</id>
        <link href="http://arxiv.org/abs/2106.02781"/>
        <updated>2021-06-08T02:20:27.980Z</updated>
        <summary type="html"><![CDATA[Due to the high complexity and occlusion, insufficient perception in the
crowded urban intersection can be a serious safety risk for both human drivers
and autonomous algorithms, whereas CVIS (Cooperative Vehicle Infrastructure
System) is a proposed solution for full-participants perception in this
scenario. However, the research on roadside multimodal perception is still in
its infancy, and there is no open-source dataset for such scenario.
Accordingly, this paper fills the gap. Through an IPS (Intersection Perception
System) installed at the diagonal of the intersection, this paper proposes a
high-quality multimodal dataset for the intersection perception task. The
center of the experimental intersection covers an area of 3000m2, and the
extended distance reaches 300m, which is typical for CVIS. The first batch of
open-source data includes 14198 frames, and each frame has an average of 319.84
labels, which is 9.6 times larger than the most crowded dataset (H3D dataset in
2019) by now. In order to facilitate further study, this dataset tries to keep
the label documents consistent with the KITTI dataset, and a standardized
benchmark is created for algorithm evaluation. Our dataset is available at:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huanan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shuyue Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yongqiang Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks. (arXiv:2106.02817v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02817</id>
        <link href="http://arxiv.org/abs/2106.02817"/>
        <updated>2021-06-08T02:20:27.959Z</updated>
        <summary type="html"><![CDATA[Imbalanced classification on graphs is ubiquitous yet challenging in many
real-world applications, such as fraudulent node detection. Recently, graph
neural networks (GNNs) have shown promising performance on many network
analysis tasks. However, most existing GNNs have almost exclusively focused on
the balanced networks, and would get unappealing performance on the imbalanced
networks. To bridge this gap, in this paper, we present a generative
adversarial graph network model, called ImGAGN to address the imbalanced
classification problem on graphs. It introduces a novel generator for graph
structure data, named GraphGenerator, which can simulate both the minority
class nodes' attribute distribution and network topological structure
distribution by generating a set of synthetic minority nodes such that the
number of nodes in different classes can be balanced. Then a graph
convolutional network (GCN) discriminator is trained to discriminate between
real nodes and fake (i.e., generated) nodes, and also between minority nodes
and majority nodes on the synthetic balanced network. To validate the
effectiveness of the proposed method, extensive experiments are conducted on
four real-world imbalanced network datasets. Experimental results demonstrate
that the proposed method ImGAGN outperforms state-of-the-art algorithms for
semi-supervised imbalanced node classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liang Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Huaisheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Ruiqi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuhui Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Discovery, Control, and Disentanglement of Semantic Attributes with Applications to Anomaly Detection. (arXiv:2002.11169v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11169</id>
        <link href="http://arxiv.org/abs/2002.11169"/>
        <updated>2021-06-08T02:20:27.950Z</updated>
        <summary type="html"><![CDATA[Our work focuses on unsupervised and generative methods that address the
following goals: (a) learning unsupervised generative representations that
discover latent factors controlling image semantic attributes, (b) studying how
this ability to control attributes formally relates to the issue of latent
factor disentanglement, clarifying related but dissimilar concepts that had
been confounded in the past, and (c) developing anomaly detection methods that
leverage representations learned in (a). For (a), we propose a network
architecture that exploits the combination of multiscale generative models with
mutual information (MI) maximization. For (b), we derive an analytical result
(Lemma 1) that brings clarity to two related but distinct concepts: the ability
of generative networks to control semantic attributes of images they generate,
resulting from MI maximization, and the ability to disentangle latent space
representations, obtained via total correlation minimization. More
specifically, we demonstrate that maximizing semantic attribute control
encourages disentanglement of latent factors. Using Lemma 1 and adopting MI in
our loss function, we then show empirically that, for image generation tasks,
the proposed approach exhibits superior performance as measured in the quality
and disentanglement trade space, when compared to other state of the art
methods, with quality assessed via the Frechet Inception Distance (FID), and
disentanglement via mutual information gap. For (c), we design several systems
for anomaly detection exploiting representations learned in (a), and
demonstrate their performance benefits when compared to state-of-the-art
generative and discriminative algorithms. The above contributions in
representation learning have potential applications in addressing other
important problems in computer vision, such as bias and privacy in AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_I/0/1/0/all/0/1"&gt;I-Jeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Philippe Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual communication of object concepts at different levels of abstraction. (arXiv:2106.02775v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02775</id>
        <link href="http://arxiv.org/abs/2106.02775"/>
        <updated>2021-06-08T02:20:27.943Z</updated>
        <summary type="html"><![CDATA[People can produce drawings of specific entities (e.g., Garfield), as well as
general categories (e.g., "cat"). What explains this ability to produce such
varied drawings of even highly familiar object concepts? We hypothesized that
drawing objects at different levels of abstraction depends on both sensory
information and representational goals, such that drawings intended to portray
a recently seen object preserve more detail than those intended to represent a
category. Participants drew objects cued either with a photo or a category
label. For each cue type, half the participants aimed to draw a specific
exemplar; the other half aimed to draw the category. We found that label-cued
category drawings were the most recognizable at the basic level, whereas
photo-cued exemplar drawings were the least recognizable. Together, these
findings highlight the importance of task context for explaining how people use
drawings to communicate visual concepts in different ways.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Justin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Judith E. Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Replay-based Continual Zero-Shot Learning. (arXiv:2101.08894v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08894</id>
        <link href="http://arxiv.org/abs/2101.08894"/>
        <updated>2021-06-08T02:20:27.934Z</updated>
        <summary type="html"><![CDATA[Zero-shot learning is a new paradigm to classify objects from classes that
are not available at training time. Zero-shot learning (ZSL) methods have
attracted considerable attention in recent years because of their ability to
classify unseen/novel class examples. Most of the existing approaches on ZSL
works when all the samples from seen classes are available to train the model,
which does not suit real life. In this paper, we tackle this hindrance by
developing a generative replay-based continual ZSL (GRCZSL). The proposed
method endows traditional ZSL to learn from streaming data and acquire new
knowledge without forgetting the previous tasks' gained experience. We handle
catastrophic forgetting in GRCZSL by replaying the synthetic samples of seen
classes, which have appeared in the earlier tasks. These synthetic samples are
synthesized using the trained conditional variational autoencoder (VAE) over
the immediate past task. Moreover, we only require the current and immediate
previous VAE at any time for training and testing. The proposed GRZSL method is
developed for a single-head setting of continual learning, simulating a
real-world problem setting. In this setting, task identity is given during
training but unavailable during testing. GRCZSL performance is evaluated on
five benchmark datasets for the generalized setup of ZSL with fixed and dynamic
(incremental class) settings of continual learning. The existing class setting
presented recently in the literature is not suitable for a class-incremental
setting. Therefore, this paper proposes a new setting to address this issue.
Experimental results show that the proposed method significantly outperforms
the baseline and the state-of-the-art method and makes it more suitable for
real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_C/0/1/0/all/0/1"&gt;Chandan Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parameswaran_S/0/1/0/all/0/1"&gt;Sethupathy Parameswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1"&gt;Ashish Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Suresh Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Story Generation on Plural Images. (arXiv:2001.10980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10980</id>
        <link href="http://arxiv.org/abs/2001.10980"/>
        <updated>2021-06-08T02:20:27.927Z</updated>
        <summary type="html"><![CDATA[Traditionally, text generation models take in a sequence of text as input,
and iteratively generate the next most probable word using pre-trained
parameters. In this work, we propose the architecture to use images instead of
text as the input of the text generation model, called StoryGen. In the
architecture, we design a Relational Text Data Generator algorithm that relates
different features from multiple images. The output samples from the model
demonstrate the ability to generate meaningful paragraphs of text containing
the extracted features from the input images. This is an undergraduate project
report. Completed Dec. 2019 at the Cooper Union.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Flow Regularization: Improving Structured Sparsity in Deep Neural Networks. (arXiv:2106.02914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02914</id>
        <link href="http://arxiv.org/abs/2106.02914"/>
        <updated>2021-06-08T02:20:27.909Z</updated>
        <summary type="html"><![CDATA[Pruning is a model compression method that removes redundant parameters in
deep neural networks (DNNs) while maintaining accuracy. Most available filter
pruning methods require complex treatments such as iterative pruning, features
statistics/ranking, or additional optimization designs in the training process.
In this paper, we propose a simple and effective regularization strategy from a
new perspective of evolution of features, which we call feature flow
regularization (FFR), for improving structured sparsity and filter pruning in
DNNs. Specifically, FFR imposes controls on the gradient and curvature of
feature flow along the neural network, which implicitly increases the sparsity
of the parameters. The principle behind FFR is that coherent and smooth
evolution of features will lead to an efficient network that avoids redundant
parameters. The high structured sparsity obtained from FFR enables us to prune
filters effectively. Experiments with VGGNets, ResNets on CIFAR-10/100, and
Tiny ImageNet datasets demonstrate that FFR can significantly improve both
unstructured and structured sparsity. Our pruning results in terms of reduction
of parameters and FLOPs are comparable to or even better than those of
state-of-the-art pruning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yuan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Luchan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yang Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[i3dLoc: Image-to-range Cross-domain Localization Robust to Inconsistent Environmental Conditions. (arXiv:2105.12883v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12883</id>
        <link href="http://arxiv.org/abs/2105.12883"/>
        <updated>2021-06-08T02:20:27.903Z</updated>
        <summary type="html"><![CDATA[We present a method for localizing a single camera with respect to a point
cloud map in indoor and outdoor scenes. The problem is challenging because
correspondences of local invariant features are inconsistent across the domains
between image and 3D. The problem is even more challenging as the method must
handle various environmental conditions such as illumination, weather, and
seasonal changes. Our method can match equirectangular images to the 3D range
projections by extracting cross-domain symmetric place descriptors. Our key
insight is to retain condition-invariant 3D geometry features from limited data
samples while eliminating the condition-related features by a designed
Generative Adversarial Network. Based on such features, we further design a
spherical convolution network to learn viewpoint-invariant symmetric place
descriptors. We evaluate our method on extensive self-collected datasets, which
involve \textit{Long-term} (variant appearance conditions),
\textit{Large-scale} (up to $2km$ structure/unstructured environment), and
\textit{Multistory} (four-floor confined space). Our method surpasses other
current state-of-the-arts by achieving around $3$ times higher place retrievals
to inconsistent environments, and above $3$ times accuracy on online
localization. To highlight our method's generalization capabilities, we also
evaluate the recognition across different datasets. With a single trained
model, i3dLoc can demonstrate reliable visual localization in random
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Peng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lingyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Ji Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choset_H/0/1/0/all/0/1"&gt;Howie Choset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1"&gt;Sebastian Scherer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Generalized Additive 2 Model with Consideration of High-Order Interactions. (arXiv:2106.02836v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02836</id>
        <link href="http://arxiv.org/abs/2106.02836"/>
        <updated>2021-06-08T02:20:27.896Z</updated>
        <summary type="html"><![CDATA[In recent years, machine learning and AI have been introduced in many
industrial fields. In fields such as finance, medicine, and autonomous driving,
where the inference results of a model may have serious consequences, high
interpretability as well as prediction accuracy is required. In this study, we
propose CGA2M+, which is based on the Generalized Additive 2 Model (GA2M) and
differs from it in two major ways. The first is the introduction of
monotonicity. Imposing monotonicity on some functions based on an analyst's
knowledge is expected to improve not only interpretability but also
generalization performance. The second is the introduction of a higher-order
term: given that GA2M considers only second-order interactions, we aim to
balance interpretability and prediction accuracy by introducing a higher-order
term that can capture higher-order interactions. In this way, we can improve
prediction performance without compromising interpretability by applying
learning innovation. Numerical experiments showed that the proposed model has
high predictive performance and interpretability. Furthermore, we confirmed
that generalization performance is improved by introducing monotonicity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_A/0/1/0/all/0/1"&gt;Akihisa Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuramata_M/0/1/0/all/0/1"&gt;Michiya Kuramata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majima_K/0/1/0/all/0/1"&gt;Kaito Majima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiyohara_H/0/1/0/all/0/1"&gt;Haruka Kiyohara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondo_K/0/1/0/all/0/1"&gt;Kensho Kondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1"&gt;Kazuhide Nakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft-Attention Improves Skin Cancer Classification Performance. (arXiv:2105.03358v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03358</id>
        <link href="http://arxiv.org/abs/2105.03358"/>
        <updated>2021-06-08T02:20:27.890Z</updated>
        <summary type="html"><![CDATA[In clinical applications, neural networks must focus on and highlight the
most important parts of an input image. Soft-Attention mechanism enables a
neural network toachieve this goal. This paper investigates the effectiveness
of Soft-Attention in deep neural architectures. The central aim of
Soft-Attention is to boost the value of important features and suppress the
noise-inducing features. We compare the performance of VGG, ResNet,
InceptionResNetv2 and DenseNet architectures with and without the
Soft-Attention mechanism, while classifying skin lesions. The original network
when coupled with Soft-Attention outperforms the baseline[16] by 4.7% while
achieving a precision of 93.7% on HAM10000 dataset [25]. Additionally,
Soft-Attention coupling improves the sensitivity score by 3.8% compared to
baseline[31] and achieves 91.6% on ISIC-2017 dataset [2]. The code is publicly
available at github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Datta_S/0/1/0/all/0/1"&gt;Soumyya Kanti Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shaikh_M/0/1/0/all/0/1"&gt;Mohammad Abuzar Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Srihari_S/0/1/0/all/0/1"&gt;Sargur N. Srihari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mingchen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Aware Sparse Deep Coordination Graphs. (arXiv:2106.02886v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02886</id>
        <link href="http://arxiv.org/abs/2106.02886"/>
        <updated>2021-06-08T02:20:27.884Z</updated>
        <summary type="html"><![CDATA[Learning sparse coordination graphs adaptive to the coordination dynamics
among agents is a long-standing problem in cooperative multi-agent learning.
This paper studies this problem by proposing several value-based and
observation-based schemes for learning dynamic topologies and evaluating them
on a new Multi-Agent COordination (MACO) benchmark. The benchmark collects
classic coordination problems in the literature, increases their difficulty,
and classifies them into different types. By analyzing the individual
advantages of each learning scheme on each type of problem and their overall
performance, we propose a novel method using the variance of utility difference
functions to learn context-aware sparse coordination topologies. Moreover, our
method learns action representations that effectively reduce the influence of
utility functions' estimation errors on graph construction. Experiments show
that our method significantly outperforms dense and static topologies across
the MACO and StarCraft II micromanagement benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tonghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1"&gt;Liang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Weijun Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qianlan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Budget Active Learning via Wasserstein Distance: An Integer Programming Approach. (arXiv:2106.02968v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02968</id>
        <link href="http://arxiv.org/abs/2106.02968"/>
        <updated>2021-06-08T02:20:27.867Z</updated>
        <summary type="html"><![CDATA[Given restrictions on the availability of data, active learning is the
process of training a model with limited labeled data by selecting a core
subset of an unlabeled data pool to label. Although selecting the most useful
points for training is an optimization problem, the scale of deep learning data
sets forces most selection strategies to employ efficient heuristics. Instead,
we propose a new integer optimization problem for selecting a core set that
minimizes the discrete Wasserstein distance from the unlabeled pool. We
demonstrate that this problem can be tractably solved with a Generalized
Benders Decomposition algorithm. Our strategy requires high-quality latent
features which we obtain by unsupervised learning on the unlabeled pool.
Numerical results on several data sets show that our optimization approach is
competitive with baselines and particularly outperforms them in the low budget
regime where less than one percent of the data set is labeled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Rafid Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Marc T. Law&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PURS: Personalized Unexpected Recommender System for Improving User Satisfaction. (arXiv:2106.02771v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02771</id>
        <link href="http://arxiv.org/abs/2106.02771"/>
        <updated>2021-06-08T02:20:27.861Z</updated>
        <summary type="html"><![CDATA[Classical recommender system methods typically face the filter bubble problem
when users only receive recommendations of their familiar items, making them
bored and dissatisfied. To address the filter bubble problem, unexpected
recommendations have been proposed to recommend items significantly deviating
from user's prior expectations and thus surprising them by presenting "fresh"
and previously unexplored items to the users. In this paper, we describe a
novel Personalized Unexpected Recommender System (PURS) model that incorporates
unexpectedness into the recommendation process by providing multi-cluster
modeling of user interests in the latent space and personalized unexpectedness
via the self-attention mechanism and via selection of an appropriate unexpected
activation function. Extensive offline experiments on three real-world datasets
illustrate that the proposed PURS model significantly outperforms the
state-of-the-art baseline approaches in terms of both accuracy and
unexpectedness measures. In addition, we conduct an online A/B test at a major
video platform Alibaba-Youku, where our model achieves over 3\% increase in the
average video view per user metric. The proposed model is in the process of
being deployed by the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Story Generation on Plural Images. (arXiv:2001.10980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10980</id>
        <link href="http://arxiv.org/abs/2001.10980"/>
        <updated>2021-06-08T02:20:27.854Z</updated>
        <summary type="html"><![CDATA[Traditionally, text generation models take in a sequence of text as input,
and iteratively generate the next most probable word using pre-trained
parameters. In this work, we propose the architecture to use images instead of
text as the input of the text generation model, called StoryGen. In the
architecture, we design a Relational Text Data Generator algorithm that relates
different features from multiple images. The output samples from the model
demonstrate the ability to generate meaningful paragraphs of text containing
the extracted features from the input images. This is an undergraduate project
report. Completed Dec. 2019 at the Cooper Union.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives. (arXiv:2001.06471v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.06471</id>
        <link href="http://arxiv.org/abs/2001.06471"/>
        <updated>2021-06-08T02:20:27.845Z</updated>
        <summary type="html"><![CDATA[We consider a discrete optimization formulation for learning sparse
classifiers, where the outcome depends upon a linear combination of a small
subset of features. Recent work has shown that mixed integer programming (MIP)
can be used to solve (to optimality) $\ell_0$-regularized regression problems
at scales much larger than what was conventionally considered possible. Despite
their usefulness, MIP-based global optimization approaches are significantly
slower compared to the relatively mature algorithms for $\ell_1$-regularization
and heuristics for nonconvex regularized problems. We aim to bridge this gap in
computation times by developing new MIP-based algorithms for
$\ell_0$-regularized classification. We propose two classes of scalable
algorithms: an exact algorithm that can handle $p\approx 50,000$ features in a
few minutes, and approximate algorithms that can address instances with
$p\approx 10^6$ in times comparable to the fast $\ell_1$-based algorithms. Our
exact algorithm is based on the novel idea of \textsl{integrality generation},
which solves the original problem (with $p$ binary variables) via a sequence of
mixed integer programs that involve a small number of binary variables. Our
approximate algorithms are based on coordinate descent and local combinatorial
search. In addition, we present new estimation error bounds for a class of
$\ell_0$-regularized estimators. Experiments on real and synthetic data
demonstrate that our approach leads to models with considerably improved
statistical performance (especially, variable selection) when compared to
competing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dedieu_A/0/1/0/all/0/1"&gt;Antoine Dedieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hazimeh_H/0/1/0/all/0/1"&gt;Hussein Hazimeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mazumder_R/0/1/0/all/0/1"&gt;Rahul Mazumder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Variational Bayesian Framework for Blind Image Deblurring. (arXiv:2106.02884v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02884</id>
        <link href="http://arxiv.org/abs/2106.02884"/>
        <updated>2021-06-08T02:20:27.839Z</updated>
        <summary type="html"><![CDATA[Blind image deblurring is an important yet very challenging problem in
low-level vision. Traditional optimization based methods generally formulate
this task as a maximum-a-posteriori estimation or variational inference
problem, whose performance highly relies on the handcraft priors for both the
latent image and the blur kernel. In contrast, recent deep learning methods
generally learn, from a large collection of training images, deep neural
networks (DNNs) directly mapping the blurry image to the clean one or to the
blur kernel, paying less attention to the physical degradation process of the
blurry image. In this paper, we present a deep variational Bayesian framework
for blind image deblurring. Under this framework, the posterior of the latent
clean image and blur kernel can be jointly estimated in an amortized inference
fashion with DNNs, and the involved inference DNNs can be trained by fully
considering the physical blur model, together with the supervision of data
driven priors for the clean image and blur kernel, which is naturally led to by
the evidence lower bound objective. Comprehensive experiments are conducted to
substantiate the effectiveness of the proposed framework. The results show that
it can not only achieve a promising performance with relatively simple
networks, but also enhance the performance of existing DNNs for deblurring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yue_Z/0/1/0/all/0/1"&gt;Zongsheng Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Failure Criterion for Composites using k-Nearest Neighbor Classification. (arXiv:2106.02714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02714</id>
        <link href="http://arxiv.org/abs/2106.02714"/>
        <updated>2021-06-08T02:20:27.824Z</updated>
        <summary type="html"><![CDATA[Numerous theories of failure have been postulated and implemented in various
commercial programs for composite materials. Even the best theories have had
limited success in predicting damage and failure in validation exercises. In
view of this background, many researchers have started exploring the use of
multiscale modeling to improve the fidelity of the modeling and simulation of
various structural and materials systems. In this paper, a multi-scale modeling
scheme is used to illustrate how a combination of virtual and laboratory
testing programs can be used to generate a point cloud of failure surface data
that can then be queried during finite element analysis at the continuum scale
to ascertain if the onset of failure has occurred. The k-nearest neighbor
(k-NN) classification concept is used to obtain the answer to the query. A
linear, elastic, static finite element example using a unidirectional composite
shows that the framework can be generated and used effectively and efficiently
with the possibility to extend the approach for all types of composite
architectures and behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_S/0/1/0/all/0/1"&gt;Subramaniam Rajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khaled_B/0/1/0/all/0/1"&gt;Bilal Khaled&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shyamsunder_L/0/1/0/all/0/1"&gt;Loukham Shyamsunder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Should Graph Convolution Trust Neighbors? A Simple Causal Inference Method. (arXiv:2010.11797v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11797</id>
        <link href="http://arxiv.org/abs/2010.11797"/>
        <updated>2021-06-08T02:20:26.099Z</updated>
        <summary type="html"><![CDATA[Graph Convolutional Network (GCN) is an emerging technique for information
retrieval (IR) applications. While GCN assumes the homophily property of a
graph, real-world graphs are never perfect: the local structure of a node may
contain discrepancy, e.g., the labels of a node's neighbors could vary. This
pushes us to consider the discrepancy of local structure in GCN modeling.
Existing work approaches this issue by introducing an additional module such as
graph attention, which is expected to learn the contribution of each neighbor.
However, such module may not work reliably as expected, especially when there
lacks supervision signal, e.g., when the labeled data is small. Moreover,
existing methods focus on modeling the nodes in the training data, and never
consider the local structure discrepancy of testing nodes.

This work focuses on the local structure discrepancy issue for testing nodes,
which has received little scrutiny. From a novel perspective of causality, we
investigate whether a GCN should trust the local structure of a testing node
when predicting its label. To this end, we analyze the working mechanism of GCN
with causal graph, estimating the causal effect of a node's local structure for
the prediction. The idea is simple yet effective: given a trained GCN model, we
first intervene the prediction by blocking the graph structure; we then compare
the original prediction with the intervened prediction to assess the causal
effect of the local structure on the prediction. Through this way, we can
eliminate the impact of local structure discrepancy and make more accurate
prediction. Extensive experiments on seven node classification datasets show
that our method effectively enhances the inference stage of GCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weiran Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qifan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitation Learning via Simultaneous Optimization of Policies and Auxiliary Trajectories. (arXiv:2105.03019v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03019</id>
        <link href="http://arxiv.org/abs/2105.03019"/>
        <updated>2021-06-08T02:20:26.066Z</updated>
        <summary type="html"><![CDATA[Imitation learning (IL) is a frequently used approach for data-efficient
policy learning. Many IL methods, such as Dataset Aggregation (DAgger), combat
challenges like distributional shift by interacting with oracular experts.
Unfortunately, assuming access to oracular experts is often unrealistic in
practice; data used in IL frequently comes from offline processes such as
lead-through or teleoperation. In this paper, we present a novel imitation
learning technique called Collocation for Demonstration Encoding (CoDE) that
operates on only a fixed set of trajectory demonstrations. We circumvent
challenges with methods like back-propagation-through-time by introducing an
auxiliary trajectory network, which takes inspiration from collocation
techniques in optimal control. Our method generalizes well and more accurately
reproduces the demonstrated behavior with fewer guiding trajectories when
compared to standard behavioral cloning methods. We present simulation results
on a 7-degree-of-freedom (DoF) robotic manipulator that learns to exhibit
lifting, target-reaching, and obstacle avoidance behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1"&gt;Mandy Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Anqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyk_K/0/1/0/all/0/1"&gt;Karl Van Wyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dellaert_F/0/1/0/all/0/1"&gt;Frank Dellaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1"&gt;Byron Boots&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratliff_N/0/1/0/all/0/1"&gt;Nathan Ratliff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Kickstarting Deep Reinforcement Learning with Privacy-Aware Learners. (arXiv:2102.09599v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09599</id>
        <link href="http://arxiv.org/abs/2102.09599"/>
        <updated>2021-06-08T02:20:26.023Z</updated>
        <summary type="html"><![CDATA[Kickstarting deep reinforcement learning algorithms facilitate a
teacher-student relationship among the agents and allow for a well-performing
teacher to share demonstrations with a student to expedite the student's
training. However, despite the known benefits, the demonstrations may contain
sensitive information about the teacher's training data and existing
kickstarting methods do not take any measures to protect it. Therefore, we use
the framework of differential privacy to develop a mechanism that securely
shares the teacher's demonstrations with the student. The mechanism allows for
the teacher to decide upon the accuracy of its demonstrations with respect to
the privacy budget that it consumes, thereby granting the teacher full control
over its data privacy. We then develop a kickstarted deep reinforcement
learning algorithm for the student that is privacy-aware because we calibrate
its objective with the parameters of the teacher's privacy mechanism. The
privacy-aware design of the algorithm makes it possible to kickstart the
student's learning despite the perturbations induced by the privacy mechanism.
From numerical experiments, we highlight three empirical results: (i) the
algorithm succeeds in expediting the student's learning, (ii) the student
converges to a performance level that was not possible without the
demonstrations, and (iii) the student maintains its enhanced performance even
after the teacher stops sharing useful demonstrations due to its privacy budget
constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gohari_P/0/1/0/all/0/1"&gt;Parham Gohari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hale_M/0/1/0/all/0/1"&gt;Matthew Hale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid-Attention Guided Network with Multiple Resolution Features for Person Re-Identification. (arXiv:2009.07536v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07536</id>
        <link href="http://arxiv.org/abs/2009.07536"/>
        <updated>2021-06-08T02:20:26.017Z</updated>
        <summary type="html"><![CDATA[Extracting effective and discriminative features is very important for
addressing the challenging person re-identification (re-ID) task. Prevailing
deep convolutional neural networks (CNNs) usually use high-level features for
identifying pedestrian. However, some essential spatial information resided in
low-level features such as shape, texture and color will be lost when learning
the high-level features, due to extensive padding and pooling operations in the
training stage. In addition, most existing person re-ID methods are mainly
based on hand-craft bounding boxes where images are precisely aligned. It is
unrealistic in practical applications, since the exploited object detection
algorithms often produce inaccurate bounding boxes. This will inevitably
degrade the performance of existing algorithms. To address these problems, we
put forward a novel person re-ID model that fuses high- and low-level
embeddings to reduce the information loss caused in learning high-level
features. Then we divide the fused embedding into several parts and reconnect
them to obtain the global feature and more significant local features, so as to
alleviate the affect caused by the inaccurate bounding boxes. In addition, we
also introduce the spatial and channel attention mechanisms in our model, which
aims to mine more discriminative features related to the target. Finally, we
reconstruct the feature extractor to ensure that our model can obtain more
richer and robust features. Extensive experiments display the superiority of
our approach compared with existing approaches. Our code is available at
https://github.com/libraflower/MutipleFeature-for-PRID.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junchuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuhui Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shengyong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03736</id>
        <link href="http://arxiv.org/abs/2104.03736"/>
        <updated>2021-06-08T02:20:25.998Z</updated>
        <summary type="html"><![CDATA[Meta-learning can extract an inductive bias from previous learning experience
and assist the training processes of new tasks. It is often realized through
optimizing a meta-model with the evaluation loss of a series of task-specific
solvers. Most existing algorithms sample non-overlapping $\mathit{support}$
sets and $\mathit{query}$ sets to train and evaluate the solvers respectively
due to simplicity ($\mathcal{S}/\mathcal{Q}$ protocol). However, another
evaluation method that assesses the discrepancy between the solver and a target
model is short of research ($\mathcal{S}/\mathcal{T}$ protocol).
$\mathcal{S}/\mathcal{T}$ protocol has unique advantages such as offering more
informative supervision, but it is computationally expensive. This paper looks
into this special evaluation method and takes a step towards putting it into
practice. We find that with a small ratio of tasks armed with target models,
classic meta-learning algorithms can be improved a lot without consuming many
resources. Furthermore, we empirically verify the effectiveness of
$\mathcal{S}/\mathcal{T}$ protocol in a typical application of meta-learning,
$\mathit{i.e.}$, few-shot learning. In detail, after constructing target models
by fine-tuning the pre-trained network on those hard tasks, we match the
task-specific solvers to target models via knowledge distillation. Experiments
demonstrate the superiority of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Su Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1"&gt;Le Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret Minimization Experience Replay. (arXiv:2105.07253v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07253</id>
        <link href="http://arxiv.org/abs/2105.07253"/>
        <updated>2021-06-08T02:20:25.992Z</updated>
        <summary type="html"><![CDATA[In reinforcement learning, experience replay stores past samples for further
reuse. Prioritized sampling is a promising technique to better utilize these
samples. Previous criteria of prioritization include TD error, recentness and
corrective feedback, which are mostly heuristically designed. In this work, we
start from the regret minimization objective, and obtain an optimal
prioritization strategy for Bellman update that can directly maximize the
return of the policy. The theory suggests that data with higher hindsight TD
error, better on-policiness and more accurate Q value should be assigned with
higher weights during sampling. Thus most previous criteria only consider this
strategy partially. We not only provide theoretical justifications for previous
criteria, but also propose two new methods to compute the prioritization
weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT
exploits the temporal ordering of states. Both methods outperform previous
prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo,
Atari and Meta-World.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zhenghai Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu-Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1"&gt;Jing-Cheng Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shengyi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Feng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yang Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Discrepancy in Strategic Learning. (arXiv:2103.01028v3 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01028</id>
        <link href="http://arxiv.org/abs/2103.01028"/>
        <updated>2021-06-08T02:20:25.986Z</updated>
        <summary type="html"><![CDATA[We study the effects of information discrepancy across sub-populations on
their ability to simultaneously improve their features in strategic learning
settings. Specifically, we consider a game where a principal deploys a decision
rule in an attempt to optimize the whole population's welfare, and agents
strategically adapt to it to receive better scores. Inspired by real-life
settings, such as loan approvals and college admissions, we remove the typical
assumption made in the strategic learning literature that the decision rule is
fully known to the agents, and focus on settings where it is inaccessible. In
their lack of knowledge, individuals try to infer this rule by learning from
their peers (e.g., friends and acquaintances who previously applied for a
loan), naturally forming groups in the population, each with possibly different
type and level of information about the decision rule. In our equilibrium
analysis, we show that the principal's decision rule optimizing the welfare
across subgroups may cause a surprising negative externality; the true quality
of some of the subgroups can actually deteriorate. On the positive side, we
show that in many natural cases, optimal improvement is guaranteed
simultaneously for all subgroups in equilibrium. We also characterize the
disparity in improvements across subgroups via a measure of their informational
overlap. Finally, we complement our theoretical analysis with experiments on
real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bechavod_Y/0/1/0/all/0/1"&gt;Yahav Bechavod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podimata_C/0/1/0/all/0/1"&gt;Chara Podimata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziani_J/0/1/0/all/0/1"&gt;Juba Ziani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Smoothing, Mean Shift, and Their Learning Theory with Directional Data. (arXiv:2010.13523v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13523</id>
        <link href="http://arxiv.org/abs/2010.13523"/>
        <updated>2021-06-08T02:20:25.979Z</updated>
        <summary type="html"><![CDATA[Directional data consist of observations distributed on a (hyper)sphere, and
appear in many applied fields, such as astronomy, ecology, and environmental
science. This paper studies both statistical and computational problems of
kernel smoothing for directional data. We generalize the classical mean shift
algorithm to directional data, which allows us to identify local modes of the
directional kernel density estimator (KDE). The statistical convergence rates
of the directional KDE and its derivatives are derived, and the problem of mode
estimation is examined. We also prove the ascending property of the directional
mean shift algorithm and investigate a general problem of gradient ascent on
the unit hypersphere. To demonstrate the applicability of the algorithm, we
evaluate it as a mode clustering method on both simulated and real-world data
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yikun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Chi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation Matters: Assessing the Importance of Subgroup Allocations in Training Data. (arXiv:2103.03399v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03399</id>
        <link href="http://arxiv.org/abs/2103.03399"/>
        <updated>2021-06-08T02:20:25.973Z</updated>
        <summary type="html"><![CDATA[Collecting more diverse and representative training data is often touted as a
remedy for the disparate performance of machine learning predictors across
subpopulations. However, a precise framework for understanding how dataset
properties like diversity affect learning outcomes is largely lacking. By
casting data collection as part of the learning process, we demonstrate that
diverse representation in training data is key not only to increasing subgroup
performances, but also to achieving population level objectives. Our analysis
and experiments describe how dataset compositions influence performance and
provide constructive results for using trends in existing data, alongside
domain knowledge, to help guide intentional, objective-aware dataset design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rolf_E/0/1/0/all/0/1"&gt;Esther Rolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worledge_T/0/1/0/all/0/1"&gt;Theodora Worledge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1"&gt;Benjamin Recht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled Simplicial Neural Networks for Trajectory Prediction. (arXiv:2102.10058v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10058</id>
        <link href="http://arxiv.org/abs/2102.10058"/>
        <updated>2021-06-08T02:20:25.965Z</updated>
        <summary type="html"><![CDATA[We consider the construction of neural network architectures for data on
simplicial complexes. In studying maps on the chain complex of a simplicial
complex, we define three desirable properties of a simplicial neural network
architecture: namely, permutation equivariance, orientation equivariance, and
simplicial awareness. The first two properties respectively account for the
fact that the node indexing and the simplex orientations in a simplicial
complex are arbitrary. The last property encodes the desirable feature that the
output of the neural network depends on the entire simplicial complex and not
on a subset of its dimensions. Based on these properties, we propose a simple
convolutional architecture, rooted in tools from algebraic topology, for the
problem of trajectory prediction, and show that it obeys all three of these
properties when an odd, nonlinear activation function is used. We then
demonstrate the effectiveness of this architecture in extrapolating
trajectories on synthetic and real datasets, with particular emphasis on the
gains in generalizability to unseen trajectories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roddenberry_T/0/1/0/all/0/1"&gt;T. Mitchell Roddenberry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaze_N/0/1/0/all/0/1"&gt;Nicholas Glaze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1"&gt;Santiago Segarra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Model-Agnostic Post-Hoc Adjustment for Balancing Ranking Fairness and Algorithm Utility. (arXiv:2006.08267v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08267</id>
        <link href="http://arxiv.org/abs/2006.08267"/>
        <updated>2021-06-08T02:20:25.948Z</updated>
        <summary type="html"><![CDATA[Bipartite ranking, which aims to learn a scoring function that ranks positive
individuals higher than negative ones from labeled data, is widely adopted in
various applications where sample prioritization is needed. Recently, there
have been rising concerns on whether the learned scoring function can cause
systematic disparity across different protected groups defined by sensitive
attributes. While there could be trade-off between fairness and performance, in
this paper we propose a model agnostic post-processing framework for balancing
them in the bipartite ranking scenario. Specifically, we maximize a weighted
sum of the utility and fairness by directly adjusting the relative ordering of
samples across groups. By formulating this problem as the identification of an
optimal warping path across different protected groups, we propose a
non-parametric method to search for such an optimal path through a dynamic
programming process. Our method is compatible with various classification
models and applicable to a variety of ranking fairness metrics. Comprehensive
experiments on a suite of benchmark data sets and two real-world patient
electronic health record repositories show that our method can achieve a great
balance between the algorithm utility and ranking fairness. Furthermore, we
experimentally verify the robustness of our method when faced with the fewer
training samples and the difference between training and testing ranking score
distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Sen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Weishen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention. (arXiv:2008.01739v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01739</id>
        <link href="http://arxiv.org/abs/2008.01739"/>
        <updated>2021-06-08T02:20:25.941Z</updated>
        <summary type="html"><![CDATA[Natural language processing techniques have demonstrated promising results in
keyphrase generation. However, one of the major challenges in \emph{neural}
keyphrase generation is processing long documents using deep neural networks.
Generally, documents are truncated before given as inputs to neural networks.
Consequently, the models may miss essential points conveyed in the target
document. To overcome this limitation, we propose \emph{SEG-Net}, a neural
keyphrase generation model that is composed of two major components, (1) a
selector that selects the salient sentences in a document and (2) an
extractor-generator that jointly extracts and generates keyphrases from the
selected sentences. SEG-Net uses Transformer, a self-attentive architecture, as
the basic building block with a novel \emph{layer-wise} coverage attention to
summarize most of the points discussed in the document. The experimental
results on seven keyphrase generation benchmarks from scientific and web
documents demonstrate that SEG-Net outperforms the state-of-the-art neural
generative methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1"&gt;Wasi Uddin Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiao Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Soomin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Efficient Representations for Keyword Spotting with Triplet Loss. (arXiv:2101.04792v4 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04792</id>
        <link href="http://arxiv.org/abs/2101.04792"/>
        <updated>2021-06-08T02:20:25.935Z</updated>
        <summary type="html"><![CDATA[In the past few years, triplet loss-based metric embeddings have become a
de-facto standard for several important computer vision problems, most
no-tably, person reidentification. On the other hand, in the area of speech
recognition the metric embeddings generated by the triplet loss are rarely used
even for classification problems. We fill this gap showing that a combination
of two representation learning techniques: a triplet loss-based embedding and a
variant of kNN for classification instead of cross-entropy loss significantly
(by 26% to 38%) improves the classification accuracy for convolutional networks
on a LibriSpeech-derived LibriWords datasets. To do so, we propose a novel
phonetic similarity based triplet mining approach. We also improve the current
best published SOTA for Google Speech Commands dataset V1 10+2 -class
classification by about 34%, achieving 98.55% accuracy, V2 10+2-class
classification by about 20%, achieving 98.37% accuracy, and V2 35-class
classification by over 50%, achieving 97.0% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Vygon_R/0/1/0/all/0/1"&gt;Roman Vygon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mikhaylovskiy_N/0/1/0/all/0/1"&gt;Nikolay Mikhaylovskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Reconstruction: Partially Local Federated Learning. (arXiv:2102.03448v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03448</id>
        <link href="http://arxiv.org/abs/2102.03448"/>
        <updated>2021-06-08T02:20:25.930Z</updated>
        <summary type="html"><![CDATA[Personalization methods in federated learning aim to balance the benefits of
federated and local training for data availability, communication cost, and
robustness to client heterogeneity. Approaches that require clients to
communicate all model parameters can be undesirable due to privacy and
communication constraints. Other approaches require always-available or
stateful clients, impractical in large-scale cross-device settings. We
introduce Federated Reconstruction, the first model-agnostic framework for
partially local federated learning suitable for training and inference at
scale. We motivate the framework via a connection to model-agnostic meta
learning, empirically demonstrate its performance over existing approaches for
collaborative filtering and next word prediction, and release an open-source
library for evaluating approaches in this setting. We also describe the
successful deployment of this approach at scale for federated collaborative
filtering in a mobile keyboard application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1"&gt;Karan Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sidahmed_H/0/1/0/all/0/1"&gt;Hakim Sidahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1"&gt;Zachary Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shanshan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rush_K/0/1/0/all/0/1"&gt;Keith Rush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1"&gt;Sushant Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modern Hopfield Networks for Few- and Zero-Shot Reaction Template Prediction. (arXiv:2104.03279v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03279</id>
        <link href="http://arxiv.org/abs/2104.03279"/>
        <updated>2021-06-08T02:20:25.923Z</updated>
        <summary type="html"><![CDATA[Finding synthesis routes for molecules of interest is an essential step in
the discovery of new drugs and materials. To find such routes,
computer-assisted synthesis planning (CASP) methods are employed which rely on
a model of chemical reactivity. In this study, we model single-step
retrosynthesis in a template-based approach using modern Hopfield networks
(MHNs). We adapt MHNs to associate different modalities, reaction templates and
molecules, which allows the model to leverage structural information about
reaction templates. This approach significantly improves the performance of
template relevance prediction, especially for templates with few or zero
training examples. With inference speed several times faster than that of
baseline methods, we improve predictive performance for top-k exact match
accuracy for $\mathrm{k}\geq5$ in the retrosynthesis benchmark USPTO-50k.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seidl_P/0/1/0/all/0/1"&gt;Philipp Seidl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renz_P/0/1/0/all/0/1"&gt;Philipp Renz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyubankova_N/0/1/0/all/0/1"&gt;Natalia Dyubankova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neves_P/0/1/0/all/0/1"&gt;Paulo Neves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verhoeven_J/0/1/0/all/0/1"&gt;Jonas Verhoeven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segler_M/0/1/0/all/0/1"&gt;Marwin Segler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg K. Wegner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1"&gt;Sepp Hochreiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nter Klambauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization. (arXiv:2103.17182v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17182</id>
        <link href="http://arxiv.org/abs/2103.17182"/>
        <updated>2021-06-08T02:20:25.906Z</updated>
        <summary type="html"><![CDATA[It is well-known that stochastic gradient noise (SGN) acts as implicit
regularization for deep learning and is essentially important for both
optimization and generalization of deep networks. Some works attempted to
artificially simulate SGN by injecting random noise to improve deep learning.
However, it turned out that the injected simple random noise cannot work as
well as SGN, which is anisotropic and parameter-dependent. For simulating SGN
at low computational costs and without changing the learning rate or batch
size, we propose the Positive-Negative Momentum (PNM) approach that is a
powerful alternative to conventional Momentum in classic optimizers. The
introduced PNM method maintains two approximate independent momentum terms.
Then, we can control the magnitude of SGN explicitly by adjusting the momentum
difference. We theoretically prove the convergence guarantee and the
generalization advantage of PNM over Stochastic Gradient Descent (SGD). By
incorporating PNM into the two conventional optimizers, SGD with Momentum and
Adam, our extensive experiments empirically verified the significant advantage
of the PNM-based variants over the corresponding conventional Momentum-based
optimizers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zeke Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Li Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhanxing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LeadCache: Regret-Optimal Caching in Networks. (arXiv:2009.08228v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08228</id>
        <link href="http://arxiv.org/abs/2009.08228"/>
        <updated>2021-06-08T02:20:25.900Z</updated>
        <summary type="html"><![CDATA[We consider a set-valued online prediction problem in the context of network
caching. Assume that users are connected to a number of caches via a bipartite
network. At any time slot, each user requests some file chosen from a large
catalog. A user's request is met if the requested file is cached in at least
one of the caches connected to the user. The objective is to predict and
optimally store the files on the caches to maximize the total number of cache
hits. We propose $\texttt{LeadCache}$ - an online caching policy based on the
Follow-the-Perturbed-Leader paradigm. We show that the policy is regret-optimal
up to a factor of $\tilde{O}(n^{3/8}),$ where $n$ is the number of users. We
implement the policy by designing a new linear-time Pipage rounding algorithm.
With an additional Strong-Law-type assumption, we show that the total number of
file fetches under $\texttt{LeadCache}$ remains almost surely finite.
Additionally, we derive a tight regret lower bound using results from graph
coloring. Our conclusion is that the proposed learning-based caching policy
decisively outperforms the classical policies both theoretically and
empirically.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paria_D/0/1/0/all/0/1"&gt;Debjit Paria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Abhishek Sinha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling a Deep Learned Volume Formula. (arXiv:2012.03955v2 [hep-th] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03955</id>
        <link href="http://arxiv.org/abs/2012.03955"/>
        <updated>2021-06-08T02:20:25.894Z</updated>
        <summary type="html"><![CDATA[We present a simple phenomenological formula which approximates the
hyperbolic volume of a knot using only a single evaluation of its Jones
polynomial at a root of unity. The average error is just $2.86$% on the first
$1.7$ million knots, which represents a large improvement over previous
formulas of this kind. To find the approximation formula, we use layer-wise
relevance propagation to reverse engineer a black box neural network which
achieves a similar average error for the same approximation task when trained
on $10$% of the total dataset. The particular roots of unity which appear in
our analysis cannot be written as $e^{2\pi i / (k+2)}$ with integer $k$;
therefore, the relevant Jones polynomial evaluations are not given by
unknot-normalized expectation values of Wilson loop operators in conventional
$SU(2)$ Chern$\unicode{x2013}$Simons theory with level $k$. Instead, they
correspond to an analytic continuation of such expectation values to fractional
level. We briefly review the continuation procedure and comment on the presence
of certain Lefschetz thimbles, to which our approximation formula is sensitive,
in the analytically continued Chern$\unicode{x2013}$Simons integration cycle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Craven_J/0/1/0/all/0/1"&gt;Jessica Craven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Jejjala_V/0/1/0/all/0/1"&gt;Vishnu Jejjala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Kar_A/0/1/0/all/0/1"&gt;Arjun Kar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation. (arXiv:2106.02833v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02833</id>
        <link href="http://arxiv.org/abs/2106.02833"/>
        <updated>2021-06-08T02:20:25.888Z</updated>
        <summary type="html"><![CDATA[Multiple different responses are often plausible for a given open domain
dialog context. Prior work has shown the importance of having multiple valid
reference responses for meaningful and robust automated evaluations. In such
cases, common practice has been to collect more human written references.
However, such collection can be expensive, time consuming, and not easily
scalable. Instead, we propose a novel technique for automatically expanding a
human generated reference to a set of candidate references. We fetch plausible
references from knowledge sources, and adapt them so that they are more fluent
in context of the dialog instance in question. More specifically, we use (1) a
commonsense knowledge base to elicit a large number of plausible reactions
given the dialog history (2) relevant instances retrieved from dialog corpus,
using similar past as well as future contexts. We demonstrate that our
automatically expanded reference sets lead to large improvements in
correlations of automated metrics with human ratings of system outputs for
DailyDialog dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1"&gt;Varun Gangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1"&gt;Harsh Jhamtani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework to Learn with Interpretation. (arXiv:2010.09345v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09345</id>
        <link href="http://arxiv.org/abs/2010.09345"/>
        <updated>2021-06-08T02:20:25.882Z</updated>
        <summary type="html"><![CDATA[To tackle interpretability in deep learning, we present a novel framework to
jointly learn a predictive model and its associated interpretation model. The
interpreter provides both local and global interpretability about the
predictive model in terms of human-understandable high level attribute
functions, with minimal loss of accuracy. This is achieved by a dedicated
architecture and well chosen regularization penalties. We seek for a small-size
dictionary of high level attribute functions that take as inputs the outputs of
selected hidden layers and whose outputs feed a linear classifier. We impose
strong conciseness on the activation of attributes with an entropy-based
criterion while enforcing fidelity to both inputs and outputs of the predictive
model. A detailed pipeline to visualize the learnt features is also developed.
Moreover, besides generating interpretable models by design, our approach can
be specialized to provide post-hoc interpretations for a pre-trained neural
network. We validate our approach against several state-of-the-art methods on
multiple datasets and show its efficacy on both kinds of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_J/0/1/0/all/0/1"&gt;Jayneel Parekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozharovskyi_P/0/1/0/all/0/1"&gt;Pavlo Mozharovskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+dAlche_Buc_F/0/1/0/all/0/1"&gt;Florence d&amp;#x27;Alch&amp;#xe9;-Buc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Composite Optimization. (arXiv:2011.08474v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08474</id>
        <link href="http://arxiv.org/abs/2011.08474"/>
        <updated>2021-06-08T02:20:25.875Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a distributed learning paradigm that scales
on-device learning collaboratively and privately. Standard FL algorithms such
as FedAvg are primarily geared towards smooth unconstrained settings. In this
paper, we study the Federated Composite Optimization (FCO) problem, in which
the loss function contains a non-smooth regularizer. Such problems arise
naturally in FL applications that involve sparsity, low-rank, monotonicity, or
more general constraints. We first show that straightforward extensions of
primal algorithms such as FedAvg are not well-suited for FCO since they suffer
from the "curse of primal averaging," resulting in poor convergence. As a
solution, we propose a new primal-dual algorithm, Federated Dual Averaging
(FedDualAvg), which by employing a novel server dual averaging procedure
circumvents the curse of primal averaging. Our theoretical analysis and
empirical experiments demonstrate that FedDualAvg outperforms the other
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Honglin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1"&gt;Sashank Reddi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of deep learning models for multi-step ahead time series prediction. (arXiv:2103.14250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14250</id>
        <link href="http://arxiv.org/abs/2103.14250"/>
        <updated>2021-06-08T02:20:25.858Z</updated>
        <summary type="html"><![CDATA[Time series prediction with neural networks has been the focus of much
research in the past few decades. Given the recent deep learning revolution,
there has been much attention in using deep learning models for time series
prediction, and hence it is important to evaluate their strengths and
weaknesses. In this paper, we present an evaluation study that compares the
performance of deep learning models for multi-step ahead time series
prediction. The deep learning methods comprise simple recurrent neural
networks, long short-term memory (LSTM) networks, bidirectional LSTM networks,
encoder-decoder LSTM networks, and convolutional neural networks. We provide a
further comparison with simple neural networks that use stochastic gradient
descent and adaptive moment estimation (Adam) for training. We focus on
univariate time series for multi-step-ahead prediction from benchmark
time-series datasets and provide a further comparison of the results with
related methods from the literature. The results show that the bidirectional
and encoder-decoder LSTM network provides the best performance in accuracy for
the given time series problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohitash Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1"&gt;Shaurya Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Rishabh Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably End-to-end Label-Noise Learning without Anchor Points. (arXiv:2102.02400v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02400</id>
        <link href="http://arxiv.org/abs/2102.02400"/>
        <updated>2021-06-08T02:20:25.852Z</updated>
        <summary type="html"><![CDATA[In label-noise learning, the transition matrix plays a key role in building
statistically consistent classifiers. Existing consistent estimators for the
transition matrix have been developed by exploiting anchor points. However, the
anchor-point assumption is not always satisfied in real scenarios. In this
paper, we propose an end-to-end framework for solving label-noise learning
without anchor points, in which we simultaneously optimize two objectives: the
cross entropy loss between the prediction by the neural network and the given
noisy label, and the volume of the simplex formed by the columns of the
transition matrix. Our proposed framework can identify the transition matrix if
the clean class-posterior probabilities are sufficiently scattered. This is by
far the mildest assumption under which the transition matrix is provably
identifiable and the learned classifier is statistically consistent.
Experimental results on benchmark datasets demonstrate the effectiveness and
robustness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation. (arXiv:2106.02960v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02960</id>
        <link href="http://arxiv.org/abs/2106.02960"/>
        <updated>2021-06-08T02:20:25.844Z</updated>
        <summary type="html"><![CDATA[A critical challenge faced by supervised word sense disambiguation (WSD) is
the lack of large annotated datasets with sufficient coverage of words in their
diversity of senses. This inspired recent research on few-shot WSD using
meta-learning. While such work has successfully applied meta-learning to learn
new word senses from very few examples, its performance still lags behind its
fully supervised counterpart. Aiming to further close this gap, we propose a
model of semantic memory for WSD in a meta-learning setting. Semantic memory
encapsulates prior experiences seen throughout the lifetime of the model, which
aids better generalization in limited data settings. Our model is based on
hierarchical variational inference and incorporates an adaptive memory update
rule via a hypernetwork. We show our model advances the state of the art in
few-shot WSD, supports effective learning in extremely data scarce (e.g.
one-shot) scenarios and produces meaning prototypes that capture similar senses
of distinct words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yingjun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holla_N/0/1/0/all/0/1"&gt;Nithin Holla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G.M. Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1"&gt;Ekaterina Shutova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Maximum Entropy Inverse Reinforcement Learning: New Perspectives and Algorithms. (arXiv:2012.00889v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00889</id>
        <link href="http://arxiv.org/abs/2012.00889"/>
        <updated>2021-06-08T02:20:25.838Z</updated>
        <summary type="html"><![CDATA[We provide new perspectives and inference algorithms for Maximum Entropy
(MaxEnt) Inverse Reinforcement Learning (IRL), which provides a principled
method to find a most non-committal reward function consistent with given
expert demonstrations, among many consistent reward functions.

We first present a generalized MaxEnt formulation based on minimizing a
KL-divergence instead of maximizing an entropy. This improves the previous
heuristic derivation of the MaxEnt IRL model (for stochastic MDPs), allows a
unified view of MaxEnt IRL and Relative Entropy IRL, and leads to a model-free
learning algorithm for the MaxEnt IRL model. Second, a careful review of
existing inference algorithms and implementations showed that they
approximately compute the marginals required for learning the model. We provide
examples to illustrate this, and present an efficient and exact inference
algorithm. Our algorithm can handle variable length demonstrations; in
addition, while a basic version takes time quadratic in the maximum
demonstration length L, an improved version of this algorithm reduces this to
linear using a padding trick.

Experiments show that our exact algorithm improves reward learning as
compared to the approximate ones. Furthermore, our algorithm scales up to a
large, real-world dataset involving driver behaviour forecasting. We provide an
optimized implementation compatible with the OpenAI Gym interface. Our new
insight and algorithms could possibly lead to further interest and exploration
of the original MaxEnt IRL model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Snoswell_A/0/1/0/all/0/1"&gt;Aaron J. Snoswell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Surya P. N. Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1"&gt;Nan Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Versus Adversarial Euler-based Generators For Time Series. (arXiv:2102.05313v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05313</id>
        <link href="http://arxiv.org/abs/2102.05313"/>
        <updated>2021-06-08T02:20:25.820Z</updated>
        <summary type="html"><![CDATA[We introduce three new generative models for time series. Based on Euler
discretization and Wasserstein metrics, they are able to capture time marginal
distributions and temporal dynamics. Two of these methods rely on the
adaptation of generative adversarial networks (GANs) to time series. Both of
them outperform state-of-the-art benchmarks by capturing the underlying
temporal structure on synthetic time series. The third algorithm, called
Conditional Euler Generator (CEGEN), minimizes a dedicated distance between the
transition probability distributions over all time steps. In the context of Ito
processes, we provide theoretical guarantees that minimizing this criterion
implies accurate estimations of the drift and volatility parameters. We
demonstrate empirically that CEGEN outperforms state-of-the-art and GAN
generators on both marginal and temporal dynamics metrics. Besides, it
identifies accurate correlation structures in high dimension. When few data
points are available, we verify the effectiveness of CEGEN, when combined with
transfer learning methods on Monte Carlo simulations. Finally, we illustrate
the robustness of our method on various real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Remlinger_C/0/1/0/all/0/1"&gt;Carl Remlinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mikael_J/0/1/0/all/0/1"&gt;Joseph Mikael&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Elie_R/0/1/0/all/0/1"&gt;Romuald Elie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning High Dimensional Wasserstein Geodesics. (arXiv:2102.02992v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02992</id>
        <link href="http://arxiv.org/abs/2102.02992"/>
        <updated>2021-06-08T02:20:25.814Z</updated>
        <summary type="html"><![CDATA[We propose a new formulation and learning strategy for computing the
Wasserstein geodesic between two probability distributions in high dimensions.
By applying the method of Lagrange multipliers to the dynamic formulation of
the optimal transport (OT) problem, we derive a minimax problem whose saddle
point is the Wasserstein geodesic. We then parametrize the functions by deep
neural networks and design a sample based bidirectional learning algorithm for
training. The trained networks enable sampling from the Wasserstein geodesic.
As by-products, the algorithm also computes the Wasserstein distance and OT map
between the marginal distributions. We demonstrate the performance of our
algorithms through a series of experiments with both synthetic and realistic
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaojun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yongxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Haomin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Model Biases in the Absence of Ground Truth. (arXiv:2103.03417v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03417</id>
        <link href="http://arxiv.org/abs/2103.03417"/>
        <updated>2021-06-08T02:20:25.808Z</updated>
        <summary type="html"><![CDATA[The measurement of bias in machine learning often focuses on model
performance across identity subgroups (such as man and woman) with respect to
groundtruth labels. However, these methods do not directly measure the
associations that a model may have learned, for example between labels and
identity subgroups. Further, measuring a model's bias requires a fully
annotated evaluation dataset which may not be easily available in practice. We
present an elegant mathematical solution that tackles both issues
simultaneously, using image classification as a working example. By treating a
classification model's predictions for a given image as a set of labels
analogous to a bag of words, we rank the biases that a model has learned with
respect to different identity labels. We use (man, woman) as a concrete example
of an identity label set (although this set need not be binary), and present
rankings for the labels that are most biased towards one identity or the other.
We demonstrate how the statistical properties of different association metrics
can lead to different rankings of the most "gender biased" labels, and conclude
that normalized pointwise mutual information (nPMI) is most useful in practice.
Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aka_O/0/1/0/all/0/1"&gt;Osman Aka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burke_K/0/1/0/all/0/1"&gt;Ken Burke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauerle_A/0/1/0/all/0/1"&gt;Alex B&amp;#xe4;uerle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greer_C/0/1/0/all/0/1"&gt;Christina Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1"&gt;Margaret Mitchell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to distribute data across tasks for meta-learning?. (arXiv:2103.08463v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08463</id>
        <link href="http://arxiv.org/abs/2103.08463"/>
        <updated>2021-06-08T02:20:25.801Z</updated>
        <summary type="html"><![CDATA[Meta-learning models transfer the knowledge acquired from previous tasks to
quickly learn new ones. They are trained on benchmarks with a fixed number of
data points per task. This number is usually arbitrary and it is unknown how it
affects performance at testing. Since labelling of data is expensive, finding
the optimal allocation of labels across training tasks may reduce costs. Given
a fixed budget of labels, should we use a small number of highly labelled
tasks, or many tasks with few labels each? Should we allocate more labels to
some tasks and less to others? We show that: 1) If tasks are homogeneous, there
is a uniform optimal allocation, whereby all tasks get the same amount of data;
2) At fixed budget, there is a trade-off between number of tasks and number of
data points per task, with a unique and constant optimum; 3) When trained
separately, harder task should get more data, at the cost of a smaller number
of tasks; 4) When training on a mixture of easy and hard tasks, more data
should be allocated to easy tasks. Interestingly, Neuroscience experiments have
shown that human visual skills also transfer better from easy tasks. We prove
these results mathematically on mixed linear regression, and we show
empirically that the same results hold for few-shot image classification on
CIFAR-FS and mini-ImageNet. Our results provide guidance for allocating labels
across tasks when collecting data for meta-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cioba_A/0/1/0/all/0/1"&gt;Alexandru Cioba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bromberg_M/0/1/0/all/0/1"&gt;Michael Bromberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niyogi_R/0/1/0/all/0/1"&gt;Ritwik Niyogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batzolis_G/0/1/0/all/0/1"&gt;Georgios Batzolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1"&gt;Jezabel Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1"&gt;Alberto Bernacchia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PeerGAN: Generative Adversarial Networks with a Competing Peer Discriminator. (arXiv:2101.07524v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07524</id>
        <link href="http://arxiv.org/abs/2101.07524"/>
        <updated>2021-06-08T02:20:25.794Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce PeerGAN, a generative adversarial network (GAN)
solution to improve the stability of the generated samples and to mitigate mode
collapse. Built upon the Vanilla GAN's two-player game between the
discriminator $D_1$ and the generator $G$, we introduce a peer discriminator
$D_2$ to the min-max game. Similar to previous work using two discriminators,
the first role of both $D_1$, $D_2$ is to distinguish between generated samples
and real ones, while the generator tries to generate high-quality samples which
are able to fool both discriminators. Different from existing methods, we
introduce another game between $D_1$ and $D_2$ to discourage their agreement
and therefore increase the level of diversity of the generated samples. This
property alleviates the issue of early mode collapse by preventing $D_1$ and
$D_2$ from converging too fast. We provide theoretical analysis for the
equilibrium of the min-max game formed among $G, D_1, D_2$. We offer
convergence behavior of PeerGAN as well as stability of the min-max game. It's
worth mentioning that PeerGAN operates in the unsupervised setting, and the
additional game between $D_1$ and $D_2$ does not need any label supervision.
Experiments results on a synthetic dataset and on real-world image datasets
(MNIST, Fashion MNIST, CIFAR-10, STL-10, CelebA, VGG) demonstrate that PeerGAN
outperforms competitive baseline work in generating diverse and high-quality
samples, while only introduces negligible computation cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiaheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minghao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiahao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiutong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;James Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Invariant State Abstractions for Model-Based Reinforcement Learning. (arXiv:2102.09850v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09850</id>
        <link href="http://arxiv.org/abs/2102.09850"/>
        <updated>2021-06-08T02:20:25.788Z</updated>
        <summary type="html"><![CDATA[Accuracy and generalization of dynamics models is key to the success of
model-based reinforcement learning (MBRL). As the complexity of tasks
increases, so does the sample inefficiency of learning accurate dynamics
models. However, many complex tasks also exhibit sparsity in the dynamics,
i.e., actions have only a local effect on the system dynamics. In this paper,
we exploit this property with a causal invariance perspective in the
single-task setting, introducing a new type of state abstraction called
\textit{model-invariance}. Unlike previous forms of state abstractions, a
model-invariance state abstraction leverages causal sparsity over state
variables. This allows for compositional generalization to unseen states,
something that non-factored forms of state abstractions cannot do. We prove
that an optimal policy can be learned over this model-invariance state
abstraction and show improved generalization in a simple toy domain. Next, we
propose a practical method to approximately learn a model-invariant
representation for complex domains and validate our approach by showing
improved modelling performance over standard maximum likelihood approaches on
challenging tasks, such as the MuJoCo-based Humanoid. Finally, within the MBRL
setting we show strong performance gains with respect to sample efficiency
across a host of other continuous control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomar_M/0/1/0/all/0/1"&gt;Manan Tomar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Amy Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1"&gt;Roberto Calandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1"&gt;Matthew E. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resource Allocation in Multi-armed Bandit Exploration: Overcoming Sublinear Scaling with Adaptive Parallelism. (arXiv:2011.00330v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00330</id>
        <link href="http://arxiv.org/abs/2011.00330"/>
        <updated>2021-06-08T02:20:25.770Z</updated>
        <summary type="html"><![CDATA[We study exploration in stochastic multi-armed bandits when we have access to
a divisible resource that can be allocated in varying amounts to arm pulls. We
focus in particular on the allocation of distributed computing resources, where
we may obtain results faster by allocating more resources per pull, but might
have reduced throughput due to nonlinear scaling. For example, in
simulation-based scientific studies, an expensive simulation can be sped up by
running it on multiple cores. This speed-up however, is partly offset by the
communication among cores, which results in lower throughput than if fewer
cores were allocated per trial to run more trials in parallel. In this paper,
we explore these trade-offs in two settings. First, in a fixed confidence
setting, we need to find the best arm with a given target success probability
as quickly as possible. We propose an algorithm which trades off between
information accumulation and throughput and show that the time taken can be
upper bounded by the solution of a dynamic program whose inputs are the gaps
between the sub-optimal and optimal arms. We also prove a matching hardness
result. Second, we present an algorithm for a fixed deadline setting, where we
are given a time deadline and need to maximize the probability of finding the
best arm. We corroborate our theoretical insights with simulation experiments
that show that the algorithms consistently match or outperform baseline
algorithms on a variety of problem instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1"&gt;Kirthevasan Kandasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1"&gt;Ion Stoica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Witness Two-Sample Test. (arXiv:2102.05573v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05573</id>
        <link href="http://arxiv.org/abs/2102.05573"/>
        <updated>2021-06-08T02:20:25.764Z</updated>
        <summary type="html"><![CDATA[The Maximum Mean Discrepancy (MMD) has been the state-of-the-art
nonparametric test for tackling the two-sample problem. Its statistic is given
by the difference in expectations of the witness function, a real-valued
function defined as a weighted sum of kernel evaluations on a set of basis
points. Typically the kernel is optimized on a training set, and hypothesis
testing is performed on a separate test set to avoid overfitting (i.e., control
type-I error). That is, the test set is used to simultaneously estimate the
expectations and define the basis points, while the training set only serves to
select the kernel and is discarded. In this work, we argue that this data
splitting scheme is overly conservative, and propose to use the training data
to also define the weights and the basis points for better data efficiency. We
show that 1) the new test is consistent and has a well-controlled type-I error;
2) the optimal witness function is given by a precision-weighted mean in the
reproducing kernel Hilbert space associated with the kernel, and is closely
related to kernel Fisher discriminant analysis; and 3) the test power of the
proposed test is comparable or exceeds that of the MMD and other modern tests,
as verified empirically on challenging synthetic and real problems (e.g., Higgs
data).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kubler_J/0/1/0/all/0/1"&gt;Jonas M. K&amp;#xfc;bler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jitkrittum_W/0/1/0/all/0/1"&gt;Wittawat Jitkrittum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muandet_K/0/1/0/all/0/1"&gt;Krikamol Muandet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End Information Extraction. (arXiv:2103.05908v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05908</id>
        <link href="http://arxiv.org/abs/2103.05908"/>
        <updated>2021-06-08T02:20:25.757Z</updated>
        <summary type="html"><![CDATA[We address the challenge of extracting structured information from business
documents without detailed annotations. We propose Deep Conditional
Probabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional
complex documents and use Recursive Neural Networks to create an end-to-end
system for finding the most probable parse that represents the structured
information to be extracted. This system is trained end-to-end with scanned
documents as input and only relational-records as labels. The
relational-records are extracted from existing databases avoiding the cost of
annotating documents by hand. We apply this approach to extract information
from scanned invoices achieving state-of-the-art results despite using no
hand-annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chua_F/0/1/0/all/0/1"&gt;Freddy C. Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1"&gt;Nigel P. Duffy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integral Probability Metric based Regularization for Optimal Transport. (arXiv:2011.05001v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05001</id>
        <link href="http://arxiv.org/abs/2011.05001"/>
        <updated>2021-06-08T02:20:25.750Z</updated>
        <summary type="html"><![CDATA[Regularization in Optimal Transport (OT) problems has been shown to
critically affect the associated computational and sample complexities. It also
has been observed that regularization effectively helps in handling noisy
marginals as well as marginals with unequal masses. However, existing works on
OT restrict themselves to $\phi$-divergences based regularization. In this
work, we propose and analyze Integral Probability Metric (IPM) based
regularization in OT problems. While it is expected that the well-established
advantages of IPMs are inherited by the IPM-regularized OT variants, we
interestingly observe that some useful aspects of $\phi$-regularization are
preserved. For example, we show that the OT formulation, where the marginal
constraints are relaxed using IPM-regularization, also lifts the ground metric
to that over (perhaps un-normalized) measures. Infact, the lifted metric turns
out to be another IPM whose generating set is the intersection of that of the
IPM employed for regularization and the set of 1-Lipschitz functions under the
ground metric. Also, in the special case where the regularization is squared
maximum mean discrepancy based, the proposed OT variant, as well as the
corresponding Barycenter formulation, turn out to be those of minimizing a
convex quadratic subject to non-negativity/simplex constraints and hence can be
solved efficiently. Simulations confirm that the optimal transport plans/maps
obtained with IPM-regularization are intrinsically different from those
obtained with $\phi$-regularization. Empirical results illustrate the efficacy
of the proposed IPM-regularized OT formulation.

This draft contains the main paper and the Appendices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manupriya_P/0/1/0/all/0/1"&gt;Piyushi Manupriya&lt;/a&gt; (IIT Hyderabad, INDIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Nath_J/0/1/0/all/0/1"&gt;J. Saketha Nath&lt;/a&gt; (IIT Hyderabad, INDIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1"&gt;Pratik Jawanpuria&lt;/a&gt; (Microsoft IDC, INDIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thresholded Lasso Bandit. (arXiv:2010.11994v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11994</id>
        <link href="http://arxiv.org/abs/2010.11994"/>
        <updated>2021-06-08T02:20:25.725Z</updated>
        <summary type="html"><![CDATA[In this paper, we revisit the regret minimization problem in sparse
stochastic contextual linear bandits, where feature vectors may be of large
dimension $d$, but where the reward function depends on a few, say $s_0\ll d$,
of these features only. We present Thresholded Lasso bandit, an algorithm that
(i) estimates the vector defining the reward function as well as its sparse
support, i.e., significant feature elements, using the Lasso framework with
thresholding, and (ii) selects an arm greedily according to this estimate
projected on its support. The algorithm does not require prior knowledge of the
sparsity index $s_0$. For this simple algorithm, we establish non-asymptotic
regret upper bounds scaling as $\mathcal{O}( \log d + \sqrt{T} )$ in general,
and as $\mathcal{O}( \log d + \log T)$ under the so-called margin condition (a
setting where arms are well separated). The regret of previous algorithms
scales as $\mathcal{O}( \log d + \sqrt{T \log (d T)})$ and $\mathcal{O}( \log T
\log d)$ in the two settings, respectively. Through numerical experiments, we
confirm that our algorithm outperforms existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ariu_K/0/1/0/all/0/1"&gt;Kaito Ariu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Abe_K/0/1/0/all/0/1"&gt;Kenshi Abe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Proutiere_A/0/1/0/all/0/1"&gt;Alexandre Prouti&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anonymizing Machine Learning Models. (arXiv:2007.13086v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13086</id>
        <link href="http://arxiv.org/abs/2007.13086"/>
        <updated>2021-06-08T02:20:25.718Z</updated>
        <summary type="html"><![CDATA[There is a known tension between the need to analyze personal data to drive
business and privacy concerns. Many data protection regulations, including the
EU General Data Protection Regulation (GDPR) and the California Consumer
Protection Act (CCPA), set out strict restrictions and obligations on companies
that collect or process personal data. Moreover, machine learning models
themselves can be used to derive personal information, as demonstrated by
recent membership and attribute inference attacks. Anonymized data, however, is
exempt from data protection principles and obligations. Thus, models built on
anonymized data are also exempt from any privacy obligations, in addition to
providing better protection against such attacks on the training data. Learning
on anonymized data typically results in a significant degradation in accuracy.
We address this challenge by guiding our anonymization using the knowledge
encoded within the model, and targeting it to minimize the impact on the
model's accuracy, a process we call accuracy-guided anonymization. We
demonstrate that by focusing on the model's accuracy rather than information
loss, our method outperforms state of the art k-anonymity methods in terms of
the achieved utility, in particular with high values of k and large numbers of
quasi-identifiers. We also demonstrate that our approach achieves similar
results in its ability to prevent membership inference attacks as alternative
approaches based on differential privacy. This shows that model-guided
anonymization can, in some cases, be a legitimate substitute for such methods,
while averting some of their inherent drawbacks such as complexity, performance
overhead and being fitted to specific model types. As opposed to methods that
rely on adding noise during training, our approach does not rely on making any
modifications to the training algorithm itself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goldsteen_A/0/1/0/all/0/1"&gt;Abigail Goldsteen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ezov_G/0/1/0/all/0/1"&gt;Gilad Ezov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moffie_M/0/1/0/all/0/1"&gt;Micha Moffie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farkash_A/0/1/0/all/0/1"&gt;Ariel Farkash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unit Ball Model for Embedding Hierarchical Structures in the Complex Hyperbolic Space. (arXiv:2105.03966v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03966</id>
        <link href="http://arxiv.org/abs/2105.03966"/>
        <updated>2021-06-08T02:20:25.711Z</updated>
        <summary type="html"><![CDATA[Learning the representation of data with hierarchical structures in the
hyperbolic space attracts increasing attention in recent years. Due to the
constant negative curvature, the hyperbolic space resembles tree metrics and
captures the tree-like properties naturally, which enables the hyperbolic
embeddings to improve over traditional Euclidean models. However, many
real-world hierarchically structured data such as taxonomies and multitree
networks have varying local structures and they are not trees, thus they do not
ubiquitously match the constant curvature property of the hyperbolic space. To
address this limitation of hyperbolic embeddings, we explore the complex
hyperbolic space, which has the variable negative curvature, for representation
learning. Specifically, we propose to learn the embeddings of hierarchically
structured data in the unit ball model of the complex hyperbolic space. The
unit ball model based embeddings have a more powerful representation capacity
to capture a variety of hierarchical structures. Through experiments on
synthetic and real-world data, we show that our approach improves over the
hyperbolic embedding models significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Huiru Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Caigao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yangqiu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;James Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Junwu Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Embed Categorical Features without Embedding Tables for Recommendation. (arXiv:2010.10784v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10784</id>
        <link href="http://arxiv.org/abs/2010.10784"/>
        <updated>2021-06-08T02:20:25.704Z</updated>
        <summary type="html"><![CDATA[Embedding learning of categorical features (e.g. user/item IDs) is at the
core of various recommendation models including matrix factorization and neural
collaborative filtering. The standard approach creates an embedding table where
each row represents a dedicated embedding vector for every unique feature
value. However, this method fails to efficiently handle high-cardinality
features and unseen feature values (e.g. new video ID) that are prevalent in
real-world recommendation systems. In this paper, we propose an alternative
embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a
deep embedding network to compute embeddings on the fly. DHE first encodes the
feature value to a unique identifier vector with multiple hashing functions and
transformations, and then applies a DNN to convert the identifier vector to an
embedding. The encoding module is deterministic, non-learnable, and free of
storage, while the embedding network is updated during the training time to
learn embedding generation. Empirical results show that DHE achieves comparable
AUC against the standard one-hot full embedding, with smaller model sizes. Our
work sheds light on the design of DNN-based alternative embedding schemes for
categorical features without using embedding table lookup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Wang-Cheng Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1"&gt;Derek Zhiyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Tiansheng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinyang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lichan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning. (arXiv:2007.11622v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11622</id>
        <link href="http://arxiv.org/abs/2007.11622"/>
        <updated>2021-06-08T02:20:25.698Z</updated>
        <summary type="html"><![CDATA[On-device learning enables edge devices to continually adapt the AI models to
new data, which requires a small memory footprint to fit the tight memory
constraint of edge devices. Existing work solves this problem by reducing the
number of trainable parameters. However, this doesn't directly translate to
memory saving since the major bottleneck is the activations, not parameters. In
this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient
on-device learning. TinyTL freezes the weights while only learns the bias
modules, thus no need to store the intermediate activations. To maintain the
adaptation capacity, we introduce a new memory-efficient bias module, the lite
residual module, to refine the feature extractor by learning small residual
feature maps adding only 3.8% memory overhead. Extensive experiments show that
TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss
compared to fine-tuning the full network. Compared to fine-tuning the last
layer, TinyTL provides significant accuracy improvements (up to 34.1%) with
little memory overhead. Furthermore, combined with feature extractor
adaptation, TinyTL provides 7.3-12.9x memory saving without sacrificing
accuracy compared to fine-tuning the full Inception-V3.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Han Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Ligeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Open-Ended Commonsense Reasoning. (arXiv:2010.14439v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14439</id>
        <link href="http://arxiv.org/abs/2010.14439"/>
        <updated>2021-06-08T02:20:25.690Z</updated>
        <summary type="html"><![CDATA[Current commonsense reasoning research focuses on developing models that use
commonsense knowledge to answer multiple-choice questions. However, systems
designed to answer multiple-choice questions may not be useful in applications
that do not provide a small list of candidate answers to choose from. As a step
towards making commonsense reasoning research more realistic, we propose to
study open-ended commonsense reasoning (OpenCSR) -- the task of answering a
commonsense question without any pre-defined choices -- using as a resource
only a corpus of commonsense facts written in natural language. OpenCSR is
challenging due to a large decision space, and because many questions require
implicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an
efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To
evaluate OpenCSR methods, we adapt several popular commonsense reasoning
benchmarks, and collect multiple new answers for each test question via
crowd-sourcing. Experiments show that DrFact outperforms strong baseline
methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Declarative Approaches to Counterfactual Explanations for Classification. (arXiv:2011.07423v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07423</id>
        <link href="http://arxiv.org/abs/2011.07423"/>
        <updated>2021-06-08T02:20:25.684Z</updated>
        <summary type="html"><![CDATA[We propose answer-set programs that specify and compute counterfactual
interventions on entities that are input on a classification model. In relation
to the outcome of the model, the resulting counterfactual entities serve as a
basis for the definition and computation of causality-based explanation scores
for the feature values in the entity under classification, namely
"responsibility scores". The approach and the programs can be applied with
black-box models, and also with models that can be specified as logic programs,
such as rule-based classifiers. The main focus of this work is on the
specification and computation of "best" counterfactual entities, i.e. those
that lead to maximum responsibility scores. From them one can read off the
explanations as maximum responsibility feature values in the original entity.
We also extend the programs to bring into the picture semantic or domain
knowledge. We show how the approach could be extended by means of probabilistic
methods, and how the underlying probability distributions could be modified
through the use of constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1"&gt;Leopoldo Bertossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Performance of Knowledge Graph Embeddings in Drug Discovery. (arXiv:2105.10488v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10488</id>
        <link href="http://arxiv.org/abs/2105.10488"/>
        <updated>2021-06-08T02:20:25.677Z</updated>
        <summary type="html"><![CDATA[Knowledge Graphs (KG) and associated Knowledge Graph Embedding (KGE) models
have recently begun to be explored in the context of drug discovery and have
the potential to assist in key challenges such as target identification. In the
drug discovery domain, KGs can be employed as part of a process which can
result in lab-based experiments being performed, or impact on other decisions,
incurring significant time and financial costs and most importantly, ultimately
influencing patient healthcare. For KGE models to have impact in this domain, a
better understanding of not only of performance, but also the various factors
which determine it, is required.

In this study we investigate, over the course of many thousands of
experiments, the predictive performance of five KGE models on two public drug
discovery-oriented KGs. Our goal is not to focus on the best overall model or
configuration, instead we take a deeper look at how performance can be affected
by changes in the training setup, choice of hyperparameters, model parameter
initialisation seed and different splits of the datasets. Our results highlight
that these factors have significant impact on performance and can even affect
the ranking of models. Indeed these factors should be reported along with model
architectures to ensure complete reproducibility and fair comparisons of future
work, and we argue this is critical for the acceptance of use, and impact of
KGEs in a biomedical setting. To aid reproducibility of our own work, we
release all experimentation code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1"&gt;Stephen Bonner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1"&gt;Ian P Barrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1"&gt;Cheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1"&gt;Rowan Swiers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Engkvist_O/0/1/0/all/0/1"&gt;Ola Engkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hoyt_C/0/1/0/all/0/1"&gt;Charles Tapley Hoyt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring the originality of intellectual property assets based on machine learning outputs. (arXiv:2010.06997v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06997</id>
        <link href="http://arxiv.org/abs/2010.06997"/>
        <updated>2021-06-08T02:20:25.623Z</updated>
        <summary type="html"><![CDATA[Originality criteria are frequently used to compare assets and, in
particular, to assess the validity of intellectual property (IP) rights such as
copyright and design rights. In this work, the originality of an asset is
formulated as a function of the distances between this asset and its
comparands, using concepts of maximum entropy and surprisal analysis. Namely,
the originality function is defined according to the surprisal associated with
a given asset. Creative assets can be justifiably compared to particles that
repel each other via an electrostatic-like pair potential. This allows a very
simple, suitably bounded formula to be obtained, in which the originality of an
asset writes as the ratio of a reference energy to an interaction energy
imparted to that asset. In particular, the originality of an asset can be
expressed as a ratio of two average distances, i.e., the harmonic mean of the
distances from this asset to its comparands divided by the harmonic mean of the
distances between the sole comparands. Accordingly, the originality of objects
such as IP assets can be simply estimated based on distances computed thanks to
unsupervised machine learning techniques or other distance computation
algorithms. Application is made to various types of assets, including emojis,
typeface designs, paintings, and novel titles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ragot_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ragot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UNiTE: Unitary N-body Tensor Equivariant Network with Applications to Quantum Chemistry. (arXiv:2105.14655v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14655</id>
        <link href="http://arxiv.org/abs/2105.14655"/>
        <updated>2021-06-08T02:20:25.610Z</updated>
        <summary type="html"><![CDATA[Equivariant neural networks have been successful in incorporating various
types of symmetries, but are mostly limited to vector representations of
geometric objects. Despite the prevalence of higher-order tensors in various
application domains, e.g. in quantum chemistry, equivariant neural networks for
general tensors remain unexplored. Previous strategies for learning equivariant
functions on tensors mostly rely on expensive tensor factorization which is not
scalable when the dimensionality of the problem becomes large. In this work, we
propose unitary $N$-body tensor equivariant neural network (UNiTE), an
architecture for a general class of symmetric tensors called $N$-body tensors.
The proposed neural network is equivariant with respect to the actions of a
unitary group, such as the group of 3D rotations. Furthermore, it has a linear
time complexity with respect to the number of non-zero elements in the tensor.
We also introduce a normalization method, viz., Equivariant Normalization, to
improve generalization of the neural network while preserving symmetry. When
applied to quantum chemistry, UNiTE outperforms all state-of-the-art machine
learning methods of that domain with over 110% average improvements on multiple
benchmarks. Finally, we show that UNiTE achieves a robust zero-shot
generalization performance on diverse down stream chemistry tasks, while being
three orders of magnitude faster than conventional numerical methods with
competitive accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhuoran Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christensen_A/0/1/0/all/0/1"&gt;Anders S. Christensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welborn_M/0/1/0/all/0/1"&gt;Matthew Welborn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manby_F/0/1/0/all/0/1"&gt;Frederick R. Manby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1"&gt;Thomas F. Miller III&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrepancy-Based Active Learning for Domain Adaptation. (arXiv:2103.03757v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03757</id>
        <link href="http://arxiv.org/abs/2103.03757"/>
        <updated>2021-06-08T02:20:25.600Z</updated>
        <summary type="html"><![CDATA[The goal of the paper is to design active learning strategies which lead to
domain adaptation under an assumption of covariate shift in the case of
Lipschitz labeling function. Building on previous work by Mansour et al. (2009)
we adapt the concept of discrepancy distance between source and target
distributions to restrict the maximization over the hypothesis class to a
localized class of functions which are performing accurate labeling on the
source domain. We derive generalization error bounds for such active learning
strategies in terms of Rademacher average and localized discrepancy for general
loss functions which satisfy a regularity condition. A practical K-medoids
algorithm that can address the case of large data set is inferred from the
theoretical bounds. Our numerical experiments show that the proposed algorithm
is competitive against other state-of-the-art active learning techniques in the
context of domain adaptation, in particular on large data sets of around one
hundred thousand images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathelin_A/0/1/0/all/0/1"&gt;Antoine de Mathelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deheeger_F/0/1/0/all/0/1"&gt;Francois Deheeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mougeot_M/0/1/0/all/0/1"&gt;Mathilde Mougeot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vayatis_N/0/1/0/all/0/1"&gt;Nicolas Vayatis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining Deep Generative Models via Discriminator Gradient Flow. (arXiv:2012.00780v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00780</id>
        <link href="http://arxiv.org/abs/2012.00780"/>
        <updated>2021-06-08T02:20:25.585Z</updated>
        <summary type="html"><![CDATA[Deep generative modeling has seen impressive advances in recent years, to the
point where it is now commonplace to see simulated samples (e.g., images) that
closely resemble real-world data. However, generation quality is generally
inconsistent for any given model and can vary dramatically between samples. We
introduce Discriminator Gradient flow (DGflow), a new technique that improves
generated samples via the gradient flow of entropy-regularized f-divergences
between the real and the generated data distributions. The gradient flow takes
the form of a non-linear Fokker-Plank equation, which can be easily simulated
by sampling from the equivalent McKean-Vlasov process. By refining inferior
samples, our technique avoids wasteful sample rejection used by previous
methods (DRS & MH-GAN). Compared to existing works that focus on specific GAN
variants, we show our refinement approach can be applied to GANs with
vector-valued critics and even other deep generative models such as VAEs and
Normalizing Flows. Empirical results on multiple synthetic, image, and text
datasets demonstrate that DGflow leads to significant improvement in the
quality of generated samples for a variety of generative models, outperforming
the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator
Driven Latent Sampling (DDLS) methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ansari_A/0/1/0/all/0/1"&gt;Abdul Fatir Ansari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Ming Liang Ang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1"&gt;Harold Soh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neograd: Near-Ideal Gradient Descent. (arXiv:2010.07873v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07873</id>
        <link href="http://arxiv.org/abs/2010.07873"/>
        <updated>2021-06-08T02:20:25.572Z</updated>
        <summary type="html"><![CDATA[The purpose of this paper is to improve upon existing variants of gradient
descent by solving two problems: (1) removing (or reducing) the plateau that
occurs while minimizing the cost function,(2) continually adjusting the
learning rate to an "ideal" value. The approach taken is to approximately solve
for the learning rate as a function of a trust metric. When this technique is
hybridized with momentum, it creates an especially effective gradient descent
variant, called NeogradM. It is shown to outperform Adam on several test
problems, and can easily reach cost function values that are smaller by a
factor of $10^8$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1"&gt;Michael F. Zimmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matching in Selective and Balanced Representation Space for Treatment Effects Estimation. (arXiv:2009.06828v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06828</id>
        <link href="http://arxiv.org/abs/2009.06828"/>
        <updated>2021-06-08T02:20:25.555Z</updated>
        <summary type="html"><![CDATA[The dramatically growing availability of observational data is being
witnessed in various domains of science and technology, which facilitates the
study of causal inference. However, estimating treatment effects from
observational data is faced with two major challenges, missing counterfactual
outcomes and treatment selection bias. Matching methods are among the most
widely used and fundamental approaches to estimating treatment effects, but
existing matching methods have poor performance when facing data with high
dimensional and complicated variables. We propose a feature selection
representation matching (FSRM) method based on deep representation learning and
matching, which maps the original covariate space into a selective, nonlinear,
and balanced representation space, and then conducts matching in the learned
representation space. FSRM adopts deep feature selection to minimize the
influence of irrelevant variables for estimating treatment effects and
incorporates a regularizer based on the Wasserstein distance to learn balanced
representations. We evaluate the performance of our FSRM method on three
datasets, and the results demonstrate superiority over the state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rathbun_S/0/1/0/all/0/1"&gt;Stephen L. Rathbun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Negative Samples in Instance Discriminative Self-supervised Representation Learning. (arXiv:2102.06866v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06866</id>
        <link href="http://arxiv.org/abs/2102.06866"/>
        <updated>2021-06-08T02:20:25.537Z</updated>
        <summary type="html"><![CDATA[Instance discriminative self-supervised representation learning has been
attracted attention thanks to its unsupervised nature and informative feature
representation for downstream tasks. In practice, it commonly uses a larger
number of negative samples than the number of supervised classes. However,
there is an inconsistency in the existing analysis; theoretically, a large
number of negative samples degrade classification performance on a downstream
supervised task, while empirically, they improve the performance. We provide a
novel framework to analyze this empirical result regarding negative samples
using the coupon collector's problem. Our bound can implicitly incorporate the
supervised loss of the downstream task in the self-supervised loss by
increasing the number of negative samples. We confirm that our proposed
analysis holds on real-world benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nozawa_K/0/1/0/all/0/1"&gt;Kento Nozawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1"&gt;Issei Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming the curse of dimensionality with Laplacian regularization in semi-supervised learning. (arXiv:2009.04324v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04324</id>
        <link href="http://arxiv.org/abs/2009.04324"/>
        <updated>2021-06-08T02:20:25.531Z</updated>
        <summary type="html"><![CDATA[As annotations of data can be scarce in large-scale practical problems,
leveraging unlabelled examples is one of the most important aspects of machine
learning. This is the aim of semi-supervised learning. To benefit from the
access to unlabelled data, it is natural to diffuse smoothly knowledge of
labelled data to unlabelled one. This induces to the use of Laplacian
regularization. Yet, current implementations of Laplacian regularization suffer
from several drawbacks, notably the well-known curse of dimensionality. In this
paper, we provide a statistical analysis to overcome those issues, and unveil a
large body of spectral filtering methods that exhibit desirable behaviors. They
are implemented through (reproducing) kernel methods, for which we provide
realistic computational guidelines in order to make our method usable with
large amounts of data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1"&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pillaud_Vivien_L/0/1/0/all/0/1"&gt;Loucas Pillaud-Vivien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deconditional Downscaling with Gaussian Processes. (arXiv:2105.12909v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12909</id>
        <link href="http://arxiv.org/abs/2105.12909"/>
        <updated>2021-06-08T02:20:25.525Z</updated>
        <summary type="html"><![CDATA[Refining low-resolution (LR) spatial fields with high-resolution (HR)
information is challenging as the diversity of spatial datasets often prevents
direct matching of observations. Yet, when LR samples are modeled as aggregate
conditional means of HR samples with respect to a mediating variable that is
globally observed, the recovery of the underlying fine-grained field can be
framed as taking an "inverse" of the conditional expectation, namely a
deconditioning problem. In this work, we introduce conditional mean processes
(CMP), a new class of Gaussian Processes describing conditional means. By
treating CMPs as inter-domain features of the underlying field, a posterior for
the latent field can be established as a solution to the deconditioning
problem. Furthermore, we show that this solution can be viewed as a two-staged
vector-valued kernel ridge regressor and show that it has a minimax optimal
convergence rate under mild assumptions. Lastly, we demonstrate its proficiency
in a synthetic and a real-world atmospheric field downscaling problem, showing
substantial improvements over existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chau_S/0/1/0/all/0/1"&gt;Siu Lun Chau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouabid_S/0/1/0/all/0/1"&gt;Shahine Bouabid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sejdinovic_D/0/1/0/all/0/1"&gt;Dino Sejdinovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning by Self-Transition for Handling Noisy Labels. (arXiv:2012.04337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04337</id>
        <link href="http://arxiv.org/abs/2012.04337"/>
        <updated>2021-06-08T02:20:25.519Z</updated>
        <summary type="html"><![CDATA[Real-world data inevitably contains noisy labels, which induce the poor
generalization of deep neural networks. It is known that the network typically
begins to rapidly memorize false-labeled samples after a certain point of
training. Thus, to counter the label noise challenge, we propose a novel
self-transitional learning method called MORPH, which automatically switches
its learning phase at the transition point from seeding to evolution. In the
seeding phase, the network is updated using all the samples to collect a seed
of clean samples. Then, in the evolution phase, the network is updated using
only the set of arguably clean samples, which precisely keeps expanding by the
updated network. Thus, MORPH effectively avoids the overfitting to
false-labeled samples throughout the entire training period. Extensive
experiments using five real-world or synthetic benchmark datasets demonstrate
substantial improvements over state-of-the-art methods in terms of robustness
and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hwanjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yooju Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae-Gil Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06419</id>
        <link href="http://arxiv.org/abs/2103.06419"/>
        <updated>2021-06-08T02:20:25.512Z</updated>
        <summary type="html"><![CDATA[Background and objective: In this paper, a modified U-Net based framework is
presented, which leverages techniques from Squeeze-and-Excitation (SE) block,
Atrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and
robust liver CT segmentation, and the effectiveness of the proposed method was
tested on two public datasets LiTS17 and SLiver07.

Methods: A new network architecture called SAR-U-Net was designed. Firstly,
the SE block is introduced to adaptively extract image features after each
convolution in the U-Net encoder, while suppressing irrelevant regions, and
highlighting features of specific segmentation task; Secondly, ASPP was
employed to replace the transition layer and the output layer, and acquire
multi-scale image information via different receptive fields. Thirdly, to
alleviate the degradation problem, the traditional convolution block was
replaced with the residual block and thus prompt the network to gain accuracy
from considerably increased depth.

Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and
MSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other
closely related 2D-based models, the proposed method achieved the highest
accuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,
ASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared
with other closely related models, the proposed method achieved the highest
segmentation accuracy except for the RVD.

Conclusion: The proposed model enables a great improvement on the accuracy
compared to 2D-based models, and its robustness in circumvent challenging
problems, such as small liver regions, discontinuous liver regions, and fuzzy
liver boundaries, is also well demonstrated and validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1"&gt;Peiqing Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Approaches for Binary Classification to Discover Liver Diseases using Clinical Data. (arXiv:2104.12055v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12055</id>
        <link href="http://arxiv.org/abs/2104.12055"/>
        <updated>2021-06-08T02:20:25.495Z</updated>
        <summary type="html"><![CDATA[For a medical diagnosis, health professionals use different kinds of
pathological ways to make a decision for medical reports in terms of patients
medical condition. In the modern era, because of the advantage of computers and
technologies, one can collect data and visualize many hidden outcomes from
them. Statistical machine learning algorithms based on specific problems can
assist one to make decisions. Machine learning data driven algorithms can be
used to validate existing methods and help researchers to suggest potential new
decisions. In this paper, multiple imputation by chained equations was applied
to deal with missing data, and Principal Component Analysis to reduce the
dimensionality. To reveal significant findings, data visualizations were
implemented. We presented and compared many binary classifier machine learning
algorithms (Artificial Neural Network, Random Forest, Support Vector Machine)
which were used to classify blood donors and non-blood donors with hepatitis,
fibrosis and cirrhosis diseases. From the data published in UCI-MLR [1], all
mentioned techniques were applied to find one better method to classify blood
donors and non-blood donors (hepatitis, fibrosis, and cirrhosis) that can help
health professionals in a laboratory to make better decisions. Our proposed
ML-method showed better accuracy score (e.g. 98.23% for SVM). Thus, it improved
the quality of classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mostafa_F/0/1/0/all/0/1"&gt;Fahad B. Mostafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Md Easin Hasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Learning with 1D Convolutions on Random Walks. (arXiv:2102.08786v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08786</id>
        <link href="http://arxiv.org/abs/2102.08786"/>
        <updated>2021-06-08T02:20:25.488Z</updated>
        <summary type="html"><![CDATA[We propose CRaWl (CNNs for Random Walks), a novel neural network architecture
for graph learning. It is based on processing sequences of small subgraphs
induced by random walks with standard 1D CNNs. Thus, CRaWl is fundamentally
different from typical message passing graph neural network architectures. It
is inspired by techniques counting small subgraphs, such as the graphlet kernel
and motif counting, and combines them with random walk based techniques in a
highly efficient and scalable neural architecture. We demonstrate empirically
that CRaWl matches or outperforms state-of-the-art GNN architectures across a
multitude of benchmark datasets for classification and regression on graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toenshoff_J/0/1/0/all/0/1"&gt;Jan Toenshoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritzert_M/0/1/0/all/0/1"&gt;Martin Ritzert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_H/0/1/0/all/0/1"&gt;Hinrikus Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1"&gt;Martin Grohe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations. (arXiv:2106.02974v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02974</id>
        <link href="http://arxiv.org/abs/2106.02974"/>
        <updated>2021-06-08T02:20:25.482Z</updated>
        <summary type="html"><![CDATA[Automatic construction of a taxonomy supports many applications in
e-commerce, web search, and question answering. Existing taxonomy expansion or
completion methods assume that new concepts have been accurately extracted and
their embedding vectors learned from the text corpus. However, one critical and
fundamental challenge in fixing the incompleteness of taxonomies is the
incompleteness of the extracted concepts, especially for those whose names have
multiple words and consequently low frequency in the corpus. To resolve the
limitations of extraction-based methods, we propose GenTaxo to enhance taxonomy
completion by identifying positions in existing taxonomies that need new
concepts and then generating appropriate concept names. Instead of relying on
the corpus for concept embeddings, GenTaxo learns the contextual embeddings
from their surrounding graph-based and language-based relational information,
and leverages the corpus for pre-training a concept name generator.
Experimental results demonstrate that GenTaxo improves the completeness of
taxonomies over existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1"&gt;Qingkai Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jinfeng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenhao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cleland_Huang_J/0/1/0/all/0/1"&gt;Jane Cleland-Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Sample Complexity of Stability Constrained Imitation Learning. (arXiv:2102.09161v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09161</id>
        <link href="http://arxiv.org/abs/2102.09161"/>
        <updated>2021-06-08T02:20:25.476Z</updated>
        <summary type="html"><![CDATA[We study the following question in the context of imitation learning for
continuous control: how are the underlying stability properties of an expert
policy reflected in the sample-complexity of an imitation learning task? We
provide the first results showing that a surprisingly granular connection can
be made between the underlying expert system's incremental gain stability, a
novel measure of robust convergence between pairs of system trajectories, and
the dependency on the task horizon $T$ of the resulting generalization bounds.
In particular, we propose and analyze incremental gain stability constrained
versions of behavior cloning and a DAgger-like algorithm, and show that the
resulting sample-complexity bounds naturally reflect the underlying stability
properties of the expert system. As a special case, we delineate a class of
systems for which the number of trajectories needed to achieve
$\varepsilon$-suboptimality is sublinear in the task horizon $T$, and do so
without requiring (strong) convexity of the loss function in the policy
parameters. Finally, we conduct numerical experiments demonstrating the
validity of our insights on both a simple nonlinear system for which the
underlying stability properties can be easily tuned, and on a high-dimensional
quadrupedal robotic simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1"&gt;Stephen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1"&gt;Alexander Robey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tingnan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matni_N/0/1/0/all/0/1"&gt;Nikolai Matni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strength of Minibatch Noise in SGD. (arXiv:2102.05375v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05375</id>
        <link href="http://arxiv.org/abs/2102.05375"/>
        <updated>2021-06-08T02:20:25.460Z</updated>
        <summary type="html"><![CDATA[The noise in stochastic gradient descent (SGD), caused by minibatch sampling,
is poorly understood despite its practical importance in deep learning. In this
work, we study the nature of SGD noise and fluctuation. We show that some
degree of mismatch between model and data complexity is needed for SGD to
``stir" a noise; such mismatch may be due to a label or input noise,
regularization, or underparametrization. Compared with previous works, the
present work focuses on deriving exactly solvable analytical results. Our work
also motivates a more accurate general formulation to describe minibatch noise,
and we show that the SGD noise takes different shapes and strengths in
different kinds of minima.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1"&gt;Liu Ziyin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1"&gt;Takashi Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masahito Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact Distribution-Free Hypothesis Tests for the Regression Function of Binary Classification via Conditional Kernel Mean Embeddings. (arXiv:2103.05126v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05126</id>
        <link href="http://arxiv.org/abs/2103.05126"/>
        <updated>2021-06-08T02:20:25.366Z</updated>
        <summary type="html"><![CDATA[In this paper we suggest two statistical hypothesis tests for the regression
function of binary classification based on conditional kernel mean embeddings.
The regression function is a fundamental object in classification as it
determines both the Bayes optimal classifier and the misclassification
probabilities. A resampling based framework is presented and combined with
consistent point estimators of the conditional kernel mean map, in order to
construct distribution-free hypothesis tests. These tests are introduced in a
flexible manner allowing us to control the exact probability of type I error
for any sample size. We also prove that both proposed techniques are consistent
under weak statistical assumptions, i.e., the type II error probabilities
pointwise converge to zero.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tamas_A/0/1/0/all/0/1"&gt;Ambrus Tam&amp;#xe1;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Csaji_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs Csan&amp;#xe1;d Cs&amp;#xe1;ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning. (arXiv:2102.13515v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13515</id>
        <link href="http://arxiv.org/abs/2102.13515"/>
        <updated>2021-06-08T02:20:25.365Z</updated>
        <summary type="html"><![CDATA[Designing agents that acquire knowledge autonomously and use it to solve new
tasks efficiently is an important challenge in reinforcement learning.
Knowledge acquired during an unsupervised pre-training phase is often
transferred by fine-tuning neural network weights once rewards are exposed, as
is common practice in supervised domains. Given the nature of the reinforcement
learning problem, we argue that standard fine-tuning strategies alone are not
enough for efficient transfer in challenging domains. We introduce Behavior
Transfer (BT), a technique that leverages pre-trained policies for exploration
and that is complementary to transferring neural network weights. Our
experiments show that, when combined with large-scale pre-training in the
absence of rewards, existing intrinsic motivation objectives can lead to the
emergence of complex behaviors. These pre-trained policies can then be
leveraged by BT to discover better solutions than without pre-training, and
combining BT with standard fine-tuning strategies results in additional
benefits. The largest gains are generally observed in domains requiring
structured exploration, including settings where the behavior of the
pre-trained policies is misaligned with the downstream task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Campos_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sprechmann_P/0/1/0/all/0/1"&gt;Pablo Sprechmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1"&gt;Steven Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1"&gt;Andre Barreto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapturowski_S/0/1/0/all/0/1"&gt;Steven Kapturowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vitvitskyi_A/0/1/0/all/0/1"&gt;Alex Vitvitskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badia_A/0/1/0/all/0/1"&gt;Adri&amp;#xe0; Puigdom&amp;#xe8;nech Badia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1"&gt;Charles Blundell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End Information Extraction. (arXiv:2103.05908v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05908</id>
        <link href="http://arxiv.org/abs/2103.05908"/>
        <updated>2021-06-08T02:20:25.295Z</updated>
        <summary type="html"><![CDATA[We address the challenge of extracting structured information from business
documents without detailed annotations. We propose Deep Conditional
Probabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional
complex documents and use Recursive Neural Networks to create an end-to-end
system for finding the most probable parse that represents the structured
information to be extracted. This system is trained end-to-end with scanned
documents as input and only relational-records as labels. The
relational-records are extracted from existing databases avoiding the cost of
annotating documents by hand. We apply this approach to extract information
from scanned invoices achieving state-of-the-art results despite using no
hand-annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chua_F/0/1/0/all/0/1"&gt;Freddy C. Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1"&gt;Nigel P. Duffy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Inference for Sparse Extreme Multi-Label Ranking Trees. (arXiv:2106.02697v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02697</id>
        <link href="http://arxiv.org/abs/2106.02697"/>
        <updated>2021-06-08T02:20:25.295Z</updated>
        <summary type="html"><![CDATA[Tree-based models underpin many modern semantic search engines and
recommender systems due to their sub-linear inference times. In industrial
applications, these models operate at extreme scales, where every bit of
performance is critical. Memory constraints at extreme scales also require that
models be sparse, hence tree-based models are often back-ended by sparse matrix
algebra routines. However, there are currently no sparse matrix techniques
specifically designed for the sparsity structure one encounters in tree-based
models for extreme multi-label ranking/classification (XMR/XMC) problems. To
address this issue, we present the masked sparse chunk multiplication (MSCM)
technique, a sparse matrix technique specifically tailored to XMR trees. MSCM
is easy to implement, embarrassingly parallelizable, and offers a significant
performance boost to any existing tree inference pipeline at no cost. We
perform a comprehensive study of MSCM applied to several different sparse
inference schemes and benchmark our methods on a general purpose extreme
multi-label ranking framework. We observe that MSCM gives consistently dramatic
speedups across both the online and batch inference settings, single- and
multi-threaded settings, and on many different tree models and datasets. To
demonstrate its utility in industrial applications, we apply MSCM to an
enterprise-scale semantic product search problem with 100 million products and
achieve sub-millisecond latency of 0.88 ms per query on a single thread -- an
8x reduction in latency over vanilla inference techniques. The MSCM technique
requires absolutely no sacrifices to model accuracy as it gives exactly the
same results as standard sparse matrix techniques. Therefore, we believe that
MSCM will enable users of XMR trees to save a substantial amount of compute
resources in their inference pipelines at very little cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Etter_P/0/1/0/all/0/1"&gt;Philip A. Etter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kai Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1"&gt;Lexing Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethink the Connections among Generalization, Memorization and the Spectral Bias of DNNs. (arXiv:2004.13954v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13954</id>
        <link href="http://arxiv.org/abs/2004.13954"/>
        <updated>2021-06-08T02:20:25.295Z</updated>
        <summary type="html"><![CDATA[Over-parameterized deep neural networks (DNNs) with sufficient capacity to
memorize random noise can achieve excellent generalization performance,
challenging the bias-variance trade-off in classical learning theory. Recent
studies claimed that DNNs first learn simple patterns and then memorize noise;
some other works showed a phenomenon that DNNs have a spectral bias to learn
target functions from low to high frequencies during training. However, we show
that the monotonicity of the learning bias does not always hold: under the
experimental setup of deep double descent, the high-frequency components of
DNNs diminish in the late stage of training, leading to the second descent of
the test error. Besides, we find that the spectrum of DNNs can be applied to
indicating the second descent of the test error, even though it is calculated
from the training set only.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Distributed Source Coding. (arXiv:2106.02797v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02797</id>
        <link href="http://arxiv.org/abs/2106.02797"/>
        <updated>2021-06-08T02:20:25.294Z</updated>
        <summary type="html"><![CDATA[Distributed source coding is the task of encoding an input in the absence of
correlated side information that is only available to the decoder. Remarkably,
Slepian and Wolf showed in 1973 that an encoder that has no access to the
correlated side information can asymptotically achieve the same compression
rate as when the side information is available at both the encoder and the
decoder. While there is significant prior work on this topic in information
theory, practical distributed source coding has been limited to synthetic
datasets and specific correlation structures. Here we present a general
framework for lossy distributed source coding that is agnostic to the
correlation structure and can scale to high dimensions. Rather than relying on
hand-crafted source-modeling, our method utilizes a powerful conditional deep
generative model to learn the distributed encoder and decoder. We evaluate our
method on realistic high-dimensional datasets and show substantial improvements
in distributed compression performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1"&gt;Jay Whang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Anish Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyeji Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selective Inference for Latent Block Models. (arXiv:2005.13273v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.13273</id>
        <link href="http://arxiv.org/abs/2005.13273"/>
        <updated>2021-06-08T02:20:25.294Z</updated>
        <summary type="html"><![CDATA[Model selection in latent block models has been a challenging but important
task in the field of statistics. Specifically, a major challenge is encountered
when constructing a test on a block structure obtained by applying a specific
clustering algorithm to a finite size matrix. In this case, it becomes crucial
to consider the selective bias in the block structure, that is, the block
structure is selected from all the possible cluster memberships based on some
criterion by the clustering algorithm. To cope with this problem, this study
provides a selective inference method for latent block models. Specifically, we
construct a statistical test on a set of row and column cluster memberships of
a latent block model, which is given by a squared residue minimization
algorithm. The proposed test, by its nature, includes and thus can also be used
as the test on the set of row and column cluster numbers. We also propose an
approximated version of the test based on simulated annealing to avoid
combinatorial explosion in searching the optimal block structure. The results
show that the proposed exact and approximated tests work effectively, compared
to the naive test that did not take the selective bias into account.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1"&gt;Chihiro Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Web based disease prediction and recommender system. (arXiv:2106.02813v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02813</id>
        <link href="http://arxiv.org/abs/2106.02813"/>
        <updated>2021-06-08T02:20:25.285Z</updated>
        <summary type="html"><![CDATA[Worldwide, several cases go undiagnosed due to poor healthcare support in
remote areas. In this context, a centralized system is needed for effective
monitoring and analysis of the medical records. A web-based patient diagnostic
system is a central platform to store the medical history and predict the
possible disease based on the current symptoms experienced by a patient to
ensure faster and accurate diagnosis. Early disease prediction can help the
users determine the severity of the disease and take quick action. The proposed
web-based disease prediction system utilizes machine learning based
classification techniques on a data set acquired from the National Centre of
Disease Control (NCDC). $K$-nearest neighbor (K-NN), random forest and naive
bayes classification approaches are utilized and an ensemble voting algorithm
is also proposed where each classifier is assigned weights dynamically based on
the prediction confidence. The proposed system is also equipped with a
recommendation scheme to recommend the type of tests based on the existing
symptoms of the patient, so that necessary precautions can be taken. A
centralized database ensures that the medical data is preserved and there is
transparency in the system. The tampering into the system is prevented by
giving the no "updation" rights once the diagnosis is created.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajora_H/0/1/0/all/0/1"&gt;Harish Rajora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Noisy Label Models: Labeler-Dependent Noise with Adversarial Awareness. (arXiv:2105.14083v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14083</id>
        <link href="http://arxiv.org/abs/2105.14083"/>
        <updated>2021-06-08T02:20:25.285Z</updated>
        <summary type="html"><![CDATA[Most studies on learning from noisy labels rely on unrealistic models of
i.i.d. label noise, such as class-conditional transition matrices. More recent
work on instance-dependent noise models are more realistic, but assume a single
generative process for label noise across the entire dataset. We propose a more
principled model of label noise that generalizes instance-dependent noise to
multiple labelers, based on the observation that modern datasets are typically
annotated using distributed crowdsourcing methods. Under our labeler-dependent
model, label noise manifests itself under two modalities: natural error of
good-faith labelers, and adversarial labels provided by malicious actors. We
present two adversarial attack vectors that more accurately reflect the label
noise that may be encountered in real-world settings, and demonstrate that
under our multimodal noisy labels model, state-of-the-art approaches for
learning from noisy labels are defeated by adversarial label attacks. Finally,
we propose a multi-stage, labeler-aware, model-agnostic framework that reliably
filters noisy labels by leveraging knowledge about which data partitions were
labeled by which labeler, and show that our proposed framework remains robust
even in the presence of extreme adversarial label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_G/0/1/0/all/0/1"&gt;Glenn Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polikar_R/0/1/0/all/0/1"&gt;Robi Polikar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation. (arXiv:2104.00994v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00994</id>
        <link href="http://arxiv.org/abs/2104.00994"/>
        <updated>2021-06-08T02:20:25.284Z</updated>
        <summary type="html"><![CDATA[This paper tackles automatically discovering phone-like acoustic units (AUD)
from unlabeled speech data. Past studies usually proposed single-step
approaches. We propose a two-stage approach: the first stage learns a
subword-discriminative feature representation and the second stage applies
clustering to the learned representation and obtains phone-like clusters as the
discovered acoustic units. In the first stage, a recently proposed method in
the task of unsupervised subword modeling is improved by replacing a
monolingual out-of-domain (OOD) ASR system with a multilingual one to create a
subword-discriminative representation that is more language-independent. In the
second stage, segment-level k-means is adopted, and two methods to represent
the variable-length speech segments as fixed-dimension feature vectors are
compared. Experiments on a very low-resource Mboshi language corpus show that
our approach outperforms state-of-the-art AUD in both normalized mutual
information (NMI) and F-score. The multilingual ASR improved upon the
monolingual ASR in providing OOD phone labels and in estimating the phone
boundaries. A comparison of our systems with and without knowing the
ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that
the current approach can significantly benefit from improved phone boundary
estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_S/0/1/0/all/0/1"&gt;Siyuan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr &amp;#x17b;elasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Vel&amp;#xe1;zquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scharenborg_O/0/1/0/all/0/1"&gt;Odette Scharenborg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics. (arXiv:2106.02749v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02749</id>
        <link href="http://arxiv.org/abs/2106.02749"/>
        <updated>2021-06-08T02:20:25.284Z</updated>
        <summary type="html"><![CDATA[Deep neural networks excel at image classification, but their performance is
far less robust to input perturbations than human perception. In this work we
explore whether this shortcoming may be partly addressed by incorporating
brain-inspired recurrent dynamics in deep convolutional networks. We take
inspiration from a popular framework in neuroscience: 'predictive coding'. At
each layer of the hierarchical model, generative feedback 'predicts' (i.e.,
reconstructs) the pattern of activity in the previous layer. The reconstruction
errors are used to iteratively update the network's representations across
timesteps, and to optimize the network's feedback weights over the natural
image dataset-a form of unsupervised training. We show that implementing this
strategy into two popular networks, VGG16 and EfficientNetB0, improves their
robustness against various corruptions. We hypothesize that other feedforward
networks could similarly benefit from the proposed framework. To promote
research in this direction, we provide an open-sourced PyTorch-based package
called Predify, which can be used to implement and investigate the impacts of
the predictive coding dynamics in any convolutional neural network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1"&gt;Bhavin Choksi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozafari_M/0/1/0/all/0/1"&gt;Milad Mozafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OMay_C/0/1/0/all/0/1"&gt;Callum Biggs O&amp;#x27;May&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ador_B/0/1/0/all/0/1"&gt;Benjamin Ador&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1"&gt;Andrea Alamia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1"&gt;Rufin VanRullen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dodrio: Exploring Transformer Models with Interactive Visualization. (arXiv:2103.14625v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14625</id>
        <link href="http://arxiv.org/abs/2103.14625"/>
        <updated>2021-06-08T02:20:25.284Z</updated>
        <summary type="html"><![CDATA[Why do large pre-trained transformer-based models perform so well across a
wide variety of NLP tasks? Recent research suggests the key may lie in
multi-headed attention mechanism's ability to learn and represent linguistic
information. Understanding how these models represent both syntactic and
semantic knowledge is vital to investigate why they succeed and fail, what they
have learned, and how they can improve. We present Dodrio, an open-source
interactive visualization tool to help NLP researchers and practitioners
analyze attention mechanisms in transformer-based models with linguistic
knowledge. Dodrio tightly integrates an overview that summarizes the roles of
different attention heads, and detailed views that help users compare attention
weights with the syntactic structure and semantic information in the input
text. To facilitate the visual comparison of attention weights and linguistic
knowledge, Dodrio applies different graph visualization techniques to represent
attention weights scalable to longer input text. Case studies highlight how
Dodrio provides insights into understanding the attention mechanism in
transformer-based models. Dodrio is available at
https://poloclub.github.io/dodrio/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turko_R/0/1/0/all/0/1"&gt;Robert Turko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1"&gt;Duen Horng Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[hBert + BiasCorp -- Fighting Racism on the Web. (arXiv:2104.02242v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02242</id>
        <link href="http://arxiv.org/abs/2104.02242"/>
        <updated>2021-06-08T02:20:25.283Z</updated>
        <summary type="html"><![CDATA[Subtle and overt racism is still present both in physical and online
communities today and has impacted many lives in different segments of the
society. In this short piece of work, we present how we're tackling this
societal issue with Natural Language Processing. We are releasing BiasCorp, a
dataset containing 139,090 comments and news segment from three specific
sources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually
annotated) is ready for publication. We are currently in the final phase of
manually labeling the remaining dataset using Amazon Mechanical Turk. BERT has
been used widely in several downstream tasks. In this work, we present hBERT,
where we modify certain layers of the pretrained BERT model with the new
Hopfield Layer. hBert generalizes well across different distributions with the
added advantage of a reduced model complexity. We are also releasing a
JavaScript library and a Chrome Extension Application, to help developers make
use of our trained model in web applications (say chat application) and for
users to identify and report racially biased contents on the web respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onabola_O/0/1/0/all/0/1"&gt;Olawale Onabola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhuang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akera_B/0/1/0/all/0/1"&gt;Benjamin Akera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibraheem_A/0/1/0/all/0/1"&gt;Abdulrahman Ibraheem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jia Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dianbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning Based Classification Algorithm Recommendation. (arXiv:2101.05993v1 [cs.IR] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2101.05993</id>
        <link href="http://arxiv.org/abs/2101.05993"/>
        <updated>2021-06-08T02:20:25.282Z</updated>
        <summary type="html"><![CDATA[Recommending appropriate algorithms to a classification problem is one of the
most challenging issues in the field of data mining. The existing algorithm
recommendation models are generally constructed on only one kind of
meta-features by single learners. Considering that i) ensemble learners usually
show better performance and ii) different kinds of meta-features characterize
the classification problems in different viewpoints independently, and further
the models constructed with different sets of meta-features will be
complementary with each other and applicable for ensemble. This paper proposes
an ensemble learning-based algorithm recommendation method. To evaluate the
proposed recommendation method, extensive experiments with 13 well-known
candidate classification algorithms and five different kinds of meta-features
are conducted on 1090 benchmark classification problems. The results show the
effectiveness of the proposed ensemble learning based recommendation method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qinbao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpinionRank: Extracting Ground Truth Labels from Unreliable Expert Opinions with Graph-Based Spectral Ranking. (arXiv:2102.05884v2 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2102.05884</id>
        <link href="http://arxiv.org/abs/2102.05884"/>
        <updated>2021-06-08T02:20:25.282Z</updated>
        <summary type="html"><![CDATA[As larger and more comprehensive datasets become standard in contemporary
machine learning, it becomes increasingly more difficult to obtain reliable,
trustworthy label information with which to train sophisticated models. To
address this problem, crowdsourcing has emerged as a popular, inexpensive, and
efficient data mining solution for performing distributed label collection.
However, crowdsourced annotations are inherently untrustworthy, as the labels
are provided by anonymous volunteers who may have varying, unreliable
expertise. Worse yet, some participants on commonly used platforms such as
Amazon Mechanical Turk may be adversarial, and provide intentionally incorrect
label information without the end user's knowledge. We discuss three
conventional models of the label generation process, describing their
parameterizations and the model-based approaches used to solve them. We then
propose OpinionRank, a model-free, interpretable, graph-based spectral
algorithm for integrating crowdsourced annotations into reliable labels for
performing supervised or semi-supervised learning. Our experiments show that
OpinionRank performs favorably when compared against more highly parameterized
algorithms. We also show that OpinionRank is scalable to very large datasets
and numbers of label sources, and requires considerably fewer computational
resources than previous approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_G/0/1/0/all/0/1"&gt;Glenn Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polikar_R/0/1/0/all/0/1"&gt;Robi Polikar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparsification for Sums of Exponentials and its Algorithmic Applications. (arXiv:2106.02774v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.02774</id>
        <link href="http://arxiv.org/abs/2106.02774"/>
        <updated>2021-06-08T02:20:25.281Z</updated>
        <summary type="html"><![CDATA[Many works in signal processing and learning theory operate under the
assumption that the underlying model is simple, e.g. that a signal is
approximately $k$-Fourier-sparse or that a distribution can be approximated by
a mixture model that has at most $k$ components. However the problem of fitting
the parameters of such a model becomes more challenging when the
frequencies/components are too close together.

In this work we introduce new methods for sparsifying sums of exponentials
and give various algorithmic applications. First we study Fourier-sparse
interpolation without a frequency gap, where Chen et al. gave an algorithm for
finding an $\epsilon$-approximate solution which uses $k' = \mbox{poly}(k, \log
1/\epsilon)$ frequencies. Second, we study learning Gaussian mixture models in
one dimension without a separation condition. Kernel density estimators give an
$\epsilon$-approximation that uses $k' = O(k/\epsilon^2)$ components. These
methods both output models that are much more complex than what we started out
with. We show how to post-process to reduce the number of
frequencies/components down to $k' = \widetilde{O}(k)$, which is optimal up to
logarithmic factors. Moreover we give applications to model selection. In
particular, we give the first algorithms for approximately (and robustly)
determining the number of components in a Gaussian mixture model that work
without a separation condition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Allen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Explanations Can Be Manipulated. (arXiv:2106.02666v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02666</id>
        <link href="http://arxiv.org/abs/2106.02666"/>
        <updated>2021-06-08T02:20:25.275Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations are emerging as an attractive option for
providing recourse to individuals adversely impacted by algorithmic decisions.
As they are deployed in critical applications (e.g. law enforcement, financial
lending), it becomes important to ensure that we clearly understand the
vulnerabilities of these methods and find ways to address them. However, there
is little understanding of the vulnerabilities and shortcomings of
counterfactual explanations. In this work, we introduce the first framework
that describes the vulnerabilities of counterfactual explanations and shows how
they can be manipulated. More specifically, we show counterfactual explanations
may converge to drastically different counterfactuals under a small
perturbation indicating they are not robust. Leveraging this insight, we
introduce a novel objective to train seemingly fair models where counterfactual
explanations find much lower cost recourse under a slight perturbation. We
describe how these models can unfairly provide low-cost recourse for specific
subgroups in the data while appearing fair to auditors. We perform experiments
on loan and violent crime prediction data sets where certain subgroups achieve
up to 20x lower cost recourse under the perturbation. These results raise
concerns regarding the dependability of current counterfactual explanation
techniques, which we hope will inspire investigations in robust counterfactual
explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1"&gt;Dylan Slack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1"&gt;Sophie Hilgard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BalaGAN: Image Translation Between Imbalanced Domains via Cross-Modal Transfer. (arXiv:2010.02036v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02036</id>
        <link href="http://arxiv.org/abs/2010.02036"/>
        <updated>2021-06-08T02:20:25.271Z</updated>
        <summary type="html"><![CDATA[State-of-the-art image-to-image translation methods tend to struggle in an
imbalanced domain setting, where one image domain lacks richness and diversity.
We introduce a new unsupervised translation network, BalaGAN, specifically
designed to tackle the domain imbalance problem. We leverage the latent
modalities of the richer domain to turn the image-to-image translation problem,
between two imbalanced domains, into a balanced, multi-class, and conditional
translation problem, more resembling the style transfer setting. Specifically,
we analyze the source domain and learn a decomposition of it into a set of
latent modes or classes, without any supervision. This leaves us with a
multitude of balanced cross-domain translation tasks, between all pairs of
classes, including the target domain. During inference, the trained network
takes as input a source image, as well as a reference or style image from one
of the modes as a condition, and produces an image which resembles the source
on the pixel-wise level, but shares the same mode as the reference. We show
that employing modalities within the dataset improves the quality of the
translated images, and that BalaGAN outperforms strong baselines of both
unconditioned and style-transfer-based image-to-image translation methods, in
terms of image quality and diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patashnik_O/0/1/0/all/0/1"&gt;Or Patashnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danon_D/0/1/0/all/0/1"&gt;Dov Danon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Hyper-Flow Diffusion. (arXiv:2102.07945v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07945</id>
        <link href="http://arxiv.org/abs/2102.07945"/>
        <updated>2021-06-08T02:20:25.270Z</updated>
        <summary type="html"><![CDATA[Recently, hypergraphs have attracted a lot of attention due to their ability
to capture complex relations among entities. The insurgence of hypergraphs has
resulted in data of increasing size and complexity that exhibit interesting
small-scale and local structure, e.g., small-scale communities and localized
node-ranking around a given set of seed nodes. Popular and principled ways to
capture the local structure are the local hypergraph clustering problem and
related seed set expansion problem. In this work, we propose the first local
diffusion method that achieves edge-size-independent Cheeger-type guarantee for
the problem of local hypergraph clustering while applying to a rich class of
higher-order relations that covers many previously studied special cases. Our
method is based on a primal-dual optimization formulation where the primal
problem has a natural network flow interpretation, and the dual problem has a
cut-based interpretation using the $\ell_2$-norm penalty on associated
cut-costs. We demonstrate the new technique is significantly better than
state-of-the-art methods on both synthetic and real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fountoulakis_K/0/1/0/all/0/1"&gt;Kimon Fountoulakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shenghao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Uncertainty under Laplace Approximations. (arXiv:2010.02720v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02720</id>
        <link href="http://arxiv.org/abs/2010.02720"/>
        <updated>2021-06-08T02:20:25.225Z</updated>
        <summary type="html"><![CDATA[Laplace approximations are classic, computationally lightweight means for
constructing Bayesian neural networks (BNNs). As in other approximate BNNs, one
cannot necessarily expect the induced predictive uncertainty to be calibrated.
Here we develop a formalism to explicitly "train" the uncertainty in a
decoupled way to the prediction itself. To this end, we introduce uncertainty
units for Laplace-approximated networks: Hidden units associated with a
particular weight structure that can be added to any pre-trained,
point-estimated network. Due to their weights, these units are inactive -- they
do not affect the predictions. But their presence changes the geometry (in
particular the Hessian) of the loss landscape, thereby affecting the network's
uncertainty estimates under a Laplace approximation. We show that such units
can be trained via an uncertainty-aware objective, improving standard Laplace
approximations' performance in various uncertainty quantification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kristiadi_A/0/1/0/all/0/1"&gt;Agustinus Kristiadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustly Learning Mixtures of $k$ Arbitrary Gaussians. (arXiv:2012.02119v3 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02119</id>
        <link href="http://arxiv.org/abs/2012.02119"/>
        <updated>2021-06-08T02:20:25.176Z</updated>
        <summary type="html"><![CDATA[We give a polynomial-time algorithm for the problem of robustly estimating a
mixture of $k$ arbitrary Gaussians in $\mathbb{R}^d$, for any fixed $k$, in the
presence of a constant fraction of arbitrary corruptions. This resolves the
main open problem in several previous works on algorithmic robust statistics,
which addressed the special cases of robustly estimating (a) a single Gaussian,
(b) a mixture of TV-distance separated Gaussians, and (c) a uniform mixture of
two Gaussians. Our main tools are an efficient \emph{partial clustering}
algorithm that relies on the sum-of-squares method, and a novel \emph{tensor
decomposition} algorithm that allows errors in both Frobenius norm and low-rank
terms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bakshi_A/0/1/0/all/0/1"&gt;Ainesh Bakshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1"&gt;He Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1"&gt;Daniel M. Kane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_P/0/1/0/all/0/1"&gt;Pravesh K. Kothari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vempala_S/0/1/0/all/0/1"&gt;Santosh S. Vempala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual Information Regularized Identity-aware Facial ExpressionRecognition in Compressed Video. (arXiv:2010.10637v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10637</id>
        <link href="http://arxiv.org/abs/2010.10637"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[How to extract effective expression representations that invariant to the
identity-specific attributes is a long-lasting problem for facial expression
recognition (FER). Most of the previous methods process the RGB images of a
sequence, while we argue that the off-the-shelf and valuable expression-related
muscle movement is already embedded in the compression format. In this paper,
we target to explore the inter-subject variations eliminated facial expression
representation in the compressed video domain. In the up to two orders of
magnitude compressed domain, we can explicitly infer the expression from the
residual frames and possibly extract identity factors from the I frame with a
pre-trained face recognition network. By enforcing the marginal independence of
them, the expression feature is expected to be purer for the expression and be
robust to identity shifts. Specifically, we propose a novel collaborative
min-min game for mutual information (MI) minimization in latent space. We do
not need the identity label or multiple expression samples from the same person
for identity elimination. Moreover, when the apex frame is annotated in the
dataset, the complementary constraint can be further added to regularize the
feature-level game. In testing, only the compressed residual frames are
required to achieve expression prediction. Our solution can achieve comparable
or better performance than the recent decoded image-based methods on the
typical FER benchmarks with about 3 times faster inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Linghao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Interpretable and Trustworthy are GAMs?. (arXiv:2006.06466v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06466</id>
        <link href="http://arxiv.org/abs/2006.06466"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[Generalized additive models (GAMs) have become a leading modelclass for
interpretable machine learning. However, there are many algorithms for training
GAMs, and these can learn different or even contradictory models, while being
equally accurate. Which GAM should we trust? In this paper, we quantitatively
and qualitatively investigate a variety of GAM algorithms on real and simulated
datasets. We find that GAMs with high feature sparsity (only using afew
variables to make predictions) can miss patterns in the data and be unfair to
rare subpopulations. Our results suggest that inductive bias plays a crucial
role in what interpretable models learn and that tree-based GAMs represent the
best balance of sparsity, fidelity and accuracy and thus appear to be the most
trustworthy GAM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chun-Hao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Sarah Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lengerich_B/0/1/0/all/0/1"&gt;Ben Lengerich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldenberg_A/0/1/0/all/0/1"&gt;Anna Goldenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1"&gt;Rich Caruana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Importance Weight Estimation and Generalization in Domain Adaptation under Label Shift. (arXiv:2011.14251v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14251</id>
        <link href="http://arxiv.org/abs/2011.14251"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[We study generalization under labeled shift for categorical and general
normed label spaces. We propose a series of methods to estimate the importance
weights from labeled source to unlabeled target domain and provide confidence
bounds for these estimators. We deploy these estimators and provide
generalization bounds in the unlabeled target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1"&gt;Kamyar Azizzadenesheli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Replay-based Continual Zero-Shot Learning. (arXiv:2101.08894v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08894</id>
        <link href="http://arxiv.org/abs/2101.08894"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[Zero-shot learning is a new paradigm to classify objects from classes that
are not available at training time. Zero-shot learning (ZSL) methods have
attracted considerable attention in recent years because of their ability to
classify unseen/novel class examples. Most of the existing approaches on ZSL
works when all the samples from seen classes are available to train the model,
which does not suit real life. In this paper, we tackle this hindrance by
developing a generative replay-based continual ZSL (GRCZSL). The proposed
method endows traditional ZSL to learn from streaming data and acquire new
knowledge without forgetting the previous tasks' gained experience. We handle
catastrophic forgetting in GRCZSL by replaying the synthetic samples of seen
classes, which have appeared in the earlier tasks. These synthetic samples are
synthesized using the trained conditional variational autoencoder (VAE) over
the immediate past task. Moreover, we only require the current and immediate
previous VAE at any time for training and testing. The proposed GRZSL method is
developed for a single-head setting of continual learning, simulating a
real-world problem setting. In this setting, task identity is given during
training but unavailable during testing. GRCZSL performance is evaluated on
five benchmark datasets for the generalized setup of ZSL with fixed and dynamic
(incremental class) settings of continual learning. The existing class setting
presented recently in the literature is not suitable for a class-incremental
setting. Therefore, this paper proposes a new setting to address this issue.
Experimental results show that the proposed method significantly outperforms
the baseline and the state-of-the-art method and makes it more suitable for
real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_C/0/1/0/all/0/1"&gt;Chandan Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parameswaran_S/0/1/0/all/0/1"&gt;Sethupathy Parameswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1"&gt;Ashish Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Suresh Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving hybrid machine learning tasks by traversing weight space geodesics. (arXiv:2106.02793v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02793</id>
        <link href="http://arxiv.org/abs/2106.02793"/>
        <updated>2021-06-08T02:20:25.173Z</updated>
        <summary type="html"><![CDATA[Machine learning problems have an intrinsic geometric structure as central
objects including a neural network's weight space and the loss function
associated with a particular task can be viewed as encoding the intrinsic
geometry of a given machine learning problem. Therefore, geometric concepts can
be applied to analyze and understand theoretical properties of machine learning
strategies as well as to develop new algorithms. In this paper, we address
three seemingly unrelated open questions in machine learning by viewing them
through a unified framework grounded in differential geometry. Specifically, we
view the weight space of a neural network as a manifold endowed with a
Riemannian metric that encodes performance on specific tasks. By defining a
metric, we can construct geodesic, minimum length, paths in weight space that
represent sets of networks of equivalent or near equivalent functional
performance on a specific task. We, then, traverse geodesic paths while
identifying networks that satisfy a second objective. Inspired by the geometric
insight, we apply our geodesic framework to 3 major applications: (i) Network
sparsification (ii) Mitigating catastrophic forgetting by constructing networks
with high performance on a series of objectives and (iii) Finding high-accuracy
paths connecting distinct local optima of deep networks in the non-convex loss
landscape. Our results are obtained on a wide range of network architectures
(MLP, VGG11/16) trained on MNIST, CIFAR-10/100. Broadly, we introduce a
geometric framework that unifies a range of machine learning objectives and
that can be applied to multiple classes of neural network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghavan_G/0/1/0/all/0/1"&gt;Guruprasad Raghavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomson_M/0/1/0/all/0/1"&gt;Matt Thomson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms. (arXiv:2102.00815v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00815</id>
        <link href="http://arxiv.org/abs/2102.00815"/>
        <updated>2021-06-08T02:20:25.173Z</updated>
        <summary type="html"><![CDATA[Finding the minimal structural assumptions that empower sample-efficient
learning is one of the most important research directions in Reinforcement
Learning (RL). This paper advances our understanding of this fundamental
question by introducing a new complexity measure -- Bellman Eluder (BE)
dimension. We show that the family of RL problems of low BE dimension is
remarkably rich, which subsumes a vast majority of existing tractable RL
problems including but not limited to tabular MDPs, linear MDPs, reactive
POMDPs, low Bellman rank problems as well as low Eluder dimension problems.
This paper further designs a new optimization-based algorithm -- GOLF, and
reanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang
et al., 2017). We prove that both algorithms learn the near-optimal policies of
low BE dimension problems in a number of samples that is polynomial in all
relevant parameters, but independent of the size of state-action space. Our
regret and sample complexity results match or improve the best existing results
for several well-known subclasses of low BE dimension problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qinghua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1"&gt;Sobhan Miryoosefi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Accelerated Stochastic Gradient Descent. (arXiv:2006.08950v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08950</id>
        <link href="http://arxiv.org/abs/2006.08950"/>
        <updated>2021-06-08T02:20:25.166Z</updated>
        <summary type="html"><![CDATA[We propose Federated Accelerated Stochastic Gradient Descent (FedAc), a
principled acceleration of Federated Averaging (FedAvg, also known as Local
SGD) for distributed optimization. FedAc is the first provable acceleration of
FedAvg that improves convergence speed and communication efficiency on various
types of convex functions. For example, for strongly convex and smooth
functions, when using $M$ workers, the previous state-of-the-art FedAvg
analysis can achieve a linear speedup in $M$ if given $M$ rounds of
synchronization, whereas FedAc only requires $M^{\frac{1}{3}}$ rounds.
Moreover, we prove stronger guarantees for FedAc when the objectives are
third-order smooth. Our technique is based on a potential-based perturbed
iterate analysis, a novel stability analysis of generalized accelerated SGD,
and a strategic tradeoff between acceleration and stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Honglin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiToD: A Bilingual Multi-Domain Dataset For Task-Oriented Dialogue Modeling. (arXiv:2106.02787v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02787</id>
        <link href="http://arxiv.org/abs/2106.02787"/>
        <updated>2021-06-08T02:20:25.159Z</updated>
        <summary type="html"><![CDATA[Task-oriented dialogue (ToD) benchmarks provide an important avenue to
measure progress and develop better conversational agents. However, existing
datasets for end-to-end ToD modeling are limited to a single language,
hindering the development of robust end-to-end ToD systems for multilingual
countries and regions. Here we introduce BiToD, the first bilingual
multi-domain dataset for end-to-end task-oriented dialogue modeling. BiToD
contains over 7k multi-domain dialogues (144k utterances) with a large and
realistic bilingual knowledge base. It serves as an effective benchmark for
evaluating bilingual ToD systems and cross-lingual transfer learning
approaches. We provide state-of-the-art baselines under three evaluation
settings (monolingual, bilingual, and cross-lingual). The analysis of our
baselines in different settings highlights 1) the effectiveness of training a
bilingual ToD system compared to two independent monolingual ToD systems, and
2) the potential of leveraging a bilingual knowledge base and cross-lingual
transfer learning to improve the system performance under low resource
condition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhaojiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1"&gt;Andrea Madotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1"&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1"&gt;Feijun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yuxiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perfect reconstruction of sparse signals with piecewise continuous nonconvex penalties and nonconvexity control. (arXiv:1902.07436v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.07436</id>
        <link href="http://arxiv.org/abs/1902.07436"/>
        <updated>2021-06-08T02:20:25.147Z</updated>
        <summary type="html"><![CDATA[We consider compressed sensing formulated as a minimization problem of
nonconvex sparse penalties, Smoothly Clipped Absolute deviation (SCAD) and
Minimax Concave Penalty (MCP). The nonconvexity of these penalties is
controlled by nonconvexity parameters, and L1 penalty is contained as a limit
with respect to these parameters. The analytically derived reconstruction limit
overcomes that of L1 and the algorithmic limit in the Bayes-optimal setting,
when the nonconvexity parameters have suitable values. However, for small
nonconvexity parameters, where the reconstruction of the relatively dense
signals is theoretically guaranteed, the corresponding approximate message
passing (AMP) cannot achieve perfect reconstruction. We identify that the
shrinks in the basin of attraction to the perfect reconstruction causes the
discrepancy between the AMP and corresponding theory using state evolution. A
part of the discrepancy is resolved by introducing the control of the
nonconvexity parameters to guide the AMP trajectory to the basin of the
attraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sakata_A/0/1/0/all/0/1"&gt;Ayaka Sakata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Obuchi_T/0/1/0/all/0/1"&gt;Tomoyuki Obuchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Topology from Synthetic Data for Unsupervised Depth Completion. (arXiv:2106.02994v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02994</id>
        <link href="http://arxiv.org/abs/2106.02994"/>
        <updated>2021-06-08T02:20:25.139Z</updated>
        <summary type="html"><![CDATA[We present a method for inferring dense depth maps from images and sparse
depth measurements by leveraging synthetic data to learn the association of
sparse point clouds with dense natural shapes, and using the image as evidence
to validate the predicted depth map. Our learned prior for natural shapes uses
only sparse depth as input, not images, so the method is not affected by the
covariate shift when attempting to transfer learned models from synthetic data
to real ones. This allows us to use abundant synthetic data with ground truth
to learn the most difficult component of the reconstruction process, which is
topology estimation, and use the image to refine the prediction based on
photometric evidence. Our approach uses fewer parameters than previous methods,
yet, achieves the state of the art on both indoor and outdoor benchmark
datasets. Code available at:
https://github.com/alexklwong/learning-topology-synthetic-data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alex Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicek_S/0/1/0/all/0/1"&gt;Safa Cicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Linear Optimization over Wasserstein Balls. (arXiv:2004.07162v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07162</id>
        <link href="http://arxiv.org/abs/2004.07162"/>
        <updated>2021-06-08T02:20:25.138Z</updated>
        <summary type="html"><![CDATA[Wasserstein balls, which contain all probability measures within a
pre-specified Wasserstein distance to a reference measure, have recently
enjoyed wide popularity in the distributionally robust optimization and machine
learning communities to formulate and solve data-driven optimization problems
with rigorous statistical guarantees. In this technical note we prove that the
Wasserstein ball is weakly compact under mild conditions, and we offer
necessary and sufficient conditions for the existence of optimal solutions. We
also characterize the sparsity of solutions if the Wasserstein ball is centred
at a discrete reference measure. In comparison with the existing literature,
which has proved similar results under different conditions, our proofs are
self-contained and shorter, yet mathematically rigorous, and our necessary and
sufficient conditions for the existence of optimal solutions are easily
verifiable in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yue_M/0/1/0/all/0/1"&gt;Man-Chung Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kuhn_D/0/1/0/all/0/1"&gt;Daniel Kuhn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wiesemann_W/0/1/0/all/0/1"&gt;Wolfram Wiesemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Vision Transformers to Adversarial Examples. (arXiv:2104.02610v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02610</id>
        <link href="http://arxiv.org/abs/2104.02610"/>
        <updated>2021-06-08T02:20:25.137Z</updated>
        <summary type="html"><![CDATA[Recent advances in attention-based networks have shown that Vision
Transformers can achieve state-of-the-art or near state-of-the-art results on
many image classification tasks. This puts transformers in the unique position
of being a promising alternative to traditional convolutional neural networks
(CNNs). While CNNs have been carefully studied with respect to adversarial
attacks, the same cannot be said of Vision Transformers. In this paper, we
study the robustness of Vision Transformers to adversarial examples. Our
analyses of transformer security is divided into three parts. First, we test
the transformer under standard white-box and black-box attacks. Second, we
study the transferability of adversarial examples between CNNs and
transformers. We show that adversarial examples do not readily transfer between
CNNs and transformers. Based on this finding, we analyze the security of a
simple ensemble defense of CNNs and transformers. By creating a new attack, the
self-attention blended gradient attack, we show that such an ensemble is not
secure under a white-box adversary. However, under a black-box adversary, we
show that an ensemble can achieve unprecedented robustness without sacrificing
clean accuracy. Our analysis for this work is done using six types of white-box
attacks and two types of black-box attacks. Our study encompasses multiple
Vision Transformers, Big Transfer Models and CNN architectures trained on
CIFAR-10, CIFAR-100 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_K/0/1/0/all/0/1"&gt;Kaleel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Rigel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1"&gt;Marten van Dijk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hybrid Inference System for Improved Curvature Estimation in the Level-Set Method Using Machine Learning. (arXiv:2104.02951v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02951</id>
        <link href="http://arxiv.org/abs/2104.02951"/>
        <updated>2021-06-08T02:20:25.136Z</updated>
        <summary type="html"><![CDATA[We present a novel hybrid strategy based on machine learning to improve
curvature estimation in the level-set method. The proposed inference system
couples enhanced neural networks with standard numerical schemes to compute
curvature more accurately. The core of our hybrid framework is a switching
mechanism that relies on well established numerical techniques to gauge
curvature. If the curvature magnitude is larger than a resolution-dependent
threshold, it uses a neural network to yield a better approximation. Our
networks are multilayer perceptrons fitted to synthetic data sets composed of
sinusoidal- and circular-interface samples at various configurations. To reduce
data set size and training complexity, we leverage the problem's characteristic
symmetry and build our models on just half of the curvature spectrum. These
savings lead to a powerful inference system able to outperform any of its
numerical or neural component alone. Experiments with static, smooth interfaces
show that our hybrid solver is notably superior to conventional numerical
methods in coarse grids and along steep interface regions. Compared to prior
research, we have observed outstanding gains in precision after training the
regression model with data pairs from more than a single interface type and
transforming data with specialized input preprocessing. In particular, our
findings confirm that machine learning is a promising venue for reducing or
removing mass loss in the level-set method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Larios_Cardenas_L/0/1/0/all/0/1"&gt;Luis &amp;#xc1;ngel Larios-C&amp;#xe1;rdenas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibou_F/0/1/0/all/0/1"&gt;Frederic Gibou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-the-Air Adversarial Flickering Attacks against Video Recognition Networks. (arXiv:2002.05123v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05123</id>
        <link href="http://arxiv.org/abs/2002.05123"/>
        <updated>2021-06-08T02:20:25.135Z</updated>
        <summary type="html"><![CDATA[Deep neural networks for video classification, just like image classification
networks, may be subjected to adversarial manipulation. The main difference
between image classifiers and video classifiers is that the latter usually use
temporal information contained within the video. In this work we present a
manipulation scheme for fooling video classifiers by introducing a flickering
temporal perturbation that in some cases may be unnoticeable by human observers
and is implementable in the real world. After demonstrating the manipulation of
action classification of single videos, we generalize the procedure to make
universal adversarial perturbation, achieving high fooling ratio. In addition,
we generalize the universal perturbation and produce a temporal-invariant
perturbation, which can be applied to the video without synchronizing the
perturbation to the input. The attack was implemented on several target models
and the transferability of the attack was demonstrated. These properties allow
us to bridge the gap between simulated environment and real-world application,
as will be demonstrated in this paper for the first time for an over-the-air
flickering attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pony_R/0/1/0/all/0/1"&gt;Roi Pony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeh_I/0/1/0/all/0/1"&gt;Itay Naeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed+: A Unified Approach to Robust Personalized Federated Learning. (arXiv:2009.06303v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06303</id>
        <link href="http://arxiv.org/abs/2009.06303"/>
        <updated>2021-06-08T02:20:25.134Z</updated>
        <summary type="html"><![CDATA[We present a class of methods for robust, personalized federated learning,
called Fed+, that unifies many federated learning algorithms. The principal
advantage of this class of methods is to better accommodate the real-world
characteristics found in federated training, such as the lack of IID data
across parties, the need for robustness to outliers or stragglers, and the
requirement to perform well on party-specific datasets. We achieve this through
a problem formulation that allows the central server to employ robust ways of
aggregating the local models while keeping the structure of local computation
intact. Without making any statistical assumption on the degree of
heterogeneity of local data across parties, we provide convergence guarantees
for Fed+ for convex and non-convex loss functions and robust aggregation. The
Fed+ theory is also equipped to handle heterogeneous computing environments
including stragglers without additional assumptions; specifically, the
convergence results cover the general setting where the number of local update
steps across parties can vary. We demonstrate the benefits of Fed+ through
extensive experiments across standard benchmark datasets as well as on a
challenging real-world problem in financial portfolio management where the
heterogeneity of party-level data can lead to training failure in standard
federated learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Pengqian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kundu_A/0/1/0/all/0/1"&gt;Achintya Kundu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wynter_L/0/1/0/all/0/1"&gt;Laura Wynter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"&gt;Shiau Hong Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Sharp Blockwise Tensor Perturbation Bound for Orthogonal Iteration. (arXiv:2008.02437v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02437</id>
        <link href="http://arxiv.org/abs/2008.02437"/>
        <updated>2021-06-08T02:20:25.133Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop novel perturbation bounds for the high-order
orthogonal iteration (HOOI) [DLDMV00b]. Under mild regularity conditions, we
establish blockwise tensor perturbation bounds for HOOI with guarantees for
both tensor reconstruction in Hilbert-Schmidt norm $\|\widehat{\bcT} - \bcT
\|_{\tHS}$ and mode-$k$ singular subspace estimation in Schatten-$q$ norm $\|
\sin \Theta (\widehat{\U}_k, \U_k) \|_q$ for any $q \geq 1$. We show the upper
bounds of mode-$k$ singular subspace estimation are unilateral and converge
linearly to a quantity characterized by blockwise errors of the perturbation
and signal strength. For the tensor reconstruction error bound, we express the
bound through a simple quantity $\xi$, which depends only on perturbation and
the multilinear rank of the underlying signal. Rate matching deterministic
lower bound for tensor reconstruction, which demonstrates the optimality of
HOOI, is also provided. Furthermore, we prove that one-step HOOI (i.e., HOOI
with only a single iteration) is also optimal in terms of tensor reconstruction
and can be used to lower the computational cost. The perturbation results are
also extended to the case that only partial modes of $\bcT$ have low-rank
structure. We support our theoretical results by extensive numerical studies.
Finally, we apply the novel perturbation bounds of HOOI on two applications,
tensor denoising and tensor co-clustering, from machine learning and
statistics, which demonstrates the superiority of the new perturbation results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuetian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Raskutti_G/0/1/0/all/0/1"&gt;Garvesh Raskutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Ming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Anru R. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T-Net: Deep Stacked Scale-Iteration Network for Image Dehazing. (arXiv:2106.02809v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02809</id>
        <link href="http://arxiv.org/abs/2106.02809"/>
        <updated>2021-06-08T02:20:25.132Z</updated>
        <summary type="html"><![CDATA[Hazy images reduce the visibility of the image content, and haze will lead to
failure in handling subsequent computer vision tasks. In this paper, we address
the problem of image dehazing by proposing a dehazing network named T-Net,
which consists of a backbone network based on the U-Net architecture and a dual
attention module. And it can achieve multi-scale feature fusion by using skip
connections with a new fusion strategy. Furthermore, by repeatedly unfolding
the plain T-Net, Stack T-Net is proposed to take advantage of the dependence of
deep features across stages via a recursive strategy. In order to reduce
network parameters, the intra-stage recursive computation of ResNet is adopted
in our Stack T-Net. And we take both the stage-wise result and the original
hazy image as input to each T-Net and finally output the prediction of clean
image. Experimental results on both synthetic and real-world images demonstrate
that our plain T-Net and the advanced Stack T-Net perform favorably against the
state-of-the-art dehazing algorithms, and show that our Stack T-Net could
further improve the dehazing effect, demonstrating the effectiveness of the
recursive strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lirong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanshan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1"&gt;Wenhan Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oops I Took A Gradient: Scalable Sampling for Discrete Distributions. (arXiv:2102.04509v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04509</id>
        <link href="http://arxiv.org/abs/2102.04509"/>
        <updated>2021-06-08T02:20:25.131Z</updated>
        <summary type="html"><![CDATA[We propose a general and scalable approximate sampling strategy for
probabilistic models with discrete variables. Our approach uses gradients of
the likelihood function with respect to its discrete inputs to propose updates
in a Metropolis-Hastings sampler. We show empirically that this approach
outperforms generic samplers in a number of difficult settings including Ising
models, Potts models, restricted Boltzmann machines, and factorial hidden
Markov models. We also demonstrate the use of our improved sampler for training
deep energy-based models on high dimensional discrete data. This approach
outperforms variational auto-encoders and existing energy-based models.
Finally, we give bounds showing that our approach is near-optimal in the class
of samplers which propose local updates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1"&gt;Will Grathwohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1"&gt;Kevin Swersky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1"&gt;Milad Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1"&gt;David Duvenaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1"&gt;Chris J. Maddison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning. (arXiv:2102.07868v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07868</id>
        <link href="http://arxiv.org/abs/2102.07868"/>
        <updated>2021-06-08T02:20:25.130Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) are non-parametric, flexible, models that work well
in many tasks. Combining GPs with deep learning methods via deep kernel
learning (DKL) is especially compelling due to the strong representational
power induced by the network. However, inference in GPs, whether with or
without DKL, can be computationally challenging on large datasets. Here, we
propose GP-Tree, a novel method for multi-class classification with Gaussian
processes and DKL. We develop a tree-based hierarchical model in which each
internal node of the tree fits a GP to the data using the P\'olya-Gamma
augmentation scheme. As a result, our method scales well with both the number
of classes and data size. We demonstrate the effectiveness of our method
against other Gaussian process training baselines, and we show how our general
GP approach achieves improved accuracy on standard incremental few-shot
learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1"&gt;Idan Achituve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1"&gt;Aviv Navon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yemini_Y/0/1/0/all/0/1"&gt;Yochai Yemini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1"&gt;Ethan Fetaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths. (arXiv:2101.09612v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09612</id>
        <link href="http://arxiv.org/abs/2101.09612"/>
        <updated>2021-06-08T02:20:25.129Z</updated>
        <summary type="html"><![CDATA[We give a simple proof for the global convergence of gradient descent in
training deep ReLU networks with the standard square loss, and show some of its
improvements over the state-of-the-art. In particular, while prior works
require all the hidden layers to be wide with width at least $\Omega(N^8)$ ($N$
being the number of training samples), we require a single wide layer of
linear, quadratic or cubic width depending on the type of initialization.
Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof
need not track the evolution of the entire NTK matrix, or more generally, any
quantities related to the changes of activation patterns during training.
Instead, we only need to track the evolution of the output at the last hidden
layer, which can be done much more easily thanks to the Lipschitz property of
ReLU. Some highlights of our setting: (i) all the layers are trained with
standard gradient descent, (ii) the network has standard parameterization as
opposed to the NTK one, and (iii) the network has a single wide layer as
opposed to having all wide hidden layers as in most of NTK-related results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quynh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orbital MCMC. (arXiv:2010.08047v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08047</id>
        <link href="http://arxiv.org/abs/2010.08047"/>
        <updated>2021-06-08T02:20:25.125Z</updated>
        <summary type="html"><![CDATA[Markov Chain Monte Carlo (MCMC) algorithms ubiquitously employ complex
deterministic transformations to generate proposal points that are then
filtered by the Metropolis-Hastings-Green (MHG) test. However, the condition of
the target measure invariance puts restrictions on the design of these
transformations. In this paper, we first derive the acceptance test for the
stochastic Markov kernel considering arbitrary deterministic maps as proposal
generators. When applied to the transformations with orbits of period two
(involutions), the test reduces to the MHG test. Based on the derived test we
propose two practical algorithms: one operates by constructing periodic orbits
from any diffeomorphism, another on contractions of the state space (such as
optimization trajectories). Finally, we perform an empirical study
demonstrating the practical advantages of both kernels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neklyudov_K/0/1/0/all/0/1"&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Implementation Matters in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v11 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03479</id>
        <link href="http://arxiv.org/abs/2102.03479"/>
        <updated>2021-06-08T02:20:25.124Z</updated>
        <summary type="html"><![CDATA[Multi-Agent Reinforcement Learning (MARL) has seen revolutionary
breakthroughs with its successful application to multi-agent cooperative tasks
such as computer games and robot swarms. QMIX, a widely popular MARL algorithm,
has been used to solve cooperative tasks, e.g. Starcraft Multi-Agent Challenge
(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX
target relaxing the monotonicity constraint of QMIX, allowing for performance
improvement in SMAC. However, in this paper, we investigate the code-level
optimizations of these variants and the monotonicity constraint. We find that
(1) such improvements of the variants are significantly affected by various
code-level optimizations; (2) QMIX with normalized optimizations outperforms
other previous works in SMAC; (3) the monotonicity constraint may improve
sample efficiency in SMAC and DEPP. Last, a discussion with theoretical
analysis is demonstrated about why QMIX works well in SMAC. We open-source the
code at \url{https://github.com/hijkzzz/pymarl2}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Siyang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1"&gt;Seth Austin Harding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haibin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1"&gt;Shih-wei Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Draw: Emergent Communication through Sketching. (arXiv:2106.02067v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.02067</id>
        <link href="http://arxiv.org/abs/2106.02067"/>
        <updated>2021-06-08T02:20:25.103Z</updated>
        <summary type="html"><![CDATA[Evidence that visual communication preceded written language and provided a
basis for it goes back to prehistory, in forms such as cave and rock paintings
depicting traces of our distant ancestors. Emergent communication research has
sought to explore how agents can learn to communicate in order to
collaboratively solve tasks. Existing research has focused on language, with a
learned communication channel transmitting sequences of discrete tokens between
the agents. In this work, we explore a visual communication channel between
agents that are allowed to draw with simple strokes. Our agents are
parameterised by deep neural networks, and the drawing procedure is
differentiable, allowing for end-to-end training. In the framework of a
referential communication game, we demonstrate that agents can not only
successfully learn to communicate by drawing, but with appropriate inductive
biases, can do so in a fashion that humans can interpret. We hope to encourage
future research to consider visual communication as a more flexible and
directly interpretable alternative of training collaborative agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1"&gt;Daniela Mihai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Theory of Reinforcement Learning with Once-per-Episode Feedback. (arXiv:2105.14363v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14363</id>
        <link href="http://arxiv.org/abs/2105.14363"/>
        <updated>2021-06-08T02:20:25.097Z</updated>
        <summary type="html"><![CDATA[We study a theory of reinforcement learning (RL) in which the learner
receives binary feedback only once at the end of an episode. While this is an
extreme test case for theory, it is also arguably more representative of
real-world applications than the traditional requirement in RL practice that
the learner receive feedback at every time step. Indeed, in many real-world
applications of reinforcement learning, such as self-driving cars and robotics,
it is easier to evaluate whether a learner's complete trajectory was either
"good" or "bad," but harder to provide a reward signal at each step. To show
that learning is possible in this more challenging setting, we study the case
where trajectory labels are generated by an unknown parametric model, and
provide a statistically and computationally efficient algorithm that achieves
sub-linear regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Positions in CountSketch. (arXiv:2007.09890v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09890</id>
        <link href="http://arxiv.org/abs/2007.09890"/>
        <updated>2021-06-08T02:20:25.089Z</updated>
        <summary type="html"><![CDATA[We consider sketching algorithms which first quickly compress data by
multiplication with a random sketch matrix, and then apply the sketch to
quickly solve an optimization problem, e.g., low rank approximation. In the
learning-based sketching paradigm proposed by Indyk et al. [2019], the sketch
matrix is found by choosing a random sparse matrix, e.g., the CountSketch, and
then updating the values of the non-zero entries by running gradient descent on
a training data set. Despite the growing body of work on this paradigm, a
noticeable omission is that the locations of the non-zero entries of previous
algorithms were fixed, and only their values were learned. In this work we
propose the first learning algorithm that also optimizes the locations of the
non-zero entries. We show this algorithm gives better accuracy for low rank
approximation than previous work, and apply it to other problems such as
$k$-means clustering for the first time. We show that our algorithm is provably
better in the spiked covariance model and for Zipfian matrices. We also show
the importance of the sketch monotonicity property for combining learned
sketches. Our empirical results show the importance of optimizing not only the
values of the non-zero entries but also their positions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Simin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianrui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1"&gt;Ali Vakilian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1"&gt;Yulin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels. (arXiv:2105.04522v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04522</id>
        <link href="http://arxiv.org/abs/2105.04522"/>
        <updated>2021-06-08T02:20:25.078Z</updated>
        <summary type="html"><![CDATA[Prior works have found it beneficial to combine provably noise-robust loss
functions e.g., mean absolute error (MAE) with standard categorical loss
function e.g. cross entropy (CE) to improve their learnability. Here, we
propose to use Jensen-Shannon divergence as a noise-robust loss function and
show that it interestingly interpolate between CE and MAE with a controllable
mixing parameter. Furthermore, we make a crucial observation that CE exhibit
lower consistency around noisy data points. Based on this observation, we adopt
a generalized version of the Jensen-Shannon divergence for multiple
distributions to encourage consistency around data points. Using this loss
function, we show state-of-the-art results on both synthetic (CIFAR), and
real-world (WebVision) noise with varying noise rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Englesson_E/0/1/0/all/0/1"&gt;Erik Englesson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1"&gt;Hossein Azizpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Biasing of Language Models for Speech Recognition in Goal-Oriented Conversational Agents. (arXiv:2103.10325v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10325</id>
        <link href="http://arxiv.org/abs/2103.10325"/>
        <updated>2021-06-08T02:20:25.064Z</updated>
        <summary type="html"><![CDATA[Goal-oriented conversational interfaces are designed to accomplish specific
tasks and typically have interactions that tend to span multiple turns adhering
to a pre-defined structure and a goal. However, conventional neural language
models (NLM) in Automatic Speech Recognition (ASR) systems are mostly trained
sentence-wise with limited context. In this paper, we explore different ways to
incorporate context into a LSTM based NLM in order to model long range
dependencies and improve speech recognition. Specifically, we use context carry
over across multiple turns and use lexical contextual cues such as system
dialog act from Natural Language Understanding (NLU) models and the user
provided structure of the chatbot. We also propose a new architecture that
utilizes context embeddings derived from BERT on sample utterances provided
during inference time. Our experiments show a word error rate (WER) relative
reduction of 7% over non-contextual utterance-level NLM rescorers on
goal-oriented audio datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS. (arXiv:2103.15060v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15060</id>
        <link href="http://arxiv.org/abs/2103.15060"/>
        <updated>2021-06-08T02:20:25.058Z</updated>
        <summary type="html"><![CDATA[This paper introduces PnG BERT, a new encoder model for neural TTS. This
model is augmented from the original BERT model, by taking both phoneme and
grapheme representations of text as input, as well as the word-level alignment
between them. It can be pre-trained on a large text corpus in a self-supervised
manner, and fine-tuned in a TTS task. Experimental results show that a neural
TTS model using a pre-trained PnG BERT as its encoder yields more natural
prosody and more accurate pronunciation than a baseline model using only
phoneme input with no pre-training. Subjective side-by-side preference
evaluations show that raters have no statistically significant preference
between the speech synthesized using a PnG BERT and ground truth recordings
from professional speakers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Ye Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1"&gt;Heiga Zen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jonathan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yonghui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable UAV Collision Avoidance using Deep Reinforcement Learning. (arXiv:2105.12254v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12254</id>
        <link href="http://arxiv.org/abs/2105.12254"/>
        <updated>2021-06-08T02:20:25.039Z</updated>
        <summary type="html"><![CDATA[The significant components of any successful autonomous flight system are
task completion and collision avoidance. Most deep learning algorithms
successfully execute these aspects under the environment and conditions they
are trained. However, they fail when subjected to novel environments. This
paper presents an autonomous multi-rotor flight algorithm, using Deep
Reinforcement Learning augmented with Self-Attention Models, that can
effectively reason when subjected to varying inputs. In addition to their
reasoning ability, they are also interpretable, enabling it to be used under
real-world conditions. We have tested our algorithm under different weather
conditions and environments and found it robust compared to conventional Deep
Reinforcement Learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1"&gt;Deepak-George Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olshanskyi_D/0/1/0/all/0/1"&gt;Daniil Olshanskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krueger_K/0/1/0/all/0/1"&gt;Karter Krueger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wongpiromsarn_T/0/1/0/all/0/1"&gt;Tichakorn Wongpiromsarn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1"&gt;Ali Jannesari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. (arXiv:2105.15203v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15203</id>
        <link href="http://arxiv.org/abs/2105.15203"/>
        <updated>2021-06-08T02:20:25.030Z</updated>
        <summary type="html"><![CDATA[We present SegFormer, a simple, efficient yet powerful semantic segmentation
framework which unifies Transformers with lightweight multilayer perception
(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a
novel hierarchically structured Transformer encoder which outputs multiscale
features. It does not need positional encoding, thereby avoiding the
interpolation of positional codes which leads to decreased performance when the
testing resolution differs from training. 2) SegFormer avoids complex decoders.
The proposed MLP decoder aggregates information from different layers, and thus
combining both local attention and global attention to render powerful
representations. We show that this simple and lightweight design is the key to
efficient segmentation on Transformers. We scale our approach up to obtain a
series of models from SegFormer-B0 to SegFormer-B5, reaching significantly
better performance and efficiency than previous counterparts. For example,
SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x
smaller and 2.2% better than the previous best method. Our best model,
SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows
excellent zero-shot robustness on Cityscapes-C. Code will be released at:
github.com/NVlabs/SegFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Navigating to the Best Policy in Markov Decision Processes. (arXiv:2106.02847v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02847</id>
        <link href="http://arxiv.org/abs/2106.02847"/>
        <updated>2021-06-08T02:20:25.022Z</updated>
        <summary type="html"><![CDATA[We investigate the classical active pure exploration problem in Markov
Decision Processes, where the agent sequentially selects actions and, from the
resulting system trajectory, aims at identifying the best policy as fast as
possible. We propose an information-theoretic lower bound on the average number
of steps required before a correct answer can be given with probability at
least $1-\delta$. This lower bound involves a non-convex optimization problem,
for which we propose a convex relaxation. We further provide an algorithm whose
sample complexity matches the relaxed lower bound up to a factor $2$. This
algorithm addresses general communicating MDPs; we propose a variant with
reduced exploration rate (and hence faster convergence) under an additional
ergodicity assumption. This work extends previous results relative to the
\emph{generative setting}~\cite{marjani2020adaptive}, where the agent could at
each step observe the random outcome of any (state, action) pair. In contrast,
we show here how to deal with the \emph{navigation constraints}. Our analysis
relies on an ergodic theorem for non-homogeneous Markov chains which we
consider of wide interest in the analysis of Markov Decision Processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Marjani_A/0/1/0/all/0/1"&gt;Aymen Al Marjani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Garivier_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Garivier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Proutiere_A/0/1/0/all/0/1"&gt;Alexandre Proutiere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiscoBox: Weakly Supervised Instance Segmentation and Semantic Correspondence from Box Supervision. (arXiv:2105.06464v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06464</id>
        <link href="http://arxiv.org/abs/2105.06464"/>
        <updated>2021-06-08T02:20:25.014Z</updated>
        <summary type="html"><![CDATA[We introduce DiscoBox, a novel framework that jointly learns instance
segmentation and semantic correspondence using bounding box supervision.
Specifically, we propose a self-ensembling framework where instance
segmentation and semantic correspondence are jointly guided by a structured
teacher in addition to the bounding box supervision. The teacher is a
structured energy model incorporating a pairwise potential and a cross-image
potential to model the pairwise pixel relationships both within and across the
boxes. Minimizing the teacher energy simultaneously yields refined object masks
and dense correspondences between intra-class objects, which are taken as
pseudo-labels to supervise the task network and provide positive/negative
correspondence pairs for dense constrastive learning. We show a symbiotic
relationship where the two tasks mutually benefit from each other. Our best
model achieves 37.9% AP on COCO instance segmentation, surpassing prior weakly
supervised methods and is competitive to supervised methods. We also obtain
state of the art weakly supervised results on PASCAL VOC12 and PF-PASCAL with
real-time inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1"&gt;Shiyi Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1"&gt;Christopher Choy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_S/0/1/0/all/0/1"&gt;Subhashree Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry S. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Price of Incentivizing Exploration: A Characterization via Thompson Sampling and Sample Complexity. (arXiv:2002.00558v4 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00558</id>
        <link href="http://arxiv.org/abs/2002.00558"/>
        <updated>2021-06-08T02:20:25.006Z</updated>
        <summary type="html"><![CDATA[We consider incentivized exploration: a version of multi-armed bandits where
the choice of arms is controlled by self-interested agents, and the algorithm
can only issue recommendations. The algorithm controls the flow of information,
and the information asymmetry can incentivize the agents to explore. Prior work
achieves optimal regret rates up to multiplicative factors that become
arbitrarily large depending on the Bayesian priors, and scale exponentially in
the number of arms. A more basic problem of sampling each arm once runs into
similar factors.

We focus on the price of incentives: the loss in performance, broadly
construed, incurred for the sake of incentive-compatibility. We prove that
Thompson Sampling, a standard bandit algorithm, is incentive-compatible if
initialized with sufficiently many data points. The performance loss due to
incentives is therefore limited to the initial rounds when these data points
are collected. The problem is largely reduced to that of sample complexity: how
many rounds are needed? We address this question, providing matching upper and
lower bounds and instantiating them in various corollaries. Typically, the
optimal sample complexity is polynomial in the number of arms and exponential
in the "strength of beliefs".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1"&gt;Mark Sellke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1"&gt;Aleksandrs Slivkins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Stochastic Simulation with Interactive Neural Processes. (arXiv:2106.02770v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02770</id>
        <link href="http://arxiv.org/abs/2106.02770"/>
        <updated>2021-06-08T02:20:24.988Z</updated>
        <summary type="html"><![CDATA[Stochastic simulations such as large-scale, spatiotemporal, age-structured
epidemic models are computationally expensive at fine-grained resolution. We
propose Interactive Neural Process (INP), an interactive framework to
continuously learn a deep learning surrogate model and accelerate simulation.
Our framework is based on the novel integration of Bayesian active learning,
stochastic simulation and deep sequence modeling. In particular, we develop a
novel spatiotemporal neural process model to mimic the underlying process
dynamics. Our model automatically infers the latent process which describes the
intrinsic uncertainty of the simulator. This also gives rise to a new
acquisition function that can quantify the uncertainty of deep learning
predictions. We design Bayesian active learning algorithms to iteratively query
the simulator, gather more data, and continuously improve the model. We perform
theoretical analysis and demonstrate that our approach reduces sample
complexity compared with random sampling in high dimension. Empirically, we
demonstrate our framework can faithfully imitate the behavior of a complex
infectious disease simulator with a small number of examples, enabling rapid
simulation and scenario exploration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongxia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinazzi_M/0/1/0/all/0/1"&gt;Matteo Chinazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vespignani_A/0/1/0/all/0/1"&gt;Alessandro Vespignani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi-An Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rose Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained episodic reinforcement learning in concave-convex and knapsack settings. (arXiv:2006.05051v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05051</id>
        <link href="http://arxiv.org/abs/2006.05051"/>
        <updated>2021-06-08T02:20:24.977Z</updated>
        <summary type="html"><![CDATA[We propose an algorithm for tabular episodic reinforcement learning with
constraints. We provide a modular analysis with strong theoretical guarantees
for settings with concave rewards and convex constraints, and for settings with
hard constraints (knapsacks). Most of the previous work in constrained
reinforcement learning is limited to linear constraints, and the remaining work
focuses on either the feasibility question or settings with a single episode.
Our experiments demonstrate that the proposed algorithm significantly
outperforms these approaches in existing constrained episodic environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1"&gt;Kiant&amp;#xe9; Brantley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1"&gt;Miroslav Dudik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1"&gt;Thodoris Lykouris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1"&gt;Sobhan Miryoosefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1"&gt;Aleksandrs Slivkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization. (arXiv:2008.11646v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11646</id>
        <link href="http://arxiv.org/abs/2008.11646"/>
        <updated>2021-06-08T02:20:24.967Z</updated>
        <summary type="html"><![CDATA[Cross-view geo-localization is to spot images of the same geographic target
from different platforms, e.g., drone-view cameras and satellites. It is
challenging in the large visual appearance changes caused by extreme viewpoint
variations. Existing methods usually concentrate on mining the fine-grained
feature of the geographic target in the image center, but underestimate the
contextual information in neighbor areas. In this work, we argue that neighbor
areas can be leveraged as auxiliary information, enriching discriminative clues
for geolocalization. Specifically, we introduce a simple and effective deep
neural network, called Local Pattern Network (LPN), to take advantage of
contextual information in an end-to-end manner. Without using extra part
estimators, LPN adopts a square-ring feature partition strategy, which provides
the attention according to the distance to the image center. It eases the part
matching and enables the part-wise representation learning. Owing to the
square-ring partition design, the proposed LPN has good scalability to rotation
variations and achieves competitive results on three prevailing benchmarks,
i.e., University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN
can be easily embedded into other frameworks to further boost performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tingyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chenggang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiyong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yaoqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bolun Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighting Adversarial Neural Network for Domain Adaptation in Regression. (arXiv:2006.08251v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08251</id>
        <link href="http://arxiv.org/abs/2006.08251"/>
        <updated>2021-06-08T02:20:24.958Z</updated>
        <summary type="html"><![CDATA[We present a novel instance-based approach to handle regression tasks in the
context of supervised domain adaptation. The approach developed in this paper
relies on the assumption that the task on the target domain can be efficiently
learned by adequately reweighting the source instances during training phase.
We introduce a novel formulation of the optimization objective for domain
adaptation which relies on a discrepancy distance characterizing the difference
between domains according to a specific task and a class of hypotheses. To
solve this problem, we develop an adversarial network algorithm which learns
both the source weighting scheme and the task in one feed-forward gradient
descent. We provide numerical evidence of the relevance of the method on public
datasets for domain adaptation through reproducible experiments accessible via
an online demo interface at: https://antoinedemathelin.github.io/demo/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathelin_A/0/1/0/all/0/1"&gt;Antoine de Mathelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1"&gt;Guillaume Richard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deheeger_F/0/1/0/all/0/1"&gt;Francois Deheeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mougeot_M/0/1/0/all/0/1"&gt;Mathilde Mougeot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vayatis_N/0/1/0/all/0/1"&gt;Nicolas Vayatis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subgroup Fairness in Two-Sided Markets. (arXiv:2106.02702v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02702</id>
        <link href="http://arxiv.org/abs/2106.02702"/>
        <updated>2021-06-08T02:20:24.947Z</updated>
        <summary type="html"><![CDATA[It is well known that two-sided markets are unfair in a number of ways. For
instance, female workers at Uber earn less than their male colleagues per mile
driven. Similar observations have been made for other minority subgroups in
other two-sided markets. Here, we suggest a novel market-clearing mechanism for
two-sided markets, which promotes equalisation of the pay per hour worked
across multiple subgroups, as well as within each subgroup. In the process, we
introduce a novel notion of subgroup fairness (which we call Inter-fairness),
which can be combined with other notions of fairness within each subgroup
(called Intra-fairness), and the utility for the customers (Customer-Care) in
the objective of the market-clearing problem. While the novel non-linear terms
in the objective complicate market clearing by making the problem non-convex,
we show that a certain non-convex augmented Lagrangian relaxation can be
approximated to any precision in time polynomial in the number of market
participants using semi-definite programming. This makes it possible to
implement the market-clearing mechanism efficiently. On the example of
driver-ride assignment in an Uber-like system, we demonstrate the efficacy and
scalability of the approach, and trade-offs between Inter- and Intra-fairness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Quan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marecek_J/0/1/0/all/0/1"&gt;Jakub Marecek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shorten_R/0/1/0/all/0/1"&gt;Robert N. Shorten&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the complexity of finding a local minimizer of a quadratic function over a polytope. (arXiv:2008.05558v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05558</id>
        <link href="http://arxiv.org/abs/2008.05558"/>
        <updated>2021-06-08T02:20:24.927Z</updated>
        <summary type="html"><![CDATA[We show that unless P=NP, there cannot be a polynomial-time algorithm that
finds a point within Euclidean distance $c^n$ (for any constant $c \ge 0$) of a
local minimizer of an $n$-variate quadratic function over a polytope. This
result (even with $c=0$) answers a question of Pardalos and Vavasis that
appeared in 1992 on a list of seven open problems in complexity theory for
numerical optimization. Our proof technique also implies that the problem of
deciding whether a quadratic function has a local minimizer over an (unbounded)
polyhedron, and that of deciding if a quartic polynomial has a local minimizer
are NP-hard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1"&gt;Amir Ali Ahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jeffrey Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Algorithm For Generalized Linear Bandit: Online Stochastic Gradient Descent and Thompson Sampling. (arXiv:2006.04012v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04012</id>
        <link href="http://arxiv.org/abs/2006.04012"/>
        <updated>2021-06-08T02:20:24.912Z</updated>
        <summary type="html"><![CDATA[We consider the contextual bandit problem, where a player sequentially makes
decisions based on past observations to maximize the cumulative reward.
Although many algorithms have been proposed for contextual bandit, most of them
rely on finding the maximum likelihood estimator at each iteration, which
requires $O(t)$ time at the $t$-th iteration and are memory inefficient. A
natural way to resolve this problem is to apply online stochastic gradient
descent (SGD) so that the per-step time and memory complexity can be reduced to
constant with respect to $t$, but a contextual bandit policy based on online
SGD updates that balances exploration and exploitation has remained elusive. In
this work, we show that online SGD can be applied to the generalized linear
bandit problem. The proposed SGD-TS algorithm, which uses a single-step SGD
update to exploit past information and uses Thompson Sampling for exploration,
achieves $\tilde{O}(\sqrt{T})$ regret with the total time complexity that
scales linearly in $T$ and $d$, where $T$ is the total number of rounds and $d$
is the number of features. Experimental results show that SGD-TS consistently
outperforms existing algorithms on both synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharpnack_J/0/1/0/all/0/1"&gt;James Sharpnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dodrio: Exploring Transformer Models with Interactive Visualization. (arXiv:2103.14625v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14625</id>
        <link href="http://arxiv.org/abs/2103.14625"/>
        <updated>2021-06-08T02:20:24.906Z</updated>
        <summary type="html"><![CDATA[Why do large pre-trained transformer-based models perform so well across a
wide variety of NLP tasks? Recent research suggests the key may lie in
multi-headed attention mechanism's ability to learn and represent linguistic
information. Understanding how these models represent both syntactic and
semantic knowledge is vital to investigate why they succeed and fail, what they
have learned, and how they can improve. We present Dodrio, an open-source
interactive visualization tool to help NLP researchers and practitioners
analyze attention mechanisms in transformer-based models with linguistic
knowledge. Dodrio tightly integrates an overview that summarizes the roles of
different attention heads, and detailed views that help users compare attention
weights with the syntactic structure and semantic information in the input
text. To facilitate the visual comparison of attention weights and linguistic
knowledge, Dodrio applies different graph visualization techniques to represent
attention weights scalable to longer input text. Case studies highlight how
Dodrio provides insights into understanding the attention mechanism in
transformer-based models. Dodrio is available at
https://poloclub.github.io/dodrio/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turko_R/0/1/0/all/0/1"&gt;Robert Turko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1"&gt;Duen Horng Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Adversarial Attacks. (arXiv:2103.02014v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02014</id>
        <link href="http://arxiv.org/abs/2103.02014"/>
        <updated>2021-06-08T02:20:24.905Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks expose important vulnerabilities of deep learning models,
yet little attention has been paid to settings where data arrives as a stream.
In this paper, we formalize the online adversarial attack problem, emphasizing
two key elements found in real-world use-cases: attackers must operate under
partial knowledge of the target model, and the decisions made by the attacker
are irrevocable since they operate on a transient data stream. We first
rigorously analyze a deterministic variant of the online threat model by
drawing parallels to the well-studied $k$-secretary problem in theoretical
computer science and propose Virtual+, a simple yet practical online algorithm.
Our main theoretical result show Virtual+ yields provably the best competitive
ratio over all single-threshold algorithms for $k<5$ -- extending previous
analysis of the $k$-secretary problem. We also introduce the \textit{stochastic
$k$-secretary} -- effectively reducing online blackbox transfer attacks to a
$k$-secretary problem under noise -- and prove theoretical bounds on the
performance of \textit{any} online algorithms adapted to this setting. Finally,
we complement our theoretical results by conducting experiments on both MNIST
and CIFAR-10 with both vanilla and robust classifiers, revealing not only the
necessity of online algorithms in achieving near-optimal performance but also
the rich interplay of a given attack strategy towards online attack selection,
enabling simple strategies like FGSM to outperform classically strong whitebox
adversaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mladenovic_A/0/1/0/all/0/1"&gt;Andjela Mladenovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1"&gt;Avishek Joey Bose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berard_H/0/1/0/all/0/1"&gt;Hugo Berard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1"&gt;Simon Lacoste-Julien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1"&gt;Pascal Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1"&gt;Gauthier Gidel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2004.09764v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09764</id>
        <link href="http://arxiv.org/abs/2004.09764"/>
        <updated>2021-06-08T02:20:24.903Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Causal Inference Using Control Variates. (arXiv:2103.16689v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16689</id>
        <link href="http://arxiv.org/abs/2103.16689"/>
        <updated>2021-06-08T02:20:24.903Z</updated>
        <summary type="html"><![CDATA[While many areas of machine learning have benefited from the increasing
availability of large and varied datasets, the benefit to causal inference has
been limited given the strong assumptions needed to ensure identifiability of
causal effects; these are often not satisfied in real-world datasets. For
example, many large observational datasets (e.g., case-control studies in
epidemiology, click-through data in recommender systems) suffer from selection
bias on the outcome, which makes the average treatment effect (ATE)
unidentifiable. We propose a general algorithm to estimate causal effects from
\emph{multiple} data sources, where the ATE may be identifiable only in some
datasets but not others. The key idea is to construct control variates using
the datasets in which the ATE is not identifiable. We show theoretically that
this reduces the variance of the ATE estimate. We apply this framework to
inference from observational data under outcome selection bias, assuming access
to an auxiliary small dataset from which we can obtain a consistent estimate of
the ATE. We construct a control variate by taking the difference of the odds
ratio estimates from the two datasets. Across simulations and two case studies
with real data, we show that this control variate can significantly reduce the
variance of the ATE estimate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenshuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Serena Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1"&gt;Peng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mirror Descent Policy Optimization. (arXiv:2005.09814v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09814</id>
        <link href="http://arxiv.org/abs/2005.09814"/>
        <updated>2021-06-08T02:20:24.895Z</updated>
        <summary type="html"><![CDATA[Mirror descent (MD), a well-known first-order method in constrained convex
optimization, has recently been shown as an important tool to analyze
trust-region algorithms in reinforcement learning (RL). However, there remains
a considerable gap between such theoretically analyzed algorithms and the ones
used in practice. Inspired by this, we propose an efficient RL algorithm,
called {\em mirror descent policy optimization} (MDPO). MDPO iteratively
updates the policy by {\em approximately} solving a trust-region problem, whose
objective function consists of two terms: a linearization of the standard RL
objective and a proximity term that restricts two consecutive policies to be
close to each other. Each update performs this approximation by taking multiple
gradient steps on this objective function. We derive {\em on-policy} and {\em
off-policy} variants of MDPO, while emphasizing important design choices
motivated by the existing theory of MD in RL. We highlight the connections
between on-policy MDPO and two popular trust-region RL algorithms: TRPO and
PPO, and show that explicitly enforcing the trust-region constraint is in fact
{\em not} a necessity for high performance gains in TRPO. We then show how the
popular soft actor-critic (SAC) algorithm can be derived by slight
modifications of off-policy MDPO. Overall, MDPO is derived from the MD
principles, offers a unified approach to viewing a number of popular RL
algorithms, and performs better than or on-par with TRPO, PPO, and SAC in a
number of continuous control tasks. Code is available at
\url{https://github.com/manantomar/Mirror-Descent-Policy-Optimization}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomar_M/0/1/0/all/0/1"&gt;Manan Tomar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shani_L/0/1/0/all/0/1"&gt;Lior Shani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1"&gt;Yonathan Efroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Random Feature Model for Input-Output Maps between Banach Spaces. (arXiv:2005.10224v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10224</id>
        <link href="http://arxiv.org/abs/2005.10224"/>
        <updated>2021-06-08T02:20:24.886Z</updated>
        <summary type="html"><![CDATA[Well known to the machine learning community, the random feature model is a
parametric approximation to kernel interpolation or regression methods. It is
typically used to approximate functions mapping a finite-dimensional input
space to the real line. In this paper, we instead propose a methodology for use
of the random feature model as a data-driven surrogate for operators that map
an input Banach space to an output Banach space. Although the methodology is
quite general, we consider operators defined by partial differential equations
(PDEs); here, the inputs and outputs are themselves functions, with the input
parameters being functions required to specify the problem, such as initial
data or coefficients, and the outputs being solutions of the problem. Upon
discretization, the model inherits several desirable attributes from this
infinite-dimensional viewpoint, including mesh-invariant approximation error
with respect to the true PDE solution map and the capability to be trained at
one mesh resolution and then deployed at different mesh resolutions. We view
the random feature model as a non-intrusive data-driven emulator, provide a
mathematical framework for its interpretation, and demonstrate its ability to
efficiently and accurately approximate the nonlinear parameter-to-solution maps
of two prototypical PDEs arising in physical science and engineering
applications: viscous Burgers' equation and a variable coefficient elliptic
equation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Nelsen_N/0/1/0/all/0/1"&gt;Nicholas H. Nelsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1"&gt;Andrew M. Stuart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization Induced Equilibrium Networks. (arXiv:2105.13228v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13228</id>
        <link href="http://arxiv.org/abs/2105.13228"/>
        <updated>2021-06-08T02:20:24.875Z</updated>
        <summary type="html"><![CDATA[Implicit equilibrium models, i.e., deep neural networks (DNNs) defined by
implicit equations, have been becoming more and more attractive recently. In
this paper, we investigate an emerging question: can an implicit equilibrium
model's equilibrium point be regarded as the solution of an optimization
problem? To this end, we first decompose DNNs into a new class of unit layer
that is the proximal operator of an implicit convex function while keeping its
output unchanged. Then, the equilibrium model of the unit layer can be derived,
named Optimization Induced Equilibrium Networks (OptEq), which can be easily
extended to deep layers. The equilibrium point of OptEq can be theoretically
connected to the solution of its corresponding convex optimization problem with
explicit objectives. Based on this, we can flexibly introduce prior properties
to the equilibrium points: 1) modifying the underlying convex problems
explicitly so as to change the architectures of OptEq; and 2) merging the
information into the fixed point iteration, which guarantees to choose the
desired equilibrium point when the fixed point set is non-singleton. We show
that deep OptEq outperforms previous implicit models even with fewer
parameters. This work establishes the first step towards the
optimization-guided design of deep models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xingyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiuhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zenan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yisen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guangcan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Into the Unknown: Active Monitoring of Neural Networks. (arXiv:2009.06429v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06429</id>
        <link href="http://arxiv.org/abs/2009.06429"/>
        <updated>2021-06-08T02:20:24.869Z</updated>
        <summary type="html"><![CDATA[Neural-network classifiers achieve high accuracy when predicting the class of
an input that they were trained to identify. Maintaining this accuracy in
dynamic environments, where inputs frequently fall outside the fixed set of
initially known classes, remains a challenge. The typical approach is to detect
inputs from novel classes and retrain the classifier on an augmented dataset.
However, not only the classifier but also the detection mechanism needs to
adapt in order to distinguish between newly learned and yet unknown input
classes. To address this challenge, we introduce an algorithmic framework for
active monitoring of a neural network. A monitor wrapped in our framework
operates in parallel with the neural network and interacts with a human user
via a series of interpretable labeling queries for incremental adaptation. In
addition, we propose an adaptive quantitative monitor to improve precision. An
experimental evaluation on a diverse set of benchmarks with varying numbers of
classes confirms the benefits of our active monitoring framework in dynamic
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lukina_A/0/1/0/all/0/1"&gt;Anna Lukina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1"&gt;Christian Schilling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1"&gt;Thomas A. Henzinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor Imagery Recognition. (arXiv:2005.00777v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00777</id>
        <link href="http://arxiv.org/abs/2005.00777"/>
        <updated>2021-06-08T02:20:24.848Z</updated>
        <summary type="html"><![CDATA[Recognition accuracy and response time are both critically essential ahead of
building practical electroencephalography (EEG) based brain-computer interface
(BCI). Recent approaches, however, have either compromised in the
classification accuracy or responding time. This paper presents a novel deep
learning approach designed towards remarkably accurate and responsive motor
imagery (MI) recognition based on scalp EEG. Bidirectional Long Short-term
Memory (BiLSTM) with the Attention mechanism manages to derive relevant
features from raw EEG signals. The connected graph convolutional neural network
(GCN) promotes the decoding performance by cooperating with the topological
structure of features, which are estimated from the overall data. The
0.4-second detection framework has shown effective and efficient prediction
based on individual and group-wise training, with 98.81% and 94.64% accuracy,
respectively, which outperformed all the state-of-the-art studies. The
introduced deep feature mining approach can precisely recognize human motion
intents from raw EEG signals, which paves the road to translate the EEG based
MI recognition to practical BCI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yimin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Shuyue Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lun_X/0/1/0/all/0/1"&gt;Xiangmin Lun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks. (arXiv:2002.10085v4 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10085</id>
        <link href="http://arxiv.org/abs/2002.10085"/>
        <updated>2021-06-08T02:20:24.819Z</updated>
        <summary type="html"><![CDATA[Spiking neural networks (SNNs) are well suited for spatio-temporal learning
and implementations on energy-efficient event-driven neuromorphic processors.
However, existing SNN error backpropagation (BP) methods lack proper handling
of spiking discontinuities and suffer from low performance compared with the BP
methods for traditional artificial neural networks. In addition, a large number
of time steps are typically required to achieve decent performance, leading to
high latency and rendering spike-based computation unscalable to deep
architectures. We present a novel Temporal Spike Sequence Learning
Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down
error backpropagation across two types of inter-neuron and intra-neuron
dependencies and leads to improved temporal learning precision. It captures
inter-neuron dependencies through presynaptic firing times by considering the
all-or-none characteristics of firing activities and captures intra-neuron
dependencies by handling the internal evolution of each neuronal state in time.
TSSL-BP efficiently trains deep SNNs within a much shortened temporal window of
a few steps while improving the accuracy for various image classification
datasets including CIFAR10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenrui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Auxiliary Information in Self-supervised Learning. (arXiv:2106.02869v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02869</id>
        <link href="http://arxiv.org/abs/2106.02869"/>
        <updated>2021-06-08T02:20:24.807Z</updated>
        <summary type="html"><![CDATA[This paper presents to integrate the auxiliary information (e.g., additional
attributes for data such as the hashtags for Instagram images) in the
self-supervised learning process. We first observe that the auxiliary
information may bring us useful information about data structures: for
instance, the Instagram images with the same hashtags can be semantically
similar. Hence, to leverage the structural information from the auxiliary
information, we present to construct data clusters according to the auxiliary
information. Then, we introduce the Clustering InfoNCE (Cl-InfoNCE) objective
that learns similar representations for augmented variants of data from the
same cluster and dissimilar representations for data from different clusters.
Our approach contributes as follows: 1) Comparing to conventional
self-supervised representations, the auxiliary-information-infused
self-supervised representations bring the performance closer to the supervised
representations; 2) The presented Cl-InfoNCE can also work with unsupervised
constructed clusters (e.g., k-means clusters) and outperform strong
clustering-based self-supervised learning approaches, such as the Prototypical
Contrastive Learning (PCL) method; 3) We show that Cl-InfoNCE may be a better
approach to leverage the data clustering information, by comparing it to the
baseline approach - learning to predict the clustering assignments with
cross-entropy loss. For analysis, we connect the goodness of the learned
representations with the statistical relationships: i) the mutual information
between the labels and the clusters and ii) the conditional entropy of the
clusters given the labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yao-Hung Hubert Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianqin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_P/0/1/0/all/0/1"&gt;Peiyuan Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Bounds between $f$-Divergences and Integral Probability Metrics. (arXiv:2006.05973v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05973</id>
        <link href="http://arxiv.org/abs/2006.05973"/>
        <updated>2021-06-08T02:20:24.801Z</updated>
        <summary type="html"><![CDATA[The families of $f$-divergences (e.g. the Kullback-Leibler divergence) and
Integral Probability Metrics (e.g. total variation distance or maximum mean
discrepancies) are widely used to quantify the similarity between probability
distributions. In this work, we systematically study the relationship between
these two families from the perspective of convex duality. Starting from a
tight variational representation of the $f$-divergence, we derive a
generalization of the moment-generating function, which we show exactly
characterizes the best lower bound of the $f$-divergence as a function of a
given IPM. Using this characterization, we obtain new bounds while also
recovering in a unified manner well-known results, such as Hoeffding's lemma,
Pinsker's inequality and its extension to subgaussian functions, and the
Hammersley-Chapman-Robbins bound. This characterization also allows us to prove
new results on topological properties of the divergence which may be of
independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Agrawal_R/0/1/0/all/0/1"&gt;Rohit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Horel_T/0/1/0/all/0/1"&gt;Thibaut Horel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Curves for SGD on Structured Features. (arXiv:2106.02713v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02713</id>
        <link href="http://arxiv.org/abs/2106.02713"/>
        <updated>2021-06-08T02:20:24.795Z</updated>
        <summary type="html"><![CDATA[The generalization performance of a machine learning algorithm such as a
neural network depends in a non-trivial way on the structure of the data
distribution. Models of generalization in machine learning theory often ignore
the low-dimensional structure of natural signals, either by considering
data-agnostic bounds or by studying the performance of the algorithm when
trained on uncorrelated features. To analyze the influence of data structure on
test loss dynamics, we study an exactly solveable model of stochastic gradient
descent (SGD) which predicts test loss when training on features with arbitrary
covariance structure. We solve the theory exactly for both Gaussian features
and arbitrary features and we show that the simpler Gaussian model accurately
predicts test loss of nonlinear random-feature models and deep neural networks
trained with SGD on real datasets such as MNIST and CIFAR-10. We show that
modeling the geometry of the data in the induced feature space is indeed
crucial to accurately predict the test error throughout learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1"&gt;Blake Bordelon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1"&gt;Cengiz Pehlevan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal Over-Parameterized Training. (arXiv:2004.04690v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.04690</id>
        <link href="http://arxiv.org/abs/2004.04690"/>
        <updated>2021-06-08T02:20:24.778Z</updated>
        <summary type="html"><![CDATA[The inductive bias of a neural network is largely determined by the
architecture and the training algorithm. To achieve good generalization, how to
effectively train a neural network is of great importance. We propose a novel
orthogonal over-parameterized training (OPT) framework that can provably
minimize the hyperspherical energy which characterizes the diversity of neurons
on a hypersphere. By maintaining the minimum hyperspherical energy during
training, OPT can greatly improve the empirical generalization. Specifically,
OPT fixes the randomly initialized weights of the neurons and learns an
orthogonal transformation that applies to these neurons. We consider multiple
ways to learn such an orthogonal transformation, including unrolling
orthogonalization algorithms, applying orthogonal parameterization, and
designing orthogonality-preserving gradient descent. For better scalability, we
propose the stochastic OPT which performs orthogonal transformation
stochastically for partial dimensions of neurons. Interestingly, OPT reveals
that learning a proper coordinate system for neurons is crucial to
generalization. We provide some insights on why OPT yields better
generalization. Extensive experiments validate the superiority of OPT over the
standard training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1"&gt;James M. Rehg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1"&gt;Liam Paull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Le Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expressive Power of Invariant and Equivariant Graph Neural Networks. (arXiv:2006.15646v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15646</id>
        <link href="http://arxiv.org/abs/2006.15646"/>
        <updated>2021-06-08T02:20:24.771Z</updated>
        <summary type="html"><![CDATA[Various classes of Graph Neural Networks (GNN) have been proposed and shown
to be successful in a wide range of applications with graph structured data. In
this paper, we propose a theoretical framework able to compare the expressive
power of these GNN architectures. The current universality theorems only apply
to intractable classes of GNNs. Here, we prove the first approximation
guarantees for practical GNNs, paving the way for a better understanding of
their generalization. Our theoretical results are proved for invariant GNNs
computing a graph embedding (permutation of the nodes of the input graph does
not affect the output) and equivariant GNNs computing an embedding of the nodes
(permutation of the input permutes the output). We show that Folklore Graph
Neural Networks (FGNN), which are tensor based GNNs augmented with matrix
multiplication are the most expressive architectures proposed so far for a
given tensor order. We illustrate our results on the Quadratic Assignment
Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to
learn how to solve the problem, leading to much better average performances
than existing algorithms (based on spectral, SDP or other GNNs architectures).
On a practical side, we also implement masked tensors to handle batches of
graphs of varying sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizian_W/0/1/0/all/0/1"&gt;Wa&amp;#xef;ss Azizian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lelarge_M/0/1/0/all/0/1"&gt;Marc Lelarge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UFO-BLO: Unbiased First-Order Bilevel Optimization. (arXiv:2006.03631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03631</id>
        <link href="http://arxiv.org/abs/2006.03631"/>
        <updated>2021-06-08T02:20:24.763Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization (BLO) is a popular approach with many applications
including hyperparameter optimization, neural architecture search, adversarial
robustness and model-agnostic meta-learning. However, the approach suffers from
time and memory complexity proportional to the length $r$ of its inner
optimization loop, which has led to several modifications being proposed. One
such modification is \textit{first-order} BLO (FO-BLO) which approximates
outer-level gradients by zeroing out second derivative terms, yielding
significant speed gains and requiring only constant memory as $r$ varies.
Despite FO-BLO's popularity, there is a lack of theoretical understanding of
its convergence properties. We make progress by demonstrating a rich family of
examples where FO-BLO-based stochastic optimization does not converge to a
stationary point of the BLO objective. We address this concern by proposing a
new FO-BLO-based unbiased estimate of outer-level gradients, enabling us to
theoretically guarantee this convergence, with no harm to memory and expected
time complexity. Our findings are supported by experimental results on Omniglot
and Mini-ImageNet, popular few-shot meta-learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1"&gt;Valerii Likhosherstov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;Jared Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Churn Reduction via Distillation. (arXiv:2106.02654v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02654</id>
        <link href="http://arxiv.org/abs/2106.02654"/>
        <updated>2021-06-08T02:20:24.754Z</updated>
        <summary type="html"><![CDATA[In real-world systems, models are frequently updated as more data becomes
available, and in addition to achieving high accuracy, the goal is to also
maintain a low difference in predictions compared to the base model (i.e.
predictive ``churn''). If model retraining results in vastly different
behavior, then it could cause negative effects in downstream systems,
especially if this churn can be avoided with limited impact on model accuracy.
In this paper, we show an equivalence between training with distillation using
the base model as the teacher and training with an explicit constraint on the
predictive churn. We then show that distillation performs strongly for low
churn training against a number of recent baselines on a wide range of datasets
and model architectures, including fully-connected networks, convolutional
networks, and transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Heinrich Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1"&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotter_A/0/1/0/all/0/1"&gt;Andrew Cotter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1"&gt;Afshin Rostamizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-rank Characteristic Tensor Density Estimation Part I: Foundations. (arXiv:2008.12315v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12315</id>
        <link href="http://arxiv.org/abs/2008.12315"/>
        <updated>2021-06-08T02:20:24.737Z</updated>
        <summary type="html"><![CDATA[Effective non-parametric density estimation is a key challenge in
high-dimensional multivariate data analysis. In this paper,we propose a novel
approach that builds upon tensor factorization tools. Any multivariate density
can be represented by its characteristic function, via the Fourier transform.
If the sought density is compactly supported, then its characteristic function
can be approximated, within controllable error, by a finite tensor of leading
Fourier coefficients, whose size de-pends on the smoothness of the underlying
density. This tensor can be naturally estimated from observed realizations of
the random vector of interest, via sample averaging. In order to circumvent the
curse of dimensionality, we introduce a low-rank model of this characteristic
tensor, which significantly improves the density estimate especially for
high-dimensional data and/or in the sample-starved regime. By virtue of
uniqueness of low-rank tensor decomposition, under certain conditions, our
method enables learning the true data-generating distribution. We demonstrate
the very promising performance of the proposed method using several measured
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Amiridi_M/0/1/0/all/0/1"&gt;Magda Amiridi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kargas_N/0/1/0/all/0/1"&gt;Nikos Kargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sidiropoulos_N/0/1/0/all/0/1"&gt;Nicholas D. Sidiropoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational (Gradient) Estimate of the Score Function in Energy-based Latent Variable Models. (arXiv:2010.08258v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08258</id>
        <link href="http://arxiv.org/abs/2010.08258"/>
        <updated>2021-06-08T02:20:24.735Z</updated>
        <summary type="html"><![CDATA[The learning and evaluation of energy-based latent variable models (EBLVMs)
without any structural assumptions are highly challenging, because the true
posteriors and the partition functions in such models are generally
intractable. This paper presents variational estimates of the score function
and its gradient with respect to the model parameters in a general EBLVM,
referred to as VaES and VaGES respectively. The variational posterior is
trained to minimize a certain divergence to the true model posterior and the
bias in both estimates can be bounded by the divergence theoretically. With a
minimal model assumption, VaES and VaGES can be applied to the kernelized Stein
discrepancy (KSD) and score matching (SM)-based methods to learn EBLVMs.
Besides, VaES can also be used to estimate the exact Fisher divergence between
the data and general EBLVMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1"&gt;Fan Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chongxuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphMI: Extracting Private Graph Data from Graph Neural Networks. (arXiv:2106.02820v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02820</id>
        <link href="http://arxiv.org/abs/2106.02820"/>
        <updated>2021-06-08T02:20:24.729Z</updated>
        <summary type="html"><![CDATA[As machine learning becomes more widely used for critical applications, the
need to study its implications in privacy turns to be urgent. Given access to
the target model and auxiliary information, the model inversion attack aims to
infer sensitive features of the training dataset, which leads to great privacy
concerns. Despite its success in grid-like domains, directly applying model
inversion techniques on non-grid domains such as graph achieves poor attack
performance due to the difficulty to fully exploit the intrinsic properties of
graphs and attributes of nodes used in Graph Neural Networks (GNN). To bridge
this gap, we present \textbf{Graph} \textbf{M}odel \textbf{I}nversion attack
(GraphMI), which aims to extract private graph data of the training graph by
inverting GNN, one of the state-of-the-art graph analysis tools. Specifically,
we firstly propose a projected gradient module to tackle the discreteness of
graph edges while preserving the sparsity and smoothness of graph features.
Then we design a graph auto-encoder module to efficiently exploit graph
topology, node attributes, and target model parameters for edge inference. With
the proposed methods, we study the connection between model inversion risk and
edge influence and show that edges with greater influence are more likely to be
recovered. Extensive experiments over several public datasets demonstrate the
effectiveness of our method. We also show that differential privacy in its
canonical form can hardly defend our attack while preserving decent utility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zaixi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhenya Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chengqiang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chuanren Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modular Analysis of Provable Acceleration via Polyak's Momentum: Training a Wide ReLU Network and a Deep Linear Network. (arXiv:2010.01618v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01618</id>
        <link href="http://arxiv.org/abs/2010.01618"/>
        <updated>2021-06-08T02:20:24.710Z</updated>
        <summary type="html"><![CDATA[Incorporating a so-called "momentum" dynamic in gradient descent methods is
widely used in neural net training as it has been broadly observed that, at
least empirically, it often leads to significantly faster convergence. At the
same time, there are very few theoretical guarantees in the literature to
explain this apparent acceleration effect. Even for the classical strongly
convex quadratic problems, several existing results only show Polyak's momentum
has an accelerated linear rate asymptotically. In this paper, we first revisit
the quadratic problems and show a non-asymptotic accelerated linear rate of
Polyak's momentum. Then, we provably show that Polyak's momentum achieves
acceleration for training a one-layer wide ReLU network and a deep linear
network, which are perhaps the two most popular canonical models for studying
optimization and deep learning in the literature. Prior work Du at al. 2019 and
Wu et al. 2019 showed that using vanilla gradient descent, and with an use of
over-parameterization, the error decays as $(1- \Theta(\frac{1}{ \kappa'}))^t$
after $t$ iterations, where $\kappa'$ is the condition number of a Gram Matrix.
Our result shows that with the appropriate choice of parameters Polyak's
momentum has a rate of $(1-\Theta(\frac{1}{\sqrt{\kappa'}}))^t$. For the deep
linear network, prior work Hu et al. 2020 showed that vanilla gradient descent
has a rate of $(1-\Theta(\frac{1}{\kappa}))^t$, where $\kappa$ is the condition
number of a data matrix. Our result shows an acceleration rate $(1-
\Theta(\frac{1}{\sqrt{\kappa}}))^t$ is achievable by Polyak's momentum. All the
results in this work are obtained from a modular analysis, which can be of
independent interest. This work establishes that momentum does indeed speed up
neural net training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun-Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chi-Heng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1"&gt;Jacob Abernethy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Custom 7nm CMOS Standard Cell Library for Implementing TNN-based Neuromorphic Processors. (arXiv:2012.05419v2 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05419</id>
        <link href="http://arxiv.org/abs/2012.05419"/>
        <updated>2021-06-08T02:20:24.710Z</updated>
        <summary type="html"><![CDATA[A set of highly-optimized custom macro extensions is developed for a 7nm CMOS
cell library for implementing Temporal Neural Networks (TNNs) that can mimic
brain-like sensory processing with extreme energy efficiency. A TNN prototype
(13,750 neurons and 315,000 synapses) for MNIST requires only 1.56mm2 die area
and consumes only 1.69mW.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nair_H/0/1/0/all/0/1"&gt;Harideep Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vellaisamy_P/0/1/0/all/0/1"&gt;Prabhu Vellaisamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhasuthkar_S/0/1/0/all/0/1"&gt;Santha Bhasuthkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;John Paul Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing the Latent Space of Molecular Deep Generative Models with Persistent Homology Metrics. (arXiv:2010.08548v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08548</id>
        <link href="http://arxiv.org/abs/2010.08548"/>
        <updated>2021-06-08T02:20:24.709Z</updated>
        <summary type="html"><![CDATA[Deep generative models are increasingly becoming integral parts of the in
silico molecule design pipeline and have dual goals of learning the chemical
and structural features that render candidate molecules viable while also being
flexible enough to generate novel designs. Specifically, Variational Auto
Encoders (VAEs) are generative models in which encoder-decoder network pairs
are trained to reconstruct training data distributions in such a way that the
latent space of the encoder network is smooth. Therefore, novel candidates can
be found by sampling from this latent space. However, the scope of
architectures and hyperparameters is vast and choosing the best combination for
in silico discovery has important implications for downstream success.
Therefore, it is important to develop a principled methodology for
distinguishing how well a given generative model is able to learn salient
molecular features. In this work, we propose a method for measuring how well
the latent space of deep generative models is able to encode structural and
chemical features of molecular datasets by correlating latent space metrics
with metrics from the field of topological data analysis (TDA). We apply our
evaluation methodology to a VAE trained on SMILES strings and show that 3D
topology information is consistently encoded throughout the latent space of the
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chenthamarakshan_V/0/1/0/all/0/1"&gt;Vijil Chenthamarakshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ramamurthy_K/0/1/0/all/0/1"&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Decompose a Tensor with Group Structure. (arXiv:2106.02680v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.02680</id>
        <link href="http://arxiv.org/abs/2106.02680"/>
        <updated>2021-06-08T02:20:24.708Z</updated>
        <summary type="html"><![CDATA[In this work we study the orbit recovery problem, which is a natural
abstraction for the problem of recovering a planted signal from noisy
measurements under unknown group actions. Many important inverse problems in
statistics, engineering and the sciences fit into this framework. Prior work
has studied cases when the group is discrete and/or abelian. However
fundamentally new techniques are needed in order to handle more complex group
actions.

Our main result is a quasi-polynomial time algorithm to solve orbit recovery
over $SO(3)$ - i.e. the cryo-electron tomography problem which asks to recover
the three-dimensional structure of a molecule from noisy measurements of
randomly rotated copies of it. We analyze a variant of the frequency marching
heuristic in the framework of smoothed analysis. Our approach exploits the
layered structure of the invariant polynomials, and simultaneously yields a new
class of tensor decomposition algorithms that work in settings when the tensor
is not low-rank but rather where the factors are algebraically related to each
other by a group action.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Allen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Contextual Bandit Bake-off. (arXiv:1802.04064v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1802.04064</id>
        <link href="http://arxiv.org/abs/1802.04064"/>
        <updated>2021-06-08T02:20:24.699Z</updated>
        <summary type="html"><![CDATA[Contextual bandit algorithms are essential for solving many real-world
interactive machine learning problems. Despite multiple recent successes on
statistically and computationally efficient methods, the practical behavior of
these algorithms is still poorly understood. We leverage the availability of
large numbers of supervised learning datasets to empirically evaluate
contextual bandit algorithms, focusing on practical methods that learn by
relying on optimization oracles from supervised learning. We find that a recent
method (Foster et al., 2018) using optimism under uncertainty works the best
overall. A surprisingly close second is a simple greedy baseline that only
explores implicitly through the diversity of contexts, followed by a variant of
Online Cover (Agarwal et al., 2014) which tends to be more conservative but
robust to problem specification by design. Along the way, we also evaluate
various components of contextual bandit algorithm design such as loss
estimators. Overall, this is a thorough study and review of contextual bandit
methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1"&gt;Alberto Bietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Langford_J/0/1/0/all/0/1"&gt;John Langford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models. (arXiv:2106.02679v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02679</id>
        <link href="http://arxiv.org/abs/2106.02679"/>
        <updated>2021-06-08T02:20:24.693Z</updated>
        <summary type="html"><![CDATA[The advent of the transformer has sparked a quick growth in the size of
language models, far outpacing hardware improvements. (Dense) transformers are
expected to reach the trillion-parameter scale in the near future, for which
training requires thousands or even tens of thousands of GPUs. We investigate
the challenges of training at this scale and beyond on commercially available
hardware. In particular, we analyse the shortest possible training time for
different configurations of distributed training, leveraging empirical scaling
laws for language models to estimate the optimal (critical) batch size.
Contrary to popular belief, we find no evidence for a memory wall, and instead
argue that the real limitation -- other than the cost -- lies in the training
duration.

In addition to this analysis, we introduce two new methods, \textit{layered
gradient accumulation} and \textit{modular pipeline parallelism}, which
together cut the shortest training time by half. The methods also reduce data
movement, lowering the network requirement to a point where a fast InfiniBand
connection is not necessary. This increased network efficiency also improve on
the methods introduced with the ZeRO optimizer, reducing the memory usage to a
tiny fraction of the available GPU memory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1"&gt;Joel Lamy-Poirier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Neural Posterior and Likelihood Approximation. (arXiv:2102.06522v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06522</id>
        <link href="http://arxiv.org/abs/2102.06522"/>
        <updated>2021-06-08T02:20:24.668Z</updated>
        <summary type="html"><![CDATA[We introduce the sequential neural posterior and likelihood approximation
(SNPLA) algorithm. SNPLA is a normalizing flows-based algorithm for inference
in implicit models, and therefore is a simulation-based inference method that
only requires simulations from a generative model. SNPLA avoids Markov chain
Monte Carlo sampling and correction-steps of the parameter proposal function
that are introduced in similar methods, but that can be numerically unstable or
restrictive. By utilizing the reverse KL divergence, SNPLA manages to learn
both the likelihood and the posterior in a sequential manner. Over four
experiments, we show that SNPLA performs competitively when utilizing the same
number of model simulations as used in other methods, even though the inference
problem for SNPLA is more complex due to the joint learning of posterior and
likelihood function. Due to utilizing normalizing flows SNPLA generates
posterior draws much faster (4 orders of magnitude) than MCMC-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wiqvist_S/0/1/0/all/0/1"&gt;Samuel Wiqvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Frellsen_J/0/1/0/all/0/1"&gt;Jes Frellsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Picchini_U/0/1/0/all/0/1"&gt;Umberto Picchini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in Context Disambiguation. (arXiv:2104.10375v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10375</id>
        <link href="http://arxiv.org/abs/2104.10375"/>
        <updated>2021-06-08T02:20:24.658Z</updated>
        <summary type="html"><![CDATA[This paper presents the PALI team's winning system for SemEval-2021 Task 2:
Multilingual and Cross-lingual Word-in-Context Disambiguation. We fine-tune
XLM-RoBERTa model to solve the task of word in context disambiguation, i.e., to
determine whether the target word in the two contexts contains the same meaning
or not. In the implementation, we first specifically design an input tag to
emphasize the target word in the contexts. Second, we construct a new vector on
the fine-tuned embeddings from XLM-RoBERTa and feed it to a fully-connected
network to output the probability of whether the target word in the context has
the same meaning or not. The new vector is attained by concatenating the
embedding of the [CLS] token and the embeddings of the target word in the
contexts. In training, we explore several tricks, such as the Ranger optimizer,
data augmentation, and adversarial training, to improve the model prediction.
Consequently, we attain first place in all four cross-lingual tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shuyi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lianxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianping Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Beam Association for High Mobility mmWave Vehicular Networks: Lightweight Parallel Reinforcement Learning Approach. (arXiv:2005.00694v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00694</id>
        <link href="http://arxiv.org/abs/2005.00694"/>
        <updated>2021-06-08T02:20:24.658Z</updated>
        <summary type="html"><![CDATA[In intelligent transportation systems (ITS), vehicles are expected to feature
with advanced applications and services which demand ultra-high data rates and
low-latency communications. For that, the millimeter wave (mmWave)
communication has been emerging as a very promising solution. However,
incorporating the mmWave into ITS is particularly challenging due to the high
mobility of vehicles and the inherent sensitivity of mmWave beams to dynamic
blockages. This article addresses these problems by developing an optimal beam
association framework for mmWave vehicular networks under high mobility.
Specifically, we use the semi-Markov decision process to capture the dynamics
and uncertainty of the environment. The Q-learning algorithm is then often used
to find the optimal policy. However, Q-learning is notorious for its
slow-convergence. Instead of adopting deep reinforcement learning structures
(like most works in the literature), we leverage the fact that there are
usually multiple vehicles on the road to speed up the learning process. To that
end, we develop a lightweight yet very effective parallel Q-learning algorithm
to quickly obtain the optimal policy by simultaneously learning from various
vehicles. Extensive simulations demonstrate that our proposed solution can
increase the data rate by 47% and reduce the disconnection probability by 29%
compared to other solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1"&gt;Nguyen Van Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Diep N. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1"&gt;Dinh Thai Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1"&gt;Eryk Dutkiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Invariant Dictionary Learning. (arXiv:2007.07550v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07550</id>
        <link href="http://arxiv.org/abs/2007.07550"/>
        <updated>2021-06-08T02:20:24.657Z</updated>
        <summary type="html"><![CDATA[The dictionary learning problem concerns the task of representing data as
sparse linear sums drawn from a smaller collection of basic building blocks. In
application domains where such techniques are deployed, we frequently encounter
datasets where some form of symmetry or invariance is present. Motivated by
this observation, we develop a framework for learning dictionaries for data
under the constraint that the collection of basic building blocks remains
invariant under such symmetries. Our procedure for learning such dictionaries
relies on representing the symmetry as the action of a matrix group acting on
the data, and subsequently introducing a convex penalty function so as to
induce sparsity with respect to the collection of matrix group elements. Our
framework specializes to the convolutional dictionary learning problem when we
consider integer shifts. Using properties of positive semidefinite Hermitian
Toeplitz matrices, we develop an extension that learns dictionaries that are
invariant under continuous shifts. Our numerical experiments on synthetic data
and ECG data show that the incorporation of such symmetries as priors are most
valuable when the dataset has few data-points, or when the full range of
symmetries is inadequately expressed in the dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Soh_Y/0/1/0/all/0/1"&gt;Yong Sheng Soh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Learning in the Jungle. (arXiv:2008.00742v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00742</id>
        <link href="http://arxiv.org/abs/2008.00742"/>
        <updated>2021-06-08T02:20:24.643Z</updated>
        <summary type="html"><![CDATA[We study Byzantine collaborative learning, where $n$ nodes seek to
collectively learn from each others' local data. The data distribution may vary
from one node to another. No node is trusted, and $f < n$ nodes can behave
arbitrarily. We prove that collaborative learning is equivalent to a new form
of agreement, which we call averaging agreement. In this problem, nodes start
each with an initial vector and seek to approximately agree on a common vector,
which is close to the average of honest nodes' initial vectors. We present two
asynchronous solutions to averaging agreement, each we prove optimal according
to some dimension. The first, based on the minimum-diameter averaging, requires
$ n \geq 6f+1$, but achieves asymptotically the best-possible averaging
constant up to a multiplicative constant. The second, based on reliable
broadcast and coordinate-wise trimmed mean, achieves optimal Byzantine
resilience, i.e., $n \geq 3f+1$. Each of these algorithms induces an optimal
Byzantine collaborative learning protocol. In particular, our equivalence
yields new impossibility theorems on what any collaborative learning algorithm
can achieve in adversarial and heterogeneous environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+El_Mhamdi_E/0/1/0/all/0/1"&gt;El-Mahdi El-Mhamdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadkhani_S/0/1/0/all/0/1"&gt;Sadegh Farhadkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1"&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guirguis_A/0/1/0/all/0/1"&gt;Arsany Guirguis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1"&gt;L&amp;#xea; Nguy&amp;#xea;n Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Rouault&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conv-MPN: Convolutional Message Passing Neural Network for Structured Outdoor Architecture Reconstruction. (arXiv:1912.01756v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.01756</id>
        <link href="http://arxiv.org/abs/1912.01756"/>
        <updated>2021-06-08T02:20:24.637Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel message passing neural (MPN) architecture
Conv-MPN, which reconstructs an outdoor building as a planar graph from a
single RGB image. Conv-MPN is specifically designed for cases where nodes of a
graph have explicit spatial embedding. In our problem, nodes correspond to
building edges in an image. Conv-MPN is different from MPN in that 1) the
feature associated with a node is represented as a feature volume instead of a
1D vector; and 2) convolutions encode messages instead of fully connected
layers. Conv-MPN learns to select a true subset of nodes (i.e., building edges)
to reconstruct a building planar graph. Our qualitative and quantitative
evaluations over 2,000 buildings show that Conv-MPN makes significant
improvements over the existing fully neural solutions. We believe that the
paper has a potential to open a new line of graph neural network research for
structured geometry reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fuyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nauata_N/0/1/0/all/0/1"&gt;Nelson Nauata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1"&gt;Yasutaka Furukawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logic of Machine Learning. (arXiv:2006.09500v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09500</id>
        <link href="http://arxiv.org/abs/2006.09500"/>
        <updated>2021-06-08T02:20:24.635Z</updated>
        <summary type="html"><![CDATA[ML is approached from logic point of view as a problem of maximizing
consistency of a hypothesis in a context of a given training set. Nonjudgmental
logic (NjL) with modalities ``It appears that'', ``Assume that'' is introduced
to formalize and quantify the concepts of inconsistency. Two conjectures are
formulated. First, there are only 5 types of steps for all learners. Second,
any learner minimizes a criterion, which can be represented as a measure of
inconsistency in a semantic of NjL. Many popular ML algorithms (from
hierarchical clustering to k-NN and SVM) are shown to corroborate both
conjectures. In addition, it is demonstrated that NjL allows to formalize and
solve several general learning problems which are not considered as ML usually.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sapir_M/0/1/0/all/0/1"&gt;Marina Sapir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Singular Dynamic Mode Decompositions. (arXiv:2106.02639v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02639</id>
        <link href="http://arxiv.org/abs/2106.02639"/>
        <updated>2021-06-08T02:20:24.615Z</updated>
        <summary type="html"><![CDATA[This manuscript is aimed at addressing several long standing limitations of
dynamic mode decompositions in the application of Koopman analysis. Principle
among these limitations are the convergence of associated Dynamic Mode
Decomposition algorithms and the existence of Koopman modes. To address these
limitations, two major modifications are made, where Koopman operators are
removed from the analysis in light of Liouville operators (known as Koopman
generators in special cases), and these operators are shown to be compact for
certain pairs of Hilbert spaces selected separately as the domain and range of
the operator. While eigenfunctions are discarded in this analysis, a viable
reconstruction algorithm is still demonstrated, and the sacrifice of
eigenfunctions realizes the theoretical goals of DMD analysis that have yet to
be achieved in other contexts. The manuscript concludes with the description of
a Dynamic Mode Decomposition algorithm that converges when a dense collection
of occupation kernels, arising from the data, are leveraged in the analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rosenfeld_J/0/1/0/all/0/1"&gt;Joel A. Rosenfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kamalapurkar_R/0/1/0/all/0/1"&gt;Rushikesh Kamalapurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Regret Active learning. (arXiv:2104.02822v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02822</id>
        <link href="http://arxiv.org/abs/2104.02822"/>
        <updated>2021-06-08T02:20:24.601Z</updated>
        <summary type="html"><![CDATA[We develop an online learning algorithm for identifying unlabeled data points
that are most informative for training (i.e., active learning). By formulating
the active learning problem as the prediction with sleeping experts problem, we
provide a framework for identifying informative data with respect to any given
definition of informativeness. At the core of our work is an efficient
algorithm for sleeping experts that is tailored to achieve low regret on
predictable (easy) instances while remaining resilient to adversarial ones.
This stands in contrast to state-of-the-art active learning methods that are
overwhelmingly based on greedy selection, and hence cannot ensure good
performance across varying problem instances. We present empirical results
demonstrating that our method (i) instantiated with an informativeness measure
consistently outperforms its greedy counterpart and (ii) reliably outperforms
uniform sampling on real-world data sets and models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baykal_C/0/1/0/all/0/1"&gt;Cenk Baykal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1"&gt;Lucas Liebenwein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1"&gt;Dan Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Constant Approximation Algorithm for Sequential Random-Order No-Substitution k-Median Clustering. (arXiv:2102.04050v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04050</id>
        <link href="http://arxiv.org/abs/2102.04050"/>
        <updated>2021-06-08T02:20:24.598Z</updated>
        <summary type="html"><![CDATA[We study k-median clustering under the sequential no-substitution setting. In
this setting, a data stream is sequentially observed, and some of the points
are selected by the algorithm as cluster centers. However, a point can be
selected as a center only immediately after it is observed, before observing
the next point. In addition, a selected center cannot be substituted later. We
give the first algorithm for this setting that obtains a constant approximation
factor on the optimal risk under a random arrival order, an exponential
improvement over previous work. This is also the first constant approximation
guarantee that holds without any structural assumptions on the input data.
Moreover, the number of selected centers is only quasi-linear in k. Our
algorithm and analysis are based on a careful risk estimation that avoids
outliers, a new concept of a linear bin division, and a multiscale approach to
center selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1"&gt;Tom Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moshkovitz_M/0/1/0/all/0/1"&gt;Michal Moshkovitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabato_S/0/1/0/all/0/1"&gt;Sivan Sabato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Treatment Effects in Panels with General Intervention Patterns. (arXiv:2106.02780v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02780</id>
        <link href="http://arxiv.org/abs/2106.02780"/>
        <updated>2021-06-08T02:20:24.597Z</updated>
        <summary type="html"><![CDATA[The problem of causal inference with panel data is a central econometric
question. The following is a fundamental version of this problem: Let $M^*$ be
a low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix
$Z$ with entries in $\{0,1\}$ we observe the matrix $O$ with entries $O_{ij} :=
M^*_{ij} + E_{ij} + \mathcal{T}_{ij} Z_{ij}$ where $\mathcal{T}_{ij} $ are
unknown, heterogenous treatment effects. The problem requires we estimate the
average treatment effect $\tau^* := \sum_{ij} \mathcal{T}_{ij} Z_{ij} /
\sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach to
estimating $\tau^*$ when $Z$ places support on a single row. This paper extends
that framework to allow rate-optimal recovery of $\tau^*$ for general $Z$, thus
broadly expanding its applicability. Our guarantees are the first of their type
in this general setting. Computational experiments on synthetic and real-world
data show a substantial advantage over competing estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Farias_V/0/1/0/all/0/1"&gt;Vivek F. Farias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_A/0/1/0/all/0/1"&gt;Andrew A. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peng_T/0/1/0/all/0/1"&gt;Tianyi Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph Symmetrisation Bound on Channel Information Leakage under Blowfish Privacy. (arXiv:2007.05975v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05975</id>
        <link href="http://arxiv.org/abs/2007.05975"/>
        <updated>2021-06-08T02:20:24.589Z</updated>
        <summary type="html"><![CDATA[Blowfish privacy is a recent generalisation of differential privacy that
enables improved utility while maintaining privacy policies with semantic
guarantees, a factor that has driven the popularity of differential privacy in
computer science. This paper relates Blowfish privacy to an important measure
of privacy loss of information channels from the communications theory
community: min-entropy leakage. Symmetry in an input data neighbouring relation
is central to known connections between differential privacy and min-entropy
leakage. But while differential privacy exhibits strong symmetry, Blowfish
neighbouring relations correspond to arbitrary simple graphs owing to the
framework's flexible privacy policies. To bound the min-entropy leakage of
Blowfish-private mechanisms we organise our analysis over symmetrical
partitions corresponding to orbits of graph automorphism groups. A construction
meeting our bound with asymptotic equality demonstrates tightness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_T/0/1/0/all/0/1"&gt;Tobias Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Benjamin I. P. Rubinstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zuhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sanming Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial Robustness. (arXiv:2106.02734v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02734</id>
        <link href="http://arxiv.org/abs/2106.02734"/>
        <updated>2021-06-08T02:20:24.576Z</updated>
        <summary type="html"><![CDATA[We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck
as a regularizer for learning an adversarially robust deep neural network
classifier. We show that the HSIC bottleneck enhances robustness to adversarial
attacks both theoretically and experimentally. Our experiments on multiple
benchmark datasets and architectures demonstrate that incorporating an HSIC
bottleneck regularizer attains competitive natural accuracy and improves
adversarial robustness, both with and without adversarial examples during
training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zifeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jian_T/0/1/0/all/0/1"&gt;Tong Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masoomi_A/0/1/0/all/0/1"&gt;Aria Masoomi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1"&gt;Stratis Ioannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dy_J/0/1/0/all/0/1"&gt;Jennifer Dy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandwidth-based Step-Sizes for Non-Convex Stochastic Optimization. (arXiv:2106.02888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02888</id>
        <link href="http://arxiv.org/abs/2106.02888"/>
        <updated>2021-06-08T02:20:24.569Z</updated>
        <summary type="html"><![CDATA[Many popular learning-rate schedules for deep neural networks combine a
decaying trend with local perturbations that attempt to escape saddle points
and bad local minima. We derive convergence guarantees for bandwidth-based
step-sizes, a general class of learning-rates that are allowed to vary in a
banded region. This framework includes cyclic and non-monotonic step-sizes for
which no theoretical guarantees were previously known. We provide worst-case
guarantees for SGD on smooth non-convex problems under several bandwidth-based
step sizes, including stagewise $1/\sqrt{t}$ and the popular step-decay
(constant and then drop by a constant), which is also shown to be optimal.
Moreover, we show that its momentum variant (SGDM) converges as fast as SGD
with the bandwidth-based step-decay step-size. Finally, we propose some novel
step-size schemes in the bandwidth-based family and verify their efficiency on
several deep neural network training tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansson_M/0/1/0/all/0/1"&gt;Mikael Johansson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attacker Behaviour Profiling using Stochastic Ensemble of Hidden Markov Models. (arXiv:1905.11824v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.11824</id>
        <link href="http://arxiv.org/abs/1905.11824"/>
        <updated>2021-06-08T02:20:24.551Z</updated>
        <summary type="html"><![CDATA[Cyber threat intelligence is one of the emerging areas of focus in
information security. Much of the recent work has focused on rule-based methods
and detection of network attacks using Intrusion Detection algorithms. In this
paper we propose a framework for inspecting and modelling the behavioural
aspect of an attacker to obtain better insight predictive power on his future
actions. For modelling we propose a novel semi-supervised algorithm called
Fusion Hidden Markov Model (FHMM) which is more robust to noise, requires
comparatively less training time, and utilizes the benefits of ensemble
learning to better model temporal relationships in data. This paper evaluates
the performances of FHMM and compares it with both traditional algorithms like
Markov Chain, Hidden Markov Model (HMM) and recently developed Deep Recurrent
Neural Network (Deep RNN) architectures. We conduct the experiments on dataset
consisting of real data attacks on a Cowrie honeypot system. FHMM provides
accuracy comparable to deep RNN architectures at significant lower training
time. Given these experimental results, we recommend using FHMM for modelling
discrete temporal data for significantly faster training and better performance
than existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1"&gt;Soham Deshmukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rade_R/0/1/0/all/0/1"&gt;Rahul Rade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazi_D/0/1/0/all/0/1"&gt;Dr. Faruk Kazi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedNL: Making Newton-Type Methods Applicable to Federated Learning. (arXiv:2106.02969v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02969</id>
        <link href="http://arxiv.org/abs/2106.02969"/>
        <updated>2021-06-08T02:20:24.536Z</updated>
        <summary type="html"><![CDATA[Inspired by recent work of Islamov et al (2021), we propose a family of
Federated Newton Learn (FedNL) methods, which we believe is a marked step in
the direction of making second-order methods applicable to FL. In contrast to
the aforementioned work, FedNL employs a different Hessian learning technique
which i) enhances privacy as it does not rely on the training data to be
revealed to the coordinating server, ii) makes it applicable beyond generalized
linear models, and iii) provably works with general contractive compression
operators for compressing the local Hessians, such as Top-$K$ or Rank-$R$,
which are vastly superior in practice. Notably, we do not need to rely on error
feedback for our methods to work with contractive compressors. Moreover, we
develop FedNL-PP, FedNL-CR and FedNL-LS, which are variants of FedNL that
support partial participation, and globalization via cubic regularization and
line search, respectively, and FedNL-BC, which is a variant that can further
benefit from bidirectional compression of gradients and models, i.e., smart
uplink gradient and smart downlink model compression. We prove local
convergence rates that are independent of the condition number, the number of
training data points, and compression variance. Our communication efficient
Hessian learning technique provably learns the Hessian at the optimum. Finally,
we perform a variety of numerical experiments that show that our FedNL methods
have state-of-the-art communication complexity when compared to key baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safaryan_M/0/1/0/all/0/1"&gt;Mher Safaryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islamov_R/0/1/0/all/0/1"&gt;Rustem Islamov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xun Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stochastic Behaviour from Aggregate Data. (arXiv:2002.03513v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03513</id>
        <link href="http://arxiv.org/abs/2002.03513"/>
        <updated>2021-06-08T02:20:24.529Z</updated>
        <summary type="html"><![CDATA[Learning nonlinear dynamics from aggregate data is a challenging problem
because the full trajectory of each individual is not available, namely, the
individual observed at one time may not be observed at the next time point, or
the identity of individual is unavailable. This is in sharp contrast to
learning dynamics with full trajectory data, on which the majority of existing
methods are based. We propose a novel method using the weak form of Fokker
Planck Equation (FPE) -- a partial differential equation -- to describe the
density evolution of data in a sampled form, which is then combined with
Wasserstein generative adversarial network (WGAN) in the training process. In
such a sample-based framework we are able to learn the nonlinear dynamics from
aggregate data without explicitly solving the partial differential equation
(PDE) FPE. We demonstrate our approach in the context of a series of synthetic
and real-world data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaojun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Haomin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constructive Universal High-Dimensional Distribution Generation through Deep ReLU Networks. (arXiv:2006.16664v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16664</id>
        <link href="http://arxiv.org/abs/2006.16664"/>
        <updated>2021-06-08T02:20:24.525Z</updated>
        <summary type="html"><![CDATA[We present an explicit deep neural network construction that transforms
uniformly distributed one-dimensional noise into an arbitrarily close
approximation of any two-dimensional Lipschitz-continuous target distribution.
The key ingredient of our design is a generalization of the "space-filling"
property of sawtooth functions discovered in (Bailey & Telgarsky, 2018). We
elicit the importance of depth - in our neural network construction - in
driving the Wasserstein distance between the target distribution and the
approximation realized by the network to zero. An extension to output
distributions of arbitrary dimension is outlined. Finally, we show that the
proposed construction does not incur a cost - in terms of error measured in
Wasserstein-distance - relative to generating $d$-dimensional target
distributions from $d$ independent random variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perekrestenko_D/0/1/0/all/0/1"&gt;Dmytro Perekrestenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1"&gt;Stephan M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolcskei_H/0/1/0/all/0/1"&gt;Helmut B&amp;#xf6;lcskei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph to Graphs Framework for Retrosynthesis Prediction. (arXiv:2003.12725v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12725</id>
        <link href="http://arxiv.org/abs/2003.12725"/>
        <updated>2021-06-08T02:20:24.516Z</updated>
        <summary type="html"><![CDATA[A fundamental problem in computational chemistry is to find a set of
reactants to synthesize a target molecule, a.k.a. retrosynthesis prediction.
Existing state-of-the-art methods rely on matching the target molecule with a
large set of reaction templates, which are very computationally expensive and
also suffer from the problem of coverage. In this paper, we propose a novel
template-free approach called G2Gs by transforming a target molecular graph
into a set of reactant molecular graphs. G2Gs first splits the target molecular
graph into a set of synthons by identifying the reaction centers, and then
translates the synthons to the final reactant graphs via a variational graph
translation framework. Experimental results show that G2Gs significantly
outperforms existing template-free approaches by up to 63% in terms of the
top-1 accuracy and achieves a performance close to that of state-of-the-art
template based approaches, but does not require domain knowledge and is much
more scalable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chence Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minkai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[k-Mixup Regularization for Deep Learning via Optimal Transport. (arXiv:2106.02933v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02933</id>
        <link href="http://arxiv.org/abs/2106.02933"/>
        <updated>2021-06-08T02:20:24.485Z</updated>
        <summary type="html"><![CDATA[Mixup is a popular regularization technique for training deep neural networks
that can improve generalization and increase adversarial robustness. It
perturbs input training data in the direction of other randomly-chosen
instances in the training set. To better leverage the structure of the data, we
extend mixup to \emph{$k$-mixup} by perturbing $k$-batches of training points
in the direction of other $k$-batches using displacement interpolation,
interpolation under the Wasserstein metric. We demonstrate theoretically and in
simulations that $k$-mixup preserves cluster and manifold structures, and we
extend theory studying efficacy of standard mixup. Our empirical results show
that training with $k$-mixup further improves generalization and robustness on
benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greenewald_K/0/1/0/all/0/1"&gt;Kristjan Greenewald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1"&gt;Anming Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1"&gt;Mikhail Yurochkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1"&gt;Justin Solomon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1"&gt;Edward Chien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[W-RST: Towards a Weighted RST-style Discourse Framework. (arXiv:2106.02658v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02658</id>
        <link href="http://arxiv.org/abs/2106.02658"/>
        <updated>2021-06-08T02:20:24.469Z</updated>
        <summary type="html"><![CDATA[Aiming for a better integration of data-driven and linguistically-inspired
approaches, we explore whether RST Nuclearity, assigning a binary assessment of
importance between text segments, can be replaced by automatically generated,
real-valued scores, in what we call a Weighted-RST framework. In particular, we
find that weighted discourse trees from auxiliary tasks can benefit key NLP
downstream applications, compared to nuclearity-centered approaches. We further
show that real-valued importance distributions partially and interestingly
align with the assessment and uncertainty of human annotators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1"&gt;Patrick Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wen Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PID-GAN: A GAN Framework based on a Physics-informed Discriminator for Uncertainty Quantification with Physics. (arXiv:2106.02993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02993</id>
        <link href="http://arxiv.org/abs/2106.02993"/>
        <updated>2021-06-08T02:20:24.456Z</updated>
        <summary type="html"><![CDATA[As applications of deep learning (DL) continue to seep into critical
scientific use-cases, the importance of performing uncertainty quantification
(UQ) with DL has become more pressing than ever before. In scientific
applications, it is also important to inform the learning of DL models with
knowledge of physics of the problem to produce physically consistent and
generalized solutions. This is referred to as the emerging field of
physics-informed deep learning (PIDL). We consider the problem of developing
PIDL formulations that can also perform UQ. To this end, we propose a novel
physics-informed GAN architecture, termed PID-GAN, where the knowledge of
physics is used to inform the learning of both the generator and discriminator
models, making ample use of unlabeled data instances. We show that our proposed
PID-GAN framework does not suffer from imbalance of generator gradients from
multiple loss terms as compared to state-of-the-art. We also empirically
demonstrate the efficacy of our proposed framework on a variety of case studies
involving benchmark physics-based PDEs as well as imperfect physics. All the
code and datasets used in this study have been made available on this link :
https://github.com/arkadaw9/PID-GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daw_A/0/1/0/all/0/1"&gt;Arka Daw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maruf_M/0/1/0/all/0/1"&gt;M. Maruf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpatne_A/0/1/0/all/0/1"&gt;Anuj Karpatne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Infomax Adversarial Learning for Treatment Effect Estimation with Networked Observational Data. (arXiv:2106.02881v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02881</id>
        <link href="http://arxiv.org/abs/2106.02881"/>
        <updated>2021-06-08T02:20:24.434Z</updated>
        <summary type="html"><![CDATA[Treatment effect estimation from observational data is a critical research
topic across many domains. The foremost challenge in treatment effect
estimation is how to capture hidden confounders. Recently, the growing
availability of networked observational data offers a new opportunity to deal
with the issue of hidden confounders. Unlike networked data in traditional
graph learning tasks, such as node classification and link detection, the
networked data under the causal inference problem has its particularity, i.e.,
imbalanced network structure. In this paper, we propose a Graph Infomax
Adversarial Learning (GIAL) model for treatment effect estimation, which makes
full use of the network structure to capture more information by recognizing
the imbalance in network structure. We evaluate the performance of our GIAL
model on two benchmark datasets, and the results demonstrate superiority over
the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathbun_S/0/1/0/all/0/1"&gt;Stephen L. Rathbun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Attribute-Aligned Strategy for Learning Speech Representation. (arXiv:2106.02810v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02810</id>
        <link href="http://arxiv.org/abs/2106.02810"/>
        <updated>2021-06-08T02:20:24.391Z</updated>
        <summary type="html"><![CDATA[Advancement in speech technology has brought convenience to our life.
However, the concern is on the rise as speech signal contains multiple personal
attributes, which would lead to either sensitive information leakage or bias
toward decision. In this work, we propose an attribute-aligned learning
strategy to derive speech representation that can flexibly address these issues
by attribute-selection mechanism. Specifically, we propose a
layered-representation variational autoencoder (LR-VAE), which factorizes
speech representation into attribute-sensitive nodes, to derive an
identity-free representation for speech emotion recognition (SER), and an
emotionless representation for speaker verification (SV). Our proposed method
achieves competitive performances on identity-free SER and a better performance
on emotionless SV, comparing to the current state-of-the-art method of using
adversarial learning applied on a large emotion corpora, the MSP-Podcast. Also,
our proposed learning strategy reduces the model and training process needed to
achieve multiple privacy-preserving tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Lin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Su_B/0/1/0/all/0/1"&gt;Bo-Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Y.-W. Peter Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chi-Chun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Time Attention Networks for Irregularly Sampled Time Series. (arXiv:2101.10318v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10318</id>
        <link href="http://arxiv.org/abs/2101.10318"/>
        <updated>2021-06-08T02:20:24.112Z</updated>
        <summary type="html"><![CDATA[Irregular sampling occurs in many time series modeling applications where it
presents a significant challenge to standard deep learning models. This work is
motivated by the analysis of physiological time series data in electronic
health records, which are sparse, irregularly sampled, and multivariate. In
this paper, we propose a new deep learning framework for this setting that we
call Multi-Time Attention Networks. Multi-Time Attention Networks learn an
embedding of continuous-time values and use an attention mechanism to produce a
fixed-length representation of a time series containing a variable number of
observations. We investigate the performance of this framework on interpolation
and classification tasks using multiple datasets. Our results show that the
proposed approach performs as well or better than a range of baseline and
recently proposed models while offering significantly faster training times
than current state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Satya Narayan Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marlin_B/0/1/0/all/0/1"&gt;Benjamin M. Marlin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HMRL: Hyper-Meta Learning for Sparse Reward Reinforcement Learning Problem. (arXiv:2002.04238v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04238</id>
        <link href="http://arxiv.org/abs/2002.04238"/>
        <updated>2021-06-08T02:20:24.070Z</updated>
        <summary type="html"><![CDATA[In spite of the success of existing meta reinforcement learning methods, they
still have difficulty in learning a meta policy effectively for RL problems
with sparse reward. In this respect, we develop a novel meta reinforcement
learning framework called Hyper-Meta RL(HMRL), for sparse reward RL problems.
It is consisted with three modules including the cross-environment meta state
embedding module which constructs a common meta state space to adapt to
different environments; the meta state based environment-specific meta reward
shaping which effectively extends the original sparse reward trajectory by
cross-environmental knowledge complementarity and as a consequence the meta
policy achieves better generalization and efficiency with the shaped meta
reward. Experiments with sparse-reward environments show the superiority of
HMRL on both transferability and policy learning efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1"&gt;Yun Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiangfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1"&gt;Bo Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Leakage: The Role of Information Complexity in Privacy Leakage. (arXiv:2106.02818v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02818</id>
        <link href="http://arxiv.org/abs/2106.02818"/>
        <updated>2021-06-08T02:20:24.063Z</updated>
        <summary type="html"><![CDATA[We study the role of information complexity in privacy leakage about an
attribute of an adversary's interest, which is not known a priori to the system
designer. Considering the supervised representation learning setup and using
neural networks to parameterize the variational bounds of information
quantities, we study the impact of the following factors on the amount of
information leakage: information complexity regularizer weight, latent space
dimension, the cardinalities of the known utility and unknown sensitive
attribute sets, the correlation between utility and sensitive attributes, and a
potential bias in a sensitive attribute of adversary's interest. We conduct
extensive experiments on Colored-MNIST and CelebA datasets to evaluate the
effect of information complexity on the amount of intrinsic leakage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atashin_A/0/1/0/all/0/1"&gt;Amir Ahooye Atashin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razeghi_B/0/1/0/all/0/1"&gt;Behrooz Razeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz G&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voloshynovskiy_S/0/1/0/all/0/1"&gt;Slava Voloshynovskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Bandits with Unknown Graph Structure. (arXiv:2106.02988v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02988</id>
        <link href="http://arxiv.org/abs/2106.02988"/>
        <updated>2021-06-08T02:20:24.056Z</updated>
        <summary type="html"><![CDATA[In causal bandit problems, the action set consists of interventions on
variables of a causal graph. Several researchers have recently studied such
bandit problems and pointed out their practical applications. However, all
existing works rely on a restrictive and impractical assumption that the
learner is given full knowledge of the causal graph structure upfront. In this
paper, we develop novel causal bandit algorithms without knowing the causal
graph. Our algorithms work well for causal trees, causal forests and a general
class of causal graphs. The regret guarantees of our algorithms greatly improve
upon those of standard multi-armed bandit (MAB) algorithms under mild
conditions. Lastly, we prove our mild conditions are necessary: without them
one cannot do better than standard MAB bandit algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yangyi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meisami_A/0/1/0/all/0/1"&gt;Amirhossein Meisami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tewari_A/0/1/0/all/0/1"&gt;Ambuj Tewari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Numerical Composition of Differential Privacy. (arXiv:2106.02848v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.02848</id>
        <link href="http://arxiv.org/abs/2106.02848"/>
        <updated>2021-06-08T02:20:24.051Z</updated>
        <summary type="html"><![CDATA[We give a fast algorithm to optimally compose privacy guarantees of
differentially private (DP) algorithms to arbitrary accuracy. Our method is
based on the notion of privacy loss random variables to quantify the privacy
loss of DP algorithms. The running time and memory needed for our algorithm to
approximate the privacy curve of a DP algorithm composed with itself $k$ times
is $\tilde{O}(\sqrt{k})$. This improves over the best prior method by Koskela
et al. (2020) which requires $\tilde{\Omega}(k^{1.5})$ running time. We
demonstrate the utility of our algorithm by accurately computing the privacy
loss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm
speeds up the privacy computations by a few orders of magnitude compared to
prior work, while maintaining similar accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1"&gt;Sivakanth Gopi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yin Tat Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1"&gt;Lukas Wutschitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Fourier Features for Multi-DimensionalSpatial Positional Encoding. (arXiv:2106.02795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02795</id>
        <link href="http://arxiv.org/abs/2106.02795"/>
        <updated>2021-06-08T02:20:24.044Z</updated>
        <summary type="html"><![CDATA[Attentional mechanisms are order-invariant. Positional encoding is a crucial
component to allow attention-based deep model architectures such as Transformer
to address sequences or images where the position of information matters. In
this paper, we propose a novel positional encoding method based on learnable
Fourier features. Instead of hard-coding each position as a token or a vector,
we represent each position, which can be multi-dimensional, as a trainable
encoding based on learnable Fourier feature mapping, modulated with a
multi-layer perceptron. The representation is particularly advantageous for a
spatial multi-dimensional position, e.g., pixel positions on an image, where
$L_2$ distances or more complex positional relationships need to be captured.
Our experiments based on several public benchmark tasks show that our learnable
Fourier feature representation for multi-dimensional positional encoding
outperforms existing methods by both improving the accuracy and allowing faster
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1"&gt;Si Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerated Learning with Robustness to Adversarial Regressors. (arXiv:2005.01529v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01529</id>
        <link href="http://arxiv.org/abs/2005.01529"/>
        <updated>2021-06-08T02:20:24.038Z</updated>
        <summary type="html"><![CDATA[High order momentum-based parameter update algorithms have seen widespread
applications in training machine learning models. Recently, connections with
variational approaches have led to the derivation of new learning algorithms
with accelerated learning guarantees. Such methods however, have only
considered the case of static regressors. There is a significant need for
parameter update algorithms which can be proven stable in the presence of
adversarial time-varying regressors, as is commonplace in control theory. In
this paper, we propose a new discrete time algorithm which 1) provides
stability and asymptotic convergence guarantees in the presence of adversarial
regressors by leveraging insights from adaptive control theory and 2) provides
non-asymptotic accelerated learning guarantees leveraging insights from convex
optimization. In particular, our algorithm reaches an $\epsilon$ sub-optimal
point in at most $\tilde{\mathcal{O}}(1/\sqrt{\epsilon})$ iterations when
regressors are constant - matching lower bounds due to Nesterov of
$\Omega(1/\sqrt{\epsilon})$, up to a $\log(1/\epsilon)$ factor and provides
guaranteed bounds for stability when regressors are time-varying. We provide
numerical experiments for a variant of Nesterov's provably hard convex
optimization problem with time-varying regressors, as well as the problem of
recovering an image with a time-varying blur and noise using streaming data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gaudio_J/0/1/0/all/0/1"&gt;Joseph E. Gaudio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Annaswamy_A/0/1/0/all/0/1"&gt;Anuradha M. Annaswamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Moreu_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; M. Moreu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bolender_M/0/1/0/all/0/1"&gt;Michael A. Bolender&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gibson_T/0/1/0/all/0/1"&gt;Travis E. Gibson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Same State, Different Task: Continual Reinforcement Learning without Interference. (arXiv:2106.02940v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02940</id>
        <link href="http://arxiv.org/abs/2106.02940"/>
        <updated>2021-06-08T02:20:24.032Z</updated>
        <summary type="html"><![CDATA[Continual Learning (CL) considers the problem of training an agent
sequentially on a set of tasks while seeking to retain performance on all
previous tasks. A key challenge in CL is catastrophic forgetting, which arises
when performance on a previously mastered task is reduced when learning a new
task. While a variety of methods exist to combat forgetting, in some cases
tasks are fundamentally incompatible with each other and thus cannot be learnt
by a single policy. This can occur, in reinforcement learning (RL) when an
agent may be rewarded for achieving different goals from the same observation.
In this paper we formalize this ``interference'' as distinct from the problem
of forgetting. We show that existing CL methods based on single neural network
predictors with shared replay buffers fail in the presence of interference.
Instead, we propose a simple method, OWL, to address this challenge. OWL learns
a factorized policy, using shared feature extraction layers, but separate
heads, each specializing on a new task. The separate heads in OWL are used to
prevent interference. At test time, we formulate policy selection as a
multi-armed bandit problem, and show it is possible to select the best policy
for an unknown task using feedback from the environment. The use of bandit
algorithms allows the OWL agent to constructively re-use different continually
learnt policies at different times during an episode. We show in multiple RL
environments that existing replay based CL methods fail, while OWL is able to
achieve close to optimal performance when training sequentially.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kessler_S/0/1/0/all/0/1"&gt;Samuel Kessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1"&gt;Philip Ball&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen J. Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Stochastic Linear Contextual Bandits Under Adversarial Attacks. (arXiv:2106.02978v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02978</id>
        <link href="http://arxiv.org/abs/2106.02978"/>
        <updated>2021-06-08T02:20:24.025Z</updated>
        <summary type="html"><![CDATA[Stochastic linear contextual bandit algorithms have substantial applications
in practice, such as recommender systems, online advertising, clinical trials,
etc. Recent works show that optimal bandit algorithms are vulnerable to
adversarial attacks and can fail completely in the presence of attacks.
Existing robust bandit algorithms only work for the non-contextual setting
under the attack of rewards and cannot improve the robustness in the general
and popular contextual bandit environment. In addition, none of the existing
methods can defend against attacked context. In this work, we provide the first
robust bandit algorithm for stochastic linear contextual bandit setting under a
fully adaptive and omniscient attack. Our algorithm not only works under the
attack of rewards, but also under attacked context. Moreover, it does not need
any information about the attack budget or the particular form of the attack.
We provide theoretical guarantees for our proposed algorithm and show by
extensive experiments that our proposed algorithm significantly improves the
robustness against various kinds of popular attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sharpnack_J/0/1/0/all/0/1"&gt;James Sharpnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising Word Embeddings by Averaging in a Shared Space. (arXiv:2106.02954v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02954</id>
        <link href="http://arxiv.org/abs/2106.02954"/>
        <updated>2021-06-08T02:20:24.001Z</updated>
        <summary type="html"><![CDATA[We introduce a new approach for smoothing and improving the quality of word
embeddings. We consider a method of fusing word embeddings that were trained on
the same corpus but with different initializations. We project all the models
to a shared vector space using an efficient implementation of the Generalized
Procrustes Analysis (GPA) procedure, previously used in multilingual word
translation. Our word representation demonstrates consistent improvements over
the raw models as well as their simplistic average, on a range of tasks. As the
new representations are more stable and reliable, there is a noticeable
improvement in rare word evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1"&gt;Avi Caciularu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1"&gt;Ido Dagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1"&gt;Jacob Goldberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural dSCA: demixing multimodal interaction among brain areas during naturalistic experiments. (arXiv:2106.02948v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.02948</id>
        <link href="http://arxiv.org/abs/2106.02948"/>
        <updated>2021-06-08T02:20:23.981Z</updated>
        <summary type="html"><![CDATA[Multi-regional interaction among neuronal populations underlies the brain's
processing of rich sensory information in our daily lives. Recent neuroscience
and neuroimaging studies have increasingly used naturalistic stimuli and
experimental design to identify such realistic sensory computation in the
brain. However, existing methods for cross-areal interaction analysis with
dimensionality reduction, such as reduced-rank regression and canonical
correlation analysis, have limited applicability and interpretability in
naturalistic settings because they usually do not appropriately 'demix' neural
interactions into those associated with different types of task parameters or
stimulus features (e.g., visual or audio). In this paper, we develop a new
method for cross-areal interaction analysis that uses the rich task or stimulus
parameters to reveal how and what types of information are shared by different
neural populations. The proposed neural demixed shared component analysis
combines existing dimensionality reduction methods with a practical neural
network implementation of functional analysis of variance with latent
variables, thereby efficiently demixing nonlinear effects of continuous and
multimodal stimuli. We also propose a simplifying alternative under the
assumptions of linear effects and unimodal stimuli. To demonstrate our methods,
we analyzed two human neuroimaging datasets of participants watching
naturalistic videos of movies and dance movements. The results demonstrate that
our methods provide new insights into multi-regional interaction in the brain
during naturalistic sensory inputs, which cannot be captured by conventional
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Takagi_Y/0/1/0/all/0/1"&gt;Yu Takagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hunt_L/0/1/0/all/0/1"&gt;Laurence T. Hunt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ohata_R/0/1/0/all/0/1"&gt;Ryu Ohata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Imamizu_H/0/1/0/all/0/1"&gt;Hiroshi Imamizu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hirayama_J/0/1/0/all/0/1"&gt;Jun-ichiro Hirayama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Temporal Graph Neural Network for Trajectory Prediction. (arXiv:2106.02930v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02930</id>
        <link href="http://arxiv.org/abs/2106.02930"/>
        <updated>2021-06-08T02:20:23.942Z</updated>
        <summary type="html"><![CDATA[An effective understanding of the contextual environment and accurate motion
forecasting of surrounding agents is crucial for the development of autonomous
vehicles and social mobile robots. This task is challenging since the behavior
of an autonomous agent is not only affected by its own intention, but also by
the static environment and surrounding dynamically interacting agents. Previous
works focused on utilizing the spatial and temporal information in time domain
while not sufficiently taking advantage of the cues in frequency domain. To
this end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which
can capture inter-agent correlations and temporal dependency simultaneously in
frequency domain in addition to time domain. SpecTGNN operates on both an agent
graph with dynamic state information and an environment graph with the features
extracted from context images in two streams. The model integrates graph
Fourier transform, spectral graph convolution and temporal gated convolution to
encode history information and forecast future trajectories. Moreover, we
incorporate a multi-head spatio-temporal attention mechanism to mitigate the
effect of error propagation in a long time horizon. We demonstrate the
performance of SpecTGNN on two public trajectory prediction benchmark datasets,
which achieves state-of-the-art performance in terms of prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1"&gt;Defu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hengbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of fake news on CoViD-19 on Web Search Engines. (arXiv:2103.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11804</id>
        <link href="http://arxiv.org/abs/2103.11804"/>
        <updated>2021-06-08T02:20:23.924Z</updated>
        <summary type="html"><![CDATA[In early January 2020, after China reported the first cases of the new
coronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully
accurate information has started spreading faster than the virus itself.
Alongside this pandemic, people have experienced a parallel infodemic, i.e., an
overabundance of information, some of which misleading or even harmful, that
has widely spread around the globe. Although Social Media are increasingly
being used as information source, Web Search Engines, like Google or Yahoo!,
still represent a powerful and trustworthy resource for finding information on
the Web. This is due to their capability to capture the largest amount of
information, helping users quickly identify the most relevant, useful, although
not always the most reliable, results for their search queries. This study aims
to detect potential misleading and fake contents by capturing and analysing
textual information, which flow through Search Engines. By using a real-world
dataset associated with recent CoViD-19 pandemic, we first apply re-sampling
techniques for class imbalance, then we use existing Machine Learning
algorithms for classification of not reliable news. By extracting lexical and
host-based features of associated Uniform Resource Locators (URLs) for news
articles, we show that the proposed methods, so common in phishing and
malicious URLs detection, can improve the efficiency and performance of
classifiers. Based on these findings, we suggest that the use of both textual
and URLs features can improve the effectiveness of fake news detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzeo_V/0/1/0/all/0/1"&gt;V. Mazzeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapisarda_A/0/1/0/all/0/1"&gt;A. Rapisarda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giuffrida_G/0/1/0/all/0/1"&gt;G. Giuffrida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIVERSE: Bayesian Data IntegratiVE learning for precise drug ResponSE prediction. (arXiv:2104.00520v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00520</id>
        <link href="http://arxiv.org/abs/2104.00520"/>
        <updated>2021-06-08T02:20:23.913Z</updated>
        <summary type="html"><![CDATA[Detecting predictive biomarkers from multi-omics data is important for
precision medicine, to improve diagnostics of complex diseases and for better
treatments. This needs substantial experimental efforts that are made difficult
by the heterogeneity of cell lines and huge cost. An effective solution is to
build a computational model over the diverse omics data, including genomic,
molecular, and environmental information. However, choosing informative and
reliable data sources from among the different types of data is a challenging
problem. We propose DIVERSE, a framework of Bayesian importance-weighted tri-
and bi-matrix factorization(DIVERSE3 or DIVERSE2) to predict drug responses
from data of cell lines, drugs, and gene interactions. DIVERSE integrates the
data sources systematically, in a step-wise manner, examining the importance of
each added data set in turn. More specifically, we sequentially integrate five
different data sets, which have not all been combined in earlier bioinformatic
methods for predicting drug responses. Empirical experiments show that DIVERSE
clearly outperformed five other methods including three state-of-the-art
approaches, under cross-validation, particularly in out-of-matrix prediction,
which is closer to the setting of real use cases and more challenging than
simpler in-matrix prediction. Additionally, case studies for discovering new
drugs further confirmed the performance advantage of DIVERSE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Paltun_B/0/1/0/all/0/1"&gt;Bet&amp;#xfc;l G&amp;#xfc;ven&amp;#xe7; Paltun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mamitsuka_H/0/1/0/all/0/1"&gt;Hiroshi Mamitsuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syndicated Bandits: A Framework for Auto Tuning Hyper-parameters in Contextual Bandit Algorithms. (arXiv:2106.02979v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02979</id>
        <link href="http://arxiv.org/abs/2106.02979"/>
        <updated>2021-06-08T02:20:23.906Z</updated>
        <summary type="html"><![CDATA[The stochastic contextual bandit problem, which models the trade-off between
exploration and exploitation, has many real applications, including recommender
systems, online advertising and clinical trials. As many other machine learning
algorithms, contextual bandit algorithms often have one or more
hyper-parameters. As an example, in most optimal stochastic contextual bandit
algorithms, there is an unknown exploration parameter which controls the
trade-off between exploration and exploitation. A proper choice of the
hyper-parameters is essential for contextual bandit algorithms to perform well.
However, it is infeasible to use offline tuning methods to select
hyper-parameters in contextual bandit environment since there is no
pre-collected dataset and the decisions have to be made in real time. To tackle
this problem, we first propose a two-layer bandit structure for auto tuning the
exploration parameter and further generalize it to the Syndicated Bandits
framework which can learn multiple hyper-parameters dynamically in contextual
bandit environment. We show our Syndicated Bandits framework can achieve the
optimal regret upper bounds and is general enough to handle the tuning tasks in
many popular contextual bandit algorithms, such as LinUCB, LinTS, UCB-GLM, etc.
Experiments on both synthetic and real datasets validate the effectiveness of
our proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi-Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sharpnack_J/0/1/0/all/0/1"&gt;James Sharpnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Privacy Filters and Odometers with R\'enyi Differential Privacy and Applications to Differentially Private Deep Learning. (arXiv:2103.01379v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01379</id>
        <link href="http://arxiv.org/abs/2103.01379"/>
        <updated>2021-06-08T02:20:23.899Z</updated>
        <summary type="html"><![CDATA[Differential Privacy (DP) is the leading approach to privacy preserving deep
learning. As such, there are multiple efforts to provide drop-in integration of
DP into popular frameworks. These efforts, which add noise to each gradient
computation to make it DP, rely on composition theorems to bound the total
privacy loss incurred over this sequence of DP computations.

However, existing composition theorems present a tension between efficiency
and flexibility. Most theorems require all computations in the sequence to have
a predefined DP parameter, called the privacy budget. This prevents the design
of training algorithms that adapt the privacy budget on the fly, or that
terminate early to reduce the total privacy loss. Alternatively, the few
existing composition results for adaptive privacy budgets provide complex
bounds on the privacy loss, with constants too large to be practical.

In this paper, we study DP composition under adaptive privacy budgets through
the lens of R\'enyi Differential Privacy, proving a simpler composition theorem
with smaller constants, making it practical enough to use in algorithm design.
We demonstrate two applications of this theorem for DP deep learning: adapting
the noise or batch size online to improve a model's accuracy within a fixed
total privacy loss, and stopping early when fine-tuning a model to reduce total
privacy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lecuyer_M/0/1/0/all/0/1"&gt;Mathias L&amp;#xe9;cuyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IM-META: Influence Maximization Using Node Metadata in Networks With Unknown Topology. (arXiv:2106.02926v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.02926</id>
        <link href="http://arxiv.org/abs/2106.02926"/>
        <updated>2021-06-08T02:20:23.893Z</updated>
        <summary type="html"><![CDATA[In real-world applications of influence maximization (IM), the network
structure is often unknown. In this case, we may identify the most influential
seed nodes by exploring only a part of the underlying network given a small
budget for node queries. Motivated by the fact that collecting node metadata is
more cost-effective than investigating the relationship between nodes via
queried nodes, we develop IM-META, an end-to-end solution to IM in networks
with unknown topology by retrieving information from both queries and node
metadata. However, using such metadata to aid the IM process is not without
risk due to the noisy nature of metadata and uncertainties in connectivity
inference. To tackle these challenges, we formulate an IM problem that aims to
find two sets, i.e., seed nodes and queried nodes. We propose an effective
method that iteratively performs three steps: 1) we learn the relationship
between collected metadata and edges via a Siamese neural network model, 2) we
select a number of inferred influential edges to construct a reinforced graph
used for discovering an optimal seed set, and 3) we identify the next node to
query by maximizing the inferred influence spread using a topology-aware
ranking strategy. By querying only 5% of nodes, IM-META reaches 93% of the
upper bound performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1"&gt;Cong Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1"&gt;Won-Yong Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spitz_A/0/1/0/all/0/1"&gt;Andreas Spitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment Restriction. (arXiv:2105.04544v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04544</id>
        <link href="http://arxiv.org/abs/2105.04544"/>
        <updated>2021-06-08T02:20:23.873Z</updated>
        <summary type="html"><![CDATA[We address the problem of causal effect estimation in the presence of
unobserved confounding, but where proxies for the latent confounder(s) are
observed. We propose two kernel-based methods for nonlinear causal effect
estimation in this setting: (a) a two-stage regression approach, and (b) a
maximum moment restriction approach. We focus on the proximal causal learning
setting, but our methods can be used to solve a wider class of inverse problems
characterised by a Fredholm integral equation. In particular, we provide a
unifying view of two-stage and moment restriction approaches for solving this
problem in a nonlinear setting. We provide consistency guarantees for each
algorithm, and we demonstrate these approaches achieve competitive results on
synthetic data and data simulating a real-world task. In particular, our
approach outperforms earlier methods that are not suited to leveraging proxy
variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mastouri_A/0/1/0/all/0/1"&gt;Afsaneh Mastouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gultchin_L/0/1/0/all/0/1"&gt;Limor Gultchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korba_A/0/1/0/all/0/1"&gt;Anna Korba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1"&gt;Ricardo Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1"&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muandet_K/0/1/0/all/0/1"&gt;Krikamol Muandet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-armed Bandit Algorithms on System-on-Chip: Go Frequentist or Bayesian?. (arXiv:2106.02855v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02855</id>
        <link href="http://arxiv.org/abs/2106.02855"/>
        <updated>2021-06-08T02:20:23.867Z</updated>
        <summary type="html"><![CDATA[Multi-armed Bandit (MAB) algorithms identify the best arm among multiple arms
via exploration-exploitation trade-off without prior knowledge of arm
statistics. Their usefulness in wireless radio, IoT, and robotics demand
deployment on edge devices, and hence, a mapping on system-on-chip (SoC) is
desired. Theoretically, the Bayesian approach-based Thompson Sampling (TS)
algorithm offers better performance than the frequentist approach-based Upper
Confidence Bound (UCB) algorithm. However, TS is not synthesizable due to Beta
function. We address this problem by approximating it via a pseudo-random
number generator-based approach and efficiently realize the TS algorithm on
Zynq SoC. In practice, the type of arms distribution (e.g., Bernoulli,
Gaussian, etc.) is unknown and hence, a single algorithm may not be optimal. We
propose a reconfigurable and intelligent MAB (RI-MAB) framework. Here,
intelligence enables the identification of appropriate MAB algorithms for a
given environment, and reconfigurability allows on-the-fly switching between
algorithms on the SoC. This eliminates the need for parallel implementation of
algorithms resulting in huge savings in resources and power consumption. We
analyze the functional correctness, area, power, and execution time of the
proposed and existing architectures for various arm distributions, word-length,
and hardware-software co-design approaches. We demonstrate the superiority of
the RI-MAB over TS and UCB only architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Santosh_S/0/1/0/all/0/1"&gt;S. V. Sai Santosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Darak_S/0/1/0/all/0/1"&gt;Sumit J. Darak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles. (arXiv:2106.02982v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02982</id>
        <link href="http://arxiv.org/abs/2106.02982"/>
        <updated>2021-06-08T02:20:23.860Z</updated>
        <summary type="html"><![CDATA[In this study, a sensor fusion based GNSS spoofing attack detection framework
is presented that consists of three concurrent strategies for an autonomous
vehicle (AV): (i) prediction of location shift, (ii) detection of turns (left
or right), and (iii) recognition of motion state (including standstill state).
Data from multiple low-cost in-vehicle sensors (i.e., accelerometer, steering
angle sensor, speed sensor, and GNSS) are fused and fed into a recurrent neural
network model, which is a long short-term memory (LSTM) network for predicting
the location shift, i.e., the distance that an AV travels between two
consecutive timestamps. We have then combined k-Nearest Neighbors (k-NN) and
Dynamic Time Warping (DTW) algorithms to detect turns using data from the
steering angle sensor. In addition, data from an AV's speed sensor is used to
recognize the AV's motion state including the standstill state. To prove the
efficacy of the sensor fusion-based attack detection framework, attack datasets
are created for three unique and sophisticated spoofing attacks turn by turn,
overshoot, and stop using the publicly available real-world Honda Research
Institute Driving Dataset (HDD). Our analysis reveals that the sensor
fusion-based detection framework successfully detects all three types of
spoofing attacks within the required computational latency threshold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1"&gt;Sagar Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mizanur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Mhafuzul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Mashrur Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relative stability toward diffeomorphisms indicates performance in deep nets. (arXiv:2105.02468v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02468</id>
        <link href="http://arxiv.org/abs/2105.02468"/>
        <updated>2021-06-08T02:20:23.854Z</updated>
        <summary type="html"><![CDATA[Understanding why deep nets can classify data in large dimensions remains a
challenge. It has been proposed that they do so by becoming stable to
diffeomorphisms, yet existing empirical measurements support that it is often
not the case. We revisit this question by defining a maximum-entropy
distribution on diffeomorphisms, that allows to study typical diffeomorphisms
of a given norm. We confirm that stability toward diffeomorphisms does not
strongly correlate to performance on benchmark data sets of images. By
contrast, we find that the stability toward diffeomorphisms relative to that of
generic transformations $R_f$ correlates remarkably with the test error
$\epsilon_t$. It is of order unity at initialization but decreases by several
decades during training for state-of-the-art architectures. For CIFAR10 and 15
known architectures, we find $\epsilon_t\approx 0.2\sqrt{R_f}$, suggesting that
obtaining a small $R_f$ is important to achieve good performance. We study how
$R_f$ depends on the size of the training set and compare it to a simple model
of invariant learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrini_L/0/1/0/all/0/1"&gt;Leonardo Petrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1"&gt;Alessandro Favero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1"&gt;Mario Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings. (arXiv:2106.02736v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02736</id>
        <link href="http://arxiv.org/abs/2106.02736"/>
        <updated>2021-06-08T02:20:23.846Z</updated>
        <summary type="html"><![CDATA[While recent work has shown that scores from models trained by the ubiquitous
masked language modeling (MLM) objective effectively discriminate probable and
improbable sequences, it is still an open question if these MLMs specify a
principled probability distribution over the space of possible sequences. In
this paper, we interpret MLMs as energy-based sequence models and propose two
energy parametrizations derivable from the trained MLMs. In order to draw
samples correctly from these models, we develop a tractable \emph{sampling}
scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our
approach, samples are proposed from the same masked conditionals used for
training the masked language models, and they are accepted or rejected based on
their energy values according to the target distribution. We validate the
effectiveness of the proposed parametrizations by exploring the quality of
samples drawn from these energy-based models on the conditional generation task
of machine translation. We theoretically and empirically justify our sampling
algorithm by showing that the masked conditionals on their own do not yield a
Markov chain whose stationary distribution is that of our target distribution,
and our approach generates higher quality samples than other recently proposed
undirected generation approaches (Wang et al., 2019, Ghazvininejad et al.,
2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1"&gt;Kartik Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1"&gt;Chris Dyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MC2G: An Efficient Algorithm for Matrix Completion with Social and Item Similarity Graphs. (arXiv:2006.04373v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04373</id>
        <link href="http://arxiv.org/abs/2006.04373"/>
        <updated>2021-06-08T02:20:23.828Z</updated>
        <summary type="html"><![CDATA[In this paper, we design and analyze MC2G (Matrix Completion with 2 Graphs),
an algorithm that performs matrix completion in the presence of social and item
similarity graphs. MC2G runs in quasilinear time and is parameter free. It is
based on spectral clustering and local refinement steps. The expected number of
sampled entries required for MC2G to succeed (i.e., recover the clusters in the
graphs and complete the matrix) matches an information-theoretic lower bound up
to a constant factor for a wide range of parameters. We show via extensive
experiments on both synthetic and real datasets that MC2G outperforms other
state-of-the-art matrix completion algorithms that leverage graph side
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiaosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_G/0/1/0/all/0/1"&gt;Geewon Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_C/0/1/0/all/0/1"&gt;Changho Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft-Attention Improves Skin Cancer Classification Performance. (arXiv:2105.03358v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03358</id>
        <link href="http://arxiv.org/abs/2105.03358"/>
        <updated>2021-06-08T02:20:23.821Z</updated>
        <summary type="html"><![CDATA[In clinical applications, neural networks must focus on and highlight the
most important parts of an input image. Soft-Attention mechanism enables a
neural network toachieve this goal. This paper investigates the effectiveness
of Soft-Attention in deep neural architectures. The central aim of
Soft-Attention is to boost the value of important features and suppress the
noise-inducing features. We compare the performance of VGG, ResNet,
InceptionResNetv2 and DenseNet architectures with and without the
Soft-Attention mechanism, while classifying skin lesions. The original network
when coupled with Soft-Attention outperforms the baseline[16] by 4.7% while
achieving a precision of 93.7% on HAM10000 dataset [25]. Additionally,
Soft-Attention coupling improves the sensitivity score by 3.8% compared to
baseline[31] and achieves 91.6% on ISIC-2017 dataset [2]. The code is publicly
available at github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Datta_S/0/1/0/all/0/1"&gt;Soumyya Kanti Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shaikh_M/0/1/0/all/0/1"&gt;Mohammad Abuzar Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Srihari_S/0/1/0/all/0/1"&gt;Sargur N. Srihari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mingchen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning of User Verification Models Without Sharing Embeddings. (arXiv:2104.08776v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08776</id>
        <link href="http://arxiv.org/abs/2104.08776"/>
        <updated>2021-06-08T02:20:23.815Z</updated>
        <summary type="html"><![CDATA[We consider the problem of training User Verification (UV) models in
federated setting, where each user has access to the data of only one class and
user embeddings cannot be shared with the server or other users. To address
this problem, we propose Federated User Verification (FedUV), a framework in
which users jointly learn a set of vectors and maximize the correlation of
their instance embeddings with a secret linear combination of those vectors. We
show that choosing the linear combinations from the codewords of an
error-correcting code allows users to collaboratively train the model without
revealing their embedding vectors. We present the experimental results for user
verification with voice, face, and handwriting data and show that FedUV is on
par with existing approaches, while not sharing the embeddings with other users
or the server.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1"&gt;Hossein Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyunsin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Sungrack Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louizos_C/0/1/0/all/0/1"&gt;Christos Louizos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soriaga_J/0/1/0/all/0/1"&gt;Joseph Soriaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TARA: Training and Representation Alteration for AI Fairness and Domain Generalization. (arXiv:2012.06387v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06387</id>
        <link href="http://arxiv.org/abs/2012.06387"/>
        <updated>2021-06-08T02:20:23.807Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for enforcing AI fairness with respect to protected
or sensitive factors. This method uses a dual strategy performing training and
representation alteration (TARA) for the mitigation of prominent causes of AI
bias by including: a) the use of representation learning alteration via
adversarial independence to suppress the bias-inducing dependence of the data
representation from protected factors; and b) training set alteration via
intelligent augmentation to address bias-causing data imbalance, by using
generative models that allow the fine control of sensitive factors related to
underrepresented populations via domain adaptation and latent space
manipulation. When testing our methods on image analytics, experiments
demonstrate that TARA significantly or fully debiases baseline models while
outperforming competing debiasing methods that have the same amount of
information, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.
the baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.
(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in
current metrics used for assessing debiasing performance, we propose novel
conjunctive debiasing metrics. Our experiments also demonstrate the ability of
these novel metrics in assessing the Pareto efficiency of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadzic_A/0/1/0/all/0/1"&gt;Armin Hadzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1"&gt;Neil Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Phil Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2106.02902v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02902</id>
        <link href="http://arxiv.org/abs/2106.02902"/>
        <updated>2021-06-08T02:20:23.795Z</updated>
        <summary type="html"><![CDATA[Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
article, we probe BERT specifically to understand and measure the relational
knowledge it captures in its parametric memory. While probing for linguistic
understanding is commonly applied to all layers of BERT as well as fine-tuned
models, this has not been done for factual knowledge. We utilize existing
knowledge base completion tasks (LAMA) to probe every layer of pre-trained as
well as fine-tuned BERT models(ranking, question answering, NER). Our findings
show that knowledge is not just contained in BERT's final layers. Intermediate
layers contribute a significant amount (17-60%) to the total knowledge found.
Probing intermediate layers also reveals how different types of knowledge
emerge at varying rates. When BERT is fine-tuned, relational knowledge is
forgotten. The extent of forgetting is impacted by the fine-tuning objective
and the training data. We found that ranking models forget the least and retain
more knowledge in their final layer compared to masked language modeling and
question-answering. However, masked language modeling performed the best at
acquiring new knowledge from the training data. When it comes to learning
facts, we found that capacity and fact density are key factors. We hope this
initial work will spur further research into understanding the parametric
memory of language models and the effect of training objectives on factual
knowledge. The code to repeat the experiments is publicly available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wallat_J/0/1/0/all/0/1"&gt;Jonas Wallat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaspreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forced Variational Integrator Networks for Prediction and Control of Mechanical Systems. (arXiv:2106.02973v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02973</id>
        <link href="http://arxiv.org/abs/2106.02973"/>
        <updated>2021-06-08T02:20:23.782Z</updated>
        <summary type="html"><![CDATA[As deep learning becomes more prevalent for prediction and control of real
physical systems, it is important that these overparameterized models are
consistent with physically plausible dynamics. This elicits a problem with how
much inductive bias to impose on the model through known physical parameters
and principles to reduce complexity of the learning problem to give us more
reliable predictions. Recent work employs discrete variational integrators
parameterized as a neural network architecture to learn conservative Lagrangian
systems. The learned model captures and enforces global energy preserving
properties of the system from very few trajectories. However, most real systems
are inherently non-conservative and, in practice, we would also like to apply
actuation. In this paper we extend this paradigm to account for general forcing
(e.g. control input and damping) via discrete d'Alembert's principle which may
ultimately be used for control applications. We show that this forced
variational integrator networks (FVIN) architecture allows us to accurately
account for energy dissipation and external forcing while still capturing the
true underlying energy-based passive dynamics. We show that in application this
can result in highly-data efficient model-based control and can predict on real
non-conservative systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havens_A/0/1/0/all/0/1"&gt;Aaron Havens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1"&gt;Girish Chowdhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Adversarial Robustness and Optimization Bias. (arXiv:2102.08868v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08868</id>
        <link href="http://arxiv.org/abs/2102.08868"/>
        <updated>2021-06-08T02:20:23.761Z</updated>
        <summary type="html"><![CDATA[We demonstrate that the choice of optimizer, neural network architecture, and
regularizer significantly affect the adversarial robustness of linear neural
networks, providing guarantees without the need for adversarial training. To
this end, we revisit a known result linking maximally robust classifiers and
minimum norm solutions, and combine it with recent results on the implicit bias
of optimizers. First, we show that, under certain conditions, it is possible to
achieve both perfect standard accuracy and a certain degree of robustness,
simply by training an overparametrized model using the implicit bias of the
optimization. In that regime, there is a direct relationship between the type
of the optimizer and the attack to which the model is robust. To the best of
our knowledge, this work is the first to study the impact of optimization
methods such as sign gradient descent and proximal methods on adversarial
robustness. Second, we characterize the robustness of linear convolutional
models, showing that they resist attacks subject to a constraint on the
Fourier-$\ell_\infty$ norm. To illustrate these findings we design a novel
Fourier-$\ell_\infty$ attack that finds adversarial examples with controllable
frequencies. We evaluate Fourier-$\ell_\infty$ robustness of
adversarially-trained deep CIFAR-10 models from the standard RobustBench
benchmark and visualize adversarial perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1"&gt;Fartash Faghri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1"&gt;Sven Gowal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1"&gt;Cristina Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1"&gt;Fabian Pedregosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1"&gt;Nicolas Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Approximation Algorithms for Active Sequential Hypothesis Testing. (arXiv:2103.04250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04250</id>
        <link href="http://arxiv.org/abs/2103.04250"/>
        <updated>2021-06-08T02:20:23.753Z</updated>
        <summary type="html"><![CDATA[In the problem of active sequential hypotheses testing (ASHT), a learner
seeks to identify the true hypothesis from among a known set of hypotheses. The
learner is given a set of actions and knows the random distribution of the
outcome of any action under any true hypothesis. Given a target error
$\delta>0$, the goal is to sequentially select the fewest number of actions so
as to identify the true hypothesis with probability at least $1 - \delta$.
Motivated by applications in which the number of hypotheses or actions is
massive (e.g. genomics-based cancer detection), we propose efficient (greedy,
in fact) algorithms and provide the first approximation guarantees for ASHT,
under two types of adaptivity. Both of our guarantees are independent of the
number of actions and logarithmic in the number of hypotheses. We numerically
evaluate the performance of our algorithms using both synthetic and real DNA
mutation data, demonstrating that our algorithms outperform previous heuristic
policies by large margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gan_K/0/1/0/all/0/1"&gt;Kyra Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1"&gt;Su Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Andrew Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Relation Alignment for Calibrated Cross-modal Retrieval. (arXiv:2105.13868v2 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.13868</id>
        <link href="http://arxiv.org/abs/2105.13868"/>
        <updated>2021-06-08T02:20:23.738Z</updated>
        <summary type="html"><![CDATA[Despite the achievements of large-scale multimodal pre-training approaches,
cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.
To bridge the semantic gap between the two modalities, previous studies mainly
focus on word-region alignment at the object level, lacking the matching
between the linguistic relation among the words and the visual relation among
the regions. The neglect of such relation consistency impairs the
contextualized representation of image-text pairs and hinders the model
performance and the interpretability. In this paper, we first propose a novel
metric, Intra-modal Self-attention Distance (ISD), to quantify the relation
consistency by measuring the semantic distance between linguistic and visual
relations. In response, we present Inter-modal Alignment on Intra-modal
Self-attentions (IAIS), a regularized training method to optimize the ISD and
calibrate intra-modal self-attentions from the two modalities mutually via
inter-modal alignment. The IAIS regularizer boosts the performance of
prevailing models on Flickr30k and MS COCO datasets by a considerable margin,
which demonstrates the superiority of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Assignment Problem with Time Constraints. (arXiv:2106.02856v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02856</id>
        <link href="http://arxiv.org/abs/2106.02856"/>
        <updated>2021-06-08T02:20:23.721Z</updated>
        <summary type="html"><![CDATA[We present an end-to-end framework for the Assignment Problem with multiple
tasks mapped to a group of workers, using reinforcement learning while
preserving many constraints. Tasks and workers have time constraints and there
is a cost associated with assigning a worker to a task. Each worker can perform
multiple tasks until it exhausts its allowed time units (capacity). We train a
reinforcement learning agent to find near optimal solutions to the problem by
minimizing total cost associated with the assignments while maintaining hard
constraints. We use proximal policy optimization to optimize model parameters.
The model generates a sequence of actions in real-time which correspond to task
assignment to workers, without having to retrain for changes in the dynamic
state of the environment. In our problem setting reward is computed as negative
of the assignment cost. We also demonstrate our results on bin packing and
capacitated vehicle routing problem, using the same framework. Our results
outperform Google OR-Tools using MIP and CP-SAT solvers with large problem
instances, in terms of solution quality and computation time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pathan_S/0/1/0/all/0/1"&gt;Sharmin Pathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_V/0/1/0/all/0/1"&gt;Vyom Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02800</id>
        <link href="http://arxiv.org/abs/2106.02800"/>
        <updated>2021-06-08T02:20:23.696Z</updated>
        <summary type="html"><![CDATA[Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides real-time
retinal images with high resolution down to 2 $\mu m$. This technique enables
detection of the morphologies of individual microaneurysms (MAs), which are one
of the earliest signs of diabetic retinopathy (DR), a frequent complication of
diabetes that can lead to visual impairment and blindness. In contrast to
previous automatic models developed for MA detection on standard fundus
photographs, currently there is no high throughput image protocol available for
automatic analysis of AOSLO photographs. To address this urgency, we introduce
AOSLO-net, a deep neural network framework with customized training policy,
including preprocessing, data augmentation and transfer learning, to
automatically segment MAs from AOSLO images. We evaluate the performance of
AOSLO-net using 87 DR AOSLO images demonstrating very accurate MA detection and
segmentation, leading to correct MA morphological classification, while
outperforming the state-of-the-art both in accuracy and cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1"&gt;Konstantina Sampani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shengze Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yixiang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;He Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer K. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant message passing for the prediction of tensorial properties and molecular spectra. (arXiv:2102.03150v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03150</id>
        <link href="http://arxiv.org/abs/2102.03150"/>
        <updated>2021-06-08T02:20:23.689Z</updated>
        <summary type="html"><![CDATA[Message passing neural networks have become a method of choice for learning
on graphs, in particular the prediction of chemical properties and the
acceleration of molecular dynamics studies. While they readily scale to large
training data sets, previous approaches have proven to be less data efficient
than kernel methods. We identify limitations of invariant representations as a
major reason and extend the message passing formulation to rotationally
equivariant representations. On this basis, we propose the polarizable atom
interaction neural network (PaiNN) and improve on common molecule benchmarks
over previous networks, while reducing model size and inference time. We
leverage the equivariant atomwise representations obtained by PaiNN for the
prediction of tensorial properties. Finally, we apply this to the simulation of
molecular spectra, achieving speedups of 4-5 orders of magnitude compared to
the electronic structure reference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schutt_K/0/1/0/all/0/1"&gt;Kristof T. Sch&amp;#xfc;tt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unke_O/0/1/0/all/0/1"&gt;Oliver T. Unke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gastegger_M/0/1/0/all/0/1"&gt;Michael Gastegger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary Layers in Deep Neural Networks. (arXiv:2104.07085v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07085</id>
        <link href="http://arxiv.org/abs/2104.07085"/>
        <updated>2021-06-08T02:20:23.677Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel layer based on fast Walsh-Hadamard
transform (WHT) and smooth-thresholding to replace $1\times 1$ convolution
layers in deep neural networks. In the WHT domain, we denoise the transform
domain coefficients using the new smooth-thresholding non-linearity, a smoothed
version of the well-known soft-thresholding operator. We also introduce a
family of multiplication-free operators from the basic 2$\times$2 Hadamard
transform to implement $3\times 3$ depthwise separable convolution layers.
Using these two types of layers, we replace the bottleneck layers in
MobileNet-V2 to reduce the network's number of parameters with a slight loss in
accuracy. For example, by replacing the final third bottleneck layers, we
reduce the number of parameters from 2.270M to 540K. This reduces the accuracy
from 95.21\% to 92.98\% on the CIFAR-10 dataset. Our approach significantly
improves the speed of data processing. The fast Walsh-Hadamard transform has a
computational complexity of $O(m\log_2 m)$. As a result, it is computationally
more efficient than the $1\times1$ convolution layer. The fast Walsh-Hadamard
layer processes a tensor in $\mathbb{R}^{10\times32\times32\times1024}$ about 2
times faster than $1\times1$ convolution layer on NVIDIA Jetson Nano computer
board.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1"&gt;Hongyi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabawi_D/0/1/0/all/0/1"&gt;Diaa Dabawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1"&gt;Ahmet Enis Cetin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Inference with Sparse and Quantized Communication. (arXiv:2004.01302v4 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.01302</id>
        <link href="http://arxiv.org/abs/2004.01302"/>
        <updated>2021-06-08T02:20:23.670Z</updated>
        <summary type="html"><![CDATA[We consider the problem of distributed inference where agents in a network
observe a stream of private signals generated by an unknown state, and aim to
uniquely identify this state from a finite set of hypotheses. We focus on
scenarios where communication between agents is costly, and takes place over
channels with finite bandwidth. To reduce the frequency of communication, we
develop a novel event-triggered distributed learning rule that is based on the
principle of diffusing low beliefs on each false hypothesis. Building on this
principle, we design a trigger condition under which an agent broadcasts only
those components of its belief vector that have adequate innovation, to only
those neighbors that require such information. We prove that our rule
guarantees convergence to the true state exponentially fast almost surely
despite sparse communication, and that it has the potential to significantly
reduce information flow from uninformative agents to informative agents. Next,
to deal with finite-precision communication channels, we propose a distributed
learning rule that leverages the idea of adaptive quantization. We show that by
sequentially refining the range of the quantizers, every agent can learn the
truth exponentially fast almost surely, while using just $1$ bit to encode its
belief on each hypothesis. For both our proposed algorithms, we rigorously
characterize the trade-offs between communication-efficiency and the learning
rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mitra_A/0/1/0/all/0/1"&gt;Aritra Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Richards_J/0/1/0/all/0/1"&gt;John A. Richards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bagchi_S/0/1/0/all/0/1"&gt;Saurabh Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Shreyas Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots. (arXiv:2103.09593v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09593</id>
        <link href="http://arxiv.org/abs/2103.09593"/>
        <updated>2021-06-08T02:20:23.663Z</updated>
        <summary type="html"><![CDATA[Multilingual models have demonstrated impressive cross-lingual transfer
performance. However, test sets like XNLI are monolingual at the example level.
In multilingual communities, it is common for polyglots to code-mix when
conversing with each other. Inspired by this phenomenon, we present two strong
black-box adversarial attacks (one word-level, one phrase-level) for
multilingual models that push their ability to handle code-mixed sentences to
the limit. The former uses bilingual dictionaries to propose perturbations and
translations of the clean example for sense disambiguation. The latter directly
aligns the clean example with its translations before extracting phrases as
perturbations. Our phrase-level attack has a success rate of 89.75% against
XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.
Finally, we propose an efficient adversarial training scheme that trains in the
same number of steps as the original model and show that it improves model
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Samson Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Tests and Always-Valid Confidence Intervals for contingency tables and beyond. (arXiv:2106.02693v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.02693</id>
        <link href="http://arxiv.org/abs/2106.02693"/>
        <updated>2021-06-08T02:20:23.657Z</updated>
        <summary type="html"><![CDATA[We develop E variables for testing whether two data streams come from the
same source or not, and more generally, whether the difference between the
sources is larger than some minimal effect size. These E variables lead to
tests that remain safe, i.e. keep their Type-I error guarantees, under flexible
sampling scenarios such as optional stopping and continuation. We also develop
the corresponding always-valid confidence intervals. In special cases our E
variables also have an optimal `growth' property under the alternative. We
illustrate the generic construction through the special case of 2x2 contingency
tables, where we also allow for the incorporation of different restrictions on
a composite alternative. Comparison to p-value analysis in simulations and a
real-world example show that E variables, through their flexibility, often
allow for early stopping of data collection, thereby retaining similar power as
classical methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1"&gt;Rosanne Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ly_A/0/1/0/all/0/1"&gt;Alexander Ly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grunwald_P/0/1/0/all/0/1"&gt;Peter Gr&amp;#xfc;nwald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Uniform Convergence for Multicalibration. (arXiv:2005.01757v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01757</id>
        <link href="http://arxiv.org/abs/2005.01757"/>
        <updated>2021-06-08T02:20:23.634Z</updated>
        <summary type="html"><![CDATA[There is a growing interest in societal concerns in machine learning systems,
especially in fairness. Multicalibration gives a comprehensive methodology to
address group fairness. In this work, we address the multicalibration error and
decouple it from the prediction error. The importance of decoupling the
fairness metric (multicalibration) and the accuracy (prediction error) is due
to the inherent trade-off between the two, and the societal decision regarding
the "right tradeoff" (as imposed many times by regulators). Our work gives
sample complexity bounds for uniform convergence guarantees of multicalibration
error, which implies that regardless of the accuracy, we can guarantee that the
empirical and (true) multicalibration errors are close. We emphasize that our
results: (1) are more general than previous bounds, as they apply to both
agnostic and realizable settings, and do not rely on a specific type of
algorithm (such as deferentially private), (2) improve over previous
multicalibration sample complexity bounds and (3) implies uniform convergence
guarantees for the classical calibration error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shabat_E/0/1/0/all/0/1"&gt;Eliran Shabat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_L/0/1/0/all/0/1"&gt;Lee Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exact Solver for the Weston-Watkins SVM Subproblem. (arXiv:2102.05640v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05640</id>
        <link href="http://arxiv.org/abs/2102.05640"/>
        <updated>2021-06-08T02:20:23.627Z</updated>
        <summary type="html"><![CDATA[Recent empirical evidence suggests that the Weston-Watkins support vector
machine is among the best performing multiclass extensions of the binary SVM.
Current state-of-the-art solvers repeatedly solve a particular subproblem
approximately using an iterative strategy. In this work, we propose an
algorithm that solves the subproblem exactly using a novel reparametrization of
the Weston-Watkins dual problem. For linear WW-SVMs, our solver shows
significant speed-up over the state-of-the-art solver when the number of
classes is large. Our exact subproblem solver also allows us to prove linear
convergence of the overall solver.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yutong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scott_C/0/1/0/all/0/1"&gt;Clayton D. Scott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Multi-Armed Bandits in the Shuffle Model. (arXiv:2106.02900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02900</id>
        <link href="http://arxiv.org/abs/2106.02900"/>
        <updated>2021-06-08T02:20:23.618Z</updated>
        <summary type="html"><![CDATA[We give an $(\varepsilon,\delta)$-differentially private algorithm for the
multi-armed bandit (MAB) problem in the shuffle model with a
distribution-dependent regret of $O\left(\left(\sum_{a\in
[k]:\Delta_a>0}\frac{\log
T}{\Delta_a}\right)+\frac{k\sqrt{\log\frac{1}{\delta}}\log
T}{\varepsilon}\right)$, and a distribution-independent regret of
$O\left(\sqrt{kT\log T}+\frac{k\sqrt{\log\frac{1}{\delta}}\log
T}{\varepsilon}\right)$, where $T$ is the number of rounds, $\Delta_a$ is the
suboptimality gap of the arm $a$, and $k$ is the total number of arms. Our
upper bound almost matches the regret of the best known algorithms for the
centralized model, and significantly outperforms the best known algorithm in
the local model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Jay Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1"&gt;Haim Kaplan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1"&gt;Uri Stemmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2004.09764v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09764</id>
        <link href="http://arxiv.org/abs/2004.09764"/>
        <updated>2021-06-08T02:20:23.611Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ClawCraneNet: Leveraging Object-level Relation for Text-based Video Segmentation. (arXiv:2103.10702v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10702</id>
        <link href="http://arxiv.org/abs/2103.10702"/>
        <updated>2021-06-08T02:20:23.579Z</updated>
        <summary type="html"><![CDATA[Text-based video segmentation is a challenging task that segments out the
natural language referred objects in videos. It essentially requires semantic
comprehension and fine-grained video understanding. Existing methods introduce
language representation into segmentation models in a bottom-up manner, which
merely conducts vision-language interaction within local receptive fields of
ConvNets. We argue that such interaction is not fulfilled since the model can
barely construct region-level relationships given partial observations, which
is contrary to the description logic of natural language/referring expressions.
In fact, people usually describe a target object using relations with other
objects, which may not be easily understood without seeing the whole video. To
address the issue, we introduce a novel top-down approach by imitating how we
human segment an object with the language guidance. We first figure out all
candidate objects in videos and then choose the refereed one by parsing
relations among those high-level objects. Three kinds of object-level relations
are investigated for precise relationship understanding, i.e., positional
relation, text-guided semantic relation, and temporal relation. Extensive
experiments on A2D Sentences and J-HMDB Sentences show our method outperforms
state-of-the-art methods by a large margin. Qualitative results also show our
results are more explainable. Besides, based on the inspiration, we win the
first place in CVPR2021 Referring Youtube-VOS challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yawei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and High-Quality Blind Multi-Spectral Image Pansharpening. (arXiv:2103.09943v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09943</id>
        <link href="http://arxiv.org/abs/2103.09943"/>
        <updated>2021-06-08T02:20:23.556Z</updated>
        <summary type="html"><![CDATA[Blind pansharpening addresses the problem of generating a high
spatial-resolution multi-spectral (HRMS) image given a low spatial-resolution
multi-spectral (LRMS) image with the guidance of its associated spatially
misaligned high spatial-resolution panchromatic (PAN) image without parametric
side information. In this paper, we propose a fast approach to blind
pansharpening and achieve state-of-the-art image reconstruction quality.
Typical blind pansharpening algorithms are often computationally intensive
since the blur kernel and the target HRMS image are often computed using
iterative solvers and in an alternating fashion. To achieve fast blind
pansharpening, we decouple the solution of the blur kernel and of the HRMS
image. First, we estimate the blur kernel by computing the kernel coefficients
with minimum total generalized variation that blur a downsampled version of the
PAN image to approximate a linear combination of the LRMS image channels. Then,
we estimate each channel of the HRMS image using local Laplacian prior to
regularize the relationship between each HRMS channel and the PAN image.
Solving the HRMS image is accelerated by both parallelizing across the channels
and by fast numerical algorithms for each channel. Due to the fast scheme and
the powerful priors we used on the blur kernel coefficients (total generalized
variation) and on the cross-channel relationship (local Laplacian prior),
numerical experiments demonstrate that our algorithm outperforms
state-of-the-art model-based counterparts in terms of both computational time
and reconstruction quality of the HRMS images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lantao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dehong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_H/0/1/0/all/0/1"&gt;Hassan Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boufounos_P/0/1/0/all/0/1"&gt;Petros T. Boufounos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiscale Principle of Relevant Information for Hyperspectral Image Classification. (arXiv:1907.06022v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.06022</id>
        <link href="http://arxiv.org/abs/1907.06022"/>
        <updated>2021-06-08T02:20:23.549Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel architecture, termed multiscale principle of
relevant information (MPRI), to learn discriminative spectral-spatial features
for hyperspectral image (HSI) classification. MPRI inherits the merits of the
principle of relevant information (PRI) to effectively extract multiscale
information embedded in the given data, and also takes advantage of the
multilayer structure to learn representations in a coarse-to-fine manner.
Specifically, MPRI performs spectral-spatial pixel characterization (using PRI)
and feature dimensionality reduction (using regularized linear discriminant
analysis) iteratively and successively. Extensive experiments on three
benchmark data sets demonstrate that MPRI outperforms existing state-of-the-art
methods (including deep learning based ones) qualitatively and quantitatively,
especially in the scenario of limited training samples. Code of MPRI is
available at \url{this http URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yantao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shujian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giraldo_L/0/1/0/all/0/1"&gt;Luis Sanchez Giraldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1"&gt;Jose C. Principe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Abstractions of Neural Networks. (arXiv:2106.02997v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02997</id>
        <link href="http://arxiv.org/abs/2106.02997"/>
        <updated>2021-06-08T02:20:23.542Z</updated>
        <summary type="html"><![CDATA[Structural analysis methods (e.g., probing and feature attribution) are
increasingly important tools for neural network analysis. We propose a new
structural analysis method grounded in a formal theory of \textit{causal
abstraction} that provides rich characterizations of model-internal
representations and their roles in input/output behavior. In this method,
neural representations are aligned with variables in interpretable causal
models, and then \textit{interchange interventions} are used to experimentally
verify that the neural representations have the causal properties of their
aligned variables. We apply this method in a case study to analyze neural
models trained on Multiply Quantified Natural Language Inference (MQNLI)
corpus, a highly complex NLI dataset that was constructed with a
tree-structured natural logic causal model. We discover that a BERT-based model
with state-of-the-art performance successfully realizes the approximate causal
structure of the natural logic causal model, whereas a simpler baseline model
fails to show any such structure, demonstrating that neural representations
encode the compositional structure of MQNLI examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Atticus Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Hanson Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1"&gt;Thomas Icard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1"&gt;Christopher Potts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Immediate Proximity Detection Using Wi-Fi-Enabled Smartphones. (arXiv:2106.02777v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02777</id>
        <link href="http://arxiv.org/abs/2106.02777"/>
        <updated>2021-06-08T02:20:23.534Z</updated>
        <summary type="html"><![CDATA[Smartphone apps for exposure notification and contact tracing have been shown
to be effective in controlling the COVID-19 pandemic. However, Bluetooth Low
Energy tokens similar to those broadcast by existing apps can still be picked
up far away from the transmitting device. In this paper, we present a new class
of methods for detecting whether or not two Wi-Fi-enabled devices are in
immediate physical proximity, i.e. 2 or fewer meters apart, as established by
the U.S. Centers for Disease Control and Prevention (CDC). Our goal is to
enhance the accuracy of smartphone-based exposure notification and contact
tracing systems. We present a set of binary machine learning classifiers that
take as input pairs of Wi-Fi RSSI fingerprints. We empirically verify that a
single classifier cannot generalize well to a range of different environments
with vastly different numbers of detectable Wi-Fi Access Points (APs). However,
specialized classifiers, tailored to situations where the number of detectable
APs falls within a certain range, are able to detect immediate physical
proximity significantly more accurately. As such, we design three classifiers
for situations with low, medium, and high numbers of detectable APs. These
classifiers distinguish between pairs of RSSI fingerprints recorded 2 or fewer
meters apart and pairs recorded further apart but still in Bluetooth range. We
characterize their balanced accuracy for this task to be between 66.8% and
77.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hyfte_Z/0/1/0/all/0/1"&gt;Zach Van Hyfte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakhor_A/0/1/0/all/0/1"&gt;Avideh Zakhor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10972</id>
        <link href="http://arxiv.org/abs/2104.10972"/>
        <updated>2021-06-08T02:20:23.495Z</updated>
        <summary type="html"><![CDATA[ImageNet-1K serves as the primary dataset for pretraining deep learning
models for computer vision tasks. ImageNet-21K dataset, which is bigger and
more diverse, is used less frequently for pretraining, mainly due to its
complexity, low accessibility, and underestimation of its added value. This
paper aims to close this gap, and make high-quality efficient pretraining on
ImageNet-21K available for everyone. Via a dedicated preprocessing stage,
utilization of WordNet hierarchical structure, and a novel training scheme
called semantic softmax, we show that various models significantly benefit from
ImageNet-21K pretraining on numerous datasets and tasks, including small
mobile-oriented models. We also show that we outperform previous ImageNet-21K
pretraining schemes for prominent new models like ViT and Mixer. Our proposed
pretraining pipeline is efficient, accessible, and leads to SoTA reproducible
results, from a publicly available dataset. The training code and pretrained
models are available at: https://github.com/Alibaba-MIIL/ImageNet21K]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tetrad: Actively Secure 4PC for Secure Training and Inference. (arXiv:2106.02850v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02850</id>
        <link href="http://arxiv.org/abs/2106.02850"/>
        <updated>2021-06-08T02:20:23.469Z</updated>
        <summary type="html"><![CDATA[In this work, we design an efficient mixed-protocol framework, Tetrad, with
applications to privacy-preserving machine learning. It is designed for the
four-party setting with at most one active corruption and supports rings.

Our fair multiplication protocol requires communicating only 5 ring elements
improving over the state-of-the-art protocol of Trident (Chaudhari et al.
NDSS'20). The technical highlights of Tetrad include efficient (a) truncation
without any overhead, (b) multi-input multiplication protocols for arithmetic
and boolean worlds, (c) garbled-world, tailor-made for the mixed-protocol
framework, and (d) conversion mechanisms to switch between the computation
styles. The fair framework is also extended to provide robustness without
inflating the costs.

The competence of Tetrad is tested with benchmarks for deep neural networks
such as LeNet and VGG16 and support vector machines. One variant of our
framework aims at minimizing the execution time, while the other focuses on the
monetary cost. We observe improvements up to 6x over Trident across these
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koti_N/0/1/0/all/0/1"&gt;Nishat Koti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1"&gt;Arpita Patra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rachuri_R/0/1/0/all/0/1"&gt;Rahul Rachuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ajith Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Time-Adaptive Drift-Diffusion Model. (arXiv:2106.02742v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02742</id>
        <link href="http://arxiv.org/abs/2106.02742"/>
        <updated>2021-06-08T02:20:23.458Z</updated>
        <summary type="html"><![CDATA[Animals can quickly learn the timing of events with fixed intervals and their
rate of acquisition does not depend on the length of the interval. In contrast,
recurrent neural networks that use gradient based learning have difficulty
predicting the timing of events that depend on stimulus that occurred long ago.
We present the latent time-adaptive drift-diffusion model (LTDDM), an extension
to the time-adaptive drift-diffusion model (TDDM), a model for animal learning
of timing that exhibits behavioural properties consistent with experimental
data from animals. The performance of LTDDM is compared to that of a state of
the art long short-term memory (LSTM) recurrent neural network across three
timing tasks. Differences in the relative performance of these two models is
discussed and it is shown how LTDDM can learn these events time series orders
of magnitude faster than recurrent neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cimolino_G/0/1/0/all/0/1"&gt;Gabriele Cimolino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rivest_F/0/1/0/all/0/1"&gt;Francois Rivest&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpikePropamine: Differentiable Plasticity in Spiking Neural Networks. (arXiv:2106.02681v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.02681</id>
        <link href="http://arxiv.org/abs/2106.02681"/>
        <updated>2021-06-08T02:20:23.220Z</updated>
        <summary type="html"><![CDATA[The adaptive changes in synaptic efficacy that occur between spiking neurons
have been demonstrated to play a critical role in learning for biological
neural networks. Despite this source of inspiration, many learning focused
applications using Spiking Neural Networks (SNNs) retain static synaptic
connections, preventing additional learning after the initial training period.
Here, we introduce a framework for simultaneously learning the underlying
fixed-weights and the rules governing the dynamics of synaptic plasticity and
neuromodulated synaptic plasticity in SNNs through gradient descent. We further
demonstrate the capabilities of this framework on a series of challenging
benchmarks, learning the parameters of several plasticity rules including BCM,
Oja's, and their respective set of neuromodulatory variants. The experimental
results display that SNNs augmented with differentiable plasticity are
sufficient for solving a set of challenging temporal learning tasks that a
traditional SNN fails to solve, even in the presence of significant noise.
These networks are also shown to be capable of producing locomotion on a
high-dimensional robotic learning task, where near-minimal degradation in
performance is observed in the presence of novel conditions not seen during the
initial training period.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmidgall_S/0/1/0/all/0/1"&gt;Samuel Schmidgall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashkanazy_J/0/1/0/all/0/1"&gt;Julia Ashkanazy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawson_W/0/1/0/all/0/1"&gt;Wallace Lawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1"&gt;Joe Hays&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy. (arXiv:2104.09435v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09435</id>
        <link href="http://arxiv.org/abs/2104.09435"/>
        <updated>2021-06-08T02:20:23.214Z</updated>
        <summary type="html"><![CDATA[Volumetric imaging by fluorescence microscopy is often limited by anisotropic
spatial resolution from inferior axial resolution compared to the lateral
resolution. To address this problem, here we present a deep-learning-enabled
unsupervised super-resolution technique that enhances anisotropic images in
volumetric fluorescence microscopy. In contrast to the existing deep learning
approaches that require matched high-resolution target volume images, our
method greatly reduces the effort to put into practice as the training of a
network requires as little as a single 3D image stack, without a priori
knowledge of the image formation process, registration of training data, or
separate acquisition of target data. This is achieved based on the optimal
transport driven cycle-consistent generative adversarial network that learns
from an unpaired matching between high-resolution 2D images in lateral image
plane and low-resolution 2D images in the other planes. Using fluorescence
confocal microscopy and light-sheet microscopy, we demonstrate that the trained
network not only enhances axial resolution, but also restores suppressed visual
details between the imaging planes and removes imaging artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyoungjun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_M/0/1/0/all/0/1"&gt;Myeongsu Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Bumju Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Soohyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Ki Hean Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Sunghoe Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Long Context NLM for ASR Rescoring in Conversational Agents. (arXiv:2104.11070v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11070</id>
        <link href="http://arxiv.org/abs/2104.11070"/>
        <updated>2021-06-08T02:20:23.207Z</updated>
        <summary type="html"><![CDATA[Neural Language Models (NLM), when trained and evaluated with context
spanning multiple utterances, have been shown to consistently outperform both
conventional n-gram language models and NLMs that use limited context. In this
paper, we investigate various techniques to incorporate turn based context
history into both recurrent (LSTM) and Transformer-XL based NLMs. For recurrent
based NLMs, we explore context carry over mechanism and feature based
augmentation, where we incorporate other forms of contextual information such
as bot response and system dialogue acts as classified by a Natural Language
Understanding (NLU) model. To mitigate the sharp nearby, fuzzy far away problem
with contextual NLM, we propose the use of attention layer over lexical
metadata to improve feature based augmentation. Additionally, we adapt our
contextual NLM towards user provided on-the-fly speech patterns by leveraging
encodings from a large pre-trained masked language model and performing fusion
with a Transformer-XL based NLM. We test our proposed models using N-best
rescoring of ASR hypotheses of task-oriented dialogues and also evaluate on
downstream NLU tasks such as intent classification and slot labeling. The best
performing model shows a relative WER between 1.6% and 9.1% and a slot labeling
F1 score improvement of 4% over non-contextual baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1"&gt;Monica Sunkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ronanki_S/0/1/0/all/0/1"&gt;Srikanth Ronanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Technologies for Trustworthy Machine Learning: A Survey in a Socio-Technical Context. (arXiv:2007.08911v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08911</id>
        <link href="http://arxiv.org/abs/2007.08911"/>
        <updated>2021-06-08T02:20:23.184Z</updated>
        <summary type="html"><![CDATA[Concerns about the societal impact of AI-based services and systems has
encouraged governments and other organisations around the world to propose AI
policy frameworks to address fairness, accountability, transparency and related
topics. To achieve the objectives of these frameworks, the data and software
engineers who build machine-learning systems require knowledge about a variety
of relevant supporting tools and techniques. In this paper we provide an
overview of technologies that support building trustworthy machine learning
systems, i.e., systems whose properties justify that people place trust in
them. We argue that four categories of system properties are instrumental in
achieving the policy objectives, namely fairness, explainability, auditability
and safety & security (FEAS). We discuss how these properties need to be
considered across all stages of the machine learning life cycle, from data
collection through run-time model inference. As a consequence, we survey in
this paper the main technologies with respect to all four of the FEAS
properties, for data-centric as well as model-centric stages of the machine
learning system life cycle. We conclude with an identification of open research
problems, with a particular focus on the connection between trustworthy machine
learning technologies and their implications for individuals and society.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toreini_E/0/1/0/all/0/1"&gt;Ehsan Toreini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aitken_M/0/1/0/all/0/1"&gt;Mhairi Aitken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coopamootoo_K/0/1/0/all/0/1"&gt;Kovila P. L. Coopamootoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elliott_K/0/1/0/all/0/1"&gt;Karen Elliott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelaya_V/0/1/0/all/0/1"&gt;Vladimiro Gonzalez Zelaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Missier_P/0/1/0/all/0/1"&gt;Paolo Missier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Magdalene Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moorsel_A/0/1/0/all/0/1"&gt;Aad van Moorsel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Birds of a Feather Flock Together: A Close Look at Cooperation Emergence via Multi-Agent RL. (arXiv:2104.11455v2 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11455</id>
        <link href="http://arxiv.org/abs/2104.11455"/>
        <updated>2021-06-08T02:20:23.178Z</updated>
        <summary type="html"><![CDATA[How cooperation emerges is a long-standing and interdisciplinary problem.
Game-theoretical studies on social dilemmas reveal that altruistic incentives
are critical to the emergence of cooperation but their analyses are limited to
stateless games. For more realistic scenarios, multi-agent reinforcement
learning has been used to study sequential social dilemmas (SSDs). Recent works
show that learning to incentivize other agents can promote cooperation in SSDs.
However, we find that, with these incentivizing mechanisms, the team
cooperation level does not converge and regularly oscillates between
cooperation and defection during learning. We show that a second-order social
dilemma resulting from the incentive mechanisms is the main reason for such
fragile cooperation. We formally analyze the dynamics of second-order social
dilemmas and find that a typical tendency of humans, called homophily, provides
a promising solution. We propose a novel learning framework to encourage
homophilic incentives and show that it achieves stable cooperation in both SSDs
of public goods and tragedy of the commons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Heng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tonghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiayuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Geometric Matrix Completion. (arXiv:1911.07255v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07255</id>
        <link href="http://arxiv.org/abs/1911.07255"/>
        <updated>2021-06-08T02:20:23.171Z</updated>
        <summary type="html"><![CDATA[Deep Matrix Factorization (DMF) is an emerging approach to the problem of
matrix completion. Recent works have established that gradient descent applied
to a DMF model induces an implicit regularization on the rank of the recovered
matrix. In this work we interpret the DMF model through the lens of spectral
geometry. This allows us to incorporate explicit regularization without
breaking the DMF structure, thus enjoying the best of both worlds. In
particular, we focus on matrix completion problems with underlying geometric or
topological relations between the rows and/or columns. Such relations are
prevalent in matrix completion problems that arise in many applications, such
as recommender systems and drug-target interaction. Our contributions enable
DMF models to exploit these relations, and make them competitive on real
benchmarks, while exhibiting one of the first successful applications of deep
linear networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boyarski_A/0/1/0/all/0/1"&gt;Amit Boyarski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1"&gt;Sanketh Vedula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1"&gt;Alex Bronstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heuristic-Guided Reinforcement Learning. (arXiv:2106.02757v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02757</id>
        <link href="http://arxiv.org/abs/2106.02757"/>
        <updated>2021-06-08T02:20:23.164Z</updated>
        <summary type="html"><![CDATA[We provide a framework for accelerating reinforcement learning (RL)
algorithms by heuristics constructed from domain knowledge or offline data.
Tabula rasa RL algorithms require environment interactions or computation that
scales with the horizon of the sequential decision-making task. Using our
framework, we show how heuristic-guided RL induces a much shorter-horizon
subproblem that provably solves the original task. Our framework can be viewed
as a horizon-based regularization for controlling bias and variance in RL under
a finite interaction budget. On the theoretical side, we characterize
properties of a good heuristic and its impact on RL acceleration. In
particular, we introduce the novel concept of an "improvable heuristic" -- a
heuristic that allows an RL agent to extrapolate beyond its prior knowledge. On
the empirical side, we instantiate our framework to accelerate several
state-of-the-art algorithms in simulated robotic control tasks and procedurally
generated games. Our framework complements the rich literature on warm-starting
RL with expert demonstrations or exploratory datasets, and introduces a
principled method for injecting prior knowledge into RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Ching-An Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1"&gt;Andrey Kolobov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1"&gt;Adith Swaminathan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end learnable EEG channel selection for deep neural networks with Gumbel-softmax. (arXiv:2102.09050v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09050</id>
        <link href="http://arxiv.org/abs/2102.09050"/>
        <updated>2021-06-08T02:20:23.158Z</updated>
        <summary type="html"><![CDATA[Many electroencephalography (EEG) applications rely on channel selection
methods to remove the least informative channels, e.g., to reduce the amount of
electrodes to be mounted, to decrease the computational load, or to reduce
overfitting effects and improve performance. Wrapper-based channel selection
methods aim to match the channel selection step to the target model, yet they
require to re-train the model multiple times on different candidate channel
subsets, which often leads to an unacceptably high computational cost,
especially when said model is a (deep) neural network. To alleviate this, we
propose a framework to embed the EEG channel selection in the neural network
itself to jointly learn the network weights and optimal channels in an
end-to-end manner by traditional backpropagation algorithms. We deal with the
discrete nature of this new optimization problem by employing continuous
relaxations of the discrete channel selection parameters based on the
Gumbel-softmax trick. We also propose a regularization method that discourages
selecting channels more than once. This generic approach is evaluated on two
different EEG tasks: motor imagery brain-computer interfaces and auditory
attention decoding. The results demonstrate that our framework is generally
applicable, while being competitive with state-of-the art EEG channel selection
methods, tailored to these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Strypsteen_T/0/1/0/all/0/1"&gt;Thomas Strypsteen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bertrand_A/0/1/0/all/0/1"&gt;Alexander Bertrand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Robust Graph Neural Networks with Topology Adaptive Edge Dropping. (arXiv:2106.02892v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02892</id>
        <link href="http://arxiv.org/abs/2106.02892"/>
        <updated>2021-06-08T02:20:23.139Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are processing architectures that exploit graph
structural information to model representations from network data. Despite
their success, GNNs suffer from sub-optimal generalization performance given
limited training data, referred to as over-fitting. This paper proposes
Topology Adaptive Edge Dropping (TADropEdge) method as an adaptive data
augmentation technique to improve generalization performance and learn robust
GNN models. We start by explicitly analyzing how random edge dropping increases
the data diversity during training, while indicating i.i.d. edge dropping does
not account for graph structural information and could result in noisy
augmented data degrading performance. To overcome this issue, we consider graph
connectivity as the key property that captures graph topology. TADropEdge
incorporates this factor into random edge dropping such that the edge-dropped
subgraphs maintain similar topology as the underlying graph, yielding more
satisfactory data augmentation. In particular, TADropEdge first leverages the
graph spectrum to assign proper weights to graph edges, which represent their
criticality for establishing the graph connectivity. It then normalizes the
edge weights and drops graph edges adaptively based on their normalized
weights. Besides improving generalization performance, TADropEdge reduces
variance for efficient training and can be applied as a generic method modular
to different GNN models. Intensive experiments on real-life and synthetic
datasets corroborate theory and verify the effectiveness of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1"&gt;Subhrajit Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Leiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blum_R/0/1/0/all/0/1"&gt;Rick S. Blum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1"&gt;Brian M. Sadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Latent Space Tuning for Non-Stationary Distributions. (arXiv:2105.03584v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03584</id>
        <link href="http://arxiv.org/abs/2105.03584"/>
        <updated>2021-06-08T02:20:23.132Z</updated>
        <summary type="html"><![CDATA[Powerful deep learning tools, such as convolutional neural networks (CNN),
are able to learn the input-output relationships of large complicated systems
directly from data. Encoder-decoder deep CNNs are able to extract features
directly from images, mix them with scalar inputs within a general
low-dimensional latent space, and then generate new complex 2D outputs which
represent complex physical phenomenon. One important challenge faced by deep
learning methods is large non-stationary systems whose characteristics change
quickly with time for which re-training is not feasible. In this paper we
present a method for adaptive tuning of the low-dimensional latent space of
deep encoder-decoder style CNNs based on real-time feedback to quickly
compensate for unknown and fast distribution shifts. We demonstrate our
approach for predicting the properties of a time-varying charged particle beam
in a particle accelerator whose components (accelerating electric fields and
focusing magnetic fields) are also quickly changing with time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Scheinker_A/0/1/0/all/0/1"&gt;Alexander Scheinker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cropp_F/0/1/0/all/0/1"&gt;Frederick Cropp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paiagua_S/0/1/0/all/0/1"&gt;Sergio Paiagua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Filippetto_D/0/1/0/all/0/1"&gt;Daniele Filippetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction. (arXiv:2106.02768v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02768</id>
        <link href="http://arxiv.org/abs/2106.02768"/>
        <updated>2021-06-08T02:20:23.125Z</updated>
        <summary type="html"><![CDATA[Cross domain recommender system constitutes a powerful method to tackle the
cold-start and sparsity problem by aggregating and transferring user
preferences across multiple category domains. Therefore, it has great potential
to improve click-through-rate prediction performance in online commerce
platforms having many domains of products. While several cross domain
sequential recommendation models have been proposed to leverage information
from a source domain to improve CTR predictions in a target domain, they did
not take into account bidirectional latent relations of user preferences across
source-target domain pairs. As such, they cannot provide enhanced cross-domain
CTR predictions for both domains simultaneously. In this paper, we propose a
novel approach to cross-domain sequential recommendations based on the dual
learning mechanism that simultaneously transfers information between two
related domains in an iterative manner until the learning process stabilizes.
In particular, the proposed Dual Attentive Sequential Learning (DASL) model
consists of two novel components Dual Embedding and Dual Attention, which
jointly establish the two-stage learning process: we first construct dual
latent embeddings that extract user preferences in both domains simultaneously,
and subsequently provide cross-domain recommendations by matching the extracted
latent embeddings with candidate items through dual-attention learning
mechanism. We conduct extensive offline experiments on three real-world
datasets to demonstrate the superiority of our proposed model, which
significantly and consistently outperforms several state-of-the-art baselines
across all experimental settings. We also conduct an online A/B test at a major
video streaming platform Alibaba-Youku, where our proposed model significantly
improves business performance over the latest production system in the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Disentanglement in Variational Auto-Encoders Using Jacobian $L_1$ Regularization. (arXiv:2106.02923v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02923</id>
        <link href="http://arxiv.org/abs/2106.02923"/>
        <updated>2021-06-08T02:20:23.112Z</updated>
        <summary type="html"><![CDATA[There have been many recent advances in representation learning; however,
unsupervised representation learning can still struggle with model
identification issues. Variational Auto-Encoders (VAEs) and their extensions
such as $\beta$-VAEs have been shown to locally align latent variables with PCA
directions, which can help to improve model disentanglement under some
conditions. Borrowing inspiration from Independent Component Analysis (ICA) and
sparse coding, we propose applying an $L_1$ loss to the VAE's generative
Jacobian during training to encourage local latent variable alignment with
independent factors of variation in the data. We demonstrate our results on a
variety of datasets, giving qualitative and quantitative results using
information theoretic and modularity measures that show our added $L_1$ cost
encourages local axis alignment of the latent representation with individual
factors of variation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rhodes_T/0/1/0/all/0/1"&gt;Travers Rhodes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Daniel D. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Temperature Imaging Using Pseudo-Inversed Convolutional Neural Network Aided TDLAS Tomography. (arXiv:2106.02901v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02901</id>
        <link href="http://arxiv.org/abs/2106.02901"/>
        <updated>2021-06-08T02:20:23.105Z</updated>
        <summary type="html"><![CDATA[As an in situ combustion diagnostic tool, Tunable Diode Laser Absorption
Spectroscopy (TDLAS) tomography has been widely used for imaging of
two-dimensional temperature distributions in reactive flows. Compared with the
computational tomographic algorithms, Convolutional Neural Networks (CNNs) have
been proofed to be more robust and accurate for image reconstruction,
particularly in case of limited access of laser beams in the Region of Interest
(RoI). In practice, flame in the RoI that requires to be reconstructed with
good spatial resolution is commonly surrounded by low-temperature background.
Although the background is not of high interest, spectroscopic absorption still
exists due to heat dissipation and gas convection. Therefore, we propose a
Pseudo-Inversed CNN (PI-CNN) for hierarchical temperature imaging that (a) uses
efficiently the training and learning resources for temperature imaging in the
RoI with good spatial resolution, and (b) reconstructs the less spatially
resolved background temperature by adequately addressing the integrity of the
spectroscopic absorption model. In comparison with the traditional CNN, the
newly introduced pseudo inversion of the RoI sensitivity matrix is more
penetrating for revealing the inherent correlation between the projection data
and the RoI to be reconstructed, thus prioritising the temperature imaging in
the RoI with high accuracy and high computational efficiency. In this paper,
the proposed algorithm was validated by both numerical simulation and lab-scale
experiment, indicating good agreement between the phantoms and the
high-fidelity reconstructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Si_J/0/1/0/all/0/1"&gt;Jingjing Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1"&gt;Guoliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yinbo Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Enemali_G/0/1/0/all/0/1"&gt;Godwin Enemali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel multi-scale loss function for classification problems in machine learning. (arXiv:2106.02676v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.02676</id>
        <link href="http://arxiv.org/abs/2106.02676"/>
        <updated>2021-06-08T02:20:23.083Z</updated>
        <summary type="html"><![CDATA[We introduce two-scale loss functions for use in various gradient descent
algorithms applied to classification problems via deep neural networks. This
new method is generic in the sense that it can be applied to a wide range of
machine learning architectures, from deep neural networks to support vector
machines for example. These two-scale loss functions allow to focus the
training onto objects in the training set which are not well classified. This
leads to an increase in several measures of performance for
appropriately-defined two-scale loss functions with respect to the more
classical cross-entropy when tested on traditional deep neural networks on the
MNIST, CIFAR10, and CIFAR100 data-sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Berlyand_L/0/1/0/all/0/1"&gt;Leonid Berlyand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Creese_R/0/1/0/all/0/1"&gt;Robert Creese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jabin_P/0/1/0/all/0/1"&gt;Pierre-Emmanuel Jabin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel approximation on algebraic varieties. (arXiv:2106.02755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02755</id>
        <link href="http://arxiv.org/abs/2106.02755"/>
        <updated>2021-06-08T02:20:23.077Z</updated>
        <summary type="html"><![CDATA[Low-rank approximation of kernels is a fundamental mathematical problem with
widespread algorithmic applications. Often the kernel is restricted to an
algebraic variety, e.g., in problems involving sparse or low-rank data. We show
that significantly better approximations are obtainable in this setting: the
rank required to achieve a given error depends on the variety's dimension
rather than the ambient dimension, which is typically much larger. This is true
in both high-precision and high-dimensional regimes. Our results are presented
for smooth isotropic kernels, the predominant class of kernels used in
applications. Our main technical insight is to approximate smooth kernels by
polynomial kernels, and leverage two key properties of polynomial kernels that
hold when they are restricted to a variety. First, their ranks decrease
exponentially in the variety's co-dimension. Second, their maximum values are
governed by their values over a small set of points. Together, our results
provide a general approach for exploiting (approximate) "algebraic structure"
in datasets in order to efficiently solve large-scale data science problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Altschuler_J/0/1/0/all/0/1"&gt;Jason M. Altschuler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parrilo_P/0/1/0/all/0/1"&gt;Pablo A. Parrilo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks using a Single Neuron: Folded-in-Time Architecture using Feedback-Modulated Delay Loops. (arXiv:2011.10115v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10115</id>
        <link href="http://arxiv.org/abs/2011.10115"/>
        <updated>2021-06-08T02:20:23.071Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are among the most widely applied machine learning tools
showing outstanding performance in a broad range of tasks. We present a method
for folding a deep neural network of arbitrary size into a single neuron with
multiple time-delayed feedback loops. This single-neuron deep neural network
comprises only a single nonlinearity and appropriately adjusted modulations of
the feedback signals. The network states emerge in time as a temporal unfolding
of the neuron's dynamics. By adjusting the feedback-modulation within the
loops, we adapt the network's connection weights. These connection weights are
determined via a back-propagation algorithm, where both the delay-induced and
local network connections must be taken into account. Our approach can fully
represent standard Deep Neural Networks (DNN), encompasses sparse DNNs, and
extends the DNN concept toward dynamical systems implementations. The new
method, which we call Folded-in-time DNN (Fit-DNN), exhibits promising
performance in a set of benchmark tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stelzer_F/0/1/0/all/0/1"&gt;Florian Stelzer&lt;/a&gt; (1, 2 and 4), &lt;a href="http://arxiv.org/find/cs/1/au:+Rohm_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; R&amp;#xf6;hm&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_R/0/1/0/all/0/1"&gt;Raul Vicente&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1"&gt;Ingo Fischer&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Yanchuk_S/0/1/0/all/0/1"&gt;Serhiy Yanchuk&lt;/a&gt; (1) ((1) Institute of Mathematics, Technische Universit&amp;#xe4;t Berlin, Germany, (2) Department of Mathematics, Humboldt-Universit&amp;#xe4;t zu Berlin, Germany, (3) Instituto de F&amp;#xed;sica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Spain, (4) Institute of Computer Science, University of Tartu, Estonia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?. (arXiv:2106.02890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02890</id>
        <link href="http://arxiv.org/abs/2106.02890"/>
        <updated>2021-06-08T02:20:23.061Z</updated>
        <summary type="html"><![CDATA[Can models with particular structure avoid being biased towards spurious
correlation in out-of-distribution (OOD) generalization? Peters et al. (2016)
provides a positive answer for linear cases. In this paper, we use a functional
modular probing method to analyze deep model structures under OOD setting. We
demonstrate that even in biased models (which focus on spurious correlation)
there still exist unbiased functional subnetworks. Furthermore, we articulate
and demonstrate the functional lottery ticket hypothesis: full network contains
a subnetwork that can achieve better OOD performance. We then propose Modular
Risk Minimization to solve the subnetwork selection problem. Our algorithm
learns the subnetwork structure from a given dataset, and can be combined with
any other OOD regularization methods. Experiments on various OOD generalization
tasks corroborate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dinghuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1"&gt;Kartik Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yilun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yisen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Defense with Data Diversity: Weak Correlation Implies Strong Robustness. (arXiv:2106.02867v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02867</id>
        <link href="http://arxiv.org/abs/2106.02867"/>
        <updated>2021-06-08T02:20:23.056Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a framework of filter-based ensemble of deep
neuralnetworks (DNNs) to defend against adversarial attacks. The framework
builds an ensemble of sub-models -- DNNs with differentiated preprocessing
filters. From the theoretical perspective of DNN robustness, we argue that
under the assumption of high quality of the filters, the weaker the
correlations of the sensitivity of the filters are, the more robust the
ensemble model tends to be, and this is corroborated by the experiments of
transfer-based attacks. Correspondingly, we propose a principle that chooses
the specific filters with smaller Pearson correlation coefficients, which
ensures the diversity of the inputs received by DNNs, as well as the
effectiveness of the entire framework against attacks. Our ensemble models are
more robust than those constructed by previous defense methods like adversarial
training, and even competitive with the classical ensemble of adversarial
trained DNNs under adversarial attacks when the attacking radius is large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Renjue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Pengfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Cheng-Chao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aimin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1"&gt;Bai Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Q-Learning in Zero-sum Markov Games. (arXiv:2106.02748v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.02748</id>
        <link href="http://arxiv.org/abs/2106.02748"/>
        <updated>2021-06-08T02:20:23.040Z</updated>
        <summary type="html"><![CDATA[We study multi-agent reinforcement learning (MARL) in infinite-horizon
discounted zero-sum Markov games. We focus on the practical but challenging
setting of decentralized MARL, where agents make decisions without coordination
by a centralized controller, but only based on their own payoffs and local
actions executed. The agents need not observe the opponent's actions or
payoffs, possibly being even oblivious to the presence of the opponent, nor be
aware of the zero-sum structure of the underlying game, a setting also referred
to as radically uncoupled in the literature of learning in games. In this
paper, we develop for the first time a radically uncoupled Q-learning dynamics
that is both rational and convergent: the learning dynamics converges to the
best response to the opponent's strategy when the opponent follows an
asymptotically stationary strategy; the value function estimates converge to
the payoffs at a Nash equilibrium when both agents adopt the dynamics. The key
challenge in this decentralized setting is the non-stationarity of the learning
environment from an agent's perspective, since both her own payoffs and the
system evolution depend on the actions of other agents, and each agent adapts
their policies simultaneously and independently. To address this issue, we
develop a two-timescale learning dynamics where each agent updates her local
Q-function and value function estimates concurrently, with the latter happening
at a slower timescale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sayin_M/0/1/0/all/0/1"&gt;Muhammed O. Sayin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaiqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leslie_D/0/1/0/all/0/1"&gt;David S. Leslie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basar_T/0/1/0/all/0/1"&gt;Tamer Basar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1"&gt;Asuman Ozdaglar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Lottery Ticket Hypothesis for Graph Neural Networks. (arXiv:2102.06790v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06790</id>
        <link href="http://arxiv.org/abs/2102.06790"/>
        <updated>2021-06-08T02:20:23.033Z</updated>
        <summary type="html"><![CDATA[With graphs rapidly growing in size and deeper graph neural networks (GNNs)
emerging, the training and inference of GNNs become increasingly expensive.
Existing network weight pruning algorithms cannot address the main space and
computational bottleneck in GNNs, caused by the size and connectivity of the
graph. To this end, this paper first presents a unified GNN sparsification
(UGS) framework that simultaneously prunes the graph adjacency matrix and the
model weights, for effectively accelerating GNN inference on large-scale
graphs. Leveraging this new tool, we further generalize the recently popular
lottery ticket hypothesis to GNNs for the first time, by defining a graph
lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network,
which can be jointly identified from the original GNN and the full dense graph
by iteratively applying UGS. Like its counterpart in convolutional neural
networks, GLT can be trained in isolation to match the performance of training
with the full model and graph, and can be drawn from both randomly initialized
and self-supervised pre-trained GNNs. Our proposal has been experimentally
verified across various GNN architectures and diverse tasks, on both
small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale
datasets from the challenging Open Graph Benchmark (OGB). Specifically, for
node classification, our found GLTs achieve the same accuracies with 20%~98%
MACs saving on small graphs and 25%~85% MACs saving on large ones. For link
prediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph
datasets, respectively, without compromising predictive performance. Codes
available at https://github.com/VITA-Group/Unified-LTH-GNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1"&gt;Yongduo Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuxi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principle Bit Analysis: Autoencoding with Schur-Concave Loss. (arXiv:2106.02796v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02796</id>
        <link href="http://arxiv.org/abs/2106.02796"/>
        <updated>2021-06-08T02:20:23.025Z</updated>
        <summary type="html"><![CDATA[We consider a linear autoencoder in which the latent variables are quantized,
or corrupted by noise, and the constraint is Schur-concave in the set of latent
variances. Although finding the optimal encoder/decoder pair for this setup is
a nonconvex optimization problem, we show that decomposing the source into its
principal components is optimal. If the constraint is strictly Schur-concave
and the empirical covariance matrix has only simple eigenvalues, then any
optimal encoder/decoder must decompose the source in this way. As one
application, we consider a strictly Schur-concave constraint that estimates the
number of bits needed to represent the latent variables under fixed-rate
encoding, a setup that we call \emph{Principal Bit Analysis (PBA)}. This yields
a practical, general-purpose, fixed-rate compressor that outperforms existing
algorithms. As a second application, we show that a prototypical
autoencoder-based variable-rate compressor is guaranteed to decompose the
source into its principal components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhadane_S/0/1/0/all/0/1"&gt;Sourbh Bhadane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1"&gt;Aaron B. Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1"&gt;Jayadev Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks. (arXiv:2106.02743v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02743</id>
        <link href="http://arxiv.org/abs/2106.02743"/>
        <updated>2021-06-08T02:20:23.019Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are the first choice methods for graph machine
learning problems thanks to their ability to learn state-of-the-art level
representations from graph-structured data. However, centralizing a massive
amount of real-world graph data for GNN training is prohibitive due to
user-side privacy concerns, regulation restrictions, and commercial
competition. Federated Learning is the de-facto standard for collaborative
training of machine learning models over many distributed edge devices without
the need for centralization. Nevertheless, training graph neural networks in a
federated setting is vaguely defined and brings statistical and systems
challenges. This work proposes SpreadGNN, a novel multi-task federated training
framework capable of operating in the presence of partial labels and absence of
a central server for the first time in the literature. SpreadGNN extends
federated multi-task learning to realistic serverless settings for GNNs, and
utilizes a novel optimization algorithm with a convergence guarantee,
Decentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized
multi-task learning problems. We empirically demonstrate the efficacy of our
framework on a variety of non-I.I.D. distributed graph-level molecular property
prediction datasets with partial labels. Our results show that SpreadGNN
outperforms GNN models trained over a central server-dependent federated
learning system, even in constrained topologies. The source code is publicly
available at https://github.com/FedML-AI/SpreadGNN]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Chaoyang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceyani_E/0/1/0/all/0/1"&gt;Emir Ceyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_K/0/1/0/all/0/1"&gt;Keshav Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Annavaram_M/0/1/0/all/0/1"&gt;Murali Annavaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No MCMC for me: Amortized sampling for fast and stable training of energy-based models. (arXiv:2010.04230v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04230</id>
        <link href="http://arxiv.org/abs/2010.04230"/>
        <updated>2021-06-08T02:20:23.012Z</updated>
        <summary type="html"><![CDATA[Energy-Based Models (EBMs) present a flexible and appealing way to represent
uncertainty. Despite recent advances, training EBMs on high-dimensional data
remains a challenging problem as the state-of-the-art approaches are costly,
unstable, and require considerable tuning and domain expertise to apply
successfully. In this work, we present a simple method for training EBMs at
scale which uses an entropy-regularized generator to amortize the MCMC sampling
typically used in EBM training. We improve upon prior MCMC-based entropy
regularization methods with a fast variational approximation. We demonstrate
the effectiveness of our approach by using it to train tractable likelihood
models. Next, we apply our estimator to the recently proposed Joint Energy
Model (JEM), where we match the original performance with faster and stable
training. This allows us to extend JEM models to semi-supervised classification
on tabular data from a variety of continuous domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1"&gt;Will Grathwohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1"&gt;Jacob Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1"&gt;Milad Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1"&gt;Kevin Swersky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1"&gt;David Duvenaud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Estimation by Mixing: Adaptivity and More. (arXiv:2106.02803v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02803</id>
        <link href="http://arxiv.org/abs/2106.02803"/>
        <updated>2021-06-08T02:20:22.994Z</updated>
        <summary type="html"><![CDATA[Networks analysis has been commonly used to study the interactions between
units of complex systems. One problem of particular interest is learning the
network's underlying connection pattern given a single and noisy instantiation.
While many methods have been proposed to address this problem in recent years,
they usually assume that the true model belongs to a known class, which is not
verifiable in most real-world applications. Consequently, network modeling
based on these methods either suffers from model misspecification or relies on
additional model selection procedures that are not well understood in theory
and can potentially be unstable in practice. To address this difficulty, we
propose a mixing strategy that leverages available arbitrary models to improve
their individual performances. The proposed method is computationally efficient
and almost tuning-free; thus, it can be used as an off-the-shelf method for
network modeling. We show that the proposed method performs equally well as the
oracle estimate when the true model is included as individual candidates. More
importantly, the method remains robust and outperforms all current estimates
even when the models are misspecified. Extensive simulation examples are used
to verify the advantage of the proposed mixing method. Evaluation of link
prediction performance on 385 real-world networks from six domains also
demonstrates the universal competitiveness of the mixing method across multiple
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Le_C/0/1/0/all/0/1"&gt;Can M. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Increasing Depth Leads to U-Shaped Test Risk in Over-parameterized Convolutional Networks. (arXiv:2010.09610v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09610</id>
        <link href="http://arxiv.org/abs/2010.09610"/>
        <updated>2021-06-08T02:20:22.987Z</updated>
        <summary type="html"><![CDATA[Recent works have demonstrated that increasing model capacity through width
in over-parameterized neural networks leads to a decrease in test risk. For
neural networks, however, model capacity can also be increased through depth,
yet understanding the impact of increasing depth on test risk remains an open
question. In this work, we demonstrate that the test risk of over-parameterized
convolutional networks is a U-shaped curve (i.e. monotonically decreasing, then
increasing) with increasing depth. We first provide empirical evidence for this
phenomenon via image classification experiments using both ResNets and the
convolutional neural tangent kernel (CNTK). We then present a novel linear
regression framework for characterizing the impact of depth on test risk, and
show that increasing depth leads to a U-shaped test risk for the linear CNTK.
In particular, we prove that the linear CNTK corresponds to a depth-dependent
linear transformation on the original space and characterize properties of this
transformation. We then analyze over-parameterized linear regression under
arbitrary linear transformations and, in simplified settings, provably identify
the depths which minimize each of the bias and variance terms of the test risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nichani_E/0/1/0/all/0/1"&gt;Eshaan Nichani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1"&gt;Adityanarayanan Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1"&gt;Caroline Uhler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[W-RST: Towards a Weighted RST-style Discourse Framework. (arXiv:2106.02658v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02658</id>
        <link href="http://arxiv.org/abs/2106.02658"/>
        <updated>2021-06-08T02:20:22.981Z</updated>
        <summary type="html"><![CDATA[Aiming for a better integration of data-driven and linguistically-inspired
approaches, we explore whether RST Nuclearity, assigning a binary assessment of
importance between text segments, can be replaced by automatically generated,
real-valued scores, in what we call a Weighted-RST framework. In particular, we
find that weighted discourse trees from auxiliary tasks can benefit key NLP
downstream applications, compared to nuclearity-centered approaches. We further
show that real-valued importance distributions partially and interestingly
align with the assessment and uncertainty of human annotators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1"&gt;Patrick Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wen Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch Slimming for Efficient Vision Transformers. (arXiv:2106.02852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02852</id>
        <link href="http://arxiv.org/abs/2106.02852"/>
        <updated>2021-06-08T02:20:22.975Z</updated>
        <summary type="html"><![CDATA[This paper studies the efficiency problem for visual transformers by
excavating redundant calculation in given networks. The recent transformer
architecture has demonstrated its effectiveness for achieving excellent
performance on a series of computer vision tasks. However, similar to that of
convolutional neural networks, the huge computational cost of vision
transformers is still a severe issue. Considering that the attention mechanism
aggregates different patches layer-by-layer, we present a novel patch slimming
approach that discards useless patches in a top-down paradigm. We first
identify the effective patches in the last layer and then use them to guide the
patch selection process of previous layers. For each layer, the impact of a
patch on the final output feature is approximated and patches with less impact
will be removed. Experimental results on benchmark datasets demonstrate that
the proposed method can significantly reduce the computational costs of vision
transformers without affecting their performances. For example, over 45% FLOPs
of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the
ImageNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jianyuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Variational Perspective on Diffusion-Based Generative Models and Score Matching. (arXiv:2106.02808v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02808</id>
        <link href="http://arxiv.org/abs/2106.02808"/>
        <updated>2021-06-08T02:20:22.968Z</updated>
        <summary type="html"><![CDATA[Discrete-time diffusion-based generative models and score matching methods
have shown promising results in modeling high-dimensional image data. Recently,
Song et al. (2021) show that diffusion processes that transform data into noise
can be reversed via learning the score function, i.e. the gradient of the
log-density of the perturbed data. They propose to plug the learned score
function into an inverse formula to define a generative diffusion process.
Despite the empirical success, a theoretical underpinning of this procedure is
still lacking. In this work, we approach the (continuous-time) generative
diffusion directly and derive a variational framework for likelihood
estimation, which includes continuous-time normalizing flows as a special case,
and can be seen as an infinitely deep variational autoencoder. Under this
framework, we show that minimizing the score-matching loss is equivalent to
maximizing a lower bound of the likelihood of the plug-in reverse SDE proposed
by Song et al. (2021), bridging the theoretical gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chin-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1"&gt;Jae Hyun Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Even More Optimal Stochastic Optimization Algorithm: Minibatching and Interpolation Learning. (arXiv:2106.02720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02720</id>
        <link href="http://arxiv.org/abs/2106.02720"/>
        <updated>2021-06-08T02:20:22.953Z</updated>
        <summary type="html"><![CDATA[We present and analyze an algorithm for optimizing smooth and convex or
strongly convex objectives using minibatch stochastic gradient estimates. The
algorithm is optimal with respect to its dependence on both the minibatch size
and minimum expected loss simultaneously. This improves over the optimal method
of Lan (2012), which is insensitive to the minimum expected loss; over the
optimistic acceleration of Cotter et al. (2011), which has suboptimal
dependence on the minibatch size; and over the algorithm of Liu and Belkin
(2018), which is limited to least squares problems and is also similarly
suboptimal with respect to the minibatch size. Applied to interpolation
learning, the improvement over Cotter et al. and Liu and Belkin translates to a
linear, rather than square-root, parallelization speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1"&gt;Blake Woodworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1"&gt;Nathan Srebro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Entity Representations for Document-level Relation Extraction. (arXiv:2012.02507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02507</id>
        <link href="http://arxiv.org/abs/2012.02507"/>
        <updated>2021-06-08T02:20:22.946Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) requires extracting relations
expressed within and across sentences. Recent works show that graph-based
methods, usually constructing a document-level graph that captures
document-aware interactions, can obtain useful entity representations thus
helping tackle document-level RE. These methods either focus more on the entire
graph, or pay more attention to a part of the graph, e.g., paths between the
target entity pair. However, we find that document-level RE may benefit from
focusing on both of them simultaneously. Therefore, to obtain more
comprehensive entity representations, we propose the Coarse-to-Fine Entity
Representation model (CFER) that adopts a coarse-to-fine strategy involving two
phases. First, CFER uses graph neural networks to integrate global information
in the entire graph at a coarse level. Next, CFER utilizes the global
information as a guidance to selectively aggregate path information between the
target entity pair at a fine level. In classification, we combine the entity
representations from both two levels into more comprehensive representations
for relation extraction. Experimental results on two document-level RE
datasets, DocRED and CDR, show that CFER outperforms existing models and is
robust to the uneven label distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Damai Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pinball-OCSVM for early-stage COVID-19 diagnosis with limited posteroanterior chest X-ray images. (arXiv:2010.08115v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08115</id>
        <link href="http://arxiv.org/abs/2010.08115"/>
        <updated>2021-06-08T02:20:22.935Z</updated>
        <summary type="html"><![CDATA[The infection of respiratory coronavirus disease 2019 (COVID-19) starts with
the upper respiratory tract and as the virus grows, the infection can progress
to lungs and develop pneumonia. The conventional way of COVID-19 diagnosis is
reverse transcription polymerase chain reaction (RT-PCR), which is less
sensitive during early stages; especially if the patient is asymptomatic, which
may further cause more severe pneumonia. In this context, several deep learning
models have been proposed to identify pulmonary infections using publicly
available chest X-ray (CXR) image datasets for early diagnosis, better
treatment and quick cure. In these datasets, presence of less number of
COVID-19 positive samples compared to other classes (normal, pneumonia and
Tuberculosis) raises the challenge for unbiased learning of deep learning
models. All deep learning models opted class balancing techniques to solve this
issue; which however should be avoided in any medical diagnosis process.
Moreover, the deep learning models are also data hungry and need massive
computation resources. Therefore for quicker diagnosis, this research proposes
a novel pinball loss function based one-class support vector machine
(PB-OCSVM), that can work in presence of limited COVID-19 positive CXR samples
with objectives to maximize the learning efficiency and to minimize the false
predictions. The performance of the proposed model is compared with
conventional OCSVM and existing deep learning models, and the experimental
results prove that the proposed model outperformed over state-of-the-art
methods. To validate the robustness of the proposed model, experiments are also
performed with noisy CXR images and UCI benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nagabhushan_P/0/1/0/all/0/1"&gt;P. Nagabhushan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilled One-Shot Federated Learning. (arXiv:2009.07999v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07999</id>
        <link href="http://arxiv.org/abs/2009.07999"/>
        <updated>2021-06-08T02:20:22.918Z</updated>
        <summary type="html"><![CDATA[Current federated learning algorithms take tens of communication rounds
transmitting unwieldy model weights under ideal circumstances and hundreds when
data is poorly distributed. Inspired by recent work on dataset distillation and
distributed one-shot learning, we propose Distilled One-Shot Federated Learning
(DOSFL) to significantly reduce the communication cost while achieving
comparable performance. In just one round, each client distills their private
dataset, sends the synthetic data (e.g. images or sentences) to the server, and
collectively trains a global model. The distilled data look like noise and are
only useful to the specific model weights, i.e., become useless after the model
updates. With this weight-less and gradient-less design, the total
communication cost of DOSFL is up to three orders of magnitude less than FedAvg
while preserving between 93% to 99% performance of a centralized counterpart.
Afterwards, clients could switch to traditional methods such as FedAvg to
finetune the last few percent to fit personalized local models with local
datasets. Through comprehensive experiments, we show the accuracy and
communication performance of DOSFL on both vision and language tasks with
different models including CNN, LSTM, Transformer, etc. We demonstrate that an
eavesdropping attacker cannot properly train a good model using the leaked
distilled data, without knowing the initial model weights. DOSFL serves as an
inexpensive method to quickly converge on a performant pre-trained model with
less than 0.1% communication cost of traditional methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yanlin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1"&gt;George Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiyao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaolin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dapeng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Normal Training for Deep Learning Models. (arXiv:2106.02925v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02925</id>
        <link href="http://arxiv.org/abs/2106.02925"/>
        <updated>2021-06-08T02:20:22.909Z</updated>
        <summary type="html"><![CDATA[Despite the predominant use of first-order methods for training deep learning
models, second-order methods, and in particular, natural gradient methods,
remain of interest because of their potential for accelerating training through
the use of curvature information. Several methods with non-diagonal
preconditioning matrices, including KFAC and Shampoo, have been proposed and
shown to be effective. Based on the so-called tensor normal (TN) distribution,
we propose and analyze a brand new approximate natural gradient method, Tensor
Normal Training (TNT), which like Shampoo, only requires knowledge on the shape
of the training parameters. By approximating the probabilistically based Fisher
matrix, as opposed to the empirical Fisher matrix, our method uses the
layer-wise covariance of the sampling based gradient as the pre-conditioning
matrix. Moreover, the assumption that the sampling-based (tensor) gradient
follows a TN distribution, ensures that its covariance has a Kronecker
separable structure, which leads to a tractable approximation to the Fisher
matrix. Consequently, TNT's memory requirements and per-iteration computational
costs are only slightly higher than those for first-order methods. In our
experiments, TNT exhibited superior optimization performance to KFAC and
Shampoo, and to state-of-the-art first-order methods. Moreover, TNT
demonstrated its ability to generalize as well as these first-order methods,
using fewer epochs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldfarb_D/0/1/0/all/0/1"&gt;Donald Goldfarb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Traffic Refinery: Cost-Aware Data Representation for Machine Learning on Network Traffic. (arXiv:2010.14605v3 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14605</id>
        <link href="http://arxiv.org/abs/2010.14605"/>
        <updated>2021-06-08T02:20:22.898Z</updated>
        <summary type="html"><![CDATA[Network management often relies on machine learning to make predictions about
performance and security from network traffic. Often, the representation of the
traffic is as important as the choice of the model. The features that the model
relies on, and the representation of those features, ultimately determine model
accuracy, as well as where and whether the model can be deployed in practice.
Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated
with deploying the model in an operational network. Towards this goal, this
paper develops a new framework and system that enables a joint evaluation of
both the conventional notions of machine learning performance (e.g., model
accuracy) and the systems-level costs of different representations of network
traffic. We highlight these two dimensions for two practical network management
tasks, video streaming quality inference and malware detection, to demonstrate
the importance of exploring different representations to find the appropriate
operating point. We demonstrate the benefit of exploring a range of
representations of network traffic and present Traffic Refinery, a
proof-of-concept implementation that both monitors network traffic at 10 Gbps
and transforms traffic in real time to produce a variety of feature
representations for machine learning. Traffic Refinery both highlights this
design space and makes it possible to explore different representations for
learning, balancing systems costs related to feature extraction and model
training against model accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bronzino_F/0/1/0/all/0/1"&gt;Francesco Bronzino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_P/0/1/0/all/0/1"&gt;Paul Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayoubi_S/0/1/0/all/0/1"&gt;Sara Ayoubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyojoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teixeira_R/0/1/0/all/0/1"&gt;Renata Teixeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feamster_N/0/1/0/all/0/1"&gt;Nick Feamster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GCNs-Net: A Graph Convolutional Neural Network Approach for Decoding Time-resolved EEG Motor Imagery Signals. (arXiv:2006.08924v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08924</id>
        <link href="http://arxiv.org/abs/2006.08924"/>
        <updated>2021-06-08T02:20:22.781Z</updated>
        <summary type="html"><![CDATA[Towards developing effective and efficient brain-computer interface (BCI)
systems, precise decoding of brain activity measured by electroencephalogram
(EEG), is highly demanded. Traditional works classify EEG signals without
considering the topological relationship among electrodes. However,
neuroscience research has increasingly emphasized network patterns of brain
dynamics. Thus, the Euclidean structure of electrodes might not adequately
reflect the interaction between signals. To fill the gap, a novel deep learning
framework based on the graph convolutional neural networks (GCNs) was presented
to enhance the decoding performance of raw EEG signals during different types
of motor imagery (MI) tasks while cooperating with the functional topological
relationship of electrodes. Based on the absolute Pearson's matrix of overall
signals, the graph Laplacian of EEG electrodes was built up. The GCNs-Net
constructed by graph convolutional layers learns the generalized features. The
followed pooling layers reduce dimensionality, and the fully-connected softmax
layer derives the final prediction. The introduced approach has been shown to
converge for both personalized and group-wise predictions. It has achieved the
highest averaged accuracy, 93.056% and 88.57% (PhysioNet Dataset), 96.24% and
80.89% (High Gamma Dataset), at the subject and group level, respectively,
compared with existing studies, which suggests adaptability and robustness to
individual variability. Moreover, the performance was stably reproducible among
repetitive experiments for cross-validation. To conclude, the GCNs-Net filters
EEG signals based on the functional topological relationship, which manages to
decode relevant features for brain motor imagery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lun_X/0/1/0/all/0/1"&gt;Xiangmin Lun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Shuyue Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yimin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Tsetlin Machines for the IDENTITY- and NOT Operators. (arXiv:2007.14268v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.14268</id>
        <link href="http://arxiv.org/abs/2007.14268"/>
        <updated>2021-06-08T02:20:22.770Z</updated>
        <summary type="html"><![CDATA[The Tsetlin Machine (TM) is a recent machine learning algorithm with several
distinct properties, such as interpretability, simplicity, and
hardware-friendliness. Although numerous empirical evaluations report on its
performance, the mathematical analysis of its convergence is still open. In
this article, we analyze the convergence of the TM with only one clause
involved for classification. More specifically, we examine two basic logical
operators, namely, the "IDENTITY"- and "NOT" operators. Our analysis reveals
that the TM, with just one clause, can converge correctly to the intended
logical operator, learning from training data over an infinite time horizon.
Besides, it can capture arbitrarily rare patterns and select the most accurate
one when two candidate patterns are incompatible, by configuring a granularity
parameter. The analysis of the convergence of the two basic operators lays the
foundation for analyzing other logical operators. These analyses altogether,
from a mathematical perspective, provide new insights on why TMs have obtained
state-of-the-art performance on several pattern recognition problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Lei Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1"&gt;Ole-Christoffer Granmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1"&gt;Morten Goodwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Topology from Synthetic Data for Unsupervised Depth Completion. (arXiv:2106.02994v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02994</id>
        <link href="http://arxiv.org/abs/2106.02994"/>
        <updated>2021-06-08T02:20:22.729Z</updated>
        <summary type="html"><![CDATA[We present a method for inferring dense depth maps from images and sparse
depth measurements by leveraging synthetic data to learn the association of
sparse point clouds with dense natural shapes, and using the image as evidence
to validate the predicted depth map. Our learned prior for natural shapes uses
only sparse depth as input, not images, so the method is not affected by the
covariate shift when attempting to transfer learned models from synthetic data
to real ones. This allows us to use abundant synthetic data with ground truth
to learn the most difficult component of the reconstruction process, which is
topology estimation, and use the image to refine the prediction based on
photometric evidence. Our approach uses fewer parameters than previous methods,
yet, achieves the state of the art on both indoor and outdoor benchmark
datasets. Code available at:
https://github.com/alexklwong/learning-topology-synthetic-data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alex Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicek_S/0/1/0/all/0/1"&gt;Safa Cicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Flow Regularization: Improving Structured Sparsity in Deep Neural Networks. (arXiv:2106.02914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02914</id>
        <link href="http://arxiv.org/abs/2106.02914"/>
        <updated>2021-06-08T02:20:22.703Z</updated>
        <summary type="html"><![CDATA[Pruning is a model compression method that removes redundant parameters in
deep neural networks (DNNs) while maintaining accuracy. Most available filter
pruning methods require complex treatments such as iterative pruning, features
statistics/ranking, or additional optimization designs in the training process.
In this paper, we propose a simple and effective regularization strategy from a
new perspective of evolution of features, which we call feature flow
regularization (FFR), for improving structured sparsity and filter pruning in
DNNs. Specifically, FFR imposes controls on the gradient and curvature of
feature flow along the neural network, which implicitly increases the sparsity
of the parameters. The principle behind FFR is that coherent and smooth
evolution of features will lead to an efficient network that avoids redundant
parameters. The high structured sparsity obtained from FFR enables us to prune
filters effectively. Experiments with VGGNets, ResNets on CIFAR-10/100, and
Tiny ImageNet datasets demonstrate that FFR can significantly improve both
unstructured and structured sparsity. Our pruning results in terms of reduction
of parameters and FLOPs are comparable to or even better than those of
state-of-the-art pruning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yuan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Luchan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yang Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Routines for Effective Off-Policy Reinforcement Learning. (arXiv:2106.02943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02943</id>
        <link href="http://arxiv.org/abs/2106.02943"/>
        <updated>2021-06-08T02:20:22.697Z</updated>
        <summary type="html"><![CDATA[The performance of reinforcement learning depends upon designing an
appropriate action space, where the effect of each action is measurable, yet,
granular enough to permit flexible behavior. So far, this process involved
non-trivial user choices in terms of the available actions and their execution
frequency. We propose a novel framework for reinforcement learning that
effectively lifts such constraints. Within our framework, agents learn
effective behavior over a routine space: a new, higher-level action space,
where each routine represents a set of 'equivalent' sequences of granular
actions with arbitrary length. Our routine space is learned end-to-end to
facilitate the accomplishment of underlying off-policy reinforcement learning
objectives. We apply our framework to two state-of-the-art off-policy
algorithms and show that the resulting agents obtain relevant performance
improvements while requiring fewer interactions with the environment per
episode, improving computational efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cetin_E/0/1/0/all/0/1"&gt;Edoardo Cetin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celiktutan_O/0/1/0/all/0/1"&gt;Oya Celiktutan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Weighted Automata for Approximate Minimization in Language Modelling. (arXiv:2106.02965v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02965</id>
        <link href="http://arxiv.org/abs/2106.02965"/>
        <updated>2021-06-08T02:20:22.691Z</updated>
        <summary type="html"><![CDATA[In this paper we study the approximate minimization problem for language
modelling. We assume we are given some language model as a black box. The
objective is to obtain a weighted finite automaton (WFA) that fits within a
given size constraint and which mimics the behaviour of the original model
while minimizing some notion of distance between the black box and the
extracted WFA. We provide an algorithm for the approximate minimization of
black boxes trained for language modelling of sequential data over a one-letter
alphabet. By reformulating the problem in terms of Hankel matrices, we leverage
classical results on the approximation of Hankel operators, namely the
celebrated Adamyan-Arov-Krein (AAK) theory. This allows us to use the spectral
norm to measure the distance between the black box and the WFA. We provide
theoretical guarantees to study the potentially infinite-rank Hankel matrix of
the black box, without accessing the training data, and we prove that our
method returns an asymptotically-optimal approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lacroce_C/0/1/0/all/0/1"&gt;Clara Lacroce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panangaden_P/0/1/0/all/0/1"&gt;Prakash Panangaden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1"&gt;Guillaume Rabusseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards High Fidelity Face Relighting with Realistic Shadows. (arXiv:2104.00825v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00825</id>
        <link href="http://arxiv.org/abs/2104.00825"/>
        <updated>2021-06-08T02:20:22.685Z</updated>
        <summary type="html"><![CDATA[Existing face relighting methods often struggle with two problems:
maintaining the local facial details of the subject and accurately removing and
synthesizing shadows in the relit image, especially hard shadows. We propose a
novel deep face relighting method that addresses both problems. Our method
learns to predict the ratio (quotient) image between a source image and the
target image with the desired lighting, allowing us to relight the image while
maintaining the local facial details. During training, our model also learns to
accurately modify shadows by using estimated shadow masks to emphasize on the
high-contrast shadow borders. Furthermore, we introduce a method to use the
shadow mask to estimate the ambient light intensity in an image, and are thus
able to leverage multiple datasets during training with different global
lighting intensities. With quantitative and qualitative evaluations on the
Multi-PIE and FFHQ datasets, we demonstrate that our proposed method faithfully
maintains the local facial details of the subject and can accurately handle
hard shadows while achieving state-of-the-art face relighting performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_A/0/1/0/all/0/1"&gt;Andrew Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ze Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkis_M/0/1/0/all/0/1"&gt;Michel Sarkis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_N/0/1/0/all/0/1"&gt;Ning Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yiying Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Relation Alignment for Calibrated Cross-modal Retrieval. (arXiv:2105.13868v2 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.13868</id>
        <link href="http://arxiv.org/abs/2105.13868"/>
        <updated>2021-06-08T02:20:22.677Z</updated>
        <summary type="html"><![CDATA[Despite the achievements of large-scale multimodal pre-training approaches,
cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.
To bridge the semantic gap between the two modalities, previous studies mainly
focus on word-region alignment at the object level, lacking the matching
between the linguistic relation among the words and the visual relation among
the regions. The neglect of such relation consistency impairs the
contextualized representation of image-text pairs and hinders the model
performance and the interpretability. In this paper, we first propose a novel
metric, Intra-modal Self-attention Distance (ISD), to quantify the relation
consistency by measuring the semantic distance between linguistic and visual
relations. In response, we present Inter-modal Alignment on Intra-modal
Self-attentions (IAIS), a regularized training method to optimize the ISD and
calibrate intra-modal self-attentions from the two modalities mutually via
inter-modal alignment. The IAIS regularizer boosts the performance of
prevailing models on Flickr30k and MS COCO datasets by a considerable margin,
which demonstrates the superiority of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning. (arXiv:2106.02705v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02705</id>
        <link href="http://arxiv.org/abs/2106.02705"/>
        <updated>2021-06-08T02:20:22.670Z</updated>
        <summary type="html"><![CDATA[As multi-task models gain popularity in a wider range of machine learning
applications, it is becoming increasingly important for practitioners to
understand the fairness implications associated with those models. Most
existing fairness literature focuses on learning a single task more fairly,
while how ML fairness interacts with multiple tasks in the joint learning
setting is largely under-explored. In this paper, we are concerned with how
group fairness (e.g., equal opportunity, equalized odds) as an ML fairness
concept plays out in the multi-task scenario. In multi-task learning, several
tasks are learned jointly to exploit task correlations for a more efficient
inductive transfer. This presents a multi-dimensional Pareto frontier on (1)
the trade-off between group fairness and accuracy with respect to each task, as
well as (2) the trade-offs across multiple tasks. We aim to provide a deeper
understanding on how group fairness interacts with accuracy in multi-task
learning, and we show that traditional approaches that mainly focus on
optimizing the Pareto frontier of multi-task accuracy might not perform well on
fairness goals. We propose a new set of metrics to better capture the
multi-dimensional Pareto frontier of fairness-accuracy trade-offs uniquely
presented in a multi-task learning setting. We further propose a
Multi-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task
learning. Experiments on several real-world datasets demonstrate the
effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1"&gt;Alex Beutel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prost_F/0/1/0/all/0/1"&gt;Flavien Prost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Optimization of Fluid Systems at Varying Length Scales. (arXiv:2105.13553v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13553</id>
        <link href="http://arxiv.org/abs/2105.13553"/>
        <updated>2021-06-08T02:20:22.664Z</updated>
        <summary type="html"><![CDATA[Autonomous optimization is a process by which hardware conditions are
discovered that generate an optimized experimental product without the guidance
of a domain expert. We design an autonomous optimization framework to discover
the experimental conditions within fluid systems that generate discrete and
uniform droplet patterns. Generating discrete and uniform droplets requires
high-precision control over the experimental conditions of a fluid system.
Fluid stream instabilities, such as Rayleigh-Plateau instability and capillary
instability, drive the separation of a flow into individual droplets. However,
because this phenomenon leverages an instability, by nature the hardware must
be precisely tuned to achieve uniform, repeatable droplets. Typically this
requires a domain expert in the loop and constant re-tuning depending on the
hardware configuration and liquid precursor selection. Herein, we propose a
computer vision-driven Bayesian optimization framework to discover the precise
hardware conditions that generate uniform, reproducible droplets with the
desired features, leveraging flow instability without a domain expert in the
loop. This framework is validated on two fluid systems, at the micrometer and
millimeter length scales, using microfluidic and inkjet systems, respectively,
indicating the application breadth of this approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siemenn_A/0/1/0/all/0/1"&gt;Alexander E. Siemenn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaulsky_E/0/1/0/all/0/1"&gt;Evyatar Shaulsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1"&gt;Matthew Beveridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buonassisi_T/0/1/0/all/0/1"&gt;Tonio Buonassisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashmi_S/0/1/0/all/0/1"&gt;Sara M. Hashmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1"&gt;Iddo Drori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manipulating SGD with Data Ordering Attacks. (arXiv:2104.09667v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09667</id>
        <link href="http://arxiv.org/abs/2104.09667"/>
        <updated>2021-06-08T02:20:22.657Z</updated>
        <summary type="html"><![CDATA[Machine learning is vulnerable to a wide variety of attacks. It is now well
understood that by changing the underlying data distribution, an adversary can
poison the model trained with it or introduce backdoors. In this paper we
present a novel class of training-time attacks that require no changes to the
underlying dataset or model architecture, but instead only change the order in
which data are supplied to the model. In particular, we find that the attacker
can either prevent the model from learning, or poison it to learn behaviours
specified by the attacker. Furthermore, we find that even a single
adversarially-ordered epoch can be enough to slow down model learning, or even
to reset all of the learning progress. Indeed, the attacks presented here are
not specific to the model or dataset, but rather target the stochastic nature
of modern learning procedures. We extensively evaluate our attacks on computer
vision and natural language benchmarks to find that the adversary can disrupt
model training and even introduce backdoors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumaylov_Z/0/1/0/all/0/1"&gt;Zakhar Shumaylov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1"&gt;Dmitry Kazhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiren Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using a Supervised Method without supervision for foreground segmentation. (arXiv:2011.07954v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07954</id>
        <link href="http://arxiv.org/abs/2011.07954"/>
        <updated>2021-06-08T02:20:22.640Z</updated>
        <summary type="html"><![CDATA[Neural networks are a powerful framework for foreground segmentation in video
acquired by static cameras, segmenting moving objects from the background in a
robust way in various challenging scenarios. The premier methods are those
based on supervision requiring a final training stage on a database of tens to
hundreds of manually segmented images from the specific static camera. In this
work, we propose a method to automatically create an "artificial" database that
is sufficient for training the supervised methods so that it performs better
than current unsupervised methods. It is based on combining a weak foreground
segmenter, compared to the supervised method, to extract suitable objects from
the training images and randomly inserting these objects back into a background
image. Test results are shown on the test sequences in CDnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kassel_L/0/1/0/all/0/1"&gt;Levi Kassel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werman_M/0/1/0/all/0/1"&gt;Michael Werman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease Progression. (arXiv:2106.02875v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02875</id>
        <link href="http://arxiv.org/abs/2106.02875"/>
        <updated>2021-06-08T02:20:22.623Z</updated>
        <summary type="html"><![CDATA[Modeling a system's temporal behaviour in reaction to external stimuli is a
fundamental problem in many areas. Pure Machine Learning (ML) approaches often
fail in the small sample regime and cannot provide actionable insights beyond
predictions. A promising modification has been to incorporate expert domain
knowledge into ML models. The application we consider is predicting the
progression of disease under medications, where a plethora of domain knowledge
is available from pharmacology. Pharmacological models describe the dynamics of
carefully-chosen medically meaningful variables in terms of systems of Ordinary
Differential Equations (ODEs). However, these models only describe a limited
collection of variables, and these variables are often not observable in
clinical environments. To close this gap, we propose the latent hybridisation
model (LHM) that integrates a system of expert-designed ODEs with
machine-learned Neural ODEs to fully describe the dynamics of the system and to
link the expert and latent variables to observable quantities. We evaluated LHM
on synthetic data as well as real-world intensive care data of COVID-19
patients. LHM consistently outperforms previous works, especially when few
training samples are available such as at the beginning of the pandemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1"&gt;Zhaozhi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zame_W/0/1/0/all/0/1"&gt;William R. Zame&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleuren_L/0/1/0/all/0/1"&gt;Lucas M. Fleuren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elbers_P/0/1/0/all/0/1"&gt;Paul Elbers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower Bounds on Cross-Entropy Loss in the Presence of Test-time Adversaries. (arXiv:2104.08382v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08382</id>
        <link href="http://arxiv.org/abs/2104.08382"/>
        <updated>2021-06-08T02:20:22.603Z</updated>
        <summary type="html"><![CDATA[Understanding the fundamental limits of robust supervised learning has
emerged as a problem of immense interest, from both practical and theoretical
standpoints. In particular, it is critical to determine classifier-agnostic
bounds on the training loss to establish when learning is possible. In this
paper, we determine optimal lower bounds on the cross-entropy loss in the
presence of test-time adversaries, along with the corresponding optimal
classification outputs. Our formulation of the bound as a solution to an
optimization problem is general enough to encompass any loss function depending
on soft classifier outputs. We also propose and provide a proof of correctness
for a bespoke algorithm to compute this lower bound efficiently, allowing us to
determine lower bounds for multiple practical datasets of interest. We use our
lower bounds as a diagnostic tool to determine the effectiveness of current
robust training methods and find a gap from optimality at larger budgets.
Finally, we investigate the possibility of using of optimal classification
outputs as soft labels to empirically improve robust training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhagoji_A/0/1/0/all/0/1"&gt;Arjun Nitin Bhagoji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cullina_D/0/1/0/all/0/1"&gt;Daniel Cullina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1"&gt;Vikash Sehwag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1"&gt;Prateek Mittal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02835</id>
        <link href="http://arxiv.org/abs/2106.02835"/>
        <updated>2021-06-08T02:20:22.589Z</updated>
        <summary type="html"><![CDATA[Causal discovery from observational data is an important but challenging task
in many scientific fields. Recently, NOTEARS [Zheng et al., 2018] formulates
the causal structure learning problem as a continuous optimization problem
using least-square loss with an acyclicity constraint. Though the least-square
loss function is well justified under the standard Gaussian noise assumption,
it is limited if the assumption does not hold. In this work, we theoretically
show that the violation of the Gaussian noise assumption will hinder the causal
direction identification, making the causal orientation fully determined by the
causal strength as well as the variances of noises in the linear case and the
noises of strong non-Gaussianity in the nonlinear case. Consequently, we
propose a more general entropy-based loss that is theoretically consistent with
the likelihood score under any noise distribution. We run extensive empirical
evaluations on both synthetic data and real-world data to validate the
effectiveness of the proposed method and show that our method achieves the best
in Structure Hamming Distance, False Discovery Rate, and True Positive Rate
matrices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1"&gt;Ruichu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1"&gt;Jie Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1"&gt;Zhifeng Hao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Hawkes Processes for Discovering Time-evolving Communities' States behind Diffusion Processes. (arXiv:2105.11152v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11152</id>
        <link href="http://arxiv.org/abs/2105.11152"/>
        <updated>2021-06-08T02:20:22.582Z</updated>
        <summary type="html"><![CDATA[Sequences of events including infectious disease outbreaks, social network
activities, and crimes are ubiquitous and the data on such events carry
essential information about the underlying diffusion processes between
communities (e.g., regions, online user groups). Modeling diffusion processes
and predicting future events are crucial in many applications including
epidemic control, viral marketing, and predictive policing. Hawkes processes
offer a central tool for modeling the diffusion processes, in which the
influence from the past events is described by the triggering kernel. However,
the triggering kernel parameters, which govern how each community is influenced
by the past events, are assumed to be static over time. In the real world, the
diffusion processes depend not only on the influences from the past, but also
the current (time-evolving) states of the communities, e.g., people's awareness
of the disease and people's current interests. In this paper, we propose a
novel Hawkes process model that is able to capture the underlying dynamics of
community states behind the diffusion processes and predict the occurrences of
events based on the dynamics. Specifically, we model the latent dynamic
function that encodes these hidden dynamics by a mixture of neural networks.
Then we design the triggering kernel using the latent dynamic function and its
integral. The proposed method, termed DHP (Dynamic Hawkes Processes), offers a
flexible way to learn complex representations of the time-evolving communities'
states, while at the same time it allows to computing the exact likelihood,
which makes parameter learning tractable. Extensive experiments on four
real-world event datasets show that DHP outperforms five widely adopted methods
for event prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okawa_M/0/1/0/all/0/1"&gt;Maya Okawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1"&gt;Tomoharu Iwata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_Y/0/1/0/all/0/1"&gt;Yusuke Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_H/0/1/0/all/0/1"&gt;Hiroyuki Toda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurashima_T/0/1/0/all/0/1"&gt;Takeshi Kurashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Causal Explanations for Graph Neural Networks. (arXiv:2104.06643v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06643</id>
        <link href="http://arxiv.org/abs/2104.06643"/>
        <updated>2021-06-08T02:20:22.575Z</updated>
        <summary type="html"><![CDATA[This paper presents Gem, a model-agnostic approach for providing
interpretable explanations for any GNNs on various graph learning tasks.
Specifically, we formulate the problem of providing explanations for the
decisions of GNNs as a causal learning task. Then we train a causal explanation
model equipped with a loss function based on Granger causality. Different from
existing explainers for GNNs, Gem explains GNNs on graph-structured data from a
causal perspective. It has better generalization ability as it has no
requirements on the internal structure of the GNNs or prior knowledge on the
graph learning tasks. In addition, Gem, once trained, can be used to explain
the target GNN very quickly. Our theoretical analysis shows that several recent
explainers fall into a unified framework of additive feature attribution
methods. Experimental results on synthetic and real-world datasets show that
Gem achieves a relative increase of the explanation accuracy by up to $30\%$
and speeds up the explanation process by up to $110\times$ as compared to its
state-of-the-art alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wanyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1"&gt;Hao Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baochun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Training of Tree Ensembles over Continuous Data. (arXiv:2106.02769v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02769</id>
        <link href="http://arxiv.org/abs/2106.02769"/>
        <updated>2021-06-08T02:20:22.556Z</updated>
        <summary type="html"><![CDATA[Most existing Secure Multi-Party Computation (MPC) protocols for
privacy-preserving training of decision trees over distributed data assume that
the features are categorical. In real-life applications, features are often
numerical. The standard ``in the clear'' algorithm to grow decision trees on
data with continuous values requires sorting of training examples for each
feature in the quest for an optimal cut-point in the range of feature values in
each node. Sorting is an expensive operation in MPC, hence finding secure
protocols that avoid such an expensive step is a relevant problem in
privacy-preserving machine learning. In this paper we propose three more
efficient alternatives for secure training of decision tree based models on
data with continuous features, namely: (1) secure discretization of the data,
followed by secure training of a decision tree over the discretized data; (2)
secure discretization of the data, followed by secure training of a random
forest over the discretized data; and (3) secure training of extremely
randomized trees (``extra-trees'') on the original data. Approaches (2) and (3)
both involve randomizing feature choices. In addition, in approach (3)
cut-points are chosen randomly as well, thereby alleviating the need to sort or
to discretize the data up front. We implemented all proposed solutions in the
semi-honest setting with additive secret sharing based MPC. In addition to
mathematically proving that all proposed approaches are correct and secure, we
experimentally evaluated and compared them in terms of classification accuracy
and runtime. We privately train tree ensembles over data sets with 1000s of
instances or features in a few minutes, with accuracies that are at par with
those obtained in the clear. This makes our solution orders of magnitude more
efficient than the existing approaches, which are based on oblivious sorting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1"&gt;Samuel Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_C/0/1/0/all/0/1"&gt;Chaitali Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cock_M/0/1/0/all/0/1"&gt;Martine De Cock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dowsley_R/0/1/0/all/0/1"&gt;Rafael Dowsley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melanson_D/0/1/0/all/0/1"&gt;David Melanson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nascimento_A/0/1/0/all/0/1"&gt;Anderson C. A. Nascimento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Railsback_D/0/1/0/all/0/1"&gt;Davis Railsback&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianwei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder-Decoder Neural Architecture Optimization for Keyword Spotting. (arXiv:2106.02738v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02738</id>
        <link href="http://arxiv.org/abs/2106.02738"/>
        <updated>2021-06-08T02:20:22.549Z</updated>
        <summary type="html"><![CDATA[Keyword spotting aims to identify specific keyword audio utterances. In
recent years, deep convolutional neural networks have been widely utilized in
keyword spotting systems. However, their model architectures are mainly based
on off-the shelfbackbones such as VGG-Net or ResNet, instead of specially
designed for the task. In this paper, we utilize neural architecture search to
design convolutional neural network models that can boost the performance of
keyword spotting while maintaining an acceptable memory footprint.
Specifically, we search the model operators and their connections in a specific
search space with Encoder-Decoder neural architecture optimization. Extensive
evaluations on Google's Speech Commands Dataset show that the model
architecture searched by our approach achieves a state-of-the-art accuracy of
over 97%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_T/0/1/0/all/0/1"&gt;Tong Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Based Learning for Cooperative Games, with Applications to Feature/Data/Model Valuations. (arXiv:2106.02938v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02938</id>
        <link href="http://arxiv.org/abs/2106.02938"/>
        <updated>2021-06-08T02:20:22.543Z</updated>
        <summary type="html"><![CDATA[Valuation problems, such as attribution-based feature interpretation, data
valuation and model valuation for ensembles, become increasingly more important
in many machine learning applications. Such problems are commonly solved by
well-known game-theoretic criteria, such as Shapley value or Banzhaf index. In
this work, we present a novel energy-based treatment for cooperative games,
with a theoretical justification by the maximum entropy framework.
Surprisingly, by conducting variational inference of the energy-based model, we
recover various game-theoretic valuation criteria, such as Shapley value and
Banzhaf index, through conducting one-step gradient ascent for maximizing the
mean-field ELBO objective. This observation also verifies the rationality of
existing criteria, as they are all trying to decouple the correlations among
the players through the mean-field approach. By running gradient ascent for
multiple steps, we achieve a trajectory of the valuations, among which we
define the valuation with the best conceivable decoupling error as the
Variational Index. We experimentally demonstrate that the proposed Variational
Index enjoys intriguing properties on certain synthetic and real-world
valuation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1"&gt;Yatao Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tingyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiaxiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Discrete Variational Derivation of Accelerated Methods in Optimization. (arXiv:2106.02700v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.02700</id>
        <link href="http://arxiv.org/abs/2106.02700"/>
        <updated>2021-06-08T02:20:22.537Z</updated>
        <summary type="html"><![CDATA[Many of the new developments in machine learning are connected with
gradient-based optimization methods. Recently, these methods have been studied
using a variational perspective. This has opened up the possibility of
introducing variational and symplectic integration methods using geometric
integrators. In particular, in this paper, we introduce variational integrators
which allow us to derive different methods for optimization. Using both,
Hamilton's principle and Lagrange-d'Alembert's, we derive two families of
optimization methods in one-to-one correspondence that generalize Polyak's
heavy ball and the well known Nesterov accelerated gradient method, mimicking
the behavior of the latter which reduces the oscillations of typical momentum
methods. However, since the systems considered are explicitly time-dependent,
the preservation of symplecticity of autonomous systems occurs here solely on
the fibers. Several experiments exemplify the result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Campos_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric M. Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mahillo_A/0/1/0/all/0/1"&gt;Alejandro Mahillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Diego_D/0/1/0/all/0/1"&gt;David Mart&amp;#xed;n de Diego&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Transformer for Explainable Recommendation. (arXiv:2105.11601v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11601</id>
        <link href="http://arxiv.org/abs/2105.11601"/>
        <updated>2021-06-08T02:20:22.530Z</updated>
        <summary type="html"><![CDATA[Personalization of natural language generation plays a vital role in a large
spectrum of tasks, such as explainable recommendation, review summarization and
dialog systems. In these tasks, user and item IDs are important identifiers for
personalization. Transformer, which is demonstrated with strong language
modeling capability, however, is not personalized and fails to make use of the
user and item IDs since the ID tokens are not even in the same semantic space
as the words. To address this problem, we present a PErsonalized Transformer
for Explainable Recommendation (PETER), on which we design a simple and
effective learning objective that utilizes the IDs to predict the words in the
target explanation, so as to endow the IDs with linguistic meanings and to
achieve personalized Transformer. Besides generating explanations, PETER can
also make recommendations, which makes it a unified model for the whole
recommendation-explanation pipeline. Extensive experiments show that our small
unpretrained model outperforms fine-tuned BERT on the generation task, in terms
of both effectiveness and efficiency, which highlights the importance and the
nice utility of our design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10972</id>
        <link href="http://arxiv.org/abs/2104.10972"/>
        <updated>2021-06-08T02:20:22.512Z</updated>
        <summary type="html"><![CDATA[ImageNet-1K serves as the primary dataset for pretraining deep learning
models for computer vision tasks. ImageNet-21K dataset, which is bigger and
more diverse, is used less frequently for pretraining, mainly due to its
complexity, low accessibility, and underestimation of its added value. This
paper aims to close this gap, and make high-quality efficient pretraining on
ImageNet-21K available for everyone. Via a dedicated preprocessing stage,
utilization of WordNet hierarchical structure, and a novel training scheme
called semantic softmax, we show that various models significantly benefit from
ImageNet-21K pretraining on numerous datasets and tasks, including small
mobile-oriented models. We also show that we outperform previous ImageNet-21K
pretraining schemes for prominent new models like ViT and Mixer. Our proposed
pretraining pipeline is efficient, accessible, and leads to SoTA reproducible
results, from a publicly available dataset. The training code and pretrained
models are available at: https://github.com/Alibaba-MIIL/ImageNet21K]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Deep Learning under the Fairness Lens. (arXiv:2106.02674v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02674</id>
        <link href="http://arxiv.org/abs/2106.02674"/>
        <updated>2021-06-08T02:20:22.505Z</updated>
        <summary type="html"><![CDATA[Differential Privacy (DP) is an important privacy-enhancing technology for
private machine learning systems. It allows to measure and bound the risk
associated with an individual participation in a computation. However, it was
recently observed that DP learning systems may exacerbate bias and unfairness
for different groups of individuals. This paper builds on these important
observations and sheds light on the causes of the disparate impacts arising in
the problem of differentially private empirical risk minimization. It focuses
on the accuracy disparity arising among groups of individuals in two
well-studied DP learning methods: output perturbation and differentially
private stochastic gradient descent. The paper analyzes which data and model
properties are responsible for the disproportionate impacts, why these aspects
are affecting different groups disproportionately and proposes guidelines to
mitigate these effects. The proposed approach is evaluated on several datasets
and settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1"&gt;Cuong Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_M/0/1/0/all/0/1"&gt;My H. Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fioretto_F/0/1/0/all/0/1"&gt;Ferdinando Fioretto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Task Adaptation using Natural Language. (arXiv:2106.02972v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02972</id>
        <link href="http://arxiv.org/abs/2106.02972"/>
        <updated>2021-06-08T02:20:22.499Z</updated>
        <summary type="html"><![CDATA[Imitation learning and instruction-following are two common approaches to
communicate a user's intent to a learning agent. However, as the complexity of
tasks grows, it could be beneficial to use both demonstrations and language to
communicate with an agent. In this work, we propose a novel setting where an
agent is given both a demonstration and a description, and must combine
information from both the modalities. Specifically, given a demonstration for a
task (the source task), and a natural language description of the differences
between the demonstrated task and a related but different task (the target
task), our goal is to train an agent to complete the target task in a zero-shot
setting, that is, without any demonstrations for the target task. To this end,
we introduce Language-Aided Reward and Value Adaptation (LARVA) which, given a
source demonstration and a linguistic description of how the target task
differs, learns to output a reward / value function that accurately describes
the target task. Our experiments show that on a diverse set of adaptations, our
approach is able to complete more than 95% of target tasks when using
template-based descriptions, and more than 70% when using free-form natural
language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Prasoon Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1"&gt;Raymond J. Mooney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven discovery of interacting particle systems using Gaussian processes. (arXiv:2106.02735v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02735</id>
        <link href="http://arxiv.org/abs/2106.02735"/>
        <updated>2021-06-08T02:20:22.491Z</updated>
        <summary type="html"><![CDATA[Interacting particle or agent systems that display a rich variety of
collection motions are ubiquitous in science and engineering. A fundamental and
challenging goal is to understand the link between individual interaction rules
and collective behaviors. In this paper, we study the data-driven discovery of
distance-based interaction laws in second-order interacting particle systems.
We propose a learning approach that models the latent interaction kernel
functions as Gaussian processes, which can simultaneously fulfill two inference
goals: one is the nonparametric inference of interaction kernel function with
the pointwise uncertainty quantification, and the other one is the inference of
unknown parameters in the non-collective forces of the system. We formulate
learning interaction kernel functions as a statistical inverse problem and
provide a detailed analysis of recoverability conditions, establishing that a
coercivity condition is sufficient for recoverability. We provide a
finite-sample analysis, showing that our posterior mean estimator converges at
an optimal rate equal to the one in the classical 1-dimensional Kernel Ridge
regression. Numerical results on systems that exhibit different collective
behaviors demonstrate efficient learning of our approach from scarce noisy
trajectory data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jinchao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yunxiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_S/0/1/0/all/0/1"&gt;Sui Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of Apophis Asteroid Flyby Optimal Trajectories and Data Fusion of Earth-Apophis Mission Launch Windows using Deep Neural Networks. (arXiv:2104.06249v2 [astro-ph.IM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06249</id>
        <link href="http://arxiv.org/abs/2104.06249"/>
        <updated>2021-06-08T02:20:22.483Z</updated>
        <summary type="html"><![CDATA[In recent years, understanding asteroids has shifted from light worlds to
geological worlds by exploring modern spacecraft and advanced radar and
telescopic surveys. However, flyby in 2029 will be an opportunity to conduct an
internal geophysical study and test the current hypothesis on the effects of
tidal forces on asteroids. The Earth-Apophis mission is driven by additional
factors and scientific goals beyond the unique opportunity for natural
experimentation. However, the internal geophysical structures remain largely
unknown. Understanding the strength and internal integrity of asteroids is not
just a matter of scientific curiosity. It is a practical imperative to advance
knowledge for planetary defense against the possibility of an asteroid impact.
This paper presents a conceptual robotics system required for efficiency at
every stage from entry to post-landing and for asteroid monitoring. In short,
asteroid surveillance missions are futuristic frontiers, with the potential for
technological growth that could revolutionize space exploration. Advanced space
technologies and robotic systems are needed to minimize risk and prepare these
technologies for future missions. A neural network model is implemented to
track and predict asteroids' orbits. Advanced algorithms are also needed to
numerically predict orbital events to minimize error]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ntumba_M/0/1/0/all/0/1"&gt;Manuel Ntumba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gore_S/0/1/0/all/0/1"&gt;Saurabh Gore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Awanyo_J/0/1/0/all/0/1"&gt;Jean-Baptiste Awanyo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Classification of Very Large Images with Tiny Objects. (arXiv:2106.02694v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02694</id>
        <link href="http://arxiv.org/abs/2106.02694"/>
        <updated>2021-06-08T02:20:22.464Z</updated>
        <summary type="html"><![CDATA[An increasing number of applications in the computer vision domain,
specially, in medical imaging and remote sensing, are challenging when the goal
is to classify very large images with tiny objects. More specifically, these
type of classification tasks face two key challenges: $i$) the size of the
input image in the target dataset is usually in the order of megapixels,
however, existing deep architectures do not easily operate on such big images
due to memory constraints, consequently, we seek a memory-efficient method to
process these images; and $ii$) only a small fraction of the input images are
informative of the label of interest, resulting in low region of interest (ROI)
to image ratio. However, most of the current convolutional neural networks
(CNNs) are designed for image classification datasets that have relatively
large ROIs and small image size (sub-megapixel). Existing approaches have
addressed these two challenges in isolation. We present an end-to-end CNN model
termed Zoom-In network that leverages hierarchical attention sampling for
classification of large images with tiny objects using a single GPU. We
evaluate our method on two large-image datasets and one gigapixel dataset.
Experimental results show that our model achieves higher accuracy than existing
methods while requiring less computing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fanjie Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Gradient Fields for Molecular Conformation Generation. (arXiv:2105.03902v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03902</id>
        <link href="http://arxiv.org/abs/2105.03902"/>
        <updated>2021-06-08T02:20:22.457Z</updated>
        <summary type="html"><![CDATA[We study a fundamental problem in computational chemistry known as molecular
conformation generation, trying to predict stable 3D structures from 2D
molecular graphs. Existing machine learning approaches usually first predict
distances between atoms and then generate a 3D structure satisfying the
distances, where noise in predicted distances may induce extra errors during 3D
coordinate generation. Inspired by the traditional force field methods for
molecular dynamics simulation, in this paper, we propose a novel approach
called ConfGF by directly estimating the gradient fields of the log density of
atomic coordinates. The estimated gradient fields allow directly generating
stable conformations via Langevin dynamics. However, the problem is very
challenging as the gradient fields are roto-translation equivariant. We notice
that estimating the gradient fields of atomic coordinates can be translated to
estimating the gradient fields of interatomic distances, and hence develop a
novel algorithm based on recent score-based generative models to effectively
estimate these gradients. Experimental results across multiple tasks show that
ConfGF outperforms previous state-of-the-art baselines by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chence Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shitong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minkai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D-Cliques: Compensating NonIIDness in Decentralized Federated Learning with Topology. (arXiv:2104.07365v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07365</id>
        <link href="http://arxiv.org/abs/2104.07365"/>
        <updated>2021-06-08T02:20:22.450Z</updated>
        <summary type="html"><![CDATA[The convergence speed of machine learning models trained with Federated
Learning is significantly affected by non-independent and identically
distributed (non-IID) data partitions, even more so in a fully decentralized
setting without a central server. In this paper, we show that the impact of
local class bias, an important type of data non-IIDness, can be significantly
reduced by carefully designing the underlying communication topology. We
present D-Cliques, a novel topology that reduces gradient bias by grouping
nodes in interconnected cliques such that the local joint distribution in a
clique is representative of the global class distribution. We also show how to
adapt the updates of decentralized SGD to obtain unbiased gradients and
implement an effective momentum with D-Cliques. Our empirical evaluation on
MNIST and CIFAR10 demonstrates that our approach provides similar convergence
speed as a fully-connected topology with a significant reduction in the number
of edges and messages. In a 1000-node topology, D-Cliques requires 98% less
edges and 96% less total messages, with further possible gains using a
small-world topology across cliques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Bellet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kermarrec_A/0/1/0/all/0/1"&gt;Anne-Marie Kermarrec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavoie_E/0/1/0/all/0/1"&gt;Erick Lavoie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Entropy Subspace Clustering Network. (arXiv:2012.03176v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03176</id>
        <link href="http://arxiv.org/abs/2012.03176"/>
        <updated>2021-06-08T02:20:22.443Z</updated>
        <summary type="html"><![CDATA[Deep subspace clustering networks have attracted much attention in subspace
clustering, in which an auto-encoder non-linearly maps the input data into a
latent space, and a fully connected layer named self-expressiveness module is
introduced to learn the affinity matrix via a typical regularization term
(e.g., sparse or low-rank). However, the adopted regularization terms ignore
the connectivity within each subspace, limiting their clustering performance.
In addition, the adopted framework suffers from the coupling issue between the
auto-encoder module and the self-expressiveness module, making the network
training non-trivial. To tackle these two issues, we propose a novel deep
subspace clustering method named Maximum Entropy Subspace Clustering Network
(MESC-Net). Specifically, MESC-Net maximizes the entropy of the affinity matrix
to promote the connectivity within each subspace, in which its elements
corresponding to the same subspace are uniformly and densely distributed.
Furthermore, we design a novel framework to explicitly decouple the
auto-encoder module and the self-expressiveness module. We also theoretically
prove that the learned affinity matrix satisfies the block-diagonal property
under the independent subspaces. Extensive quantitative and qualitative results
on commonly used benchmark datasets validate MESC-Net significantly outperforms
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhihao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yuheng Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Junhui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qingfu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chromatic and spatial analysis of one-pixel attacks against an image classifier. (arXiv:2105.13771v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13771</id>
        <link href="http://arxiv.org/abs/2105.13771"/>
        <updated>2021-06-08T02:20:22.436Z</updated>
        <summary type="html"><![CDATA[One-pixel attack is a curious way of deceiving neural network classifier by
changing only one pixel in the input image. The full potential and boundaries
of this attack method are not yet fully understood. In this research, the
successful and unsuccessful attacks are studied in more detail to illustrate
the working mechanisms of a one-pixel attack created using differential
evolution. The data comes from our earlier studies where we applied the attack
against medical imaging. We used a real breast cancer tissue dataset and a real
classifier as the attack target. This research presents ways to analyze
chromatic and spatial distributions of one-pixel attacks. In addition, we
present one-pixel attack confidence maps to illustrate the behavior of the
target classifier. We show that the more effective attacks change the color of
the pixel more, and that the successful attacks are situated at the center of
the images. This kind of analysis is not only useful for understanding the
behavior of the attack but also the qualities of the classifying neural
network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alatalo_J/0/1/0/all/0/1"&gt;Janne Alatalo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1"&gt;Joni Korpihalkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1"&gt;Tuomo Sipola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1"&gt;Tero Kokkonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: Pay Less Attention in Vision Transformers. (arXiv:2105.14217v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14217</id>
        <link href="http://arxiv.org/abs/2105.14217"/>
        <updated>2021-06-08T02:20:22.415Z</updated>
        <summary type="html"><![CDATA[Transformers have become one of the dominant architectures in deep learning,
particularly as a powerful alternative to convolutional neural networks (CNNs)
in computer vision. However, Transformer training and inference in previous
works can be prohibitively expensive due to the quadratic complexity of
self-attention over a long sequence of representations, especially for
high-resolution dense prediction tasks. To this end, we present a novel Less
attention vIsion Transformer (LIT), building upon the fact that convolutions,
fully-connected (FC) layers, and self-attentions have almost equivalent
mathematical expressions for processing image patch sequences. Specifically, we
propose a hierarchical Transformer where we use pure multi-layer perceptrons
(MLPs) to encode rich local patterns in the early stages while applying
self-attention modules to capture longer dependencies in deeper layers.
Moreover, we further propose a learned deformable token merging module to
adaptively fuse informative patches in a non-uniform manner. The proposed LIT
achieves promising performance on image recognition tasks, including image
classification, object detection and instance segmentation, serving as a strong
backbone for many vision tasks. Code is available at:
https://github.com/MonashAI/LIT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zizheng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Haoyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Policies with Zero or Bounded Constraint Violation for Constrained MDPs. (arXiv:2106.02684v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02684</id>
        <link href="http://arxiv.org/abs/2106.02684"/>
        <updated>2021-06-08T02:20:22.407Z</updated>
        <summary type="html"><![CDATA[We address the issue of safety in reinforcement learning. We pose the problem
in an episodic framework of a constrained Markov decision process. Existing
results have shown that it is possible to achieve a reward regret of
$\tilde{\mathcal{O}}(\sqrt{K})$ while allowing an
$\tilde{\mathcal{O}}(\sqrt{K})$ constraint violation in $K$ episodes. A
critical question that arises is whether it is possible to keep the constraint
violation even smaller. We show that when a strictly safe policy is known, then
one can confine the system to zero constraint violation with arbitrarily high
probability while keeping the reward regret of order
$\tilde{\mathcal{O}}(\sqrt{K})$. The algorithm which does so employs the
principle of optimistic pessimism in the face of uncertainty to achieve safe
exploration. When no strictly safe policy is known, though one is known to
exist, then it is possible to restrict the system to bounded constraint
violation with arbitrarily high probability. This is shown to be realized by a
primal-dual algorithm with an optimistic primal estimate and a pessimistic dual
update.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1"&gt;Ruida Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalathil_D/0/1/0/all/0/1"&gt;Dileep Kalathil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1"&gt;P. R. Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1"&gt;Chao Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BO-DBA: Query-Efficient Decision-Based Adversarial Attacks via Bayesian Optimization. (arXiv:2106.02732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02732</id>
        <link href="http://arxiv.org/abs/2106.02732"/>
        <updated>2021-06-08T02:20:22.400Z</updated>
        <summary type="html"><![CDATA[Decision-based attacks (DBA), wherein attackers perturb inputs to spoof
learning algorithms by observing solely the output labels, are a type of severe
adversarial attacks against Deep Neural Networks (DNNs) requiring minimal
knowledge of attackers. State-of-the-art DBA attacks relying on zeroth-order
gradient estimation require an excessive number of queries. Recently, Bayesian
optimization (BO) has shown promising in reducing the number of queries in
score-based attacks (SBA), in which attackers need to observe real-valued
probability scores as outputs. However, extending BO to the setting of DBA is
nontrivial because in DBA only output labels instead of real-valued scores, as
needed by BO, are available to attackers. In this paper, we close this gap by
proposing an efficient DBA attack, namely BO-DBA. Different from existing
approaches, BO-DBA generates adversarial examples by searching so-called
\emph{directions of perturbations}. It then formulates the problem as a BO
problem that minimizes the real-valued distortion of perturbations. With the
optimized perturbation generation process, BO-DBA converges much faster than
the state-of-the-art DBA techniques. Experimental results on pre-trained
ImageNet classifiers show that BO-DBA converges within 200 queries while the
state-of-the-art DBA techniques need over 15,000 queries to achieve the same
level of perturbation distortion. BO-DBA also shows similar attack success
rates even as compared to BO-based SBA attacks but with less distortion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shucheng Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. (arXiv:2105.15203v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15203</id>
        <link href="http://arxiv.org/abs/2105.15203"/>
        <updated>2021-06-08T02:20:22.389Z</updated>
        <summary type="html"><![CDATA[We present SegFormer, a simple, efficient yet powerful semantic segmentation
framework which unifies Transformers with lightweight multilayer perception
(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a
novel hierarchically structured Transformer encoder which outputs multiscale
features. It does not need positional encoding, thereby avoiding the
interpolation of positional codes which leads to decreased performance when the
testing resolution differs from training. 2) SegFormer avoids complex decoders.
The proposed MLP decoder aggregates information from different layers, and thus
combining both local attention and global attention to render powerful
representations. We show that this simple and lightweight design is the key to
efficient segmentation on Transformers. We scale our approach up to obtain a
series of models from SegFormer-B0 to SegFormer-B5, reaching significantly
better performance and efficiency than previous counterparts. For example,
SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x
smaller and 2.2% better than the previous best method. Our best model,
SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows
excellent zero-shot robustness on Cityscapes-C. Code will be released at:
github.com/NVlabs/SegFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real Time Video based Heart and Respiration Rate Monitoring. (arXiv:2106.02669v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02669</id>
        <link href="http://arxiv.org/abs/2106.02669"/>
        <updated>2021-06-08T02:20:22.368Z</updated>
        <summary type="html"><![CDATA[In recent years, research about monitoring vital signs by smartphones grows
significantly. There are some special sensors like Electrocardiogram (ECG) and
Photoplethysmographic (PPG) to detect heart rate (HR) and respiration rate
(RR). Smartphone cameras also can measure HR by detecting and processing
imaging Photoplethysmographic (iPPG) signals from the video of a user's face.
Indeed, the variation in the intensity of the green channel can be measured by
the iPPG signals of the video. This study aimed to provide a method to extract
heart rate and respiration rate using the video of individuals' faces. The
proposed method is based on measuring fluctuations in the Hue, and can
therefore extract both HR and RR from the video of a user's face. The proposed
method is evaluated by performing on 25 healthy individuals. For each subject,
20 seconds video of his/her face is recorded. Results show that the proposed
approach of measuring iPPG using Hue gives more accurate rates than the Green
channel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pourbemany_J/0/1/0/all/0/1"&gt;Jafar Pourbemany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Essa_A/0/1/0/all/0/1"&gt;Almabrok Essa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Ye Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity. (arXiv:2106.02692v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02692</id>
        <link href="http://arxiv.org/abs/2106.02692"/>
        <updated>2021-06-08T02:20:22.359Z</updated>
        <summary type="html"><![CDATA[Humans are increasingly interacting with machines through language, sometimes
in contexts where the user may not know they are talking to a machine (like
over the phone or a text chatbot). We aim to understand how system designers
and researchers might allow their systems to confirm its non-human identity. We
collect over 2,500 phrasings related to the intent of ``Are you a robot?". This
is paired with over 2,500 adversarially selected utterances where only
confirming the system is non-human would be insufficient or disfluent. We
compare classifiers to recognize the intent and discuss the precision/recall
and model complexity tradeoffs. Such classifiers could be integrated into
dialog systems to avoid undesired deception. We then explore how both a
generative research model (Blender) as well as two deployed systems (Amazon
Alexa, Google Assistant) handle this intent, finding that systems often fail to
confirm their non-human identity. Finally, we try to understand what a good
response to the intent would be, and conduct a user study to compare the
important aspects when responding to this intent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gros_D/0/1/0/all/0/1"&gt;David Gros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03736</id>
        <link href="http://arxiv.org/abs/2104.03736"/>
        <updated>2021-06-08T02:20:22.234Z</updated>
        <summary type="html"><![CDATA[Meta-learning can extract an inductive bias from previous learning experience
and assist the training processes of new tasks. It is often realized through
optimizing a meta-model with the evaluation loss of a series of task-specific
solvers. Most existing algorithms sample non-overlapping $\mathit{support}$
sets and $\mathit{query}$ sets to train and evaluate the solvers respectively
due to simplicity ($\mathcal{S}/\mathcal{Q}$ protocol). However, another
evaluation method that assesses the discrepancy between the solver and a target
model is short of research ($\mathcal{S}/\mathcal{T}$ protocol).
$\mathcal{S}/\mathcal{T}$ protocol has unique advantages such as offering more
informative supervision, but it is computationally expensive. This paper looks
into this special evaluation method and takes a step towards putting it into
practice. We find that with a small ratio of tasks armed with target models,
classic meta-learning algorithms can be improved a lot without consuming many
resources. Furthermore, we empirically verify the effectiveness of
$\mathcal{S}/\mathcal{T}$ protocol in a typical application of meta-learning,
$\mathit{i.e.}$, few-shot learning. In detail, after constructing target models
by fine-tuning the pre-trained network on those hard tasks, we match the
task-specific solvers to target models via knowledge distillation. Experiments
demonstrate the superiority of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Su Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1"&gt;Le Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ResT: An Efficient Transformer for Visual Recognition. (arXiv:2105.13677v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13677</id>
        <link href="http://arxiv.org/abs/2105.13677"/>
        <updated>2021-06-08T02:20:22.202Z</updated>
        <summary type="html"><![CDATA[This paper presents an efficient multi-scale vision Transformer, called ResT,
that capably served as a general-purpose backbone for image recognition. Unlike
existing Transformer methods, which employ standard Transformer blocks to
tackle raw images with a fixed resolution, our ResT have several advantages:
(1) A memory-efficient multi-head self-attention is built, which compresses the
memory by a simple depth-wise convolution, and projects the interaction across
the attention-heads dimension while keeping the diversity ability of
multi-heads; (2) Position encoding is constructed as spatial attention, which
is more flexible and can tackle with input images of arbitrary size without
interpolation or fine-tune; (3) Instead of the straightforward tokenization at
the beginning of each stage, we design the patch embedding as a stack of
overlapping convolution operation with stride on the 2D-reshaped token map. We
comprehensively validate ResT on image classification and downstream tasks.
Experimental results show that the proposed ResT can outperform the recently
state-of-the-art backbones by a large margin, demonstrating the potential of
ResT as strong backbones. The code and models will be made publicly available
at https://github.com/wofmanaf/ResT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yubin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stylizing 3D Scene via Implicit Representation and HyperNetwork. (arXiv:2105.13016v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13016</id>
        <link href="http://arxiv.org/abs/2105.13016"/>
        <updated>2021-06-08T02:20:22.196Z</updated>
        <summary type="html"><![CDATA[In this work, we aim to address the 3D scene stylization problem - generating
stylized images of the scene at arbitrary novel view angles. A straightforward
solution is to combine existing novel view synthesis and image/video style
transfer approaches, which often leads to blurry results or inconsistent
appearance. Inspired by the high quality results of the neural radiance fields
(NeRF) method, we propose a joint framework to directly render novel views with
the desired style. Our framework consists of two components: an implicit
representation of the 3D scene with the neural radiance field model, and a
hypernetwork to transfer the style information into the scene representation.
In particular, our implicit representation model disentangles the scene into
the geometry and appearance branches, and the hypernetwork learns to predict
the parameters of the appearance branch from the reference style image. To
alleviate the training difficulties and memory burden, we propose a two-stage
training procedure and a patch sub-sampling approach to optimize the style and
content losses with the neural radiance field model. After optimization, our
model is able to render consistent novel views at arbitrary view angles with
arbitrary style. Both quantitative evaluation and human subject study have
demonstrated that the proposed method generates faithful stylization results
with consistent appearance across different views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1"&gt;Pei-Ze Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_M/0/1/0/all/0/1"&gt;Meng-Shiun Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1"&gt;Hung-Yu Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1"&gt;Wei-sheng Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1"&gt;Wei-Chen Chiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06419</id>
        <link href="http://arxiv.org/abs/2103.06419"/>
        <updated>2021-06-08T02:20:22.189Z</updated>
        <summary type="html"><![CDATA[Background and objective: In this paper, a modified U-Net based framework is
presented, which leverages techniques from Squeeze-and-Excitation (SE) block,
Atrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and
robust liver CT segmentation, and the effectiveness of the proposed method was
tested on two public datasets LiTS17 and SLiver07.

Methods: A new network architecture called SAR-U-Net was designed. Firstly,
the SE block is introduced to adaptively extract image features after each
convolution in the U-Net encoder, while suppressing irrelevant regions, and
highlighting features of specific segmentation task; Secondly, ASPP was
employed to replace the transition layer and the output layer, and acquire
multi-scale image information via different receptive fields. Thirdly, to
alleviate the degradation problem, the traditional convolution block was
replaced with the residual block and thus prompt the network to gain accuracy
from considerably increased depth.

Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and
MSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other
closely related 2D-based models, the proposed method achieved the highest
accuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,
ASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared
with other closely related models, the proposed method achieved the highest
segmentation accuracy except for the RVD.

Conclusion: The proposed model enables a great improvement on the accuracy
compared to 2D-based models, and its robustness in circumvent challenging
problems, such as small liver regions, discontinuous liver regions, and fuzzy
liver boundaries, is also well demonstrated and validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1"&gt;Peiqing Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Learning with Gated and Attentive Neighbor Aggregator for Few-Shot Knowledge Graph Completion. (arXiv:2104.13095v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13095</id>
        <link href="http://arxiv.org/abs/2104.13095"/>
        <updated>2021-06-08T02:20:22.169Z</updated>
        <summary type="html"><![CDATA[Aiming at expanding few-shot relations' coverage in knowledge graphs (KGs),
few-shot knowledge graph completion (FKGC) has recently gained more research
interests. Some existing models employ a few-shot relation's multi-hop neighbor
information to enhance its semantic representation. However, noise neighbor
information might be amplified when the neighborhood is excessively sparse and
no neighbor is available to represent the few-shot relation. Moreover, modeling
and inferring complex relations of one-to-many (1-N), many-to-one (N-1), and
many-to-many (N-N) by previous knowledge graph completion approaches requires
high model complexity and a large amount of training instances. Thus, inferring
complex relations in the few-shot scenario is difficult for FKGC models due to
limited training instances. In this paper, we propose a few-shot relational
learning with global-local framework to address the above issues. At the global
stage, a novel gated and attentive neighbor aggregator is built for accurately
integrating the semantics of a few-shot relation's neighborhood, which helps
filtering the noise neighbors even if a KG contains extremely sparse
neighborhoods. For the local stage, a meta-learning based TransH (MTransH)
method is designed to model complex relations and train our model in a few-shot
learning fashion. Extensive experiments show that our model outperforms the
state-of-the-art FKGC approaches on the frequently-used benchmark datasets
NELL-One and Wiki-One. Compared with the strong baseline model MetaR, our model
achieves 5-shot FKGC performance improvements of 8.0% on NELL-One and 2.8% on
Wiki-One by the metric Hits@10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guanglin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chengguang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1"&gt;Ruiying Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jian Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Vision Transformers to Adversarial Examples. (arXiv:2104.02610v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02610</id>
        <link href="http://arxiv.org/abs/2104.02610"/>
        <updated>2021-06-08T02:20:22.163Z</updated>
        <summary type="html"><![CDATA[Recent advances in attention-based networks have shown that Vision
Transformers can achieve state-of-the-art or near state-of-the-art results on
many image classification tasks. This puts transformers in the unique position
of being a promising alternative to traditional convolutional neural networks
(CNNs). While CNNs have been carefully studied with respect to adversarial
attacks, the same cannot be said of Vision Transformers. In this paper, we
study the robustness of Vision Transformers to adversarial examples. Our
analyses of transformer security is divided into three parts. First, we test
the transformer under standard white-box and black-box attacks. Second, we
study the transferability of adversarial examples between CNNs and
transformers. We show that adversarial examples do not readily transfer between
CNNs and transformers. Based on this finding, we analyze the security of a
simple ensemble defense of CNNs and transformers. By creating a new attack, the
self-attention blended gradient attack, we show that such an ensemble is not
secure under a white-box adversary. However, under a black-box adversary, we
show that an ensemble can achieve unprecedented robustness without sacrificing
clean accuracy. Our analysis for this work is done using six types of white-box
attacks and two types of black-box attacks. Our study encompasses multiple
Vision Transformers, Big Transfer Models and CNN architectures trained on
CIFAR-10, CIFAR-100 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_K/0/1/0/all/0/1"&gt;Kaleel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Rigel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1"&gt;Marten van Dijk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Scene Completion via Integrating Instances and Scene in-the-Loop. (arXiv:2104.03640v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03640</id>
        <link href="http://arxiv.org/abs/2104.03640"/>
        <updated>2021-06-08T02:20:22.154Z</updated>
        <summary type="html"><![CDATA[Semantic Scene Completion aims at reconstructing a complete 3D scene with
precise voxel-wise semantics from a single-view depth or RGBD image. It is a
crucial but challenging problem for indoor scene understanding. In this work,
we present a novel framework named Scene-Instance-Scene Network
(\textit{SISNet}), which takes advantages of both instance and scene level
semantic information. Our method is capable of inferring fine-grained shape
details as well as nearby objects whose semantic categories are easily
mixed-up. The key insight is that we decouple the instances from a coarsely
completed semantic scene instead of a raw input image to guide the
reconstruction of instances and the overall scene. SISNet conducts iterative
scene-to-instance (SI) and instance-to-scene (IS) semantic completion.
Specifically, the SI is able to encode objects' surrounding context for
effectively decoupling instances from the scene and each instance could be
voxelized into higher resolution to capture finer details. With IS,
fine-grained instance information can be integrated back into the 3D scene and
thus leads to more accurate semantic scene completion. Utilizing such an
iterative mechanism, the scene and instance completion benefits each other to
achieve higher completion accuracy. Extensively experiments show that our
proposed method consistently outperforms state-of-the-art methods on both real
NYU, NYUCAD and synthetic SUNCG-RGBD datasets. The code and the supplementary
material will be available at \url{https://github.com/yjcaimeow/SISNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yingjie Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuesong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kwan-Yee Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConVEx: Data-Efficient and Few-Shot Slot Labeling. (arXiv:2010.11791v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11791</id>
        <link href="http://arxiv.org/abs/2010.11791"/>
        <updated>2021-06-08T02:20:22.147Z</updated>
        <summary type="html"><![CDATA[We propose ConVEx (Conversational Value Extractor), an efficient pretraining
and fine-tuning neural approach for slot-labeling dialog tasks. Instead of
relying on more general pretraining objectives from prior work (e.g., language
modeling, response selection), ConVEx's pretraining objective, a novel pairwise
cloze task using Reddit data, is well aligned with its intended usage on
sequence labeling tasks. This enables learning domain-specific slot labelers by
simply fine-tuning decoding layers of the pretrained general-purpose sequence
labeling model, while the majority of the pretrained model's parameters are
kept frozen. We report state-of-the-art performance of ConVEx across a range of
diverse domains and data sets for dialog slot-labeling, with the largest gains
in the most challenging, few-shot setups. We believe that ConVEx's reduced
pretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its
efficient fine-tuning and strong performance, promise wider portability and
scalability for data-efficient sequence-labeling tasks in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_M/0/1/0/all/0/1"&gt;Matthew Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1"&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization. (arXiv:2008.11646v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11646</id>
        <link href="http://arxiv.org/abs/2008.11646"/>
        <updated>2021-06-08T02:20:22.136Z</updated>
        <summary type="html"><![CDATA[Cross-view geo-localization is to spot images of the same geographic target
from different platforms, e.g., drone-view cameras and satellites. It is
challenging in the large visual appearance changes caused by extreme viewpoint
variations. Existing methods usually concentrate on mining the fine-grained
feature of the geographic target in the image center, but underestimate the
contextual information in neighbor areas. In this work, we argue that neighbor
areas can be leveraged as auxiliary information, enriching discriminative clues
for geolocalization. Specifically, we introduce a simple and effective deep
neural network, called Local Pattern Network (LPN), to take advantage of
contextual information in an end-to-end manner. Without using extra part
estimators, LPN adopts a square-ring feature partition strategy, which provides
the attention according to the distance to the image center. It eases the part
matching and enables the part-wise representation learning. Owing to the
square-ring partition design, the proposed LPN has good scalability to rotation
variations and achieves competitive results on three prevailing benchmarks,
i.e., University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN
can be easily embedded into other frameworks to further boost performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tingyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chenggang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiyong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yaoqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bolun Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels. (arXiv:2105.04522v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04522</id>
        <link href="http://arxiv.org/abs/2105.04522"/>
        <updated>2021-06-08T02:20:22.114Z</updated>
        <summary type="html"><![CDATA[Prior works have found it beneficial to combine provably noise-robust loss
functions e.g., mean absolute error (MAE) with standard categorical loss
function e.g. cross entropy (CE) to improve their learnability. Here, we
propose to use Jensen-Shannon divergence as a noise-robust loss function and
show that it interestingly interpolate between CE and MAE with a controllable
mixing parameter. Furthermore, we make a crucial observation that CE exhibit
lower consistency around noisy data points. Based on this observation, we adopt
a generalized version of the Jensen-Shannon divergence for multiple
distributions to encourage consistency around data points. Using this loss
function, we show state-of-the-art results on both synthetic (CIFAR), and
real-world (WebVision) noise with varying noise rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Englesson_E/0/1/0/all/0/1"&gt;Erik Englesson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1"&gt;Hossein Azizpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Conditional Disentanglement Framework for Multimodal Brain MR Image Translation. (arXiv:2101.05434v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05434</id>
        <link href="http://arxiv.org/abs/2101.05434"/>
        <updated>2021-06-08T02:20:22.107Z</updated>
        <summary type="html"><![CDATA[Multimodal MRI provides complementary and clinically relevant information to
probe tissue condition and to characterize various diseases. However, it is
often difficult to acquire sufficiently many modalities from the same subject
due to limitations in study plans, while quantitative analysis is still
demanded. In this work, we propose a unified conditional disentanglement
framework to synthesize any arbitrary modality from an input modality. Our
framework hinges on a cycle-constrained conditional adversarial training
approach, where it can extract a modality-invariant anatomical feature with a
modality-agnostic encoder and generate a target modality with a conditioned
decoder. We validate our framework on four MRI modalities, including
T1-weighted, T1 contrast enhanced, T2-weighted, and FLAIR MRI, from the
BraTS'18 database, showing superior performance on synthesis quality over the
comparison methods. In addition, we report results from experiments on a tumor
segmentation task carried out with synthesized data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Registration of serial sections: An evaluation method based on distortions of the ground truths. (arXiv:2011.11060v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11060</id>
        <link href="http://arxiv.org/abs/2011.11060"/>
        <updated>2021-06-08T02:20:22.100Z</updated>
        <summary type="html"><![CDATA[Registration of histological serial sections is a challenging task. Serial
sections exhibit distortions and damage from sectioning. Missing information on
how the tissue looked before cutting makes a realistic validation of 2D
registrations extremely difficult.

This work proposes methods for ground-truth-based evaluation of
registrations. Firstly, we present a methodology to generate test data for
registrations. We distort an innately registered image stack in the manner
similar to the cutting distortion of serial sections. Test cases are generated
from existing 3D data sets, thus the ground truth is known. Secondly, our test
case generation premises evaluation of the registrations with known ground
truths. Our methodology for such an evaluation technique distinguishes this
work from other approaches. Both under- and over-registration become evident in
our evaluations. We also survey existing validation efforts.

We present a full-series evaluation across six different registration methods
applied to our distorted 3D data sets of animal lungs. Our distorted and ground
truth data sets are made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lobachev_O/0/1/0/all/0/1"&gt;Oleg Lobachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funatomi_T/0/1/0/all/0/1"&gt;Takuya Funatomi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfaffenroth_A/0/1/0/all/0/1"&gt;Alexander Pfaffenroth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forster_R/0/1/0/all/0/1"&gt;Reinhold F&amp;#xf6;rster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knudsen_L/0/1/0/all/0/1"&gt;Lars Knudsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wrede_C/0/1/0/all/0/1"&gt;Christoph Wrede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guthe_M/0/1/0/all/0/1"&gt;Michael Guthe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haberthur_D/0/1/0/all/0/1"&gt;David Haberth&amp;#xfc;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hlushchuk_R/0/1/0/all/0/1"&gt;Ruslan Hlushchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salaets_T/0/1/0/all/0/1"&gt;Thomas Salaets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toelen_J/0/1/0/all/0/1"&gt;Jaan Toelen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaffling_S/0/1/0/all/0/1"&gt;Simone Gaffling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muhlfeld_C/0/1/0/all/0/1"&gt;Christian M&amp;#xfc;hlfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grothausmann_R/0/1/0/all/0/1"&gt;Roman Grothausmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relative stability toward diffeomorphisms indicates performance in deep nets. (arXiv:2105.02468v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02468</id>
        <link href="http://arxiv.org/abs/2105.02468"/>
        <updated>2021-06-08T02:20:22.090Z</updated>
        <summary type="html"><![CDATA[Understanding why deep nets can classify data in large dimensions remains a
challenge. It has been proposed that they do so by becoming stable to
diffeomorphisms, yet existing empirical measurements support that it is often
not the case. We revisit this question by defining a maximum-entropy
distribution on diffeomorphisms, that allows to study typical diffeomorphisms
of a given norm. We confirm that stability toward diffeomorphisms does not
strongly correlate to performance on benchmark data sets of images. By
contrast, we find that the stability toward diffeomorphisms relative to that of
generic transformations $R_f$ correlates remarkably with the test error
$\epsilon_t$. It is of order unity at initialization but decreases by several
decades during training for state-of-the-art architectures. For CIFAR10 and 15
known architectures, we find $\epsilon_t\approx 0.2\sqrt{R_f}$, suggesting that
obtaining a small $R_f$ is important to achieve good performance. We study how
$R_f$ depends on the size of the training set and compare it to a simple model
of invariant learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrini_L/0/1/0/all/0/1"&gt;Leonardo Petrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1"&gt;Alessandro Favero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1"&gt;Mario Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Disentanglement in Variational Auto-Encoders Using Jacobian $L_1$ Regularization. (arXiv:2106.02923v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02923</id>
        <link href="http://arxiv.org/abs/2106.02923"/>
        <updated>2021-06-08T02:20:22.084Z</updated>
        <summary type="html"><![CDATA[There have been many recent advances in representation learning; however,
unsupervised representation learning can still struggle with model
identification issues. Variational Auto-Encoders (VAEs) and their extensions
such as $\beta$-VAEs have been shown to locally align latent variables with PCA
directions, which can help to improve model disentanglement under some
conditions. Borrowing inspiration from Independent Component Analysis (ICA) and
sparse coding, we propose applying an $L_1$ loss to the VAE's generative
Jacobian during training to encourage local latent variable alignment with
independent factors of variation in the data. We demonstrate our results on a
variety of datasets, giving qualitative and quantitative results using
information theoretic and modularity measures that show our added $L_1$ cost
encourages local axis alignment of the latent representation with individual
factors of variation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rhodes_T/0/1/0/all/0/1"&gt;Travers Rhodes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Daniel D. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What If We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels. (arXiv:2103.04400v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04400</id>
        <link href="http://arxiv.org/abs/2103.04400"/>
        <updated>2021-06-08T02:20:22.061Z</updated>
        <summary type="html"><![CDATA[Scene text recognition (STR) task has a common practice: All state-of-the-art
STR models are trained on large synthetic data. In contrast to this practice,
training STR models only on fewer real labels (STR with fewer labels) is
important when we have to train STR models without synthetic data: for
handwritten or artistic texts that are difficult to generate synthetically and
for languages other than English for which we do not always have synthetic
data. However, there has been implicit common knowledge that training STR
models on real data is nearly impossible because real data is insufficient. We
consider that this common knowledge has obstructed the study of STR with fewer
labels. In this work, we would like to reactivate STR with fewer labels by
disproving the common knowledge. We consolidate recently accumulated public
real data and show that we can train STR models satisfactorily only with real
labeled data. Subsequently, we find simple data augmentation to fully exploit
real data. Furthermore, we improve the models by collecting unlabeled data and
introducing semi- and self-supervised methods. As a result, we obtain a
competitive model to state-of-the-art methods. To the best of our knowledge,
this is the first study that 1) shows sufficient performance by only using real
labels and 2) introduces semi- and self-supervised methods into STR with fewer
labels. Our code and data are available:
https://github.com/ku21fan/STR-Fewer-Labels]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1"&gt;Jeonghun Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsui_Y/0/1/0/all/0/1"&gt;Yusuke Matsui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1"&gt;Kiyoharu Aizawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Local Self-Attention for Parameter Efficient Visual Backbones. (arXiv:2103.12731v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12731</id>
        <link href="http://arxiv.org/abs/2103.12731"/>
        <updated>2021-06-08T02:20:22.054Z</updated>
        <summary type="html"><![CDATA[Self-attention has the promise of improving computer vision systems due to
parameter-independent scaling of receptive fields and content-dependent
interactions, in contrast to parameter-dependent scaling and
content-independent interactions of convolutions. Self-attention models have
recently been shown to have encouraging improvements on accuracy-parameter
trade-offs compared to baseline convolutional models such as ResNet-50. In this
work, we aim to develop self-attention models that can outperform not just the
canonical baseline models, but even the high-performing convolutional models.
We propose two extensions to self-attention that, in conjunction with a more
efficient implementation of self-attention, improve the speed, memory usage,
and accuracy of these models. We leverage these improvements to develop a new
self-attention model family, HaloNets, which reach state-of-the-art accuracies
on the parameter-limited setting of the ImageNet classification benchmark. In
preliminary transfer learning experiments, we find that HaloNet models
outperform much larger models and have better inference performance. On harder
tasks such as object detection and instance segmentation, our simple local
self-attention and convolutional hybrids show improvements over very strong
baselines. These results mark another step in demonstrating the efficacy of
self-attention models on settings traditionally dominated by convolutional
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1"&gt;Ashish Vaswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1"&gt;Prajit Ramachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1"&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1"&gt;Niki Parmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hechtman_B/0/1/0/all/0/1"&gt;Blake Hechtman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiscoBox: Weakly Supervised Instance Segmentation and Semantic Correspondence from Box Supervision. (arXiv:2105.06464v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06464</id>
        <link href="http://arxiv.org/abs/2105.06464"/>
        <updated>2021-06-08T02:20:22.044Z</updated>
        <summary type="html"><![CDATA[We introduce DiscoBox, a novel framework that jointly learns instance
segmentation and semantic correspondence using bounding box supervision.
Specifically, we propose a self-ensembling framework where instance
segmentation and semantic correspondence are jointly guided by a structured
teacher in addition to the bounding box supervision. The teacher is a
structured energy model incorporating a pairwise potential and a cross-image
potential to model the pairwise pixel relationships both within and across the
boxes. Minimizing the teacher energy simultaneously yields refined object masks
and dense correspondences between intra-class objects, which are taken as
pseudo-labels to supervise the task network and provide positive/negative
correspondence pairs for dense constrastive learning. We show a symbiotic
relationship where the two tasks mutually benefit from each other. Our best
model achieves 37.9% AP on COCO instance segmentation, surpassing prior weakly
supervised methods and is competitive to supervised methods. We also obtain
state of the art weakly supervised results on PASCAL VOC12 and PF-PASCAL with
real-time inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1"&gt;Shiyi Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1"&gt;Christopher Choy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_S/0/1/0/all/0/1"&gt;Subhashree Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry S. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sattiy at SemEval-2021 Task 9: An Ensemble Solution for Statement Verification and Evidence Finding with Tables. (arXiv:2104.10366v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10366</id>
        <link href="http://arxiv.org/abs/2104.10366"/>
        <updated>2021-06-08T02:20:22.037Z</updated>
        <summary type="html"><![CDATA[Question answering from semi-structured tables can be seen as a semantic
parsing task and is significant and practical for pushing the boundary of
natural language understanding. Existing research mainly focuses on
understanding contents from unstructured evidence, e.g., news, natural language
sentences, and documents. The task of verification from structured evidence,
such as tables, charts, and databases, is still less explored. This paper
describes sattiy team's system in SemEval-2021 task 9: Statement Verification
and Evidence Finding with Tables (SEM-TAB-FACT). This competition aims to
verify statements and to find evidence from tables for scientific articles and
to promote the proper interpretation of the surrounding article. In this paper,
we exploited ensemble models of pre-trained language models over tables, TaPas
and TaBERT, for Task A and adjust the result based on some rules extracted for
Task B. Finally, in the leaderboard, we attain the F1 scores of 0.8496 and
0.7732 in Task A for the 2-way and 3-way evaluation, respectively, and the F1
score of 0.4856 in Task B.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1"&gt;Xiaoyi Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1"&gt;Meizhi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lianxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Resolution Network. (arXiv:2106.02898v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02898</id>
        <link href="http://arxiv.org/abs/2106.02898"/>
        <updated>2021-06-08T02:20:22.029Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) are often of sophisticated design
with numerous convolutional layers and learnable parameters for the accuracy
reason. To alleviate the expensive costs of deploying them on mobile devices,
recent works have made huge efforts for excavating redundancy in pre-defined
architectures. Nevertheless, the redundancy on the input resolution of modern
CNNs has not been fully investigated, i.e., the resolution of input image is
fixed. In this paper, we observe that the smallest resolution for accurately
predicting the given image is different using the same neural network. To this
end, we propose a novel dynamic-resolution network (DRNet) in which the
resolution is determined dynamically based on each input sample. Thus, a
resolution predictor with negligible computational costs is explored and
optimized jointly with the desired network. In practice, the predictor learns
the smallest resolution that can retain and even exceed the original
recognition accuracy for each image. During the inference, each input image
will be resized to its predicted resolution for minimizing the overall
computation burden. We then conduct extensive experiments on several benchmark
networks and datasets. The results show that our DRNet can be embedded in any
off-the-shelf network architecture to obtain a considerable reduction in
computational complexity. For instance, DRNet achieves similar performance with
an about 34% computation reduction, while gains 1.4% accuracy increase with 10%
computation reduction compared to the original ResNet-50 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingjian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1"&gt;Enhua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiulin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Ying Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zhenzhong Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Long Context NLM for ASR Rescoring in Conversational Agents. (arXiv:2104.11070v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11070</id>
        <link href="http://arxiv.org/abs/2104.11070"/>
        <updated>2021-06-08T02:20:22.023Z</updated>
        <summary type="html"><![CDATA[Neural Language Models (NLM), when trained and evaluated with context
spanning multiple utterances, have been shown to consistently outperform both
conventional n-gram language models and NLMs that use limited context. In this
paper, we investigate various techniques to incorporate turn based context
history into both recurrent (LSTM) and Transformer-XL based NLMs. For recurrent
based NLMs, we explore context carry over mechanism and feature based
augmentation, where we incorporate other forms of contextual information such
as bot response and system dialogue acts as classified by a Natural Language
Understanding (NLU) model. To mitigate the sharp nearby, fuzzy far away problem
with contextual NLM, we propose the use of attention layer over lexical
metadata to improve feature based augmentation. Additionally, we adapt our
contextual NLM towards user provided on-the-fly speech patterns by leveraging
encodings from a large pre-trained masked language model and performing fusion
with a Transformer-XL based NLM. We test our proposed models using N-best
rescoring of ASR hypotheses of task-oriented dialogues and also evaluate on
downstream NLU tasks such as intent classification and slot labeling. The best
performing model shows a relative WER between 1.6% and 9.1% and a slot labeling
F1 score improvement of 4% over non-contextual baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1"&gt;Monica Sunkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ronanki_S/0/1/0/all/0/1"&gt;Srikanth Ronanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Geometric Matrix Completion. (arXiv:1911.07255v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07255</id>
        <link href="http://arxiv.org/abs/1911.07255"/>
        <updated>2021-06-08T02:20:22.003Z</updated>
        <summary type="html"><![CDATA[Deep Matrix Factorization (DMF) is an emerging approach to the problem of
matrix completion. Recent works have established that gradient descent applied
to a DMF model induces an implicit regularization on the rank of the recovered
matrix. In this work we interpret the DMF model through the lens of spectral
geometry. This allows us to incorporate explicit regularization without
breaking the DMF structure, thus enjoying the best of both worlds. In
particular, we focus on matrix completion problems with underlying geometric or
topological relations between the rows and/or columns. Such relations are
prevalent in matrix completion problems that arise in many applications, such
as recommender systems and drug-target interaction. Our contributions enable
DMF models to exploit these relations, and make them competitive on real
benchmarks, while exhibiting one of the first successful applications of deep
linear networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boyarski_A/0/1/0/all/0/1"&gt;Amit Boyarski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1"&gt;Sanketh Vedula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1"&gt;Alex Bronstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PCT: Point cloud transformer. (arXiv:2012.09688v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09688</id>
        <link href="http://arxiv.org/abs/2012.09688"/>
        <updated>2021-06-08T02:20:21.995Z</updated>
        <summary type="html"><![CDATA[The irregular domain and lack of ordering make it challenging to design deep
neural networks for point cloud processing. This paper presents a novel
framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is
based on Transformer, which achieves huge success in natural language
processing and displays great potential in image processing. It is inherently
permutation invariant for processing a sequence of points, making it
well-suited for point cloud learning. To better capture local context within
the point cloud, we enhance input embedding with the support of farthest point
sampling and nearest neighbor search. Extensive experiments demonstrate that
the PCT achieves the state-of-the-art performance on shape classification, part
segmentation and normal estimation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Meng-Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jun-Xiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheng-Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tai-Jiang Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Ralph R. Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shi-Min Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Search Asymmetry: Deep Nets and Humans Share Similar Inherent Biases. (arXiv:2106.02953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02953</id>
        <link href="http://arxiv.org/abs/2106.02953"/>
        <updated>2021-06-08T02:20:21.988Z</updated>
        <summary type="html"><![CDATA[Visual search is a ubiquitous and often challenging daily task, exemplified
by looking for the car keys at home or a friend in a crowd. An intriguing
property of some classical search tasks is an asymmetry such that finding a
target A among distractors B can be easier than finding B among A. To elucidate
the mechanisms responsible for asymmetry in visual search, we propose a
computational model that takes a target and a search image as inputs and
produces a sequence of eye movements until the target is found. The model
integrates eccentricity-dependent visual recognition with target-dependent
top-down cues. We compared the model against human behavior in six paradigmatic
search tasks that show asymmetry in humans. Without prior exposure to the
stimuli or task-specific training, the model provides a plausible mechanism for
search asymmetry. We hypothesized that the polarity of search asymmetry arises
from experience with the natural environment. We tested this hypothesis by
training the model on an augmented version of ImageNet where the biases of
natural images were either removed or reversed. The polarity of search
asymmetry disappeared or was altered depending on the training protocol. This
study highlights how classical perceptual properties can emerge in neural
network models, without the need for task-specific training, but rather as a
consequence of the statistical properties of the developmental diet fed to the
model. All source code and stimuli are publicly available
https://github.com/kreimanlab/VisualSearchAsymmetry]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Shashi Kant Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengmi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chia-Chien Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolfe_J/0/1/0/all/0/1"&gt;Jeremy M. Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1"&gt;Gabriel Kreiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch Slimming for Efficient Vision Transformers. (arXiv:2106.02852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02852</id>
        <link href="http://arxiv.org/abs/2106.02852"/>
        <updated>2021-06-08T02:20:21.981Z</updated>
        <summary type="html"><![CDATA[This paper studies the efficiency problem for visual transformers by
excavating redundant calculation in given networks. The recent transformer
architecture has demonstrated its effectiveness for achieving excellent
performance on a series of computer vision tasks. However, similar to that of
convolutional neural networks, the huge computational cost of vision
transformers is still a severe issue. Considering that the attention mechanism
aggregates different patches layer-by-layer, we present a novel patch slimming
approach that discards useless patches in a top-down paradigm. We first
identify the effective patches in the last layer and then use them to guide the
patch selection process of previous layers. For each layer, the impact of a
patch on the final output feature is approximated and patches with less impact
will be removed. Experimental results on benchmark datasets demonstrate that
the proposed method can significantly reduce the computational costs of vision
transformers without affecting their performances. For example, over 45% FLOPs
of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the
ImageNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jianyuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Joint Sparse Non-negative Matrix Factorization Framework for Identifying the Common and Subject-specific Functional Units of Tongue Motion During Speech. (arXiv:2007.04865v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04865</id>
        <link href="http://arxiv.org/abs/2007.04865"/>
        <updated>2021-06-08T02:20:21.974Z</updated>
        <summary type="html"><![CDATA[Intelligible speech is produced by creating varying internal local muscle
groupings -- i.e., functional units -- that are generated in a systematic and
coordinated manner. There are two major challenges in characterizing and
analyzing functional units.~First, due to the complex and convoluted nature of
tongue structure and function, it is of great importance to develop a method
that can accurately decode complex muscle coordination patterns during speech.
Second, it is challenging to keep identified functional units across subjects
comparable due to their substantial variability. In this work, to address these
challenges, we develop a new deep learning framework to identify common and
subject-specific functional units of tongue motion during speech.~Our framework
hinges on joint deep graph-regularized sparse non-negative matrix factorization
(NMF) using motion quantities derived from displacements by tagged Magnetic
Resonance Imaging. More specifically, we transform NMF with sparse and graph
regularizations into modular architectures akin to deep neural networks by
means of unfolding the Iterative Shrinkage-Thresholding Algorithm to learn
interpretable building blocks and associated weighting map. We then apply
spectral clustering to common and subject-specific weighting maps from which we
jointly determine the common and subject-specific functional units. Experiments
carried out with simulated datasets show that the proposed method achieved on
par or better clustering performance over the comparison methods. Experiments
carried out with in vivo tongue motion data show that the proposed method can
determine the common and subject-specific functional units with increased
interpretability and decreased size variability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1"&gt;Maureen Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1"&gt;Arnold Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reese_T/0/1/0/all/0/1"&gt;Timothy G. Reese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wedeen_V/0/1/0/all/0/1"&gt;Van J. Wedeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation. (arXiv:2010.06255v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06255</id>
        <link href="http://arxiv.org/abs/2010.06255"/>
        <updated>2021-06-08T02:20:21.955Z</updated>
        <summary type="html"><![CDATA[Aerial tracking, which has exhibited its omnipresent dedication and splendid
performance, is one of the most active applications in the remote sensing
field. Especially, unmanned aerial vehicle (UAV)-based remote sensing system,
equipped with a visual tracking approach, has been widely used in aviation,
navigation, agriculture,transportation, and public security, etc. As is
mentioned above, the UAV-based aerial tracking platform has been gradually
developed from research to practical application stage, reaching one of the
main aerial remote sensing technologies in the future. However, due to the
real-world onerous situations, e.g., harsh external challenges, the vibration
of the UAV mechanical structure (especially under strong wind conditions), the
maneuvering flight in complex environment, and the limited computation
resources onboard, accuracy, robustness, and high efficiency are all crucial
for the onboard tracking methods. Recently, the discriminative correlation
filter (DCF)-based trackers have stood out for their high computational
efficiency and appealing robustness on a single CPU, and have flourished in the
UAV visual tracking community. In this work, the basic framework of the
DCF-based trackers is firstly generalized, based on which, 23 state-of-the-art
DCF-based trackers are orderly summarized according to their innovations for
solving various issues. Besides, exhaustive and quantitative experiments have
been extended on various prevailing UAV tracking benchmarks, i.e., UAV123,
UAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903
frames in total. The experiments show the performance, verify the feasibility,
and demonstrate the current challenges of DCF-based trackers onboard UAV
tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fuling Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Geng Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor Imagery Recognition. (arXiv:2005.00777v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00777</id>
        <link href="http://arxiv.org/abs/2005.00777"/>
        <updated>2021-06-08T02:20:21.917Z</updated>
        <summary type="html"><![CDATA[Recognition accuracy and response time are both critically essential ahead of
building practical electroencephalography (EEG) based brain-computer interface
(BCI). Recent approaches, however, have either compromised in the
classification accuracy or responding time. This paper presents a novel deep
learning approach designed towards remarkably accurate and responsive motor
imagery (MI) recognition based on scalp EEG. Bidirectional Long Short-term
Memory (BiLSTM) with the Attention mechanism manages to derive relevant
features from raw EEG signals. The connected graph convolutional neural network
(GCN) promotes the decoding performance by cooperating with the topological
structure of features, which are estimated from the overall data. The
0.4-second detection framework has shown effective and efficient prediction
based on individual and group-wise training, with 98.81% and 94.64% accuracy,
respectively, which outperformed all the state-of-the-art studies. The
introduced deep feature mining approach can precisely recognize human motion
intents from raw EEG signals, which paves the road to translate the EEG based
MI recognition to practical BCI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yimin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Shuyue Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lun_X/0/1/0/all/0/1"&gt;Xiangmin Lun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Fourier Features for Multi-DimensionalSpatial Positional Encoding. (arXiv:2106.02795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02795</id>
        <link href="http://arxiv.org/abs/2106.02795"/>
        <updated>2021-06-08T02:20:21.870Z</updated>
        <summary type="html"><![CDATA[Attentional mechanisms are order-invariant. Positional encoding is a crucial
component to allow attention-based deep model architectures such as Transformer
to address sequences or images where the position of information matters. In
this paper, we propose a novel positional encoding method based on learnable
Fourier features. Instead of hard-coding each position as a token or a vector,
we represent each position, which can be multi-dimensional, as a trainable
encoding based on learnable Fourier feature mapping, modulated with a
multi-layer perceptron. The representation is particularly advantageous for a
spatial multi-dimensional position, e.g., pixel positions on an image, where
$L_2$ distances or more complex positional relationships need to be captured.
Our experiments based on several public benchmark tasks show that our learnable
Fourier feature representation for multi-dimensional positional encoding
outperforms existing methods by both improving the accuracy and allowing faster
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1"&gt;Si Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Category Contrast for Unsupervised Domain Adaptation in Visual Tasks. (arXiv:2106.02885v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02885</id>
        <link href="http://arxiv.org/abs/2106.02885"/>
        <updated>2021-06-08T02:20:21.862Z</updated>
        <summary type="html"><![CDATA[Instance contrast for unsupervised representation learning has achieved great
success in recent years. In this work, we explore the idea of instance
contrastive learning in unsupervised domain adaptation (UDA) and propose a
novel Category Contrast technique (CaCo) that introduces semantic priors on top
of instance discrimination for visual UDA tasks. By considering instance
contrastive learning as a dictionary look-up operation, we construct a
semantics-aware dictionary with samples from both source and target domains
where each target sample is assigned a (pseudo) category label based on the
category priors of source samples. This allows category contrastive learning
(between target queries and the category-level dictionary) for
category-discriminative yet domain-invariant feature representations: samples
of the same category (from either source or target domain) are pulled closer
while those of different categories are pushed apart simultaneously. Extensive
UDA experiments in multiple visual tasks ($e.g.$, segmentation, classification
and detection) show that the simple implementation of CaCo achieves superior
performance as compared with the highly-optimized state-of-the-art methods.
Analytically and empirically, the experiments also demonstrate that CaCo is
complementary to existing UDA methods and generalizable to other learning
setups such as semi-supervised learning, unsupervised model adaptation, etc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning. (arXiv:2009.08348v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08348</id>
        <link href="http://arxiv.org/abs/2009.08348"/>
        <updated>2021-06-08T02:20:21.830Z</updated>
        <summary type="html"><![CDATA[Deep Metric Learning (DML) provides a crucial tool for visual similarity and
zero-shot applications by learning generalizing embedding spaces, although
recent work in DML has shown strong performance saturation across training
objectives. However, generalization capacity is known to scale with the
embedding space dimensionality. Unfortunately, high dimensional embeddings also
create higher retrieval cost for downstream applications. To remedy this, we
propose \emph{Simultaneous Similarity-based Self-distillation (S2SD). S2SD
extends DML with knowledge distillation from auxiliary, high-dimensional
embedding and feature spaces to leverage complementary context during training
while retaining test-time cost and with negligible changes to the training
time. Experiments and ablations across different objectives and standard
benchmarks show S2SD offers notable improvements of up to 7% in Recall@1, while
also setting a new state-of-the-art. Code available at
https://github.com/MLforHealth/S2SD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1"&gt;Timo Milbich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Joseph Paul Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Region-aware Adaptive Instance Normalization for Image Harmonization. (arXiv:2106.02853v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02853</id>
        <link href="http://arxiv.org/abs/2106.02853"/>
        <updated>2021-06-08T02:20:21.794Z</updated>
        <summary type="html"><![CDATA[Image composition plays a common but important role in photo editing. To
acquire photo-realistic composite images, one must adjust the appearance and
visual style of the foreground to be compatible with the background. Existing
deep learning methods for harmonizing composite images directly learn an image
mapping network from the composite to the real one, without explicit
exploration on visual style consistency between the background and the
foreground images. To ensure the visual style consistency between the
foreground and the background, in this paper, we treat image harmonization as a
style transfer problem. In particular, we propose a simple yet effective
Region-aware Adaptive Instance Normalization (RAIN) module, which explicitly
formulates the visual style from the background and adaptively applies them to
the foreground. With our settings, our RAIN module can be used as a drop-in
module for existing image harmonization networks and is able to bring
significant improvements. Extensive experiments on the existing image
harmonization benchmark datasets show the superior capability of the proposed
method. Code is available at {https://github.com/junleen/RainNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1"&gt;Jun Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Han Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Li Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1"&gt;Rong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xiao Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Networks with Gated Recurrent Connections. (arXiv:2106.02859v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02859</id>
        <link href="http://arxiv.org/abs/2106.02859"/>
        <updated>2021-06-08T02:20:21.772Z</updated>
        <summary type="html"><![CDATA[The convolutional neural network (CNN) has become a basic model for solving
many computer vision problems. In recent years, a new class of CNNs, recurrent
convolution neural network (RCNN), inspired by abundant recurrent connections
in the visual systems of animals, was proposed. The critical element of RCNN is
the recurrent convolutional layer (RCL), which incorporates recurrent
connections between neurons in the standard convolutional layer. With
increasing number of recurrent computations, the receptive fields (RFs) of
neurons in RCL expand unboundedly, which is inconsistent with biological facts.
We propose to modulate the RFs of neurons by introducing gates to the recurrent
connections. The gates control the amount of context information inputting to
the neurons and the neurons' RFs therefore become adaptive. The resulting layer
is called gated recurrent convolution layer (GRCL). Multiple GRCLs constitute a
deep model called gated RCNN (GRCNN). The GRCNN was evaluated on several
computer vision tasks including object recognition, scene text recognition and
object detection, and obtained much better results than the RCNN. In addition,
when combined with other adaptive RF techniques, the GRCNN demonstrated
competitive performance to the state-of-the-art models on benchmark datasets
for these tasks. The codes are released at
\href{https://github.com/Jianf-Wang/GRCNN}{https://github.com/Jianf-Wang/GRCNN}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaolin Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principle Bit Analysis: Autoencoding with Schur-Concave Loss. (arXiv:2106.02796v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02796</id>
        <link href="http://arxiv.org/abs/2106.02796"/>
        <updated>2021-06-08T02:20:21.763Z</updated>
        <summary type="html"><![CDATA[We consider a linear autoencoder in which the latent variables are quantized,
or corrupted by noise, and the constraint is Schur-concave in the set of latent
variances. Although finding the optimal encoder/decoder pair for this setup is
a nonconvex optimization problem, we show that decomposing the source into its
principal components is optimal. If the constraint is strictly Schur-concave
and the empirical covariance matrix has only simple eigenvalues, then any
optimal encoder/decoder must decompose the source in this way. As one
application, we consider a strictly Schur-concave constraint that estimates the
number of bits needed to represent the latent variables under fixed-rate
encoding, a setup that we call \emph{Principal Bit Analysis (PBA)}. This yields
a practical, general-purpose, fixed-rate compressor that outperforms existing
algorithms. As a second application, we show that a prototypical
autoencoder-based variable-rate compressor is guaranteed to decompose the
source into its principal components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhadane_S/0/1/0/all/0/1"&gt;Sourbh Bhadane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1"&gt;Aaron B. Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1"&gt;Jayadev Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Video Generation for Complex Data. (arXiv:2106.02719v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02719</id>
        <link href="http://arxiv.org/abs/2106.02719"/>
        <updated>2021-06-08T02:20:21.755Z</updated>
        <summary type="html"><![CDATA[Videos can often be created by first outlining a global description of the
scene and then adding local details. Inspired by this we propose a hierarchical
model for video generation which follows a coarse to fine approach. First our
model generates a low resolution video, establishing the global scene
structure, that is then refined by subsequent levels in the hierarchy. We train
each level in our hierarchy sequentially on partial views of the videos. This
reduces the computational complexity of our generative model, which scales to
high-resolution videos beyond a few frames. We validate our approach on
Kinetics-600 and BDD100K, for which we train a three level model capable of
generating 256x256 videos with 48 frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castrejon_L/0/1/0/all/0/1"&gt;Lluis Castrejon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1"&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shellcode_IA32: A Dataset for Automatic Shellcode Generation. (arXiv:2104.13100v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13100</id>
        <link href="http://arxiv.org/abs/2104.13100"/>
        <updated>2021-06-08T02:20:21.731Z</updated>
        <summary type="html"><![CDATA[We take the first step to address the task of automatically generating
shellcodes, i.e., small pieces of code used as a payload in the exploitation of
a software vulnerability, starting from natural language comments. We assemble
and release a novel dataset (Shellcode_IA32), consisting of challenging but
common assembly instructions with their natural language descriptions. We
experiment with standard methods in neural machine translation (NMT) to
establish baseline performance levels on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liguori_P/0/1/0/all/0/1"&gt;Pietro Liguori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Hossami_E/0/1/0/all/0/1"&gt;Erfan Al-Hossami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotroneo_D/0/1/0/all/0/1"&gt;Domenico Cotroneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natella_R/0/1/0/all/0/1"&gt;Roberto Natella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cukic_B/0/1/0/all/0/1"&gt;Bojan Cukic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaikh_S/0/1/0/all/0/1"&gt;Samira Shaikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Random Network for Fine-grained Image Classification. (arXiv:2103.07230v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07230</id>
        <link href="http://arxiv.org/abs/2103.07230"/>
        <updated>2021-06-08T02:20:21.716Z</updated>
        <summary type="html"><![CDATA[Deep Convolutional Neural Network (DCNN) and Transformer have achieved
remarkable successes in image recognition. However, their performance in
fine-grained image recognition is still difficult to meet the requirements of
actual needs. This paper proposes a Sequence Random Network (SRN) to enhance
the performance of DCNN. The output of DCNN is one-dimensional features. This
one-dimensional feature abstractly represents image information, but it does
not express well the detailed information of image. To address this issue, we
use the proposed SRN which composed of BiLSTM and several Tanh-Dropout blocks
(called BiLSTM-TDN), to further process DCNN one-dimensional features for
highlighting the detail information of image. After the feature transform by
BiLSTM-TDN, the recognition performance has been greatly improved. We conducted
the experiments on six fine-grained image datasets. Except for FGVC-Aircraft,
the accuracies of the proposed methods on the other datasets exceeded 99%.
Experimental results show that BiLSTM-TDN is far superior to the existing
state-of-the-art methods. In addition to DCNN, BiLSTM-TDN can also be extended
to other models, such as Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chaorong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Malu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1"&gt;Fengqing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Anping Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuanyuan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of fake news on CoViD-19 on Web Search Engines. (arXiv:2103.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11804</id>
        <link href="http://arxiv.org/abs/2103.11804"/>
        <updated>2021-06-08T02:20:21.706Z</updated>
        <summary type="html"><![CDATA[In early January 2020, after China reported the first cases of the new
coronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully
accurate information has started spreading faster than the virus itself.
Alongside this pandemic, people have experienced a parallel infodemic, i.e., an
overabundance of information, some of which misleading or even harmful, that
has widely spread around the globe. Although Social Media are increasingly
being used as information source, Web Search Engines, like Google or Yahoo!,
still represent a powerful and trustworthy resource for finding information on
the Web. This is due to their capability to capture the largest amount of
information, helping users quickly identify the most relevant, useful, although
not always the most reliable, results for their search queries. This study aims
to detect potential misleading and fake contents by capturing and analysing
textual information, which flow through Search Engines. By using a real-world
dataset associated with recent CoViD-19 pandemic, we first apply re-sampling
techniques for class imbalance, then we use existing Machine Learning
algorithms for classification of not reliable news. By extracting lexical and
host-based features of associated Uniform Resource Locators (URLs) for news
articles, we show that the proposed methods, so common in phishing and
malicious URLs detection, can improve the efficiency and performance of
classifiers. Based on these findings, we suggest that the use of both textual
and URLs features can improve the effectiveness of fake news detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzeo_V/0/1/0/all/0/1"&gt;V. Mazzeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapisarda_A/0/1/0/all/0/1"&gt;A. Rapisarda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giuffrida_G/0/1/0/all/0/1"&gt;G. Giuffrida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding. (arXiv:2009.06097v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06097</id>
        <link href="http://arxiv.org/abs/2009.06097"/>
        <updated>2021-06-08T02:20:21.699Z</updated>
        <summary type="html"><![CDATA[Transformer has become ubiquitous in the deep learning field. One of the key
ingredients that destined its success is the self-attention mechanism, which
allows fully-connected contextual encoding over input tokens. However, despite
its effectiveness in modeling short sequences, self-attention suffers when
handling inputs with extreme long-range dependencies, as its complexity grows
quadratically with respect to the sequence length. Therefore, long sequences
are often encoded by Transformer in chunks using a sliding window. In this
paper, we propose Cluster-Former, a novel clustering-based sparse Transformer
to perform attention across chunked sequences. The proposed framework is
pivoted on two unique types of Transformer layer: Sliding-Window Layer and
Cluster-Former Layer, which encode local sequence information and global
context jointly and iteratively. This new design allows information integration
beyond local windows, which is especially beneficial for question answering
(QA) tasks that rely on long-range dependencies. Experiments show that
Cluster-Former achieves state-of-the-art performance on several major QA
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luowei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuwei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Siqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Damaging Contrastive Learning. (arXiv:2106.02990v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02990</id>
        <link href="http://arxiv.org/abs/2106.02990"/>
        <updated>2021-06-08T02:20:21.693Z</updated>
        <summary type="html"><![CDATA[The recent breakthrough achieved by contrastive learning accelerates the pace
for deploying unsupervised training on real-world data applications. However,
unlabeled data in reality is commonly imbalanced and shows a long-tail
distribution, and it is unclear how robustly the latest contrastive learning
methods could perform in the practical scenario. This paper proposes to
explicitly tackle this challenge, via a principled framework called
Self-Damaging Contrastive Learning (SDCLR), to automatically balance the
representation learning without knowing the classes. Our main inspiration is
drawn from the recent finding that deep models have difficult-to-memorize
samples, and those may be exposed through network pruning. It is further
natural to hypothesize that long-tail samples are also tougher for the model to
learn well due to insufficient examples. Hence, the key innovation in SDCLR is
to create a dynamic self-competitor model to contrast with the target model,
which is a pruned version of the latter. During training, contrasting the two
models will lead to adaptive online mining of the most easily forgotten samples
for the current target model, and implicitly emphasize them more in the
contrastive loss. Extensive experiments across multiple datasets and imbalance
settings show that SDCLR significantly improves not only overall accuracies but
also balancedness, in terms of linear evaluation on the full-shot and few-shot
settings. Our code is available at: https://github.com/VITA-Group/SDCLR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Ziyu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortazavi_B/0/1/0/all/0/1"&gt;Bobak Mortazavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02701</id>
        <link href="http://arxiv.org/abs/2106.02701"/>
        <updated>2021-06-08T02:20:21.686Z</updated>
        <summary type="html"><![CDATA[Recent advances in brain clearing and imaging have made it possible to image
entire mammalian brains at sub-micron resolution. These images offer the
potential to assemble brain-wide atlases of projection neuron morphology, but
manual neuron reconstruction remains a bottleneck. Here we present a method
inspired by hidden Markov modeling and appearance modeling of fluorescent
neuron images that can automatically trace neuronal processes. Our method
leverages dynamic programming to scale to terabyte sized image data and can be
applied to images with one or more neurons. We applied our algorithm to the
output of image segmentation models where false negatives severed neuronal
processes, and showed that it can follow axons in the presence of noise or
nearby neurons. Our method has the potential to be integrated into a semi or
fully automated reconstruction pipeline. Additionally, it creates a framework
through which users can intervene with hard constraints to, for example, rule
out certain reconstructions, or assign axons to particular cell bodies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1"&gt;Thomas L. Athey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tward_D/0/1/0/all/0/1"&gt;Daniel Tward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_U/0/1/0/all/0/1"&gt;Ulrich Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1"&gt;Michael I. Miller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEEPMIR: A DEEP neural network for differential detection of cerebral Microbleeds and IRon deposits in MRI. (arXiv:2010.00148v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00148</id>
        <link href="http://arxiv.org/abs/2010.00148"/>
        <updated>2021-06-08T02:20:21.659Z</updated>
        <summary type="html"><![CDATA[Lobar cerebral microbleeds (CMBs) and localized non-hemorrhage iron deposits
in the basal ganglia have been associated with brain aging, vascular disease
and neurodegenerative disorders. Particularly, CMBs are small lesions and
require multiple neuroimaging modalities for accurate detection. Quantitative
susceptibility mapping (QSM) derived from in vivo magnetic resonance imaging
(MRI) is necessary to differentiate between iron content and mineralization. We
set out to develop a deep learning-based segmentation method suitable for
segmenting both CMBs and iron deposits. We included a convenience sample of 24
participants from the MESA cohort and used T2-weighted images, susceptibility
weighted imaging (SWI), and QSM to segment the two types of lesions. We
developed a protocol for simultaneous manual annotation of CMBs and
non-hemorrhage iron deposits in the basal ganglia. This manual annotation was
then used to train a deep convolution neural network (CNN). Specifically, we
adapted the U-Net model with a higher number of resolution layers to be able to
detect small lesions such as CMBs from standard resolution MRI. We tested
different combinations of the three modalities to determine the most
informative data sources for the detection tasks. In the detection of CMBs
using single class and multiclass models, we achieved an average sensitivity
and precision of between 0.84-0.88 and 0.40-0.59, respectively. The same
framework detected non-hemorrhage iron deposits with an average sensitivity and
precision of about 0.75-0.81 and 0.62-0.75, respectively. Our results showed
that deep learning could automate the detection of small vessel disease lesions
and including multimodal MR data (particularly QSM) can improve the detection
of CMB and non-hemorrhage iron deposits with sensitivity and precision that is
compatible with use in large-scale research studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1"&gt;Tanweer Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdulkadir_A/0/1/0/all/0/1"&gt;Ahmed Abdulkadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nasrallah_I/0/1/0/all/0/1"&gt;Ilya M. Nasrallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ware_J/0/1/0/all/0/1"&gt;Jeffrey B. Ware&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hangfan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Spincemaille_P/0/1/0/all/0/1"&gt;Pascal Spincemaille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Romero_J/0/1/0/all/0/1"&gt;J. Rafael Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bryan_R/0/1/0/all/0/1"&gt;R. Nick Bryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heckbert_S/0/1/0/all/0/1"&gt;Susan R. Heckbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Habes_M/0/1/0/all/0/1"&gt;Mohamad Habes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Entity Representations for Document-level Relation Extraction. (arXiv:2012.02507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02507</id>
        <link href="http://arxiv.org/abs/2012.02507"/>
        <updated>2021-06-08T02:20:21.652Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) requires extracting relations
expressed within and across sentences. Recent works show that graph-based
methods, usually constructing a document-level graph that captures
document-aware interactions, can obtain useful entity representations thus
helping tackle document-level RE. These methods either focus more on the entire
graph, or pay more attention to a part of the graph, e.g., paths between the
target entity pair. However, we find that document-level RE may benefit from
focusing on both of them simultaneously. Therefore, to obtain more
comprehensive entity representations, we propose the Coarse-to-Fine Entity
Representation model (CFER) that adopts a coarse-to-fine strategy involving two
phases. First, CFER uses graph neural networks to integrate global information
in the entire graph at a coarse level. Next, CFER utilizes the global
information as a guidance to selectively aggregate path information between the
target entity pair at a fine level. In classification, we combine the entity
representations from both two levels into more comprehensive representations
for relation extraction. Experimental results on two document-level RE
datasets, DocRED and CDR, show that CFER outperforms existing models and is
robust to the uneven label distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Damai Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on Crowd Videos. (arXiv:1811.11849v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.11849</id>
        <link href="http://arxiv.org/abs/1811.11849"/>
        <updated>2021-06-08T02:20:21.643Z</updated>
        <summary type="html"><![CDATA[Group-level emotion recognition (ER) is a growing research area as the
demands for assessing crowds of all sizes are becoming an interest in both the
security arena as well as social media. This work extends the earlier ER
investigations, which focused on either group-level ER on single images or
within a video, by fully investigating group-level expression recognition on
crowd videos. In this paper, we propose an effective deep feature level fusion
mechanism to model the spatial-temporal information in the crowd videos. In our
approach, the fusing process is performed on the deep feature domain by a
generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that
models spatial information relationships. Furthermore, we extend our proposed
spatial NVPF approach to the spatial-temporal NVPF approach to learn the
temporal information between frames. To demonstrate the robustness and
effectiveness of each component in the proposed approach, three experiments
were conducted: (i) evaluation on AffectNet database to benchmark the proposed
EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to
benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii)
examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos
(GECV) dataset composed of 627 videos collected from publicly available
sources. GECV dataset is a collection of videos containing crowds of people.
Each video is labeled with emotion categories at three levels: individual
faces, group of people, and the entire video frame.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quach_K/0/1/0/all/0/1"&gt;Kha Gia Quach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Ngan Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1"&gt;Chi Nhan Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalata_I/0/1/0/all/0/1"&gt;Ibsa Jalata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1"&gt;Kaushik Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1"&gt;Khoa Luu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal Over-Parameterized Training. (arXiv:2004.04690v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.04690</id>
        <link href="http://arxiv.org/abs/2004.04690"/>
        <updated>2021-06-08T02:20:21.636Z</updated>
        <summary type="html"><![CDATA[The inductive bias of a neural network is largely determined by the
architecture and the training algorithm. To achieve good generalization, how to
effectively train a neural network is of great importance. We propose a novel
orthogonal over-parameterized training (OPT) framework that can provably
minimize the hyperspherical energy which characterizes the diversity of neurons
on a hypersphere. By maintaining the minimum hyperspherical energy during
training, OPT can greatly improve the empirical generalization. Specifically,
OPT fixes the randomly initialized weights of the neurons and learns an
orthogonal transformation that applies to these neurons. We consider multiple
ways to learn such an orthogonal transformation, including unrolling
orthogonalization algorithms, applying orthogonal parameterization, and
designing orthogonality-preserving gradient descent. For better scalability, we
propose the stochastic OPT which performs orthogonal transformation
stochastically for partial dimensions of neurons. Interestingly, OPT reveals
that learning a proper coordinate system for neurons is crucial to
generalization. We provide some insights on why OPT yields better
generalization. Extensive experiments validate the superiority of OPT over the
standard training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1"&gt;James M. Rehg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1"&gt;Liam Paull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Le Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-the-Air Adversarial Flickering Attacks against Video Recognition Networks. (arXiv:2002.05123v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05123</id>
        <link href="http://arxiv.org/abs/2002.05123"/>
        <updated>2021-06-08T02:20:21.618Z</updated>
        <summary type="html"><![CDATA[Deep neural networks for video classification, just like image classification
networks, may be subjected to adversarial manipulation. The main difference
between image classifiers and video classifiers is that the latter usually use
temporal information contained within the video. In this work we present a
manipulation scheme for fooling video classifiers by introducing a flickering
temporal perturbation that in some cases may be unnoticeable by human observers
and is implementable in the real world. After demonstrating the manipulation of
action classification of single videos, we generalize the procedure to make
universal adversarial perturbation, achieving high fooling ratio. In addition,
we generalize the universal perturbation and produce a temporal-invariant
perturbation, which can be applied to the video without synchronizing the
perturbation to the input. The attack was implemented on several target models
and the transferability of the attack was demonstrated. These properties allow
us to bridge the gap between simulated environment and real-world application,
as will be demonstrated in this paper for the first time for an over-the-air
flickering attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pony_R/0/1/0/all/0/1"&gt;Roi Pony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeh_I/0/1/0/all/0/1"&gt;Itay Naeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEFT: Detection Embeddings for Tracking. (arXiv:2102.02267v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02267</id>
        <link href="http://arxiv.org/abs/2102.02267"/>
        <updated>2021-06-08T02:20:21.611Z</updated>
        <summary type="html"><![CDATA[Most modern multiple object tracking (MOT) systems follow the
tracking-by-detection paradigm, consisting of a detector followed by a method
for associating detections into tracks. There is a long history in tracking of
combining motion and appearance features to provide robustness to occlusions
and other challenges, but typically this comes with the trade-off of a more
complex and slower implementation. Recent successes on popular 2D tracking
benchmarks indicate that top-scores can be achieved using a state-of-the-art
detector and relatively simple associations relying on single-frame spatial
offsets -- notably outperforming contemporary methods that leverage learned
appearance features to help re-identify lost tracks. In this paper, we propose
an efficient joint detection and tracking model named DEFT, or "Detection
Embeddings for Tracking." Our approach relies on an appearance-based object
matching network jointly-learned with an underlying object detection network.
An LSTM is also added to capture motion constraints. DEFT has comparable
accuracy and speed to the top methods on 2D online tracking leaderboards while
having significant advantages in robustness when applied to more challenging
tracking data. DEFT raises the bar on the nuScenes monocular 3D tracking
challenge, more than doubling the performance of the previous top method. Code
is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaabane_M/0/1/0/all/0/1"&gt;Mohamed Chaabane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peter Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beveridge_J/0/1/0/all/0/1"&gt;J. Ross Beveridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OHara_S/0/1/0/all/0/1"&gt;Stephen O&amp;#x27;Hara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Middle-level Fusion for Lightweight RGB-D Salient Object Detection. (arXiv:2104.11543v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11543</id>
        <link href="http://arxiv.org/abs/2104.11543"/>
        <updated>2021-06-08T02:20:21.601Z</updated>
        <summary type="html"><![CDATA[Most existing lightweight RGB-D salient object detection (SOD) models are
based on two-stream structure or single-stream structure. The former one first
uses two sub-networks to extract unimodal features from RGB and depth images,
respectively, and then fuses them for SOD. While, the latter one directly
extracts multi-modal features from the input RGB-D images and then focuses on
exploiting cross-level complementary information. However, two-stream structure
based models inevitably require more parameters and single-stream structure
based ones cannot well exploit the cross-modal complementary information since
they ignore the modality difference. To address these issues, we propose to
employ the middle-level fusion structure for designing lightweight RGB-D SOD
model in this paper, which first employs two sub-networks to extract low- and
middle-level unimodal features, respectively, and then fuses those extracted
middle-level unimodal features for extracting corresponding high-level
multi-modal features in the subsequent sub-network. Different from existing
models, this structure can effectively exploit the cross-modal complementary
information and significantly reduce the network's parameters, simultaneously.
Therefore, a novel lightweight SOD model is designed, which contains a
information-aware multi-modal feature fusion (IMFF) module for effectively
capturing the cross-modal complementary information and a lightweight
feature-level and decision-level feature fusion (LFDF) module for aggregating
the feature-level and the decision-level saliency information in different
stages with less parameters. Our proposed model has only 3.9M parameters and
runs at 33 FPS. The experimental results on several benchmark datasets verify
the effectiveness and superiority of the proposed method over some
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1"&gt;Nianchang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jungong Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real Time Video based Heart and Respiration Rate Monitoring. (arXiv:2106.02669v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02669</id>
        <link href="http://arxiv.org/abs/2106.02669"/>
        <updated>2021-06-08T02:20:21.593Z</updated>
        <summary type="html"><![CDATA[In recent years, research about monitoring vital signs by smartphones grows
significantly. There are some special sensors like Electrocardiogram (ECG) and
Photoplethysmographic (PPG) to detect heart rate (HR) and respiration rate
(RR). Smartphone cameras also can measure HR by detecting and processing
imaging Photoplethysmographic (iPPG) signals from the video of a user's face.
Indeed, the variation in the intensity of the green channel can be measured by
the iPPG signals of the video. This study aimed to provide a method to extract
heart rate and respiration rate using the video of individuals' faces. The
proposed method is based on measuring fluctuations in the Hue, and can
therefore extract both HR and RR from the video of a user's face. The proposed
method is evaluated by performing on 25 healthy individuals. For each subject,
20 seconds video of his/her face is recorded. Results show that the proposed
approach of measuring iPPG using Hue gives more accurate rates than the Green
channel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pourbemany_J/0/1/0/all/0/1"&gt;Jafar Pourbemany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Essa_A/0/1/0/all/0/1"&gt;Almabrok Essa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Ye Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intent Classification and Slot Filling for Privacy Policies. (arXiv:2101.00123v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00123</id>
        <link href="http://arxiv.org/abs/2101.00123"/>
        <updated>2021-06-08T02:20:21.587Z</updated>
        <summary type="html"><![CDATA[Understanding privacy policies is crucial for users as it empowers them to
learn about the information that matters to them. Sentences written in a
privacy policy document explain privacy practices, and the constituent text
spans convey further specific information about that practice. We refer to
predicting the privacy practice explained in a sentence as intent
classification and identifying the text spans sharing specific information as
slot filling. In this work, we propose PolicyIE, an English corpus consisting
of 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of
websites and mobile applications. PolicyIE corpus is a challenging real-world
benchmark with limited labeled examples reflecting the cost of collecting
large-scale annotations from domain experts. We present two alternative neural
approaches as baselines, (1) intent classification and slot filling as a joint
sequence tagging and (2) modeling them as a sequence-to-sequence (Seq2Seq)
learning task. The experiment results show that both approaches perform
comparably in intent classification, while the Seq2Seq method outperforms the
sequence tagging approach in slot filling by a large margin. We perform a
detailed error analysis to reveal the challenges of the proposed corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1"&gt;Wasi Uddin Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1"&gt;Jianfeng Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Tu Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norton_T/0/1/0/all/0/1"&gt;Thomas Norton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RegionViT: Regional-to-Local Attention for Vision Transformers. (arXiv:2106.02689v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02689</id>
        <link href="http://arxiv.org/abs/2106.02689"/>
        <updated>2021-06-08T02:20:21.580Z</updated>
        <summary type="html"><![CDATA[Vision transformer (ViT) has recently showed its strong capability in
achieving comparable results to convolutional neural networks (CNNs) on image
classification. However, vanilla ViT simply inherits the same architecture from
the natural language processing directly, which is often not optimized for
vision applications. Motivated by this, in this paper, we propose a new
architecture that adopts the pyramid structure and employ a novel
regional-to-local attention rather than global self-attention in vision
transformers. More specifically, our model first generates regional tokens and
local tokens from an image with different patch sizes, where each regional
token is associated with a set of local tokens based on the spatial location.
The regional-to-local attention includes two steps: first, the regional
self-attention extract global information among all regional tokens and then
the local self-attention exchanges the information among one regional token and
the associated local tokens via self-attention. Therefore, even though local
self-attention confines the scope in a local region but it can still receive
global information. Extensive experiments on three vision tasks, including
image classification, object detection and action recognition, show that our
approach outperforms or is on par with state-of-the-art ViT variants including
many concurrent works. Our source codes and models will be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Fu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1"&gt;Quanfu Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Open-Ended Commonsense Reasoning. (arXiv:2010.14439v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14439</id>
        <link href="http://arxiv.org/abs/2010.14439"/>
        <updated>2021-06-08T02:20:21.562Z</updated>
        <summary type="html"><![CDATA[Current commonsense reasoning research focuses on developing models that use
commonsense knowledge to answer multiple-choice questions. However, systems
designed to answer multiple-choice questions may not be useful in applications
that do not provide a small list of candidate answers to choose from. As a step
towards making commonsense reasoning research more realistic, we propose to
study open-ended commonsense reasoning (OpenCSR) -- the task of answering a
commonsense question without any pre-defined choices -- using as a resource
only a corpus of commonsense facts written in natural language. OpenCSR is
challenging due to a large decision space, and because many questions require
implicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an
efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To
evaluate OpenCSR methods, we adapt several popular commonsense reasoning
benchmarks, and collect multiple new answers for each test question via
crowd-sourcing. Experiments show that DrFact outperforms strong baseline
methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Adversarial Robustness and Optimization Bias. (arXiv:2102.08868v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08868</id>
        <link href="http://arxiv.org/abs/2102.08868"/>
        <updated>2021-06-08T02:20:21.553Z</updated>
        <summary type="html"><![CDATA[We demonstrate that the choice of optimizer, neural network architecture, and
regularizer significantly affect the adversarial robustness of linear neural
networks, providing guarantees without the need for adversarial training. To
this end, we revisit a known result linking maximally robust classifiers and
minimum norm solutions, and combine it with recent results on the implicit bias
of optimizers. First, we show that, under certain conditions, it is possible to
achieve both perfect standard accuracy and a certain degree of robustness,
simply by training an overparametrized model using the implicit bias of the
optimization. In that regime, there is a direct relationship between the type
of the optimizer and the attack to which the model is robust. To the best of
our knowledge, this work is the first to study the impact of optimization
methods such as sign gradient descent and proximal methods on adversarial
robustness. Second, we characterize the robustness of linear convolutional
models, showing that they resist attacks subject to a constraint on the
Fourier-$\ell_\infty$ norm. To illustrate these findings we design a novel
Fourier-$\ell_\infty$ attack that finds adversarial examples with controllable
frequencies. We evaluate Fourier-$\ell_\infty$ robustness of
adversarially-trained deep CIFAR-10 models from the standard RobustBench
benchmark and visualize adversarial perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1"&gt;Fartash Faghri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1"&gt;Sven Gowal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1"&gt;Cristina Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1"&gt;Fabian Pedregosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1"&gt;Nicolas Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascaded Context Enhancement Network for Automatic Skin Lesion Segmentation. (arXiv:2004.08107v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08107</id>
        <link href="http://arxiv.org/abs/2004.08107"/>
        <updated>2021-06-08T02:20:21.547Z</updated>
        <summary type="html"><![CDATA[Skin lesion segmentation is an important step for automatic melanoma
diagnosis. Due to the non-negligible diversity of lesions from different
patients, extracting powerful context for fine-grained semantic segmentation is
still challenging today. Although the deep convolutional neural network (CNNs)
have made significant improvements on skin lesion segmentation, they often fail
to reserve the spatial details and long-range dependencies context due to
consecutive convolution striding and pooling operations inside CNNs. In this
paper, we formulate a cascaded context enhancement neural network for automatic
skin lesion segmentation. A new cascaded context aggregation (CCA) module with
a gate-based information integration approach is proposed to sequentially and
selectively aggregate original image and multi-level features from the encoder
sub-network. The generated context is further utilized to guide discriminative
features extraction by the designed context-guided local affinity (CGL) module.
Furthermore, an auxiliary loss is added to the CCA module for refining the
prediction. In our work, we evaluate our approach on four public skin
dermoscopy image datasets. The proposed method achieves the Jaccard Index (JA)
of 87.1%, 80.3%, 83.4%, and 86.6% on ISIC-2016, ISIC-2017, ISIC-2018, and PH2
datasets, which are higher than other state-of-the-art models respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1"&gt;Chaojie Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Ye Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning by Self-Transition for Handling Noisy Labels. (arXiv:2012.04337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04337</id>
        <link href="http://arxiv.org/abs/2012.04337"/>
        <updated>2021-06-08T02:20:21.531Z</updated>
        <summary type="html"><![CDATA[Real-world data inevitably contains noisy labels, which induce the poor
generalization of deep neural networks. It is known that the network typically
begins to rapidly memorize false-labeled samples after a certain point of
training. Thus, to counter the label noise challenge, we propose a novel
self-transitional learning method called MORPH, which automatically switches
its learning phase at the transition point from seeding to evolution. In the
seeding phase, the network is updated using all the samples to collect a seed
of clean samples. Then, in the evolution phase, the network is updated using
only the set of arguably clean samples, which precisely keeps expanding by the
updated network. Thus, MORPH effectively avoids the overfitting to
false-labeled samples throughout the entire training period. Extensive
experiments using five real-world or synthetic benchmark datasets demonstrate
substantial improvements over state-of-the-art methods in terms of robustness
and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hwanjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yooju Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae-Gil Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial Image Deformation Based on Landmark Detection. (arXiv:1910.13671v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.13671</id>
        <link href="http://arxiv.org/abs/1910.13671"/>
        <updated>2021-06-08T02:20:21.524Z</updated>
        <summary type="html"><![CDATA[In this work, we use facial landmarks to make the deformation for facial
images more authentic. The deformation includes the expansion of eyes and the
shrinking of noses, mouths, and cheeks. An advanced 106-point facial landmark
detector is utilized to provide control points for deformation. Bilinear
interpolation is used in the expansion and Moving Least Squares methods (MLS)
including Affine Deformation, Similarity Deformation and Rigid Deformation are
used in the shrinking. We compare the running time as well as the quality of
deformed images using different MLS methods. The experimental results show that
the Rigid Deformation which can keep other parts of the images unchanged
performs better even if it takes the longest time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chaoyue Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yugang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shulai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Human Mesh Regression with Dense Correspondence. (arXiv:2006.05734v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05734</id>
        <link href="http://arxiv.org/abs/2006.05734"/>
        <updated>2021-06-08T02:20:21.516Z</updated>
        <summary type="html"><![CDATA[Estimating 3D mesh of the human body from a single 2D image is an important
task with many applications such as augmented reality and Human-Robot
interaction. However, prior works reconstructed 3D mesh from global image
feature extracted by using convolutional neural network (CNN), where the dense
correspondences between the mesh surface and the image pixels are missing,
leading to suboptimal solution. This paper proposes a model-free 3D human mesh
estimation framework, named DecoMR, which explicitly establishes the dense
correspondence between the mesh and the local image features in the UV space
(i.e. a 2D space used for texture mapping of 3D mesh). DecoMR first predicts
pixel-to-surface dense correspondence map (i.e., IUV image), with which we
transfer local features from the image space to the UV space. Then the
transferred local image features are processed in the UV space to regress a
location map, which is well aligned with transferred features. Finally we
reconstruct 3D human mesh from the regressed location map with a predefined
mapping function. We also observe that the existing discontinuous UV map are
unfriendly to the learning of network. Therefore, we propose a novel UV map
that maintains most of the neighboring relations on the original mesh surface.
Experiments demonstrate that our proposed local feature alignment and
continuous UV map outperforms existing 3D mesh based methods on multiple public
benchmarks. Code will be made available at
https://github.com/zengwang430521/DecoMR]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Temporal Graph Neural Network for Trajectory Prediction. (arXiv:2106.02930v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02930</id>
        <link href="http://arxiv.org/abs/2106.02930"/>
        <updated>2021-06-08T02:20:21.508Z</updated>
        <summary type="html"><![CDATA[An effective understanding of the contextual environment and accurate motion
forecasting of surrounding agents is crucial for the development of autonomous
vehicles and social mobile robots. This task is challenging since the behavior
of an autonomous agent is not only affected by its own intention, but also by
the static environment and surrounding dynamically interacting agents. Previous
works focused on utilizing the spatial and temporal information in time domain
while not sufficiently taking advantage of the cues in frequency domain. To
this end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which
can capture inter-agent correlations and temporal dependency simultaneously in
frequency domain in addition to time domain. SpecTGNN operates on both an agent
graph with dynamic state information and an environment graph with the features
extracted from context images in two streams. The model integrates graph
Fourier transform, spectral graph convolution and temporal gated convolution to
encode history information and forecast future trajectories. Moreover, we
incorporate a multi-head spatio-temporal attention mechanism to mitigate the
effect of error propagation in a long time horizon. We demonstrate the
performance of SpecTGNN on two public trajectory prediction benchmark datasets,
which achieves state-of-the-art performance in terms of prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1"&gt;Defu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hengbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism. (arXiv:2103.07054v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07054</id>
        <link href="http://arxiv.org/abs/2103.07054"/>
        <updated>2021-06-08T02:20:21.501Z</updated>
        <summary type="html"><![CDATA[In this paper, we focus on category-level 6D pose and size estimation from
monocular RGB-D image. Previous methods suffer from inefficient category-level
pose feature extraction which leads to low accuracy and inference speed. To
tackle this problem, we propose a fast shape-based network (FS-Net) with
efficient category-level feature extraction for 6D pose estimation. First, we
design an orientation aware autoencoder with 3D graph convolution for latent
feature extraction. The learned latent feature is insensitive to point shift
and object size thanks to the shift and scale-invariance properties of the 3D
graph convolution. Then, to efficiently decode category-level rotation
information from the latent feature, we propose a novel decoupled rotation
mechanism that employs two decoders to complementarily access the rotation
information. Meanwhile, we estimate translation and size by two residuals,
which are the difference between the mean of object points and ground truth
translation, and the difference between the mean size of the category and
ground truth size, respectively. Finally, to increase the generalization
ability of FS-Net, we propose an online box-cage based 3D deformation mechanism
to augment the training data. Extensive experiments on two benchmark datasets
show that the proposed method achieves state-of-the-art performance in both
category- and instance-level 6D object pose estimation. Especially in
category-level pose estimation, without extra synthetic data, our method
outperforms existing methods by 6.3% on the NOCS-REAL dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xi Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hyung Jin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinming Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Linlin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ales Leonardis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guidance and Teaching Network for Video Salient Object Detection. (arXiv:2105.10110v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10110</id>
        <link href="http://arxiv.org/abs/2105.10110"/>
        <updated>2021-06-08T02:20:21.494Z</updated>
        <summary type="html"><![CDATA[Owing to the difficulties of mining spatial-temporal cues, the existing
approaches for video salient object detection (VSOD) are limited in
understanding complex and noisy scenarios, and often fail in inferring
prominent objects. To alleviate such shortcomings, we propose a simple yet
efficient architecture, termed Guidance and Teaching Network (GTNet), to
independently distil effective spatial and temporal cues with implicit guidance
and explicit teaching at feature- and decision-level, respectively. To be
specific, we (a) introduce a temporal modulator to implicitly bridge features
from motion into the appearance branch, which is capable of fusing cross-modal
features collaboratively, and (b) utilise motion-guided mask to propagate the
explicit cues during the feature aggregation. This novel learning strategy
achieves satisfactory results via decoupling the complex spatial-temporal cues
and mapping informative cues across different modalities. Extensive experiments
on three challenging benchmarks show that the proposed method can run at ~28
fps on a single TITAN Xp GPU and perform competitively against 14 cutting-edge
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yingxia Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yu-Cheng Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shouyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1"&gt;Ge Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Prostate Cancer Detection in bpMRI via 3D CNNs: Effects of Attention Mechanisms, Clinical Priori and Decoupled False Positive Reduction. (arXiv:2101.03244v8 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03244</id>
        <link href="http://arxiv.org/abs/2101.03244"/>
        <updated>2021-06-08T02:20:21.487Z</updated>
        <summary type="html"><![CDATA[We present a multi-stage 3D computer-aided detection and diagnosis (CAD)
model for automated localization of clinically significant prostate cancer
(csPCa) in bi-parametric MR imaging (bpMRI). Deep attention mechanisms drive
its detection network, targeting salient structures and highly discriminative
feature dimensions across multiple resolutions. Its goal is to accurately
identify csPCa lesions from indolent cancer and the wide range of benign
pathology that can afflict the prostate gland. Simultaneously, a decoupled
residual classifier is used to achieve consistent false positive reduction,
without sacrificing high sensitivity or computational efficiency. In order to
guide model generalization with domain-specific clinical knowledge, a
probabilistic anatomical prior is used to encode the spatial prevalence and
zonal distinction of csPCa. Using a large dataset of 1950 prostate bpMRI paired
with radiologically-estimated annotations, we hypothesize that such CNN-based
models can be trained to detect biopsy-confirmed malignancies in an independent
cohort.

For 486 institutional testing scans, the 3D CAD system achieves
83.69$\pm$5.22% and 93.19$\pm$2.96% detection sensitivity at 0.50 and 1.46
false positive(s) per patient, respectively, with 0.882$\pm$0.030 AUROC in
patient-based diagnosis $-$significantly outperforming four state-of-the-art
baseline architectures (U-SEResNet, UNet++, nnU-Net, Attention U-Net) from
recent literature. For 296 external biopsy-confirmed testing scans, the
ensembled CAD system shares moderate agreement with a consensus of expert
radiologists (76.69%; $kappa$ $=$ 0.51$\pm$0.04) and independent pathologists
(81.08%; $kappa$ $=$ 0.56$\pm$0.06); demonstrating strong generalization to
histologically-confirmed csPCa diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saha_A/0/1/0/all/0/1"&gt;Anindo Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hosseinzadeh_M/0/1/0/all/0/1"&gt;Matin Hosseinzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huisman_H/0/1/0/all/0/1"&gt;Henkjan Huisman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Segmentation Learning from Sparse Annotations and Hierarchical Descriptors. (arXiv:2105.12885v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12885</id>
        <link href="http://arxiv.org/abs/2105.12885"/>
        <updated>2021-06-08T02:20:21.471Z</updated>
        <summary type="html"><![CDATA[One of the main obstacles to 3D semantic segmentation is the significant
amount of endeavor required to generate expensive point-wise annotations for
fully supervised training. To alleviate manual efforts, we propose GIDSeg, a
novel approach that can simultaneously learn segmentation from sparse
annotations via reasoning global-regional structures and individual-vicinal
properties. GIDSeg depicts global- and individual- relation via a dynamic edge
convolution network coupled with a kernelized identity descriptor. The ensemble
effects are obtained by endowing a fine-grained receptive field to a
low-resolution voxelized map. In our GIDSeg, an adversarial learning module is
also designed to further enhance the conditional constraint of identity
descriptors within the joint feature distribution. Despite the apparent
simplicity, our proposed approach achieves superior performance over
state-of-the-art for inferencing 3D dense segmentation with only sparse
annotations. Particularly, with $5\%$ annotations of raw data, GIDSeg
outperforms other 3D segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Peng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lingyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jianmin Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1"&gt;Sebastian Scherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choset_H/0/1/0/all/0/1"&gt;Howie Choset&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pinball-OCSVM for early-stage COVID-19 diagnosis with limited posteroanterior chest X-ray images. (arXiv:2010.08115v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08115</id>
        <link href="http://arxiv.org/abs/2010.08115"/>
        <updated>2021-06-08T02:20:21.444Z</updated>
        <summary type="html"><![CDATA[The infection of respiratory coronavirus disease 2019 (COVID-19) starts with
the upper respiratory tract and as the virus grows, the infection can progress
to lungs and develop pneumonia. The conventional way of COVID-19 diagnosis is
reverse transcription polymerase chain reaction (RT-PCR), which is less
sensitive during early stages; especially if the patient is asymptomatic, which
may further cause more severe pneumonia. In this context, several deep learning
models have been proposed to identify pulmonary infections using publicly
available chest X-ray (CXR) image datasets for early diagnosis, better
treatment and quick cure. In these datasets, presence of less number of
COVID-19 positive samples compared to other classes (normal, pneumonia and
Tuberculosis) raises the challenge for unbiased learning of deep learning
models. All deep learning models opted class balancing techniques to solve this
issue; which however should be avoided in any medical diagnosis process.
Moreover, the deep learning models are also data hungry and need massive
computation resources. Therefore for quicker diagnosis, this research proposes
a novel pinball loss function based one-class support vector machine
(PB-OCSVM), that can work in presence of limited COVID-19 positive CXR samples
with objectives to maximize the learning efficiency and to minimize the false
predictions. The performance of the proposed model is compared with
conventional OCSVM and existing deep learning models, and the experimental
results prove that the proposed model outperformed over state-of-the-art
methods. To validate the robustness of the proposed model, experiments are also
performed with noisy CXR images and UCI benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nagabhushan_P/0/1/0/all/0/1"&gt;P. Nagabhushan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TARA: Training and Representation Alteration for AI Fairness and Domain Generalization. (arXiv:2012.06387v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06387</id>
        <link href="http://arxiv.org/abs/2012.06387"/>
        <updated>2021-06-08T02:20:21.419Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for enforcing AI fairness with respect to protected
or sensitive factors. This method uses a dual strategy performing training and
representation alteration (TARA) for the mitigation of prominent causes of AI
bias by including: a) the use of representation learning alteration via
adversarial independence to suppress the bias-inducing dependence of the data
representation from protected factors; and b) training set alteration via
intelligent augmentation to address bias-causing data imbalance, by using
generative models that allow the fine control of sensitive factors related to
underrepresented populations via domain adaptation and latent space
manipulation. When testing our methods on image analytics, experiments
demonstrate that TARA significantly or fully debiases baseline models while
outperforming competing debiasing methods that have the same amount of
information, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.
the baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.
(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in
current metrics used for assessing debiasing performance, we propose novel
conjunctive debiasing metrics. Our experiments also demonstrate the ability of
these novel metrics in assessing the Pareto efficiency of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadzic_A/0/1/0/all/0/1"&gt;Armin Hadzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1"&gt;Neil Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Phil Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End Breast Tumour Classification Model Using Context-Based Patch Modelling- A BiLSTM Approach for Image Classification. (arXiv:2106.02864v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02864</id>
        <link href="http://arxiv.org/abs/2106.02864"/>
        <updated>2021-06-08T02:20:21.393Z</updated>
        <summary type="html"><![CDATA[Researchers working on computational analysis of Whole Slide Images (WSIs) in
histopathology have primarily resorted to patch-based modelling due to large
resolution of each WSI. The large resolution makes WSIs infeasible to be fed
directly into the machine learning models due to computational constraints.
However, due to patch-based analysis, most of the current methods fail to
exploit the underlying spatial relationship among the patches. In our work, we
have tried to integrate this relationship along with feature-based correlation
among the extracted patches from the particular tumorous region. For the given
task of classification, we have used BiLSTMs to model both forward and backward
contextual relationship. RNN based models eliminate the limitation of sequence
size by allowing the modelling of variable size images within a deep learning
model. We have also incorporated the effect of spatial continuity by exploring
different scanning techniques used to sample patches. To establish the
efficiency of our approach, we trained and tested our model on two datasets,
microscopy images and WSI tumour regions. After comparing with contemporary
literature we achieved the better performance with accuracy of 90% for
microscopy image dataset. For WSI tumour region dataset, we compared the
classification results with deep learning networks such as ResNet, DenseNet,
and InceptionV3 using maximum voting technique. We achieved the highest
performance accuracy of 84%. We found out that BiLSTMs with CNN features have
performed much better in modelling patches into an end-to-end Image
classification network. Additionally, the variable dimensions of WSI tumour
regions were used for classification without the need for resizing. This
suggests that our method is independent of tumour image size and can process
large dimensional images without losing the resolution details.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Suvidha Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hwee Kuan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Model Biases in the Absence of Ground Truth. (arXiv:2103.03417v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03417</id>
        <link href="http://arxiv.org/abs/2103.03417"/>
        <updated>2021-06-08T02:20:21.386Z</updated>
        <summary type="html"><![CDATA[The measurement of bias in machine learning often focuses on model
performance across identity subgroups (such as man and woman) with respect to
groundtruth labels. However, these methods do not directly measure the
associations that a model may have learned, for example between labels and
identity subgroups. Further, measuring a model's bias requires a fully
annotated evaluation dataset which may not be easily available in practice. We
present an elegant mathematical solution that tackles both issues
simultaneously, using image classification as a working example. By treating a
classification model's predictions for a given image as a set of labels
analogous to a bag of words, we rank the biases that a model has learned with
respect to different identity labels. We use (man, woman) as a concrete example
of an identity label set (although this set need not be binary), and present
rankings for the labels that are most biased towards one identity or the other.
We demonstrate how the statistical properties of different association metrics
can lead to different rankings of the most "gender biased" labels, and conclude
that normalized pointwise mutual information (nPMI) is most useful in practice.
Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aka_O/0/1/0/all/0/1"&gt;Osman Aka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burke_K/0/1/0/all/0/1"&gt;Ken Burke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauerle_A/0/1/0/all/0/1"&gt;Alex B&amp;#xe4;uerle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greer_C/0/1/0/all/0/1"&gt;Christina Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1"&gt;Margaret Mitchell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning. (arXiv:2105.14167v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14167</id>
        <link href="http://arxiv.org/abs/2105.14167"/>
        <updated>2021-06-08T02:20:21.356Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) based language models achieve high performance on various
benchmarks for Natural Language Inference (NLI). And at this time, symbolic
approaches to NLI are receiving less attention. Both approaches (symbolic and
DL) have their advantages and weaknesses. However, currently, no method
combines them in a system to solve the task of NLI. To merge symbolic and deep
learning methods, we propose an inference framework called NeuralLog, which
utilizes both a monotonicity-based logical inference engine and a neural
network language model for phrase alignment. Our framework models the NLI task
as a classic search problem and uses the beam search algorithm to search for
optimal inference paths. Experiments show that our joint logic and neural
inference system improves accuracy on the NLI task and can achieve state-of-art
accuracy on the SICK and MED datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qiyue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moss_L/0/1/0/all/0/1"&gt;Lawrence S. Moss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZeroWaste Dataset: Towards Automated Waste Recycling. (arXiv:2106.02740v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02740</id>
        <link href="http://arxiv.org/abs/2106.02740"/>
        <updated>2021-06-08T02:20:21.296Z</updated>
        <summary type="html"><![CDATA[Less than 35% of recyclable waste is being actually recycled in the US, which
leads to increased soil and sea pollution and is one of the major concerns of
environmental researchers as well as the common public. At the heart of the
problem is the inefficiencies of the waste sorting process (separating paper,
plastic, metal, glass, etc.) due to the extremely complex and cluttered nature
of the waste stream. Automated waste detection strategies have a great
potential to enable more efficient, reliable and safer waste sorting practices,
but the literature lacks comprehensive datasets and methodology for the
industrial waste sorting solutions. In this paper, we take a step towards
computer-aided waste detection and present the first in-the-wild
industrial-grade waste detection and segmentation dataset, ZeroWaste. This
dataset contains over1800fully segmented video frames collected from a real
waste sorting plant along with waste material labels for training and
evaluation of the segmentation methods, as well as over6000unlabeled frames
that can be further used for semi-supervised and self-supervised learning
techniques. ZeroWaste also provides frames of the conveyor belt before and
after the sorting process, comprising a novel setup that can be used for
weakly-supervised segmentation. We present baselines for fully-, semi- and
weakly-supervised segmentation methods. Our experimental results demonstrate
that state-of-the-art segmentation methods struggle to correctly detect and
classify target objects which suggests the challenging nature of our proposed
in-the-wild dataset. We believe that ZeroWastewill catalyze research in object
detection and semantic segmentation in extreme clutter as well as applications
in the recycling domain. Our project page can be found
atthis http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1"&gt;Dina Bashkirova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Ziliang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akl_J/0/1/0/all/0/1"&gt;James Akl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alladkani_F/0/1/0/all/0/1"&gt;Fadi Alladkani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1"&gt;Ping Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1"&gt;Vitaly Ablavsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calli_B/0/1/0/all/0/1"&gt;Berk Calli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1"&gt;Sarah Adel Bargal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RDA: Robust Domain Adaptation via Fourier Adversarial Attacking. (arXiv:2106.02874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02874</id>
        <link href="http://arxiv.org/abs/2106.02874"/>
        <updated>2021-06-08T02:20:21.234Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled
source domain and an unsupervised loss in an unlabeled target domain, which
often faces more severe overfitting (than classical supervised learning) as the
supervised source loss has clear domain gap and the unsupervised target loss is
often noisy due to the lack of annotations. This paper presents RDA, a robust
domain adaptation technique that introduces adversarial attacking to mitigate
overfitting in UDA. We achieve robust domain adaptation by a novel Fourier
adversarial attacking (FAA) method that allows large magnitude of perturbation
noises but has minimal modification of image semantics, the former is critical
to the effectiveness of its generated adversarial samples due to the existence
of 'domain gaps'. Specifically, FAA decomposes images into multiple frequency
components (FCs) and generates adversarial samples by just perturbating certain
FCs that capture little semantic information. With FAA-generated samples, the
training can continue the 'random walk' and drift into an area with a flat loss
landscape, leading to more robust domain adaptation. Extensive experiments over
multiple domain adaptation tasks show that RDA can work with different computer
vision tasks with superior performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Story Generation on Plural Images. (arXiv:2001.10980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10980</id>
        <link href="http://arxiv.org/abs/2001.10980"/>
        <updated>2021-06-08T02:20:21.199Z</updated>
        <summary type="html"><![CDATA[Traditionally, text generation models take in a sequence of text as input,
and iteratively generate the next most probable word using pre-trained
parameters. In this work, we propose the architecture to use images instead of
text as the input of the text generation model, called StoryGen. In the
architecture, we design a Relational Text Data Generator algorithm that relates
different features from multiple images. The output samples from the model
demonstrate the ability to generate meaningful paragraphs of text containing
the extracted features from the input images. This is an undergraduate project
report. Completed Dec. 2019 at the Cooper Union.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Radar-Camera Pixel Depth Association for Depth Completion. (arXiv:2106.02778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02778</id>
        <link href="http://arxiv.org/abs/2106.02778"/>
        <updated>2021-06-08T02:20:21.190Z</updated>
        <summary type="html"><![CDATA[While radar and video data can be readily fused at the detection level,
fusing them at the pixel level is potentially more beneficial. This is also
more challenging in part due to the sparsity of radar, but also because
automotive radar beams are much wider than a typical pixel combined with a
large baseline between camera and radar, which results in poor association
between radar pixels and color pixel. A consequence is that depth completion
methods designed for LiDAR and video fare poorly for radar and video. Here we
propose a radar-to-pixel association stage which learns a mapping from radar
returns to pixels. This mapping also serves to densify radar returns. Using
this as a first stage, followed by a more traditional depth completion method,
we are able to achieve image-guided depth completion with radar and video. We
demonstrate performance superior to camera and radar alone on the nuScenes
dataset. Our source code is available at https://github.com/longyunf/rc-pda.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yunfei Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1"&gt;Daniel Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castro_M/0/1/0/all/0/1"&gt;Marcos Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarty_P/0/1/0/all/0/1"&gt;Punarjay Chakravarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1"&gt;Praveen Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Task Adaptation using Natural Language. (arXiv:2106.02972v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02972</id>
        <link href="http://arxiv.org/abs/2106.02972"/>
        <updated>2021-06-08T02:20:21.183Z</updated>
        <summary type="html"><![CDATA[Imitation learning and instruction-following are two common approaches to
communicate a user's intent to a learning agent. However, as the complexity of
tasks grows, it could be beneficial to use both demonstrations and language to
communicate with an agent. In this work, we propose a novel setting where an
agent is given both a demonstration and a description, and must combine
information from both the modalities. Specifically, given a demonstration for a
task (the source task), and a natural language description of the differences
between the demonstrated task and a related but different task (the target
task), our goal is to train an agent to complete the target task in a zero-shot
setting, that is, without any demonstrations for the target task. To this end,
we introduce Language-Aided Reward and Value Adaptation (LARVA) which, given a
source demonstration and a linguistic description of how the target task
differs, learns to output a reward / value function that accurately describes
the target task. Our experiments show that on a diverse set of adaptations, our
approach is able to complete more than 95% of target tasks when using
template-based descriptions, and more than 70% when using free-form natural
language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Prasoon Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1"&gt;Raymond J. Mooney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Domain Adaptation via Adaptive and Progressive Feature Alignment. (arXiv:2106.02845v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02845</id>
        <link href="http://arxiv.org/abs/2106.02845"/>
        <updated>2021-06-08T02:20:21.174Z</updated>
        <summary type="html"><![CDATA[Contemporary domain adaptive semantic segmentation aims to address data
annotation challenges by assuming that target domains are completely
unannotated. However, annotating a few target samples is usually very
manageable and worthwhile especially if it improves the adaptation performance
substantially. This paper presents SSDAS, a Semi-Supervised Domain Adaptive
image Segmentation network that employs a few labeled target samples as anchors
for adaptive and progressive feature alignment between labeled source samples
and unlabeled target samples. We position the few labeled target samples as
references that gauge the similarity between source and target features and
guide adaptive inter-domain alignment for learning more similar source
features. In addition, we replace the dissimilar source features by
high-confidence target features continuously during the iterative training
process, which achieves progressive intra-domain alignment between confident
and unconfident target features. Extensive experiments show the proposed SSDAS
greatly outperforms a number of baselines, i.e., UDA-based semantic
segmentation and SSDA-based image classification. In addition, SSDAS is
complementary and can be easily incorporated into UDA-based methods with
consistent improvements in domain adaptive semantic segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Table2Charts: Recommending Charts by Learning Shared Table Representations. (arXiv:2008.11015v3 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11015</id>
        <link href="http://arxiv.org/abs/2008.11015"/>
        <updated>2021-06-08T02:20:21.152Z</updated>
        <summary type="html"><![CDATA[It is common for people to create different types of charts to explore a
multi-dimensional dataset (table). However, to recommend commonly composed
charts in real world, one should take the challenges of efficiency, imbalanced
data and table context into consideration. In this paper, we propose
Table2Charts framework which learns common patterns from a large corpus of
(table, charts) pairs. Based on deep Q-learning with copying mechanism and
heuristic searching, Table2Charts does table-to-sequence generation, where each
sequence follows a chart template. On a large spreadsheet corpus with 165k
tables and 266k charts, we show that Table2Charts could learn a shared
representation of table fields so that recommendation tasks on different chart
types could mutually enhance each other. Table2Charts outperforms other chart
recommendation systems in both multi-type task (with doubled recall numbers
R@3=0.61 and R@1=0.43) and human evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingtao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xinyi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuejiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1"&gt;Wei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots. (arXiv:2103.09593v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09593</id>
        <link href="http://arxiv.org/abs/2103.09593"/>
        <updated>2021-06-08T02:20:21.141Z</updated>
        <summary type="html"><![CDATA[Multilingual models have demonstrated impressive cross-lingual transfer
performance. However, test sets like XNLI are monolingual at the example level.
In multilingual communities, it is common for polyglots to code-mix when
conversing with each other. Inspired by this phenomenon, we present two strong
black-box adversarial attacks (one word-level, one phrase-level) for
multilingual models that push their ability to handle code-mixed sentences to
the limit. The former uses bilingual dictionaries to propose perturbations and
translations of the clean example for sense disambiguation. The latter directly
aligns the clean example with its translations before extracting phrases as
perturbations. Our phrase-level attack has a success rate of 89.75% against
XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.
Finally, we propose an efficient adversarial training scheme that trains in the
same number of steps as the original model and show that it improves model
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Samson Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making CNNs Interpretable by Building Dynamic Sequential Decision Forests with Top-down Hierarchy Learning. (arXiv:2106.02824v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02824</id>
        <link href="http://arxiv.org/abs/2106.02824"/>
        <updated>2021-06-08T02:20:21.130Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a generic model transfer scheme to make
Convlutional Neural Networks (CNNs) interpretable, while maintaining their high
classification accuracy. We achieve this by building a differentiable decision
forest on top of CNNs, which enjoys two characteristics: 1) During training,
the tree hierarchies of the forest are learned in a top-down manner under the
guidance from the category semantics embedded in the pre-trained CNN weights;
2) During inference, a single decision tree is dynamically selected from the
forest for each input sample, enabling the transferred model to make sequential
decisions corresponding to the attributes shared by semantically-similar
categories, rather than directly performing flat classification. We name the
transferred model deep Dynamic Sequential Decision Forest (dDSDF). Experimental
results show that dDSDF not only achieves higher classification accuracy than
its conuterpart, i.e., the original CNN, but has much better interpretability,
as qualitatively it has plausible hierarchies and quantitatively it leads to
more precise saliency maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yilin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shaozuo Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval. (arXiv:2010.11915v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11915</id>
        <link href="http://arxiv.org/abs/2010.11915"/>
        <updated>2021-06-08T02:20:21.119Z</updated>
        <summary type="html"><![CDATA[Recent pretrained language models "solved" many reading comprehension
benchmarks, where questions are written with access to the evidence document.
However, datasets containing information-seeking queries where evidence
documents are provided after the queries are written independently remain
challenging. We analyze why answering information-seeking queries is more
challenging and where their prevalent unanswerabilities arise, on Natural
Questions and TyDi QA. Our controlled experiments suggest two headrooms --
paragraph selection and answerability prediction, i.e. whether the paired
evidence document contains the answer to the query or not. When provided with a
gold paragraph and knowing when to abstain from answering, existing models
easily outperform a human annotator. However, predicting answerability itself
remains challenging. We manually annotate 800 unanswerable examples across six
languages on what makes them challenging to answer. With this new data, we
conduct per-category answerability prediction, revealing issues in the current
dataset collection as well as task formulation. Together, our study points to
avenues for future research in information-seeking question answering, both for
dataset creation and model development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1"&gt;Akari Asai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1"&gt;Eunsol Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MagicPai at SemEval-2021 Task 7: Method for Detecting and Rating Humor Based on Multi-Task Adversarial Training. (arXiv:2104.10336v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10336</id>
        <link href="http://arxiv.org/abs/2104.10336"/>
        <updated>2021-06-08T02:20:21.109Z</updated>
        <summary type="html"><![CDATA[This paper describes MagicPai's system for SemEval 2021 Task 7, HaHackathon:
Detecting and Rating Humor and Offense. This task aims to detect whether the
text is humorous and how humorous it is. There are four subtasks in the
competition. In this paper, we mainly present our solution, a multi-task
learning model based on adversarial examples, for task 1a and 1b. More
specifically, we first vectorize the cleaned dataset and add the perturbation
to obtain more robust embedding representations. We then correct the loss via
the confidence level. Finally, we perform interactive joint learning on
multiple tasks to capture the relationship between whether the text is humorous
and how humorous it is. The final result shows the effectiveness of our system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shuyi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lianxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1"&gt;Xiaoyi Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SketchGen: Generating Constrained CAD Sketches. (arXiv:2106.02711v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02711</id>
        <link href="http://arxiv.org/abs/2106.02711"/>
        <updated>2021-06-08T02:20:21.086Z</updated>
        <summary type="html"><![CDATA[Computer-aided design (CAD) is the most widely used modeling approach for
technical design. The typical starting point in these designs is 2D sketches
which can later be extruded and combined to obtain complex three-dimensional
assemblies. Such sketches are typically composed of parametric primitives, such
as points, lines, and circular arcs, augmented with geometric constraints
linking the primitives, such as coincidence, parallelism, or orthogonality.
Sketches can be represented as graphs, with the primitives as nodes and the
constraints as edges. Training a model to automatically generate CAD sketches
can enable several novel workflows, but is challenging due to the complexity of
the graphs and the heterogeneity of the primitives and constraints. In
particular, each type of primitive and constraint may require a record of
different size and parameter types. We propose SketchGen as a generative model
based on a transformer architecture to address the heterogeneity problem by
carefully designing a sequential language for the primitives and constraints
that allows distinguishing between different primitive or constraint types and
their parameters, while encouraging our model to re-use information across
related parameters, encoding shared structure. A particular highlight of our
work is the ability to produce primitives linked via constraints that enables
the final output to be further regularized via a constraint solver. We evaluate
our model by demonstrating constraint prediction for given sets of primitives
and full sketch generation from scratch, showing that our approach
significantly out performs the state-of-the-art in CAD sketch generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Para_W/0/1/0/all/0/1"&gt;Wamiq Reyaz Para&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1"&gt;Paul Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1"&gt;Tom Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Camera Vehicle Counting Using Edge-AI. (arXiv:2106.02842v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02842</id>
        <link href="http://arxiv.org/abs/2106.02842"/>
        <updated>2021-06-08T02:20:21.079Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel solution to automatically count vehicles in a
parking lot using images captured by smart cameras. Unlike most of the
literature on this task, which focuses on the analysis of single images, this
paper proposes the use of multiple visual sources to monitor a wider parking
area from different perspectives. The proposed multi-camera system is capable
of automatically estimate the number of cars present in the entire parking lot
directly on board the edge devices. It comprises an on-device deep
learning-based detector that locates and counts the vehicles from the captured
images and a decentralized geometric-based approach that can analyze the
inter-camera shared areas and merge the data acquired by all the devices. We
conduct the experimental evaluation on an extended version of the CNRPark-EXT
dataset, a collection of images taken from the parking lot on the campus of the
National Research Council (CNR) in Pisa, Italy. We show that our system is
robust and takes advantage of the redundant information deriving from the
different cameras, improving the overall performance without requiring any
extra geometrical information of the monitored scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ciampi_L/0/1/0/all/0/1"&gt;Luca Ciampi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1"&gt;Claudio Gennaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carrara_F/0/1/0/all/0/1"&gt;Fabio Carrara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1"&gt;Fabrizio Falchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vairo_C/0/1/0/all/0/1"&gt;Claudio Vairo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1"&gt;Giuseppe Amato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISCO: accurate Discrete Scale Convolutions. (arXiv:2106.02733v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02733</id>
        <link href="http://arxiv.org/abs/2106.02733"/>
        <updated>2021-06-08T02:20:21.069Z</updated>
        <summary type="html"><![CDATA[Scale is often seen as a given, disturbing factor in many vision tasks. When
doing so it is one of the factors why we need more data during learning. In
recent work scale equivariance was added to convolutional neural networks. It
was shown to be effective for a range of tasks. We aim for accurate
scale-equivariant convolutional neural networks (SE-CNNs) applicable for
problems where high granularity of scale and small filter sizes are required.
Current SE-CNNs rely on weight sharing and filter rescaling, the latter of
which is accurate for integer scales only. To reach accurate scale
equivariance, we derive general constraints under which scale-convolution
remains equivariant to discrete rescaling. We find the exact solution for all
cases where it exists, and compute the approximation for the rest. The discrete
scale-convolution pays off, as demonstrated in a new state-of-the-art
classification on MNIST-scale and improving the results on STL-10. With the
same SE scheme, we also improve the computational effort of a scale-equivariant
Siamese tracker on OTB-13.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1"&gt;Ivan Sosnovik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moskalev_A/0/1/0/all/0/1"&gt;Artem Moskalev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1"&gt;Arnold Smeulders&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-hop Question Answering via Reasoning Chains. (arXiv:1910.02610v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.02610</id>
        <link href="http://arxiv.org/abs/1910.02610"/>
        <updated>2021-06-08T02:20:21.060Z</updated>
        <summary type="html"><![CDATA[Multi-hop question answering requires models to gather information from
different parts of a text to answer a question. Most current approaches learn
to address this task in an end-to-end way with neural networks, without
maintaining an explicit representation of the reasoning process. We propose a
method to extract a discrete reasoning chain over the text, which consists of a
series of sentences leading to the answer. We then feed the extracted chains to
a BERT-based QA model to do final answer prediction. Critically, we do not rely
on gold annotated chains or "supporting facts:" at training time, we derive
pseudogold reasoning chains using heuristics based on named entity recognition
and coreference resolution. Nor do we rely on these annotations at test time,
as our model learns to extract chains from raw text alone. We test our approach
on two recently proposed large multi-hop question answering datasets: WikiHop
and HotpotQA, and achieve state-of-art performance on WikiHop and strong
performance on HotpotQA. Our analysis shows the properties of chains that are
crucial for high performance: in particular, modeling extraction sequentially
is important, as is dealing with each candidate sentence in a context-aware
way. Furthermore, human evaluation shows that our extracted chains allow humans
to give answers with high confidence, indicating that these are a strong
intermediate abstraction for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jifan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shih-ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLSD: The Global Large-Scale Ship Database and Baseline Evaluations. (arXiv:2106.02773v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02773</id>
        <link href="http://arxiv.org/abs/2106.02773"/>
        <updated>2021-06-08T02:20:21.051Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a challenging global large-scale ship database
(called GLSD), designed specifically for ship detection tasks. The designed
GLSD database includes a total of 140,616 annotated instances from 100,729
images. Based on the collected images, we propose 13 categories that widely
exists in international routes. These categories include sailing boat, fishing
boat, passenger ship, war ship, general cargo ship, container ship, bulk cargo
carrier, barge, ore carrier, speed boat, canoe, oil carrier, and tug. The
motivations of developing GLSD include the following: 1) providing a refined
ship detection database; 2) providing the worldwide researchers of ship
detection and exhaustive label information (bounding box and ship class label)
in one uniform global database; and 3) providing a large-scale ship database
with geographic information (port and country information) that benefits
multi-modal analysis. In addition, we discuss the evaluation protocols given
image characteristics in GLSD and analyze the performance of selected
state-of-the-art object detection algorithms on GSLD, providing baselines for
future studies. More information regarding the designed GLSD can be found at
https://github.com/jiaming-wang/GLSD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhenfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lianbing Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruiqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xianwei Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qing Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Step Inference for Reasoning Over Paragraphs. (arXiv:2004.02995v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.02995</id>
        <link href="http://arxiv.org/abs/2004.02995"/>
        <updated>2021-06-08T02:20:21.026Z</updated>
        <summary type="html"><![CDATA[Complex reasoning over text requires understanding and chaining together
free-form predicates and logical connectives. Prior work has largely tried to
do this either symbolically or with black-box transformers. We present a middle
ground between these two extremes: a compositional model reminiscent of neural
module networks that can perform chained logical reasoning. This model first
finds relevant sentences in the context and then chains them together using
neural modules. Our model gives significant performance improvements (up to
29\% relative error reduction when comfibined with a reranker) on ROPES, a
recently introduced complex reasoning dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiangming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1"&gt;Matt Gardner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Shay B. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GO FIGURE: A Meta Evaluation of Factuality in Summarization. (arXiv:2010.12834v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12834</id>
        <link href="http://arxiv.org/abs/2010.12834"/>
        <updated>2021-06-08T02:20:21.017Z</updated>
        <summary type="html"><![CDATA[While neural language models can generate text with remarkable fluency and
coherence, controlling for factual correctness in generation remains an open
research question. This major discrepancy between the surface-level fluency and
the content-level correctness of neural generation has motivated a new line of
research that seeks automatic metrics for evaluating the factuality of machine
text. In this paper, we introduce GO FIGURE, a meta-evaluation framework for
evaluating factuality evaluation metrics. We propose five necessary and
intuitive conditions to evaluate factuality metrics on diagnostic factuality
data across three different summarization tasks. Our benchmark analysis on ten
factuality metrics reveals that our meta-evaluation framework provides a robust
and efficient evaluation that is extensible to multiple types of factual
consistency and standard generation metrics, including QA metrics. It also
reveals that while QA metrics generally improve over standard metrics that
measure factuality across domains, performance is highly dependent on the way
in which questions are generated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1"&gt;Saadia Gabriel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1"&gt;Asli Celikyilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_R/0/1/0/all/0/1"&gt;Rahul Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lookup-Table Recurrent Language Models for Long Tail Speech Recognition. (arXiv:2104.04552v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04552</id>
        <link href="http://arxiv.org/abs/2104.04552"/>
        <updated>2021-06-08T02:20:21.004Z</updated>
        <summary type="html"><![CDATA[We introduce Lookup-Table Language Models (LookupLM), a method for scaling up
the size of RNN language models with only a constant increase in the floating
point operations, by increasing the expressivity of the embedding table. In
particular, we instantiate an (additional) embedding table which embeds the
previous n-gram token sequence, rather than a single token. This allows the
embedding table to be scaled up arbitrarily -- with a commensurate increase in
performance -- without changing the token vocabulary. Since embeddings are
sparsely retrieved from the table via a lookup; increasing the size of the
table adds neither extra operations to each forward pass nor extra parameters
that need to be stored on limited GPU/TPU memory. We explore scaling n-gram
embedding tables up to nearly a billion parameters. When trained on a 3-billion
sentence corpus, we find that LookupLM improves long tail log perplexity by
2.44 and long tail WER by 23.4% on a downstream speech recognition task over a
standard RNN language model baseline, an improvement comparable to a scaling up
the baseline by 6.2x the number of floating point operations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;W. Ronny Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1"&gt;Tara N. Sainath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1"&gt;Cal Peyser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Shankar Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rybach_D/0/1/0/all/0/1"&gt;David Rybach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1"&gt;Trevor Strohman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better Robustness by More Coverage: Adversarial Training with Mixup Augmentation for Robust Fine-tuning. (arXiv:2012.15699v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15699</id>
        <link href="http://arxiv.org/abs/2012.15699"/>
        <updated>2021-06-08T02:20:20.995Z</updated>
        <summary type="html"><![CDATA[Pretrained language models (PLMs) perform poorly under adversarial attacks.
To improve the adversarial robustness, adversarial data augmentation (ADA) has
been widely adopted to cover more search space of adversarial attacks by adding
textual adversarial examples during training. However, the number of
adversarial examples for text augmentation is still extremely insufficient due
to the exponentially large attack search space. In this work, we propose a
simple and effective method to cover a much larger proportion of the attack
search space, called Adversarial and Mixup Data Augmentation (AMDA).
Specifically, AMDA linearly interpolates the representations of pairs of
training samples to form new virtual samples, which are more abundant and
diverse than the discrete text adversarial examples in conventional ADA.
Moreover, to fairly evaluate the robustness of different models, we adopt a
challenging evaluation setup, which generates a new set of adversarial examples
targeting each model. In text classification experiments of BERT and RoBERTa,
AMDA achieves significant robustness gains under two strong adversarial attacks
and alleviates the performance degradation of ADA on the clean data. Our code
is available at: https://github.com/thunlp/MixADA .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Chenglei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yasheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Entity Representations for Document-level Relation Extraction. (arXiv:2012.02507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02507</id>
        <link href="http://arxiv.org/abs/2012.02507"/>
        <updated>2021-06-08T02:20:20.962Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) requires extracting relations
expressed within and across sentences. Recent works show that graph-based
methods, usually constructing a document-level graph that captures
document-aware interactions, can obtain useful entity representations thus
helping tackle document-level RE. These methods either focus more on the entire
graph, or pay more attention to a part of the graph, e.g., paths between the
target entity pair. However, we find that document-level RE may benefit from
focusing on both of them simultaneously. Therefore, to obtain more
comprehensive entity representations, we propose the Coarse-to-Fine Entity
Representation model (CFER) that adopts a coarse-to-fine strategy involving two
phases. First, CFER uses graph neural networks to integrate global information
in the entire graph at a coarse level. Next, CFER utilizes the global
information as a guidance to selectively aggregate path information between the
target entity pair at a fine level. In classification, we combine the entity
representations from both two levels into more comprehensive representations
for relation extraction. Experimental results on two document-level RE
datasets, DocRED and CDR, show that CFER outperforms existing models and is
robust to the uneven label distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Damai Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global-aware Beam Search for Neural Abstractive Summarization. (arXiv:2009.06891v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06891</id>
        <link href="http://arxiv.org/abs/2009.06891"/>
        <updated>2021-06-08T02:20:20.879Z</updated>
        <summary type="html"><![CDATA[This study develops a calibrated beam-based algorithm with global awareness
for neural abstractive summarization, aiming to improve the local optimality
problem of the original beam search in a rigorous way. Specifically, a novel
global protocol is proposed based on the attention distribution to stipulate
how a global optimal hypothesis should attend to the source. A global scoring
function is then developed to regulate beam search to generate summaries in a
more near-global optimal fashion. This novel design enjoys a distinctive
property, i.e. the global attention distribution could be predicted before
inference, enabling stepwise improvements on the beam search through the global
scoring function. Extensive experiments on $9$ datasets show that the
global-aware inference significantly improves state-of-the-art summarization
models even using empirical hyper-parameters. The algorithm is also proven
robust as it remains to generate meaningful texts with corrupted attention
distributions. The codes and a comprehensive set of examples are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Ye Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zixun Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_L/0/1/0/all/0/1"&gt;Lu Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising Word Embeddings by Averaging in a Shared Space. (arXiv:2106.02954v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02954</id>
        <link href="http://arxiv.org/abs/2106.02954"/>
        <updated>2021-06-08T02:20:20.848Z</updated>
        <summary type="html"><![CDATA[We introduce a new approach for smoothing and improving the quality of word
embeddings. We consider a method of fusing word embeddings that were trained on
the same corpus but with different initializations. We project all the models
to a shared vector space using an efficient implementation of the Generalized
Procrustes Analysis (GPA) procedure, previously used in multilingual word
translation. Our word representation demonstrates consistent improvements over
the raw models as well as their simplistic average, on a range of tasks. As the
new representations are more stable and reliable, there is a noticeable
improvement in rare word evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1"&gt;Avi Caciularu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1"&gt;Ido Dagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1"&gt;Jacob Goldberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains. (arXiv:2106.02792v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02792</id>
        <link href="http://arxiv.org/abs/2106.02792"/>
        <updated>2021-06-08T02:20:20.839Z</updated>
        <summary type="html"><![CDATA[Social media has become a valuable resource for the study of suicidal
ideation and the assessment of suicide risk. Among social media platforms,
Reddit has emerged as the most promising one due to its anonymity and its focus
on topic-based communities (subreddits) that can be indicative of someone's
state of mind or interest regarding mental health disorders such as
r/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on
suicide risk assessment has been the small amount of labeled data. We propose
an empirical investigation into several classes of weakly-supervised
approaches, and show that using pseudo-labeling based on related issues around
mental health (e.g., anxiety, depression) helps improve model performance for
suicide risk assessment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenghao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yudong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1"&gt;Smaranda Muresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Learning of Hate Speech Classification on Social Media. (arXiv:2106.02821v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02821</id>
        <link href="http://arxiv.org/abs/2106.02821"/>
        <updated>2021-06-08T02:20:20.826Z</updated>
        <summary type="html"><![CDATA[Existing work on automated hate speech classification assumes that the
dataset is fixed and the classes are pre-defined. However, the amount of data
in social media increases every day, and the hot topics changes rapidly,
requiring the classifiers to be able to continuously adapt to new data without
forgetting the previously learned knowledge. This ability, referred to as
lifelong learning, is crucial for the real-word application of hate speech
classifiers in social media. In this work, we propose lifelong learning of hate
speech classification on social media. To alleviate catastrophic forgetting, we
propose to use Variational Representation Learning (VRL) along with a memory
module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural
Network). Experimentally, we show that combining variational representation
learning and the LB-SOINN memory module achieves better performance than the
commonly-used lifelong learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jing Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1"&gt;Mai ElSherief&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xifeng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel method for recommendation systems using invasive weed optimization. (arXiv:2106.02831v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02831</id>
        <link href="http://arxiv.org/abs/2106.02831"/>
        <updated>2021-06-08T02:20:20.805Z</updated>
        <summary type="html"><![CDATA[One of the popular approaches in recommendation systems is Collaborative
Filtering (CF). The most significant step in CF is choosing the appropriate set
of users. For this purpose, similarity measures are usually used for computing
the similarity between a specific user and the other users. This paper proposes
a new invasive weed optimization (IWO) based CF approach that uses users'
context to identify important and effective users set. By using a newly defined
similarity measure based on both rating values and a measure values called
confidence, the proposed approach calculates the similarity between users and
thus identifies and filters the most similar users to a specific user. It then
uses IWO to calculate the importance degree of users and finally, by using the
identified important users and their importance degrees it predicts unknown
ratings. To evaluate the proposed method, several experiments have been
performed on two known real world datasets and the results show that the
proposed method improves the state of the art results up to 15% in terms of
Root Mean Square Error (RMSE) and Mean Absolute Error (MAE).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soltaninejad_F/0/1/0/all/0/1"&gt;Fahimeh Soltaninejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bidgoly_A/0/1/0/all/0/1"&gt;Amir Jalaly Bidgoly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MergeDistill: Merging Pre-trained Language Models using Distillation. (arXiv:2106.02834v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02834</id>
        <link href="http://arxiv.org/abs/2106.02834"/>
        <updated>2021-06-08T02:20:20.787Z</updated>
        <summary type="html"><![CDATA[Pre-trained multilingual language models (LMs) have achieved state-of-the-art
results in cross-lingual transfer, but they often lead to an inequitable
representation of languages due to limited capacity, skewed pre-training data,
and sub-optimal vocabularies. This has prompted the creation of an ever-growing
pre-trained model universe, where each model is trained on large amounts of
language or domain specific data with a carefully curated, linguistically
informed vocabulary. However, doing so brings us back full circle and prevents
one from leveraging the benefits of multilinguality. To address the gaps at
both ends of the spectrum, we propose MergeDistill, a framework to merge
pre-trained LMs in a way that can best leverage their assets with minimal
dependencies, using task-agnostic knowledge distillation. We demonstrate the
applicability of our framework in a practical setting by leveraging
pre-existing teacher LMs and training student LMs that perform competitively
with or even outperform teacher LMs trained on several orders of magnitude more
data and with a fixed model capacity. We also highlight the importance of
teacher selection and its impact on student model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1"&gt;Simran Khanuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"&gt;Melvin Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Talukdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiOpEd: A Corpus of Multi-Perspective News Editorials. (arXiv:2106.02725v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02725</id>
        <link href="http://arxiv.org/abs/2106.02725"/>
        <updated>2021-06-08T02:20:20.776Z</updated>
        <summary type="html"><![CDATA[We propose MultiOpEd, an open-domain news editorial corpus that supports
various tasks pertaining to the argumentation structure in news editorials,
focusing on automatic perspective discovery. News editorial is a genre of
persuasive text, where the argumentation structure is usually implicit.
However, the arguments presented in an editorial typically center around a
concise, focused thesis, which we refer to as their perspective. MultiOpEd aims
at supporting the study of multiple tasks relevant to automatic perspective
discovery, where a system is expected to produce a single-sentence thesis
statement summarizing the arguments presented. We argue that identifying and
abstracting such natural language perspectives from editorials is a crucial
step toward studying the implicit argumentation structure in news editorials.
We first discuss the challenges and define a few conceptual tasks towards our
goal. To demonstrate the utility of MultiOpEd and the induced tasks, we study
the problem of perspective summarization in a multi-task learning setting, as a
case study. We show that, with the induced tasks as auxiliary tasks, we can
improve the quality of the perspective summary generated. We hope that
MultiOpEd will be a useful resource for future studies on argumentation in the
news editorial domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sihao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uyttendaele_X/0/1/0/all/0/1"&gt;Xander Uyttendaele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1"&gt;Dan Roth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings. (arXiv:2106.02736v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02736</id>
        <link href="http://arxiv.org/abs/2106.02736"/>
        <updated>2021-06-08T02:20:20.767Z</updated>
        <summary type="html"><![CDATA[While recent work has shown that scores from models trained by the ubiquitous
masked language modeling (MLM) objective effectively discriminate probable and
improbable sequences, it is still an open question if these MLMs specify a
principled probability distribution over the space of possible sequences. In
this paper, we interpret MLMs as energy-based sequence models and propose two
energy parametrizations derivable from the trained MLMs. In order to draw
samples correctly from these models, we develop a tractable \emph{sampling}
scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our
approach, samples are proposed from the same masked conditionals used for
training the masked language models, and they are accepted or rejected based on
their energy values according to the target distribution. We validate the
effectiveness of the proposed parametrizations by exploring the quality of
samples drawn from these energy-based models on the conditional generation task
of machine translation. We theoretically and empirically justify our sampling
algorithm by showing that the masked conditionals on their own do not yield a
Markov chain whose stationary distribution is that of our target distribution,
and our approach generates higher quality samples than other recently proposed
undirected generation approaches (Wang et al., 2019, Ghazvininejad et al.,
2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1"&gt;Kartik Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1"&gt;Chris Dyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PURS: Personalized Unexpected Recommender System for Improving User Satisfaction. (arXiv:2106.02771v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02771</id>
        <link href="http://arxiv.org/abs/2106.02771"/>
        <updated>2021-06-08T02:20:20.759Z</updated>
        <summary type="html"><![CDATA[Classical recommender system methods typically face the filter bubble problem
when users only receive recommendations of their familiar items, making them
bored and dissatisfied. To address the filter bubble problem, unexpected
recommendations have been proposed to recommend items significantly deviating
from user's prior expectations and thus surprising them by presenting "fresh"
and previously unexplored items to the users. In this paper, we describe a
novel Personalized Unexpected Recommender System (PURS) model that incorporates
unexpectedness into the recommendation process by providing multi-cluster
modeling of user interests in the latent space and personalized unexpectedness
via the self-attention mechanism and via selection of an appropriate unexpected
activation function. Extensive offline experiments on three real-world datasets
illustrate that the proposed PURS model significantly outperforms the
state-of-the-art baseline approaches in terms of both accuracy and
unexpectedness measures. In addition, we conduct an online A/B test at a major
video platform Alibaba-Youku, where our model achieves over 3\% increase in the
average video view per user metric. The proposed model is in the process of
being deployed by the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Communication of Generalizations. (arXiv:2106.02668v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02668</id>
        <link href="http://arxiv.org/abs/2106.02668"/>
        <updated>2021-06-08T02:20:20.733Z</updated>
        <summary type="html"><![CDATA[To build agents that can collaborate effectively with others, recent research
has trained artificial agents to communicate with each other in Lewis-style
referential games. However, this often leads to successful but uninterpretable
communication. We argue that this is due to the game objective: communicating
about a single object in a shared visual context is prone to overfitting and
does not encourage language useful beyond concrete reference. In contrast,
human language conveys a rich variety of abstract ideas. To promote such
skills, we propose games that require communicating generalizations over sets
of objects representing abstract visual concepts, optionally with separate
contexts for each agent. We find that these games greatly improve systematicity
and interpretability of the learned languages, according to several metrics in
the literature. Finally, we propose a method for identifying logical operations
embedded in the emergent languages by learning an approximate compositional
reconstruction of the language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1"&gt;Jesse Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models. (arXiv:2106.02679v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02679</id>
        <link href="http://arxiv.org/abs/2106.02679"/>
        <updated>2021-06-08T02:20:20.719Z</updated>
        <summary type="html"><![CDATA[The advent of the transformer has sparked a quick growth in the size of
language models, far outpacing hardware improvements. (Dense) transformers are
expected to reach the trillion-parameter scale in the near future, for which
training requires thousands or even tens of thousands of GPUs. We investigate
the challenges of training at this scale and beyond on commercially available
hardware. In particular, we analyse the shortest possible training time for
different configurations of distributed training, leveraging empirical scaling
laws for language models to estimate the optimal (critical) batch size.
Contrary to popular belief, we find no evidence for a memory wall, and instead
argue that the real limitation -- other than the cost -- lies in the training
duration.

In addition to this analysis, we introduce two new methods, \textit{layered
gradient accumulation} and \textit{modular pipeline parallelism}, which
together cut the shortest training time by half. The methods also reduce data
movement, lowering the network requirement to a point where a fast InfiniBand
connection is not necessary. This increased network efficiency also improve on
the methods introduced with the ZeRO optimizer, reducing the memory usage to a
tiny fraction of the available GPU memory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1"&gt;Joel Lamy-Poirier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning Based Classification Algorithm Recommendation. (arXiv:2101.05993v1 [cs.IR] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2101.05993</id>
        <link href="http://arxiv.org/abs/2101.05993"/>
        <updated>2021-06-08T02:20:20.580Z</updated>
        <summary type="html"><![CDATA[Recommending appropriate algorithms to a classification problem is one of the
most challenging issues in the field of data mining. The existing algorithm
recommendation models are generally constructed on only one kind of
meta-features by single learners. Considering that i) ensemble learners usually
show better performance and ii) different kinds of meta-features characterize
the classification problems in different viewpoints independently, and further
the models constructed with different sets of meta-features will be
complementary with each other and applicable for ensemble. This paper proposes
an ensemble learning-based algorithm recommendation method. To evaluate the
proposed recommendation method, extensive experiments with 13 well-known
candidate classification algorithms and five different kinds of meta-features
are conducted on 1090 benchmark classification problems. The results show the
effectiveness of the proposed ensemble learning based recommendation method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qinbao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidirectional Distillation for Top-K Recommender System. (arXiv:2106.02870v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02870</id>
        <link href="http://arxiv.org/abs/2106.02870"/>
        <updated>2021-06-08T02:20:20.569Z</updated>
        <summary type="html"><![CDATA[Recommender systems (RS) have started to employ knowledge distillation, which
is a model compression technique training a compact model (student) with the
knowledge transferred from a cumbersome model (teacher). The state-of-the-art
methods rely on unidirectional distillation transferring the knowledge only
from the teacher to the student, with an underlying assumption that the teacher
is always superior to the student. However, we demonstrate that the student
performs better than the teacher on a significant proportion of the test set,
especially for RS. Based on this observation, we propose Bidirectional
Distillation (BD) framework whereby both the teacher and the student
collaboratively improve with each other. Specifically, each model is trained
with the distillation loss that makes to follow the other's prediction along
with its original loss function. For effective bidirectional distillation, we
propose rank discrepancy-aware sampling scheme to distill only the informative
knowledge that can fully enhance each other. The proposed scheme is designed to
effectively cope with a large performance gap between the teacher and the
student. Trained in the bidirectional way, it turns out that both the teacher
and the student are significantly improved compared to when being trained
separately. Our extensive experiments on real-world datasets show that our
proposed framework consistently outperforms the state-of-the-art competitors.
We also provide analyses for an in-depth understanding of BD and ablation
studies to verify the effectiveness of each proposed component.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_W/0/1/0/all/0/1"&gt;Wonbin Kweon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;SeongKu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hwanjo Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction. (arXiv:2106.02768v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02768</id>
        <link href="http://arxiv.org/abs/2106.02768"/>
        <updated>2021-06-08T02:20:20.522Z</updated>
        <summary type="html"><![CDATA[Cross domain recommender system constitutes a powerful method to tackle the
cold-start and sparsity problem by aggregating and transferring user
preferences across multiple category domains. Therefore, it has great potential
to improve click-through-rate prediction performance in online commerce
platforms having many domains of products. While several cross domain
sequential recommendation models have been proposed to leverage information
from a source domain to improve CTR predictions in a target domain, they did
not take into account bidirectional latent relations of user preferences across
source-target domain pairs. As such, they cannot provide enhanced cross-domain
CTR predictions for both domains simultaneously. In this paper, we propose a
novel approach to cross-domain sequential recommendations based on the dual
learning mechanism that simultaneously transfers information between two
related domains in an iterative manner until the learning process stabilizes.
In particular, the proposed Dual Attentive Sequential Learning (DASL) model
consists of two novel components Dual Embedding and Dual Attention, which
jointly establish the two-stage learning process: we first construct dual
latent embeddings that extract user preferences in both domains simultaneously,
and subsequently provide cross-domain recommendations by matching the extracted
latent embeddings with candidate items through dual-attention learning
mechanism. We conduct extensive offline experiments on three real-world
datasets to demonstrate the superiority of our proposed model, which
significantly and consistently outperforms several state-of-the-art baselines
across all experimental settings. We also conduct an online A/B test at a major
video streaming platform Alibaba-Youku, where our proposed model significantly
improves business performance over the latest production system in the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual Information Regularized Identity-aware Facial ExpressionRecognition in Compressed Video. (arXiv:2010.10637v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10637</id>
        <link href="http://arxiv.org/abs/2010.10637"/>
        <updated>2021-06-08T02:20:20.501Z</updated>
        <summary type="html"><![CDATA[How to extract effective expression representations that invariant to the
identity-specific attributes is a long-lasting problem for facial expression
recognition (FER). Most of the previous methods process the RGB images of a
sequence, while we argue that the off-the-shelf and valuable expression-related
muscle movement is already embedded in the compression format. In this paper,
we target to explore the inter-subject variations eliminated facial expression
representation in the compressed video domain. In the up to two orders of
magnitude compressed domain, we can explicitly infer the expression from the
residual frames and possibly extract identity factors from the I frame with a
pre-trained face recognition network. By enforcing the marginal independence of
them, the expression feature is expected to be purer for the expression and be
robust to identity shifts. Specifically, we propose a novel collaborative
min-min game for mutual information (MI) minimization in latent space. We do
not need the identity label or multiple expression samples from the same person
for identity elimination. Moreover, when the apex frame is annotated in the
dataset, the complementary constraint can be further added to regularize the
feature-level game. In testing, only the compressed residual frames are
required to achieve expression prediction. Our solution can achieve comparable
or better performance than the recent decoded image-based methods on the
typical FER benchmarks with about 3 times faster inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Linghao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auditing Source Diversity Bias in Video Search Results Using Virtual Agents. (arXiv:2106.02715v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02715</id>
        <link href="http://arxiv.org/abs/2106.02715"/>
        <updated>2021-06-08T02:20:20.489Z</updated>
        <summary type="html"><![CDATA[We audit the presence of domain-level source diversity bias in video search
results. Using a virtual agent-based approach, we compare outputs of four
Western and one non-Western search engines for English and Russian queries. Our
findings highlight that source diversity varies substantially depending on the
language with English queries returning more diverse outputs. We also find
disproportionately high presence of a single platform, YouTube, in top search
outputs for all Western search engines except Google. At the same time, we
observe that Youtube's major competitors such as Vimeo or Dailymotion do not
appear in the sampled Google's video search results. This finding suggests that
Google might be downgrading the results from the main competitors of
Google-owned Youtube and highlights the necessity for further studies focusing
on the presence of own-content bias in Google's search results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Urman_A/0/1/0/all/0/1"&gt;Aleksandra Urman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makhortykh_M/0/1/0/all/0/1"&gt;Mykola Makhortykh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulloa_R/0/1/0/all/0/1"&gt;Roberto Ulloa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System. (arXiv:2010.15363v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15363</id>
        <link href="http://arxiv.org/abs/2010.15363"/>
        <updated>2021-06-08T02:20:20.471Z</updated>
        <summary type="html"><![CDATA[The general aim of the recommender system is to provide personalized
suggestions to users, which is opposed to suggesting popular items. However,
the normal training paradigm, i.e., fitting a recommender model to recover the
user behavior data with pointwise or pairwise loss, makes the model biased
towards popular items. This results in the terrible Matthew effect, making
popular items be more frequently recommended and become even more popular.
Existing work addresses this issue with Inverse Propensity Weighting (IPW),
which decreases the impact of popular items on the training and increases the
impact of long-tail items. Although theoretically sound, IPW methods are highly
sensitive to the weighting strategy, which is notoriously difficult to tune. In
this work, we explore the popularity bias issue from a novel and fundamental
perspective -- cause-effect. We identify that popularity bias lies in the
direct effect from the item node to the ranking score, such that an item's
intrinsic property is the cause of mistakenly assigning it a higher ranking
score. To eliminate popularity bias, it is essential to answer the
counterfactual question that what the ranking score would be if the model only
uses item property. To this end, we formulate a causal graph to describe the
important cause-effect relations in the recommendation process. During
training, we perform multi-task learning to achieve the contribution of each
cause; during testing, we perform counterfactual inference to remove the effect
of item popularity. Remarkably, our solution amends the learning process of
recommendation which is agnostic to a wide range of models -- it can be easily
implemented in existing methods. We demonstrate it on Matrix Factorization (MF)
and LightGCN [20]. Experiments on five real-world datasets demonstrate the
effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tianxin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MC2G: An Efficient Algorithm for Matrix Completion with Social and Item Similarity Graphs. (arXiv:2006.04373v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04373</id>
        <link href="http://arxiv.org/abs/2006.04373"/>
        <updated>2021-06-08T02:20:20.446Z</updated>
        <summary type="html"><![CDATA[In this paper, we design and analyze MC2G (Matrix Completion with 2 Graphs),
an algorithm that performs matrix completion in the presence of social and item
similarity graphs. MC2G runs in quasilinear time and is parameter free. It is
based on spectral clustering and local refinement steps. The expected number of
sampled entries required for MC2G to succeed (i.e., recover the clusters in the
graphs and complete the matrix) matches an information-theoretic lower bound up
to a constant factor for a wide range of parameters. We show via extensive
experiments on both synthetic and real datasets that MC2G outperforms other
state-of-the-art matrix completion algorithms that leverage graph side
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiaosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_G/0/1/0/all/0/1"&gt;Geewon Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_C/0/1/0/all/0/1"&gt;Changho Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Embed Categorical Features without Embedding Tables for Recommendation. (arXiv:2010.10784v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10784</id>
        <link href="http://arxiv.org/abs/2010.10784"/>
        <updated>2021-06-08T02:20:20.433Z</updated>
        <summary type="html"><![CDATA[Embedding learning of categorical features (e.g. user/item IDs) is at the
core of various recommendation models including matrix factorization and neural
collaborative filtering. The standard approach creates an embedding table where
each row represents a dedicated embedding vector for every unique feature
value. However, this method fails to efficiently handle high-cardinality
features and unseen feature values (e.g. new video ID) that are prevalent in
real-world recommendation systems. In this paper, we propose an alternative
embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a
deep embedding network to compute embeddings on the fly. DHE first encodes the
feature value to a unique identifier vector with multiple hashing functions and
transformations, and then applies a DNN to convert the identifier vector to an
embedding. The encoding module is deterministic, non-learnable, and free of
storage, while the embedding network is updated during the training time to
learn embedding generation. Empirical results show that DHE achieves comparable
AUC against the standard one-hot full embedding, with smaller model sizes. Our
work sheds light on the design of DNN-based alternative embedding schemes for
categorical features without using embedding table lookup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Wang-Cheng Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1"&gt;Derek Zhiyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Tiansheng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinyang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lichan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of fake news on CoViD-19 on Web Search Engines. (arXiv:2103.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11804</id>
        <link href="http://arxiv.org/abs/2103.11804"/>
        <updated>2021-06-08T02:20:20.418Z</updated>
        <summary type="html"><![CDATA[In early January 2020, after China reported the first cases of the new
coronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully
accurate information has started spreading faster than the virus itself.
Alongside this pandemic, people have experienced a parallel infodemic, i.e., an
overabundance of information, some of which misleading or even harmful, that
has widely spread around the globe. Although Social Media are increasingly
being used as information source, Web Search Engines, like Google or Yahoo!,
still represent a powerful and trustworthy resource for finding information on
the Web. This is due to their capability to capture the largest amount of
information, helping users quickly identify the most relevant, useful, although
not always the most reliable, results for their search queries. This study aims
to detect potential misleading and fake contents by capturing and analysing
textual information, which flow through Search Engines. By using a real-world
dataset associated with recent CoViD-19 pandemic, we first apply re-sampling
techniques for class imbalance, then we use existing Machine Learning
algorithms for classification of not reliable news. By extracting lexical and
host-based features of associated Uniform Resource Locators (URLs) for news
articles, we show that the proposed methods, so common in phishing and
malicious URLs detection, can improve the efficiency and performance of
classifiers. Based on these findings, we suggest that the use of both textual
and URLs features can improve the effectiveness of fake news detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzeo_V/0/1/0/all/0/1"&gt;V. Mazzeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapisarda_A/0/1/0/all/0/1"&gt;A. Rapisarda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giuffrida_G/0/1/0/all/0/1"&gt;G. Giuffrida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepQTMT: A Deep Learning Approach for Fast QTMT-based CU Partition of Intra-mode VVC. (arXiv:2006.13125v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13125</id>
        <link href="http://arxiv.org/abs/2006.13125"/>
        <updated>2021-06-08T02:20:20.377Z</updated>
        <summary type="html"><![CDATA[Versatile Video Coding (VVC), as the latest standard, significantly improves
the coding efficiency over its ancestor standard High Efficiency Video Coding
(HEVC), but at the expense of sharply increased complexity. In VVC, the
quad-tree plus multi-type tree (QTMT) structure of coding unit (CU) partition
accounts for over 97% of the encoding time, due to the brute-force search for
recursive rate-distortion (RD) optimization. Instead of the brute-force QTMT
search, this paper proposes a deep learning approach to predict the QTMT-based
CU partition, for drastically accelerating the encoding process of intra-mode
VVC. First, we establish a large-scale database containing sufficient CU
partition patterns with diverse video content, which can facilitate the
data-driven VVC complexity reduction. Next, we propose a multi-stage exit CNN
(MSE-CNN) model with an early-exit mechanism to determine the CU partition, in
accord with the flexible QTMT structure at multiple stages. Then, we design an
adaptive loss function for training the MSE-CNN model, synthesizing both the
uncertain number of split modes and the target on minimized RD cost. Finally, a
multi-threshold decision scheme is developed, achieving desirable trade-off
between complexity and RD performance. Experimental results demonstrate that
our approach can reduce the encoding time of VVC by 44.65%-66.88% with the
negligible Bj{\o}ntegaard delta bit-rate (BD-BR) of 1.322%-3.188%, which
significantly outperforms other state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_R/0/1/0/all/0/1"&gt;Runzhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_Q/0/1/0/all/0/1"&gt;Qunliang Xing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End Breast Tumour Classification Model Using Context-Based Patch Modelling- A BiLSTM Approach for Image Classification. (arXiv:2106.02864v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02864</id>
        <link href="http://arxiv.org/abs/2106.02864"/>
        <updated>2021-06-08T02:20:20.291Z</updated>
        <summary type="html"><![CDATA[Researchers working on computational analysis of Whole Slide Images (WSIs) in
histopathology have primarily resorted to patch-based modelling due to large
resolution of each WSI. The large resolution makes WSIs infeasible to be fed
directly into the machine learning models due to computational constraints.
However, due to patch-based analysis, most of the current methods fail to
exploit the underlying spatial relationship among the patches. In our work, we
have tried to integrate this relationship along with feature-based correlation
among the extracted patches from the particular tumorous region. For the given
task of classification, we have used BiLSTMs to model both forward and backward
contextual relationship. RNN based models eliminate the limitation of sequence
size by allowing the modelling of variable size images within a deep learning
model. We have also incorporated the effect of spatial continuity by exploring
different scanning techniques used to sample patches. To establish the
efficiency of our approach, we trained and tested our model on two datasets,
microscopy images and WSI tumour regions. After comparing with contemporary
literature we achieved the better performance with accuracy of 90% for
microscopy image dataset. For WSI tumour region dataset, we compared the
classification results with deep learning networks such as ResNet, DenseNet,
and InceptionV3 using maximum voting technique. We achieved the highest
performance accuracy of 84%. We found out that BiLSTMs with CNN features have
performed much better in modelling patches into an end-to-end Image
classification network. Additionally, the variable dimensions of WSI tumour
regions were used for classification without the need for resizing. This
suggests that our method is independent of tumour image size and can process
large dimensional images without losing the resolution details.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Suvidha Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hwee Kuan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder-Decoder Neural Architecture Optimization for Keyword Spotting. (arXiv:2106.02738v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02738</id>
        <link href="http://arxiv.org/abs/2106.02738"/>
        <updated>2021-06-08T02:20:20.207Z</updated>
        <summary type="html"><![CDATA[Keyword spotting aims to identify specific keyword audio utterances. In
recent years, deep convolutional neural networks have been widely utilized in
keyword spotting systems. However, their model architectures are mainly based
on off-the shelfbackbones such as VGG-Net or ResNet, instead of specially
designed for the task. In this paper, we utilize neural architecture search to
design convolutional neural network models that can boost the performance of
keyword spotting while maintaining an acceptable memory footprint.
Specifically, we search the model operators and their connections in a specific
search space with Encoder-Decoder neural architecture optimization. Extensive
evaluations on Google's Speech Commands Dataset show that the model
architecture searched by our approach achieves a state-of-the-art accuracy of
over 97%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_T/0/1/0/all/0/1"&gt;Tong Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[About Explicit Variance Minimization: Training Neural Networks for Medical Imaging With Limited Data Annotations. (arXiv:2105.14117v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14117</id>
        <link href="http://arxiv.org/abs/2105.14117"/>
        <updated>2021-06-07T23:29:40.150Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning methods for computer vision have demonstrated the
effectiveness of pre-training feature representations, resulting in
well-generalizing Deep Neural Networks, even if the annotated data are limited.
However, representation learning techniques require a significant amount of
time for model training, with most of it time spent on precise hyper-parameter
optimization and selection of augmentation techniques. We hypothesized that if
the annotated dataset has enough morphological diversity to capture the general
population's as is common in medical imaging, for example, due to conserved
similarities of tissue mythologies, the variance error of the trained model is
the prevalent component of the Bias-Variance Trade-off. We propose the Variance
Aware Training (VAT) method that exploits this property by introducing the
variance error into the model loss function, i.e., enabling minimizing the
variance explicitly. Additionally, we provide the theoretical formulation and
proof of the proposed method to aid in interpreting the approach. Our method
requires selecting only one hyper-parameter and was able to match or improve
the state-of-the-art performance of self-supervised methods while achieving an
order of magnitude reduction in the GPU training time. We validated VAT on
three medical imaging datasets from diverse domains and various learning
objectives. These included a Magnetic Resonance Imaging (MRI) dataset for the
heart semantic segmentation (MICCAI 2017 ACDC challenge), fundus photography
dataset for ordinary regression of diabetic retinopathy progression (Kaggle
2019 APTOS Blindness Detection challenge), and classification of
histopathologic scans of lymph node sections (PatchCamelyon dataset).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shubin_D/0/1/0/all/0/1"&gt;Dmitrii Shubin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eytan_D/0/1/0/all/0/1"&gt;Danny Eytan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodfellow_S/0/1/0/all/0/1"&gt;Sebastian D. Goodfellow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning. (arXiv:2105.14106v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14106</id>
        <link href="http://arxiv.org/abs/2105.14106"/>
        <updated>2021-06-07T23:29:40.127Z</updated>
        <summary type="html"><![CDATA[The design of machines and algorithms capable of learning in a dynamically
changing environment has become an increasingly topical problem with the
increase of the size and heterogeneity of data available to learning systems.
As a consequence, the key issue of Continual Learning has become that of
addressing the stability-plasticity dilemma of connectionist systems, as they
need to adapt their model without forgetting previously acquired knowledge.
Within this context, rehearsal-based methods i.e., solutions in where the
learner exploits memory to revisit past data, has proven to be very effective,
leading to performance at the state-of-the-art. In our study, we propose an
analysis of the memory quantity/quality trade-off adopting various data
reduction approaches to increase the number of instances storable in memory. In
particular, we investigate complex instance compression techniques such as deep
encoders, but also trivial approaches such as image resizing and linear
dimensionality reduction. Our findings suggest that the optimal trade-off is
severely skewed toward instance quantity, where rehearsal approaches with
several heavily compressed instances easily outperform state-of-the-art
approaches with the same amount of memory at their disposal. Further, in high
memory configurations, deep approaches extracting spatial structure combined
with extreme resizing (of the order of $8\times8$ images) yield the best
results, while in memory-constrained configurations where deep approaches
cannot be used due to their memory requirement in training, Extreme Learning
Machines (ELM) offer a clear advantage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1"&gt;Francesco Pelosin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torsello_A/0/1/0/all/0/1"&gt;Andrea Torsello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems. (arXiv:2105.11558v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11558</id>
        <link href="http://arxiv.org/abs/2105.11558"/>
        <updated>2021-06-07T22:33:05.470Z</updated>
        <summary type="html"><![CDATA[We consider the setting of vector valued non-linear dynamical systems
$X_{t+1} = \phi(A^* X_t) + \eta_t$, where $\eta_t$ is unbiased noise and $\phi
: \mathbb{R} \to \mathbb{R}$ is a known link function that satisfies certain
{\em expansivity property}. The goal is to learn $A^*$ from a single trajectory
$X_1,\cdots,X_T$ of {\em dependent or correlated} samples. While the problem is
well-studied in the linear case, where $\phi$ is identity, with optimal error
rates even for non-mixing systems, existing results in the non-linear case hold
only for mixing systems. In this work, we improve existing results for learning
nonlinear systems in a number of ways: a) we provide the first offline
algorithm that can learn non-linear dynamical systems without the mixing
assumption, b) we significantly improve upon the sample complexity of existing
results for mixing systems, c) in the much harder one-pass, streaming setting
we study a SGD with Reverse Experience Replay ($\mathsf{SGD-RER}$) method, and
demonstrate that for mixing systems, it achieves the same sample complexity as
our offline algorithm, d) we justify the expansivity assumption by showing that
for the popular ReLU link function -- a non-expansive but easy to learn link
function with i.i.d. samples -- any method would require exponentially many
samples (with respect to dimension of $X_t$) from the dynamical system. We
validate our results via. simulations and demonstrate that a naive application
of SGD can be highly sub-optimal. Indeed, our work demonstrates that for
correlated data, specialized methods designed for the dependency structure in
data can significantly outperform standard SGD based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Suhas S Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraj_D/0/1/0/all/0/1"&gt;Dheeraj Nagaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equilibrium and non-Equilibrium regimes in the learning of Restricted Boltzmann Machines. (arXiv:2105.13889v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13889</id>
        <link href="http://arxiv.org/abs/2105.13889"/>
        <updated>2021-06-07T22:33:05.420Z</updated>
        <summary type="html"><![CDATA[Training Restricted Boltzmann Machines (RBMs) has been challenging for a long
time due to the difficulty of computing precisely the log-likelihood gradient.
Over the past decades, many works have proposed more or less successful
training recipes but without studying the crucial quantity of the problem: the
mixing time i.e. the number of Monte Carlo iterations needed to sample new
configurations from a model. In this work, we show that this mixing time plays
a crucial role in the dynamics and stability of the trained model, and that
RBMs operate in two well-defined regimes, namely equilibrium and
out-of-equilibrium, depending on the interplay between this mixing time of the
model and the number of steps, $k$, used to approximate the gradient. We
further show empirically that this mixing time increases with the learning,
which often implies a transition from one regime to another as soon as $k$
becomes smaller than this time. In particular, we show that using the popular
$k$ (persistent) contrastive divergence approaches, with $k$ small, the
dynamics of the learned model are extremely slow and often dominated by strong
out-of-equilibrium effects. On the contrary, RBMs trained in equilibrium
display faster dynamics, and a smooth convergence to dataset-like
configurations during the sampling. Finally we discuss how to exploit in
practice both regimes depending on the task one aims to fulfill: (i) short $k$s
can be used to generate convincing samples in short times, (ii) large $k$ (or
increasingly large) must be used to learn the correct equilibrium distribution
of the RBM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Decelle_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Decelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furtlehner_C/0/1/0/all/0/1"&gt;Cyril Furtlehner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seoane_B/0/1/0/all/0/1"&gt;Beatriz Seoane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Online-Bandit Strategies for Minimax Learning Problems. (arXiv:2105.13939v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13939</id>
        <link href="http://arxiv.org/abs/2105.13939"/>
        <updated>2021-06-07T22:33:05.409Z</updated>
        <summary type="html"><![CDATA[Several learning problems involve solving min-max problems, e.g., empirical
distributional robust learning or learning with non-standard aggregated losses.
More specifically, these problems are convex-linear problems where the
minimization is carried out over the model parameters $w\in\mathcal{W}$ and the
maximization over the empirical distribution $p\in\mathcal{K}$ of the training
set indexes, where $\mathcal{K}$ is the simplex or a subset of it. To design
efficient methods, we let an online learning algorithm play against a
(combinatorial) bandit algorithm. We argue that the efficiency of such
approaches critically depends on the structure of $\mathcal{K}$ and propose two
properties of $\mathcal{K}$ that facilitate designing efficient algorithms. We
focus on a specific family of sets $\mathcal{S}_{n,k}$ encompassing various
learning applications and provide high-probability convergence guarantees to
the minimax values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roux_C/0/1/0/all/0/1"&gt;Christophe Roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wirth_E/0/1/0/all/0/1"&gt;Elias Wirth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokutta_S/0/1/0/all/0/1"&gt;Sebastian Pokutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerdreux_T/0/1/0/all/0/1"&gt;Thomas Kerdreux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A nearly Blackwell-optimal policy gradient method. (arXiv:2105.13609v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13609</id>
        <link href="http://arxiv.org/abs/2105.13609"/>
        <updated>2021-06-07T22:33:05.399Z</updated>
        <summary type="html"><![CDATA[For continuing environments, reinforcement learning methods commonly maximize
a discounted reward criterion with discount factor close to 1 in order to
approximate the steady-state reward (the gain). However, such a criterion only
considers the long-run performance, ignoring the transient behaviour. In this
work, we develop a policy gradient method that optimizes the gain, then the
bias (which indicates the transient performance and is important to capably
select from policies with equal gain). We derive expressions that enable
sampling for the gradient of the bias, and its preconditioning Fisher matrix.
We further propose an algorithm that solves the corresponding bi-level
optimization using a logarithmic barrier. Experimental results provide insights
into the fundamental mechanisms of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dewanto_V/0/1/0/all/0/1"&gt;Vektor Dewanto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1"&gt;Marcus Gallagher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Bandit under Differential Privacy. (arXiv:2105.11126v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11126</id>
        <link href="http://arxiv.org/abs/2105.11126"/>
        <updated>2021-06-07T22:33:05.386Z</updated>
        <summary type="html"><![CDATA[This paper studies \emph{differential privacy (DP)} and \emph{local
differential privacy (LDP)} in cascading bandits. Under DP, we propose an
algorithm which guarantees $\epsilon$-indistinguishability and a regret of
$\mathcal{O}((\frac{\log T}{\epsilon})^{1+\xi})$ for an arbitrarily small
$\xi$. This is a significant improvement from the previous work of
$\mathcal{O}(\frac{\log^3 T}{\epsilon})$ regret. Under
($\epsilon$,$\delta$)-LDP, we relax the $K^2$ dependence through the tradeoff
between privacy budget $\epsilon$ and error probability $\delta$, and obtain a
regret of $\mathcal{O}(\frac{K\log (1/\delta) \log T}{\epsilon^2})$, where $K$
is the size of the arm subset. This result holds for both Gaussian mechanism
and Laplace mechanism by analyses on the composition. Our results extend to
combinatorial semi-bandit. We show respective lower bounds for DP and LDP
cascading bandits. Extensive experiments corroborate our theoretic findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Baoxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1"&gt;Shuo Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feedback Capacity and a Variant of the Kalman Filter with ARMA Gaussian Noises: Explicit Bounds and Feedback Coding Design. (arXiv:2001.03108v6 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03108</id>
        <link href="http://arxiv.org/abs/2001.03108"/>
        <updated>2021-06-07T22:33:05.322Z</updated>
        <summary type="html"><![CDATA[In this paper, we relate a feedback channel with any finite-order
autoregressive moving-average (ARMA) Gaussian noises to a variant of the Kalman
filter. In light of this, we obtain relatively explicit lower bounds on the
feedback capacity for such colored Gaussian noises, and the bounds are seen to
be consistent with various existing results in the literature. Meanwhile, this
variant of the Kalman filter also leads to explicit recursive coding schemes
with clear structures to achieve the lower bounds. In general, our results
provide an alternative perspective while pointing to potentially tighter bounds
for the feedback capacity problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Song Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Quanyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Value Function is All You Need: A Unified Learning Framework for Ride Hailing Platforms. (arXiv:2105.08791v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08791</id>
        <link href="http://arxiv.org/abs/2105.08791"/>
        <updated>2021-06-07T22:33:05.308Z</updated>
        <summary type="html"><![CDATA[Large ride-hailing platforms, such as DiDi, Uber and Lyft, connect tens of
thousands of vehicles in a city to millions of ride demands throughout the day,
providing great promises for improving transportation efficiency through the
tasks of order dispatching and vehicle repositioning. Existing studies,
however, usually consider the two tasks in simplified settings that hardly
address the complex interactions between the two, the real-time fluctuations
between supply and demand, and the necessary coordinations due to the
large-scale nature of the problem. In this paper we propose a unified
value-based dynamic learning framework (V1D3) for tackling both tasks. At the
center of the framework is a globally shared value function that is updated
continuously using online experiences generated from real-time platform
transactions. To improve the sample-efficiency and the robustness, we further
propose a novel periodic ensemble method combining the fast online learning
with a large-scale offline training scheme that leverages the abundant
historical driver trajectory data. This allows the proposed framework to adapt
quickly to the highly dynamic environment, to generalize robustly to recurrent
patterns and to drive implicit coordinations among the population of managed
vehicles. Extensive experiments based on real-world datasets show considerably
improvements over other recently proposed methods on both tasks. Particularly,
V1D3 outperforms the first prize winners of both dispatching and repositioning
tracks in the KDD Cup 2020 RL competition, achieving state-of-the-art results
on improving both total driver income and user experience related metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yansheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dingyuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1"&gt;Bingchen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yongxin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jieping Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Training Approach for Very Large Scale Face Recognition. (arXiv:2105.10375v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10375</id>
        <link href="http://arxiv.org/abs/2105.10375"/>
        <updated>2021-06-07T22:33:05.238Z</updated>
        <summary type="html"><![CDATA[Face recognition has achieved significant progress in deep-learning era due
to the ultra-large-scale and well-labeled datasets.

However, training on ultra-large-scale datasets is time-consuming and takes
up a lot of hardware resource.

Therefore, designing an efficient training approach is crucial and
indispensable.

The heavy computational and memory costs mainly result from the high
dimensionality of the Fully-Connected (FC) layer.

Specifically, the dimensionality is determined by the number of face
identities, which can be million-level or even more.

To this end, we propose a novel training approach for ultra-large-scale face
datasets, termed Faster Face Classification (F$^2$C).

In F$^2$C, we first define a Gallery Net and a Probe Net that are used to
generate identities' centers and extract faces' features for face recognition,
respectively.

Gallery Net has the same structure as Probe Net and inherits the parameters
from Probe Net with a moving average paradigm.

After that, to reduce the training time and hardware costs of the FC layer,
we propose a Dynamic Class Pool (DCP) that stores the features from Gallery Net
and calculates the inner product (logits) with positive samples (whose
identities are in the DCP) in each mini-batch.

DCP can be regarded as a substitute for the FC layer but it is far smaller,
thus greatly reducing the computational and memory costs.

For negative samples (whose identities are not in DCP), we minimize the
cosine similarities between negative samples and those in DCP.

Then, to improve the update efficiency of DCP's parameters, we design a dual
data-loader including identity-based and instance-based loaders to generate a
certain of identities and samples in mini-batches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhipeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaobo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xiaojiang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Baigui Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking. (arXiv:2105.11237v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11237</id>
        <link href="http://arxiv.org/abs/2105.11237"/>
        <updated>2021-06-07T22:33:05.218Z</updated>
        <summary type="html"><![CDATA[Recently, most siamese network based trackers locate targets via object
classification and bounding-box regression. Generally, they select the
bounding-box with maximum classification confidence as the final prediction.
This strategy may miss the right result due to the accuracy misalignment
between classification and regression. In this paper, we propose a novel
siamese tracking algorithm called SiamRCR, addressing this problem with a
simple, light and effective solution. It builds reciprocal links between
classification and regression branches, which can dynamically re-weight their
losses for each positive sample. In addition, we add a localization branch to
predict the localization accuracy, so that it can work as the replacement of
the regression assistance link during inference. This branch makes the training
and inference more consistent. Extensive experimental results demonstrate the
effectiveness of SiamRCR and its superiority over the state-of-the-art
competitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.
Moreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jinlong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengkai Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yueyang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiyao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHD360: A Benchmark Dataset for Salient Human Detection in 360{\deg} Videos. (arXiv:2105.11578v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11578</id>
        <link href="http://arxiv.org/abs/2105.11578"/>
        <updated>2021-06-07T22:33:05.198Z</updated>
        <summary type="html"><![CDATA[Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of
great importance for various applications such as robotics, inter-human and
human-object interaction in augmented reality. However, 360{\deg} video SHD has
been seldom discussed in the computer vision community due to a lack of
datasets with large-scale omnidirectional videos and rich annotations. To this
end, we propose SHD360, the first 360{\deg} video SHD dataset containing
various real-life daily scenes borrowed from this http URL, with
hierarchical annotations for 6,268 key frames uniformly sampled from 37,403
omnidirectional video frames at 4K resolution. Since so far there is no method
proposed for 360{\deg} image/video SHD, we systematically benchmark 11
representative state-of-the-art salient object detection approaches on our
SHD360. We hope our proposed dataset and benchmark could serve as a good
starting point for advancing human-centric researches towards 360{\deg}
panoramic data. Our dataset and benchmark will be publicly available at
https://github.com/PanoAsh/SHD360.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1"&gt;Olivier Deforges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10762</id>
        <link href="http://arxiv.org/abs/2104.10762"/>
        <updated>2021-06-07T22:33:05.164Z</updated>
        <summary type="html"><![CDATA[Random field and random cluster theory is used to prove certain mathematical
results concerning the probability distribution of images characterized as
generic $2D$ integer arrays during simultaneous learning. Example models in
image classification and object segmentation illustrate the mathematical
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1"&gt;Robert A. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness Testing of Language Understanding in Task-Oriented Dialog. (arXiv:2012.15262v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15262</id>
        <link href="http://arxiv.org/abs/2012.15262"/>
        <updated>2021-06-07T22:33:05.098Z</updated>
        <summary type="html"><![CDATA[Most language understanding models in task-oriented dialog systems are
trained on a small amount of annotated training data, and evaluated in a small
set from the same distribution. However, these models can lead to system
failure or undesirable output when being exposed to natural language
perturbation or variation in practice. In this paper, we conduct comprehensive
evaluation and analysis with respect to the robustness of natural language
understanding models, and introduce three important aspects related to language
understanding in real-world dialog systems, namely, language variety, speech
characteristics, and noise perturbation. We propose a model-agnostic toolkit
LAUG to approximate natural language perturbations for testing the robustness
issues in task-oriented dialog. Four data augmentation approaches covering the
three aspects are assembled in LAUG, which reveals critical robustness issues
in state-of-the-art models. The augmented dataset through LAUG can be used to
facilitate future research on the robustness testing of language understanding
in task-oriented dialog.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiexi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1"&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Jiaxin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1"&gt;Dazhen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongguang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1"&gt;Weiran Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sigma-Delta and Distributed Noise-Shaping Quantization Methods for Random Fourier Features. (arXiv:2106.02614v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02614</id>
        <link href="http://arxiv.org/abs/2106.02614"/>
        <updated>2021-06-07T03:06:17.007Z</updated>
        <summary type="html"><![CDATA[We propose the use of low bit-depth Sigma-Delta and distributed noise-shaping
methods for quantizing the Random Fourier features (RFFs) associated with
shift-invariant kernels. We prove that our quantized RFFs -- even in the case
of $1$-bit quantization -- allow a high accuracy approximation of the
underlying kernels, and the approximation error decays at least polynomially
fast as the dimension of the RFFs increases. We also show that the quantized
RFFs can be further compressed, yielding an excellent trade-off between memory
use and accuracy. Namely, the approximation error now decays exponentially as a
function of the bits used. Moreover, we empirically show by testing the
performance of our methods on several machine learning tasks that our method
compares favorably to other state of the art quantization methods in this
context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cloninger_A/0/1/0/all/0/1"&gt;Alexander Cloninger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saab_R/0/1/0/all/0/1"&gt;Rayan Saab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ES-ENAS: Controller-Based Architecture Search for Evolutionary Reinforcement Learning. (arXiv:2101.07415v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07415</id>
        <link href="http://arxiv.org/abs/2101.07415"/>
        <updated>2021-06-07T03:06:17.000Z</updated>
        <summary type="html"><![CDATA[We introduce ES-ENAS, a simple yet general evolutionary joint optimization
procedure by combining continuous optimization via Evolutionary Strategies (ES)
and combinatorial optimization via Efficient NAS (ENAS) in a highly scalable
and intuitive way. Our main insight is noticing that ES is already a highly
distributed algorithm involving hundreds of forward passes which can not only
be used for training neural network weights, but also for jointly training a
NAS controller, both in a blackbox fashion. By doing so, we also bridge the gap
from NAS research in supervised learning settings to the reinforcement learning
scenario through this relatively simple marriage between two different yet
common lines of research. We demonstrate the utility and effectiveness of our
method over a large search space by training highly combinatorial neural
network architectures for RL problems in continuous control, via edge pruning
and quantization. We also incorporate a wide variety of popular techniques from
modern NAS literature including multiobjective optimization along with various
controller methods, to showcase their promise in the RL field and discuss
possible extensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yunhao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Daiyi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1"&gt;Deepali Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wenbo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarlos_T/0/1/0/all/0/1"&gt;Tamas Sarlos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuxiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SmartON: Just-in-Time Active Event Detection on Energy Harvesting Systems. (arXiv:2103.00749v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00749</id>
        <link href="http://arxiv.org/abs/2103.00749"/>
        <updated>2021-06-07T03:06:16.993Z</updated>
        <summary type="html"><![CDATA[We propose SmartON, a batteryless system that learns to wake up proactively
at the right moment in order to detect events of interest. It does so by
adapting the duty cycle to match the distribution of event arrival times under
the constraints of harvested energy. While existing energy harvesting systems
either wake up periodically at a fixed rate to sense and process the data, or
wake up only in accordance with the availability of the energy source, SmartON
employs a three-phase learning framework to learn the energy harvesting pattern
as well as the pattern of events at run-time, and uses that knowledge to wake
itself up when events are most likely to occur. The three-phase learning
framework enables rapid adaptation to environmental changes in both short and
long terms. Being able to remain asleep more often than a CTID
(charging-then-immediate-discharging) wake-up system and adapt to the event
pattern, SmartON is able to reduce energy waste, increase energy efficiency,
and capture more events. To realize SmartON we have developed a dedicated
hardware platform whose power management module activates capacitors on-the-fly
to dynamically increase its storage capacitance. We conduct both
simulation-driven and real-system experiments to demonstrate that SmartON
captures 1X--7X more events and is 8X--17X more energy-efficient than a CTID
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yubo Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nirjon_S/0/1/0/all/0/1"&gt;Shahriar Nirjon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Convergent and Dimension-Independent First-Order Algorithm for Min-Max Optimization. (arXiv:2006.12376v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12376</id>
        <link href="http://arxiv.org/abs/2006.12376"/>
        <updated>2021-06-07T03:06:16.987Z</updated>
        <summary type="html"><![CDATA[Motivated by the recent work of Mangoubi and Vishnoi (STOC 2021), we propose
a variant of the min-max optimization framework where the max-player is
constrained to update the maximization variable in a greedy manner until it
reaches a *first-order* stationary point. We present an algorithm that provably
converges to an approximate local equilibrium for our framework from any
initialization and for nonconvex-nonconcave loss functions. Compared to the
second-order algorithm of Mangoubi and Vishnoi, whose iteration bound is
polynomial in the dimension, our algorithm is first-order and its iteration
bound is independent of dimension. We empirically evaluate our algorithm on
challenging nonconvex-nonconcave test-functions and loss functions that arise
in GAN training. Our algorithm converges on these test functions and, when used
to train GANs on synthetic and real-world datasets, trains stably and avoids
mode collapse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1"&gt;Vijay Keswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangoubi_O/0/1/0/all/0/1"&gt;Oren Mangoubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_S/0/1/0/all/0/1"&gt;Sushant Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1"&gt;Nisheeth K. Vishnoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperparameter Optimization Is Deceiving Us, and How to Stop It. (arXiv:2102.03034v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03034</id>
        <link href="http://arxiv.org/abs/2102.03034"/>
        <updated>2021-06-07T03:06:16.981Z</updated>
        <summary type="html"><![CDATA[Recent empirical work shows that inconsistent results, based on choice of
hyperparameter optimization (HPO) configuration, are a widespread problem in ML
research. When comparing two algorithms J and K, searching one subspace can
yield the conclusion that J outperforms K, whereas searching another can entail
the opposite. In short, the way we choose hyperparameters can deceive us. We
provide a theoretical complement to this prior work, arguing that, to avoid
such deception, the process of drawing conclusions from HPO should be made more
rigorous. We call this process epistemic hyperparameter optimization (EHPO),
and put forth a logical framework to capture its semantics and how it can lead
to inconsistent conclusions about performance. Our framework enables us to
prove EHPO methods that are guaranteed to be defended against deception. We
demonstrate its utility by proving and empirically validating a defended
variant of random search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1"&gt;A. Feder Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yucheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1"&gt;Jessica Zosa Forde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Exploration via Axiomatic Bargaining. (arXiv:2106.02553v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02553</id>
        <link href="http://arxiv.org/abs/2106.02553"/>
        <updated>2021-06-07T03:06:16.974Z</updated>
        <summary type="html"><![CDATA[Motivated by the consideration of fairly sharing the cost of exploration
between multiple groups in learning problems, we develop the Nash bargaining
solution in the context of multi-armed bandits. Specifically, the 'grouped'
bandit associated with any multi-armed bandit problem associates, with each
time step, a single group from some finite set of groups. The utility gained by
a given group under some learning policy is naturally viewed as the reduction
in that group's regret relative to the regret that group would have incurred
'on its own'. We derive policies that yield the Nash bargaining solution
relative to the set of incremental utilities possible under any policy. We show
that on the one hand, the 'price of fairness' under such policies is limited,
while on the other hand, regret optimal policies are arbitrarily unfair under
generic conditions. Our theoretical development is complemented by a case study
on contextual bandits for warfarin dosing where we are concerned with the cost
of exploration across multiple races and age groups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1"&gt;Jackie Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farias_V/0/1/0/all/0/1"&gt;Vivek F. Farias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Influence Estimation and Maximization via Neural Mean-Field Dynamics. (arXiv:2106.02608v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02608</id>
        <link href="http://arxiv.org/abs/2106.02608"/>
        <updated>2021-06-07T03:06:16.956Z</updated>
        <summary type="html"><![CDATA[We propose a novel learning framework using neural mean-field (NMF) dynamics
for inference and estimation problems on heterogeneous diffusion networks. Our
new framework leverages the Mori-Zwanzig formalism to obtain an exact evolution
equation of the individual node infection probabilities, which renders a delay
differential equation with memory integral approximated by learnable time
convolution operators. Directly using information diffusion cascade data, our
framework can simultaneously learn the structure of the diffusion network and
the evolution of node infection probabilities. Connections between parameter
learning and optimal control are also established, leading to a rigorous and
implementable algorithm for training NMF. Moreover, we show that the projected
gradient descent method can be employed to solve the challenging influence
maximization problem, where the gradient is computed extremely fast by
integrating NMF forward in time just once in each iteration. Extensive
empirical studies show that our approach is versatile and robust to variations
of the underlying diffusion network models, and significantly outperform
existing approaches in accuracy and efficiency on both synthetic and real-world
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shushan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaojing Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Graph Models for Retrosynthesis Prediction. (arXiv:2006.07038v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07038</id>
        <link href="http://arxiv.org/abs/2006.07038"/>
        <updated>2021-06-07T03:06:16.949Z</updated>
        <summary type="html"><![CDATA[Retrosynthesis prediction is a fundamental problem in organic synthesis,
where the task is to identify precursor molecules that can be used to
synthesize a target molecule. A key consideration in building neural models for
this task is aligning model design with strategies adopted by chemists.
Building on this viewpoint, this paper introduces a graph-based approach that
capitalizes on the idea that the graph topology of precursor molecules is
largely unaltered during a chemical reaction. The model first predicts the set
of graph edits transforming the target into incomplete molecules called
synthons. Next, the model learns to expand synthons into complete molecules by
attaching relevant leaving groups. This decomposition simplifies the
architecture, making its predictions more interpretable, and also amenable to
manual correction. Our model achieves a top-1 accuracy of $53.7\%$,
outperforming previous template-free and semi-template-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Somnath_V/0/1/0/all/0/1"&gt;Vignesh Ram Somnath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bunne_C/0/1/0/all/0/1"&gt;Charlotte Bunne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coley_C/0/1/0/all/0/1"&gt;Connor W. Coley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Procedural World Generation Framework for Systematic Evaluation of Continual Learning. (arXiv:2106.02585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02585</id>
        <link href="http://arxiv.org/abs/2106.02585"/>
        <updated>2021-06-07T03:06:16.942Z</updated>
        <summary type="html"><![CDATA[Several families of continual learning techniques have been proposed to
alleviate catastrophic interference in deep neural network training on
non-stationary data. However, a comprehensive comparison and analysis of
limitations remains largely open due to the inaccessibility to suitable
datasets. Empirical examination not only varies immensely between individual
works, it further currently relies on contrived composition of benchmarks
through subdivision and concatenation of various prevalent static vision
datasets. In this work, our goal is to bridge this gap by introducing a
computer graphics simulation framework that repeatedly renders only upcoming
urban scene fragments in an endless real-time procedural world generation
process. At its core lies a modular parametric generative model with adaptable
generative factors. The latter can be used to flexibly compose data streams,
which significantly facilitates a detailed analysis and allows for effortless
investigation of various continual learning schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1"&gt;Timm Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1"&gt;Martin Mundt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pliushch_I/0/1/0/all/0/1"&gt;Iuliia Pliushch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1"&gt;Visvanathan Ramesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causally-motivated Shortcut Removal Using Auxiliary Labels. (arXiv:2105.06422v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06422</id>
        <link href="http://arxiv.org/abs/2105.06422"/>
        <updated>2021-06-07T03:06:16.936Z</updated>
        <summary type="html"><![CDATA[Robustness to certain forms of distribution shift is a key concern in many ML
applications. Often, robustness can be formulated as enforcing invariances to
particular interventions on the data generating process. Here, we study a
flexible, causally-motivated approach to enforcing such invariances, paying
special attention to shortcut learning, where a robust predictor can achieve
optimal i.i.d generalization in principle, but instead it relies on spurious
correlations or shortcuts in practice. Our approach uses auxiliary labels,
typically available at training time, to enforce conditional independences
between the latent factors that determine these labels. We show both
theoretically and empirically that causally-motivated regularization schemes
(a) lead to more robust estimators that generalize well under distribution
shift, and (b) have better finite sample efficiency compared to usual
regularization schemes, even in the absence of distribution shifts. Our
analysis highlights important theoretical properties of training techniques
commonly used in causal inference, fairness, and disentanglement literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makar_M/0/1/0/all/0/1"&gt;Maggie Makar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Packer_B/0/1/0/all/0/1"&gt;Ben Packer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moldovan_D/0/1/0/all/0/1"&gt;Dan Moldovan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blalock_D/0/1/0/all/0/1"&gt;Davis Blalock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halpern_Y/0/1/0/all/0/1"&gt;Yoni Halpern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAmour_A/0/1/0/all/0/1"&gt;Alexander D&amp;#x27;Amour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Growing 3D Artefacts and Functional Machines with Neural Cellular Automata. (arXiv:2103.08737v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08737</id>
        <link href="http://arxiv.org/abs/2103.08737"/>
        <updated>2021-06-07T03:06:16.920Z</updated>
        <summary type="html"><![CDATA[Neural Cellular Automata (NCAs) have been proven effective in simulating
morphogenetic processes, the continuous construction of complex structures from
very few starting cells. Recent developments in NCAs lie in the 2D domain,
namely reconstructing target images from a single pixel or infinitely growing
2D textures. In this work, we propose an extension of NCAs to 3D, utilizing 3D
convolutions in the proposed neural network architecture. Minecraft is selected
as the environment for our automaton since it allows the generation of both
static structures and moving machines. We show that despite their simplicity,
NCAs are capable of growing complex entities such as castles, apartment blocks,
and trees, some of which are composed of over 3,000 blocks. Additionally, when
trained for regeneration, the system is able to regrow parts of simple
functional machines, significantly expanding the capabilities of simulated
morphogenetic systems. The code for the experiment in this paper can be found
at: https://github.com/real-itu/3d-artefacts-nca.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1"&gt;Shyam Sudhakaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grbic_D/0/1/0/all/0/1"&gt;Djordje Grbic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Siyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katona_A/0/1/0/all/0/1"&gt;Adam Katona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najarro_E/0/1/0/all/0/1"&gt;Elias Najarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1"&gt;Claire Glanois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1"&gt;Sebastian Risi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do Quadratic Regularizers Prevent Catastrophic Forgetting: The Role of Interpolation. (arXiv:2102.02805v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02805</id>
        <link href="http://arxiv.org/abs/2102.02805"/>
        <updated>2021-06-07T03:06:16.914Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting undermines the effectiveness of deep neural networks
(DNNs) in scenarios such as continual learning and lifelong learning. While
several methods have been proposed to tackle this problem, there is limited
work explaining why these methods work well. This paper has the goal of better
explaining a popularly used technique for avoiding catastrophic forgetting:
quadratic regularization. We show that quadratic regularizers prevent
forgetting of past tasks by interpolating current and previous values of model
parameters at every training iteration. Over multiple training iterations, this
interpolation operation reduces the learning rates of more important model
parameters, thereby minimizing their movement. Our analysis also reveals two
drawbacks of quadratic regularization: (a) dependence of parameter
interpolation on training hyperparameters, which often leads to training
instability and (b) assignment of lower importance to deeper layers, which are
generally the place forgetting occurs in DNNs. Via a simple modification to the
order of operations, we show these drawbacks can be easily avoided, resulting
in 6.2% higher average accuracy at 4.5% lower average forgetting. Code
available at \url{https://github.com/EkdeepSLubana/QRforgetting}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1"&gt;Ekdeep Singh Lubana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_P/0/1/0/all/0/1"&gt;Puja Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1"&gt;Danai Koutra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1"&gt;Robert P. Dick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates. (arXiv:1905.09997v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.09997</id>
        <link href="http://arxiv.org/abs/1905.09997"/>
        <updated>2021-06-07T03:06:16.908Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that stochastic gradient descent (SGD) achieves the
fast convergence rates of full-batch gradient descent for over-parameterized
models satisfying certain interpolation conditions. However, the step-size used
in these works depends on unknown quantities and SGD's practical performance
heavily relies on the choice of this step-size. We propose to use line-search
techniques to automatically set the step-size when training models that can
interpolate the data. In the interpolation setting, we prove that SGD with a
stochastic variant of the classic Armijo line-search attains the deterministic
convergence rates for both convex and strongly-convex functions. Under
additional assumptions, SGD with Armijo line-search is shown to achieve fast
convergence for non-convex functions. Furthermore, we show that stochastic
extra-gradient with a Lipschitz line-search attains linear convergence for an
important class of non-convex functions and saddle-point problems satisfying
interpolation. To improve the proposed methods' practical performance, we give
heuristics to use larger step-sizes and acceleration. We compare the proposed
algorithms against numerous optimization methods on standard classification
tasks using both kernel methods and deep networks. The proposed methods result
in competitive performance across all models and datasets, while being robust
to the precise choices of hyper-parameters. For multi-class classification
using deep networks, SGD with Armijo line-search results in both faster
convergence and better generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaswani_S/0/1/0/all/0/1"&gt;Sharan Vaswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishkin_A/0/1/0/all/0/1"&gt;Aaron Mishkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1"&gt;Issam Laradji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1"&gt;Gauthier Gidel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1"&gt;Simon Lacoste-Julien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Effects of Quantisation on Model Uncertainty in Bayesian Neural Networks. (arXiv:2102.11062v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11062</id>
        <link href="http://arxiv.org/abs/2102.11062"/>
        <updated>2021-06-07T03:06:16.902Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks (BNNs) are making significant progress in many
research areas where decision-making needs to be accompanied by uncertainty
estimation. Being able to quantify uncertainty while making decisions is
essential for understanding when the model is over-/under-confident, and hence
BNNs are attracting interest in safety-critical applications, such as
autonomous driving, healthcare, and robotics. Nevertheless, BNNs have not been
as widely used in industrial practice, mainly because of their increased memory
and compute costs. In this work, we investigate quantisation of BNNs by
compressing 32-bit floating-point weights and activations to their integer
counterparts, that has already been successful in reducing the compute demand
in standard pointwise neural networks. We study three types of quantised BNNs,
we evaluate them under a wide range of different settings, and we empirically
demonstrate that a uniform quantisation scheme applied to BNNs does not
substantially decrease their quality of uncertainty estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1"&gt;Martin Ferianc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_P/0/1/0/all/0/1"&gt;Partha Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1"&gt;Matthew Mattina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1"&gt;Miguel Rodrigues&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collection and harmonization of system logs and prototypal Analytics services with the Elastic (ELK) suite at the INFN-CNAF computing centre. (arXiv:2106.02612v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.02612</id>
        <link href="http://arxiv.org/abs/2106.02612"/>
        <updated>2021-06-07T03:06:16.895Z</updated>
        <summary type="html"><![CDATA[The distributed Grid infrastructure for High Energy Physics experiments at
the Large Hadron Collider (LHC) in Geneva comprises a set of computing centres,
spread all over the world, as part of the Worldwide LHC Computing Grid (WLCG).
In Italy, the Tier-1 functionalities are served by the INFN-CNAF data center,
which provides also computing and storage resources to more than twenty non-LHC
experiments. For this reason, a high amount of logs are collected each day from
various sources, which are highly heterogeneous and difficult to harmonize. In
this contribution, a working implementation of a system that collects, parses
and displays the log information from CNAF data sources and the investigation
of a Machine Learning based predictive maintenance system, is presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diotalevi_T/0/1/0/all/0/1"&gt;Tommaso Diotalevi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falabella_A/0/1/0/all/0/1"&gt;Antonio Falabella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martelli_B/0/1/0/all/0/1"&gt;Barbara Martelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michelotto_D/0/1/0/all/0/1"&gt;Diego Michelotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morganti_L/0/1/0/all/0/1"&gt;Lucia Morganti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonacorsi_D/0/1/0/all/0/1"&gt;Daniele Bonacorsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giommi_L/0/1/0/all/0/1"&gt;Luca Giommi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tisbeni_S/0/1/0/all/0/1"&gt;Simone Rossi Tisbeni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Multi-aspect Detection of Misinformation using Hierarchical Joint Decomposition. (arXiv:2005.04310v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.04310</id>
        <link href="http://arxiv.org/abs/2005.04310"/>
        <updated>2021-06-07T03:06:16.886Z</updated>
        <summary type="html"><![CDATA[Distinguishing between misinformation and real information is one of the most
challenging problems in today's interconnected world. The vast majority of the
state-of-the-art in detecting misinformation is fully supervised, requiring a
large number of high-quality human annotations. However, the availability of
such annotations cannot be taken for granted, since it is very costly,
time-consuming, and challenging to do so in a way that keeps up with the
proliferation of misinformation. In this work, we are interested in exploring
scenarios where the number of annotations is limited. In such scenarios, we
investigate how tapping on a diverse number of resources that characterize a
news article, henceforth referred to as "aspects" can compensate for the lack
of labels. In particular, our contributions in this paper are twofold: 1) We
propose the use of three different aspects: article content, context of social
sharing behaviors, and host website/domain features, and 2) We introduce a
principled tensor based embedding framework that combines all those aspects
effectively. We propose HiJoD a 2-level decomposition pipeline which not only
outperforms state-of-the-art methods with F1-scores of 74% and 81% on Twitter
and Politifact datasets respectively but also is an order of magnitude faster
than similar ensemble approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1"&gt;Sara Abdali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Neil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1"&gt;Evangelos E. Papalexakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental tradeoffs between memorization and robustness in random features and neural tangent regimes. (arXiv:2106.02630v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02630</id>
        <link href="http://arxiv.org/abs/2106.02630"/>
        <updated>2021-06-07T03:06:16.878Z</updated>
        <summary type="html"><![CDATA[This work studies the (non)robustness of two-layer neural networks in various
high-dimensional linearized regimes. We establish fundamental trade-offs
between memorization and robustness, as measured by the Sobolev-seminorm of the
model w.r.t the data distribution, i.e the square root of the average squared
$L_2$-norm of the gradients of the model w.r.t the its input. More precisely,
if $n$ is the number of training examples, $d$ is the input dimension, and $k$
is the number of hidden neurons in a two-layer neural network, we prove for a
large class of activation functions that, if the model memorizes even a
fraction of the training, then its Sobolev-seminorm is lower-bounded by (i)
$\sqrt{n}$ in case of infinite-width random features (RF) or neural tangent
kernel (NTK) with $d \gtrsim n$; (ii) $\sqrt{n}$ in case of finite-width RF
with proportionate scaling of $d$ and $k$; and (iii) $\sqrt{n/k}$ in case of
finite-width NTK with proportionate scaling of $d$ and $k$. Moreover, all of
these lower-bounds are tight: they are attained by the min-norm / least-squares
interpolator (when $n$, $d$, and $k$ are in the appropriate interpolating
regime). All our results hold as soon as data is log-concave isotropic, and
there is label-noise, i.e the target variable is not a deterministic function
of the data / features. We empirically validate our theoretical results with
experiments. Accidentally, these experiments also reveal for the first time,
(iv) a multiple-descent phenomenon in the robustness of the min-norm
interpolator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dohmatob_E/0/1/0/all/0/1"&gt;Elvis Dohmatob&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoubleML -- An Object-Oriented Implementation of Double Machine Learning in R. (arXiv:2103.09603v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09603</id>
        <link href="http://arxiv.org/abs/2103.09603"/>
        <updated>2021-06-07T03:06:16.871Z</updated>
        <summary type="html"><![CDATA[The R package DoubleML implements the double/debiased machine learning
framework of Chernozhukov et al. (2018). It provides functionalities to
estimate parameters in causal models based on machine learning methods. The
double machine learning framework consist of three key ingredients: Neyman
orthogonality, high-quality machine learning estimation and sample splitting.
Estimation of nuisance components can be performed by various state-of-the-art
machine learning methods that are available in the mlr3 ecosystem. DoubleML
makes it possible to perform inference in a variety of causal models, including
partially linear and interactive regression models and their extensions to
instrumental variable estimation. The object-oriented implementation of
DoubleML enables a high flexibility for the model specification and makes it
easily extendable. This paper serves as an introduction to the double machine
learning framework and the R package DoubleML. In reproducible code examples
with simulated and real data sets, we demonstrate how DoubleML users can
perform valid inference based on machine learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bach_P/0/1/0/all/0/1"&gt;Philipp Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1"&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kurz_M/0/1/0/all/0/1"&gt;Malte S. Kurz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spindler_M/0/1/0/all/0/1"&gt;Martin Spindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Precision Reinforcement Learning: Running Soft Actor-Critic in Half Precision. (arXiv:2102.13565v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13565</id>
        <link href="http://arxiv.org/abs/2102.13565"/>
        <updated>2021-06-07T03:06:16.840Z</updated>
        <summary type="html"><![CDATA[Low-precision training has become a popular approach to reduce compute
requirements, memory footprint, and energy consumption in supervised learning.
In contrast, this promising approach has not yet enjoyed similarly widespread
adoption within the reinforcement learning (RL) community, partly because RL
agents can be notoriously hard to train even in full precision. In this paper
we consider continuous control with the state-of-the-art SAC agent and
demonstrate that a na\"ive adaptation of low-precision methods from supervised
learning fails. We propose a set of six modifications, all straightforward to
implement, that leaves the underlying agent and its hyperparameters unchanged
but improves the numerical stability dramatically. The resulting modified SAC
agent has lower memory and compute requirements while matching full-precision
rewards, demonstrating that low-precision training can substantially accelerate
state-of-the-art RL without parameter tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bjorck_J/0/1/0/all/0/1"&gt;Johan Bjorck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1"&gt;Kilian Q. Weinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Way to Overcome the Catastrophic Forgetting in Neural Networks. (arXiv:2005.07107v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07107</id>
        <link href="http://arxiv.org/abs/2005.07107"/>
        <updated>2021-06-07T03:06:16.822Z</updated>
        <summary type="html"><![CDATA[Not so long ago, a method was discovered that successfully overcomes the
catastrophic forgetting in neural networks. Although we know about the cases of
using this method to preserve skills when adapting pre-trained networks to
particular tasks, it has not obtained widespread distribution yet. In this
paper, we would like to propose an alternative method of overcoming
catastrophic forgetting based on the total absolute signal passing through each
connection in the network. This method has a simple implementation and seems to
us essentially close to the processes occurring in the brain of animals to
preserve previously learned skills during subsequent learning. We hope that the
ease of implementation of this method will serve its wide application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kutalev_A/0/1/0/all/0/1"&gt;Alexey Kutalev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance-Guided Modeling of Object Dynamics for Reinforcement Learning. (arXiv:2003.01384v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01384</id>
        <link href="http://arxiv.org/abs/2003.01384"/>
        <updated>2021-06-07T03:06:16.801Z</updated>
        <summary type="html"><![CDATA[Current deep reinforcement learning (RL) approaches incorporate minimal prior
knowledge about the environment, limiting computational and sample efficiency.
\textit{Objects} provide a succinct and causal description of the world, and
many recent works have proposed unsupervised object representation learning
using priors and losses over static object properties like visual consistency.
However, object dynamics and interactions are also critical cues for
objectness. In this paper we propose a framework for reasoning about object
dynamics and behavior to rapidly determine minimal and task-specific object
representations. To demonstrate the need to reason over object behavior and
dynamics, we introduce a suite of RGBD MuJoCo object collection and avoidance
tasks that, while intuitive and visually simple, confound state-of-the-art
unsupervised object representation learning algorithms. We also highlight the
potential of this framework on several Atari games, using our object
representation and standard RL and planning algorithms to learn dramatically
faster than existing deep RL algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agnew_W/0/1/0/all/0/1"&gt;William Agnew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domingos_P/0/1/0/all/0/1"&gt;Pedro Domingos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental Limits of Controlled Stochastic Dynamical Systems: An Information-Theoretic Approach. (arXiv:2012.12174v6 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12174</id>
        <link href="http://arxiv.org/abs/2012.12174"/>
        <updated>2021-06-07T03:06:16.779Z</updated>
        <summary type="html"><![CDATA[In this paper, we examine the fundamental performance limitations in the
control of stochastic dynamical systems; more specifically, we derive generic
$\mathcal{L}_p$ bounds that hold for any causal (stabilizing) controllers and
any stochastic disturbances, by an information-theoretic analysis. We first
consider the scenario where the plant (i.e., the dynamical system to be
controlled) is linear time-invariant, and it is seen in general that the lower
bounds are characterized by the unstable poles (or nonminimum-phase zeros) of
the plant as well as the conditional entropy of the disturbance. We then
analyze the setting where the plant is assumed to be (strictly) causal, for
which case the lower bounds are determined by the conditional entropy of the
disturbance. We also discuss the special cases of $p = 2$ and $p = \infty$,
which correspond to minimum-variance control and controlling the maximum
deviations, respectively. In addition, we investigate the power-spectral
characterization of the lower bounds as well as its relation to the
Kolmogorov-Szeg\"o formula.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fang_S/0/1/0/all/0/1"&gt;Song Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Quanyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MUSBO: Model-based Uncertainty Regularized and Sample Efficient Batch Optimization for Deployment Constrained Reinforcement Learning. (arXiv:2102.11448v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11448</id>
        <link href="http://arxiv.org/abs/2102.11448"/>
        <updated>2021-06-07T03:06:16.770Z</updated>
        <summary type="html"><![CDATA[In many contemporary applications such as healthcare, finance, robotics, and
recommendation systems, continuous deployment of new policies for data
collection and online learning is either cost ineffective or impractical. We
consider a setting that lies between pure offline reinforcement learning (RL)
and pure online RL called deployment constrained RL in which the number of
policy deployments for data sampling is limited. To solve this challenging
task, we propose a new algorithmic learning framework called Model-based
Uncertainty regularized and Sample Efficient Batch Optimization (MUSBO). Our
framework discovers novel and high quality samples for each deployment to
enable efficient data collection. During each offline training session, we
bootstrap the policy update by quantifying the amount of uncertainty within our
collected data. In the high support region (low uncertainty), we encourage our
policy by taking an aggressive update. In the low support region (high
uncertainty) when the policy bootstraps into the out-of-distribution region, we
downweight it by our estimated uncertainty quantification. Experimental results
show that MUSBO achieves state-of-the-art performance in the deployment
constrained RL setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1"&gt;DiJia Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulvey_J/0/1/0/all/0/1"&gt;John M. Mulvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout. (arXiv:2102.13451v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13451</id>
        <link href="http://arxiv.org/abs/2102.13451"/>
        <updated>2021-06-07T03:06:16.751Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has been gaining significant traction across
different ML tasks, ranging from vision to keyboard predictions. In large-scale
deployments, client heterogeneity is a fact, and constitutes a primary problem
for fairness, training performance and accuracy. Although significant efforts
have been made into tackling statistical data heterogeneity, the diversity in
the processing capabilities and network bandwidth of clients, termed as system
heterogeneity, has remained largely unexplored. Current solutions either
disregard a large portion of available devices or set a uniform limit on the
model's capacity, restricted by the least capable participants. In this work,
we introduce Ordered Dropout, a mechanism that achieves an ordered, nested
representation of knowledge in Neural Networks and enables the extraction of
lower footprint submodels without the need of retraining. We further show that
for linear maps our Ordered Dropout is equivalent to SVD. We employ this
technique, along with a self-distillation methodology, in the realm of FL in a
framework called FjORD. FjORD alleviates the problem of client system
heterogeneity by tailoring the model width to the client's capabilities.
Extensive evaluation on both CNNs and RNNs across diverse modalities shows that
FjORD consistently leads to significant performance gains over state-of-the-art
baselines, while maintaining its nested structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1"&gt;Samuel Horvath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskaridis_S/0/1/0/all/0/1"&gt;Stefanos Laskaridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almeida_M/0/1/0/all/0/1"&gt;Mario Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1"&gt;Ilias Leontiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1"&gt;Stylianos I. Venieris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas D. Lane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion. (arXiv:2004.13631v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13631</id>
        <link href="http://arxiv.org/abs/2004.13631"/>
        <updated>2021-06-07T03:06:16.744Z</updated>
        <summary type="html"><![CDATA[A comprehensive knowledge graph (KG) contains an instance-level entity graph
and an ontology-level concept graph. The two-view KG provides a testbed for
models to "simulate" human's abilities on knowledge abstraction,
concretization, and completion (KACC), which are crucial for human to recognize
the world and manage learned knowledge. Existing studies mainly focus on
partial aspects of KACC. In order to promote thorough analyses for KACC
abilities of models, we propose a unified KG benchmark by improving existing
benchmarks in terms of dataset scale, task coverage, and difficulty.
Specifically, we collect new datasets that contain larger concept graphs,
abundant cross-view links as well as dense entity graphs. Based on the
datasets, we propose novel tasks such as multi-hop knowledge abstraction (MKA),
multi-hop knowledge concretization (MKC) and then design a comprehensive
benchmark. For MKA and MKC tasks, we further annotate multi-hop hierarchical
triples as harder samples. The experimental results of existing methods
demonstrate the challenges of our benchmark. The resource is available at
https://github.com/thunlp/KACC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengding Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xin Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning One Representation to Optimize All Rewards. (arXiv:2103.07945v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07945</id>
        <link href="http://arxiv.org/abs/2103.07945"/>
        <updated>2021-06-07T03:06:16.728Z</updated>
        <summary type="html"><![CDATA[We introduce the forward-backward (FB) representation of the dynamics of a
reward-free Markov decision process. It provides explicit near-optimal policies
for any reward specified a posteriori. During an unsupervised phase, we use
reward-free interactions with the environment to learn two representations via
off-the-shelf deep learning methods and temporal difference (TD) learning. In
the test phase, a reward representation is estimated either from observations
or an explicit reward description (e.g., a target state). The optimal policy
for that reward is directly obtained from these representations, with no
planning. We assume access to an exploration scheme or replay buffer for the
first phase.

The unsupervised FB loss is well-principled: if training is perfect, the
policies obtained are provably optimal for any reward function. With imperfect
training, the sub-optimality is proportional to the unsupervised approximation
error. The FB representation learns long-range relationships between states and
actions, via a predictive occupancy map, without having to synthesize states as
in model-based approaches.

This is a step towards learning controllable agents in arbitrary black-box
stochastic environments. This approach compares well to goal-oriented RL
algorithms on discrete and continuous mazes, pixel-based MsPacman, and the
FetchReach virtual robot arm. We also illustrate how the agent can immediately
adapt to new tasks beyond goal-oriented RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Touati_A/0/1/0/all/0/1"&gt;Ahmed Touati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ollivier_Y/0/1/0/all/0/1"&gt;Yann Ollivier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems. (arXiv:2005.12964v9 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12964</id>
        <link href="http://arxiv.org/abs/2005.12964"/>
        <updated>2021-06-07T03:06:16.706Z</updated>
        <summary type="html"><![CDATA[Deep candidate generation (DCG) that narrows down the collection of relevant
items from billions to hundreds via representation learning has become
prevalent in industrial recommender systems. Standard approaches approximate
maximum likelihood estimation (MLE) through sampling for better scalability and
address the problem of DCG in a way similar to language modeling. However, live
recommender systems face severe exposure bias and have a vocabulary several
orders of magnitude larger than that of natural language, implying that MLE
will preserve and even exacerbate the exposure bias in the long run in order to
faithfully fit the observed samples. In this paper, we theoretically prove that
a popular choice of contrastive loss is equivalent to reducing the exposure
bias via inverse propensity weighting, which provides a new perspective for
understanding the effectiveness of contrastive learning. Based on the
theoretical discovery, we design CLRec, a contrastive learning method to
improve DCG in terms of fairness, effectiveness and efficiency in recommender
systems with extremely large candidate size. We further improve upon CLRec and
propose Multi-CLRec, for accurate multi-intention aware bias reduction. Our
methods have been successfully deployed in Taobao, where at least four-month
online A/B tests and offline analyses demonstrate its substantial improvements,
including a dramatic reduction in the Matthew effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianxin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Convex Optimization Perspective for Learning from Dynamically Revealed Preferences. (arXiv:2008.10460v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10460</id>
        <link href="http://arxiv.org/abs/2008.10460"/>
        <updated>2021-06-07T03:06:16.665Z</updated>
        <summary type="html"><![CDATA[We study the problem of online learning (OL) from revealed preferences: a
learner wishes to learn a non-strategic agent's private utility function
through observing the agent's utility-maximizing actions in a changing
environment. We adopt an online inverse optimization setup, where the learner
observes a stream of agent's actions in an online fashion and the learning
performance is measured by regret associated with a loss function. We first
characterize a special but broad class of agent's utility functions, then
utilize this structure in designing a new convex loss function. We establish
that the regret with respect to our new loss function also bounds the regret
with respect to all other usual loss functions in the literature. This allows
us to design a flexible OL framework that enables a unified treatment of loss
functions and supports a variety of online convex optimization algorithms. We
demonstrate with theoretical and empirical evidence that our framework based on
the new loss function (in particular online Mirror Descent) has significant
advantages in terms of regret performance and solution time over other OL
algorithms from the literature and bypasses the previous technical assumptions
as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_V/0/1/0/all/0/1"&gt;Violet Xinying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kilinc_Karzan_F/0/1/0/all/0/1"&gt;Fatma K&amp;#x131;l&amp;#x131;n&amp;#xe7;-Karzan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Covering. (arXiv:2106.02552v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02552</id>
        <link href="http://arxiv.org/abs/2106.02552"/>
        <updated>2021-06-07T03:06:16.658Z</updated>
        <summary type="html"><![CDATA[We analyze the problem of active covering, where the learner is given an
unlabeled dataset and can sequentially label query examples. The objective is
to label query all of the positive examples in the fewest number of total label
queries. We show under standard non-parametric assumptions that a classical
support estimator can be repurposed as an offline algorithm attaining an excess
query cost of $\widetilde{\Theta}(n^{D/(D+1)})$ compared to the optimal
learner, where $n$ is the number of datapoints and $D$ is the dimension. We
then provide a simple active learning method that attains an improved excess
query cost of $\widetilde{O}(n^{(D-1)/D})$. Furthermore, the proposed
algorithms only require access to the positive labeled examples, which in
certain settings provides additional computational and privacy benefits.
Finally, we show that the active learning method consistently outperforms
offline methods as well as a variety of baselines on a wide range of benchmark
image-based datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Heinrich Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1"&gt;Afshin Rostamizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivalence of Dataflow Graphs via Rewrite Rules Using a Graph-to-Sequence Neural Model. (arXiv:2002.06799v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.06799</id>
        <link href="http://arxiv.org/abs/2002.06799"/>
        <updated>2021-06-07T03:06:16.651Z</updated>
        <summary type="html"><![CDATA[In this work we target the problem of provably computing the equivalence
between two programs represented as dataflow graphs. To this end, we formalize
the problem of equivalence between two programs as finding a set of
semantics-preserving rewrite rules from one into the other, such that after the
rewrite the two programs are structurally identical, and therefore trivially
equivalent. We then develop the first graph-to-sequence neural network system
for program equivalence, trained to produce such rewrite sequences from a
carefully crafted automatic example generation algorithm. We extensively
evaluate our system on a rich multi-type linear algebra expression language,
using arbitrary combinations of 100+ graph-rewriting axioms of equivalence. Our
system outputs via inference a correct rewrite sequence for 96% of the 10,000
program pairs isolated for testing, using 30-term programs. And in all cases,
the validity of the sequence produced and therefore the provable assertion of
program equivalence is computable, in negligible time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kommrusch_S/0/1/0/all/0/1"&gt;Steve Kommrusch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barollet_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Barollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouchet_L/0/1/0/all/0/1"&gt;Louis-No&amp;#xeb;l Pouchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02636</id>
        <link href="http://arxiv.org/abs/2106.02636"/>
        <updated>2021-06-07T03:06:16.634Z</updated>
        <summary type="html"><![CDATA[As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).

Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1"&gt;Jack Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jize Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Estimation of Derivatives Using Plug-in KRR Estimators. (arXiv:2006.01350v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01350</id>
        <link href="http://arxiv.org/abs/2006.01350"/>
        <updated>2021-06-07T03:06:16.624Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating the derivatives of the regression
function, which has a wide range of applications as a key nonparametric
functional of unknown functions. Standard analysis may be tailored to specific
derivative orders, and parameter tuning remains a daunting challenge
particularly for high-order derivatives. In this article, we propose a simple
plug-in kernel ridge regression (KRR) estimator in nonparametric regression
with random design that is broadly applicable for multi-dimensional support and
arbitrary mixed-partial derivatives. We provide a non-asymptotic analysis to
study the behavior of the proposed estimator, leading to two error bounds for a
general class of kernels under the strong $L_\infty$ norm. In a concrete
example specialized to kernels with polynomially decaying eigenvalues, the
proposed estimator recovers the minimax optimal rate up to a logarithmic factor
for estimating derivatives of functions in H\"older class. Interestingly, the
proposed estimator achieves the optimal rate of convergence with the same
choice of tuning parameter for any order of derivatives. Hence, the proposed
estimator enjoys a remarkable \textit{plug-in property} for derivatives in that
it automatically adapts to the order of derivatives to be estimated, enabling
easy tuning in practice. Our simulation studies show favorable finite sample
performance of the proposed method relative to several existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zejian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1"&gt;Meng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Aggregation Functions. (arXiv:2012.08482v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08482</id>
        <link href="http://arxiv.org/abs/2012.08482"/>
        <updated>2021-06-07T03:06:16.611Z</updated>
        <summary type="html"><![CDATA[Learning on sets is increasingly gaining attention in the machine learning
community, due to its widespread applicability. Typically, representations over
sets are computed by using fixed aggregation functions such as sum or maximum.
However, recent results showed that universal function representation by sum-
(or max-) decomposition requires either highly discontinuous (and thus poorly
learnable) mappings, or a latent dimension equal to the maximum number of
elements in the set. To mitigate this problem, we introduce a learnable
aggregation function (LAF) for sets of arbitrary cardinality. LAF can
approximate several extensively used aggregators (such as average, sum,
maximum) as well as more complex functions (e.g., variance and skewness). We
report experiments on semi-synthetic and real data showing that LAF outperforms
state-of-the-art sum- (max-) decomposition architectures such as DeepSets and
library-based architectures like Principal Neighborhood Aggregation, and can be
effectively combined with attention-based architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pellegrini_G/0/1/0/all/0/1"&gt;Giovanni Pellegrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tibo_A/0/1/0/all/0/1"&gt;Alessandro Tibo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frasconi_P/0/1/0/all/0/1"&gt;Paolo Frasconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1"&gt;Andrea Passerini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaeger_M/0/1/0/all/0/1"&gt;Manfred Jaeger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Editing Conditional Radiance Fields. (arXiv:2105.06466v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06466</id>
        <link href="http://arxiv.org/abs/2105.06466"/>
        <updated>2021-06-07T03:06:16.515Z</updated>
        <summary type="html"><![CDATA[A neural radiance field (NeRF) is a scene model supporting high-quality view
synthesis, optimized per scene. In this paper, we explore enabling user editing
of a category-level NeRF - also known as a conditional radiance field - trained
on a shape category. Specifically, we introduce a method for propagating coarse
2D user scribbles to the 3D space, to modify the color or shape of a local
region. First, we propose a conditional radiance field that incorporates new
modular network components, including a shape branch that is shared across
object instances. Observing multiple instances of the same category, our model
learns underlying part semantics without any supervision, thereby allowing the
propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
seat). Next, we propose a hybrid network update strategy that targets specific
network components, which balances efficiency and accuracy. During user
interaction, we formulate an optimization problem that both satisfies the
user's constraints and preserves the original object structure. We demonstrate
our approach on various editing tasks over three shape datasets and show that
it outperforms prior neural editing approaches. Finally, we edit the appearance
and shape of a real photograph and show that the edit propagates to
extrapolated novel views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Steven Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Richard Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1"&gt;Bryan Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Importance Weighted Policy Learning and Adaptation. (arXiv:2009.04875v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04875</id>
        <link href="http://arxiv.org/abs/2009.04875"/>
        <updated>2021-06-07T03:06:16.506Z</updated>
        <summary type="html"><![CDATA[The ability to exploit prior experience to solve novel problems rapidly is a
hallmark of biological learning systems and of great practical importance for
artificial ones. In the meta reinforcement learning literature much recent work
has focused on the problem of optimizing the learning process itself. In this
paper we study a complementary approach which is conceptually simple, general,
modular and built on top of recent improvements in off-policy learning. The
framework is inspired by ideas from the probabilistic inference literature and
combines robust off-policy learning with a behavior prior, or default behavior
that constrains the space of solutions and serves as a bias for exploration; as
well as a representation for the value function, both of which are easily
learned from a number of training tasks in a multi-task scenario. Our approach
achieves competitive adaptation performance on hold-out tasks compared to meta
reinforcement learning baselines and can scale to complex sparse-reward
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galashov_A/0/1/0/all/0/1"&gt;Alexandre Galashov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sygnowski_J/0/1/0/all/0/1"&gt;Jakub Sygnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desjardins_G/0/1/0/all/0/1"&gt;Guillaume Desjardins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Humplik_J/0/1/0/all/0/1"&gt;Jan Humplik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasenclever_L/0/1/0/all/0/1"&gt;Leonard Hasenclever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_R/0/1/0/all/0/1"&gt;Rae Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1"&gt;Nicolas Heess&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some observations on high-dimensional partial differential equations with Barron data. (arXiv:2012.01484v3 [math.AP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01484</id>
        <link href="http://arxiv.org/abs/2012.01484"/>
        <updated>2021-06-07T03:06:16.243Z</updated>
        <summary type="html"><![CDATA[We use explicit representation formulas to show that solutions to certain
partial differential equations lie in Barron spaces or multilayer spaces if the
PDE data lie in such function spaces. Consequently, these solutions can be
represented efficiently using artificial neural networks, even in high
dimension. Conversely, we present examples in which the solution fails to lie
in the function space associated to a neural network under consideration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+E_W/0/1/0/all/0/1"&gt;Weinan E&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why does CTC result in peaky behavior?. (arXiv:2105.14849v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14849</id>
        <link href="http://arxiv.org/abs/2105.14849"/>
        <updated>2021-06-07T03:06:16.242Z</updated>
        <summary type="html"><![CDATA[The peaky behavior of CTC models is well known experimentally. However, an
understanding about why peaky behavior occurs is missing, and whether this is a
good property. We provide a formal analysis of the peaky behavior and gradient
descent convergence properties of the CTC loss and related training criteria.
Our analysis provides a deep understanding why peaky behavior occurs and when
it is suboptimal. On a simple example which should be trivial to learn for any
model, we prove that a feed-forward neural network trained with CTC from
uniform initialization converges towards peaky behavior with a 100% error rate.
Our analysis further explains why CTC only works well together with the blank
label. We further demonstrate that peaky behavior does not occur on other
related losses including a label prior model, and that this improves
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1"&gt;Albert Zeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regular Expressions for Fast-response COVID-19 Text Classification. (arXiv:2102.09507v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09507</id>
        <link href="http://arxiv.org/abs/2102.09507"/>
        <updated>2021-06-07T03:06:16.231Z</updated>
        <summary type="html"><![CDATA[Text classifiers are at the core of many NLP applications and use a variety
of algorithmic approaches and software. This paper introduces infrastructure
and methodologies for text classifiers based on large-scale regular
expressions. In particular, we describe how Facebook determines if a given
piece of text - anything from a hashtag to a post - belongs to a narrow topic
such as COVID-19. To fully define a topic and evaluate classifier performance
we employ human-guided iterations of keyword discovery, but do not require
labeled data. For COVID-19, we build two sets of regular expressions: (1) for
66 languages, with 99% precision and recall >50%, (2) for the 11 most common
languages, with precision >90% and recall >90%. Regular expressions enable
low-latency queries from multiple platforms. Response to challenges like
COVID-19 is fast and so are revisions. Comparisons to a DNN classifier show
explainable results, higher precision and recall, and less overfitting. Our
learnings can be applied to other narrow-topic classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Igor L. Markov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jacqueline Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vagner_A/0/1/0/all/0/1"&gt;Adam Vagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-agnostic and Scalable Counterfactual Explanations via Reinforcement Learning. (arXiv:2106.02597v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02597</id>
        <link href="http://arxiv.org/abs/2106.02597"/>
        <updated>2021-06-07T03:06:16.230Z</updated>
        <summary type="html"><![CDATA[Counterfactual instances are a powerful tool to obtain valuable insights into
automated decision processes, describing the necessary minimal changes in the
input space to alter the prediction towards a desired target. Most previous
approaches require a separate, computationally expensive optimization procedure
per instance, making them impractical for both large amounts of data and
high-dimensional data. Moreover, these methods are often restricted to certain
subclasses of machine learning models (e.g. differentiable or tree-based
models). In this work, we propose a deep reinforcement learning approach that
transforms the optimization procedure into an end-to-end learnable process,
allowing us to generate batches of counterfactual instances in a single forward
pass. Our experiments on real-world data show that our method i) is
model-agnostic (does not assume differentiability), relying only on feedback
from model predictions; ii) allows for generating target-conditional
counterfactual instances; iii) allows for flexible feature range constraints
for numerical and categorical attributes, including the immutability of
protected features (e.g. gender, race); iv) is easily extended to other data
modalities such as images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samoilescu_R/0/1/0/all/0/1"&gt;Robert-Florian Samoilescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Looveren_A/0/1/0/all/0/1"&gt;Arnaud Van Looveren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klaise_J/0/1/0/all/0/1"&gt;Janis Klaise&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NF-GNN: Network Flow Graph Neural Networks for Malware Detection and Classification. (arXiv:2103.03939v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03939</id>
        <link href="http://arxiv.org/abs/2103.03939"/>
        <updated>2021-06-07T03:06:16.224Z</updated>
        <summary type="html"><![CDATA[Malicious software (malware) poses an increasing threat to the security of
communication systems as the number of interconnected mobile devices increases
exponentially. While some existing malware detection and classification
approaches successfully leverage network traffic data, they treat network flows
between pairs of endpoints independently and thus fail to leverage rich
communication patterns present in the complete network. Our approach first
extracts flow graphs and subsequently classifies them using a novel edge
feature-based graph neural network model. We present three variants of our base
model, which support malware detection and classification in supervised and
unsupervised settings. We evaluate our approach on flow graphs that we extract
from a recently published dataset for mobile malware detection that addresses
several issues with previously available datasets. Experiments on four
different prediction tasks consistently demonstrate the advantages of our
approach and show that our graph neural network model can boost detection
performance by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Busch_J/0/1/0/all/0/1"&gt;Julian Busch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kocheturov_A/0/1/0/all/0/1"&gt;Anton Kocheturov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidl_T/0/1/0/all/0/1"&gt;Thomas Seidl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Likelihood Training of Score-Based Diffusion Models. (arXiv:2101.09258v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09258</id>
        <link href="http://arxiv.org/abs/2101.09258"/>
        <updated>2021-06-07T03:06:16.211Z</updated>
        <summary type="html"><![CDATA[Score-based diffusion models synthesize samples by reversing a stochastic
process that diffuses data to noise, and are trained by minimizing a weighted
combination of score matching losses. The log-likelihood of score-based models
can be tractably computed through a connection to continuous normalizing flows,
but log-likelihood is not directly optimized by the weighted combination of
score matching losses. We show that for a specific weighting scheme, the
objective upper bounds the negative log-likelihood, thus enabling approximate
maximum likelihood training of score-based models. We empirically observe that
maximum likelihood training consistently improves the likelihood of score-based
models across multiple datasets, stochastic processes, and model architectures.
Our best models achieve negative log-likelihoods of 2.74 and 3.76 bits/dim on
CIFAR-10 and down-sampled ImageNet, outperforming all existing likelihood-based
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durkan_C/0/1/0/all/0/1"&gt;Conor Durkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1"&gt;Iain Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alert Classification for the ALeRCE Broker System: The Real-time Stamp Classifier. (arXiv:2008.03309v2 [astro-ph.IM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03309</id>
        <link href="http://arxiv.org/abs/2008.03309"/>
        <updated>2021-06-07T03:06:16.204Z</updated>
        <summary type="html"><![CDATA[We present a real-time stamp classifier of astronomical events for the ALeRCE
(Automatic Learning for the Rapid Classification of Events) broker. The
classifier is based on a convolutional neural network, trained on alerts
ingested from the Zwicky Transient Facility (ZTF). Using only the
\textit{science, reference} and \textit{difference} images of the first
detection as inputs, along with the metadata of the alert as features, the
classifier is able to correctly classify alerts from active galactic nuclei,
supernovae (SNe), variable stars, asteroids and bogus classes, with high
accuracy ($\sim$94\%) in a balanced test set. In order to find and analyze SN
candidates selected by our classifier from the ZTF alert stream, we designed
and deployed a visualization tool called SN Hunter, where relevant information
about each possible SN is displayed for the experts to choose among candidates
to report to the Transient Name Server database. From June 26th 2019 to
February 28th 2021, we have reported 6846 SN candidates to date (11.8
candidates per day on average), of which 971 have been confirmed
spectroscopically. Our ability to report objects using only a single detection
means that 70\% of the reported SNe occurred within one day after the first
detection. ALeRCE has only reported candidates not otherwise detected or
selected by other groups, therefore adding new early transients to the bulk of
objects available for early follow-up. Our work represents an important
milestone toward rapid alert classifications with the next generation of large
etendue telescopes, such as the Vera C. Rubin Observatory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Carrasco_Davis_R/0/1/0/all/0/1"&gt;Rodrigo Carrasco-Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Reyes_E/0/1/0/all/0/1"&gt;Esteban Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Valenzuela_C/0/1/0/all/0/1"&gt;Camilo Valenzuela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Forster_F/0/1/0/all/0/1"&gt;Francisco F&amp;#xf6;rster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Estevez_P/0/1/0/all/0/1"&gt;Pablo A. Est&amp;#xe9;vez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pignata_G/0/1/0/all/0/1"&gt;Giuliano Pignata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bauer_F/0/1/0/all/0/1"&gt;Franz E. Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Reyes_I/0/1/0/all/0/1"&gt;Ignacio Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sanchez_Saez_P/0/1/0/all/0/1"&gt;Paula S&amp;#xe1;nchez-S&amp;#xe1;ez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Cabrera_Vives_G/0/1/0/all/0/1"&gt;Guillermo Cabrera-Vives&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Eyheramendy_S/0/1/0/all/0/1"&gt;Susana Eyheramendy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Catelan_M/0/1/0/all/0/1"&gt;M&amp;#xe1;rcio Catelan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Arredondo_J/0/1/0/all/0/1"&gt;Javier Arredondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Castillo_Navarrete_E/0/1/0/all/0/1"&gt;Ernesto Castillo-Navarrete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rodriguez_Mancini_D/0/1/0/all/0/1"&gt;Diego Rodr&amp;#xed;guez-Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ruz_Mieres_D/0/1/0/all/0/1"&gt;Daniela Ruz-Mieres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Moya_A/0/1/0/all/0/1"&gt;Alberto Moya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sabatini_Gacitua_L/0/1/0/all/0/1"&gt;Luis Sabatini-Gacit&amp;#xfa;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sepulveda_Cobo_C/0/1/0/all/0/1"&gt;Crist&amp;#xf3;bal Sep&amp;#xfa;lveda-Cobo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Mahabal_A/0/1/0/all/0/1"&gt;Ashish A. Mahabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Silva_Farfan_J/0/1/0/all/0/1"&gt;Javier Silva-Farf&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Camacho_Iniquez_E/0/1/0/all/0/1"&gt;Ernesto Camacho-I&amp;#xf1;iquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Galbany_L/0/1/0/all/0/1"&gt;Llu&amp;#xed;s Galbany&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Democracy: Using Deep Learning to Identify and Prevent Misinformation. (arXiv:2106.02607v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.02607</id>
        <link href="http://arxiv.org/abs/2106.02607"/>
        <updated>2021-06-07T03:06:16.183Z</updated>
        <summary type="html"><![CDATA[The rise in online misinformation in recent years threatens democracies by
distorting authentic public discourse and causing confusion, fear, and even, in
extreme cases, violence. There is a need to understand the spread of false
content through online networks for developing interventions that disrupt
misinformation before it achieves virality. Using a Deep Bidirectional
Transformer for Language Understanding (BERT) and propagation graphs, this
study classifies and visualizes the spread of misinformation on a social media
network using publicly available Twitter data. The results confirm prior
research around user clusters and the virality of false content while improving
the precision of deep learning models for misinformation detection. The study
further demonstrates the suitability of BERT for providing a scalable model for
false information detection, which can contribute to the development of more
timely and accurate interventions to slow the spread of misinformation in
online environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhm_A/0/1/0/all/0/1"&gt;Alyssa Suhm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahankal_P/0/1/0/all/0/1"&gt;Prathamesh Mahankal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuntharaj_S/0/1/0/all/0/1"&gt;Subhiksha Mukuntharaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parab_M/0/1/0/all/0/1"&gt;Meghana D. Parab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_M/0/1/0/all/0/1"&gt;Malvika Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1"&gt;Meredith Berger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethumadhavan_A/0/1/0/all/0/1"&gt;Arathi Sethumadhavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaiman_A/0/1/0/all/0/1"&gt;Ashish Jaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Enhanced Rotation Prediction for Self-Supervised Learning. (arXiv:1912.11603v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.11603</id>
        <link href="http://arxiv.org/abs/1912.11603"/>
        <updated>2021-06-07T03:06:16.176Z</updated>
        <summary type="html"><![CDATA[The rotation prediction (Rotation) is a simple pretext-task for
self-supervised learning (SSL), where models learn useful representations for
target vision tasks by solving pretext-tasks. Although Rotation captures
information of object shapes, it hardly captures information of textures. To
tackle this problem, we introduce a novel pretext-task called image enhanced
rotation prediction (IE-Rot) for SSL. IE-Rot simultaneously solves Rotation and
another pretext-task based on image enhancement (e.g., sharpening and
solarizing) while maintaining simplicity. Through the simultaneous prediction
of rotation and image enhancement, models learn representations to capture the
information of not only object shapes but also textures. Our experimental
results show that IE-Rot models outperform Rotation on various standard
benchmarks including ImageNet classification, PASCAL-VOC detection, and COCO
detection/segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shioda_T/0/1/0/all/0/1"&gt;Tetsuya Shioda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takeda_S/0/1/0/all/0/1"&gt;Shoichiro Takeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilinear Classes: A Structural Framework for Provable Generalization in RL. (arXiv:2103.10897v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10897</id>
        <link href="http://arxiv.org/abs/2103.10897"/>
        <updated>2021-06-07T03:06:16.169Z</updated>
        <summary type="html"><![CDATA[This work introduces Bilinear Classes, a new structural framework, which
permit generalization in reinforcement learning in a wide variety of settings
through the use of function approximation. The framework incorporates nearly
all existing models in which a polynomial sample complexity is achievable, and,
notably, also includes new models, such as the Linear $Q^*/V^*$ model in which
both the optimal $Q$-function and the optimal $V$-function are linear in some
known feature space. Our main result provides an RL algorithm which has
polynomial sample complexity for Bilinear Classes; notably, this sample
complexity is stated in terms of a reduction to the generalization error of an
underlying supervised learning sub-problem. These bounds nearly match the best
known sample complexity bounds for existing models. Furthermore, this framework
also extends to the infinite dimensional (RKHS) setting: for the the Linear
$Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample
complexities that have no explicit dependence on the explicit feature dimension
(which could be infinite), but instead depends only on information theoretic
quantities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lovett_S/0/1/0/all/0/1"&gt;Shachar Lovett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1"&gt;Gaurav Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruosong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental Limits of Prediction, Generalization, and Recursion: An Entropic-Innovations Perspective. (arXiv:2001.03813v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03813</id>
        <link href="http://arxiv.org/abs/2001.03813"/>
        <updated>2021-06-07T03:06:16.162Z</updated>
        <summary type="html"><![CDATA[In this paper, we examine the fundamental performance limits of prediction,
with or without side information. More specifically, we derive generic lower
bounds on the $\mathcal{L}_p$ norms of the prediction errors that are valid for
any prediction algorithms and for any data distributions. Meanwhile, we combine
the entropic analysis from information theory and the innovations approach from
prediction/estimation theory to characterize the conditions (in terms of, e.g.,
directed information or mutual information) to achieve the bounds. We also
investigate the implications of the results in analyzing the fundamental limits
of generalization in fitting (learning) problems from the perspective of
prediction with side information, as well as the fundamental limits of
recursive algorithms by viewing them as generalized prediction problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Song Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Quanyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12056</id>
        <link href="http://arxiv.org/abs/2102.12056"/>
        <updated>2021-06-07T03:06:16.150Z</updated>
        <summary type="html"><![CDATA[Liver segmentation from abdominal CT images is an essential step for liver
cancer computer-aided diagnosis and surgical planning. However, both the
accuracy and robustness of existing liver segmentation methods cannot meet the
requirements of clinical applications. In particular, for the common clinical
cases where the liver tissue contains major pathology, current segmentation
methods show poor performance. In this paper, we propose a novel low-rank
tensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that
achieves accurate and robust pathological liver segmentation of CT images.
Firstly, we propose a multi-slice LRTD scheme to recover the underlying
low-rank structure embedded in 3D medical images. It performs the LRTD on small
image segments consisting of multiple consecutive image slices. Then, we
present an LRTD-based atlas construction method to generate tumor-free liver
atlases that mitigates the performance degradation of liver segmentation due to
the presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to
derive patient-specific liver atlases for each test image, and to achieve
accurate pairwise image registration and label propagation. Extensive
experiments on three public databases of pathological liver cases validate the
effectiveness of the proposed method. Both qualitative and quantitative results
demonstrate that, in the presence of major pathology, the proposed method is
more accurate and robust than state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1"&gt;Min Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiancheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Da Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Difficulty of Unbiased Alpha Divergence Minimization. (arXiv:2010.09541v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09541</id>
        <link href="http://arxiv.org/abs/2010.09541"/>
        <updated>2021-06-07T03:06:16.132Z</updated>
        <summary type="html"><![CDATA[Several approximate inference algorithms have been proposed to minimize an
alpha-divergence between an approximating distribution and a target
distribution. Many of these algorithms introduce bias, the magnitude of which
becomes problematic in high dimensions. Other algorithms are unbiased. These
often seem to suffer from high variance, but little is rigorously known. In
this work we study unbiased methods for alpha-divergence minimization through
the Signal-to-Noise Ratio (SNR) of the gradient estimator. We study several
representative scenarios where strong analytical results are possible, such as
fully-factorized or Gaussian distributions. We find that when alpha is not
zero, the SNR worsens exponentially in the dimensionality of the problem. This
casts doubt on the practicality of these methods. We empirically confirm these
theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Geffner_T/0/1/0/all/0/1"&gt;Tomas Geffner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Domke_J/0/1/0/all/0/1"&gt;Justin Domke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Iterative Graph Matching. (arXiv:2106.02206v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02206</id>
        <link href="http://arxiv.org/abs/2106.02206"/>
        <updated>2021-06-07T03:06:16.126Z</updated>
        <summary type="html"><![CDATA[Recent works leveraging Graph Neural Networks to approach graph matching
tasks have shown promising results. Recent progress in learning discrete
distributions poses new opportunities for learning graph matching models. In
this work, we propose a new model, Stochastic Iterative Graph MAtching (SIGMA),
to address the graph matching problem. Our model defines a distribution of
matchings for a graph pair so the model can explore a wide range of possible
matchings. We further introduce a novel multi-step matching procedure, which
learns how to refine a graph pair's matching results incrementally. The model
also includes dummy nodes so that the model does not have to find matchings for
nodes without correspondence. We fit this model to data via scalable stochastic
optimization. We conduct extensive experiments across synthetic graph datasets
as well as biochemistry and computer vision applications. Across all tasks, our
results show that SIGMA can produce significantly improved graph matching
results compared to state-of-the-art models. Ablation studies verify that each
of our components (stochastic training, iterative matching, and dummy nodes)
offers noticeable improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Linfeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassoun_S/0/1/0/all/0/1"&gt;Soha Hassoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Li-Ping Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02549</id>
        <link href="http://arxiv.org/abs/2106.02549"/>
        <updated>2021-06-07T03:06:16.120Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms are developing into a viable alternative to
convolutional layers as elementary building block of NNs. Their main advantage
is that they are not restricted to capture local dependencies in the input, but
can draw arbitrary connections. This unprecedented capability coincides with
the long-standing problem of modeling global atomic interactions in molecular
force fields and other many-body problems. In its original formulation,
however, attention is not applicable to the continuous domains in which the
atoms live. For this purpose we propose a variant to describe geometric
relations for arbitrary atomic configurations in Euclidean space that also
respects all relevant physical symmetries. We furthermore demonstrate, how the
successive application of our learned attention matrices effectively translates
the molecular geometry into a set of individual atomic contributions
on-the-fly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frank_T/0/1/0/all/0/1"&gt;Thorben Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chmiela_S/0/1/0/all/0/1"&gt;Stefan Chmiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PCA Initialization for Approximate Message Passing in Rotationally Invariant Models. (arXiv:2106.02356v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02356</id>
        <link href="http://arxiv.org/abs/2106.02356"/>
        <updated>2021-06-07T03:06:16.112Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating a rank-$1$ signal in the presence of
rotationally invariant noise-a class of perturbations more general than
Gaussian noise. Principal Component Analysis (PCA) provides a natural
estimator, and sharp results on its performance have been obtained in the
high-dimensional regime. Recently, an Approximate Message Passing (AMP)
algorithm has been proposed as an alternative estimator with the potential to
improve the accuracy of PCA. However, the existing analysis of AMP requires an
initialization that is both correlated with the signal and independent of the
noise, which is often unrealistic in practice. In this work, we combine the two
methods, and propose to initialize AMP with PCA. Our main result is a rigorous
asymptotic characterization of the performance of this estimator. Both the AMP
algorithm and its analysis differ from those previously derived in the Gaussian
setting: at every iteration, our AMP algorithm requires a specific term to
account for PCA initialization, while in the Gaussian case, PCA initialization
affects only the first iteration of AMP. The proof is based on a two-phase
artificial AMP that first approximates the PCA estimator and then mimics the
true AMP. Our numerical simulations show an excellent agreement between AMP
results and theoretical predictions, and suggest an interesting open direction
on achieving Bayes-optimal performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mondelli_M/0/1/0/all/0/1"&gt;Marco Mondelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venkataramanan_R/0/1/0/all/0/1"&gt;Ramji Venkataramanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Streaming Linear System Identification with Reverse Experience Replay. (arXiv:2103.05896v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05896</id>
        <link href="http://arxiv.org/abs/2103.05896"/>
        <updated>2021-06-07T03:06:16.106Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating a linear time-invariant (LTI) dynamical
system from a single trajectory via streaming algorithms, which is encountered
in several applications including reinforcement learning (RL) and time-series
analysis. While the LTI system estimation problem is well-studied in the {\em
offline} setting, the practically important streaming/online setting has
received little attention. Standard streaming methods like stochastic gradient
descent (SGD) are unlikely to work since streaming points can be highly
correlated. In this work, we propose a novel streaming algorithm, SGD with
Reverse Experience Replay ($\mathsf{SGD}-\mathsf{RER}$), that is inspired by
the experience replay (ER) technique popular in the RL literature.
$\mathsf{SGD}-\mathsf{RER}$ divides data into small buffers and runs SGD
backwards on the data stored in the individual buffers. We show that this
algorithm exactly deconstructs the dependency structure and obtains information
theoretically optimal guarantees for both parameter error and prediction error.
Thus, we provide the first -- to the best of our knowledge -- optimal SGD-style
algorithm for the classical problem of linear system identification with a
first order oracle. Furthermore, $\mathsf{SGD}-\mathsf{RER}$ can be applied to
more general settings like sparse LTI identification with known sparsity
pattern, and non-linear dynamical systems. Our work demonstrates that the
knowledge of data dependency structure can aid us in designing statistically
and computationally efficient algorithms which can "decorrelate" streaming
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Suhas S Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraj_D/0/1/0/all/0/1"&gt;Dheeraj Nagaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning phylogenetic trees as hyperbolic point configurations. (arXiv:2104.11430v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11430</id>
        <link href="http://arxiv.org/abs/2104.11430"/>
        <updated>2021-06-07T03:06:16.086Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for the inference of phylogenetic trees that
utilises point configurations on hyperbolic space as its optimisation
landscape. Each taxon corresponds to a point of the point configuration, while
the evolutionary distance between taxa is represented by the geodesic distance
between their corresponding points. The point configuration is iteratively
modified to increase an objective function that additively combines pairwise
log-likelihood terms. After convergence, the final tree is derived from the
inter-point distances using a standard distance-based method. The objective
function, which is shown to mimic the log-likelihood on tree space, is a
differentiable function on a Riemannian manifold. Thus gradient-based
optimisation techniques can be applied, avoiding the need for combinatorial
rearrangements of tree topology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1"&gt;Benjamin Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sobolev Norm Learning Rates for Conditional Mean Embeddings. (arXiv:2105.07446v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07446</id>
        <link href="http://arxiv.org/abs/2105.07446"/>
        <updated>2021-06-07T03:06:16.080Z</updated>
        <summary type="html"><![CDATA[We develop novel learning rates for conditional mean embeddings by applying
the theory of interpolation for reproducing kernel Hilbert spaces (RKHS). We
derive explicit, adaptive convergence rates for the sample estimator under the
misspecifed setting, where the target operator is not Hilbert-Schmidt or
bounded with respect to the input/output RKHSs. We demonstrate that in certain
parameter regimes, we can achieve uniform convergence rates in the output RKHS.
We hope our analyses will allow the much broader application of conditional
mean embeddings to more complex ML/RL settings involving infinite dimensional
RKHSs and continuous state spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Talwai_P/0/1/0/all/0/1"&gt;Prem Talwai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shameli_A/0/1/0/all/0/1"&gt;Ali Shameli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Simchi_Levi_D/0/1/0/all/0/1"&gt;David Simchi-Levi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation formulas and pointwise properties for Barron functions. (arXiv:2006.05982v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05982</id>
        <link href="http://arxiv.org/abs/2006.05982"/>
        <updated>2021-06-07T03:06:16.074Z</updated>
        <summary type="html"><![CDATA[We study the natural function space for infinitely wide two-layer neural
networks with ReLU activation (Barron space) and establish different
representation formulae. In two cases, we describe the space explicitly up to
isomorphism.

Using a convenient representation, we study the pointwise properties of
two-layer networks and show that functions whose singular set is fractal or
curved (for example distance functions from smooth submanifolds) cannot be
represented by infinitely wide two-layer networks with finite path-norm. We use
this structure theorem to show that the only $C^1$-diffeomorphisms which Barron
space are affine.

Furthermore, we show that every Barron function can be decomposed as the sum
of a bounded and a positively one-homogeneous function and that there exist
Barron functions which decay rapidly at infinity and are globally
Lebesgue-integrable. This result suggests that two-layer neural networks may be
able to approximate a greater variety of functions than commonly believed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+E_W/0/1/0/all/0/1"&gt;Weinan E&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Be Considerate: Objectives, Side Effects, and Deciding How to Act. (arXiv:2106.02617v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02617</id>
        <link href="http://arxiv.org/abs/2106.02617"/>
        <updated>2021-06-07T03:06:16.068Z</updated>
        <summary type="html"><![CDATA[Recent work in AI safety has highlighted that in sequential decision making,
objectives are often underspecified or incomplete. This gives discretion to the
acting agent to realize the stated objective in ways that may result in
undesirable outcomes. We contend that to learn to act safely, a reinforcement
learning (RL) agent should include contemplation of the impact of its actions
on the wellbeing and agency of others in the environment, including other
acting agents and reactive processes. We endow RL agents with the ability to
contemplate such impact by augmenting their reward based on expectation of
future return by others in the environment, providing different criteria for
characterizing impact. We further endow these agents with the ability to
differentially factor this impact into their decision making, manifesting
behavior that ranges from self-centred to self-less, as demonstrated by
experiments in gridworld environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alamdari_P/0/1/0/all/0/1"&gt;Parand Alizadeh Alamdari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klassen_T/0/1/0/all/0/1"&gt;Toryn Q. Klassen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Icarte_R/0/1/0/all/0/1"&gt;Rodrigo Toro Icarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1"&gt;Sheila A. McIlraith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators. (arXiv:2106.02205v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02205</id>
        <link href="http://arxiv.org/abs/2106.02205"/>
        <updated>2021-06-07T03:06:16.062Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel pre-trained language models (PLM) compression
approach based on the matrix product operator (short as MPO) from quantum
many-body physics. It can decompose an original matrix into central tensors
(containing the core information) and auxiliary tensors (with only a small
proportion of parameters). With the decomposed MPO structure, we propose a
novel fine-tuning strategy by only updating the parameters from the auxiliary
tensors, and design an optimization algorithm for MPO-based approximation over
stacked network architectures. Our approach can be applied to the original or
the compressed PLMs in a general way, which derives a lighter network and
significantly reduces the parameters to be fine-tuned. Extensive experiments
have demonstrated the effectiveness of the proposed approach in model
compression, especially the reduction in finetuning parameters (91% reduction
on average).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peiyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Ze-Feng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Z.Y. Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhong-Yi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve the Interpretability of Attention: A Fast, Accurate, and Interpretable High-Resolution Attention Model. (arXiv:2106.02566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02566</id>
        <link href="http://arxiv.org/abs/2106.02566"/>
        <updated>2021-06-07T03:06:16.056Z</updated>
        <summary type="html"><![CDATA[The prevalence of employing attention mechanisms has brought along concerns
on the interpretability of attention distributions. Although it provides
insights about how a model is operating, utilizing attention as the explanation
of model predictions is still highly dubious. The community is still seeking
more interpretable strategies for better identifying local active regions that
contribute the most to the final decision. To improve the interpretability of
existing attention models, we propose a novel Bilinear Representative
Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant
human-interpretable information. The target model is first distilled to have
higher-resolution intermediate feature maps. From which, representative
features are then grouped based on local pairwise feature similarity, to
produce finer-grained, more precise attention maps highlighting task-relevant
parts of the input. The obtained attention maps are ranked according to the
`active level' of the compound feature, which provides information regarding
the important level of the highlighted regions. The proposed model can be
easily adapted in a wide variety of modern deep models, where classification is
involved. It is also more accurate, faster, and with a smaller memory footprint
than usual neural attention modules. Extensive experiments showcase more
comprehensive visual explanations compared to the state-of-the-art
visualization model across multiple tasks including few-shot classification,
person re-identification, fine-grained image classification. The proposed
visualization model sheds imperative light on how neural networks `pay their
attention' differently in different tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1"&gt;Tristan Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1"&gt;Suiyi Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1"&gt;Thomas Fr&amp;#xe9;our&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1"&gt;Harold Mouch&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Driven Road Maintenance Inspection. (arXiv:2106.02567v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02567</id>
        <link href="http://arxiv.org/abs/2106.02567"/>
        <updated>2021-06-07T03:06:16.030Z</updated>
        <summary type="html"><![CDATA[Road infrastructure maintenance inspection is typically a labour-intensive
and critical task to ensure the safety of all the road users. In this work, we
propose a detailed methodology to use state-of-the-art techniques in artificial
intelligence and computer vision to automate a sizeable portion of the
maintenance inspection subtasks and reduce the labour costs. The proposed
methodology uses state-of-the-art computer vision techniques such as object
detection and semantic segmentation to automate inspections on primary road
structures such as the road surface, markings, barriers (guardrails) and
traffic signs. The models are mostly trained on commercially viable datasets
and augmented with proprietary data. We demonstrate that our AI models can not
only automate and scale maintenance inspections on primary road structures but
also result in higher recall compared to traditional manual inspections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1"&gt;Ratnajit Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1"&gt;Haris Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1"&gt;Shabbir Marzban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badar_A/0/1/0/all/0/1"&gt;Ahmed Badar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouns_T/0/1/0/all/0/1"&gt;Terence Brouns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1"&gt;Shruthi Gowda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contracting Neural-Newton Solver. (arXiv:2106.02543v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02543</id>
        <link href="http://arxiv.org/abs/2106.02543"/>
        <updated>2021-06-07T03:06:16.024Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have set the focus on neural networks (NNs)
that can successfully replace traditional numerical solvers in many
applications, achieving impressive computing gains. One such application is
time domain simulation, which is indispensable for the design, analysis and
operation of many engineering systems. Simulating dynamical systems with
implicit Newton-based solvers is a computationally heavy task, as it requires
the solution of a parameterized system of differential and algebraic equations
at each time step. A variety of NN-based methodologies have been shown to
successfully approximate the dynamical trajectories computed by numerical time
domain solvers at a fraction of the time. However, so far no previous NN-based
model has explicitly captured the fact that any predicted point on the time
domain trajectory also represents the fixed point of the numerical solver
itself. As we show, explicitly capturing this property can lead to
significantly increased NN accuracy and much smaller NN sizes. In this paper,
we model the Newton solver at the heart of an implicit Runge-Kutta integrator
as a contracting map iteratively seeking this fixed point. Our primary
contribution is to develop a recurrent NN simulation tool, termed the
Contracting Neural-Newton Solver (CoNNS), which explicitly captures the
contracting nature of these Newton iterations. To build CoNNS, we train a
feedforward NN and mimic this contraction behavior by embedding a series of
training constraints which guarantee the mapping provided by the NN satisfies
the Banach fixed-point theorem; thus, we are able to prove that successive
passes through the NN are guaranteed to converge to a unique, fixed point.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chevalier_S/0/1/0/all/0/1"&gt;Samuel Chevalier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiasny_J/0/1/0/all/0/1"&gt;Jochen Stiasny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1"&gt;Spyros Chatzivasileiadis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Deep Learning for Communication Networks: A Survey. (arXiv:2106.02533v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02533</id>
        <link href="http://arxiv.org/abs/2106.02533"/>
        <updated>2021-06-07T03:06:16.018Z</updated>
        <summary type="html"><![CDATA[Communication networks are important infrastructures in contemporary society.
There are still many challenges that are not fully solved and new solutions are
proposed continuously in this active research area. In recent years, to model
the network topology, graph-based deep learning has achieved state-of-the-art
performance in a series of problems in communication networks. In this survey,
we review the rapidly growing body of research using different graph-based deep
learning models, e.g. graph convolutional and graph attention networks, in
various problems from different communication networks, e.g. wireless networks,
wired networks, and software-defined networks. We also present a well-organized
list of the problem and solution for each study and identify future research
directions. To the best of our knowledge, this paper is the first survey that
focuses on the application of graph-based deep learning methods in
communication networks. To track the follow-up research, a public GitHub
repository is created, where the relevant papers will be updated continuously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Weiwei Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning-based synthetic-CT generation in radiotherapy and PET: a review. (arXiv:2102.02734v2 [physics.med-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02734</id>
        <link href="http://arxiv.org/abs/2102.02734"/>
        <updated>2021-06-07T03:06:16.011Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning (DL)-based methods for the generation of synthetic
computed tomography (sCT) have received significant research attention as an
alternative to classical ones. We present here a systematic review of these
methods by grouping them into three categories, according to their clinical
applications: I) To replace CT in magnetic resonance (MR)-based treatment
planning. II) Facilitate cone-beam computed tomography (CBCT)-based
image-guided adaptive radiotherapy. III) Derive attenuation maps for the
correction of positron emission tomography (PET). Appropriate database
searching was performed on journal articles published between January 2014 and
December 2020. The DL methods' key characteristics were extracted from each
eligible study, and a comprehensive comparison among network architectures and
metrics was reported. A detailed review of each category was given,
highlighting essential contributions, identifying specific challenges, and
summarising the achievements. Lastly, the statistics of all the cited works
from various aspects were analysed, revealing the popularity and future trends,
and the potential of DL-based sCT generation. The current status of DL-based
sCT generation was evaluated, assessing the clinical readiness of the presented
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Spadea_M/0/1/0/all/0/1"&gt;Maria Francesca Spadea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Maspero_M/0/1/0/all/0/1"&gt;Matteo Maspero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zaffino_P/0/1/0/all/0/1"&gt;Paolo Zaffino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Seco_J/0/1/0/all/0/1"&gt;Joao Seco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bag of Tricks for Node Classification with Graph Neural Networks. (arXiv:2103.13355v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13355</id>
        <link href="http://arxiv.org/abs/2103.13355"/>
        <updated>2021-06-07T03:06:16.001Z</updated>
        <summary type="html"><![CDATA[Over the past few years, graph neural networks (GNN) and label
propagation-based methods have made significant progress in addressing node
classification tasks on graphs. However, in addition to their reliance on
elaborate architectures and algorithms, there are several key technical details
that are frequently overlooked, and yet nonetheless can play a vital role in
achieving satisfactory performance. In this paper, we first summarize a series
of existing tricks-of-the-trade, and then propose several new ones related to
label usage, loss function formulation, and model design that can significantly
improve various GNN architectures. We empirically evaluate their impact on
final node classification accuracy by conducting ablation studies and
demonstrate consistently-improved performance, often to an extent that
outweighs the gains from more dramatic changes in the underlying GNN
architecture. Notably, many of the top-ranked models on the Open Graph
Benchmark (OGB) leaderboard benefit from our techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yangkun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Jiarui Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weinan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1"&gt;David Wipf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness-Aware Unsupervised Feature Selection. (arXiv:2106.02216v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02216</id>
        <link href="http://arxiv.org/abs/2106.02216"/>
        <updated>2021-06-07T03:06:15.981Z</updated>
        <summary type="html"><![CDATA[Feature selection is a prevalent data preprocessing paradigm for various
learning tasks. Due to the expensive cost of acquiring supervision information,
unsupervised feature selection sparks great interests recently. However,
existing unsupervised feature selection algorithms do not have fairness
considerations and suffer from a high risk of amplifying discrimination by
selecting features that are over associated with protected attributes such as
gender, race, and ethnicity. In this paper, we make an initial investigation of
the fairness-aware unsupervised feature selection problem and develop a
principled framework, which leverages kernel alignment to find a subset of
high-quality features that can best preserve the information in the original
feature space while being minimally correlated with protected attributes.
Specifically, different from the mainstream in-processing debiasing methods,
our proposed framework can be regarded as a model-agnostic debiasing strategy
that eliminates biases and discrimination before downstream learning algorithms
are involved. Experimental results on multiple real-world datasets demonstrate
that our framework achieves a good trade-off between utility maximization and
fairness promotion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1"&gt;Xiaoying Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jundong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence of Gradient Algorithms for Nonconvex C^{1+alpha} Cost Functions. (arXiv:2012.00628v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00628</id>
        <link href="http://arxiv.org/abs/2012.00628"/>
        <updated>2021-06-07T03:06:15.975Z</updated>
        <summary type="html"><![CDATA[This paper is concerned with convergence of stochastic gradient algorithms
with momentum terms in the nonconvex setting. A class of stochastic momentum
methods, including stochastic gradient descent, heavy ball, and Nesterov's
accelerated gradient, is analyzed in a general framework under mild
assumptions. Based on the convergence result of expected gradients, we prove
the almost sure convergence by a detailed discussion of the effects of momentum
and the number of upcrossings. It is worth noting that there are not additional
restrictions imposed on the objective function and stepsize. Another
improvement over previous results is that the existing Lipschitz condition of
the gradient is relaxed into the condition of Holder continuity. As a
byproduct, we apply a localization procedure to extend our results to
stochastic stepsizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zixuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shanjian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring Granger Causality from Irregularly Sampled Time Series. (arXiv:2106.02600v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02600</id>
        <link href="http://arxiv.org/abs/2106.02600"/>
        <updated>2021-06-07T03:06:15.968Z</updated>
        <summary type="html"><![CDATA[Continuous, automated surveillance systems that incorporate machine learning
models are becoming increasingly more common in healthcare environments. These
models can capture temporally dependent changes across multiple patient
variables and can enhance a clinician's situational awareness by providing an
early warning alarm of an impending adverse event such as sepsis. However, most
commonly used methods, e.g., XGBoost, fail to provide an interpretable
mechanism for understanding why a model produced a sepsis alarm at a given
time. The black-box nature of many models is a severe limitation as it prevents
clinicians from independently corroborating those physiologic features that
have contributed to the sepsis alarm. To overcome this limitation, we propose a
generalized linear model (GLM) approach to fit a Granger causal graph based on
the physiology of several major sepsis-associated derangements (SADs). We adopt
a recently developed stochastic monotone variational inequality-based estimator
coupled with forwarding feature selection to learn the graph structure from
both continuous and discrete-valued as well as regularly and irregularly
sampled time series. Most importantly, we develop a non-asymptotic upper bound
on the estimation error for any monotone link function in the GLM. We conduct
real-data experiments and demonstrate that our proposed method can achieve
comparable performance to popular and powerful prediction methods such as
XGBoost while simultaneously maintaining a high level of interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1"&gt;Song Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Josef_C/0/1/0/all/0/1"&gt;Christopher S. Josef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamaleswaran_R/0/1/0/all/0/1"&gt;Rishikesan Kamaleswaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02634</id>
        <link href="http://arxiv.org/abs/2106.02634"/>
        <updated>2021-06-07T03:06:15.958Z</updated>
        <summary type="html"><![CDATA[Inferring representations of 3D scenes from 2D observations is a fundamental
problem of computer graphics, computer vision, and artificial intelligence.
Emerging 3D-structured neural scene representations are a promising approach to
3D scene understanding. In this work, we propose a novel neural scene
representation, Light Field Networks or LFNs, which represent both geometry and
appearance of the underlying 3D scene in a 360-degree, four-dimensional light
field parameterized via a neural implicit representation. Rendering a ray from
an LFN requires only a *single* network evaluation, as opposed to hundreds of
evaluations per ray for ray-marching or volumetric based renderers in
3D-structured neural scene representations. In the setting of simple scenes, we
leverage meta-learning to learn a prior over LFNs that enables multi-view
consistent light field reconstruction from as little as a single image
observation. This results in dramatic reductions in time and memory complexity,
and enables real-time rendering. The cost of storing a 360-degree light field
via an LFN is two orders of magnitude lower than conventional methods such as
the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further
demonstrate the extraction of sparse depth maps from LFNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1"&gt;Semon Rezchikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fredo Durand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing. (arXiv:2106.02559v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02559</id>
        <link href="http://arxiv.org/abs/2106.02559"/>
        <updated>2021-06-07T03:06:15.952Z</updated>
        <summary type="html"><![CDATA[Analysing whether neural language models encode linguistic information has
become popular in NLP. One method of doing so, which is frequently cited to
support the claim that models like BERT encode syntax, is called probing;
probes are small supervised models trained to extract linguistic information
from another model's output. If a probe is able to predict a particular
structure, it is argued that the model whose output it is trained on must have
implicitly learnt to encode it. However, drawing a generalisation about a
model's linguistic knowledge about a specific phenomena based on what a probe
is able to learn may be problematic: in this work, we show that semantic cues
in training data means that syntactic probes do not properly isolate syntax. We
generate a new corpus of semantically nonsensical but syntactically well-formed
Jabberwocky sentences, which we use to evaluate two probes trained on normal
data. We train the probes on several popular language models (BERT, GPT, and
RoBERTa), and find that in all settings they perform worse when evaluated on
these data, for one probe by an average of 15.4 UUAS points absolute. Although
in most cases they still outperform the baselines, their lead is reduced
substantially, e.g. by 53% in the case of BERT for one probe. This begs the
question: what empirical scores constitute knowing syntax?]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maudslay_R/0/1/0/all/0/1"&gt;Rowan Hall Maudslay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViViT: Curvature access through the generalized Gauss-Newton's low-rank structure. (arXiv:2106.02624v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02624</id>
        <link href="http://arxiv.org/abs/2106.02624"/>
        <updated>2021-06-07T03:06:15.929Z</updated>
        <summary type="html"><![CDATA[Curvature in form of the Hessian or its generalized Gauss-Newton (GGN)
approximation is valuable for algorithms that rely on a local model for the
loss to train, compress, or explain deep networks. Existing methods based on
implicit multiplication via automatic differentiation or Kronecker-factored
block diagonal approximations do not consider noise in the mini-batch. We
present ViViT, a curvature model that leverages the GGN's low-rank structure
without further approximations. It allows for efficient computation of
eigenvalues, eigenvectors, as well as per-sample first- and second-order
directional derivatives. The representation is computed in parallel with
gradients in one backward pass and offers a fine-grained cost-accuracy
trade-off, which allows it to scale. As examples for ViViT's usefulness, we
investigate the directional gradients and curvatures during training, and how
noise information can be used to improve the stability of second-order methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dangel_F/0/1/0/all/0/1"&gt;Felix Dangel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tatzel_L/0/1/0/all/0/1"&gt;Lukas Tatzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus Multiplicative Weights Update: Learning to Learn using Projector-based Game Signatures. (arXiv:2106.02615v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.02615</id>
        <link href="http://arxiv.org/abs/2106.02615"/>
        <updated>2021-06-07T03:06:15.922Z</updated>
        <summary type="html"><![CDATA[Recently, Optimistic Multiplicative Weights Update (OMWU) was proven to be
the first constant step-size algorithm in the online no-regret framework to
enjoy last-iterate convergence to Nash Equilibria in the constrained zero-sum
bimatrix case, where weights represent the probabilities of playing pure
strategies. We introduce the second such algorithm, \textit{Consensus MWU}, for
which we prove local convergence and show empirically that it enjoys faster and
more robust convergence than OMWU. Our algorithm shows the importance of a new
object, the \textit{simplex Hessian}, as well as of the interaction of the game
with the (eigen)space of vectors summing to zero, which we believe future
research can build on. As for OMWU, CMWU has convergence guarantees in the
zero-sum case only, but Cheung and Piliouras (2020) recently showed that OMWU
and MWU display opposite convergence properties depending on whether the game
is zero-sum or cooperative. Inspired by this work and the recent literature on
learning to optimize for single functions, we extend CMWU to non zero-sum games
by introducing a new framework for online learning in games, where the update
rule's gradient and Hessian coefficients along a trajectory are learnt by a
reinforcement learning policy that is conditioned on the nature of the game:
\textit{the game signature}. We construct the latter using a new canonical
decomposition of two-player games into eight components corresponding to
commutative projection operators, generalizing and unifying recent game
concepts studied in the literature. We show empirically that our new learning
policy is able to exploit the game signature across a wide range of game types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vadori_N/0/1/0/all/0/1"&gt;Nelson Vadori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savani_R/0/1/0/all/0/1"&gt;Rahul Savani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1"&gt;Thomas Spooner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1"&gt;Sumitra Ganesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation. (arXiv:2004.03238v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03238</id>
        <link href="http://arxiv.org/abs/2004.03238"/>
        <updated>2021-06-07T03:06:15.914Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) models for reading comprehension have achieved
human-level accuracy on in-distribution test sets. However, they have been
demonstrated to lack robustness to challenge sets, whose distribution is
different from that of training sets. Existing data augmentation methods
mitigate this problem by simply augmenting training sets with synthetic
examples sampled from the same distribution as the challenge sets. However,
these methods assume that the distribution of a challenge set is known a
priori, making them less applicable to unseen challenge sets. In this study, we
focus on question-answer pair generation (QAG) to mitigate this problem. While
most existing QAG methods aim to improve the quality of synthetic examples, we
conjecture that diversity-promoting QAG can mitigate the sparsity of training
sets and lead to better robustness. We present a variational QAG model that
generates multiple diverse QA pairs from a paragraph. Our experiments show that
our method can improve the accuracy of 12 challenge sets, as well as the
in-distribution accuracy. Our code and data are available at
https://github.com/KazutoshiShinoda/VQAG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1"&gt;Kazutoshi Shinoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1"&gt;Saku Sugawara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1"&gt;Akiko Aizawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Empirical Objective Functions for MCMC Proposal Optimization. (arXiv:2106.02104v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02104</id>
        <link href="http://arxiv.org/abs/2106.02104"/>
        <updated>2021-06-07T03:06:15.898Z</updated>
        <summary type="html"><![CDATA[We introduce and demonstrate a semi-empirical procedure for determining
approximate objective functions suitable for optimizing arbitrarily
parameterized proposal distributions in MCMC methods. Our proposed Ab Initio
objective functions consist of the weighted combination of functions following
constraints on their global optima and of coordinate invariance that we argue
should be upheld by general measures of MCMC efficiency for use in proposal
optimization. The coefficients of Ab Initio objective functions are determined
so as to recover the optimal MCMC behavior prescribed by established
theoretical analysis for chosen reference problems. Our experimental results
demonstrate that Ab Initio objective functions maintain favorable performance
and preferable optimization behavior compared to existing objective functions
for MCMC optimization when optimizing highly expressive proposal distributions.
We argue that Ab Initio objective functions are sufficiently robust to enable
the confident optimization of MCMC proposal distributions parameterized by deep
generative networks that extend beyond the traditional limitations of
individual MCMC schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cannella_C/0/1/0/all/0/1"&gt;Chris Cannella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1"&gt;Vahid Tarokh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Question Rewriting for Increasing Response Rate. (arXiv:2106.02257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02257</id>
        <link href="http://arxiv.org/abs/2106.02257"/>
        <updated>2021-06-07T03:06:15.889Z</updated>
        <summary type="html"><![CDATA[When a human asks questions online, or when a conversational virtual agent
asks human questions, questions triggering emotions or with details might more
likely to get responses or answers. we explore how to automatically rewrite
natural language questions to improve the response rate from people. In
particular, a new task of Visual Question Rewriting(VQR) task is introduced to
explore how visual information can be used to improve the new questions. A data
set containing around 4K bland questions, attractive questions and images
triples is collected. We developed some baseline sequence to sequence models
and more advanced transformer based models, which take a bland question and a
related image as input and output a rewritten question that is expected to be
more attractive. Offline experiments and mechanical Turk based evaluations show
that it is possible to rewrite bland questions in a more detailed and
attractive way to increase the response rate, and images can be helpful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiayi Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xilian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled change point detection via representation learning. (arXiv:2106.02602v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02602</id>
        <link href="http://arxiv.org/abs/2106.02602"/>
        <updated>2021-06-07T03:06:15.872Z</updated>
        <summary type="html"><![CDATA[Change points are abrupt alterations in the distribution of sequential data.
A change-point detection (CPD) model aims at quick detection of such changes.
Classic approaches perform poorly for semi-structured sequential data because
of the absence of adequate data representation learning. To deal with it, we
introduce a principled differentiable loss function that considers the
specificity of the CPD task. The theoretical results suggest that this function
approximates well classic rigorous solutions. For such loss function, we
propose an end-to-end method for the training of deep representation learning
CPD models. Our experiments provide evidence that the proposed approach
improves baseline results of change point detection for various data types,
including real-world videos and image sequences, and improve representations
for them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Romanenkova_E/0/1/0/all/0/1"&gt;Evgenia Romanenkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1"&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zainulin_R/0/1/0/all/0/1"&gt;Ramil Zainulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morozov_M/0/1/0/all/0/1"&gt;Matvey Morozov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning transition times in event sequences: the Event-Based Hidden Markov Model of disease progression. (arXiv:2011.01023v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01023</id>
        <link href="http://arxiv.org/abs/2011.01023"/>
        <updated>2021-06-07T03:06:15.865Z</updated>
        <summary type="html"><![CDATA[Progressive diseases worsen over time and are characterised by monotonic
change in features that track disease progression. Here we connect ideas from
two formerly separate methodologies -- event-based and hidden Markov modelling
-- to derive a new generative model of disease progression. Our model can
uniquely infer the most likely group-level sequence and timing of events
(natural history) from limited datasets. Moreover, it can infer and predict
individual-level trajectories (prognosis) even when data are missing, giving it
high clinical utility. Here we derive the model and provide an inference scheme
based on the expectation maximisation algorithm. We use clinical, imaging and
biofluid data from the Alzheimer's Disease Neuroimaging Initiative to
demonstrate the validity and utility of our model. First, we train our model to
uncover a new group-level sequence of feature changes in Alzheimer's disease
over a period of ${\sim}17.3$ years. Next, we demonstrate that our model
provides improved utility over a continuous time hidden Markov model by area
under the receiver operator characteristic curve ${\sim}0.23$. Finally, we
demonstrate that our model maintains predictive accuracy with up to $50\%$
missing data. These results support the clinical validity of our model and its
broader utility in resource-limited medical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wijeratne_P/0/1/0/all/0/1"&gt;Peter A. Wijeratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1"&gt;Daniel C. Alexander&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Graph Neural Networks. (arXiv:2102.07835v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07835</id>
        <link href="http://arxiv.org/abs/2102.07835"/>
        <updated>2021-06-07T03:06:15.851Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are a powerful architecture for tackling graph
learning tasks, yet have been shown to be oblivious to eminent substructures,
such as cycles. We present TOGL, a novel layer that incorporates global
topological information of a graph using persistent homology. TOGL can be
easily integrated into any type of GNN and is strictly more expressive in terms
of the Weisfeiler--Lehman test of isomorphism. Augmenting GNNs with our layer
leads to beneficial predictive performance for graph and node classification
tasks, both on synthetic data sets, which can be classified by humans using
their topology but not by ordinary GNNs, and on real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1"&gt;Max Horn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouwer_E/0/1/0/all/0/1"&gt;Edward De Brouwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1"&gt;Michael Moor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_Y/0/1/0/all/0/1"&gt;Yves Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1"&gt;Bastian Rieck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1"&gt;Karsten Borgwardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homological Time Series Analysis of Sensor Signals from Power Plants. (arXiv:2106.02493v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02493</id>
        <link href="http://arxiv.org/abs/2106.02493"/>
        <updated>2021-06-07T03:06:15.840Z</updated>
        <summary type="html"><![CDATA[In this paper, we use topological data analysis techniques to construct a
suitable neural network classifier for the task of learning sensor signals of
entire power plants according to their reference designation system. We use
representations of persistence diagrams to derive necessary preprocessing steps
and visualize the large amounts of data. We derive architectures with deep
one-dimensional convolutional layers combined with stacked long short-term
memories as residual networks suitable for processing the persistence features.
We combine three separate sub-networks, obtaining as input the time series
itself and a representation of the persistent homology for the zeroth and first
dimension. We give a mathematical derivation for most of the used
hyper-parameters. For validation, numerical experiments were performed with
sensor data from four power plants of the same construction type.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melodia_L/0/1/0/all/0/1"&gt;Luciano Melodia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenz_R/0/1/0/all/0/1"&gt;Richard Lenz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Sliced Embedding Discrepancy for Incomparable Distributions. (arXiv:2106.02542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02542</id>
        <link href="http://arxiv.org/abs/2106.02542"/>
        <updated>2021-06-07T03:06:15.828Z</updated>
        <summary type="html"><![CDATA[Gromov-Wasserstein (GW) distance is a key tool for manifold learning and
cross-domain learning, allowing the comparison of distributions that do not
live in the same metric space. Because of its high computational complexity,
several approximate GW distances have been proposed based on entropy
regularization or on slicing, and one-dimensional GW computation. In this
paper, we propose a novel approach for comparing two incomparable
distributions, that hinges on the idea of distributional slicing, embeddings,
and on computing the closed-form Wasserstein distance between the sliced
distributions. We provide a theoretical analysis of this new divergence, called
distributional sliced embedding (DSE) discrepancy, and we show that it
preserves several interesting properties of GW distance including
rotation-invariance. We show that the embeddings involved in DSE can be
efficiently learned. Finally, we provide a large set of experiments
illustrating the behavior of DSE as a divergence in the context of generative
modeling and in query framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alaya_M/0/1/0/all/0/1"&gt;Mokhtar Z. Alaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berar_M/0/1/0/all/0/1"&gt;Maxime Berar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakotomamonjy_A/0/1/0/all/0/1"&gt;Alain Rakotomamonjy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning Guarantees for Online Receding Horizon Learning Control. (arXiv:2010.11327v12 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11327</id>
        <link href="http://arxiv.org/abs/2010.11327"/>
        <updated>2021-06-07T03:06:15.820Z</updated>
        <summary type="html"><![CDATA[In this paper we provide provable regret guarantees for an online
meta-learning receding horizon control algorithm in an iterative control
setting. We consider the setting where, in each iteration the system to be
controlled is a linear deterministic system that is different and unknown, the
cost for the controller in an iteration is a general additive cost function and
there are affine control input constraints. By analysing conditions under which
sub-linear regret is achievable, we prove that the online receding horizon
controller achieves a regret for the controller cost and constraint violation
that are $\tilde{O}(T^{3/4})$ with respect to the best policy that satisfies
the control input control constraints, when the preview of the cost functions
is limited to an interval and the interval size is doubled from one to the
next. We then show that the average of the regret for the controller cost and
constraint violation with respect to the same policy vary as
$\tilde{O}((1+1/\sqrt{N})T^{3/4})$ with the number of iterations $N$, under the
same setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Muthirayan_D/0/1/0/all/0/1"&gt;Deepan Muthirayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khargonekar_P/0/1/0/all/0/1"&gt;Pramod P. Khargonekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable COVID-19 Chest X-Ray Classification via Orthogonality Constraint. (arXiv:2102.08360v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08360</id>
        <link href="http://arxiv.org/abs/2102.08360"/>
        <updated>2021-06-07T03:06:15.801Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have increasingly been used as an auxiliary tool in
healthcare applications, due to their ability to improve performance of several
diagnosis tasks. However, these methods are not widely adopted in clinical
settings due to the practical limitations in the reliability, generalizability,
and interpretability of deep learning based systems. As a result, methods have
been developed that impose additional constraints during network training to
gain more control as well as improve interpretabilty, facilitating their
acceptance in healthcare community. In this work, we investigate the benefit of
using Orthogonal Spheres (OS) constraint for classification of COVID-19 cases
from chest X-ray images. The OS constraint can be written as a simple
orthonormality term which is used in conjunction with the standard
cross-entropy loss during classification network training. Previous studies
have demonstrated significant benefits in applying such constraints to deep
learning models. Our findings corroborate these observations, indicating that
the orthonormality loss function effectively produces improved semantic
localization via GradCAM visualizations, enhanced classification performance,
and reduced model calibration error. Our approach achieves an improvement in
accuracy of 1.6% and 4.8% for two- and three-class classification,
respectively; similar results are found for models with data augmentation
applied. In addition to these findings, our work also presents a new
application of the OS regularizer in healthcare, increasing the post-hoc
interpretability and performance of deep learning models for COVID-19
classification to facilitate adoption of these methods in clinical settings. We
also identify the limitations of our strategy that can be explored for further
research in future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Ella Y. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Som_A/0/1/0/all/0/1"&gt;Anirudh Som&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1"&gt;Ankita Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Hongjun Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1"&gt;Pavan Turaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Holistic Approach to Interpretability in Financial Lending: Models, Visualizations, and Summary-Explanations. (arXiv:2106.02605v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02605</id>
        <link href="http://arxiv.org/abs/2106.02605"/>
        <updated>2021-06-07T03:06:15.797Z</updated>
        <summary type="html"><![CDATA[Lending decisions are usually made with proprietary models that provide
minimally acceptable explanations to users. In a future world without such
secrecy, what decision support tools would one want to use for justified
lending decisions? This question is timely, since the economy has dramatically
shifted due to a pandemic, and a massive number of new loans will be necessary
in the short term. We propose a framework for such decisions, including a
globally interpretable machine learning model, an interactive visualization of
it, and several types of summaries and explanations for any given decision. The
machine learning model is a two-layer additive risk model, which resembles a
two-layer neural network, but is decomposable into subscales. In this model,
each node in the first (hidden) layer represents a meaningful subscale model,
and all of the nonlinearities are transparent. Our online visualization tool
allows exploration of this model, showing precisely how it came to its
conclusion. We provide three types of explanations that are simpler than, but
consistent with, the global model: case-based reasoning explanations that use
neighboring past cases, a set of features that were the most important for the
model's prediction, and summary-explanations that provide a customized sparse
explanation for any particular lending decision made by the model. Our
framework earned the FICO recognition award for the Explainable Machine
Learning Challenge, which was the first public challenge in the domain of
explainable machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaofan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kangcheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1"&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaposhnik_Y/0/1/0/all/0/1"&gt;Yaron Shaposhnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sijia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation Algorithms for Sparse Principal Component Analysis. (arXiv:2006.12748v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12748</id>
        <link href="http://arxiv.org/abs/2006.12748"/>
        <updated>2021-06-07T03:06:15.789Z</updated>
        <summary type="html"><![CDATA[Principal component analysis (PCA) is a widely used dimension reduction
technique in machine learning and multivariate statistics. To improve the
interpretability of PCA, various approaches to obtain sparse principal
direction loadings have been proposed, which are termed Sparse Principal
Component Analysis (SPCA). In this paper, we present thresholding as a provably
accurate, polynomial time, approximation algorithm for the SPCA problem,
without imposing any restrictive assumptions on the input covariance matrix.
Our first thresholding algorithm using the Singular Value Decomposition is
conceptually simple; is faster than current state-of-the-art; and performs well
in practice. On the negative side, our (novel) theoretical bounds do not
accurately predict the strong practical performance of this approach. The
second algorithm solves a well-known semidefinite programming relaxation and
then uses a novel, two step, deterministic thresholding scheme to compute a
sparse principal vector. It works very well in practice and, remarkably, this
solid practical performance is accurately predicted by our theoretical bounds,
which bridge the theory-practice gap better than current state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1"&gt;Agniva Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drineas_P/0/1/0/all/0/1"&gt;Petros Drineas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Samson Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bonsai-Net: One-Shot Neural Architecture Search via Differentiable Pruners. (arXiv:2006.09264v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09264</id>
        <link href="http://arxiv.org/abs/2006.09264"/>
        <updated>2021-06-07T03:06:15.769Z</updated>
        <summary type="html"><![CDATA[One-shot Neural Architecture Search (NAS) aims to minimize the computational
expense of discovering state-of-the-art models. However, in the past year
attention has been drawn to the comparable performance of naive random search
across the same search spaces used by leading NAS algorithms. To address this,
we explore the effects of drastically relaxing the NAS search space, and we
present Bonsai-Net, an efficient one-shot NAS method to explore our relaxed
search space. Bonsai-Net is built around a modified differential pruner and can
consistently discover state-of-the-art architectures that are significantly
better than random search with fewer parameters than other state-of-the-art
methods. Additionally, Bonsai-Net performs simultaneous model search and
training, dramatically reducing the total time it takes to generate
fully-trained models from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geada_R/0/1/0/all/0/1"&gt;Rob Geada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prangle_D/0/1/0/all/0/1"&gt;Dennis Prangle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGough_A/0/1/0/all/0/1"&gt;Andrew Stephen McGough&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex. (arXiv:2106.02331v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02331</id>
        <link href="http://arxiv.org/abs/2106.02331"/>
        <updated>2021-06-07T03:06:15.758Z</updated>
        <summary type="html"><![CDATA[This paper presents a new deep clustering (DC) method called manifold-aware
DC (M-DC) that can enhance hyperspace utilization more effectively than the
original DC. The original DC has a limitation in that a pair of two speakers
has to be embedded having an orthogonal relationship due to its use of the
one-hot vector-based loss function, while our method derives a unique loss
function aimed at maximizing the target angle in the hyperspace based on the
nature of a regular simplex. Our proposed loss imposes a higher penalty than
the original DC when the speaker is assigned incorrectly. The change from DC to
M-DC can be easily achieved by rewriting just one term in the loss function of
DC, without any other modifications to the network architecture or model
parameters. As such, our method has high practicability because it does not
affect the original inference part. The experimental results show that the
proposed method improves the performances of the original DC and its expansion
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tanaka_K/0/1/0/all/0/1"&gt;Keitaro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sawata_R/0/1/0/all/0/1"&gt;Ryosuke Sawata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Takahashi_S/0/1/0/all/0/1"&gt;Shusuke Takahashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Online Mirror Descent. (arXiv:2106.02393v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02393</id>
        <link href="http://arxiv.org/abs/2106.02393"/>
        <updated>2021-06-07T03:06:15.741Z</updated>
        <summary type="html"><![CDATA[We introduce and analyze MT-OMD, a multitask generalization of Online Mirror
Descent (OMD) which operates by sharing updates between tasks. We prove that
the regret of MT-OMD is of order $\sqrt{1 + \sigma^2(N-1)}\sqrt{T}$, where
$\sigma^2$ is the task variance according to the geometry induced by the
regularizer, $N$ is the number of tasks, and $T$ is the time horizon. Whenever
tasks are similar, that is, $\sigma^2 \le 1$, this improves upon the
$\sqrt{NT}$ bound obtained by running independent OMDs on each task. Our
multitask extensions of Online Gradient Descent and Exponentiated Gradient, two
important instances of OMD, are shown to enjoy closed-form updates, making them
easy to use in practice. Finally, we provide numerical experiments on four
real-world datasets which support our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Cesa-Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laforgue_P/0/1/0/all/0/1"&gt;Pierre Laforgue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paudice_A/0/1/0/all/0/1"&gt;Andrea Paudice&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1"&gt;Massimiliano Pontil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovery of Causal Additive Models in the Presence of Unobserved Variables. (arXiv:2106.02234v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02234</id>
        <link href="http://arxiv.org/abs/2106.02234"/>
        <updated>2021-06-07T03:06:15.702Z</updated>
        <summary type="html"><![CDATA[Causal discovery from data affected by unobserved variables is an important
but difficult problem to solve. The effects that unobserved variables have on
the relationships between observed variables are more complex in nonlinear
cases than in linear cases. In this study, we focus on causal additive models
in the presence of unobserved variables. Causal additive models exhibit
structural equations that are additive in the variables and error terms. We
take into account the presence of not only unobserved common causes but also
unobserved intermediate variables. Our theoretical results show that, when the
causal relationships are nonlinear and there are unobserved variables, it is
not possible to identify all the causal relationships between observed
variables through regression and independence tests. However, our theoretical
results also show that it is possible to avoid incorrect inferences. We propose
a method to identify all the causal relationships that are theoretically
possible to identify without being biased by unobserved variables. The
empirical results using artificial data and simulated functional magnetic
resonance imaging (fMRI) data show that our method effectively infers causal
structures in the presence of unobserved variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maeda_T/0/1/0/all/0/1"&gt;Takashi Nicholas Maeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1"&gt;Shohei Shimizu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top-$k$ Regularization for Supervised Feature Selection. (arXiv:2106.02197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02197</id>
        <link href="http://arxiv.org/abs/2106.02197"/>
        <updated>2021-06-07T03:06:15.657Z</updated>
        <summary type="html"><![CDATA[Feature selection identifies subsets of informative features and reduces
dimensions in the original feature space, helping provide insights into data
generation or a variety of domain problems. Existing methods mainly depend on
feature scoring functions or sparse regularizations; nonetheless, they have
limited ability to reconcile the representativeness and inter-correlations of
features. In this paper, we introduce a novel, simple yet effective
regularization approach, named top-$k$ regularization, to supervised feature
selection in regression and classification tasks. Structurally, the top-$k$
regularization induces a sub-architecture on the architecture of a learning
model to boost its ability to select the most informative features and model
complex nonlinear relationships simultaneously. Theoretically, we derive and
mathematically prove a uniform approximation error bound for using this
approach to approximate high-dimensional sparse functions. Extensive
experiments on a wide variety of benchmarking datasets show that the top-$k$
regularization is effective and stable for supervised feature selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xinxing Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1"&gt;Qiang Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Strict Generalisation Benefit for Invariance in Kernel Methods. (arXiv:2106.02346v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02346</id>
        <link href="http://arxiv.org/abs/2106.02346"/>
        <updated>2021-06-07T03:06:15.572Z</updated>
        <summary type="html"><![CDATA[It is a commonly held belief that enforcing invariance improves
generalisation. Although this approach enjoys widespread popularity, it is only
very recently that a rigorous theoretical demonstration of this benefit has
been established. In this work we build on the function space perspective of
Elesedy and Zaidi arXiv:2102.10333 to derive a strictly non-zero generalisation
benefit of incorporating invariance in kernel ridge regression when the target
is invariant to the action of a compact group. We study invariance enforced by
feature averaging and find that generalisation is governed by a notion of
effective dimension that arises from the interplay between the kernel and the
group. In building towards this result, we find that the action of the group
induces an orthogonal decomposition of both the reproducing kernel Hilbert
space and its kernel, which may be of interest in its own right.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Elesedy_B/0/1/0/all/0/1"&gt;Bryn Elesedy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Rates of (Locally) Differentially Private Heavy-tailed Multi-Armed Bandits. (arXiv:2106.02575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02575</id>
        <link href="http://arxiv.org/abs/2106.02575"/>
        <updated>2021-06-07T03:06:15.207Z</updated>
        <summary type="html"><![CDATA[In this paper we study the problem of stochastic multi-armed bandits (MAB) in
the (local) differential privacy (DP/LDP) model. Unlike the previous results
which need to assume bounded reward distributions, here we mainly focus on the
case the reward distribution of each arm only has $(1+v)$-th moment with some
$v\in (0, 1]$. In the first part, we study the problem in the central
$\epsilon$-DP model. We first provide a near-optimal result by developing a
private and robust Upper Confidence Bound (UCB) algorithm. Then, we improve the
result via a private and robust version of the Successive Elimination (SE)
algorithm. Finally, we show that the instance-dependent regret bound of our
improved algorithm is optimal by showing its lower bound. In the second part of
the paper, we study the problem in the $\epsilon$-LDP model. We propose an
algorithm which could be seen as locally private and robust version of the SE
algorithm, and show it could achieve (near) optimal rates for both
instance-dependent and instance-independent regrets. All of the above results
can also reveal the differences between the problem of private MAB with bounded
rewards and heavy-tailed rewards. To achieve these (near) optimal rates, we
develop several new hard instances and private robust estimators as byproducts,
which might could be used to other related problems. Finally, experimental
results also support our theoretical analysis and show the effectiveness of our
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Youming Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yulian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regressive Domain Adaptation for Unsupervised Keypoint Detection. (arXiv:2103.06175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06175</id>
        <link href="http://arxiv.org/abs/2103.06175"/>
        <updated>2021-06-07T03:06:15.201Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) aims at transferring knowledge from a labeled source
domain to an unlabeled target domain. Though many DA theories and algorithms
have been proposed, most of them are tailored into classification settings and
may fail in regression tasks, especially in the practical keypoint detection
task. To tackle this difficult but significant task, we present a method of
regressive domain adaptation (RegDA) for unsupervised keypoint detection.
Inspired by the latest theoretical work, we first utilize an adversarial
regressor to maximize the disparity on the target domain and train a feature
generator to minimize this disparity. However, due to the high dimension of the
output space, this regressor fails to detect samples that deviate from the
support of the source. To overcome this problem, we propose two important
ideas. First, based on our observation that the probability density of the
output space is sparse, we introduce a spatial probability distribution to
describe this sparsity and then use it to guide the learning of the adversarial
regressor. Second, to alleviate the optimization difficulty in the
high-dimensional space, we innovatively convert the minimax game in the
adversarial training to the minimization of two opposite goals. Extensive
experiments show that our method brings large improvement by 8% to 11% in terms
of PCK on different datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junguang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yifei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Ximei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yufeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide Network Learning with Differential Privacy. (arXiv:2103.01294v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01294</id>
        <link href="http://arxiv.org/abs/2103.01294"/>
        <updated>2021-06-07T03:06:15.194Z</updated>
        <summary type="html"><![CDATA[Despite intense interest and considerable effort, the current generation of
neural networks suffers a significant loss of accuracy under most practically
relevant privacy training regimes. One particularly challenging class of neural
networks are the wide ones, such as those deployed for NLP typeahead prediction
or recommender systems. Observing that these models share something in
common--an embedding layer that reduces the dimensionality of the input--we
focus on developing a general approach towards training these models that takes
advantage of the sparsity of the gradients. More abstractly, we address the
problem of differentially private empirical risk minimization (ERM) for models
that admit sparse gradients. We demonstrate that for non-convex ERM problems,
the loss is logarithmically dependent on the number of parameters, in contrast
with polynomial dependence for the general case. Following the same intuition,
we propose a novel algorithm for privately training neural networks. Finally,
we provide an empirical study of a DP wide neural network on a real-world
dataset, which has been rarely explored in the previous work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mironov_I/0/1/0/all/0/1"&gt;Ilya Mironov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hejazinia_M/0/1/0/all/0/1"&gt;Meisam Hejazinia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cybersecurity Threats in Connected and Automated Vehicles based Federated Learning Systems. (arXiv:2102.13256v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13256</id>
        <link href="http://arxiv.org/abs/2102.13256"/>
        <updated>2021-06-07T03:06:15.186Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a machine learning technique that aims at training
an algorithm across decentralized entities holding their local data private.
Wireless mobile networks allow users to communicate with other fixed or mobile
users. The road traffic network represents an infrastructure-based
configuration of a wireless mobile network where the Connected and Automated
Vehicles (CAV) represent the communicating entities. Applying FL in a wireless
mobile network setting gives rise to a new threat in the mobile environment
that is very different from the traditional fixed networks. The threat is due
to the intrinsic characteristics of the wireless medium and is caused by the
characteristics of the vehicular networks such as high node-mobility and
rapidly changing topology. Most cyber defense techniques depend on highly
reliable and connected networks. This paper explores falsified information
attacks, which target the FL process that is ongoing at the RSU. We identified
a number of attack strategies conducted by the malicious CAVs to disrupt the
training of the global model in vehicular networks. We show that the attacks
were able to increase the convergence time and decrease the accuracy the model.
We demonstrate that our attacks bypass FL defense strategies in their primary
form and highlight the need for novel poisoning resilience defense mechanisms
in the wireless mobile setting of the future road networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mallah_R/0/1/0/all/0/1"&gt;Ranwa Al Mallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badu_Marfo_G/0/1/0/all/0/1"&gt;Godwin Badu-Marfo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farooq_B/0/1/0/all/0/1"&gt;Bilal Farooq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedCCEA : A Practical Approach of Client Contribution Evaluation for Federated Learning. (arXiv:2106.02310v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02310</id>
        <link href="http://arxiv.org/abs/2106.02310"/>
        <updated>2021-06-07T03:06:15.169Z</updated>
        <summary type="html"><![CDATA[Client contribution evaluation, also known as data valuation, is a crucial
approach in federated learning(FL) for client selection and incentive
allocation. However, due to restrictions of accessibility of raw data, only
limited information such as local weights and local data size of each client is
open for quantifying the client contribution. Using data size from available
information, we introduce an empirical evaluation method called Federated
Client Contribution Evaluation through Accuracy Approximation(FedCCEA). This
method builds the Accuracy Approximation Model(AAM), which estimates a
simulated test accuracy using inputs of sampled data size and extracts the
clients' data quality and data size to measure client contribution. FedCCEA
strengthens some advantages: (1) enablement of data size selection to the
clients, (2) feasible evaluation time regardless of the number of clients, and
(3) precise estimation in non-IID settings. We demonstrate the superiority of
FedCCEA compared to previous methods through several experiments: client
contribution distribution, client removal, and robustness test to partial
participation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shyn_S/0/1/0/all/0/1"&gt;Sung Kuk Shyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Donghee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kwangsu Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Surrogate Models for Absorptivity and Emissivity Spectra of Multiple Elements. (arXiv:2106.02528v1 [physics.plasm-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02528</id>
        <link href="http://arxiv.org/abs/2106.02528"/>
        <updated>2021-06-07T03:06:15.163Z</updated>
        <summary type="html"><![CDATA[Simulations of high energy density physics are expensive in terms of
computational resources. In particular, the computation of opacities of
plasmas, which are needed to accurately compute radiation transport in the
non-local thermal equilibrium (NLTE) regime, are expensive to the point of
easily requiring multiple times the sum-total compute time of all other
components of the simulation. As such, there is great interest in finding ways
to accelerate NLTE computations. Previous work has demonstrated that a
combination of fully-connected autoencoders and a deep jointly-informed neural
network (DJINN) can successfully replace the standard NLTE calculations for the
opacity of krypton. This work expands this idea to multiple elements in
demonstrating that individual surrogate models can be also be generated for
other elements with the focus being on creating autoencoders that can
accurately encode and decode the absorptivity and emissivity spectra.
Furthermore, this work shows that multiple elements across a large range of
atomic numbers can be combined into a single autoencoder when using a
convolutional autoencoder while maintaining accuracy that is comparable to
individual fully-connected autoencoders. Lastly, it is demonstrated that DJINN
can effectively learn the latent space of a convolutional autoencoder that can
encode multiple elements allowing the combination to effectively function as a
surrogate model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Wal_M/0/1/0/all/0/1"&gt;Michael D. Vander Wal&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+McClarren_R/0/1/0/all/0/1"&gt;Ryan G. McClarren&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+Humbird_K/0/1/0/all/0/1"&gt;Kelli D. Humbird&lt;/a&gt; (2) ((1) University of Notre Dame, (2) Lawrence Livermore National Laboratory)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of the robustness of NMF algorithms. (arXiv:2106.02213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02213</id>
        <link href="http://arxiv.org/abs/2106.02213"/>
        <updated>2021-06-07T03:06:15.157Z</updated>
        <summary type="html"><![CDATA[We examine three non-negative matrix factorization techniques; L2-norm,
L1-norm, and L2,1-norm. Our aim is to establish the performance of these
different approaches, and their robustness in real-world applications such as
feature selection while managing computational complexity, sensitivity to noise
and more. We thoroughly examine each approach from a theoretical perspective,
and examine the performance of each using a series of experiments drawing on
both the ORL and YaleB datasets. We examine the Relative Reconstruction Errors
(RRE), Average Accuracy and Normalized Mutual Information (NMI) as criteria
under a range of simulated noise scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_A/0/1/0/all/0/1"&gt;Alex D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steele_D/0/1/0/all/0/1"&gt;Damian Steele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Local Model-Agnostic Explanations Using Ground Truth. (arXiv:2106.02488v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02488</id>
        <link href="http://arxiv.org/abs/2106.02488"/>
        <updated>2021-06-07T03:06:15.152Z</updated>
        <summary type="html"><![CDATA[Explanation techniques are commonly evaluated using human-grounded methods,
limiting the possibilities for large-scale evaluations and rapid progress in
the development of new techniques. We propose a functionally-grounded
evaluation procedure for local model-agnostic explanation techniques. In our
approach, we generate ground truth for explanations when the black-box model is
Logistic Regression and Gaussian Naive Bayes and compare how similar each
explanation is to the extracted ground truth. In our empirical study,
explanations of Local Interpretable Model-agnostic Explanations (LIME), SHapley
Additive exPlanations (SHAP), and Local Permutation Importance (LPI) are
compared in terms of how similar they are to the extracted ground truth. In the
case of Logistic Regression, we find that the performance of the explanation
techniques is highly dependent on the normalization of the data. In contrast,
Local Permutation Importance outperforms the other techniques on Naive Bayes,
irrespective of normalization. We hope that this work lays the foundation for
further research into functionally-grounded evaluation methods for explanation
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahnama_A/0/1/0/all/0/1"&gt;Amir Hossein Akhavan Rahnama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Butepage_J/0/1/0/all/0/1"&gt;Judith Butepage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geurts_P/0/1/0/all/0/1"&gt;Pierre Geurts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bostrom_H/0/1/0/all/0/1"&gt;Henrik Bostrom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval. (arXiv:2106.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02400</id>
        <link href="http://arxiv.org/abs/2106.02400"/>
        <updated>2021-06-07T03:06:15.145Z</updated>
        <summary type="html"><![CDATA[Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph's nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Manh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurrin_C/0/1/0/all/0/1"&gt;Cathal Gurrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Event Classification with Multi-step Machine Learning. (arXiv:2106.02301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02301</id>
        <link href="http://arxiv.org/abs/2106.02301"/>
        <updated>2021-06-07T03:06:15.138Z</updated>
        <summary type="html"><![CDATA[The usefulness and value of Multi-step Machine Learning (ML), where a task is
organized into connected sub-tasks with known intermediate inference goals, as
opposed to a single large model learned end-to-end without intermediate
sub-tasks, is presented. Pre-optimized ML models are connected and better
performance is obtained by re-optimizing the connected one. The selection of an
ML model from several small ML model candidates for each sub-task has been
performed by using the idea based on Neural Architecture Search (NAS). In this
paper, Differentiable Architecture Search (DARTS) and Single Path One-Shot NAS
(SPOS-NAS) are tested, where the construction of loss functions is improved to
keep all ML models smoothly learning. Using DARTS and SPOS-NAS as an
optimization and selection as well as the connections for multi-step machine
learning systems, we find that (1) such a system can quickly and successfully
select highly performant model combinations, and (2) the selected models are
consistent with baseline algorithms, such as grid search, and their outputs are
well controlled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saito_M/0/1/0/all/0/1"&gt;Masahiko Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kishimoto_T/0/1/0/all/0/1"&gt;Tomoe Kishimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaneta_Y/0/1/0/all/0/1"&gt;Yuya Kaneta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itoh_T/0/1/0/all/0/1"&gt;Taichi Itoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umeda_Y/0/1/0/all/0/1"&gt;Yoshiaki Umeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_J/0/1/0/all/0/1"&gt;Junichi Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iiyama_Y/0/1/0/all/0/1"&gt;Yutaro Iiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sawada_R/0/1/0/all/0/1"&gt;Ryu Sawada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terashi_K/0/1/0/all/0/1"&gt;Koji Terashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Equal Gender Representation in the Annotations of Toxic Language Detection. (arXiv:2106.02183v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02183</id>
        <link href="http://arxiv.org/abs/2106.02183"/>
        <updated>2021-06-07T03:06:15.120Z</updated>
        <summary type="html"><![CDATA[Classifiers tend to propagate biases present in the data on which they are
trained. Hence, it is important to understand how the demographic identities of
the annotators of comments affect the fairness of the resulting model. In this
paper, we focus on the differences in the ways men and women annotate comments
for toxicity, investigating how these differences result in models that amplify
the opinions of male annotators. We find that the BERT model as-sociates toxic
comments containing offensive words with male annotators, causing the model to
predict 67.7% of toxic comments as having been annotated by men. We show that
this disparity between gender predictions can be mitigated by removing
offensive words and highly toxic comments from the training data. We then apply
the learned associations between gender and language to toxic language
classifiers, finding that models trained exclusively on female-annotated data
perform 1.8% better than those trained solely on male-annotated data and that
training models on data after removing all offensive words reduces bias in the
model by 55.5% while increasing the sensitivity by 0.4%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Excell_E/0/1/0/all/0/1"&gt;Elizabeth Excell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1"&gt;Noura Al Moubayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path. (arXiv:2106.02073v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02073</id>
        <link href="http://arxiv.org/abs/2106.02073"/>
        <updated>2021-06-07T03:06:15.112Z</updated>
        <summary type="html"><![CDATA[Recent work [Papyan, Han, and Donoho, 2020] discovered a phenomenon called
Neural Collapse (NC) that occurs pervasively in today's deep net training
paradigm of driving cross-entropy loss towards zero. In this phenomenon, the
last-layer features collapse to their class-means, both the classifiers and
class-means collapse to the same Simplex Equiangular Tight Frame (ETF), and the
behavior of the last-layer classifier converges to that of the
nearest-class-mean decision rule. Since then, follow-ups-such as Mixon et al.
[2020] and Poggio and Liao [2020a,b]-formally analyzed this inductive bias by
replacing the hard-to-study cross-entropy by the more tractable mean squared
error (MSE) loss. But, these works stopped short of demonstrating the empirical
reality of MSE-NC on benchmark datasets and canonical networks-as had been done
in Papyan, Han, and Donoho [2020] for the cross-entropy loss. In this work, we
establish the empirical reality of MSE-NC by reporting experimental
observations for three prototypical networks and five canonical datasets with
code for reproducing NC. Following this, we develop three main contributions
inspired by MSE-NC. Firstly, we show a new theoretical decomposition of the MSE
loss into (A) a term assuming the last-layer classifier is exactly the
least-squares or Webb and Lowe [1990] classifier and (B) a term capturing the
deviation from this least-squares classifier. Secondly, we exhibit experiments
on canonical datasets and networks demonstrating that, during training,
term-(B) is negligible. This motivates a new theoretical construct: the central
path, where the linear classifier stays MSE-optimal-for the given feature
activations-throughout the dynamics. Finally, through our study of continually
renormalized gradient flow along the central path, we produce closed-form
dynamics that predict full Neural Collapse in an unconstrained features model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;X.Y. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papyan_V/0/1/0/all/0/1"&gt;Vardan Papyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donoho_D/0/1/0/all/0/1"&gt;David L. Donoho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can convolutional ResNets approximately preserve input distances? A frequency analysis perspective. (arXiv:2106.02469v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02469</id>
        <link href="http://arxiv.org/abs/2106.02469"/>
        <updated>2021-06-07T03:06:15.106Z</updated>
        <summary type="html"><![CDATA[ResNets constrained to be bi-Lipschitz, that is, approximately distance
preserving, have been a crucial component of recently proposed techniques for
deterministic uncertainty quantification in neural models. We show that
theoretical justifications for recent regularisation schemes trying to enforce
such a constraint suffer from a crucial flaw -- the theoretical link between
the regularisation scheme used and bi-Lipschitzness is only valid under
conditions which do not hold in practice, rendering existing theory of limited
use, despite the strong empirical performance of these models. We provide a
theoretical explanation for the effectiveness of these regularisation schemes
using a frequency analysis perspective, showing that under mild conditions
these schemes will enforce a lower Lipschitz bound on the low-frequency
projection of images. We then provide empirical evidence supporting our
theoretical claims, and perform further experiments which demonstrate that our
broader conclusions appear to hold when some of the mathematical assumptions of
our proof are relaxed, corresponding to the setup used in prior work. In
addition, we present a simple constructive algorithm to search for counter
examples to the distance preservation condition, and discuss possible
implications of our theory for future model design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1"&gt;Lewis Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amersfoort_J/0/1/0/all/0/1"&gt;Joost van Amersfoort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haiwen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent Transportation Systems to Mitigate Road Traffic Congestion. (arXiv:2106.02315v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02315</id>
        <link href="http://arxiv.org/abs/2106.02315"/>
        <updated>2021-06-07T03:06:15.100Z</updated>
        <summary type="html"><![CDATA[Intelligent transport systems have efficiently and effectively proved
themselves in settling up the problem of traffic congestion around the world.
The multi-agent based transportation system is one of the most important
intelligent transport systems, which represents an interaction among the
neighbouring vehicles, drivers, roads, infrastructure and vehicles. In this
paper, two traffic management models have been created to mitigate congestion
and to ensure that emergency vehicles arrive as quickly as possible. A
tool-chain SUMO-JADE is employed to create a microscopic simulation symbolizing
the interactions of traffic. The simulation model has showed a significant
reduction of at least 50% in the average time delay and thus a real improvement
in the entire journey time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hamadeh_N/0/1/0/all/0/1"&gt;Nizar Hamadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karouni_A/0/1/0/all/0/1"&gt;Ali Karouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Farhat_Z/0/1/0/all/0/1"&gt;Zeinab Farhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghor_H/0/1/0/all/0/1"&gt;Hussein El Ghor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghor_M/0/1/0/all/0/1"&gt;Mohamad El Ghor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katea_I/0/1/0/all/0/1"&gt;Israa Katea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Visual Exploration of Relations in Data Sets. (arXiv:1905.02515v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.02515</id>
        <link href="http://arxiv.org/abs/1905.02515"/>
        <updated>2021-06-07T03:06:15.084Z</updated>
        <summary type="html"><![CDATA[Efficient explorative data analysis systems must take into account both what
a user knows and wants to know. This paper proposes a principled framework for
interactive visual exploration of relations in data, through views most
informative given the user's current knowledge and objectives. The user can
input pre-existing knowledge of relations in the data and also formulate
specific exploration interests, which are then taken into account in the
exploration. The idea is to steer the exploration process towards the interests
of the user, instead of showing uninteresting or already known relations. The
user's knowledge is modelled by a distribution over data sets parametrised by
subsets of rows and columns of data, called tile constraints. We provide a
computationally efficient implementation of this concept based on constrained
randomisation. Furthermore, we describe a novel dimensionality reduction method
for finding the views most informative to the user, which at the limit of no
background knowledge and with generic objectives reduces to PCA. We show that
the method is suitable for interactive use and is robust to noise, outperforms
standard projection pursuit visualisation methods, and gives understandable and
useful results in analysis of real-world data. We provide an open-source
implementation of the framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Puolamaki_K/0/1/0/all/0/1"&gt;Kai Puolam&amp;#xe4;ki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oikarinen_E/0/1/0/all/0/1"&gt;Emilia Oikarinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Henelius_A/0/1/0/all/0/1"&gt;Andreas Henelius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tractable Regularization of Probabilistic Circuits. (arXiv:2106.02264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02264</id>
        <link href="http://arxiv.org/abs/2106.02264"/>
        <updated>2021-06-07T03:06:15.078Z</updated>
        <summary type="html"><![CDATA[Probabilistic Circuits (PCs) are a promising avenue for probabilistic
modeling. They combine advantages of probabilistic graphical models (PGMs) with
those of neural networks (NNs). Crucially, however, they are tractable
probabilistic models, supporting efficient and exact computation of many
probabilistic inference queries, such as marginals and MAP. Further, since PCs
are structured computation graphs, they can take advantage of
deep-learning-style parameter updates, which greatly improves their
scalability. However, this innovation also makes PCs prone to overfitting,
which has been observed in many standard benchmarks. Despite the existence of
abundant regularization techniques for both PGMs and NNs, they are not
effective enough when applied to PCs. Instead, we re-think regularization for
PCs and propose two intuitive techniques, data softening and entropy
regularization, that both take advantage of PCs' tractability and still have an
efficient implementation as a computation graph. Specifically, data softening
provides a principled way to add uncertainty in datasets in closed form, which
implicitly regularizes PC parameters. To learn parameters from a softened
dataset, PCs only need linear time by virtue of their tractability. In entropy
regularization, the exact entropy of the distribution encoded by a PC can be
regularized directly, which is again infeasible for most other density
estimation models. We show that both methods consistently improve the
generalization performance of a wide variety of PCs. Moreover, when paired with
a simple PC structure, we achieved state-of-the-art results on 10 out of 20
standard discrete density estimation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Anji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1"&gt;Guy Van den Broeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Domain Adaptation for LiDAR Perception. (arXiv:2106.02377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02377</id>
        <link href="http://arxiv.org/abs/2106.02377"/>
        <updated>2021-06-07T03:06:15.072Z</updated>
        <summary type="html"><![CDATA[Scalable systems for automated driving have to reliably cope with an
open-world setting. This means, the perception systems are exposed to drastic
domain shifts, like changes in weather conditions, time-dependent aspects, or
geographic regions. Covering all domains with annotated data is impossible
because of the endless variations of domains and the time-consuming and
expensive annotation process. Furthermore, fast development cycles of the
system additionally introduce hardware changes, such as sensor types and
vehicle setups, and the required knowledge transfer from simulation. To enable
scalable automated driving, it is therefore crucial to address these domain
shifts in a robust and efficient manner. Over the last years, a vast amount of
different domain adaptation techniques evolved. There already exists a number
of survey papers for domain adaptation on camera images, however, a survey for
LiDAR perception is absent. Nevertheless, LiDAR is a vital sensor for automated
driving that provides detailed 3D scans of the vehicle's surroundings. To
stimulate future research, this paper presents a comprehensive review of recent
progress in domain adaptation methods and formulates interesting research
questions specifically targeted towards LiDAR perception.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Triess_L/0/1/0/all/0/1"&gt;Larissa T. Triess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dreissig_M/0/1/0/all/0/1"&gt;Mariella Dreissig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rist_C/0/1/0/all/0/1"&gt;Christoph B. Rist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1"&gt;J. Marius Z&amp;#xf6;llner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularizing Deep Networks with Semantic Data Augmentation. (arXiv:2007.10538v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10538</id>
        <link href="http://arxiv.org/abs/2007.10538"/>
        <updated>2021-06-07T03:06:15.065Z</updated>
        <summary type="html"><![CDATA[Data augmentation is widely known as a simple yet surprisingly effective
technique for regularizing deep networks. Conventional data augmentation
schemes, e.g., flipping, translation or rotation, are low-level,
data-independent and class-agnostic operations, leading to limited diversity
for augmented samples. To this end, we propose a novel semantic data
augmentation algorithm to complement traditional approaches. The proposed
method is inspired by the intriguing property that deep networks are effective
in learning linearized features, i.e., certain directions in the deep feature
space correspond to meaningful semantic transformations, e.g., changing the
background or view angle of an object. Based on this observation, translating
training samples along many such directions in the feature space can
effectively augment the dataset for more diversity. To implement this idea, we
first introduce a sampling based method to obtain semantically meaningful
directions efficiently. Then, an upper bound of the expected cross-entropy (CE)
loss on the augmented training set is derived by assuming the number of
augmented samples goes to infinity, yielding a highly efficient algorithm. In
fact, we show that the proposed implicit semantic data augmentation (ISDA)
algorithm amounts to minimizing a novel robust CE loss, which adds minimal
extra computational cost to a normal training procedure. In addition to
supervised learning, ISDA can be applied to semi-supervised learning tasks
under the consistency regularization framework, where ISDA amounts to
minimizing the upper bound of the expected KL-divergence between the augmented
features and the original features. Although being simple, ISDA consistently
improves the generalization performance of popular deep models (e.g., ResNets
and DenseNets) on a variety of datasets, i.e., CIFAR-10, CIFAR-100, SVHN,
ImageNet, and Cityscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xuran Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yitong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cheng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privately Learning Mixtures of Axis-Aligned Gaussians. (arXiv:2106.02162v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02162</id>
        <link href="http://arxiv.org/abs/2106.02162"/>
        <updated>2021-06-07T03:06:15.059Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning mixtures of Gaussians under the
constraint of approximate differential privacy. We prove that
$\widetilde{O}(k^2 d \log^{3/2}(1/\delta) / \alpha^2 \varepsilon)$ samples are
sufficient to learn a mixture of $k$ axis-aligned Gaussians in $\mathbb{R}^d$
to within total variation distance $\alpha$ while satisfying $(\varepsilon,
\delta)$-differential privacy. This is the first result for privately learning
mixtures of unbounded axis-aligned (or even unbounded univariate) Gaussians. If
the covariance matrices of each of the Gaussians is the identity matrix, we
show that $\widetilde{O}(kd/\alpha^2 + kd \log(1/\delta) / \alpha \varepsilon)$
samples are sufficient.

Recently, the "local covering" technique of Bun, Kamath, Steinke, and Wu has
been successfully used for privately learning high-dimensional Gaussians with a
known covariance matrix and extended to privately learning general
high-dimensional Gaussians by Aden-Ali, Ashtiani, and Kamath. Given these
positive results, this approach has been proposed as a promising direction for
privately learning mixtures of Gaussians. Unfortunately, we show that this is
not possible.

We design a new technique for privately learning mixture distributions. A
class of distributions $\mathcal{F}$ is said to be list-decodable if there is
an algorithm that, given "heavily corrupted" samples from $f\in \mathcal{F}$,
outputs a list of distributions, $\widehat{\mathcal{F}}$, such that one of the
distributions in $\widehat{\mathcal{F}}$ approximates $f$. We show that if
$\mathcal{F}$ is privately list-decodable, then we can privately learn mixtures
of distributions in $\mathcal{F}$. Finally, we show axis-aligned Gaussian
distributions are privately list-decodable, thereby proving mixtures of such
distributions are privately learnable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aden_Ali_I/0/1/0/all/0/1"&gt;Ishaq Aden-Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashtiani_H/0/1/0/all/0/1"&gt;Hassan Ashtiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liaw_C/0/1/0/all/0/1"&gt;Christopher Liaw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Materials Representation and Transfer Learning for Multi-Property Prediction. (arXiv:2106.02225v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02225</id>
        <link href="http://arxiv.org/abs/2106.02225"/>
        <updated>2021-06-07T03:06:15.053Z</updated>
        <summary type="html"><![CDATA[The adoption of machine learning in materials science has rapidly transformed
materials property prediction. Hurdles limiting full capitalization of recent
advancements in machine learning include the limited development of methods to
learn the underlying interactions of multiple elements, as well as the
relationships among multiple properties, to facilitate property prediction in
new composition spaces. To address these issues, we introduce the Hierarchical
Correlation Learning for Multi-property Prediction (H-CLMP) framework that
seamlessly integrates (i) prediction using only a material's composition, (ii)
learning and exploitation of correlations among target properties in
multi-target regression, and (iii) leveraging training data from tangential
domains via generative transfer learning. The model is demonstrated for
prediction of spectral optical absorption of complex metal oxides spanning 69
3-cation metal oxide composition spaces. H-CLMP accurately predicts non-linear
composition-property relationships in composition spaces for which no training
data is available, which broadens the purview of machine learning to the
discovery of materials with exceptional properties. This achievement results
from the principled integration of latent embedding learning, property
correlation learning, generative transfer learning, and attention models. The
best performance is obtained using H-CLMP with Transfer learning (H-CLMP(T))
wherein a generative adversarial network is trained on computational density of
states data and deployed in the target domain to augment prediction of optical
absorption from composition. H-CLMP(T) aggregates multiple knowledge sources
with a framework that is well-suited for multi-target regression across the
physical sciences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1"&gt;Shufeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guevarra_D/0/1/0/all/0/1"&gt;Dan Guevarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregoire_J/0/1/0/all/0/1"&gt;John M. Gregoire&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporally coherent video anonymization through GAN inpainting. (arXiv:2106.02328v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02328</id>
        <link href="http://arxiv.org/abs/2106.02328"/>
        <updated>2021-06-07T03:06:15.045Z</updated>
        <summary type="html"><![CDATA[This work tackles the problem of temporally coherent face anonymization in
natural video streams.We propose JaGAN, a two-stage system starting with
detecting and masking out faces with black image patches in all individual
frames of the video. The second stage leverages a privacy-preserving Video
Generative Adversarial Network designed to inpaint the missing image patches
with artificially generated faces. Our initial experiments reveal that image
based generative models are not capable of inpainting patches showing temporal
coherent appearance across neighboring video frames. To address this issue we
introduce a newly curated video collection, which is made publicly available
for the research community along with this paper. We also introduce the
Identity Invariance Score IdI as a means to quantify temporal coherency between
neighboring frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balaji_T/0/1/0/all/0/1"&gt;Thangapavithraa Balaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blies_P/0/1/0/all/0/1"&gt;Patrick Blies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_G/0/1/0/all/0/1"&gt;Georg G&amp;#xf6;ri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitsch_R/0/1/0/all/0/1"&gt;Raphael Mitsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wasserer_M/0/1/0/all/0/1"&gt;Marcel Wasserer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1"&gt;Torsten Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entity Concept-enhanced Few-shot Relation Extraction. (arXiv:2106.02401v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02401</id>
        <link href="http://arxiv.org/abs/2106.02401"/>
        <updated>2021-06-07T03:06:15.039Z</updated>
        <summary type="html"><![CDATA[Few-shot relation extraction (FSRE) is of great importance in long-tail
distribution problem, especially in special domain with low-resource data. Most
existing FSRE algorithms fail to accurately classify the relations merely based
on the information of the sentences together with the recognized entity pairs,
due to limited samples and lack of knowledge. To address this problem, in this
paper, we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction
scheme (ConceptFERE), which introduces the inherent concepts of entities to
provide clues for relation prediction and boost the relations classification
performance. Firstly, a concept-sentence attention module is developed to
select the most appropriate concept from multiple concepts of each entity by
calculating the semantic similarity between sentences and concepts. Secondly, a
self-attention based fusion module is presented to bridge the gap of concept
embedding and sentence embedding from different semantic spaces. Extensive
experiments on the FSRE benchmark dataset FewRel have demonstrated the
effectiveness and the superiority of the proposed ConceptFERE scheme as
compared to the state-of-the-art baselines. Code is available at
https://github.com/LittleGuoKe/ConceptFERE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guanglin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qinghua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Graph Learning for Link Prediction. (arXiv:2106.02172v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02172</id>
        <link href="http://arxiv.org/abs/2106.02172"/>
        <updated>2021-06-07T03:06:15.009Z</updated>
        <summary type="html"><![CDATA[Learning to predict missing links is important for many graph-based
applications. Existing methods were designed to learn the observed association
between two sets of variables: (1) the observed graph structure and (2) the
existence of link between a pair of nodes. However, the causal relationship
between these variables was ignored and we visit the possibility of learning it
by simply asking a counterfactual question: "would the link exist or not if the
observed graph structure became different?" To answer this question by causal
inference, we consider the information of the node pair as context, global
graph structural properties as treatment, and link existence as outcome. In
this work, we propose a novel link prediction method that enhances graph
learning by the counterfactual inference. It creates counterfactual links from
the observed ones, and our method learns representations from both of them.
Experiments on a number of benchmark datasets show that our proposed method
achieves the state-of-the-art performance on link prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Gang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Daheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenhao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proving Equivalence Between Complex Expressions Using Graph-to-Sequence Neural Models. (arXiv:2106.02452v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2106.02452</id>
        <link href="http://arxiv.org/abs/2106.02452"/>
        <updated>2021-06-07T03:06:14.983Z</updated>
        <summary type="html"><![CDATA[We target the problem of provably computing the equivalence between two
complex expression trees. To this end, we formalize the problem of equivalence
between two such programs as finding a set of semantics-preserving rewrite
rules from one into the other, such that after the rewrite the two programs are
structurally identical, and therefore trivially equivalent.We then develop a
graph-to-sequence neural network system for program equivalence, trained to
produce such rewrite sequences from a carefully crafted automatic example
generation algorithm. We extensively evaluate our system on a rich multi-type
linear algebra expression language, using arbitrary combinations of 100+
graph-rewriting axioms of equivalence. Our machine learning system guarantees
correctness for all true negatives, and ensures 0 false positive by design. It
outputs via inference a valid proof of equivalence for 93% of the 10,000
equivalent expression pairs isolated for testing, using up to 50-term
expressions. In all cases, the validity of the sequence produced and therefore
the provable assertion of program equivalence is always computable, in
negligible time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kommrusch_S/0/1/0/all/0/1"&gt;Steve Kommrusch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barollet_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Barollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouchet_L/0/1/0/all/0/1"&gt;Louis-No&amp;#xeb;l Pouchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic gradient descent with noise of machine learning type. Part II: Continuous time analysis. (arXiv:2106.02588v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02588</id>
        <link href="http://arxiv.org/abs/2106.02588"/>
        <updated>2021-06-07T03:06:14.972Z</updated>
        <summary type="html"><![CDATA[The representation of functions by artificial neural networks depends on a
large number of parameters in a non-linear fashion. Suitable parameters of
these are found by minimizing a 'loss functional', typically by stochastic
gradient descent (SGD) or an advanced SGD-based algorithm.

In a continuous time model for SGD with noise that follows the 'machine
learning scaling', we show that in a certain noise regime, the optimization
algorithm prefers 'flat' minima of the objective function in a sense which is
different from the flat minimum selection of continuous time SGD with
homogeneous noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Signed Cumulative Distribution Transform for 1-D Signal Analysis and Classification. (arXiv:2106.02146v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02146</id>
        <link href="http://arxiv.org/abs/2106.02146"/>
        <updated>2021-06-07T03:06:14.964Z</updated>
        <summary type="html"><![CDATA[This paper presents a new mathematical signal transform that is especially
suitable for decoding information related to non-rigid signal displacements. We
provide a measure theoretic framework to extend the existing Cumulative
Distribution Transform [ACHA 45 (2018), no. 3, 616-641] to arbitrary (signed)
signals on $\overline{\mathbb{R}}$. We present both forward (analysis) and
inverse (synthesis) formulas for the transform, and describe several of its
properties including translation, scaling, convexity, linear separability and
others. Finally, we describe a metric in transform space, and demonstrate the
application of the transform in classifying (detecting) signals under random
displacements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aldroubi_A/0/1/0/all/0/1"&gt;Akram Aldroubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Rocio Diaz Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medri_I/0/1/0/all/0/1"&gt;Ivan Medri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1"&gt;Gustavo K. Rohde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thareja_S/0/1/0/all/0/1"&gt;Sumati Thareja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fuzzy Clustering with Similarity Queries. (arXiv:2106.02212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02212</id>
        <link href="http://arxiv.org/abs/2106.02212"/>
        <updated>2021-06-07T03:06:14.945Z</updated>
        <summary type="html"><![CDATA[The fuzzy or soft $k$-means objective is a popular generalization of the
well-known $k$-means problem, extending the clustering capability of the
$k$-means to datasets that are uncertain, vague, and otherwise hard to cluster.
In this paper, we propose a semi-supervised active clustering framework, where
the learner is allowed to interact with an oracle (domain expert), asking for
the similarity between a certain set of chosen items. We study the query and
computational complexities of clustering in this framework. We prove that
having a few of such similarity queries enables one to get a polynomial-time
approximation algorithm to an otherwise conjecturally NP-hard problem. In
particular, we provide probabilistic algorithms for fuzzy clustering in this
setting that asks $O(\mathsf{poly}(k)\log n)$ similarity queries and run with
polynomial-time-complexity, where $n$ is the number of items. The fuzzy
$k$-means objective is nonconvex, with $k$-means as a special case, and is
equivalent to some other generic nonconvex problem such as non-negative matrix
factorization. The ubiquitous Lloyd-type algorithms (or,
expectation-maximization algorithm) can get stuck at a local minima. Our
results show that by making few similarity queries, the problem becomes easier
to solve. Finally, we test our algorithms over real-world datasets, showing
their effectiveness in real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huleihel_W/0/1/0/all/0/1"&gt;Wasim Huleihel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1"&gt;Soumyabrata Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intelligent Resource Reservation for Crowdsourced Live Video Streaming Applications in Geo-Distributed Cloud Environment. (arXiv:2106.02420v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02420</id>
        <link href="http://arxiv.org/abs/2106.02420"/>
        <updated>2021-06-07T03:06:14.933Z</updated>
        <summary type="html"><![CDATA[Crowdsourced live video streaming (livecast) services such as Facebook Live,
YouNow, Douyu and Twitch are gaining more momentum recently. Allocating the
limited resources in a cost-effective manner while maximizing the Quality of
Service (QoS) through real-time delivery and the provision of the appropriate
representations for all viewers is a challenging problem. In our paper, we
introduce a machine-learning based predictive resource allocation framework for
geo-distributed cloud sites, considering the delay and quality constraints to
guarantee the maximum QoS for viewers and the minimum cost for content
providers. First, we present an offline optimization that decides the required
transcoding resources in distributed regions near the viewers with a trade-off
between the QoS and the overall cost. Second, we use machine learning to build
forecasting models that proactively predict the approximate transcoding
resources to be reserved at each cloud site ahead of time. Finally, we develop
a Greedy Nearest and Cheapest algorithm (GNCA) to perform the resource
allocation of real-time broadcasted videos on the rented resources. Extensive
simulations have shown that GNCA outperforms the state-of-the art resource
allocation approaches for crowdsourced live streaming by achieving more than
20% gain in terms of system cost while serving the viewers with relatively
lower latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1"&gt;Emna Baccour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haouari_F/0/1/0/all/0/1"&gt;Fatima Haouari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilal_K/0/1/0/all/0/1"&gt;Kashif Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1"&gt;Mohsen Guizani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1"&gt;Mounir Hamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Target Networks: Improving Deep $Q$-learning with Functional Regularization. (arXiv:2106.02613v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02613</id>
        <link href="http://arxiv.org/abs/2106.02613"/>
        <updated>2021-06-07T03:06:14.915Z</updated>
        <summary type="html"><![CDATA[Target networks are at the core of recent success in Reinforcement Learning.
They stabilize the training by using old parameters to estimate the $Q$-values,
but this also limits the propagation of newly-encountered rewards which could
ultimately slow down the training. In this work, we propose an alternative
training method based on functional regularization which does not have this
deficiency. Unlike target networks, our method uses up-to-date parameters to
estimate the target $Q$-values, thereby speeding up training while maintaining
stability. Surprisingly, in some cases, we can show that target networks are a
special, restricted type of functional regularizers. Using this approach, we
show empirical improvements in sample efficiency and performance across a range
of Atari and simulated robotics environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Piche_A/0/1/0/all/0/1"&gt;Alexandre Pich&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marino_J/0/1/0/all/0/1"&gt;Joseph Marino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marconi_G/0/1/0/all/0/1"&gt;Gian Maria Marconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery. (arXiv:2106.02190v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02190</id>
        <link href="http://arxiv.org/abs/2106.02190"/>
        <updated>2021-06-07T03:06:14.892Z</updated>
        <summary type="html"><![CDATA[We developed Distilled Graph Attention Policy Networks (DGAPNs), a
curiosity-driven reinforcement learning model to generate novel
graph-structured chemical representations that optimize user-defined objectives
by efficiently navigating a physically constrained domain. The framework is
examined on the task of generating molecules that are designed to bind,
noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial
Graph Attention Network (sGAT) that leverages self-attention over both node and
edge attributes as well as encoding spatial structure -- this capability is of
considerable interest in areas such as molecular and synthetic biology and drug
discovery. An attentional policy network is then introduced to learn decision
rules for a dynamic, fragment-based chemical environment, and state-of-the-art
policy gradient techniques are employed to train the network with enhanced
stability. Exploration is efficiently encouraged by incorporating innovation
reward bonuses learned and proposed by random network distillation. In
experiments, our framework achieved outstanding results compared to
state-of-the-art algorithms, while increasing the diversity of proposed
molecules and reducing the complexity of paths to chemical synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yulun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choma_N/0/1/0/all/0/1"&gt;Nicholas Choma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Andrew Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cashman_M/0/1/0/all/0/1"&gt;Mikaela Cashman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prates_E/0/1/0/all/0/1"&gt;&amp;#xc9;rica T. Prates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Manesh Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vergara_V/0/1/0/all/0/1"&gt;Ver&amp;#xf3;nica G. Melesse Vergara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clyde_A/0/1/0/all/0/1"&gt;Austin Clyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brettin_T/0/1/0/all/0/1"&gt;Thomas S. Brettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jong_W/0/1/0/all/0/1"&gt;Wibe A. de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Head_M/0/1/0/all/0/1"&gt;Martha S. Head&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1"&gt;Rick L. Stevens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugent_P/0/1/0/all/0/1"&gt;Peter Nugent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobson_D/0/1/0/all/0/1"&gt;Daniel A. Jacobson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1"&gt;James B. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of General-Purpose Embeddings for Code Changes. (arXiv:2106.02087v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.02087</id>
        <link href="http://arxiv.org/abs/2106.02087"/>
        <updated>2021-06-07T03:06:14.886Z</updated>
        <summary type="html"><![CDATA[A lot of problems in the field of software engineering - bug fixing, commit
message generation, etc. - require analyzing not only the code itself but
specifically code changes. Applying machine learning models to these tasks
requires us to create numerical representations of the changes, i.e.
embeddings. Recent studies demonstrate that the best way to obtain these
embeddings is to pre-train a deep neural network in an unsupervised manner on a
large volume of unlabeled data and then further fine-tune it for a specific
task.

In this work, we propose an approach for obtaining such embeddings of code
changes during pre-training and evaluate them on two different downstream tasks
- applying changes to code and commit message generation. The pre-training
consists of the model learning to apply the given change (an edit sequence) to
the code in a correct way, and therefore requires only the code change itself.
To increase the quality of the obtained embeddings, we only consider the
changed tokens in the edit sequence. In the task of applying code changes, our
model outperforms the model that uses full edit sequences by 5.9 percentage
points in accuracy. As for the commit message generation, our model
demonstrated the same results as supervised models trained for this specific
task, which indicates that it can encode code changes well and can be improved
in the future by pre-training on a larger dataset of easily gathered code
changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pravilov_M/0/1/0/all/0/1"&gt;Mikhail Pravilov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogomolov_E/0/1/0/all/0/1"&gt;Egor Bogomolov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golubev_Y/0/1/0/all/0/1"&gt;Yaroslav Golubev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bryksin_T/0/1/0/all/0/1"&gt;Timofey Bryksin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hallucination In Object Detection -- A Study In Visual Part Verification. (arXiv:2106.02523v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02523</id>
        <link href="http://arxiv.org/abs/2106.02523"/>
        <updated>2021-06-07T03:06:14.880Z</updated>
        <summary type="html"><![CDATA[We show that object detectors can hallucinate and detect missing objects;
potentially even accurately localized at their expected, but non-existing,
position. This is particularly problematic for applications that rely on visual
part verification: detecting if an object part is present or absent. We show
how popular object detectors hallucinate objects in a visual part verification
task and introduce the first visual part verification dataset: DelftBikes,
which has 10,000 bike photographs, with 22 densely annotated parts per image,
where some parts may be missing. We explicitly annotated an extra object state
label for each part to reflect if a part is missing or intact. We propose to
evaluate visual part verification by relying on recall and compare popular
object detectors on DelftBikes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kayhan_O/0/1/0/all/0/1"&gt;Osman Semih Kayhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vredebregt_B/0/1/0/all/0/1"&gt;Bart Vredebregt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan C. van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustifying Reinforcement Learning Policies with $\mathcal{L}_1$ Adaptive Control. (arXiv:2106.02249v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02249</id>
        <link href="http://arxiv.org/abs/2106.02249"/>
        <updated>2021-06-07T03:06:14.874Z</updated>
        <summary type="html"><![CDATA[A reinforcement learning (RL) policy trained in a nominal environment could
fail in a new/perturbed environment due to the existence of dynamic variations.
Existing robust methods try to obtain a fixed policy for all envisioned dynamic
variation scenarios through robust or adversarial training. These methods could
lead to conservative performance due to emphasis on the worst case, and often
involve tedious modifications to the training environment. We propose an
approach to robustifying a pre-trained non-robust RL policy with
$\mathcal{L}_1$ adaptive control. Leveraging the capability of an
$\mathcal{L}_1$ control law in the fast estimation of and active compensation
for dynamic variations, our approach can significantly improve the robustness
of an RL policy trained in a standard (i.e., non-robust) way, either in a
simulator or in the real world. Numerical experiments are provided to validate
the efficacy of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yikun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1"&gt;Manan Gandhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1"&gt;Evangelos Theodorou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1"&gt;Naira Hovakimyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02359</id>
        <link href="http://arxiv.org/abs/2106.02359"/>
        <updated>2021-06-07T03:06:14.867Z</updated>
        <summary type="html"><![CDATA[Recent years have seen many breakthroughs in natural language processing
(NLP), transitioning it from a mostly theoretical field to one with many
real-world applications. Noting the rising number of applications of other
machine learning and AI techniques with pervasive societal impact, we
anticipate the rising importance of developing NLP technologies for social
good. Inspired by theories in moral philosophy and global priorities research,
we aim to promote a guideline for social good in the context of NLP. We lay the
foundations via moral philosophy's definition of social good, propose a
framework to evaluate NLP tasks' direct and indirect real-world impact, and
adopt the methodology of global priorities research to identify priority causes
for NLP research. Finally, we use our theoretical framework to provide some
practical guidelines for future NLP research for social good. Our data and
codes are available at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhijing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1"&gt;Geeticka Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1"&gt;Brian Tse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double Descent Optimization Pattern and Aliasing: Caveats of Noisy Labels. (arXiv:2106.02100v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02100</id>
        <link href="http://arxiv.org/abs/2106.02100"/>
        <updated>2021-06-07T03:06:14.849Z</updated>
        <summary type="html"><![CDATA[Optimization plays a key role in the training of deep neural networks.
Deciding when to stop training can have a substantial impact on the performance
of the network during inference. Under certain conditions, the generalization
error can display a double descent pattern during training: the learning curve
is non-monotonic and seemingly diverges before converging again after
additional epochs. This optimization pattern can lead to early stopping
procedures to stop training before the second convergence and consequently
select a suboptimal set of parameters for the network, with worse performance
during inference. In this work, in addition to confirming that double descent
occurs with small datasets and noisy labels as evidenced by others, we show
that noisy labels must be present both in the training and generalization sets
to observe a double descent pattern. We also show that the learning rate has an
influence on double descent, and study how different optimizers and optimizer
parameters influence the apparition of double descent. Finally, we show that
increasing the learning rate can create an aliasing effect that masks the
double descent pattern without suppressing it. We study this phenomenon through
extensive experiments on variants of CIFAR-10 and show that they translate to a
real world application: the forecast of seizure events in epileptic patients
from continuous electroencephalographic recordings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubost_F/0/1/0/all/0/1"&gt;Florian Dubost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saab_K/0/1/0/all/0/1"&gt;Khaled Kamal Saab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_E/0/1/0/all/0/1"&gt;Erin Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1"&gt;Daniel Yang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pike_M/0/1/0/all/0/1"&gt;Max Pike&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Siddharth Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siyi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskhar_N/0/1/0/all/0/1"&gt;Nandita Bhaskhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Messer_C/0/1/0/all/0/1"&gt;Christopher Lee-Messer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Closer Look at the Worst-case Behavior of Multi-armed Bandit Algorithms. (arXiv:2106.02126v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02126</id>
        <link href="http://arxiv.org/abs/2106.02126"/>
        <updated>2021-06-07T03:06:14.842Z</updated>
        <summary type="html"><![CDATA[One of the key drivers of complexity in the classical (stochastic)
multi-armed bandit (MAB) problem is the difference between mean rewards in the
top two arms, also known as the instance gap. The celebrated Upper Confidence
Bound (UCB) policy is among the simplest optimism-based MAB algorithms that
naturally adapts to this gap: for a horizon of play n, it achieves optimal
O(log n) regret in instances with "large" gaps, and a near-optimal O(\sqrt{n
log n}) minimax regret when the gap can be arbitrarily "small." This paper
provides new results on the arm-sampling behavior of UCB, leading to several
important insights. Among these, it is shown that arm-sampling rates under UCB
are asymptotically deterministic, regardless of the problem complexity. This
discovery facilitates new sharp asymptotics and a novel alternative proof for
the O(\sqrt{n log n}) minimax regret of UCB. Furthermore, the paper also
provides the first complete process-level characterization of the MAB problem
under UCB in the conventional diffusion scaling. Among other things, the
"small" gap worst-case lens adopted in this paper also reveals profound
distinctions between the behavior of UCB and Thompson Sampling, such as an
"incomplete learning" phenomenon characteristic of the latter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalvit_A/0/1/0/all/0/1"&gt;Anand Kalvit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeevi_A/0/1/0/all/0/1"&gt;Assaf Zeevi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAND-mask: An Enhanced Gradient Masking Strategy for the Discovery of Invariances in Domain Generalization. (arXiv:2106.02266v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02266</id>
        <link href="http://arxiv.org/abs/2106.02266"/>
        <updated>2021-06-07T03:06:14.835Z</updated>
        <summary type="html"><![CDATA[A major bottleneck in the real-world applications of machine learning models
is their failure in generalizing to unseen domains whose data distribution is
not i.i.d to the training domains. This failure often stems from learning
non-generalizable features in the training domains that are spuriously
correlated with the label of data. To address this shortcoming, there has been
a growing surge of interest in learning good explanations that are hard to
vary, which is studied under the notion of Out-of-Distribution (OOD)
Generalization. The search for good explanations that are \textit{invariant}
across different domains can be seen as finding local (global) minimas in the
loss landscape that hold true across all of the training domains. In this
paper, we propose a masking strategy, which determines a continuous weight
based on the agreement of gradients that flow in each edge of network, in order
to control the amount of update received by the edge in each step of
optimization. Particularly, our proposed technique referred to as "Smoothed-AND
(SAND)-masking", not only validates the agreement in the direction of gradients
but also promotes the agreement among their magnitudes to further ensure the
discovery of invariances across training domains. SAND-mask is validated over
the Domainbed benchmark for domain generalization and significantly improves
the state-of-the-art accuracy on the Colored MNIST dataset while providing
competitive results on other domain generalization datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahtalebi_S/0/1/0/all/0/1"&gt;Soroosh Shahtalebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gagnon_Audet_J/0/1/0/all/0/1"&gt;Jean-Christophe Gagnon-Audet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laleh_T/0/1/0/all/0/1"&gt;Touraj Laleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faramarzi_M/0/1/0/all/0/1"&gt;Mojtaba Faramarzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1"&gt;Kartik Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding and Fixing Spurious Patterns with Explanations. (arXiv:2106.02112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02112</id>
        <link href="http://arxiv.org/abs/2106.02112"/>
        <updated>2021-06-07T03:06:14.827Z</updated>
        <summary type="html"><![CDATA[Machine learning models often use spurious patterns such as "relying on the
presence of a person to detect a tennis racket," which do not generalize. In
this work, we present an end-to-end pipeline for identifying and mitigating
spurious patterns for image classifiers. We start by finding patterns such as
"the model's prediction for tennis racket changes 63% of the time if we hide
the people." Then, if a pattern is spurious, we mitigate it via a novel form of
data augmentation. We demonstrate that this approach identifies a diverse set
of spurious patterns and that it mitigates them by producing a model that is
both more accurate on a distribution where the spurious pattern is not helpful
and more robust to distribution shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1"&gt;Gregory Plumb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1"&gt;Marco Tulio Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits. (arXiv:2002.02518v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02518</id>
        <link href="http://arxiv.org/abs/2002.02518"/>
        <updated>2021-06-07T03:06:14.822Z</updated>
        <summary type="html"><![CDATA[Many of the recent triumphs in machine learning are dependent on well-tuned
hyperparameters. This is particularly prominent in reinforcement learning (RL)
where a small change in the configuration can lead to failure. Despite the
importance of tuning hyperparameters, it remains expensive and is often done in
a naive and laborious way. A recent solution to this problem is Population
Based Training (PBT) which updates both weights and hyperparameters in a single
training run of a population of agents. PBT has been shown to be particularly
effective in RL, leading to widespread use in the field. However, PBT lacks
theoretical guarantees since it relies on random heuristics to explore the
hyperparameter space. This inefficiency means it typically requires vast
computational resources, which is prohibitive for many small and medium sized
labs. In this work, we introduce the first provably efficient PBT-style
algorithm, Population-Based Bandits (PB2). PB2 uses a probabilistic model to
guide the search in an efficient way, making it possible to discover high
performing hyperparameter configurations with far fewer agents than typically
required by PBT. We show in a series of RL experiments that PB2 is able to
achieve high performance with a modest computational budget.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Vu Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Laplacian-Based Dimensionality Reduction Including Spectral Clustering, Laplacian Eigenmap, Locality Preserving Projection, Graph Embedding, and Diffusion Map: Tutorial and Survey. (arXiv:2106.02154v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02154</id>
        <link href="http://arxiv.org/abs/2106.02154"/>
        <updated>2021-06-07T03:06:14.802Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper for nonlinear dimensionality and feature
extraction methods which are based on the Laplacian of graph of data. We first
introduce adjacency matrix, definition of Laplacian matrix, and the
interpretation of Laplacian. Then, we cover the cuts of graph and spectral
clustering which applies clustering in a subspace of data. Different
optimization variants of Laplacian eigenmap and its out-of-sample extension are
explained. Thereafter, we introduce the locality preserving projection and its
kernel variant as linear special cases of Laplacian eigenmap. Versions of graph
embedding are then explained which are generalized versions of Laplacian
eigenmap and locality preserving projection. Finally, diffusion map is
introduced which is a method based on Laplacian of data and random walks on the
data graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nara: Learning Network-Aware Resource Allocation Algorithms for Cloud Data Centres. (arXiv:2106.02412v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02412</id>
        <link href="http://arxiv.org/abs/2106.02412"/>
        <updated>2021-06-07T03:06:14.795Z</updated>
        <summary type="html"><![CDATA[Data centres (DCs) underline many prominent future technological trends such
as distributed training of large scale machine learning models and
internet-of-things based platforms. DCs will soon account for over 3\% of
global energy demand, so efficient use of DC resources is essential. Robust DC
networks (DCNs) are essential to form the large scale systems needed to handle
this demand, but can bottleneck how efficiently DC-server resources can be used
when servers with insufficient connectivity between them cannot be jointly
allocated to a job. However, allocating servers' resources whilst accounting
for their inter-connectivity maps to an NP-hard combinatorial optimisation
problem, and so is often ignored in DC resource management schemes. We present
Nara, a framework based on reinforcement learning (RL) and graph neural
networks (GNN) to learn network-aware allocation policies that increase the
number of requests allocated over time compared to previous methods. Unique to
our solution is the use of a GNN to generate representations of server-nodes in
the DCN, which are then interpreted as actions by a RL policy-network which
chooses from which servers resources will be allocated to incoming requests.
Nara is agnostic to the topology size and shape and is trained end-to-end. The
method can accept up to 33\% more requests than the best baseline when deployed
on DCNs with up to the order of $10\times$ more compute nodes than the DCN seen
during training and is able to maintain its policy's performance on DCNs with
the order of $100\times$ more servers than seen during training. It also
generalises to unseen DCN topologies with varied network structure and unseen
request distributions without re-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shabka_Z/0/1/0/all/0/1"&gt;Zacharaya Shabka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zervas_G/0/1/0/all/0/1"&gt;Georgios Zervas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Schr\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02081</id>
        <link href="http://arxiv.org/abs/2106.02081"/>
        <updated>2021-06-07T03:06:14.788Z</updated>
        <summary type="html"><![CDATA[The Schr\"odinger bridge problem (SBP) finds the most likely stochastic
evolution between two probability distributions given a prior stochastic
evolution. As well as applications in the natural sciences, problems of this
kind have important applications in machine learning such as dataset alignment
and hypothesis testing. Whilst the theory behind this problem is relatively
mature, scalable numerical recipes to estimate the Schr\"odinger bridge remain
an active area of research. We prove an equivalence between the SBP and maximum
likelihood estimation enabling direct application of successful machine
learning techniques. We propose a numerical procedure to estimate SBPs using
Gaussian process and demonstrate the practical usage of our approach in
numerical simulations and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francisco Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1"&gt;Pierre Thodoroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1"&gt;Neil D. Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1"&gt;Austen Lamacraft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ambulatory blood pressure monitoring versus office blood pressure measurement: Are there sex differences?. (arXiv:2106.02392v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.02392</id>
        <link href="http://arxiv.org/abs/2106.02392"/>
        <updated>2021-06-07T03:06:14.782Z</updated>
        <summary type="html"><![CDATA[The accurate measurement of blood pressure (BP) is an important prerequisite
for the reliable diagnosis and efficient management of hypertension and other
medical conditions. Office Blood Pressure Measurement (OBP) is a technique
performed in-office with the sphygmomanometer, while Ambulatory Blood Pressure
Monitoring (ABPM) is a technique that measures blood pressure during 24h. The
BP fluctuations also depend on other factors such as physical activity,
temperature, mood, age, sex, any pathologies, a hormonal activity that may
intrinsically influence the differences between OBP and ABPM. The aim of this
study is to examine the possible influence of sex on the discrepancies between
OBP and ABPM in 872 subjects with known or suspected hypertension. A
significant correlation was observed between OBP and ABPM mean values
calculated during the day, night and 24h (ABPMday, ABPMnight, ABPM24h) in both
groups (p<0.0001). The main finding of this study is that no difference between
sexes was observed in the relation between OBP and mean ABMP values except
between systolic OBP and systolic ABPM during the night. In addition, this
study showed a moderate correlation between BPs obtained with the two
approaches with a great dispersion around the regression line which suggests
that the two approaches cannot be used interchangeably.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Miladinovic_A/0/1/0/all/0/1"&gt;Aleksandar Miladinovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ajcevic_M/0/1/0/all/0/1"&gt;Milo&amp;#x161; Aj&amp;#x10d;evi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Siveri_G/0/1/0/all/0/1"&gt;Giulia Siveri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liguori_L/0/1/0/all/0/1"&gt;Laura Liguori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pascazio_L/0/1/0/all/0/1"&gt;Lorenzo Pascazio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Accardo_A/0/1/0/all/0/1"&gt;Agostino Accardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching keyword spotters to spot new keywords with limited examples. (arXiv:2106.02443v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02443</id>
        <link href="http://arxiv.org/abs/2106.02443"/>
        <updated>2021-06-07T03:06:14.775Z</updated>
        <summary type="html"><![CDATA[Learning to recognize new keywords with just a few examples is essential for
personalizing keyword spotting (KWS) models to a user's choice of keywords.
However, modern KWS models are typically trained on large datasets and
restricted to a small vocabulary of keywords, limiting their transferability to
a broad range of unseen keywords. Towards easily customizable KWS models, we
present KeySEM (Keyword Speech EMbedding), a speech embedding model pre-trained
on the task of recognizing a large number of keywords. Speech representations
offered by KeySEM are highly effective for learning new keywords from a limited
number of examples. Comparisons with a diverse range of related work across
several datasets show that our method achieves consistently superior
performance with fewer training examples. Although KeySEM was pre-trained only
on English utterances, the performance gains also extend to datasets from four
other languages indicating that KeySEM learns useful representations well
aligned with the task of keyword spotting. Finally, we demonstrate KeySEM's
ability to learn new keywords sequentially without requiring to re-train on
previously learned keywords. Our experimental observations suggest that KeySEM
is well suited to on-device environments where post-deployment learning and
ease of customization are often desirable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Awasthi_A/0/1/0/all/0/1"&gt;Abhijeet Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kilgour_K/0/1/0/all/0/1"&gt;Kevin Kilgour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rom_H/0/1/0/all/0/1"&gt;Hassan Rom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Distribution Generalization in Kernel Regression. (arXiv:2106.02261v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02261</id>
        <link href="http://arxiv.org/abs/2106.02261"/>
        <updated>2021-06-07T03:06:14.755Z</updated>
        <summary type="html"><![CDATA[In real word applications, data generating process for training a machine
learning model often differs from what the model encounters in the test stage.
Understanding how and whether machine learning models generalize under such
distributional shifts have been a theoretical challenge. Here, we study
generalization in kernel regression when the training and test distributions
are different using methods from statistical physics. Using the replica method,
we derive an analytical formula for the out-of-distribution generalization
error applicable to any kernel and real datasets. We identify an overlap matrix
that quantifies the mismatch between distributions for a given kernel as a key
determinant of generalization performance under distribution shift. Using our
analytical expressions we elucidate various generalization phenomena including
possible improvement in generalization when there is a mismatch. We develop
procedures for optimizing training and test distributions for a given data
budget to find best and worst case generalizations under the shift. We present
applications of our theory to real and synthetic datasets and for many kernels.
We compare results of our theory applied to Neural Tangent Kernel with
simulations of wide networks and show agreement. We analyze linear regression
in further depth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Canatar_A/0/1/0/all/0/1"&gt;Abdulkadir Canatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1"&gt;Blake Bordelon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1"&gt;Cengiz Pehlevan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Prospective Observational Study to Investigate Performance of a Chest X-ray Artificial Intelligence Diagnostic Support Tool Across 12 U.S. Hospitals. (arXiv:2106.02118v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02118</id>
        <link href="http://arxiv.org/abs/2106.02118"/>
        <updated>2021-06-07T03:06:14.749Z</updated>
        <summary type="html"><![CDATA[Importance: An artificial intelligence (AI)-based model to predict COVID-19
likelihood from chest x-ray (CXR) findings can serve as an important adjunct to
accelerate immediate clinical decision making and improve clinical decision
making. Despite significant efforts, many limitations and biases exist in
previously developed AI diagnostic models for COVID-19. Utilizing a large set
of local and international CXR images, we developed an AI model with high
performance on temporal and external validation.

Conclusions and Relevance: AI-based diagnostic tools may serve as an adjunct,
but not replacement, for clinical decision support of COVID-19 diagnosis, which
largely hinges on exposure history, signs, and symptoms. While AI-based tools
have not yet reached full diagnostic potential in COVID-19, they may still
offer valuable information to clinicians taken into consideration along with
clinical signs and symptoms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1"&gt;Le Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Taihui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adila_D/0/1/0/all/0/1"&gt;Dyah Adila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaiman_Z/0/1/0/all/0/1"&gt;Zach Zaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Melton_G/0/1/0/all/0/1"&gt;Genevieve B. Melton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ingraham_N/0/1/0/all/0/1"&gt;Nicholas Ingraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murray_E/0/1/0/all/0/1"&gt;Eric Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boley_D/0/1/0/all/0/1"&gt;Daniel Boley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Switzer_S/0/1/0/all/0/1"&gt;Sean Switzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burns_J/0/1/0/all/0/1"&gt;John L. Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Allen_T/0/1/0/all/0/1"&gt;Tadashi Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Steenburg_S/0/1/0/all/0/1"&gt;Scott D. Steenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1"&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kummerfeld_E/0/1/0/all/0/1"&gt;Erich Kummerfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tignanelli_C/0/1/0/all/0/1"&gt;Christopher Tignanelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ukiyo-e Analysis and Creativity with Attribute and Geometry Annotation. (arXiv:2106.02267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02267</id>
        <link href="http://arxiv.org/abs/2106.02267"/>
        <updated>2021-06-07T03:06:14.742Z</updated>
        <summary type="html"><![CDATA[The study of Ukiyo-e, an important genre of pre-modern Japanese art, focuses
on the object and style like other artwork researches. Such study has benefited
from the renewed interest by the machine learning community in culturally
important topics, leading to interdisciplinary works including collections of
images, quantitative approaches, and machine learning-based creativities. They,
however, have several drawbacks, and it remains challenging to integrate these
works into a comprehensive view. To bridge this gap, we propose a holistic
approach We first present a large-scale Ukiyo-e dataset with coherent semantic
labels and geometric annotations, then show its value in a quantitative study
of Ukiyo-e paintings' object using these labels and annotations. We further
demonstrate the machine learning methods could help style study through soft
color decomposition of Ukiyo-e, and finally provides joint insights into object
and style by composing sketches and colors using colorization. Dataset
available at https://github.com/rois-codh/arc-ukiyoe-faces]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yingtao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clanuwat_T/0/1/0/all/0/1"&gt;Tarin Clanuwat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_C/0/1/0/all/0/1"&gt;Chikahiko Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamoto_A/0/1/0/all/0/1"&gt;Asanobu Kitamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Dense Multi-Cable Knots. (arXiv:2106.02252v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.02252</id>
        <link href="http://arxiv.org/abs/2106.02252"/>
        <updated>2021-06-07T03:06:14.737Z</updated>
        <summary type="html"><![CDATA[Disentangling two or more cables requires many steps to remove crossings
between and within cables. We formalize the problem of disentangling multiple
cables and present an algorithm, Iterative Reduction Of Non-planar Multiple
cAble kNots (IRON-MAN), that outputs robot actions to remove crossings from
multi-cable knotted structures. We instantiate this algorithm with a learned
perception system, inspired by prior work in single-cable untying that given an
image input, can disentangle two-cable twists, three-cable braids, and knots of
two or three cables, such as overhand, square, carrick bend, sheet bend, crown,
and fisherman's knots. IRON-MAN keeps track of task-relevant keypoints
corresponding to target cable endpoints and crossings and iteratively
disentangles the cables by identifying and undoing crossings that are critical
to knot structure. Using a da Vinci surgical robot, we experimentally evaluate
the effectiveness of IRON-MAN on untangling multi-cable knots of types that
appear in the training data, as well as generalizing to novel classes of
multi-cable knots. Results suggest that IRON-MAN is effective in disentangling
knots involving up to three cables with 80.5% success and generalizing to knot
types that are not present during training, with cables of both distinct or
identical colors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Viswanath_V/0/1/0/all/0/1"&gt;Vainavi Viswanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grannen_J/0/1/0/all/0/1"&gt;Jennifer Grannen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_P/0/1/0/all/0/1"&gt;Priya Sundaresan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1"&gt;Ashwin Balakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novoseller_E/0/1/0/all/0/1"&gt;Ellen Novoseller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1"&gt;Jeffrey Ichnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1"&gt;Michael Laskey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series Forecasting with Regime Switching. (arXiv:2106.02329v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02329</id>
        <link href="http://arxiv.org/abs/2106.02329"/>
        <updated>2021-06-07T03:06:14.730Z</updated>
        <summary type="html"><![CDATA[We propose a deep switching state space model (DS$^3$M) for efficient
inference and forecasting of nonlinear time series with irregularly switching
among various regimes. The switching among regimes is captured by both discrete
and continuous latent variables with recurrent neural networks. The model is
estimated with variational inference using a reparameterization trick. We test
the approach on a variety of simulated and real datasets. In all cases, DS$^3$M
achieves competitive performance compared to several state-of-the-art methods
(e.g. GRU, SRNN, DSARF, SNLDS), with superior forecasting accuracy, convincing
interpretability of the discrete latent variables, and powerful representation
of the continuous latent variables for different kinds of time series.
Specifically, the MAPE values increase by 0.09\% to 15.71\% against the
second-best performing alternative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiuqin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Multi-Armed Bandits with Unrestricted Delay Distributions. (arXiv:2106.02436v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02436</id>
        <link href="http://arxiv.org/abs/2106.02436"/>
        <updated>2021-06-07T03:06:14.724Z</updated>
        <summary type="html"><![CDATA[We study the stochastic Multi-Armed Bandit (MAB) problem with random delays
in the feedback received by the algorithm. We consider two settings: the
reward-dependent delay setting, where realized delays may depend on the
stochastic rewards, and the reward-independent delay setting. Our main
contribution is algorithms that achieve near-optimal regret in each of the
settings, with an additional additive dependence on the quantiles of the delay
distribution. Our results do not make any assumptions on the delay
distributions: in particular, we do not assume they come from any parametric
family of distributions and allow for unbounded support and expectation; we
further allow for infinite delays where the algorithm might occasionally not
observe any feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lancewicki_T/0/1/0/all/0/1"&gt;Tal Lancewicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segal_S/0/1/0/all/0/1"&gt;Shahar Segal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subdivision-Based Mesh Convolution Networks. (arXiv:2106.02285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02285</id>
        <link href="http://arxiv.org/abs/2106.02285"/>
        <updated>2021-06-07T03:06:14.684Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have made great breakthroughs in 2D
computer vision. However, the irregular structure of meshes makes it hard to
exploit the power of CNNs directly. A subdivision surface provides a
hierarchical multi-resolution structure, and each face in a closed 2-manifold
triangle mesh is exactly adjacent to three faces. Motivated by these two
properties, this paper introduces a novel and flexible CNN framework, named
SubdivNet, for 3D triangle meshes with Loop subdivision sequence connectivity.
Making an analogy between mesh faces and pixels in a 2D image allows us to
present a mesh convolution operator to aggregate local features from adjacent
faces. By exploiting face neighborhoods, this convolution can support standard
2D convolutional network concepts, e.g. variable kernel size, stride, and
dilation. Based on the multi-resolution hierarchy, we propose a spatial uniform
pooling layer which merges four faces into one and an upsampling method which
splits one face into four. As a result, many popular 2D CNN architectures can
be readily adapted to processing 3D meshes. Meshes with arbitrary connectivity
can be remeshed to hold Loop subdivision sequence connectivity via
self-parameterization, making SubdivNet a general approach. Experiments on mesh
classification, segmentation, correspondence, and retrieval from the real-world
demonstrate the effectiveness and efficiency of SubdivNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shi-Min Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheng-Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Meng-Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jun-Xiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiahui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tai-Jiang Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Ralph R. Martin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL-DARTS: Differentiable Architecture Search for Reinforcement Learning. (arXiv:2106.02229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02229</id>
        <link href="http://arxiv.org/abs/2106.02229"/>
        <updated>2021-06-07T03:06:14.659Z</updated>
        <summary type="html"><![CDATA[We introduce RL-DARTS, one of the first applications of Differentiable
Architecture Search (DARTS) in reinforcement learning (RL) to search for
convolutional cells, applied to the Procgen benchmark. We outline the initial
difficulties of applying neural architecture search techniques in RL, and
demonstrate that by simply replacing the image encoder with a DARTS supernet,
our search method is sample-efficient, requires minimal extra compute
resources, and is also compatible with off-policy and on-policy RL algorithms,
needing only minor changes in preexisting code. Surprisingly, we find that the
supernet can be used as an actor for inference to generate replay data in
standard RL training loops, and thus train end-to-end. Throughout this training
process, we show that the supernet gradually learns better cells, leading to
alternative architectures which can be highly competitive against manually
designed policies, but also verify previous design choices for RL policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1"&gt;Yingjie Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Daiyi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1"&gt;Summer Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1"&gt;Eugene Brevdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1"&gt;Aleksandra Faust&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Learning-based Optimal Market Bidding Strategy for Price-Maker Energy Storage. (arXiv:2106.02396v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02396</id>
        <link href="http://arxiv.org/abs/2106.02396"/>
        <updated>2021-06-07T03:06:14.610Z</updated>
        <summary type="html"><![CDATA[Load serving entities with storage units reach sizes and performances that
can significantly impact clearing prices in electricity markets. Nevertheless,
price endogeneity is rarely considered in storage bidding strategies and
modeling the electricity market is a challenging task. Meanwhile, model-free
reinforcement learning such as the Actor-Critic are becoming increasingly
popular for designing energy system controllers. Yet implementation frequently
requires lengthy, data-intense, and unsafe trial-and-error training. To fill
these gaps, we implement an online Supervised Actor-Critic (SAC) algorithm,
supervised with a model-based controller -- Model Predictive Control (MPC). The
energy storage agent is trained with this algorithm to optimally bid while
learning and adjusting to its impact on the market clearing prices. We compare
the supervised Actor-Critic algorithm with the MPC algorithm as a supervisor,
finding that the former reaps higher profits via learning. Our contribution,
thus, is an online and safe SAC algorithm that outperforms the current
model-based state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Badoual_M/0/1/0/all/0/1"&gt;Mathilde D. Badoual&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moura_S/0/1/0/all/0/1"&gt;Scott J. Moura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Barlow Twins: A self-supervised representation learning framework for graphs. (arXiv:2106.02466v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02466</id>
        <link href="http://arxiv.org/abs/2106.02466"/>
        <updated>2021-06-07T03:06:14.601Z</updated>
        <summary type="html"><![CDATA[The self-supervised learning (SSL) paradigm is an essential exploration area,
which tries to eliminate the need for expensive data labeling. Despite the
great success of SSL methods in computer vision and natural language
processing, most of them employ contrastive learning objectives that require
negative samples, which are hard to define. This becomes even more challenging
in the case of graphs and is a bottleneck for achieving robust representations.
To overcome such limitations, we propose a framework for self-supervised graph
representation learning -- Graph Barlow Twins, which utilizes a
cross-correlation-based loss function instead of negative samples. Moreover, it
does not rely on non-symmetric neural network architectures -- in contrast to
state-of-the-art self-supervised graph representation learning method BGRL. We
show that our method achieves as competitive results as BGRL, best
self-supervised methods, and fully supervised ones while requiring
substantially fewer hyperparameters and converging in an order of magnitude
training steps earlier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bielak_P/0/1/0/all/0/1"&gt;Piotr Bielak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1"&gt;Tomasz Kajdanowicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1"&gt;Nitesh V. Chawla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning. (arXiv:2106.02584v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02584</id>
        <link href="http://arxiv.org/abs/2106.02584"/>
        <updated>2021-06-07T03:06:14.434Z</updated>
        <summary type="html"><![CDATA[We challenge a common assumption underlying most supervised deep learning:
that a model makes a prediction depending only on its parameters and the
features of a single input. To this end, we introduce a general-purpose deep
learning architecture that takes as input the entire dataset instead of
processing one datapoint at a time. Our approach uses self-attention to reason
about relationships between datapoints explicitly, which can be seen as
realizing non-parametric models using parametric attention mechanisms. However,
unlike conventional non-parametric models, we let the model learn end-to-end
from the data how to make use of other datapoints for prediction. Empirically,
our models solve cross-datapoint lookup and complex reasoning tasks unsolvable
by traditional deep learning models. We show highly competitive results on
tabular data, early results on CIFAR-10, and give insight into how the model
makes use of the interactions between points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kossen_J/0/1/0/all/0/1"&gt;Jannik Kossen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1"&gt;Neil Band&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyle_C/0/1/0/all/0/1"&gt;Clare Lyle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1"&gt;Aidan N. Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRIBO: Robust Deep Reinforcement Learning via Multi-View Information Bottleneck. (arXiv:2102.13268v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13268</id>
        <link href="http://arxiv.org/abs/2102.13268"/>
        <updated>2021-06-07T03:06:14.355Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning (DRL) agents are often sensitive to visual
changes that were unseen in their training environments. To address this
problem, we leverage the sequential nature of RL to learn robust
representations that encode only task-relevant information from observations
based on the unsupervised multi-view setting. Specifically, we introduce an
auxiliary objective based on the multi-view in-formation bottleneck (MIB)
principle which quantifies the amount of task-irrelevant information and
encourages learning representations that are both predictive of the future and
less sensitive to task-irrelevant distractions. This enables us to train
high-performance policies that are robust to visual distractions and can
generalize to unseen environments. We demonstrate that our approach can achieve
SOTA performance on diverse visual control tasks on the DeepMind Control Suite,
even when the background is replaced with natural videos. In addition, we show
that our approach outperforms well-established baselines for generalization to
unseen environments on the Procgen benchmark. Our code is open-sourced and
available at https://github.com/JmfanBU/DRIBO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiameng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenchao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions. (arXiv:2106.02619v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02619</id>
        <link href="http://arxiv.org/abs/2106.02619"/>
        <updated>2021-06-07T03:06:14.330Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are among the most successful models
for learning high-complexity, real-world distributions. However, in theory, due
to the highly non-convex, non-concave landscape of the minmax training
objective, GAN remains one of the least understood deep learning models. In
this work, we formally study how GANs can efficiently learn certain
hierarchically generated distributions that are close to the distribution of
images in practice. We prove that when a distribution has a structure that we
refer to as Forward Super-Resolution, then simply training generative
adversarial networks using gradient descent ascent (GDA) can indeed learn this
distribution efficiently, both in terms of sample and time complexities. We
also provide concrete empirical evidence that not only our assumption "forward
super-resolution" is very natural in practice, but also the underlying learning
mechanisms that we study in this paper (to allow us efficiently train GAN via
GDA in theory) simulates the actual learning process of GANs in practice on
real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1"&gt;Zeyuan Allen-Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Contextual Learners for Protein Networks. (arXiv:2106.02246v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02246</id>
        <link href="http://arxiv.org/abs/2106.02246"/>
        <updated>2021-06-07T03:06:14.321Z</updated>
        <summary type="html"><![CDATA[Spatial context is central to understanding health and disease. Yet reference
protein interaction networks lack such contextualization, thereby limiting the
study of where protein interactions likely occur in the human body.
Contextualized protein interactions could better characterize genes with
disease-specific interactions and elucidate diseases' manifestation in specific
cell types. Here, we introduce AWARE, a graph neural message passing approach
to inject cellular and tissue context into protein embeddings. AWARE optimizes
for a multi-scale embedding space, whose structure reflects the topology of
cell type specific networks. We construct a multi-scale network of the Human
Cell Atlas and apply AWARE to learn protein, cell type, and tissue embeddings
that uphold cell type and tissue hierarchies. We demonstrate AWARE on the novel
task of predicting whether a gene is associated with a disease and where it
most likely manifests in the human body. AWARE embeddings outperform global
embeddings by at least 12.5%, highlighting the importance of contextual
learners for protein networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Michelle M. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1"&gt;Marinka Zitnik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAFLOW: Conditional Autoregressive Flows. (arXiv:2106.02531v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02531</id>
        <link href="http://arxiv.org/abs/2106.02531"/>
        <updated>2021-06-07T03:06:14.264Z</updated>
        <summary type="html"><![CDATA[We introduce CAFLOW, a new diverse image-to-image translation model that
simultaneously leverages the power of auto-regressive modeling and the modeling
efficiency of conditional normalizing flows. We transform the conditioning
image into a sequence of latent encodings using a multi-scale normalizing flow
and repeat the process for the conditioned image. We model the conditional
distribution of the latent encodings by modeling the auto-regressive
distributions with an efficient multi-scale normalizing flow, where each
conditioning factor affects image synthesis at its respective resolution scale.
Our proposed framework performs well on a range of image-to-image translation
tasks. It outperforms former designs of conditional flows because of its
expressive auto-regressive structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batzolis_G/0/1/0/all/0/1"&gt;Georgios Batzolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carioni_M/0/1/0/all/0/1"&gt;Marcello Carioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etmann_C/0/1/0/all/0/1"&gt;Christian Etmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Afyouni_S/0/1/0/all/0/1"&gt;Soroosh Afyouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kourtzi_Z/0/1/0/all/0/1"&gt;Zoe Kourtzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Text Modeling through Short Run Inference. (arXiv:2106.02513v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02513</id>
        <link href="http://arxiv.org/abs/2106.02513"/>
        <updated>2021-06-07T03:06:14.221Z</updated>
        <summary type="html"><![CDATA[Latent variable models for text, when trained successfully, accurately model
the data distribution and capture global semantic and syntactic features of
sentences. The prominent approach to train such models is variational
autoencoders (VAE). It is nevertheless challenging to train and often results
in a trivial local optimum where the latent variable is ignored and its
posterior collapses into the prior, an issue known as posterior collapse.
Various techniques have been proposed to mitigate this issue. Most of them
focus on improving the inference model to yield latent codes of higher quality.
The present work proposes a short run dynamics for inference. It is initialized
from the prior distribution of the latent variable and then runs a small number
(e.g., 20) of Langevin dynamics steps guided by its posterior distribution. The
major advantage of our method is that it does not require a separate inference
model or assume simple geometry of the posterior distribution, thus rendering
an automatic, natural and flexible inference engine. We show that the models
trained with short run dynamics more accurately model the data, compared to
strong language model and VAE baselines, and exhibit no sign of posterior
collapse. Analyses of the latent space show that interpolation in the latent
space is able to generate coherent sentences with smooth transition and
demonstrate improved classification over strong baselines with latent features
from unsupervised pretraining. These results together expose a well-structured
latent space of our generative model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1"&gt;Erik Nijkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Tian Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Nian Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Influence of cognitive, geographical, and collaborative proximity on knowledge production of Canadian nanotechnology. (arXiv:2106.02110v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02110</id>
        <link href="http://arxiv.org/abs/2106.02110"/>
        <updated>2021-06-07T03:06:14.212Z</updated>
        <summary type="html"><![CDATA[Incorporating existing knowledge is vital for innovating, discovering, and
generating new ideas. Knowledge production through research and invention is
the key to scientific and technological development. As an emerging technology,
nanotechnology has already proved its great potential for the global economy,
attracting considerable federal investments. Canada is reported as one of the
major players in producing nanotechnology research. In this paper, we focused
on the main drivers of knowledge production and diffusion by analyzing Canadian
nanotechnology researchers. We hypothesized that knowledge production in
Canadian nanotechnology is influenced by three key proximity factors, namely
cognitive, geographical, and collaborative. Using statistical analysis, social
network analysis, and machine learning techniques we comprehensively assessed
the influence of the proximity factors on academic knowledge production. Our
results not only prove a significant impact of the three key proximity factors
but also their predictive potential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Neira_E/0/1/0/all/0/1"&gt;Elva Luz Crespo Neira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ebadi_A/0/1/0/all/0/1"&gt;Ashkan Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Beaudry_C/0/1/0/all/0/1"&gt;Catherine Beaudry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schiffauerova_A/0/1/0/all/0/1"&gt;Andrea Schiffauerova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Adversarial and Conditional State Space Model for Imitation Learning. (arXiv:2001.11628v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.11628</id>
        <link href="http://arxiv.org/abs/2001.11628"/>
        <updated>2021-06-07T03:06:14.203Z</updated>
        <summary type="html"><![CDATA[State representation learning (SRL) in partially observable Markov decision
processes has been studied to learn abstract features of data useful for robot
control tasks. For SRL, acquiring domain-agnostic states is essential for
achieving efficient imitation learning. Without these states, imitation
learning is hampered by domain-dependent information useless for control.
However, existing methods fail to remove such disturbances from the states when
the data from experts and agents show large domain shifts. To overcome this
issue, we propose a domain-adversarial and conditional state space model
(DAC-SSM) that enables control systems to obtain domain-agnostic and task- and
dynamics-aware states. DAC-SSM jointly optimizes the state inference,
observation reconstruction, forward dynamics, and reward models. To remove
domain-dependent information from the states, the model is trained with domain
discriminators in an adversarial manner, and the reconstruction is conditioned
on domain labels. We experimentally evaluated the model predictive control
performance via imitation learning for continuous control of sparse reward
tasks in simulators and compared it with the performance of the existing SRL
method. The agents from DAC-SSM achieved performance comparable to experts and
more than twice the baselines. We conclude domain-agnostic states are essential
for imitation learning that has large domain shifts and can be obtained using
DAC-SSM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okumura_R/0/1/0/all/0/1"&gt;Ryo Okumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okada_M/0/1/0/all/0/1"&gt;Masashi Okada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1"&gt;Tadahiro Taniguchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularization and Reparameterization Avoid Vanishing Gradients in Sigmoid-Type Networks. (arXiv:2106.02260v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02260</id>
        <link href="http://arxiv.org/abs/2106.02260"/>
        <updated>2021-06-07T03:06:14.197Z</updated>
        <summary type="html"><![CDATA[Deep learning requires several design choices, such as the nodes' activation
functions and the widths, types, and arrangements of the layers. One
consideration when making these choices is the vanishing-gradient problem,
which is the phenomenon of algorithms getting stuck at suboptimal points due to
small gradients. In this paper, we revisit the vanishing-gradient problem in
the context of sigmoid-type activation. We use mathematical arguments to
highlight two different sources of the phenomenon, namely large individual
parameters and effects across layers, and to illustrate two simple remedies,
namely regularization and rescaling. We then demonstrate the effectiveness of
the two remedies in practice. In view of the vanishing-gradient problem being a
main reason why tanh and other sigmoid-type activation has become much less
popular than relu-type activation, our results bring sigmoid-type activation
back to the table.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ven_L/0/1/0/all/0/1"&gt;Leni Ven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1"&gt;Johannes Lederer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Price graphs: Utilizing the structural information of financial time series for stock prediction. (arXiv:2106.02522v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2106.02522</id>
        <link href="http://arxiv.org/abs/2106.02522"/>
        <updated>2021-06-07T03:06:14.186Z</updated>
        <summary type="html"><![CDATA[Stock prediction, with the purpose of forecasting the future price trends of
stocks, is crucial for maximizing profits from stock investments. While great
research efforts have been devoted to exploiting deep neural networks for
improved stock prediction, the existing studies still suffer from two major
issues. First, the long-range dependencies in time series are not sufficiently
captured. Second, the chaotic property of financial time series fundamentally
lowers prediction performance. In this study, we propose a novel framework to
address both issues regarding stock prediction. Specifically, in terms of
transforming time series into complex networks, we convert market price series
into graphs. Then, structural information, referring to associations among
temporal points and the node weights, is extracted from the mapped graphs to
resolve the problems regarding long-range dependencies and the chaotic
property. We take graph embeddings to represent the associations among temporal
points as the prediction model inputs. Node weights are used as a priori
knowledge to enhance the learning of temporal attention. The effectiveness of
our proposed framework is validated using real-world stock data, and our
approach obtains the best performance among several state-of-the-art
benchmarks. Moreover, in the conducted trading simulations, our framework
further obtains the highest cumulative profits. Our results supplement the
existing applications of complex network methods in the financial realm and
provide insightful implications for investment applications regarding decision
support in financial markets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junran Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xueyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Li_S/0/1/0/all/0/1"&gt;Shangzhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jichang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adam in Private: Secure and Fast Training of Deep Neural Networks with Adaptive Moment Estimation. (arXiv:2106.02203v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02203</id>
        <link href="http://arxiv.org/abs/2106.02203"/>
        <updated>2021-06-07T03:06:14.163Z</updated>
        <summary type="html"><![CDATA[Privacy-preserving machine learning (PPML) aims at enabling machine learning
(ML) algorithms to be used on sensitive data. We contribute to this line of
research by proposing a framework that allows efficient and secure evaluation
of full-fledged state-of-the-art ML algorithms via secure multi-party
computation (MPC). This is in contrast to most prior works, which substitute ML
algorithms with approximated "MPC-friendly" variants. A drawback of the latter
approach is that fine-tuning of the combined ML and MPC algorithms is required,
which might lead to less efficient algorithms or inferior quality ML. This is
an issue for secure deep neural networks (DNN) training in particular, as this
involves arithmetic algorithms thought to be "MPC-unfriendly", namely, integer
division, exponentiation, inversion, and square root. In this work, we propose
secure and efficient protocols for the above seemingly MPC-unfriendly
computations. Our protocols are three-party protocols in the honest-majority
setting, and we propose both passively secure and actively secure with abort
variants. A notable feature of our protocols is that they simultaneously
provide high accuracy and efficiency. This framework enables us to efficiently
and securely compute modern ML algorithms such as Adam and the softmax function
"as is", without resorting to approximations. As a result, we obtain secure DNN
training that outperforms state-of-the-art three-party systems; our full
training is up to 6.7 times faster than just the online phase of the recently
proposed FALCON@PETS'21 on a standard benchmark network. We further perform
measurements on real-world DNNs, AlexNet and VGG16. The performance of our
framework is up to a factor of about 12-14 faster for AlexNet and 46-48 faster
for VGG16 to achieve an accuracy of 70% and 75%, respectively, when compared to
FALCON.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Attrapadung_N/0/1/0/all/0/1"&gt;Nuttapong Attrapadung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamada_K/0/1/0/all/0/1"&gt;Koki Hamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikarashi_D/0/1/0/all/0/1"&gt;Dai Ikarashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kikuchi_R/0/1/0/all/0/1"&gt;Ryo Kikuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsuda_T/0/1/0/all/0/1"&gt;Takahiro Matsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishina_I/0/1/0/all/0/1"&gt;Ibuki Mishina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morita_H/0/1/0/all/0/1"&gt;Hiraku Morita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuldt_J/0/1/0/all/0/1"&gt;Jacob C. N. Schuldt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition. (arXiv:2106.02302v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02302</id>
        <link href="http://arxiv.org/abs/2106.02302"/>
        <updated>2021-06-07T03:06:14.152Z</updated>
        <summary type="html"><![CDATA[Integrating external language models (LMs) into end-to-end (E2E) models
remains a challenging task for domain-adaptive speech recognition. Recently,
internal language model estimation (ILME)-based LM fusion has shown significant
word error rate (WER) reduction from Shallow Fusion by subtracting a weighted
internal LM score from an interpolation of E2E model and external LM scores
during beam search. However, on different test sets, the optimal LM
interpolation weights vary over a wide range and have to be tuned extensively
on well-matched validation sets. In this work, we perform LM fusion in the
minimum WER (MWER) training of an E2E model to obviate the need for LM weights
tuning during inference. Besides MWER training with Shallow Fusion (MWER-SF),
we propose a novel MWER training with ILME (MWER-ILME) where the ILME-based
fusion is conducted to generate N-best hypotheses and their posteriors.
Additional gradient is induced when internal LM is engaged in MWER-ILME loss
computation. During inference, LM weights pre-determined in MWER training
enable robust LM integrations on test sets from different domains. Experimented
with 30K-hour trained transformer transducers, MWER-ILME achieves on average
8.8% and 5.8% relative WER reductions from MWER and MWER-SF training,
respectively, on 6 different test sets]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1"&gt;Naoyuki Kanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_G/0/1/0/all/0/1"&gt;Guoli Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_E/0/1/0/all/0/1"&gt;Eric Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yifan Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers. (arXiv:2012.05420v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05420</id>
        <link href="http://arxiv.org/abs/2012.05420"/>
        <updated>2021-06-07T03:06:14.139Z</updated>
        <summary type="html"><![CDATA[A recent numerical study observed that neural network classifiers enjoy a
large degree of symmetry in the penultimate layer. Namely, if $h(x) = Af(x) +b$
where $A$ is a linear map and $f$ is the output of the penultimate layer of the
network (after activation), then all data points $x_{i, 1}, \dots, x_{i, N_i}$
in a class $C_i$ are mapped to a single point $y_i$ by $f$ and the points $y_i$
are located at the vertices of a regular $k-1$-dimensional standard simplex in
a high-dimensional Euclidean space.

We explain this observation analytically in toy models for highly expressive
deep neural networks. In complementary examples, we demonstrate rigorously that
even the final output of the classifier $h$ is not uniform over data samples
from a class $C_i$ if $h$ is a shallow network (or if the deeper layers do not
bring the data samples into a convenient geometric configuration).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+E_W/0/1/0/all/0/1"&gt;Weinan E&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOCTOR: A Simple Method for Detecting Misclassification Errors. (arXiv:2106.02395v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02395</id>
        <link href="http://arxiv.org/abs/2106.02395"/>
        <updated>2021-06-07T03:06:14.132Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have shown to perform very well on large scale
object recognition problems and lead to widespread use for real-world
applications, including situations where DNN are implemented as "black boxes".
A promising approach to secure their use is to accept decisions that are likely
to be correct while discarding the others. In this work, we propose DOCTOR, a
simple method that aims to identify whether the prediction of a DNN classifier
should (or should not) be trusted so that, consequently, it would be possible
to accept it or to reject it. Two scenarios are investigated: Totally Black Box
(TBB) where only the soft-predictions are available and Partially Black Box
(PBB) where gradient-propagation to perform input pre-processing is allowed.
Empirically, we show that DOCTOR outperforms all state-of-the-art methods on
various well-known images and sentiment analysis datasets. In particular, we
observe a reduction of up to $4\%$ of the false rejection rate (FRR) in the PBB
scenario. DOCTOR can be applied to any pre-trained model, it does not require
prior information about the underlying dataset and is as simple as the simplest
available methods in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Granese_F/0/1/0/all/0/1"&gt;Federica Granese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanelli_M/0/1/0/all/0/1"&gt;Marco Romanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorla_D/0/1/0/all/0/1"&gt;Daniele Gorla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1"&gt;Catuscia Palamidessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Epidemic Forecasting and Community Risk Evaluation of COVID-19. (arXiv:2106.02094v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.02094</id>
        <link href="http://arxiv.org/abs/2106.02094"/>
        <updated>2021-06-07T03:06:14.126Z</updated>
        <summary type="html"><![CDATA[Pandemic control measures like lock-down, restrictions on restaurants and
gatherings, social-distancing have shown to be effective in curtailing the
spread of COVID-19. However, their sustained enforcement has negative economic
effects. To craft strategies and policies that reduce the hardship on the
people and the economy while being effective against the pandemic, authorities
need to understand the disease dynamics at the right geo-spatial granularity.
Considering factors like the hospitals' ability to handle the fluctuating
demands, evaluating various reopening scenarios, and accurate forecasting of
cases are vital to decision making. Towards this end, we present a flexible
end-to-end solution that seamlessly integrates public health data with tertiary
client data to accurately estimate the risk of reopening a community. At its
core lies a state-of-the-art prediction model that auto-captures changing
trends in transmission and mobility. Benchmarking against various published
baselines confirm the superiority of our forecasting algorithm. Combined with
the ability to extend to multiple client-specific requirements and perform
deductive reasoning through counter-factual analysis, this solution provides
actionable insights to multiple client domains ranging from government to
educational institutions, hospitals, and commercial establishments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_V/0/1/0/all/0/1"&gt;Vishrawas Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navalekar_S/0/1/0/all/0/1"&gt;Sayali Navalekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1"&gt;Pan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooley_R/0/1/0/all/0/1"&gt;Ryan Hooley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;Jacob Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_R/0/1/0/all/0/1"&gt;Raman Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1"&gt;Ajay Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bianco_S/0/1/0/all/0/1"&gt;Simone Bianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufman_J/0/1/0/all/0/1"&gt;James H. Kaufman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using latent space regression to analyze and leverage compositionality in GANs. (arXiv:2103.10426v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10426</id>
        <link href="http://arxiv.org/abs/2103.10426"/>
        <updated>2021-06-07T03:06:14.106Z</updated>
        <summary type="html"><![CDATA[In recent years, Generative Adversarial Networks have become ubiquitous in
both research and public perception, but how GANs convert an unstructured
latent code to a high quality output is still an open question. In this work,
we investigate regression into the latent space as a probe to understand the
compositional properties of GANs. We find that combining the regressor and a
pretrained generator provides a strong image prior, allowing us to create
composite images from a collage of random image parts at inference time while
maintaining global consistency. To compare compositional properties across
different generators, we measure the trade-offs between reconstruction of the
unrealistic input and image quality of the regenerated samples. We find that
the regression approach enables more localized editing of individual image
parts compared to direct editing in the latent space, and we conduct
experiments to quantify this independence effect. Our method is agnostic to the
semantics of edits, and does not require labels or predefined concepts during
training. Beyond image composition, our method extends to a number of related
applications, such as image inpainting or example-based image editing, which we
demonstrate on several GANs and datasets, and because it uses only a single
forward pass, it can operate in real-time. Code is available on our project
page: https://chail.github.io/latent-composition/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1"&gt;Lucy Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wulff_J/0/1/0/all/0/1"&gt;Jonas Wulff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adiabatic Quantum Feature Selection for Sparse Linear Regression. (arXiv:2106.02357v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02357</id>
        <link href="http://arxiv.org/abs/2106.02357"/>
        <updated>2021-06-07T03:06:14.099Z</updated>
        <summary type="html"><![CDATA[Linear regression is a popular machine learning approach to learn and predict
real valued outputs or dependent variables from independent variables or
features. In many real world problems, its beneficial to perform sparse linear
regression to identify important features helpful in predicting the dependent
variable. It not only helps in getting interpretable results but also avoids
overfitting when the number of features is large, and the amount of data is
small. The most natural way to achieve this is by using `best subset selection'
which penalizes non-zero model parameters by adding $\ell_0$ norm over
parameters to the least squares loss. However, this makes the objective
function non-convex and intractable even for a small number of features. This
paper aims to address the intractability of sparse linear regression with
$\ell_0$ norm using adiabatic quantum computing, a quantum computing paradigm
that is particularly useful for solving optimization problems faster. We
formulate the $\ell_0$ optimization problem as a Quadratic Unconstrained Binary
Optimization (QUBO) problem and solve it using the D-Wave adiabatic quantum
computer. We study and compare the quality of QUBO solution on synthetic and
real world datasets. The results demonstrate the effectiveness of the proposed
adiabatic quantum computing approach in finding the optimal solution. The QUBO
solution matches the optimal solution for a wide range of sparsity penalty
values across the datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desu_S/0/1/0/all/0/1"&gt;Surya Sai Teja Desu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srijith_P/0/1/0/all/0/1"&gt;P.K. Srijith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1"&gt;M.V. Panduranga Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivadasan_N/0/1/0/all/0/1"&gt;Naveen Sivadasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes. (arXiv:2106.02524v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02524</id>
        <link href="http://arxiv.org/abs/2106.02524"/>
        <updated>2021-06-07T03:06:14.093Z</updated>
        <summary type="html"><![CDATA[Continuity of care is crucial to ensuring positive health outcomes for
patients discharged from an inpatient hospital setting, and improved
information sharing can help. To share information, caregivers write discharge
notes containing action items to share with patients and their future
caregivers, but these action items are easily lost due to the lengthiness of
the documents. In this work, we describe our creation of a dataset of clinical
action items annotated over MIMIC-III, the largest publicly available dataset
of real clinical notes. This dataset, which we call CLIP, is annotated by
physicians and covers 718 documents representing 100K sentences. We describe
the task of extracting the action items from these documents as multi-aspect
extractive summarization, with each aspect representing a type of action to be
taken. We evaluate several machine learning models on this task, and show that
the best models exploit in-domain language model pre-training on 59K
unannotated documents, and incorporate context from neighboring sentences. We
also propose an approach to pre-training data selection that allows us to
explore the trade-off between size and domain-specificity of pre-training
datasets for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mullenbach_J/0/1/0/all/0/1"&gt;James Mullenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1"&gt;Yada Pruksachatkun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1"&gt;Sean Adler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seale_J/0/1/0/all/0/1"&gt;Jennifer Seale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swartz_J/0/1/0/all/0/1"&gt;Jordan Swartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKelvey_T/0/1/0/all/0/1"&gt;T. Greg McKelvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online reinforcement learning with sparse rewards through an active inference capsule. (arXiv:2106.02390v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02390</id>
        <link href="http://arxiv.org/abs/2106.02390"/>
        <updated>2021-06-07T03:06:14.085Z</updated>
        <summary type="html"><![CDATA[Intelligent agents must pursue their goals in complex environments with
partial information and often limited computational capacity. Reinforcement
learning methods have achieved great success by creating agents that optimize
engineered reward functions, but which often struggle to learn in sparse-reward
environments, generally require many environmental interactions to perform
well, and are typically computationally very expensive. Active inference is a
model-based approach that directs agents to explore uncertain states while
adhering to a prior model of their goal behaviour. This paper introduces an
active inference agent which minimizes the novel free energy of the expected
future. Our model is capable of solving sparse-reward problems with a very high
sample efficiency due to its objective function, which encourages directed
exploration of uncertain states. Moreover, our model is computationally very
light and can operate in a fully online manner while achieving comparable
performance to offline RL methods. We showcase the capabilities of our model by
solving the mountain car problem, where we demonstrate its superior exploration
properties and its robustness to observation noise, which in fact improves
performance. We also introduce a novel method for approximating the prior model
from the reward function, which simplifies the expression of complex objectives
and improves performance over previous active inference approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noel_A/0/1/0/all/0/1"&gt;Alejandro Daniel Noel&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Hoof_C/0/1/0/all/0/1"&gt;Charel van Hoof&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Millidge_B/0/1/0/all/0/1"&gt;Beren Millidge&lt;/a&gt; (2) ((1) Delft University of Technology, (2) University of Oxford)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Earth Mover's Pinball Loss: Quantiles for Histogram-Valued Regression. (arXiv:2106.02051v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02051</id>
        <link href="http://arxiv.org/abs/2106.02051"/>
        <updated>2021-06-07T03:06:14.079Z</updated>
        <summary type="html"><![CDATA[Although ubiquitous in the sciences, histogram data have not received much
attention by the Deep Learning community. Whilst regression and classification
tasks for scalar and vector data are routinely solved by neural networks, a
principled approach for estimating histogram labels as a function of an input
vector or image is lacking in the literature. We present a dedicated method for
Deep Learning-based histogram regression, which incorporates cross-bin
information and yields distributions over possible histograms, expressed by
$\tau$-quantiles of the cumulative histogram in each bin. The crux of our
approach is a new loss function obtained by applying the pinball loss to the
cumulative histogram, which for 1D histograms reduces to the Earth Mover's
distance (EMD) in the special case of the median ($\tau = 0.5$), and
generalizes it to arbitrary quantiles. We validate our method with an
illustrative toy example, a football-related task, and an astrophysical
computer vision problem. We show that with our loss function, the accuracy of
the predicted median histograms is very similar to the standard EMD case (and
higher than for per-bin loss functions such as cross-entropy), while the
predictions become much more informative at almost no additional computational
cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+List_F/0/1/0/all/0/1"&gt;Florian List&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to select and use tools? : Active Perception of Target Objects Using Multimodal Deep Learning. (arXiv:2106.02445v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.02445</id>
        <link href="http://arxiv.org/abs/2106.02445"/>
        <updated>2021-06-07T03:06:14.061Z</updated>
        <summary type="html"><![CDATA[Selection of appropriate tools and use of them when performing daily tasks is
a critical function for introducing robots for domestic applications. In
previous studies, however, adaptability to target objects was limited, making
it difficult to accordingly change tools and adjust actions. To manipulate
various objects with tools, robots must both understand tool functions and
recognize object characteristics to discern a tool-object-action relation. We
focus on active perception using multimodal sensorimotor data while a robot
interacts with objects, and allow the robot to recognize their extrinsic and
intrinsic characteristics. We construct a deep neural networks (DNN) model that
learns to recognize object characteristics, acquires tool-object-action
relations, and generates motions for tool selection and handling. As an example
tool-use situation, the robot performs an ingredients transfer task, using a
turner or ladle to transfer an ingredient from a pot to a bowl. The results
confirm that the robot recognizes object characteristics and servings even when
the target ingredients are unknown. We also examine the contributions of
images, force, and tactile data and show that learning a variety of multimodal
information results in rich perception for tool use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saito_N/0/1/0/all/0/1"&gt;Namiko Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogata_T/0/1/0/all/0/1"&gt;Tetsuya Ogata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funabashi_S/0/1/0/all/0/1"&gt;Satoshi Funabashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_H/0/1/0/all/0/1"&gt;Hiroki Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugano_S/0/1/0/all/0/1"&gt;Shigeki Sugano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Perceptron Revisited: Computational-Statistical Tradeoffs. (arXiv:2106.02496v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02496</id>
        <link href="http://arxiv.org/abs/2106.02496"/>
        <updated>2021-06-07T03:06:14.055Z</updated>
        <summary type="html"><![CDATA[Quantum machine learning algorithms could provide significant speed-ups over
their classical counterparts; however, whether they could also achieve good
generalization remains unclear. Recently, two quantum perceptron models which
give a quadratic improvement over the classical perceptron algorithm using
Grover's search have been proposed by Wiebe et al. arXiv:1602.04799 . While the
first model reduces the complexity with respect to the size of the training
set, the second one improves the bound on the number of mistakes made by the
perceptron. In this paper, we introduce a hybrid quantum-classical perceptron
algorithm with lower complexity and better generalization ability than the
classical perceptron. We show a quadratic improvement over the classical
perceptron in both the number of samples and the margin of the data. We derive
a bound on the expected error of the hypothesis returned by our algorithm,
which compares favorably to the one obtained with the classical online
perceptron. We use numerical experiments to illustrate the trade-off between
computational complexity and statistical accuracy in quantum perceptron
learning and discuss some of the key practical issues surrounding the
implementation of quantum perceptron models into near-term quantum devices,
whose practical implementation represents a serious challenge due to inherent
noise. However, the potential benefits make correcting this worthwhile.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Roget_M/0/1/0/all/0/1"&gt;Mathieu Roget&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Molfetta_G/0/1/0/all/0/1"&gt;Giuseppe Di Molfetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kadri_H/0/1/0/all/0/1"&gt;Hachem Kadri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02343</id>
        <link href="http://arxiv.org/abs/2106.02343"/>
        <updated>2021-06-07T03:06:14.049Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks built from deep convolutional neural networks
(GANs) lack the ability to exactly replicate the high-frequency components of
natural images. To alleviate this issue, we introduce two novel training
techniques called frequency dropping (F-Drop) and frequency matching (F-Match).
The key idea of F-Drop is to filter out unnecessary high-frequency components
from the input images of the discriminators. This simple modification prevents
the discriminators from being confused by perturbations of the high-frequency
components. In addition, F-Drop makes the GANs focus on fitting in the
low-frequency domain, in which there are the dominant components of natural
images. F-Match minimizes the difference between real and fake images in the
frequency domain for generating more realistic images. F-Match is implemented
as a regularization term in the objective functions of the generators; it
penalizes the batch mean error in the frequency domain. F-Match helps the
generators to fit in the high-frequency domain filtered out by F-Drop to the
real image. We experimentally demonstrate that the combination of F-Drop and
F-Match improves the generative performance of GANs in both the frequency and
spatial domain on multiple image benchmarks (CIFAR, TinyImageNet, STL-10,
CelebA, and ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiasing a First-order Heuristic for Approximate Bi-level Optimization. (arXiv:2106.02487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02487</id>
        <link href="http://arxiv.org/abs/2106.02487"/>
        <updated>2021-06-07T03:06:14.041Z</updated>
        <summary type="html"><![CDATA[Approximate bi-level optimization (ABLO) consists of (outer-level)
optimization problems, involving numerical (inner-level) optimization loops.
While ABLO has many applications across deep learning, it suffers from time and
memory complexity proportional to the length $r$ of its inner optimization
loop. To address this complexity, an earlier first-order method (FOM) was
proposed as a heuristic that omits second derivative terms, yielding
significant speed gains and requiring only constant memory. Despite FOM's
popularity, there is a lack of theoretical understanding of its convergence
properties. We contribute by theoretically characterizing FOM's gradient bias
under mild assumptions. We further demonstrate a rich family of examples where
FOM-based SGD does not converge to a stationary point of the ABLO objective. We
address this concern by proposing an unbiased FOM (UFOM) enjoying constant
memory complexity as a function of $r$. We characterize the introduced
time-variance tradeoff, demonstrate convergence bounds, and find an optimal
UFOM for a given ABLO problem. Finally, we propose an efficient adaptive UFOM
scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1"&gt;Valerii Likhosherstov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;Jared Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-07T03:06:14.035Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fre-GAN: Adversarial Frequency-consistent Audio Synthesis. (arXiv:2106.02297v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02297</id>
        <link href="http://arxiv.org/abs/2106.02297"/>
        <updated>2021-06-07T03:06:14.028Z</updated>
        <summary type="html"><![CDATA[Although recent works on neural vocoder have improved the quality of
synthesized audio, there still exists a gap between generated and ground-truth
audio in frequency space. This difference leads to spectral artifacts such as
hissing noise or robotic sound, and thus degrades the sample quality. In this
paper, we propose Fre-GAN which achieves frequency-consistent audio synthesis
with highly improved generation quality. Specifically, we first present
resolution-connected generator and resolution-wise discriminators, which help
learn various scales of spectral distributions over multiple frequency bands.
Additionally, to reproduce high-frequency components accurately, we leverage
discrete wavelet transform in the discriminators. From our experiments, Fre-GAN
achieves high-fidelity waveform generation with a gap of only 0.03 MOS compared
to ground-truth audio while outperforming standard models in quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1"&gt;Ji-Hoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sang-Hoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1"&gt;Ji-Hyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Noisy Short Signal Camouflage in Multi-Domain Environment Decision-Making. (arXiv:2106.02044v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02044</id>
        <link href="http://arxiv.org/abs/2106.02044"/>
        <updated>2021-06-07T03:06:14.010Z</updated>
        <summary type="html"><![CDATA[Data transmission between two or more digital devices in industry and
government demands secure and agile technology. Digital information
distribution often requires deployment of Internet of Things (IoT) devices and
Data Fusion techniques which have also gained popularity in both, civilian and
military environments, such as, emergence of Smart Cities and Internet of
Battlefield Things (IoBT). This usually requires capturing and consolidating
data from multiple sources. Because datasets do not necessarily originate from
identical sensors, fused data typically results in a complex Big Data problem.
Due to potentially sensitive nature of IoT datasets, Blockchain technology is
used to facilitate secure sharing of IoT datasets, which allows digital
information to be distributed, but not copied. However, blockchain has several
limitations related to complexity, scalability, and excessive energy
consumption. We propose an approach to hide information (sensor signal) by
transforming it to an image or an audio signal. In one of the latest attempts
to the military modernization, we investigate sensor fusion approach by
investigating the challenges of enabling an intelligent identification and
detection operation and demonstrates the feasibility of the proposed Deep
Learning and Anomaly Detection models that can support future application for
specific hand gesture alert system from wearable devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush K. Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next Generation Multitarget Trackers: Random Finite Set Methods vs Transformer-based Deep Learning. (arXiv:2104.00734v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00734</id>
        <link href="http://arxiv.org/abs/2104.00734"/>
        <updated>2021-06-07T03:06:14.004Z</updated>
        <summary type="html"><![CDATA[Multitarget Tracking (MTT) is the problem of tracking the states of an
unknown number of objects using noisy measurements, with important applications
to autonomous driving, surveillance, robotics, and others. In the model-based
Bayesian setting, there are conjugate priors that enable us to express the
multi-object posterior in closed form, which could theoretically provide
Bayes-optimal estimates. However, the posterior involves a super-exponential
growth of the number of hypotheses over time, forcing state-of-the-art methods
to resort to approximations for remaining tractable, which can impact their
performance in complex scenarios. Model-free methods based on deep-learning
provide an attractive alternative, as they can, in principle, learn the optimal
filter from data, but to the best of our knowledge were never compared to
current state-of-the-art Bayesian filters, specially not in contexts where
accurate models are available. In this paper, we propose a high-performing
deep-learning method for MTT based on the Transformer architecture and compare
it to two state-of-the-art Bayesian filters, in a setting where we assume the
correct model is provided. Although this gives an edge to the model-based
filters, it also allows us to generate unlimited training data. We show that
the proposed model outperforms state-of-the-art Bayesian filters in complex
scenarios, while matching their performance in simpler cases, which validates
the applicability of deep-learning also in the model-based regime. The code for
all our implementations is made available at
https://github.com/JulianoLagana/MT3 .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1"&gt;Juliano Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hess_G/0/1/0/all/0/1"&gt;Georg Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ljungbergh_W/0/1/0/all/0/1"&gt;William Ljungbergh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yuxuan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1"&gt;Lennart Svensson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wymeersch_H/0/1/0/all/0/1"&gt;Henk Wymeersch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-Optimal Confidence Sequences for Bounded Random Variables. (arXiv:2006.05022v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05022</id>
        <link href="http://arxiv.org/abs/2006.05022"/>
        <updated>2021-06-07T03:06:13.996Z</updated>
        <summary type="html"><![CDATA[Many inference problems, such as sequential decision problems like A/B
testing, adaptive sampling schemes like bandit selection, are often online in
nature. The fundamental problem for online inference is to provide a sequence
of confidence intervals that are valid uniformly over the growing-into-infinity
sample sizes. To address this question, we provide a near-optimal confidence
sequence for bounded random variables by utilizing Bentkus' concentration
results. We show that it improves on the existing approaches that use the
Cram{\'e}r-Chernoff technique such as the Hoeffding, Bernstein, and Bennett
inequalities. The resulting confidence sequence is confirmed to be favorable in
both synthetic coverage problems and an application to adaptive stopping
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kuchibhotla_A/0/1/0/all/0/1"&gt;Arun Kumar Kuchibhotla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qinqing Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning via Persistency of Excitation. (arXiv:2106.02078v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02078</id>
        <link href="http://arxiv.org/abs/2106.02078"/>
        <updated>2021-06-07T03:06:13.988Z</updated>
        <summary type="html"><![CDATA[Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a network is a parameter estimation problem.
In adaptive control theory, maintaining persistency of excitation (PoE) is
integral to ensuring convergence of parameter estimates in dynamical systems to
their robust optima. In this work, we show that network training using gradient
descent is equivalent to a dynamical system parameter estimation problem.
Leveraging this relationship, we prove a sufficient condition for PoE of
gradient descent is achieved when the learning rate is less than the inverse of
the Lipschitz constant of the gradient of loss function. We provide an
efficient technique for estimating the corresponding Lipschitz constant using
extreme value theory and demonstrate that by only scaling the learning rate
schedule we can increase adversarial accuracy by up to 15% on benchmark
datasets. Our approach also universally increases the adversarial accuracy by
0.1% to 0.3% in various state-of-the-art adversarially trained models on the
AutoAttack benchmark, where every small margin of improvement is significant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1"&gt;Kaustubh Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1"&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1"&gt;Insup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1"&gt;James Weimer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Musical Prosody-Driven Emotion Classification: Interpreting Vocalists Portrayal of Emotions Through Machine Learning. (arXiv:2106.02556v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.02556</id>
        <link href="http://arxiv.org/abs/2106.02556"/>
        <updated>2021-06-07T03:06:13.980Z</updated>
        <summary type="html"><![CDATA[The task of classifying emotions within a musical track has received
widespread attention within the Music Information Retrieval (MIR) community.
Music emotion recognition has traditionally relied on the use of acoustic
features, verbal features, and metadata-based filtering. The role of musical
prosody remains under-explored despite several studies demonstrating a strong
connection between prosody and emotion. In this study, we restrict the input of
traditional machine learning algorithms to the features of musical prosody.
Furthermore, our proposed approach builds upon the prior by classifying
emotions under an expanded emotional taxonomy, using the Geneva Wheel of
Emotion. We utilize a methodology for individual data collection from
vocalists, and personal ground truth labeling by the artist themselves. We
found that traditional machine learning algorithms when limited to the features
of musical prosody (1) achieve high accuracies for a single singer, (2)
maintain high accuracy when the dataset is expanded to multiple singers, and
(3) achieve high accuracies when trained on a reduced subset of the total
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nicholas_F/0/1/0/all/0/1"&gt;Farris Nicholas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brian_M/0/1/0/all/0/1"&gt;Model Brian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_S/0/1/0/all/0/1"&gt;Savery Richard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gil_W/0/1/0/all/0/1"&gt;Weinberg Gil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strategyproof Learning: Building Trustworthy User-Generated Datasets. (arXiv:2106.02398v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02398</id>
        <link href="http://arxiv.org/abs/2106.02398"/>
        <updated>2021-06-07T03:06:13.964Z</updated>
        <summary type="html"><![CDATA[Today's large-scale machine learning algorithms harness massive amounts of
user-generated data to train large models. However, especially in the context
of content recommendation with enormous social, economical and political
incentives to promote specific views, products or ideologies, strategic users
might be tempted to fabricate or mislabel data in order to bias algorithms in
their favor. Unfortunately, today's learning schemes strongly incentivize such
strategic data misreporting. This is a major concern, as it endangers the
trustworthiness of the entire training datasets, and questions the safety of
any algorithm trained on such datasets. In this paper, we show that, perhaps
surprisingly, incentivizing data misreporting is not a fatality. We propose the
first personalized collaborative learning framework, Licchavi, with provable
strategyproofness guarantees through a careful design of the underlying loss
function. Interestingly, we also prove that Licchavi is Byzantine resilient: it
tolerates a minority of users that provide arbitrary data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farhadkhani_S/0/1/0/all/0/1"&gt;Sadegh Farhadkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1"&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1"&gt;L&amp;#xea;-Nguy&amp;#xea;n Hoang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Adaptivity in Federated Learning: Convergence and Consistency. (arXiv:2106.02305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02305</id>
        <link href="http://arxiv.org/abs/2106.02305"/>
        <updated>2021-06-07T03:06:13.958Z</updated>
        <summary type="html"><![CDATA[The federated learning (FL) framework trains a machine learning model using
decentralized data stored at edge client devices by periodically aggregating
locally trained models. Popular optimization algorithms of FL use vanilla
(stochastic) gradient descent for both local updates at clients and global
updates at the aggregating server. Recently, adaptive optimization methods such
as AdaGrad have been studied for server updates. However, the effect of using
adaptive optimization methods for local updates at clients is not yet
understood. We show in both theory and practice that while local adaptive
methods can accelerate convergence, they can cause a non-vanishing solution
bias, where the final converged solution may be different from the stationary
point of the global objective function. We propose correction techniques to
overcome this inconsistency and complement the local adaptive methods for FL.
Extensive experiments on realistic federated training tasks show that the
proposed algorithms can achieve faster convergence and higher test accuracy
than the baselines without local adaptivity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1"&gt;Zachary Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1"&gt;Zachary Charles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Luyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1"&gt;Gauri Joshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COLD: Concurrent Loads Disaggregator for Non-Intrusive Load Monitoring. (arXiv:2106.02352v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.02352</id>
        <link href="http://arxiv.org/abs/2106.02352"/>
        <updated>2021-06-07T03:06:13.951Z</updated>
        <summary type="html"><![CDATA[The modern artificial intelligence techniques show the outstanding
performances in the field of Non-Intrusive Load Monitoring (NILM). However, the
problem related to the identification of a large number of appliances working
simultaneously is underestimated. One of the reasons is the absence of a
specific data. In this research we propose the Synthesizer of Normalized
Signatures (SNS) algorithm to simulate the aggregated consumption with up to 10
concurrent loads. The results show that the synthetic data provides the models
with at least as a powerful identification accuracy as the real-world
measurements. We have developed the neural architecture named Concurrent Loads
Disaggregator (COLD) which is relatively simple and easy to understand in
comparison to the previous approaches. Our model allows identifying from 1 to
10 appliances working simultaneously with mean F1-score 78.95%. The source code
of the experiments performed is available at
https://github.com/arx7ti/cold-nilm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kamyshev_I/0/1/0/all/0/1"&gt;Ilia Kamyshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kriukov_D/0/1/0/all/0/1"&gt;Dmitrii Kriukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gryazina_E/0/1/0/all/0/1"&gt;Elena Gryazina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution. (arXiv:2106.02295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02295</id>
        <link href="http://arxiv.org/abs/2106.02295"/>
        <updated>2021-06-07T03:06:13.945Z</updated>
        <summary type="html"><![CDATA[Model quantization is challenging due to many tedious hyper-parameters such
as precision (bitwidth), dynamic range (minimum and maximum discrete values)
and stepsize (interval between discrete values). Unlike prior arts that
carefully tune these values, we present a fully differentiable approach to
learn all of them, named Differentiable Dynamic Quantization (DDQ), which has
several benefits. (1) DDQ is able to quantize challenging lightweight
architectures like MobileNets, where different layers prefer different
quantization parameters. (2) DDQ is hardware-friendly and can be easily
implemented using low-precision matrix-vector multiplication, making it capable
in many hardware such as ARM. (3) Extensive experiments show that DDQ
outperforms prior arts on many networks and benchmarks, especially when models
are already efficient and compact. e.g., DDQ is the first approach that
achieves lossless 4-bit quantization for MobileNetV2 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhaoyang_Z/0/1/0/all/0/1"&gt;Zhang Zhaoyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenqi_S/0/1/0/all/0/1"&gt;Shao Wenqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jinwei_G/0/1/0/all/0/1"&gt;Gu Jinwei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiaogang_W/0/1/0/all/0/1"&gt;Wang Xiaogang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_L/0/1/0/all/0/1"&gt;Luo Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning domain-agnostic visual representation for computational pathology using medically-irrelevant style transfer augmentation. (arXiv:2102.01678v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01678</id>
        <link href="http://arxiv.org/abs/2102.01678"/>
        <updated>2021-06-07T03:06:13.938Z</updated>
        <summary type="html"><![CDATA[Suboptimal generalization of machine learning models on unseen data is a key
challenge which hampers the clinical applicability of such models to medical
imaging. Although various methods such as domain adaptation and domain
generalization have evolved to combat this challenge, learning robust and
generalizable representations is core to medical image understanding, and
continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation
for histoPathology), a form of data augmentation based on random style transfer
from non-medical style source such as artistic paintings, for learning
domain-agnostic visual representations in computational pathology. Style
transfer replaces the low-level texture content of an image with the
uninformative style of randomly selected style source image, while preserving
the original high-level semantic content. This improves robustness to domain
shift and can be used as a simple yet powerful tool for learning
domain-agnostic representations. We demonstrate that STRAP leads to
state-of-the-art performance, particularly in the presence of domain shifts, on
two particular classification tasks in computational pathology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yamashita_R/0/1/0/all/0/1"&gt;Rikiya Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_J/0/1/0/all/0/1"&gt;Jin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Banda_S/0/1/0/all/0/1"&gt;Snikitha Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jeanne Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferable and Distributed User Association Policies for 5G and Beyond Networks. (arXiv:2106.02540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02540</id>
        <link href="http://arxiv.org/abs/2106.02540"/>
        <updated>2021-06-07T03:06:13.932Z</updated>
        <summary type="html"><![CDATA[We study the problem of user association, namely finding the optimal
assignment of user equipment to base stations to achieve a targeted network
performance. In this paper, we focus on the knowledge transferability of
association policies. Indeed, traditional non-trivial user association schemes
are often scenario-specific or deployment-specific and require a policy
re-design or re-learning when the number or the position of the users change.
In contrast, transferability allows to apply a single user association policy,
devised for a specific scenario, to other distinct user deployments, without
needing a substantial re-learning or re-design phase and considerably reducing
its computational and management complexity. To achieve transferability, we
first cast user association as a multi-agent reinforcement learning problem.
Then, based on a neural attention mechanism that we specifically conceived for
this context, we propose a novel distributed policy network architecture, which
is transferable among users with zero-shot generalization capability i.e.,
without requiring additional training.Numerical results show the effectiveness
of our solution in terms of overall network communication rate, outperforming
centralized benchmarks even when the number of users doubles with respect to
the initial training point.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sana_M/0/1/0/all/0/1"&gt;Mohamed Sana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietro_N/0/1/0/all/0/1"&gt;Nicola di Pietro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strinati_E/0/1/0/all/0/1"&gt;Emilio Calvanese Strinati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape-Preserving Dimensionality Reduction : An Algorithm and Measures of Topological Equivalence. (arXiv:2106.02096v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02096</id>
        <link href="http://arxiv.org/abs/2106.02096"/>
        <updated>2021-06-07T03:06:13.913Z</updated>
        <summary type="html"><![CDATA[We introduce a linear dimensionality reduction technique preserving
topological features via persistent homology. The method is designed to find
linear projection $L$ which preserves the persistent diagram of a point cloud
$\mathbb{X}$ via simulated annealing. The projection $L$ induces a set of
canonical simplicial maps from the Rips (or \v{C}ech) filtration of
$\mathbb{X}$ to that of $L\mathbb{X}$. In addition to the distance between
persistent diagrams, the projection induces a map between filtrations, called
filtration homomorphism. Using the filtration homomorphism, one can measure the
difference between shapes of two filtrations directly comparing simplicial
complexes with respect to quasi-isomorphism $\mu_{\operatorname{quasi-iso}}$ or
strong homotopy equivalence $\mu_{\operatorname{equiv}}$. These
$\mu_{\operatorname{quasi-iso}}$ and $\mu_{\operatorname{equiv}}$ measures how
much portion of corresponding simplicial complexes is quasi-isomorphic or
homotopy equivalence respectively. We validate the effectiveness of our
framework with simple examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1"&gt;Byeongsu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+You_K/0/1/0/all/0/1"&gt;Kisung You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Hard Optimization Problems: A Data Generation Perspective. (arXiv:2106.02601v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.02601</id>
        <link href="http://arxiv.org/abs/2106.02601"/>
        <updated>2021-06-07T03:06:13.902Z</updated>
        <summary type="html"><![CDATA[Optimization problems are ubiquitous in our societies and are present in
almost every segment of the economy. Most of these optimization problems are
NP-hard and computationally demanding, often requiring approximate solutions
for large-scale instances. Machine learning frameworks that learn to
approximate solutions to such hard optimization problems are a potentially
promising avenue to address these difficulties, particularly when many closely
related problem instances must be solved repeatedly. Supervised learning
frameworks can train a model using the outputs of pre-solved instances.
However, when the outputs are themselves approximations, when the optimization
problem has symmetric solutions, and/or when the solver uses randomization,
solutions to closely related instances may exhibit large differences and the
learning task can become inherently more difficult. This paper demonstrates
this critical challenge, connects the volatility of the training data to the
ability of a model to approximate it, and proposes a method for producing
(exact or approximate) solutions to optimization problems that are more
amenable to supervised learning tasks. The effectiveness of the method is
tested on hard non-linear nonconvex and discrete combinatorial problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kotary_J/0/1/0/all/0/1"&gt;James Kotary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fioretto_F/0/1/0/all/0/1"&gt;Ferdinando Fioretto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hentenryck_P/0/1/0/all/0/1"&gt;Pascal Van Hentenryck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Slice-Aware Representations with Mixture of Attentions. (arXiv:2106.02363v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02363</id>
        <link href="http://arxiv.org/abs/2106.02363"/>
        <updated>2021-06-07T03:06:13.894Z</updated>
        <summary type="html"><![CDATA[Real-world machine learning systems are achieving remarkable performance in
terms of coarse-grained metrics like overall accuracy and F-1 score. However,
model improvement and development often require fine-grained modeling on
individual data subsets or slices, for instance, the data slices where the
models have unsatisfactory results. In practice, it gives tangible values for
developing such models that can pay extra attention to critical or interested
slices while retaining the original overall performance. This work extends the
recent slice-based learning (SBL)~\cite{chen2019slice} with a mixture of
attentions (MoA) to learn slice-aware dual attentive representations. We
empirically show that the MoA approach outperforms the baseline method as well
as the original SBL approach on monitored slices with two natural language
understanding (NLU) tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sunghyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Han Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Bum Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarikaya_R/0/1/0/all/0/1"&gt;Ruhi Sarikaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fluctuation-dissipation Type Theorem in Stochastic Linear Learning. (arXiv:2106.02220v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02220</id>
        <link href="http://arxiv.org/abs/2106.02220"/>
        <updated>2021-06-07T03:06:13.888Z</updated>
        <summary type="html"><![CDATA[The fluctuation-dissipation theorem (FDT) is a simple yet powerful
consequence of the first-order differential equation governing the dynamics of
systems subject simultaneously to dissipative and stochastic forces. The linear
learning dynamics, in which the input vector maps to the output vector by a
linear matrix whose elements are the subject of learning, has a stochastic
version closely mimicking the Langevin dynamics when a full-batch gradient
descent scheme is replaced by that of stochastic gradient descent. We derive a
generalized FDT for the stochastic linear learning dynamics and verify its
validity among the well-known machine learning data sets such as MNIST,
CIFAR-10 and EMNIST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1"&gt;Manhyung Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jeonghyeok Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1"&gt;Taewoong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jung Hoon Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning. (arXiv:2106.02097v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02097</id>
        <link href="http://arxiv.org/abs/2106.02097"/>
        <updated>2021-06-07T03:06:13.871Z</updated>
        <summary type="html"><![CDATA[We present an end-to-end, model-based deep reinforcement learning agent which
dynamically attends to relevant parts of its state, in order to plan and to
generalize better out-of-distribution. The agent's architecture uses a set
representation and a bottleneck mechanism, forcing the number of entities to
which the agent attends at each planning step to be small. In experiments with
customized MiniGrid environments with different dynamics, we observe that the
design allows agents to learn to plan effectively, by attending to the relevant
objects, leading to better out-of-distribution generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingde Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luan_S/0/1/0/all/0/1"&gt;Sitao Luan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Architecture Search via Bregman Iterations. (arXiv:2106.02479v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02479</id>
        <link href="http://arxiv.org/abs/2106.02479"/>
        <updated>2021-06-07T03:06:13.865Z</updated>
        <summary type="html"><![CDATA[We propose a novel strategy for Neural Architecture Search (NAS) based on
Bregman iterations. Starting from a sparse neural network our gradient-based
one-shot algorithm gradually adds relevant parameters in an inverse scale space
manner. This allows the network to choose the best architecture in the search
space which makes it well-designed for a given task, e.g., by adding neurons or
skip connections. We demonstrate that using our approach one can unveil, for
instance, residual autoencoders for denoising, deblurring, and classification
tasks. Code is available at https://github.com/TimRoith/BregmanLearning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bungert_L/0/1/0/all/0/1"&gt;Leon Bungert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roith_T/0/1/0/all/0/1"&gt;Tim Roith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenbrinck_D/0/1/0/all/0/1"&gt;Daniel Tenbrinck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burger_M/0/1/0/all/0/1"&gt;Martin Burger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Little Robustness Goes a Long Way: Leveraging Universal Features for Targeted Transfer Attacks. (arXiv:2106.02105v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02105</id>
        <link href="http://arxiv.org/abs/2106.02105"/>
        <updated>2021-06-07T03:06:13.858Z</updated>
        <summary type="html"><![CDATA[Adversarial examples for neural network image classifiers are known to be
transferable: examples optimized to be misclassified by a source classifier are
often misclassified as well by classifiers with different architectures.
However, targeted adversarial examples -- optimized to be classified as a
chosen target class -- tend to be less transferable between architectures.
While prior research on constructing transferable targeted attacks has focused
on improving the optimization procedure, in this work we examine the role of
the source classifier. Here, we show that training the source classifier to be
"slightly robust" -- that is, robust to small-magnitude adversarial examples --
substantially improves the transferability of targeted attacks, even between
architectures as different as convolutional neural networks and transformers.
We argue that this result supports a non-intuitive hypothesis: on the spectrum
from non-robust (standard) to highly robust classifiers, those that are only
slightly robust exhibit the most universal features -- ones that tend to
overlap with the features learned by other classifiers trained on the same
dataset. The results we present provide insight into the nature of adversarial
examples as well as the mechanisms underlying so-called "robust" classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Springer_J/0/1/0/all/0/1"&gt;Jacob M. Springer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1"&gt;Melanie Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenyon_G/0/1/0/all/0/1"&gt;Garrett T. Kenyon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Celebrating Diversity in Shared Multi-Agent Reinforcement Learning. (arXiv:2106.02195v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02195</id>
        <link href="http://arxiv.org/abs/2106.02195"/>
        <updated>2021-06-07T03:06:13.852Z</updated>
        <summary type="html"><![CDATA[Recently, deep multi-agent reinforcement learning (MARL) has shown the
promise to solve complex cooperative tasks. Its success is partly because of
parameter sharing among agents. However, such sharing may lead agents to behave
similarly and limit their coordination capacity. In this paper, we aim to
introduce diversity in both optimization and representation of shared
multi-agent reinforcement learning. Specifically, we propose an
information-theoretical regularization to maximize the mutual information
between agents' identities and their trajectories, encouraging extensive
exploration and diverse individualized behaviors. In representation, we
incorporate agent-specific modules in the shared neural network architecture,
which are regularized by L1-norm to promote learning sharing among agents while
keeping necessary diversity. Empirical results show that our method achieves
state-of-the-art performance on Google Research Football and super hard
StarCraft II micromanagement tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenghao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+WU_C/0/1/0/all/0/1"&gt;Chengjie WU&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tonghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qianchuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Surprising Power of Graph Neural Networks with Random Node Initialization. (arXiv:2010.01179v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01179</id>
        <link href="http://arxiv.org/abs/2010.01179"/>
        <updated>2021-06-07T03:06:13.844Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are effective models for representation learning
on relational data. However, standard GNNs are limited in their expressive
power, as they cannot distinguish graphs beyond the capability of the
Weisfeiler-Leman graph isomorphism heuristic. In order to break this
expressiveness barrier, GNNs have been enhanced with random node initialization
(RNI), where the idea is to train and run the models with randomized initial
node features. In this work, we analyze the expressive power of GNNs with RNI,
and prove that these models are universal, a first such result for GNNs not
relying on computationally demanding higher-order properties. This universality
result holds even with partially randomized initial node features, and
preserves the invariance properties of GNNs in expectation. We then empirically
analyze the effect of RNI on GNNs, based on carefully constructed datasets. Our
empirical findings support the superior performance of GNNs with RNI over
standard GNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abboud_R/0/1/0/all/0/1"&gt;Ralph Abboud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1"&gt;&amp;#x130;smail &amp;#x130;lkan Ceylan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1"&gt;Martin Grohe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The impact of using biased performance metrics on software defect prediction research. (arXiv:2103.10201v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10201</id>
        <link href="http://arxiv.org/abs/2103.10201"/>
        <updated>2021-06-07T03:06:13.837Z</updated>
        <summary type="html"><![CDATA[Context: Software engineering researchers have undertaken many experiments
investigating the potential of software defect prediction algorithms.
Unfortunately, some widely used performance metrics are known to be
problematic, most notably F1, but nevertheless F1 is widely used.

Objective: To investigate the potential impact of using F1 on the validity of
this large body of research.

Method: We undertook a systematic review to locate relevant experiments and
then extract all pairwise comparisons of defect prediction performance using F1
and the un-biased Matthews correlation coefficient (MCC).

Results: We found a total of 38 primary studies. These contain 12,471 pairs
of results. Of these, 21.95% changed direction when the MCC metric is used
instead of the biased F1 metric. Unfortunately, we also found evidence
suggesting that F1 remains widely used in software defect prediction research.

Conclusions: We reiterate the concerns of statisticians that the F1 is a
problematic metric outside of an information retrieval context, since we are
concerned about both classes (defect-prone and not defect-prone units). This
inappropriate usage has led to a substantial number (more than one fifth) of
erroneous (in terms of direction) results. Therefore we urge researchers to (i)
use an unbiased metric and (ii) publish detailed results including confusion
matrices such that alternative analyses become possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jingxiu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shepperd_M/0/1/0/all/0/1"&gt;Martin Shepperd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Semi-supervised Framework for Call Center Agent Malpractice Detection via Neural Feature Learning. (arXiv:2106.02433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02433</id>
        <link href="http://arxiv.org/abs/2106.02433"/>
        <updated>2021-06-07T03:06:13.820Z</updated>
        <summary type="html"><![CDATA[This work presents a practical solution to the problem of call center agent
malpractice. A semi-supervised framework comprising of non-linear power
transformation, neural feature learning and k-means clustering is outlined. We
put these building blocks together and tune the parameters so that the best
performance was obtained. The data used in the experiments is obtained from our
in-house call center. It is made up of recorded agent-customer conversations
which have been annotated using a convolutional neural network based segmenter.
The methods provided a means of tuning the parameters of the neural network to
achieve a desirable result. We show that, using our proposed framework, it is
possible to significantly reduce the malpractice classification error of a
k-means-only clustering model which would serve the same purpose. Additionally,
by presenting the amount of silence per call as a key performance indicator, we
show that the proposed system has enhanced agents performance at our call
center since deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iheme_L/0/1/0/all/0/1"&gt;Leonardo Obinna Iheme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Misinformation from Website Screenshots. (arXiv:2102.07849v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07849</id>
        <link href="http://arxiv.org/abs/2102.07849"/>
        <updated>2021-06-07T03:06:13.813Z</updated>
        <summary type="html"><![CDATA[Can the look and the feel of a website give information about the
trustworthiness of an article? In this paper, we propose to use a promising,
yet neglected aspect in detecting the misinformativeness: the overall look of
the domain webpage. To capture this overall look, we take screenshots of news
articles served by either misinformative or trustworthy web domains and
leverage a tensor decomposition based semi-supervised classification technique.
The proposed approach i.e., VizFake is insensitive to a number of image
transformations such as converting the image to grayscale, vectorizing the
image and losing some parts of the screenshots. VizFake leverages a very small
amount of known labels, mirroring realistic and practical scenarios, where
labels (especially for known misinformative articles), are scarce and quickly
become dated. The F1 score of VizFake on a dataset of 50k screenshots of news
articles spanning more than 500 domains is roughly 85% using only 5% of ground
truth labels. Furthermore, tensor representations of VizFake, obtained in an
unsupervised manner, allow for exploratory analysis of the data that provides
valuable insights into the problem. Finally, we compare VizFake with deep
transfer learning, since it is a very popular black-box approach for image
classification and also well-known text text-based methods. VizFake achieves
competitive accuracy with deep transfer learning models while being two orders
of magnitude faster and not requiring laborious hyper-parameter tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1"&gt;Sara Abdali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurav_R/0/1/0/all/0/1"&gt;Rutuja Gurav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_S/0/1/0/all/0/1"&gt;Siddharth Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_D/0/1/0/all/0/1"&gt;Daniel Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Entezari_N/0/1/0/all/0/1"&gt;Negin Entezari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Neil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1"&gt;Evangelos E. Papalexakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation. (arXiv:2106.02170v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02170</id>
        <link href="http://arxiv.org/abs/2106.02170"/>
        <updated>2021-06-07T03:06:13.788Z</updated>
        <summary type="html"><![CDATA[Automatic detection of phoneme or word-like units is one of the core
objectives in zero-resource speech processing. Recent attempts employ
self-supervised training methods, such as contrastive predictive coding (CPC),
where the next frame is predicted given past context. However, CPC only looks
at the audio signal's frame-level structure. We overcome this limitation with a
segmental contrastive predictive coding (SCPC) framework that can model the
signal structure at a higher level e.g. at the phoneme level. In this
framework, a convolutional neural network learns frame-level representation
from the raw waveform via noise-contrastive estimation (NCE). A differentiable
boundary detector finds variable-length segments, which are then used to
optimize a segment encoder via NCE to learn segment representations. The
differentiable boundary detector allows us to train frame-level and
segment-level encoders jointly. Typically, phoneme and word segmentation are
treated as separate tasks. We unify them and experimentally show that our
single model outperforms existing phoneme and word segmentation methods on
TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and
when is the right time to include the segmental loss in the learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhati_S/0/1/0/all/0/1"&gt;Saurabhchand Bhati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Villalba_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Villalba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr &amp;#x17b;elasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Velazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dehak_N/0/1/0/all/0/1"&gt;Najim Dehak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\ell_2$-norm Flow Diffusion in Near-Linear Time. (arXiv:2105.14629v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14629</id>
        <link href="http://arxiv.org/abs/2105.14629"/>
        <updated>2021-06-07T03:06:13.772Z</updated>
        <summary type="html"><![CDATA[Diffusion is a fundamental graph procedure and has been a basic building
block in a wide range of theoretical and empirical applications such as graph
partitioning and semi-supervised learning on graphs. In this paper, we study
computationally efficient diffusion primitives beyond random walk.

We design an $\widetilde{O}(m)$-time randomized algorithm for the
$\ell_2$-norm flow diffusion problem, a recently proposed diffusion model based
on network flow with demonstrated graph clustering related applications both in
theory and in practice. Examples include finding locally-biased low conductance
cuts. Using a known connection between the optimal dual solution of the flow
diffusion problem and the local cut structure, our algorithm gives an
alternative approach for finding such cuts in nearly linear time.

From a technical point of view, our algorithm contributes a novel way of
dealing with inequality constraints in graph optimization problems. It adapts
the high-level algorithmic framework of nearly linear time Laplacian system
solvers, but requires several new tools: vertex elimination under constraints,
a new family of graph ultra-sparsifiers, and accelerated proximal gradient
methods with inexact proximal mapping computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1"&gt;Richard Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme sparsity gives rise to functional specialization. (arXiv:2106.02626v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.02626</id>
        <link href="http://arxiv.org/abs/2106.02626"/>
        <updated>2021-06-07T03:06:13.761Z</updated>
        <summary type="html"><![CDATA[Modularity of neural networks -- both biological and artificial -- can be
thought of either structurally or functionally, and the relationship between
these is an open question. We show that enforcing structural modularity via
sparse connectivity between two dense sub-networks which need to communicate to
solve the task leads to functional specialization of the sub-networks, but only
at extreme levels of sparsity. With even a moderate number of interconnections,
the sub-networks become functionally entangled. Defining functional
specialization is in itself a challenging problem without a universally agreed
solution. To address this, we designed three different measures of
specialization (based on weight masks, retraining and correlation) and found
them to qualitatively agree. Our results have implications in both neuroscience
and machine learning. For neuroscience, it shows that we cannot conclude that
there is functional modularity simply by observing moderate levels of
structural modularity: knowing the brain's connectome is not sufficient for
understanding how it breaks down into functional modules. For machine learning,
using structure to promote functional modularity -- which may be important for
robustness and generalization -- may require extremely narrow bottlenecks
between modules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bena_G/0/1/0/all/0/1"&gt;Gabriel B&amp;#xe9;na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Goodman_D/0/1/0/all/0/1"&gt;Dan F. M. Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incomplete Graph Representation and Learning via Partial Graph Neural Networks. (arXiv:2003.10130v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10130</id>
        <link href="http://arxiv.org/abs/2003.10130"/>
        <updated>2021-06-07T03:06:13.648Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are gaining increasing attention on graph data
learning tasks in recent years. However, in many applications, graph may be
coming in an incomplete form where attributes of graph nodes are partially
unknown/missing. Existing GNNs are generally designed on complete graphs which
can not deal with attribute-incomplete graph data directly. To address this
problem, we develop a novel partial aggregation based GNNs, named Partial Graph
Neural Networks (PaGNNs), for attribute-incomplete graph representation and
learning. Our work is motivated by the observation that the neighborhood
aggregation function in standard GNNs can be equivalently viewed as the
neighborhood reconstruction formulation. Based on it, we define two novel
partial aggregation (reconstruction) functions on incomplete graph and derive
PaGNNs for incomplete graph data learning. Extensive experiments on several
datasets demonstrate the effectiveness and efficiency of the proposed PaGNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziyan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL. (arXiv:2106.02193v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02193</id>
        <link href="http://arxiv.org/abs/2106.02193"/>
        <updated>2021-06-07T03:06:13.624Z</updated>
        <summary type="html"><![CDATA[A highly desirable property of a reinforcement learning (RL) agent -- and a
major difficulty for deep RL approaches -- is the ability to generalize
policies learned on a few tasks over a high-dimensional observation space to
similar tasks not seen during training. Many promising approaches to this
challenge consider RL as a process of training two functions simultaneously: a
complex nonlinear encoder that maps high-dimensional observations to a latent
representation space, and a simple linear policy over this space. We posit that
a superior encoder for zero-shot generalization in RL can be trained by using
solely an auxiliary SSL objective if the training process encourages the
encoder to map behaviorally similar observations to similar representations, as
reward-based signal can cause overfitting in the encoder (Raileanu et al.,
2021). We propose Cross-Trajectory Representation Learning (CTRL), a method
that runs within an RL agent and conditions its encoder to recognize behavioral
similarity in observations by applying a novel SSL objective to pairs of
trajectories from the agent's policies. CTRL can be viewed as having the same
effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use
of rewards and associated overfitting risks. Our experiments ablate various
components of CTRL and demonstrate that in combination with PPO it achieves
better generalization performance on the challenging Procgen benchmark suite
(Cobbe et al., 2020).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazoure_B/0/1/0/all/0/1"&gt;Bogdan Mazoure&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Ahmed M. Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MacAlpine_P/0/1/0/all/0/1"&gt;Patrick MacAlpine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1"&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1"&gt;Andrey Kolobov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using latent space regression to analyze and leverage compositionality in GANs. (arXiv:2103.10426v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10426</id>
        <link href="http://arxiv.org/abs/2103.10426"/>
        <updated>2021-06-07T03:06:13.443Z</updated>
        <summary type="html"><![CDATA[In recent years, Generative Adversarial Networks have become ubiquitous in
both research and public perception, but how GANs convert an unstructured
latent code to a high quality output is still an open question. In this work,
we investigate regression into the latent space as a probe to understand the
compositional properties of GANs. We find that combining the regressor and a
pretrained generator provides a strong image prior, allowing us to create
composite images from a collage of random image parts at inference time while
maintaining global consistency. To compare compositional properties across
different generators, we measure the trade-offs between reconstruction of the
unrealistic input and image quality of the regenerated samples. We find that
the regression approach enables more localized editing of individual image
parts compared to direct editing in the latent space, and we conduct
experiments to quantify this independence effect. Our method is agnostic to the
semantics of edits, and does not require labels or predefined concepts during
training. Beyond image composition, our method extends to a number of related
applications, such as image inpainting or example-based image editing, which we
demonstrate on several GANs and datasets, and because it uses only a single
forward pass, it can operate in real-time. Code is available on our project
page: https://chail.github.io/latent-composition/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1"&gt;Lucy Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wulff_J/0/1/0/all/0/1"&gt;Jonas Wulff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable COVID-19 Chest X-Ray Classification via Orthogonality Constraint. (arXiv:2102.08360v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08360</id>
        <link href="http://arxiv.org/abs/2102.08360"/>
        <updated>2021-06-07T03:06:13.428Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have increasingly been used as an auxiliary tool in
healthcare applications, due to their ability to improve performance of several
diagnosis tasks. However, these methods are not widely adopted in clinical
settings due to the practical limitations in the reliability, generalizability,
and interpretability of deep learning based systems. As a result, methods have
been developed that impose additional constraints during network training to
gain more control as well as improve interpretabilty, facilitating their
acceptance in healthcare community. In this work, we investigate the benefit of
using Orthogonal Spheres (OS) constraint for classification of COVID-19 cases
from chest X-ray images. The OS constraint can be written as a simple
orthonormality term which is used in conjunction with the standard
cross-entropy loss during classification network training. Previous studies
have demonstrated significant benefits in applying such constraints to deep
learning models. Our findings corroborate these observations, indicating that
the orthonormality loss function effectively produces improved semantic
localization via GradCAM visualizations, enhanced classification performance,
and reduced model calibration error. Our approach achieves an improvement in
accuracy of 1.6% and 4.8% for two- and three-class classification,
respectively; similar results are found for models with data augmentation
applied. In addition to these findings, our work also presents a new
application of the OS regularizer in healthcare, increasing the post-hoc
interpretability and performance of deep learning models for COVID-19
classification to facilitate adoption of these methods in clinical settings. We
also identify the limitations of our strategy that can be explored for further
research in future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Ella Y. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Som_A/0/1/0/all/0/1"&gt;Anirudh Som&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1"&gt;Ankita Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Hongjun Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1"&gt;Pavan Turaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization. (arXiv:2103.14862v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14862</id>
        <link href="http://arxiv.org/abs/2103.14862"/>
        <updated>2021-06-07T03:06:13.409Z</updated>
        <summary type="html"><![CDATA[Weakly supervised object localization (WSOL) is a challenging problem when
given image category labels but requires to learn object localization models.
Optimizing a convolutional neural network (CNN) for classification tends to
activate local discriminative regions while ignoring complete object extent,
causing the partial activation issue. In this paper, we argue that partial
activation is caused by the intrinsic characteristics of CNN, where the
convolution operations produce local receptive fields and experience difficulty
to capture long-range feature dependency among pixels. We introduce the token
semantic coupled attention map (TS-CAM) to take full advantage of the
self-attention mechanism in visual transformer for long-range dependency
extraction. TS-CAM first splits an image into a sequence of patch tokens for
spatial embedding, which produce attention maps of long-range visual dependency
to avoid partial activation. TS-CAM then re-allocates category-related
semantics for patch tokens, enabling each of them to be aware of object
categories. TS-CAM finally couples the patch tokens with the semantic-agnostic
attention map to achieve semantic-aware localization. Experiments on the
ILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM
counterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1"&gt;Fang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xingjia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhiliang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhenjun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Good Practices and A Strong Baseline for Traffic Anomaly Detection. (arXiv:2105.03827v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03827</id>
        <link href="http://arxiv.org/abs/2105.03827"/>
        <updated>2021-06-07T03:06:13.374Z</updated>
        <summary type="html"><![CDATA[The detection of traffic anomalies is a critical component of the intelligent
city transportation management system. Previous works have proposed a variety
of notable insights and taken a step forward in this field, however, dealing
with the complex traffic environment remains a challenge. Moreover, the lack of
high-quality data and the complexity of the traffic scene, motivate us to study
this problem from a hand-crafted perspective. In this paper, we propose a
straightforward and efficient framework that includes pre-processing, a dynamic
track module, and post-processing. With video stabilization, background
modeling, and vehicle detection, the pro-processing phase aims to generate
candidate anomalies. The dynamic tracking module seeks and locates the start
time of anomalies by utilizing vehicle motion patterns and spatiotemporal
status. Finally, we use post-processing to fine-tune the temporal boundary of
anomalies. Not surprisingly, our proposed framework was ranked $1^{st}$ in the
NVIDIA AI CITY 2021 leaderboard for traffic anomaly detection. The code is
available at: https://github.com/Endeavour10020/AICity2021-Anomaly-Detection .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yue He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-level Knowledge Distillation via Knowledge Alignment and Correlation. (arXiv:2012.00573v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00573</id>
        <link href="http://arxiv.org/abs/2012.00573"/>
        <updated>2021-06-07T03:06:13.351Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation (KD) has become an important technique for model
compression and knowledge transfer. In this work, we first perform a
comprehensive analysis of the knowledge transferred by different KD methods. We
demonstrate that traditional KD methods, which minimize the KL divergence of
softmax outputs between networks, are related to the knowledge alignment of an
individual sample only. Meanwhile, recent contrastive learning-based KD methods
mainly transfer relational knowledge between different samples, namely,
knowledge correlation. While it is important to transfer the full knowledge
from teacher to student, we introduce the Multi-level Knowledge Distillation
(MLKD) by effectively considering both knowledge alignment and correlation.
MLKD is task-agnostic and model-agnostic, and can easily transfer knowledge
from supervised or self-supervised pretrained teachers. We show that MLKD can
improve the reliability and transferability of learned representations.
Experiments demonstrate that MLKD outperforms other state-of-the-art methods on
a large number of experimental settings including different (a) pretraining
strategies (b) network architectures (c) datasets (d) tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hongxin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krovi_V/0/1/0/all/0/1"&gt;Venkat Krovi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1"&gt;Feng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Editing Conditional Radiance Fields. (arXiv:2105.06466v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06466</id>
        <link href="http://arxiv.org/abs/2105.06466"/>
        <updated>2021-06-07T03:06:13.345Z</updated>
        <summary type="html"><![CDATA[A neural radiance field (NeRF) is a scene model supporting high-quality view
synthesis, optimized per scene. In this paper, we explore enabling user editing
of a category-level NeRF - also known as a conditional radiance field - trained
on a shape category. Specifically, we introduce a method for propagating coarse
2D user scribbles to the 3D space, to modify the color or shape of a local
region. First, we propose a conditional radiance field that incorporates new
modular network components, including a shape branch that is shared across
object instances. Observing multiple instances of the same category, our model
learns underlying part semantics without any supervision, thereby allowing the
propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
seat). Next, we propose a hybrid network update strategy that targets specific
network components, which balances efficiency and accuracy. During user
interaction, we formulate an optimization problem that both satisfies the
user's constraints and preserves the original object structure. We demonstrate
our approach on various editing tasks over three shape datasets and show that
it outperforms prior neural editing approaches. Finally, we edit the appearance
and shape of a real photograph and show that the edit propagates to
extrapolated novel views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Steven Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Richard Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1"&gt;Bryan Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularizing Deep Networks with Semantic Data Augmentation. (arXiv:2007.10538v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10538</id>
        <link href="http://arxiv.org/abs/2007.10538"/>
        <updated>2021-06-07T03:06:13.323Z</updated>
        <summary type="html"><![CDATA[Data augmentation is widely known as a simple yet surprisingly effective
technique for regularizing deep networks. Conventional data augmentation
schemes, e.g., flipping, translation or rotation, are low-level,
data-independent and class-agnostic operations, leading to limited diversity
for augmented samples. To this end, we propose a novel semantic data
augmentation algorithm to complement traditional approaches. The proposed
method is inspired by the intriguing property that deep networks are effective
in learning linearized features, i.e., certain directions in the deep feature
space correspond to meaningful semantic transformations, e.g., changing the
background or view angle of an object. Based on this observation, translating
training samples along many such directions in the feature space can
effectively augment the dataset for more diversity. To implement this idea, we
first introduce a sampling based method to obtain semantically meaningful
directions efficiently. Then, an upper bound of the expected cross-entropy (CE)
loss on the augmented training set is derived by assuming the number of
augmented samples goes to infinity, yielding a highly efficient algorithm. In
fact, we show that the proposed implicit semantic data augmentation (ISDA)
algorithm amounts to minimizing a novel robust CE loss, which adds minimal
extra computational cost to a normal training procedure. In addition to
supervised learning, ISDA can be applied to semi-supervised learning tasks
under the consistency regularization framework, where ISDA amounts to
minimizing the upper bound of the expected KL-divergence between the augmented
features and the original features. Although being simple, ISDA consistently
improves the generalization performance of popular deep models (e.g., ResNets
and DenseNets) on a variety of datasets, i.e., CIFAR-10, CIFAR-100, SVHN,
ImageNet, and Cityscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xuran Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yitong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cheng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incomplete Graph Representation and Learning via Partial Graph Neural Networks. (arXiv:2003.10130v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10130</id>
        <link href="http://arxiv.org/abs/2003.10130"/>
        <updated>2021-06-07T03:06:13.317Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are gaining increasing attention on graph data
learning tasks in recent years. However, in many applications, graph may be
coming in an incomplete form where attributes of graph nodes are partially
unknown/missing. Existing GNNs are generally designed on complete graphs which
can not deal with attribute-incomplete graph data directly. To address this
problem, we develop a novel partial aggregation based GNNs, named Partial Graph
Neural Networks (PaGNNs), for attribute-incomplete graph representation and
learning. Our work is motivated by the observation that the neighborhood
aggregation function in standard GNNs can be equivalently viewed as the
neighborhood reconstruction formulation. Based on it, we define two novel
partial aggregation (reconstruction) functions on incomplete graph and derive
PaGNNs for incomplete graph data learning. Extensive experiments on several
datasets demonstrate the effectiveness and efficiency of the proposed PaGNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziyan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Attention for Visual Question Answering. (arXiv:2011.02164v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02164</id>
        <link href="http://arxiv.org/abs/2011.02164"/>
        <updated>2021-06-07T03:06:13.310Z</updated>
        <summary type="html"><![CDATA[We consider the problem of Visual Question Answering (VQA). Given an image
and a free-form, open-ended, question, expressed in natural language, the goal
of VQA system is to provide accurate answer to this question with respect to
the image. The task is challenging because it requires simultaneous and
intricate understanding of both visual and textual information. Attention,
which captures intra- and inter-modal dependencies, has emerged as perhaps the
most widely used mechanism for addressing these challenges. In this paper, we
propose an improved attention-based architecture to solve VQA. We incorporate
an Attention on Attention (AoA) module within encoder-decoder framework, which
is able to determine the relation between attention results and queries.
Attention module generates weighted average for each query. On the other hand,
AoA module first generates an information vector and an attention gate using
attention results and current context; and then adds another attention to
generate final attended information by multiplying the two. We also propose
multimodal fusion module to combine both visual and textual information. The
goal of this fusion module is to dynamically decide how much information should
be considered from each modality. Extensive experiments on VQA-v2 benchmark
dataset show that our method achieves the state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tanzila Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1"&gt;Shih-Han Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1"&gt;Leonid Sigal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12056</id>
        <link href="http://arxiv.org/abs/2102.12056"/>
        <updated>2021-06-07T03:06:13.303Z</updated>
        <summary type="html"><![CDATA[Liver segmentation from abdominal CT images is an essential step for liver
cancer computer-aided diagnosis and surgical planning. However, both the
accuracy and robustness of existing liver segmentation methods cannot meet the
requirements of clinical applications. In particular, for the common clinical
cases where the liver tissue contains major pathology, current segmentation
methods show poor performance. In this paper, we propose a novel low-rank
tensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that
achieves accurate and robust pathological liver segmentation of CT images.
Firstly, we propose a multi-slice LRTD scheme to recover the underlying
low-rank structure embedded in 3D medical images. It performs the LRTD on small
image segments consisting of multiple consecutive image slices. Then, we
present an LRTD-based atlas construction method to generate tumor-free liver
atlases that mitigates the performance degradation of liver segmentation due to
the presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to
derive patient-specific liver atlases for each test image, and to achieve
accurate pairwise image registration and label propagation. Extensive
experiments on three public databases of pathological liver cases validate the
effectiveness of the proposed method. Both qualitative and quantitative results
demonstrate that, in the presence of major pathology, the proposed method is
more accurate and robust than state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1"&gt;Min Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiancheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Da Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-interactive Dual-decoder for RGB-thermal Salient Object Detection. (arXiv:2005.02315v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02315</id>
        <link href="http://arxiv.org/abs/2005.02315"/>
        <updated>2021-06-07T03:06:13.294Z</updated>
        <summary type="html"><![CDATA[RGB-thermal salient object detection (SOD) aims to segment the common
prominent regions of visible image and corresponding thermal infrared image
that we call it RGBT SOD. Existing methods don't fully explore and exploit the
potentials of complementarity of different modalities and multi-type cues of
image contents, which play a vital role in achieving accurate results. In this
paper, we propose a multi-interactive dual-decoder to mine and model the
multi-type interactions for accurate RGBT SOD. In specific, we first encode two
modalities into multi-level multi-modal feature representations. Then, we
design a novel dual-decoder to conduct the interactions of multi-level
features, two modalities and global contexts. With these interactions, our
method works well in diversely challenging scenarios even in the presence of
invalid modality. Finally, we carry out extensive experiments on public RGBT
and RGBD SOD datasets, and the results show that the proposed method achieves
the outstanding performance against state-of-the-art algorithms. The source
code has been released
at:https://github.com/lz118/Multi-interactive-Dual-decoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhengzheng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_Y/0/1/0/all/0/1"&gt;Yang Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning domain-agnostic visual representation for computational pathology using medically-irrelevant style transfer augmentation. (arXiv:2102.01678v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01678</id>
        <link href="http://arxiv.org/abs/2102.01678"/>
        <updated>2021-06-07T03:06:13.277Z</updated>
        <summary type="html"><![CDATA[Suboptimal generalization of machine learning models on unseen data is a key
challenge which hampers the clinical applicability of such models to medical
imaging. Although various methods such as domain adaptation and domain
generalization have evolved to combat this challenge, learning robust and
generalizable representations is core to medical image understanding, and
continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation
for histoPathology), a form of data augmentation based on random style transfer
from non-medical style source such as artistic paintings, for learning
domain-agnostic visual representations in computational pathology. Style
transfer replaces the low-level texture content of an image with the
uninformative style of randomly selected style source image, while preserving
the original high-level semantic content. This improves robustness to domain
shift and can be used as a simple yet powerful tool for learning
domain-agnostic representations. We demonstrate that STRAP leads to
state-of-the-art performance, particularly in the presence of domain shifts, on
two particular classification tasks in computational pathology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yamashita_R/0/1/0/all/0/1"&gt;Rikiya Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_J/0/1/0/all/0/1"&gt;Jin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Banda_S/0/1/0/all/0/1"&gt;Snikitha Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jeanne Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusing CNNs and statistical indicators to improve image classification. (arXiv:2012.11049v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11049</id>
        <link href="http://arxiv.org/abs/2012.11049"/>
        <updated>2021-06-07T03:06:13.271Z</updated>
        <summary type="html"><![CDATA[Convolutional Networks have dominated the field of computer vision for the
last ten years, exhibiting extremely powerful feature extraction capabilities
and outstanding classification performance. The main strategy to prolong this
trend relies on further upscaling networks in size. However, costs increase
rapidly while performance improvements may be marginal. We hypothesise that
adding heterogeneous sources of information may be more cost-effective to a CNN
than building a bigger network. In this paper, an ensemble method is proposed
for accurate image classification, fusing automatically detected features
through Convolutional Neural Network architectures with a set of manually
defined statistical indicators. Through a combination of the predictions of a
CNN and a secondary classifier trained on statistical features, better
classification performance can be cheaply achieved. We test multiple learning
algorithms and CNN architectures on a diverse number of datasets to validate
our proposal, making public all our code and data via GitHub. According to our
results, the inclusion of additional indicators and an ensemble classification
approach helps to increase the performance in 8 of 9 datasets, with a
remarkable increase of more than 10% precision in two of them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huertas_Tato_J/0/1/0/all/0/1"&gt;Javier Huertas-Tato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1"&gt;Alejandro Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Juli&amp;#xe1;n Fierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_D/0/1/0/all/0/1"&gt;David Camacho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction or Comparison: Toward Interpretable Qualitative Reasoning. (arXiv:2106.02399v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02399</id>
        <link href="http://arxiv.org/abs/2106.02399"/>
        <updated>2021-06-07T03:06:13.263Z</updated>
        <summary type="html"><![CDATA[Qualitative relationships illustrate how changing one property (e.g., moving
velocity) affects another (e.g., kinetic energy) and constitutes a considerable
portion of textual knowledge. Current approaches use either semantic parsers to
transform natural language inputs into logical expressions or a "black-box"
model to solve them in one step. The former has a limited application range,
while the latter lacks interpretability. In this work, we categorize
qualitative reasoning tasks into two types: prediction and comparison. In
particular, we adopt neural network modules trained in an end-to-end manner to
simulate the two reasoning processes. Experiments on two qualitative reasoning
question answering datasets, QuaRTz and QuaRel, show our methods' effectiveness
and generalization capability, and the intermediate outputs provided by the
modules make the reasoning process interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heyan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Enhanced Rotation Prediction for Self-Supervised Learning. (arXiv:1912.11603v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.11603</id>
        <link href="http://arxiv.org/abs/1912.11603"/>
        <updated>2021-06-07T03:06:13.256Z</updated>
        <summary type="html"><![CDATA[The rotation prediction (Rotation) is a simple pretext-task for
self-supervised learning (SSL), where models learn useful representations for
target vision tasks by solving pretext-tasks. Although Rotation captures
information of object shapes, it hardly captures information of textures. To
tackle this problem, we introduce a novel pretext-task called image enhanced
rotation prediction (IE-Rot) for SSL. IE-Rot simultaneously solves Rotation and
another pretext-task based on image enhancement (e.g., sharpening and
solarizing) while maintaining simplicity. Through the simultaneous prediction
of rotation and image enhancement, models learn representations to capture the
information of not only object shapes but also textures. Our experimental
results show that IE-Rot models outperform Rotation on various standard
benchmarks including ImageNet classification, PASCAL-VOC detection, and COCO
detection/segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shioda_T/0/1/0/all/0/1"&gt;Tetsuya Shioda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takeda_S/0/1/0/all/0/1"&gt;Shoichiro Takeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prototype Completion with Primitive Knowledge for Few-Shot Learning. (arXiv:2009.04960v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04960</id>
        <link href="http://arxiv.org/abs/2009.04960"/>
        <updated>2021-06-07T03:06:13.249Z</updated>
        <summary type="html"><![CDATA[Few-shot learning is a challenging task, which aims to learn a classifier for
novel classes with few examples. Pre-training based meta-learning methods
effectively tackle the problem by pre-training a feature extractor and then
fine-tuning it through the nearest centroid based meta-learning. However,
results show that the fine-tuning step makes very marginal improvements. In
this paper, 1) we figure out the key reason, i.e., in the pre-trained feature
space, the base classes already form compact clusters while novel classes
spread as groups with large variances, which implies that fine-tuning the
feature extractor is less meaningful; 2) instead of fine-tuning the feature
extractor, we focus on estimating more representative prototypes during
meta-learning. Consequently, we propose a novel prototype completion based
meta-learning framework. This framework first introduces primitive knowledge
(i.e., class-level part or attribute annotations) and extracts representative
attribute features as priors. Then, we design a prototype completion network to
learn to complete prototypes with these priors. To avoid the prototype
completion error caused by primitive knowledge noises or class differences, we
further develop a Gaussian based prototype fusion strategy that combines the
mean-based and completed prototypes by exploiting the unlabeled samples.
Extensive experiments show that our method: (i) can obtain more accurate
prototypes; (ii) outperforms state-of-the-art techniques by 2% - 9% in terms of
classification accuracy. Our code is available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Baoquan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xutao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yunming Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhichao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lisai Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02343</id>
        <link href="http://arxiv.org/abs/2106.02343"/>
        <updated>2021-06-07T03:06:13.231Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks built from deep convolutional neural networks
(GANs) lack the ability to exactly replicate the high-frequency components of
natural images. To alleviate this issue, we introduce two novel training
techniques called frequency dropping (F-Drop) and frequency matching (F-Match).
The key idea of F-Drop is to filter out unnecessary high-frequency components
from the input images of the discriminators. This simple modification prevents
the discriminators from being confused by perturbations of the high-frequency
components. In addition, F-Drop makes the GANs focus on fitting in the
low-frequency domain, in which there are the dominant components of natural
images. F-Match minimizes the difference between real and fake images in the
frequency domain for generating more realistic images. F-Match is implemented
as a regularization term in the objective functions of the generators; it
penalizes the batch mean error in the frequency domain. F-Match helps the
generators to fit in the high-frequency domain filtered out by F-Drop to the
real image. We experimentally demonstrate that the combination of F-Drop and
F-Match improves the generative performance of GANs in both the frequency and
spatial domain on multiple image benchmarks (CIFAR, TinyImageNet, STL-10,
CelebA, and ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGBT Tracking via Multi-Adapter Network with Hierarchical Divergence Loss. (arXiv:2011.07189v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07189</id>
        <link href="http://arxiv.org/abs/2011.07189"/>
        <updated>2021-06-07T03:06:13.225Z</updated>
        <summary type="html"><![CDATA[RGBT tracking has attracted increasing attention since RGB and thermal
infrared data have strong complementary advantages, which could make trackers
all-day and all-weather work. However, how to effectively represent RGBT data
for visual tracking remains unstudied well. Existing works usually focus on
extracting modality-shared or modality-specific information, but the potentials
of these two cues are not well explored and exploited in RGBT tracking. In this
paper, we propose a novel multi-adapter network to jointly perform
modality-shared, modality-specific and instance-aware target representation
learning for RGBT tracking. To this end, we design three kinds of adapters
within an end-to-end deep learning framework. In specific, we use the modified
VGG-M as the generality adapter to extract the modality-shared target
representations.To extract the modality-specific features while reducing the
computational complexity, we design a modality adapter, which adds a small
block to the generality adapter in each layer and each modality in a parallel
manner. Such a design could learn multilevel modality-specific representations
with a modest number of parameters as the vast majority of parameters are
shared with the generality adapter. We also design instance adapter to capture
the appearance properties and temporal variations of a certain target.
Moreover, to enhance the shared and specific features, we employ the loss of
multiple kernel maximum mean discrepancy to measure the distribution divergence
of different modal features and integrate it into each layer for more robust
representation learning. Extensive experiments on two RGBT tracking benchmark
datasets demonstrate the outstanding performance of the proposed tracker
against the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1"&gt;Andong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yuqing Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1"&gt;Bin Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reactive Human-to-Robot Handovers of Arbitrary Objects. (arXiv:2011.08961v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08961</id>
        <link href="http://arxiv.org/abs/2011.08961"/>
        <updated>2021-06-07T03:06:13.218Z</updated>
        <summary type="html"><![CDATA[Human-robot object handovers have been an actively studied area of robotics
over the past decade; however, very few techniques and systems have addressed
the challenge of handing over diverse objects with arbitrary appearance, size,
shape, and rigidity. In this paper, we present a vision-based system that
enables reactive human-to-robot handovers of unknown objects. Our approach
combines closed-loop motion planning with real-time, temporally-consistent
grasp generation to ensure reactivity and motion smoothness. Our system is
robust to different object positions and orientations, and can grasp both rigid
and non-rigid objects. We demonstrate the generalizability, usability, and
robustness of our approach on a novel benchmark set of 26 diverse household
objects, a user study with naive users (N=6) handing over a subset of 15
objects, and a systematic evaluation examining different ways of handing
objects. More results and videos can be found at
https://sites.google.com/nvidia.com/handovers-of-arbitrary-objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1"&gt;Chris Paxton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1"&gt;Arsalan Mousavian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1"&gt;Yu-Wei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cakmak_M/0/1/0/all/0/1"&gt;Maya Cakmak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02634</id>
        <link href="http://arxiv.org/abs/2106.02634"/>
        <updated>2021-06-07T03:06:13.212Z</updated>
        <summary type="html"><![CDATA[Inferring representations of 3D scenes from 2D observations is a fundamental
problem of computer graphics, computer vision, and artificial intelligence.
Emerging 3D-structured neural scene representations are a promising approach to
3D scene understanding. In this work, we propose a novel neural scene
representation, Light Field Networks or LFNs, which represent both geometry and
appearance of the underlying 3D scene in a 360-degree, four-dimensional light
field parameterized via a neural implicit representation. Rendering a ray from
an LFN requires only a *single* network evaluation, as opposed to hundreds of
evaluations per ray for ray-marching or volumetric based renderers in
3D-structured neural scene representations. In the setting of simple scenes, we
leverage meta-learning to learn a prior over LFNs that enables multi-view
consistent light field reconstruction from as little as a single image
observation. This results in dramatic reductions in time and memory complexity,
and enables real-time rendering. The cost of storing a 360-degree light field
via an LFN is two orders of magnitude lower than conventional methods such as
the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further
demonstrate the extraction of sparse depth maps from LFNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1"&gt;Semon Rezchikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fredo Durand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SkeletonNet: A Topology-Preserving Solution for Learning Mesh Reconstruction of Object Surfaces from RGB Images. (arXiv:2008.05742v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05742</id>
        <link href="http://arxiv.org/abs/2008.05742"/>
        <updated>2021-06-07T03:06:13.206Z</updated>
        <summary type="html"><![CDATA[This paper focuses on the challenging task of learning 3D object surface
reconstructions from RGB images. Existingmethods achieve varying degrees of
success by using different surface representations. However, they all have
their own drawbacks,and cannot properly reconstruct the surface shapes of
complex topologies, arguably due to a lack of constraints on the
topologicalstructures in their learning frameworks. To this end, we propose to
learn and use the topology-preserved, skeletal shape representationto assist
the downstream task of object surface reconstruction from RGB images.
Technically, we propose the novelSkeletonNetdesign that learns a volumetric
representation of a skeleton via a bridged learning of a skeletal point set,
where we use paralleldecoders each responsible for the learning of points on 1D
skeletal curves and 2D skeletal sheets, as well as an efficient module
ofglobally guided subvolume synthesis for a refined, high-resolution skeletal
volume; we present a differentiablePoint2Voxellayer tomake SkeletonNet
end-to-end and trainable. With the learned skeletal volumes, we propose two
models, the Skeleton-Based GraphConvolutional Neural Network (SkeGCNN) and the
Skeleton-Regularized Deep Implicit Surface Network (SkeDISN), which
respectivelybuild upon and improve over the existing frameworks of explicit
mesh deformation and implicit field learning for the downstream
surfacereconstruction task. We conduct thorough experiments that verify the
efficacy of our proposed SkeletonNet. SkeGCNN and SkeDISNoutperform existing
methods as well, and they have their own merits when measured by different
metrics. Additional results ingeneralized task settings further demonstrate the
usefulness of our proposed methods. We have made both our implementation
codeand the ShapeNet-Skeleton dataset publicly available at ble at
https://github.com/tangjiapeng/SkeletonNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiapeng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1"&gt;Xin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Semi-Supervised Gross Target Volume of Nasopharyngeal Carcinoma Segmentation via Uncertainty Rectified Pyramid Consistency. (arXiv:2012.07042v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07042</id>
        <link href="http://arxiv.org/abs/2012.07042"/>
        <updated>2021-06-07T03:06:13.197Z</updated>
        <summary type="html"><![CDATA[Gross Target Volume (GTV) segmentation plays an irreplaceable role in
radiotherapy planning for Nasopharyngeal Carcinoma (NPC). Despite that
Convolutional Neural Networks (CNN) have achieved good performance for this
task, they rely on a large set of labeled images for training, which is
expensive and time-consuming to acquire. In this paper, we propose a novel
framework with Uncertainty Rectified Pyramid Consistency (URPC) regularization
for semi-supervised NPC GTV segmentation. Concretely, we extend a backbone
segmentation network to produce pyramid predictions at different scales. The
pyramid predictions network (PPNet) is supervised by the ground truth of
labeled images and a multi-scale consistency loss for unlabeled images,
motivated by the fact that prediction at different scales for the same input
should be similar and consistent. However, due to the different resolution of
these predictions, encouraging them to be consistent at each pixel directly has
low robustness and may lose some fine details. To address this problem, we
further design a novel uncertainty rectifying module to enable the framework to
gradually learn from meaningful and reliable consensual regions at different
scales. Experimental results on a dataset with 258 NPC MR images showed that
with only 10% or 20% images labeled, our method largely improved the
segmentation performance by leveraging the unlabeled images, and it also
outperformed five state-of-the-art semi-supervised segmentation methods.
Moreover, when only 50% images labeled, URPC achieved an average Dice score of
82.74% that was close to fully supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiangde Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1"&gt;Wenjun Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jieneng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1"&gt;Tao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shichuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nianyong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guotai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaoting Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regressive Domain Adaptation for Unsupervised Keypoint Detection. (arXiv:2103.06175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06175</id>
        <link href="http://arxiv.org/abs/2103.06175"/>
        <updated>2021-06-07T03:06:13.180Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) aims at transferring knowledge from a labeled source
domain to an unlabeled target domain. Though many DA theories and algorithms
have been proposed, most of them are tailored into classification settings and
may fail in regression tasks, especially in the practical keypoint detection
task. To tackle this difficult but significant task, we present a method of
regressive domain adaptation (RegDA) for unsupervised keypoint detection.
Inspired by the latest theoretical work, we first utilize an adversarial
regressor to maximize the disparity on the target domain and train a feature
generator to minimize this disparity. However, due to the high dimension of the
output space, this regressor fails to detect samples that deviate from the
support of the source. To overcome this problem, we propose two important
ideas. First, based on our observation that the probability density of the
output space is sparse, we introduce a spatial probability distribution to
describe this sparsity and then use it to guide the learning of the adversarial
regressor. Second, to alleviate the optimization difficulty in the
high-dimensional space, we innovatively convert the minimax game in the
adversarial training to the minimization of two opposite goals. Extensive
experiments show that our method brings large improvement by 8% to 11% in terms
of PCK on different datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junguang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yifei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Ximei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yufeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sign-Agnostic Implicit Learning of Surface Self-Similarities for Shape Modeling and Reconstruction from Raw Point Clouds. (arXiv:2012.07498v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07498</id>
        <link href="http://arxiv.org/abs/2012.07498"/>
        <updated>2021-06-07T03:06:13.173Z</updated>
        <summary type="html"><![CDATA[Shape modeling and reconstruction from raw point clouds of objects stand as a
fundamental challenge in vision and graphics research. Classical methods
consider analytic shape priors; however, their performance degraded when the
scanned points deviate from the ideal conditions of cleanness and completeness.
Important progress has been recently made by data-driven approaches, which
learn global and/or local models of implicit surface representations from
auxiliary sets of training shapes. Motivated from a universal phenomenon that
self-similar shape patterns of local surface patches repeat across the entire
surface of an object, we aim to push forward the data-driven strategies and
propose to learn a local implicit surface network for a shared, adaptive
modeling of the entire surface for a direct surface reconstruction from raw
point cloud; we also enhance the leveraging of surface self-similarities by
improving correlations among the optimized latent codes of individual surface
patches. Given that orientations of raw points could be unavailable or noisy,
we extend sign agnostic learning into our local implicit model, which enables
our recovery of signed implicit fields of local surfaces from the unsigned
inputs. We term our framework as Sign-Agnostic Implicit Learning of Surface
Self-Similarities (SAIL-S3). With a global post-optimization of local sign
flipping, SAIL-S3 is able to directly model raw, un-oriented point clouds and
reconstruct high-quality object surfaces. Experiments show its superiority over
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenbin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jiabao Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yuxin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Training of Convolutional Neural Networks with Low-bitwidth Weights and Activations. (arXiv:1908.04680v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.04680</id>
        <link href="http://arxiv.org/abs/1908.04680"/>
        <updated>2021-06-07T03:06:13.167Z</updated>
        <summary type="html"><![CDATA[This paper tackles the problem of training a deep convolutional neural
network of both low-bitwidth weights and activations. Optimizing a
low-precision network is very challenging due to the non-differentiability of
the quantizer, which may result in substantial accuracy loss. To address this,
we propose three practical approaches, including (i) progressive quantization;
(ii) stochastic precision; and (iii) joint knowledge distillation to improve
the network training. First, for progressive quantization, we propose two
schemes to progressively find good local minima. Specifically, we propose to
first optimize a net with quantized weights and subsequently quantize
activations. This is in contrast to the traditional methods which optimize them
simultaneously. Furthermore, we propose a second progressive quantization
scheme which gradually decreases the bit-width from high-precision to
low-precision during training. Second, to alleviate the excessive training
burden due to the multi-round training stages, we further propose a one-stage
stochastic precision strategy to randomly sample and quantize sub-networks
while keeping other parts in full-precision. Finally, we adopt a novel learning
scheme to jointly train a full-precision model alongside the low-precision one.
By doing so, the full-precision model provides hints to guide the low-precision
model training and significantly improves the performance of the low-precision
network. Extensive experiments on various datasets (e.g., CIFAR-100, ImageNet)
show the effectiveness of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1"&gt;Ian Reid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signed Distance Function Computation from an Implicit Surface. (arXiv:2104.08057v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08057</id>
        <link href="http://arxiv.org/abs/2104.08057"/>
        <updated>2021-06-07T03:06:13.160Z</updated>
        <summary type="html"><![CDATA[We describe in this short note a technique to convert an implicit surface
into a Signed Distance Function (SDF) while exactly preserving the zero
level-set of the implicit. The proposed approach relies on embedding the input
implicit in the final layer of a neural network, which is trained to minimize a
loss function characterizing the SDF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fayolle_P/0/1/0/all/0/1"&gt;Pierre-Alain Fayolle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly- and Semi-Supervised Probabilistic Segmentation and Quantification of Ultrasound Needle-Reverberation Artifacts to Allow Better AI Understanding of Tissue Beneath Needles. (arXiv:2011.11958v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11958</id>
        <link href="http://arxiv.org/abs/2011.11958"/>
        <updated>2021-06-07T03:06:13.154Z</updated>
        <summary type="html"><![CDATA[Ultrasound image quality has continually been improving. However, when
needles or other metallic objects are operating inside the tissue, the
resulting reverberation artifacts can severely corrupt the surrounding image
quality. Such effects are challenging for existing computer vision algorithms
for medical image analysis. Needle reverberation artifacts can be hard to
identify at times and affect various pixel values to different degrees. The
boundaries of such artifacts are ambiguous, leading to disagreement among human
experts labeling the artifacts. We propose a weakly- and semi-supervised,
probabilistic needle-and-reverberation-artifact segmentation algorithm to
separate the desired tissue-based pixel values from the superimposed artifacts.
Our method models the intensity decay of artifact intensities and is designed
to minimize the human labeling error. We demonstrate the applicability of the
approach and compare it against other segmentation algorithms. Our method is
capable of differentiating between the reverberations from artifact-free
patches as well as of modeling the intensity fall-off in the artifacts. Our
method matches state-of-the-art artifact segmentation performance and sets a
new standard in estimating the per-pixel contributions of artifact vs
underlying anatomy, especially in the immediately adjacent regions between
reverberation lines. Our algorithm is also able to improve the performance
downstream image analysis algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1"&gt;Alex Ling Yu Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1"&gt;Edward Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1"&gt;John Galeotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Descriptor Enhancement via Self-Labelling Triplets for Visual Data Association. (arXiv:2011.10471v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10471</id>
        <link href="http://arxiv.org/abs/2011.10471"/>
        <updated>2021-06-07T03:06:13.136Z</updated>
        <summary type="html"><![CDATA[Object-level data association is central to robotic applications such as
tracking-by-detection and object-level simultaneous localization and mapping.
While current learned visual data association methods outperform hand-crafted
algorithms, many rely on large collections of domain-specific training examples
that can be difficult to obtain without prior knowledge. Additionally, such
methods often remain fixed during inference-time and do not harness observed
information to better their performance. We propose a self-supervised method
for incrementally refining visual descriptors to improve performance in the
task of object-level visual data association. Our method optimizes deep
descriptor generators online, by continuously training a widely available image
classification network pre-trained with domain-independent data. We show that
earlier layers in the network outperform later-stage layers for the data
association task while also allowing for a 94% reduction in the number of
parameters, enabling the online optimization. We show that self-labelling
challenging triplets--choosing positive examples separated by large temporal
distances and negative examples close in the descriptor space--improves the
quality of the learned descriptors for the multi-object tracking task. Finally,
we demonstrate that our approach surpasses other visual data-association
methods applied to a tracking-by-detection task, and show that it provides
better performance-gains when compared to other methods that attempt to adapt
to observed information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaoul_Y/0/1/0/all/0/1"&gt;Yorai Shaoul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Katherine Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ok_K/0/1/0/all/0/1"&gt;Kyel Ok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1"&gt;Nicholas Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepActsNet: Spatial and Motion features from Face, Hands, and Body Combined with Convolutional and Graph Networks for Improved Action Recognition. (arXiv:2009.09818v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09818</id>
        <link href="http://arxiv.org/abs/2009.09818"/>
        <updated>2021-06-07T03:06:13.130Z</updated>
        <summary type="html"><![CDATA[Existing action recognition methods mainly focus on joint and bone
information in human body skeleton data due to its robustness to complex
backgrounds and dynamic characteristics of the environments. In this paper, we
combine body skeleton data with spatial and motion features from face and two
hands, and present "Deep Action Stamps (DeepActs)", a novel data representation
to encode actions from video sequences. We also present "DeepActsNet", a deep
learning based ensemble model which learns convolutional and structural
features from Deep Action Stamps for highly accurate action recognition.
Experiments on three challenging action recognition datasets (NTU60, NTU120,
and SYSU) show that the proposed model trained using Deep Action Stamps produce
considerable improvements in the action recognition accuracy with less
computational cost compared to the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asif_U/0/1/0/all/0/1"&gt;Umar Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1"&gt;Deval Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallar_S/0/1/0/all/0/1"&gt;Stefan von Cavallar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jianbin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrer_S/0/1/0/all/0/1"&gt;Stefan Harrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MANTRA: Memory Augmented Networks for Multiple Trajectory Prediction. (arXiv:2006.03340v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03340</id>
        <link href="http://arxiv.org/abs/2006.03340"/>
        <updated>2021-06-07T03:06:13.124Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicles are expected to drive in complex scenarios with several
independent non cooperating agents. Path planning for safely navigating in such
environments can not just rely on perceiving present location and motion of
other agents. It requires instead to predict such variables in a far enough
future. In this paper we address the problem of multimodal trajectory
prediction exploiting a Memory Augmented Neural Network. Our method learns past
and future trajectory embeddings using recurrent neural networks and exploits
an associative external memory to store and retrieve such embeddings.
Trajectory prediction is then performed by decoding in-memory future encodings
conditioned with the observed past. We incorporate scene knowledge in the
decoding state by learning a CNN on top of semantic scene maps. Memory growth
is limited by learning a writing controller based on the predictive capability
of existing embeddings. We show that our method is able to natively perform
multi-modal trajectory prediction obtaining state-of-the art results on three
datasets. Moreover, thanks to the non-parametric nature of the memory module,
we show how once trained our system can continuously improve by ingesting novel
patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marchetti_F/0/1/0/all/0/1"&gt;Francesco Marchetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becattini_F/0/1/0/all/0/1"&gt;Federico Becattini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidenari_L/0/1/0/all/0/1"&gt;Lorenzo Seidenari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1"&gt;Alberto Del Bimbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Domain Invariant Features for Depth Estimation. (arXiv:2106.02594v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02594</id>
        <link href="http://arxiv.org/abs/2106.02594"/>
        <updated>2021-06-07T03:06:13.117Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of unsupervised synthetic-to-realistic domain
adaptation for single image depth estimation. An essential building block of
single image depth estimation is an encoder-decoder task network that takes RGB
images as input and produces depth maps as output. In this paper, we propose a
novel training strategy to force the task network to learn domain invariant
representations in a self-supervised manner. Specifically, we extend
self-supervised learning from traditional representation learning, which works
on images from a single domain, to domain invariant representation learning,
which works on images from two different domains by utilizing an image-to-image
translation network. Firstly, we use our bidirectional image-to-image
translation network to transfer domain-specific styles between synthetic and
real domains. This style transfer operation allows us to obtain similar images
from the different domains. Secondly, we jointly train our task network and
Siamese network with the same images from the different domains to obtain
domain invariance for the task network. Finally, we fine-tune the task network
using labeled synthetic and unlabeled real-world data. Our training strategy
yields improved generalization capability in the real-world domain. We carry
out an extensive evaluation on two popular datasets for depth estimation, KITTI
and Make3D. The results demonstrate that our proposed method outperforms the
state-of-the-art both qualitatively and quantitatively. The source code and
model weights will be made available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akada_H/0/1/0/all/0/1"&gt;Hiroyasu Akada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alhashim_I/0/1/0/all/0/1"&gt;Ibraheem Alhashim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve the Interpretability of Attention: A Fast, Accurate, and Interpretable High-Resolution Attention Model. (arXiv:2106.02566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02566</id>
        <link href="http://arxiv.org/abs/2106.02566"/>
        <updated>2021-06-07T03:06:13.109Z</updated>
        <summary type="html"><![CDATA[The prevalence of employing attention mechanisms has brought along concerns
on the interpretability of attention distributions. Although it provides
insights about how a model is operating, utilizing attention as the explanation
of model predictions is still highly dubious. The community is still seeking
more interpretable strategies for better identifying local active regions that
contribute the most to the final decision. To improve the interpretability of
existing attention models, we propose a novel Bilinear Representative
Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant
human-interpretable information. The target model is first distilled to have
higher-resolution intermediate feature maps. From which, representative
features are then grouped based on local pairwise feature similarity, to
produce finer-grained, more precise attention maps highlighting task-relevant
parts of the input. The obtained attention maps are ranked according to the
`active level' of the compound feature, which provides information regarding
the important level of the highlighted regions. The proposed model can be
easily adapted in a wide variety of modern deep models, where classification is
involved. It is also more accurate, faster, and with a smaller memory footprint
than usual neural attention modules. Extensive experiments showcase more
comprehensive visual explanations compared to the state-of-the-art
visualization model across multiple tasks including few-shot classification,
person re-identification, fine-grained image classification. The proposed
visualization model sheds imperative light on how neural networks `pay their
attention' differently in different tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1"&gt;Tristan Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1"&gt;Suiyi Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1"&gt;Thomas Fr&amp;#xe9;our&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1"&gt;Harold Mouch&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embedded Deep Regularized Block HSIC Thermomics for Early Diagnosis of Breast Cancer. (arXiv:2106.02106v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02106</id>
        <link href="http://arxiv.org/abs/2106.02106"/>
        <updated>2021-06-07T03:06:13.084Z</updated>
        <summary type="html"><![CDATA[Thermography has been used extensively as a complementary diagnostic tool in
breast cancer detection. Among thermographic methods matrix factorization (MF)
techniques show an unequivocal capability to detect thermal patterns
corresponding to vasodilation in cancer cases. One of the biggest challenges in
such techniques is selecting the best representation of the thermal basis. In
this study, an embedding method is proposed to address this problem and
Deep-semi-nonnegative matrix factorization (Deep-SemiNMF) for thermography is
introduced, then tested for 208 breast cancer screening cases. First, we apply
Deep-SemiNMF to infrared images to extract low-rank thermal representations for
each case. Then, we embed low-rank bases to obtain one basis for each patient.
After that, we extract 300 thermal imaging features, called thermomics, to
decode imaging information for the automatic diagnostic model. We reduced the
dimensionality of thermomics by spanning them onto Hilbert space using RBF
kernel and select the three most efficient features using the block Hilbert
Schmidt Independence Criterion Lasso (block HSIC Lasso). The preserved thermal
heterogeneity successfully classified asymptomatic versus symptomatic patients
applying a random forest model (cross-validated accuracy of 71.36%
(69.42%-73.3%)).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yousefi_B/0/1/0/all/0/1"&gt;Bardia Yousefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sharifipour_H/0/1/0/all/0/1"&gt;Hossein Memarzadeh Sharifipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maldague_X/0/1/0/all/0/1"&gt;Xavier P.V. Maldague&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Methods for Vessel Trajectory Prediction based on Recurrent Neural Networks. (arXiv:2101.02486v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02486</id>
        <link href="http://arxiv.org/abs/2101.02486"/>
        <updated>2021-06-07T03:06:13.075Z</updated>
        <summary type="html"><![CDATA[Data-driven methods open up unprecedented possibilities for maritime
surveillance using Automatic Identification System (AIS) data. In this work, we
explore deep learning strategies using historical AIS observations to address
the problem of predicting future vessel trajectories with a prediction horizon
of several hours. We propose novel sequence-to-sequence vessel trajectory
prediction models based on encoder-decoder recurrent neural networks (RNNs)
that are trained on historical trajectory data to predict future trajectory
samples given previous observations. The proposed architecture combines Long
Short-Term Memory (LSTM) RNNs for sequence modeling to encode the observed data
and generate future predictions with different intermediate aggregation layers
to capture space-time dependencies in sequential data. Experimental results on
vessel trajectories from an AIS dataset made freely available by the Danish
Maritime Authority show the effectiveness of deep-learning methods for
trajectory prediction based on sequence-to-sequence neural networks, which
achieve better performance than baseline approaches based on linear regression
or on the Multi-Layer Perceptron (MLP) architecture. The comparative evaluation
of results shows: i) the superiority of attention pooling over static pooling
for the specific application, and ii) the remarkable performance improvement
that can be obtained with labeled trajectories, i.e., when predictions are
conditioned on a low-level context representation encoded from the sequence of
past observations, as well as on additional inputs (e.g., port of departure or
arrival) about the vessel's high-level intention, which may be available from
AIS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Capobianco_S/0/1/0/all/0/1"&gt;Samuele Capobianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Millefiori_L/0/1/0/all/0/1"&gt;Leonardo M. Millefiori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forti_N/0/1/0/all/0/1"&gt;Nicola Forti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braca_P/0/1/0/all/0/1"&gt;Paolo Braca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willett_P/0/1/0/all/0/1"&gt;Peter Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Fixed-Points in Recurrent Neural Networks. (arXiv:2106.02417v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02417</id>
        <link href="http://arxiv.org/abs/2106.02417"/>
        <updated>2021-06-07T03:06:13.067Z</updated>
        <summary type="html"><![CDATA[Recurrent neural networks are widely used in speech and language processing.
Due to dependency on the past, standard algorithms for training these models,
such as back-propagation through time (BPTT), cannot be efficiently
parallelised. Furthermore, applying these models to more complex structures
than sequences requires inference time approximations, which introduce
inconsistency between inference and training. This paper shows that recurrent
neural networks can be reformulated as fixed-points of non-linear equation
systems. These fixed-points can be computed using an iterative algorithm
exactly and in as many iterations as the length of any given sequence. Each
iteration of this algorithm adds one additional Markovian-like order of
dependencies such that upon termination all dependencies modelled by the
recurrent neural networks have been incorporated. Although exact fixed-points
inherit the same parallelization and inconsistency issues, this paper shows
that approximate fixed-points can be computed in parallel and used consistently
in training and inference including tasks such as lattice rescoring.
Experimental validation is performed in two tasks, Penn Tree Bank and
WikiText-2, and shows that approximate fixed-points yield competitive
prediction performance to recurrent neural networks trained using the BPTT
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ragni_A/0/1/0/all/0/1"&gt;Anton Ragni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Domain Adaptation for LiDAR Perception. (arXiv:2106.02377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02377</id>
        <link href="http://arxiv.org/abs/2106.02377"/>
        <updated>2021-06-07T03:06:13.061Z</updated>
        <summary type="html"><![CDATA[Scalable systems for automated driving have to reliably cope with an
open-world setting. This means, the perception systems are exposed to drastic
domain shifts, like changes in weather conditions, time-dependent aspects, or
geographic regions. Covering all domains with annotated data is impossible
because of the endless variations of domains and the time-consuming and
expensive annotation process. Furthermore, fast development cycles of the
system additionally introduce hardware changes, such as sensor types and
vehicle setups, and the required knowledge transfer from simulation. To enable
scalable automated driving, it is therefore crucial to address these domain
shifts in a robust and efficient manner. Over the last years, a vast amount of
different domain adaptation techniques evolved. There already exists a number
of survey papers for domain adaptation on camera images, however, a survey for
LiDAR perception is absent. Nevertheless, LiDAR is a vital sensor for automated
driving that provides detailed 3D scans of the vehicle's surroundings. To
stimulate future research, this paper presents a comprehensive review of recent
progress in domain adaptation methods and formulates interesting research
questions specifically targeted towards LiDAR perception.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Triess_L/0/1/0/all/0/1"&gt;Larissa T. Triess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dreissig_M/0/1/0/all/0/1"&gt;Mariella Dreissig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rist_C/0/1/0/all/0/1"&gt;Christoph B. Rist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1"&gt;J. Marius Z&amp;#xf6;llner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation. (arXiv:2004.03238v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03238</id>
        <link href="http://arxiv.org/abs/2004.03238"/>
        <updated>2021-06-07T03:06:13.054Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) models for reading comprehension have achieved
human-level accuracy on in-distribution test sets. However, they have been
demonstrated to lack robustness to challenge sets, whose distribution is
different from that of training sets. Existing data augmentation methods
mitigate this problem by simply augmenting training sets with synthetic
examples sampled from the same distribution as the challenge sets. However,
these methods assume that the distribution of a challenge set is known a
priori, making them less applicable to unseen challenge sets. In this study, we
focus on question-answer pair generation (QAG) to mitigate this problem. While
most existing QAG methods aim to improve the quality of synthetic examples, we
conjecture that diversity-promoting QAG can mitigate the sparsity of training
sets and lead to better robustness. We present a variational QAG model that
generates multiple diverse QA pairs from a paragraph. Our experiments show that
our method can improve the accuracy of 12 challenge sets, as well as the
in-distribution accuracy. Our code and data are available at
https://github.com/KazutoshiShinoda/VQAG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1"&gt;Kazutoshi Shinoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1"&gt;Saku Sugawara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1"&gt;Akiko Aizawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Prospective Observational Study to Investigate Performance of a Chest X-ray Artificial Intelligence Diagnostic Support Tool Across 12 U.S. Hospitals. (arXiv:2106.02118v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02118</id>
        <link href="http://arxiv.org/abs/2106.02118"/>
        <updated>2021-06-07T03:06:13.030Z</updated>
        <summary type="html"><![CDATA[Importance: An artificial intelligence (AI)-based model to predict COVID-19
likelihood from chest x-ray (CXR) findings can serve as an important adjunct to
accelerate immediate clinical decision making and improve clinical decision
making. Despite significant efforts, many limitations and biases exist in
previously developed AI diagnostic models for COVID-19. Utilizing a large set
of local and international CXR images, we developed an AI model with high
performance on temporal and external validation.

Conclusions and Relevance: AI-based diagnostic tools may serve as an adjunct,
but not replacement, for clinical decision support of COVID-19 diagnosis, which
largely hinges on exposure history, signs, and symptoms. While AI-based tools
have not yet reached full diagnostic potential in COVID-19, they may still
offer valuable information to clinicians taken into consideration along with
clinical signs and symptoms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1"&gt;Le Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Taihui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adila_D/0/1/0/all/0/1"&gt;Dyah Adila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaiman_Z/0/1/0/all/0/1"&gt;Zach Zaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Melton_G/0/1/0/all/0/1"&gt;Genevieve B. Melton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ingraham_N/0/1/0/all/0/1"&gt;Nicholas Ingraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murray_E/0/1/0/all/0/1"&gt;Eric Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boley_D/0/1/0/all/0/1"&gt;Daniel Boley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Switzer_S/0/1/0/all/0/1"&gt;Sean Switzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burns_J/0/1/0/all/0/1"&gt;John L. Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Allen_T/0/1/0/all/0/1"&gt;Tadashi Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Steenburg_S/0/1/0/all/0/1"&gt;Scott D. Steenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1"&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kummerfeld_E/0/1/0/all/0/1"&gt;Erich Kummerfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tignanelli_C/0/1/0/all/0/1"&gt;Christopher Tignanelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching keyword spotters to spot new keywords with limited examples. (arXiv:2106.02443v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02443</id>
        <link href="http://arxiv.org/abs/2106.02443"/>
        <updated>2021-06-07T03:06:13.022Z</updated>
        <summary type="html"><![CDATA[Learning to recognize new keywords with just a few examples is essential for
personalizing keyword spotting (KWS) models to a user's choice of keywords.
However, modern KWS models are typically trained on large datasets and
restricted to a small vocabulary of keywords, limiting their transferability to
a broad range of unseen keywords. Towards easily customizable KWS models, we
present KeySEM (Keyword Speech EMbedding), a speech embedding model pre-trained
on the task of recognizing a large number of keywords. Speech representations
offered by KeySEM are highly effective for learning new keywords from a limited
number of examples. Comparisons with a diverse range of related work across
several datasets show that our method achieves consistently superior
performance with fewer training examples. Although KeySEM was pre-trained only
on English utterances, the performance gains also extend to datasets from four
other languages indicating that KeySEM learns useful representations well
aligned with the task of keyword spotting. Finally, we demonstrate KeySEM's
ability to learn new keywords sequentially without requiring to re-train on
previously learned keywords. Our experimental observations suggest that KeySEM
is well suited to on-device environments where post-deployment learning and
ease of customization are often desirable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Awasthi_A/0/1/0/all/0/1"&gt;Abhijeet Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kilgour_K/0/1/0/all/0/1"&gt;Kevin Kilgour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rom_H/0/1/0/all/0/1"&gt;Hassan Rom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Backdoors in Human-Centric Language Models. (arXiv:2105.00164v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00164</id>
        <link href="http://arxiv.org/abs/2105.00164"/>
        <updated>2021-06-07T03:06:13.015Z</updated>
        <summary type="html"><![CDATA[Natural language processing (NLP) systems have been proven to be vulnerable
to backdoor attacks, whereby hidden features (backdoors) are trained into a
language model and may only be activated by specific inputs (called triggers),
to trick the model into producing unexpected behaviors. In this paper, we
create covert and natural triggers for textual backdoor attacks, \textit{hidden
backdoors}, where triggers can fool both modern language models and human
inspection. We deploy our hidden backdoors through two state-of-the-art trigger
embedding methods. The first approach via homograph replacement, embeds the
trigger into deep neural networks through the visual spoofing of lookalike
character replacement. The second approach uses subtle differences between text
generated by language models and real natural text to produce trigger sentences
with correct grammar and high fluency. We demonstrate that the proposed hidden
backdoors can be effective across three downstream security-critical NLP tasks,
representative of modern human-centric NLP systems, including toxic comment
detection, neural machine translation (NMT), and question answering (QA). Our
two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at
least $97\%$ with an injection rate of only $3\%$ in toxic comment detection,
$95.1\%$ ASR in NMT with less than $0.5\%$ injected data, and finally $91.12\%$
ASR against QA updated with only 27 poisoning data samples on a model
previously trained with 92,024 samples (0.029\%). We are able to demonstrate
the adversary's high success rate of attacks, while maintaining functionality
for regular users, with triggers inconspicuous by the human administrators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaofeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_T/0/1/0/all/0/1"&gt;Tian Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Benjamin Zi Hao Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Minhui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Haojin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jialiang Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Procedural World Generation Framework for Systematic Evaluation of Continual Learning. (arXiv:2106.02585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02585</id>
        <link href="http://arxiv.org/abs/2106.02585"/>
        <updated>2021-06-07T03:06:13.009Z</updated>
        <summary type="html"><![CDATA[Several families of continual learning techniques have been proposed to
alleviate catastrophic interference in deep neural network training on
non-stationary data. However, a comprehensive comparison and analysis of
limitations remains largely open due to the inaccessibility to suitable
datasets. Empirical examination not only varies immensely between individual
works, it further currently relies on contrived composition of benchmarks
through subdivision and concatenation of various prevalent static vision
datasets. In this work, our goal is to bridge this gap by introducing a
computer graphics simulation framework that repeatedly renders only upcoming
urban scene fragments in an endless real-time procedural world generation
process. At its core lies a modular parametric generative model with adaptable
generative factors. The latter can be used to flexibly compose data streams,
which significantly facilitates a detailed analysis and allows for effortless
investigation of various continual learning schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1"&gt;Timm Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1"&gt;Martin Mundt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pliushch_I/0/1/0/all/0/1"&gt;Iuliia Pliushch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1"&gt;Visvanathan Ramesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT based sentiment analysis: A software engineering perspective. (arXiv:2106.02581v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02581</id>
        <link href="http://arxiv.org/abs/2106.02581"/>
        <updated>2021-06-07T03:06:13.003Z</updated>
        <summary type="html"><![CDATA[Sentiment analysis can provide a suitable lead for the tools used in software
engineering along with the API recommendation systems and relevant libraries to
be used. In this context, the existing tools like SentiCR, SentiStrength-SE,
etc. exhibited low f1-scores that completely defeats the purpose of deployment
of such strategies, thereby there is enough scope of performance improvement.
Recent advancements show that transformer based pre-trained models (e.g., BERT,
RoBERTa, ALBERT, etc.) have displayed better results in the text classification
task. Following this context, the present research explores different
BERT-based models to analyze the sentences in GitHub comments, Jira comments,
and Stack Overflow posts. The paper presents three different strategies to
analyse BERT based model for sentiment analysis, where in the first strategy
the BERT based pre-trained models are fine-tuned; in the second strategy an
ensemble model is developed from BERT variants; and in the third strategy a
compressed model (Distil BERT) is used. The experimental results show that the
BERT based ensemble approach and the compressed BERT model attain improvements
by 6-12% over prevailing tools for the F1 measure on all three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batra_H/0/1/0/all/0/1"&gt;Himanshu Batra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02636</id>
        <link href="http://arxiv.org/abs/2106.02636"/>
        <updated>2021-06-07T03:06:12.981Z</updated>
        <summary type="html"><![CDATA[As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).

Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1"&gt;Jack Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jize Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Driven Road Maintenance Inspection. (arXiv:2106.02567v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02567</id>
        <link href="http://arxiv.org/abs/2106.02567"/>
        <updated>2021-06-07T03:06:12.953Z</updated>
        <summary type="html"><![CDATA[Road infrastructure maintenance inspection is typically a labour-intensive
and critical task to ensure the safety of all the road users. In this work, we
propose a detailed methodology to use state-of-the-art techniques in artificial
intelligence and computer vision to automate a sizeable portion of the
maintenance inspection subtasks and reduce the labour costs. The proposed
methodology uses state-of-the-art computer vision techniques such as object
detection and semantic segmentation to automate inspections on primary road
structures such as the road surface, markings, barriers (guardrails) and
traffic signs. The models are mostly trained on commercially viable datasets
and augmented with proprietary data. We demonstrate that our AI models can not
only automate and scale maintenance inspections on primary road structures but
also result in higher recall compared to traditional manual inspections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1"&gt;Ratnajit Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1"&gt;Haris Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1"&gt;Shabbir Marzban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badar_A/0/1/0/all/0/1"&gt;Ahmed Badar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouns_T/0/1/0/all/0/1"&gt;Terence Brouns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1"&gt;Shruthi Gowda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose and Semantic Map Based Probabilistic Forecast of Vulnerable Road Users' Trajectories. (arXiv:2106.02598v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02598</id>
        <link href="http://arxiv.org/abs/2106.02598"/>
        <updated>2021-06-07T03:06:12.938Z</updated>
        <summary type="html"><![CDATA[In this article, an approach for probabilistic trajectory forecasting of
vulnerable road users (VRUs) is presented, which considers past movements and
the surrounding scene. Past movements are represented by 3D poses reflecting
the posture and movements of individual body parts. The surrounding scene is
modeled in the form of semantic maps showing, e.g., the course of streets,
sidewalks, and the occurrence of obstacles. The forecasts are generated in
grids discretizing the space and in the form of arbitrary discrete probability
distributions. The distributions are evaluated in terms of their reliability,
sharpness, and positional accuracy. We compare our method with an approach that
provides forecasts in the form of Gaussian distributions and discuss the
respective advantages and disadvantages. Thereby, we investigate the impact of
using poses and semantic maps. With a technique called spatial label smoothing,
our approach achieves reliable forecasts. Overall, the poses have a positive
impact on the forecasts. The semantic maps offer the opportunity to adapt the
probability distributions to the individual situation, although at the
considered forecasted time horizon of 2.52 s they play a minor role compared to
the past movements of the VRU. Our method is evaluated on a dataset recorded in
inner-city traffic using a research vehicle. The dataset is made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1"&gt;Viktor Kress&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeske_F/0/1/0/all/0/1"&gt;Fabian Jeske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1"&gt;Stefan Zernetsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1"&gt;Konrad Doll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1"&gt;Bernhard Sick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOUP-GAN: Super-Resolution MRI Using Generative Adversarial Networks. (arXiv:2106.02599v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02599</id>
        <link href="http://arxiv.org/abs/2106.02599"/>
        <updated>2021-06-07T03:06:12.928Z</updated>
        <summary type="html"><![CDATA[There is a growing demand for high-resolution (HR) medical images in both the
clinical and research applications. Image quality is inevitably traded off with
the acquisition time for better patient comfort, lower examination costs, dose,
and fewer motion-induced artifacts. For many image-based tasks, increasing the
apparent resolution in the perpendicular plane to produce multi-planar
reformats or 3D images is commonly used. Single image super-resolution (SR) is
a promising technique to provide HR images based on unsupervised learning to
increase resolution of a 2D image, but there are few reports on 3D SR. Further,
perceptual loss is proposed in the literature to better capture the textual
details and edges than using pixel-wise loss functions, by comparing the
semantic distances in the high-dimensional feature space of a pre-trained 2D
network (e.g., VGG). However, it is not clear how one should generalize it to
3D medical images, and the attendant implications are still unclear. In this
paper, we propose a framework called SOUP-GAN: Super-resolution Optimized Using
Perceptual-tuned Generative Adversarial Network (GAN), in order to produce
thinner slice (e.g., high resolution in the 'Z' plane) medical images with
anti-aliasing and deblurring. The proposed method outperforms other
conventional resolution-enhancement methods and previous SR work on medical
images upon both qualitative and quantitative comparisons. Specifically, we
examine the model in terms of its generalization for various SR ratios and
imaging modalities. By addressing those limitations, our model shows promise as
a novel 3D SR interpolation technique, providing potential applications in both
clinical and research settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haoji Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Philbrick_K/0/1/0/all/0/1"&gt;Kenneth Philbrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1"&gt;Gian Marco Conte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sobek_J/0/1/0/all/0/1"&gt;Joseph D. Sobek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rouzrokh_P/0/1/0/all/0/1"&gt;Pouria Rouzrokh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Erickson_B/0/1/0/all/0/1"&gt;Bradley J. Erickson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking. (arXiv:2106.02495v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02495</id>
        <link href="http://arxiv.org/abs/2106.02495"/>
        <updated>2021-06-07T03:06:12.845Z</updated>
        <summary type="html"><![CDATA[Prior correlation filter (CF)-based tracking methods for unmanned aerial
vehicles (UAVs) have virtually focused on tracking in the daytime. However,
when the night falls, the trackers will encounter more harsh scenes, which can
easily lead to tracking failure. In this regard, this work proposes a novel
tracker with anti-dark function (ADTrack). The proposed method integrates an
efficient and effective low-light image enhancer into a CF-based tracker.
Besides, a target-aware mask is simultaneously generated by virtue of image
illumination variation. The target-aware mask can be applied to jointly train a
target-focused filter that assists the context filter for robust tracking.
Specifically, ADTrack adopts dual regression, where the context filter and the
target-focused filter restrict each other for dual filter learning. Exhaustive
experiments are conducted on typical dark sceneries benchmark, consisting of 37
typical night sequences from authoritative benchmarks, i.e., UAVDark, and our
newly constructed benchmark UAVDark70. The results have shown that ADTrack
favorably outperforms other state-of-the-art trackers and achieves a real-time
speed of 34 frames/s on a single CPU, greatly extending robust UAV tracking to
night scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fuling Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAFLOW: Conditional Autoregressive Flows. (arXiv:2106.02531v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02531</id>
        <link href="http://arxiv.org/abs/2106.02531"/>
        <updated>2021-06-07T03:06:12.818Z</updated>
        <summary type="html"><![CDATA[We introduce CAFLOW, a new diverse image-to-image translation model that
simultaneously leverages the power of auto-regressive modeling and the modeling
efficiency of conditional normalizing flows. We transform the conditioning
image into a sequence of latent encodings using a multi-scale normalizing flow
and repeat the process for the conditioned image. We model the conditional
distribution of the latent encodings by modeling the auto-regressive
distributions with an efficient multi-scale normalizing flow, where each
conditioning factor affects image synthesis at its respective resolution scale.
Our proposed framework performs well on a range of image-to-image translation
tasks. It outperforms former designs of conditional flows because of its
expressive auto-regressive structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batzolis_G/0/1/0/all/0/1"&gt;Georgios Batzolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carioni_M/0/1/0/all/0/1"&gt;Marcello Carioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etmann_C/0/1/0/all/0/1"&gt;Christian Etmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Afyouni_S/0/1/0/all/0/1"&gt;Soroosh Afyouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kourtzi_Z/0/1/0/all/0/1"&gt;Zoe Kourtzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Image Local Autoregressive Transformer. (arXiv:2106.02514v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02514</id>
        <link href="http://arxiv.org/abs/2106.02514"/>
        <updated>2021-06-07T03:06:12.805Z</updated>
        <summary type="html"><![CDATA[Recently, AutoRegressive (AR) models for the whole image generation empowered
by transformers have achieved comparable or even better performance to
Generative Adversarial Networks (GANs). Unfortunately, directly applying such
AR models to edit/change local image regions, may suffer from the problems of
missing global information, slow inference speed, and information leakage of
local guidance. To address these limitations, we propose a novel model -- image
Local Autoregressive Transformer (iLAT), to better facilitate the locally
guided image synthesis. Our iLAT learns the novel local discrete
representations, by the newly proposed local autoregressive (LA) transformer of
the attention mask and convolution mechanism. Thus iLAT can efficiently
synthesize the local image regions by key guidance information. Our iLAT is
evaluated on various locally guided image syntheses, such as pose-guided person
image synthesis and face editing. Both the quantitative and qualitative results
show the efficacy of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1"&gt;Chenjie Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yuxin Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengrong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chengming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1"&gt;XiangYang Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning Pretraining for Detection via Object-Level Contrastive Learning. (arXiv:2106.02637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02637</id>
        <link href="http://arxiv.org/abs/2106.02637"/>
        <updated>2021-06-07T03:06:12.731Z</updated>
        <summary type="html"><![CDATA[Image-level contrastive representation learning has proven to be highly
effective as a generic model for transfer learning. Such generality for
transfer learning, however, sacrifices specificity if we are interested in a
certain downstream task. We argue that this could be sub-optimal and thus
advocate a design principle which encourages alignment between the
self-supervised pretext task and the downstream task. In this paper, we follow
this principle with a pretraining method specifically designed for the task of
object detection. We attain alignment in the following three aspects: 1)
object-level representations are introduced via selective search bounding boxes
as object proposals; 2) the pretraining network architecture incorporates the
same dedicated modules used in the detection pipeline (e.g. FPN); 3) the
pretraining is equipped with object detection properties such as object-level
translation invariance and scale invariance. Our method, called Selective
Object COntrastive learning (SoCo), achieves state-of-the-art results for
transfer performance on COCO detection using a Mask R-CNN framework. Code and
models will be made available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Fangyun Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhirong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Stephen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOLQ: Segmenting Objects by Learning Queries. (arXiv:2106.02351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02351</id>
        <link href="http://arxiv.org/abs/2106.02351"/>
        <updated>2021-06-07T03:06:12.724Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end framework for instance segmentation.
Based on the recently introduced DETR [1], our method, termed SOLQ, segments
objects by learning unified queries. In SOLQ, each query represents one object
and has multiple representations: class, location and mask. The object queries
learned perform classification, box regression and mask encoding simultaneously
in an unified vector form. During training phase, the mask vectors encoded are
supervised by the compression coding of raw spatial masks. In inference time,
mask vectors produced can be directly transformed to spatial masks by the
inverse process of compression coding. Experimental results show that SOLQ can
achieve state-of-the-art performance, surpassing most of existing approaches.
Moreover, the joint learning of unified query representation can greatly
improve the detection performance of original DETR. We hope our SOLQ can serve
as a strong baseline for the Transformer-based instance segmentation. Code is
available at https://github.com/megvii-research/SOLQ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1"&gt;Bin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1"&gt;Fangao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tiancai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yichen Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval. (arXiv:2106.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02400</id>
        <link href="http://arxiv.org/abs/2106.02400"/>
        <updated>2021-06-07T03:06:12.717Z</updated>
        <summary type="html"><![CDATA[Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph's nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Manh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurrin_C/0/1/0/all/0/1"&gt;Cathal Gurrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid attention network based on progressive embedding scale-context for crowd counting. (arXiv:2106.02324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02324</id>
        <link href="http://arxiv.org/abs/2106.02324"/>
        <updated>2021-06-07T03:06:12.699Z</updated>
        <summary type="html"><![CDATA[The existing crowd counting methods usually adopted attention mechanism to
tackle background noise, or applied multi-level features or multi-scales
context fusion to tackle scale variation. However, these approaches deal with
these two problems separately. In this paper, we propose a Hybrid Attention
Network (HAN) by employing Progressive Embedding Scale-context (PES)
information, which enables the network to simultaneously suppress noise and
adapt head scale variation. We build the hybrid attention mechanism through
paralleling spatial attention and channel attention module, which makes the
network to focus more on the human head area and reduce the interference of
background objects. Besides, we embed certain scale-context to the hybrid
attention along the spatial and channel dimensions for alleviating these
counting errors caused by the variation of perspective and head scale. Finally,
we propose a progressive learning strategy through cascading multiple hybrid
attention modules with embedding different scale-context, which can gradually
integrate different scale-context information into the current feature map from
global to local. Ablation experiments provides that the network architecture
can gradually learn multi-scale features and suppress background noise.
Extensive experiments demonstrate that HANet obtain state-of-the-art counting
performance on four mainstream datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fusen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1"&gt;Jun Sang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongyuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Gastric Histopathology Subsize Image Database (GasHisSDB) for Classification Algorithm Test: from Linear Regression to Visual Transformer. (arXiv:2106.02473v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02473</id>
        <link href="http://arxiv.org/abs/2106.02473"/>
        <updated>2021-06-07T03:06:12.678Z</updated>
        <summary type="html"><![CDATA[GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total
of 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,
120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to
realize the function of valuating image classification. In order to prove that
the methods of different periods in the field of image classification have
discrepancies on GasHisSDB, we select a variety of classifiers for evaluation.
Seven classical machine learning classifiers, three CNN classifiers and a novel
transformer-based classifier are selected for testing on image classification
tasks. GasHisSDB is available at the
URL:https://github.com/NEUhwm/GasHisSDB.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changhao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling False Positive/Negative Rates for Deep-Learning-Based Prostate Cancer Detection on Multiparametric MR images. (arXiv:2106.02385v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02385</id>
        <link href="http://arxiv.org/abs/2106.02385"/>
        <updated>2021-06-07T03:06:12.649Z</updated>
        <summary type="html"><![CDATA[Prostate cancer (PCa) is one of the leading causes of death for men
worldwide. Multi-parametric magnetic resonance (mpMR) imaging has emerged as a
non-invasive diagnostic tool for detecting and localising prostate tumours by
specialised radiologists. These radiological examinations, for example, for
differentiating malignant lesions from benign prostatic hyperplasia in
transition zones and for defining the boundaries of clinically significant
cancer, remain challenging and highly skill-and-experience-dependent. We first
investigate experimental results in developing object detection neural networks
that are trained to predict the radiological assessment, using these
high-variance labels. We further argue that such a computer-assisted diagnosis
(CAD) system needs to have the ability to control the false-positive rate (FPR)
or false-negative rate (FNR), in order to be usefully deployed in a clinical
workflow, informing clinical decisions without further human intervention. This
work proposes a novel PCa detection network that incorporates a lesion-level
cost-sensitive loss and an additional slice-level loss based on a
lesion-to-slice mapping function, to manage the lesion- and slice-level costs,
respectively. Our experiments based on 290 clinical patients concludes that 1)
The lesion-level FNR was effectively reduced from 0.19 to 0.10 and the
lesion-level FPR was reduced from 1.03 to 0.66 by changing the lesion-level
cost; 2) The slice-level FNR was reduced from 0.19 to 0.00 by taking into
account the slice-level cost; (3) Both lesion-level and slice-level FNRs were
reduced with lower FP/FPR by changing the lesion-level or slice-level costs,
compared with post-training threshold adjustment using networks without the
proposed cost-aware training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Min_Z/0/1/0/all/0/1"&gt;Zhe Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bianco_F/0/1/0/all/0/1"&gt;Fernando J. Bianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qianye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodell_R/0/1/0/all/0/1"&gt;Rachael Rodell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_W/0/1/0/all/0/1"&gt;Wen Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1"&gt;Dean Barratt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yipeng Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-Adversarial Visual Question Answering. (arXiv:2106.02280v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02280</id>
        <link href="http://arxiv.org/abs/2106.02280"/>
        <updated>2021-06-07T03:06:12.620Z</updated>
        <summary type="html"><![CDATA[Performance on the most commonly used Visual Question Answering dataset (VQA
v2) is starting to approach human accuracy. However, in interacting with
state-of-the-art VQA models, it is clear that the problem is far from being
solved. In order to stress test VQA models, we benchmark them against
human-adversarial examples. Human subjects interact with a state-of-the-art VQA
model, and for each image in the dataset, attempt to find a question where the
model's predicted answer is incorrect. We find that a wide range of
state-of-the-art models perform poorly when evaluated on these examples. We
conduct an extensive analysis of the collected adversarial examples and provide
guidance on future research directions. We hope that this Adversarial VQA
(AdVQA) benchmark can help drive progress in the field and advance the state of
the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_S/0/1/0/all/0/1"&gt;Sasha Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amanpreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1"&gt;Vedanuj Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magana_J/0/1/0/all/0/1"&gt;Jose Alberto Lopez Magana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1"&gt;Wojciech Galuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MASA-SR: Matching Acceleration and Spatial Adaptation for Reference-Based Image Super-Resolution. (arXiv:2106.02299v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02299</id>
        <link href="http://arxiv.org/abs/2106.02299"/>
        <updated>2021-06-07T03:06:12.614Z</updated>
        <summary type="html"><![CDATA[Reference-based image super-resolution (RefSR) has shown promising success in
recovering high-frequency details by utilizing an external reference image
(Ref). In this task, texture details are transferred from the Ref image to the
low-resolution (LR) image according to their point- or patch-wise
correspondence. Therefore, high-quality correspondence matching is critical. It
is also desired to be computationally efficient. Besides, existing RefSR
methods tend to ignore the potential large disparity in distributions between
the LR and Ref images, which hurts the effectiveness of the information
utilization. In this paper, we propose the MASA network for RefSR, where two
novel modules are designed to address these problems. The proposed Match &
Extraction Module significantly reduces the computational cost by a
coarse-to-fine correspondence matching scheme. The Spatial Adaptation Module
learns the difference of distribution between the LR and Ref images, and remaps
the distribution of Ref features to that of LR features in a spatially adaptive
way. This scheme makes the network robust to handle different reference images.
Extensive quantitative and qualitative experiments validate the effectiveness
of our proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liying Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenbo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1"&gt;Xin Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiangbo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Correspondence with Transformers. (arXiv:2106.02520v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02520</id>
        <link href="http://arxiv.org/abs/2106.02520"/>
        <updated>2021-06-07T03:06:12.593Z</updated>
        <summary type="html"><![CDATA[We propose a novel cost aggregation network, called Cost Aggregation with
Transformers (CATs), to find dense correspondences between semantically similar
images with additional challenges posed by large intra-class appearance and
geometric variations. Compared to previous hand-crafted or CNN-based methods
addressing the cost aggregation stage, which either lack robustness to severe
deformations or inherit the limitation of CNNs that fail to discriminate
incorrect matches due to limited receptive fields, CATs explore global
consensus among initial correlation map with the help of some architectural
designs that allow us to exploit full potential of self-attention mechanism.
Specifically, we include appearance affinity modelling to disambiguate the
initial correlation maps and multi-level aggregation to benefit from
hierarchical feature representations within Transformer-based aggregator, and
combine with swapping self-attention and residual connections not only to
enforce consistent matching, but also to ease the learning process. We conduct
experiments to demonstrate the effectiveness of the proposed model over the
latest methods and provide extensive ablation studies. Code and trained models
will be made available at https://github.com/SunghwanHong/CATs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Seokju Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1"&gt;Sunghwan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1"&gt;Sangryul Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yunsung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kwanghoon Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seungryong Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hallucination In Object Detection -- A Study In Visual Part Verification. (arXiv:2106.02523v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02523</id>
        <link href="http://arxiv.org/abs/2106.02523"/>
        <updated>2021-06-07T03:06:12.586Z</updated>
        <summary type="html"><![CDATA[We show that object detectors can hallucinate and detect missing objects;
potentially even accurately localized at their expected, but non-existing,
position. This is particularly problematic for applications that rely on visual
part verification: detecting if an object part is present or absent. We show
how popular object detectors hallucinate objects in a visual part verification
task and introduce the first visual part verification dataset: DelftBikes,
which has 10,000 bike photographs, with 22 densely annotated parts per image,
where some parts may be missing. We explicitly annotated an extra object state
label for each part to reflect if a part is missing or intact. We propose to
evaluate visual part verification by relying on recall and compare popular
object detectors on DelftBikes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kayhan_O/0/1/0/all/0/1"&gt;Osman Semih Kayhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vredebregt_B/0/1/0/all/0/1"&gt;Bart Vredebregt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan C. van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covering Polygons is Even Harder. (arXiv:2106.02335v1 [cs.CG])]]></title>
        <id>http://arxiv.org/abs/2106.02335</id>
        <link href="http://arxiv.org/abs/2106.02335"/>
        <updated>2021-06-07T03:06:12.579Z</updated>
        <summary type="html"><![CDATA[In the MINIMUM CONVEX COVER (MCC) problem, we are given a simple polygon
$\mathcal P$ and an integer $k$, and the question is if there exist $k$ convex
polygons whose union is $\mathcal P$. It is known that MCC is
$\mathsf{NP}$-hard [Culberson & Reckhow: Covering polygons is hard, FOCS
1988/Journal of Algorithms 1994] and in $\exists\mathbb{R}$ [O'Rourke: The
complexity of computing minimum convex covers for polygons, Allerton 1982]. We
prove that MCC is $\exists\mathbb{R}$-hard, and the problem is thus
$\exists\mathbb{R}$-complete. In other words, the problem is equivalent to
deciding whether a system of polynomial equations and inequalities with integer
coefficients has a real solution.

If a cover for our constructed polygon exists, then so does a cover
consisting entirely of triangles. As a byproduct, we therefore also establish
that it is $\exists\mathbb{R}$-complete to decide whether $k$ triangles cover a
given polygon.

The issue that it was not known if finding a minimum cover is in
$\mathsf{NP}$ has repeatedly been raised in the literature, and it was
mentioned as a "long-standing open question" already in 2001 [Eidenbenz &
Widmayer: An approximation algorithm for minimum convex cover with logarithmic
performance guarantee, ESA 2001/SIAM Journal on Computing 2003]. We prove that
assuming the widespread belief that $\mathsf{NP}\neq\exists\mathbb{R}$, the
problem is not in $\mathsf{NP}$.

An implication of the result is that many natural approaches to finding small
covers are bound to give suboptimal solutions in some cases, since irrational
coordinates of arbitrarily high algebraic degree can be needed for the corners
of the pieces in an optimal solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrahamsen_M/0/1/0/all/0/1"&gt;Mikkel Abrahamsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RoadMap: A Light-Weight Semantic Map for Visual Localization towards Autonomous Driving. (arXiv:2106.02527v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02527</id>
        <link href="http://arxiv.org/abs/2106.02527"/>
        <updated>2021-06-07T03:06:12.572Z</updated>
        <summary type="html"><![CDATA[Accurate localization is of crucial importance for autonomous driving tasks.
Nowadays, we have seen a lot of sensor-rich vehicles (e.g. Robo-taxi) driving
on the street autonomously, which rely on high-accurate sensors (e.g. Lidar and
RTK GPS) and high-resolution map. However, low-cost production cars cannot
afford such high expenses on sensors and maps. How to reduce costs? How do
sensor-rich vehicles benefit low-cost cars? In this paper, we proposed a
light-weight localization solution, which relies on low-cost cameras and
compact visual semantic maps. The map is easily produced and updated by
sensor-rich vehicles in a crowd-sourced way. Specifically, the map consists of
several semantic elements, such as lane line, crosswalk, ground sign, and stop
line on the road surface. We introduce the whole framework of on-vehicle
mapping, on-cloud maintenance, and user-end localization. The map data is
collected and preprocessed on vehicles. Then, the crowd-sourced data is
uploaded to a cloud server. The mass data from multiple vehicles are merged on
the cloud so that the semantic map is updated in time. Finally, the semantic
map is compressed and distributed to production cars, which use this map for
localization. We validate the performance of the proposed map in real-world
experiments and compare it against other algorithms. The average size of the
semantic map is $36$ kb/km. We highlight that this framework is a reliable and
practical localization solution for autonomous driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tong Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tongqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yilun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1"&gt;Qing Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporally coherent video anonymization through GAN inpainting. (arXiv:2106.02328v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02328</id>
        <link href="http://arxiv.org/abs/2106.02328"/>
        <updated>2021-06-07T03:06:12.565Z</updated>
        <summary type="html"><![CDATA[This work tackles the problem of temporally coherent face anonymization in
natural video streams.We propose JaGAN, a two-stage system starting with
detecting and masking out faces with black image patches in all individual
frames of the video. The second stage leverages a privacy-preserving Video
Generative Adversarial Network designed to inpaint the missing image patches
with artificially generated faces. Our initial experiments reveal that image
based generative models are not capable of inpainting patches showing temporal
coherent appearance across neighboring video frames. To address this issue we
introduce a newly curated video collection, which is made publicly available
for the research community along with this paper. We also introduce the
Identity Invariance Score IdI as a means to quantify temporal coherency between
neighboring frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balaji_T/0/1/0/all/0/1"&gt;Thangapavithraa Balaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blies_P/0/1/0/all/0/1"&gt;Patrick Blies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_G/0/1/0/all/0/1"&gt;Georg G&amp;#xf6;ri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitsch_R/0/1/0/all/0/1"&gt;Raphael Mitsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wasserer_M/0/1/0/all/0/1"&gt;Marcel Wasserer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1"&gt;Torsten Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained Visual Classification of Plant Species In The Wild: Object Detection as A Reinforced Means of Attention. (arXiv:2106.02141v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02141</id>
        <link href="http://arxiv.org/abs/2106.02141"/>
        <updated>2021-06-07T03:06:12.546Z</updated>
        <summary type="html"><![CDATA[Plant species identification in the wild is a difficult problem in part due
to the high variability of the input data, but also because of complications
induced by the long-tail effects of the datasets distribution. Inspired by the
most recent fine-grained visual classification approaches which are based on
attention to mitigate the effects of data variability, we explore the idea of
using object detection as a form of attention. We introduce a bottom-up
approach based on detecting plant organs and fusing the predictions of a
variable number of organ-based species classifiers. We also curate a new
dataset with a long-tail distribution for evaluating plant organ detection and
organ-based species identification, which is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keaton_M/0/1/0/all/0/1"&gt;Matthew R. Keaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaveri_R/0/1/0/all/0/1"&gt;Ram J. Zaveri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovur_M/0/1/0/all/0/1"&gt;Meghana Kovur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_C/0/1/0/all/0/1"&gt;Cole Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1"&gt;Donald A. Adjeroh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1"&gt;Gianfranco Doretto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of the robustness of NMF algorithms. (arXiv:2106.02213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02213</id>
        <link href="http://arxiv.org/abs/2106.02213"/>
        <updated>2021-06-07T03:06:12.539Z</updated>
        <summary type="html"><![CDATA[We examine three non-negative matrix factorization techniques; L2-norm,
L1-norm, and L2,1-norm. Our aim is to establish the performance of these
different approaches, and their robustness in real-world applications such as
feature selection while managing computational complexity, sensitivity to noise
and more. We thoroughly examine each approach from a theoretical perspective,
and examine the performance of each using a series of experiments drawing on
both the ORL and YaleB datasets. We examine the Relative Reconstruction Errors
(RRE), Average Accuracy and Normalized Mutual Information (NMI) as criteria
under a range of simulated noise scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_A/0/1/0/all/0/1"&gt;Alex D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steele_D/0/1/0/all/0/1"&gt;Damian Steele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subdivision-Based Mesh Convolution Networks. (arXiv:2106.02285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02285</id>
        <link href="http://arxiv.org/abs/2106.02285"/>
        <updated>2021-06-07T03:06:12.531Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have made great breakthroughs in 2D
computer vision. However, the irregular structure of meshes makes it hard to
exploit the power of CNNs directly. A subdivision surface provides a
hierarchical multi-resolution structure, and each face in a closed 2-manifold
triangle mesh is exactly adjacent to three faces. Motivated by these two
properties, this paper introduces a novel and flexible CNN framework, named
SubdivNet, for 3D triangle meshes with Loop subdivision sequence connectivity.
Making an analogy between mesh faces and pixels in a 2D image allows us to
present a mesh convolution operator to aggregate local features from adjacent
faces. By exploiting face neighborhoods, this convolution can support standard
2D convolutional network concepts, e.g. variable kernel size, stride, and
dilation. Based on the multi-resolution hierarchy, we propose a spatial uniform
pooling layer which merges four faces into one and an upsampling method which
splits one face into four. As a result, many popular 2D CNN architectures can
be readily adapted to processing 3D meshes. Meshes with arbitrary connectivity
can be remeshed to hold Loop subdivision sequence connectivity via
self-parameterization, making SubdivNet a general approach. Experiments on mesh
classification, segmentation, correspondence, and retrieval from the real-world
demonstrate the effectiveness and efficiency of SubdivNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shi-Min Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheng-Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Meng-Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jun-Xiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiahui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tai-Jiang Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Ralph R. Martin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOCTOR: A Simple Method for Detecting Misclassification Errors. (arXiv:2106.02395v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02395</id>
        <link href="http://arxiv.org/abs/2106.02395"/>
        <updated>2021-06-07T03:06:12.525Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have shown to perform very well on large scale
object recognition problems and lead to widespread use for real-world
applications, including situations where DNN are implemented as "black boxes".
A promising approach to secure their use is to accept decisions that are likely
to be correct while discarding the others. In this work, we propose DOCTOR, a
simple method that aims to identify whether the prediction of a DNN classifier
should (or should not) be trusted so that, consequently, it would be possible
to accept it or to reject it. Two scenarios are investigated: Totally Black Box
(TBB) where only the soft-predictions are available and Partially Black Box
(PBB) where gradient-propagation to perform input pre-processing is allowed.
Empirically, we show that DOCTOR outperforms all state-of-the-art methods on
various well-known images and sentiment analysis datasets. In particular, we
observe a reduction of up to $4\%$ of the false rejection rate (FRR) in the PBB
scenario. DOCTOR can be applied to any pre-trained model, it does not require
prior information about the underlying dataset and is as simple as the simplest
available methods in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Granese_F/0/1/0/all/0/1"&gt;Federica Granese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanelli_M/0/1/0/all/0/1"&gt;Marco Romanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorla_D/0/1/0/all/0/1"&gt;Daniele Gorla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1"&gt;Catuscia Palamidessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Associating Objects with Transformers for Video Object Segmentation. (arXiv:2106.02638v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02638</id>
        <link href="http://arxiv.org/abs/2106.02638"/>
        <updated>2021-06-07T03:06:12.518Z</updated>
        <summary type="html"><![CDATA[This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computing resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
uniformly. In detail, AOT employs an identification mechanism to associate
multiple targets into the same high-dimensional embedding space. Thus, we can
simultaneously process the matching and segmentation decoding of multiple
objects as efficiently as processing a single object. For sufficiently modeling
multi-object association, a Long Short-Term Transformer is designed for
constructing hierarchical matching and propagation. We conduct extensive
experiments on both multi-object and single-object benchmarks to examine AOT
variant networks with different complexities. Particularly, our AOT-L
outperforms all the state-of-the-art competitors on three popular benchmarks,
i.e., YouTube-VOS (83.7% J&F), DAVIS 2017 (83.0%), and DAVIS 2016 (91.0%),
while keeping better multi-object efficiency. Meanwhile, our AOT-T can maintain
real-time multi-object speed on above benchmarks. We ranked 1st in the 3rd
Large-scale Video Object Segmentation Challenge. The code will be publicly
available at https://github.com/z-x-yang/AOT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Segmentation via Cycle-Consistent Transformer. (arXiv:2106.02320v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02320</id>
        <link href="http://arxiv.org/abs/2106.02320"/>
        <updated>2021-06-07T03:06:12.511Z</updated>
        <summary type="html"><![CDATA[Few-shot segmentation aims to train a segmentation model that can fast adapt
to novel classes with few exemplars. The conventional training paradigm is to
learn to make predictions on query images conditioned on the features from
support images. Previous methods only utilized the semantic-level prototypes of
support images as the conditional information. These methods cannot utilize all
pixel-wise support information for the query predictions, which is however
critical for the segmentation task. In this paper, we focus on utilizing
pixel-wise relationships between support and target images to facilitate the
few-shot semantic segmentation task. We design a novel Cycle-Consistent
Transformer (CyCTR) module to aggregate pixel-wise support features into query
ones. CyCTR performs cross-attention between features from different images,
i.e. support and query images. We observe that there may exist unexpected
irrelevant pixel-level support features. Directly performing cross-attention
may aggregate these features from support to query and bias the query features.
Thus, we propose using a novel cycle-consistent attention mechanism to filter
out possible harmful support features and encourage query features to attend to
the most informative pixels from support images. Experiments on all few-shot
segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable
improvement compared to previous state-of-the-art methods. Specifically, on
Pascal-$5^i$ and COCO-$20^i$ datasets, we achieve 66.6% and 45.6% mIoU for
5-shot segmentation, outperforming previous state-of-the-art by 4.6% and 7.1%
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gengwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1"&gt;Guoliang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NMS-Loss: Learning with Non-Maximum Suppression for Crowded Pedestrian Detection. (arXiv:2106.02426v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02426</id>
        <link href="http://arxiv.org/abs/2106.02426"/>
        <updated>2021-06-07T03:06:12.494Z</updated>
        <summary type="html"><![CDATA[Non-Maximum Suppression (NMS) is essential for object detection and affects
the evaluation results by incorporating False Positives (FP) and False
Negatives (FN), especially in crowd occlusion scenes. In this paper, we raise
the problem of weak connection between the training targets and the evaluation
metrics caused by NMS and propose a novel NMS-Loss making the NMS procedure can
be trained end-to-end without any additional network parameters. Our NMS-Loss
punishes two cases when FP is not suppressed and FN is wrongly eliminated by
NMS. Specifically, we propose a pull loss to pull predictions with the same
target close to each other, and a push loss to push predictions with different
targets away from each other. Experimental results show that with the help of
NMS-Loss, our detector, namely NMS-Ped, achieves impressive results with Miss
Rate of 5.92% on Caltech dataset and 10.08% on CityPersons dataset, which are
both better than state-of-the-art competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zekun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Sixiao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Glance-and-Gaze Vision Transformer. (arXiv:2106.02277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02277</id>
        <link href="http://arxiv.org/abs/2106.02277"/>
        <updated>2021-06-07T03:06:12.486Z</updated>
        <summary type="html"><![CDATA[Recently, there emerges a series of vision Transformers, which show superior
performance with a more compact model size than conventional convolutional
neural networks, thanks to the strong ability of Transformers to model
long-range dependencies. However, the advantages of vision Transformers also
come with a price: Self-attention, the core part of Transformer, has a
quadratic complexity to the input sequence length. This leads to a dramatic
increase of computation and memory cost with the increase of sequence length,
thus introducing difficulties when applying Transformers to the vision tasks
that require dense predictions based on high-resolution feature maps. In this
paper, we propose a new vision Transformer, named Glance-and-Gaze Transformer
(GG-Transformer), to address the aforementioned issues. It is motivated by the
Glance and Gaze behavior of human beings when recognizing objects in natural
scenes, with the ability to efficiently model both long-range dependencies and
local context. In GG-Transformer, the Glance and Gaze behavior is realized by
two parallel branches: The Glance branch is achieved by performing
self-attention on the adaptively-dilated partitions of the input, which leads
to a linear complexity while still enjoying a global receptive field; The Gaze
branch is implemented by a simple depth-wise convolutional layer, which
compensates local image context to the features obtained by the Glance
mechanism. We empirically demonstrate our method achieves consistently superior
performance over previous state-of-the-art Transformers on various vision tasks
and benchmarks. The codes and models will be made available at
https://github.com/yucornetto/GG-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qihang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yingda Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yutong Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yongyi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ukiyo-e Analysis and Creativity with Attribute and Geometry Annotation. (arXiv:2106.02267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02267</id>
        <link href="http://arxiv.org/abs/2106.02267"/>
        <updated>2021-06-07T03:06:12.479Z</updated>
        <summary type="html"><![CDATA[The study of Ukiyo-e, an important genre of pre-modern Japanese art, focuses
on the object and style like other artwork researches. Such study has benefited
from the renewed interest by the machine learning community in culturally
important topics, leading to interdisciplinary works including collections of
images, quantitative approaches, and machine learning-based creativities. They,
however, have several drawbacks, and it remains challenging to integrate these
works into a comprehensive view. To bridge this gap, we propose a holistic
approach We first present a large-scale Ukiyo-e dataset with coherent semantic
labels and geometric annotations, then show its value in a quantitative study
of Ukiyo-e paintings' object using these labels and annotations. We further
demonstrate the machine learning methods could help style study through soft
color decomposition of Ukiyo-e, and finally provides joint insights into object
and style by composing sketches and colors using colorization. Dataset
available at https://github.com/rois-codh/arc-ukiyoe-faces]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yingtao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clanuwat_T/0/1/0/all/0/1"&gt;Tarin Clanuwat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_C/0/1/0/all/0/1"&gt;Chikahiko Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamoto_A/0/1/0/all/0/1"&gt;Asanobu Kitamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Neural Networks with Mixed Hierarchical Structures for Natural Language Processing. (arXiv:2106.02562v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02562</id>
        <link href="http://arxiv.org/abs/2106.02562"/>
        <updated>2021-06-07T03:06:12.473Z</updated>
        <summary type="html"><![CDATA[Hierarchical structures exist in both linguistics and Natural Language
Processing (NLP) tasks. How to design RNNs to learn hierarchical
representations of natural languages remains a long-standing challenge. In this
paper, we define two different types of boundaries referred to as static and
dynamic boundaries, respectively, and then use them to construct a multi-layer
hierarchical structure for document classification tasks. In particular, we
focus on a three-layer hierarchical structure with static word- and sentence-
layers and a dynamic phrase-layer. LSTM cells and two boundary detectors are
used to implement the proposed structure, and the resulting network is called
the {\em Recurrent Neural Network with Mixed Hierarchical Structures}
(MHS-RNN). We further add three layers of attention mechanisms to the MHS-RNN
model. Incorporating attention mechanisms allows our model to use more
important content to construct document representation and enhance its
performance on document classification tasks. Experiments on five different
datasets show that the proposed architecture outperforms previous methods on
all the five tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhaoxin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Michael Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeChat AI & ICT's Submission for DSTC9 Interactive Dialogue Evaluation Track. (arXiv:2101.07947v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07947</id>
        <link href="http://arxiv.org/abs/2101.07947"/>
        <updated>2021-06-07T03:06:12.456Z</updated>
        <summary type="html"><![CDATA[We participate in the DSTC9 Interactive Dialogue Evaluation Track (Gunasekara
et al. 2020) sub-task 1 (Knowledge Grounded Dialogue) and sub-task 2
(Interactive Dialogue). In sub-task 1, we employ a pre-trained language model
to generate topic-related responses and propose a response ensemble method for
response selection. In sub-task2, we propose a novel Dialogue Planning Model
(DPM) to capture conversation flow in the interaction with humans. We also
design an integrated open-domain dialogue system containing pre-process,
dialogue model, scoring model, and post-process, which can generate fluent,
coherent, consistent, and humanlike responses. We tie 1st on human ratings and
also get the highest Meteor, and Bert-score in sub-task 1, and rank 3rd on
interactive human evaluation in sub-task 2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zongjia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On (co-lex) Ordering Automata. (arXiv:2106.02309v1 [cs.FL])]]></title>
        <id>http://arxiv.org/abs/2106.02309</id>
        <link href="http://arxiv.org/abs/2106.02309"/>
        <updated>2021-06-07T03:06:12.450Z</updated>
        <summary type="html"><![CDATA[The states of a deterministic finite automaton A can be identified with
collections of words in Pf(L(A)) -- the set of prefixes of words belonging to
the regular language accepted by A. But words can be ordered and among the many
possible orders a very natural one is the co-lexicographic one. Such
naturalness stems from the fact that it suggests a transfer of the order from
words to the automaton's states. In a number of papers automata admitting a
total ordering of states coherent with the ordering of the set of words
reaching them have been proposed. Such class of ordered automata -- the Wheeler
automata -- turned out to be efficiently stored/searched using an index.
Unfortunately not all automata can be totally ordered as previously outlined.
However, automata can always be partially ordered and an intrinsic measure of
their complexity can be defined and effectively determined, as the minimum
width of one of their admissible partial orders. As shown in previous works,
this new concept of width of an automaton has useful consequences in the fields
of graph compression, indexing data structures, and automata theory. In this
paper we prove that a canonical, minimum-width, partially-ordered automaton
accepting a language L -- dubbed the Hasse automaton H of L -- can be
exhibited. H provides, in a precise sense, the best possible way to (partially)
order the states of any automaton accepting L, as long as we want to maintain
an operational link with the (co-lexicographic) order of Pf(L(A)). Using H we
prove that the width of the language can be effectively computed from the
minimum automaton recognizing the language. Finally, we explore the
relationship between two (often conflicting) objectives: minimizing the width
and minimizing the number of states of an automaton.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAgostino_G/0/1/0/all/0/1"&gt;Giovanna D&amp;#x27;Agostino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotumaccio_N/0/1/0/all/0/1"&gt;Nicola Cotumaccio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Policriti_A/0/1/0/all/0/1"&gt;Alberto Policriti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prezza_N/0/1/0/all/0/1"&gt;Nicola Prezza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings. (arXiv:2106.02490v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02490</id>
        <link href="http://arxiv.org/abs/2106.02490"/>
        <updated>2021-06-07T03:06:12.443Z</updated>
        <summary type="html"><![CDATA[Artificial Neural networks are mathematical models at their core. This
truismpresents some fundamental difficulty when networks are tasked with
Natural Language Processing. A key problem lies in measuring the similarity or
distance among vectors in NLP embedding space, since the mathematical concept
of distance does not always agree with the linguistic concept. We suggest that
the best way to measure linguistic distance among vectors is by employing the
Language Model (LM) that created them. We introduce Language Model Distance
(LMD) for measuring accuracy of vector transformations based on the
Distributional Hypothesis ( LMD Accuracy ). We show the efficacy of this metric
by applying it to a simple neural network learning the Procrustes algorithm for
bilingual word mapping.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conley_T/0/1/0/all/0/1"&gt;Thomas Conley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[History Encoding Representation Design for Human Intention Inference. (arXiv:2106.02222v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02222</id>
        <link href="http://arxiv.org/abs/2106.02222"/>
        <updated>2021-06-07T03:06:12.437Z</updated>
        <summary type="html"><![CDATA[In this extended abstract, we investigate the design of learning
representation for human intention inference. In our designed human intention
prediction task, we propose a history encoding representation that is both
interpretable and effective for prediction. Through extensive experiments, we
show our prediction framework with a history encoding representation design is
successful on the human intention prediction problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhuo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL-DARTS: Differentiable Architecture Search for Reinforcement Learning. (arXiv:2106.02229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02229</id>
        <link href="http://arxiv.org/abs/2106.02229"/>
        <updated>2021-06-07T03:06:12.430Z</updated>
        <summary type="html"><![CDATA[We introduce RL-DARTS, one of the first applications of Differentiable
Architecture Search (DARTS) in reinforcement learning (RL) to search for
convolutional cells, applied to the Procgen benchmark. We outline the initial
difficulties of applying neural architecture search techniques in RL, and
demonstrate that by simply replacing the image encoder with a DARTS supernet,
our search method is sample-efficient, requires minimal extra compute
resources, and is also compatible with off-policy and on-policy RL algorithms,
needing only minor changes in preexisting code. Surprisingly, we find that the
supernet can be used as an actor for inference to generate replay data in
standard RL training loops, and thus train end-to-end. Throughout this training
process, we show that the supernet gradually learns better cells, leading to
alternative architectures which can be highly competitive against manually
designed policies, but also verify previous design choices for RL policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1"&gt;Yingjie Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Daiyi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1"&gt;Summer Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1"&gt;Eugene Brevdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1"&gt;Aleksandra Faust&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-volution: On the unification of convolution and self-attention. (arXiv:2106.02253v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02253</id>
        <link href="http://arxiv.org/abs/2106.02253"/>
        <updated>2021-06-07T03:06:12.423Z</updated>
        <summary type="html"><![CDATA[Convolution and self-attention are acting as two fundamental building blocks
in deep neural networks, where the former extracts local image features in a
linear way while the latter non-locally encodes high-order contextual
relationships. Though essentially complementary to each other, i.e.,
first-/high-order, stat-of-the-art architectures, i.e., CNNs or transformers
lack a principled way to simultaneously apply both operations in a single
computational module, due to their heterogeneous computing pattern and
excessive burden of global dot-product for visual tasks. In this work, we
theoretically derive a global self-attention approximation scheme, which
approximates a self-attention via the convolution operation on transformed
features. Based on the approximated scheme, we establish a multi-branch
elementary module composed of both convolution and self-attention operation,
capable of unifying both local and non-local feature interaction. Importantly,
once trained, this multi-branch module could be conditionally converted into a
single standard convolution operation via structural re-parameterization,
rendering a pure convolution styled operator named X-volution, ready to be
plugged into any modern networks as an atomic operation. Extensive experiments
demonstrate that the proposed X-volution, achieves highly competitive visual
understanding improvements (+1.2% top-1 accuracy on ImageNet classification,
+1.7 box AP and +1.5 mask AP on COCO detection and segmentation).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuanhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNNs and GANs in MRI-based cross-modality medical image estimation. (arXiv:2106.02198v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02198</id>
        <link href="http://arxiv.org/abs/2106.02198"/>
        <updated>2021-06-07T03:06:12.417Z</updated>
        <summary type="html"><![CDATA[Cross-modality image estimation involves the generation of images of one
medical imaging modality from that of another modality. Convolutional neural
networks (CNNs) have been shown to be useful in identifying, characterising and
extracting image patterns. Generative adversarial networks (GANs) use CNNs as
generators and estimated images are discriminated as true or false based on an
additional network. CNNs and GANs within the image estimation framework may be
considered more generally as deep learning approaches, since imaging data tends
to be large, leading to a larger number of network weights. Almost all research
in the CNN/GAN image estimation literature has involved the use of MRI data
with the other modality primarily being PET or CT. This review provides an
overview of the use of CNNs and GANs for MRI-based cross-modality medical image
estimation. We outline the neural networks implemented, and detail network
constructs employed for CNN and GAN image-to-image estimators. Motivations
behind cross-modality image estimation are provided as well. GANs appear to
provide better utility in cross-modality image estimation in comparison with
CNNs, a finding drawn based on our analysis involving metrics comparing
estimated and actual images. Our final remarks highlight key challenges faced
by the cross-modality medical image estimation field, and suggestions for
future research are outlined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fard_A/0/1/0/all/0/1"&gt;Azin Shokraei Fard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Reutens_D/0/1/0/all/0/1"&gt;David C. Reutens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vegh_V/0/1/0/all/0/1"&gt;Viktor Vegh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Question Rewriting for Increasing Response Rate. (arXiv:2106.02257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02257</id>
        <link href="http://arxiv.org/abs/2106.02257"/>
        <updated>2021-06-07T03:06:12.399Z</updated>
        <summary type="html"><![CDATA[When a human asks questions online, or when a conversational virtual agent
asks human questions, questions triggering emotions or with details might more
likely to get responses or answers. we explore how to automatically rewrite
natural language questions to improve the response rate from people. In
particular, a new task of Visual Question Rewriting(VQR) task is introduced to
explore how visual information can be used to improve the new questions. A data
set containing around 4K bland questions, attractive questions and images
triples is collected. We developed some baseline sequence to sequence models
and more advanced transformer based models, which take a bland question and a
related image as input and output a rewritten question that is expected to be
more attractive. Offline experiments and mechanical Turk based evaluations show
that it is possible to rewrite bland questions in a more detailed and
attractive way to increase the response rate, and images can be helpful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiayi Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xilian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASCNet: Self-supervised Video Representation Learning with Appearance-Speed Consistency. (arXiv:2106.02342v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02342</id>
        <link href="http://arxiv.org/abs/2106.02342"/>
        <updated>2021-06-07T03:06:12.392Z</updated>
        <summary type="html"><![CDATA[We study self-supervised video representation learning, which is a
challenging task due to 1) a lack of labels for explicit supervision and 2)
unstructured and noisy visual information. Existing methods mainly use
contrastive loss with video clips as the instances and learn visual
representation by discriminating instances from each other, but they require
careful treatment of negative pairs by relying on large batch sizes, memory
banks, extra modalities, or customized mining strategies, inevitably including
noisy data. In this paper, we observe that the consistency between positive
samples is the key to learn robust video representations. Specifically, we
propose two tasks to learn the appearance and speed consistency, separately.
The appearance consistency task aims to maximize the similarity between two
clips of the same video with different playback speeds. The speed consistency
task aims to maximize the similarity between two clips with the same playback
speed but different appearance information. We show that joint optimization of
the two tasks consistently improves the performance on downstream tasks, e.g.,
action recognition and video retrieval. Remarkably, for action recognition on
the UCF-101 dataset, we achieve 90.8% accuracy without using any additional
modalities or negative pairs for unsupervised pretraining, outperforming the
ImageNet supervised pre-trained model. Codes and models will be available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Deng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhihua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiangmiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Adversarial Learning for Deep Semi-Supervised Facial Action Unit Recognition. (arXiv:2106.02258v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02258</id>
        <link href="http://arxiv.org/abs/2106.02258"/>
        <updated>2021-06-07T03:06:12.385Z</updated>
        <summary type="html"><![CDATA[Current works formulate facial action unit (AU) recognition as a supervised
learning problem, requiring fully AU-labeled facial images during training. It
is challenging if not impossible to provide AU annotations for large numbers of
facial images. Fortunately, AUs appear on all facial images, whether manually
labeled or not, satisfy the underlying anatomic mechanisms and human behavioral
habits. In this paper, we propose a deep semi-supervised framework for facial
action unit recognition from partially AU-labeled facial images. Specifically,
the proposed deep semi-supervised AU recognition approach consists of a deep
recognition network and a discriminator D. The deep recognition network R
learns facial representations from large-scale facial images and AU classifiers
from limited ground truth AU labels. The discriminator D is introduced to
enforce statistical similarity between the AU distribution inherent in ground
truth AU labels and the distribution of the predicted AU labels from labeled
and unlabeled facial images. The deep recognition network aims to minimize
recognition loss from the labeled facial images, to faithfully represent
inherent AU distribution for both labeled and unlabeled facial images, and to
confuse the discriminator. During training, the deep recognition network R and
the discriminator D are optimized alternately. Thus, the inherent AU
distributions caused by underlying anatomic mechanisms are leveraged to
construct better feature representations and AU classifiers from partially
AU-labeled data during training. Experiments on two benchmark databases
demonstrate that the proposed approach successfully captures AU distributions
through adversarial learning and outperforms state-of-the-art AU recognition
work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shangfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yanan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1"&gt;Guozhu Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1"&gt;Bowen Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why does CTC result in peaky behavior?. (arXiv:2105.14849v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14849</id>
        <link href="http://arxiv.org/abs/2105.14849"/>
        <updated>2021-06-07T03:06:12.367Z</updated>
        <summary type="html"><![CDATA[The peaky behavior of CTC models is well known experimentally. However, an
understanding about why peaky behavior occurs is missing, and whether this is a
good property. We provide a formal analysis of the peaky behavior and gradient
descent convergence properties of the CTC loss and related training criteria.
Our analysis provides a deep understanding why peaky behavior occurs and when
it is suboptimal. On a simple example which should be trivial to learn for any
model, we prove that a feed-forward neural network trained with CTC from
uniform initialization converges towards peaky behavior with a 100% error rate.
Our analysis further explains why CTC only works well together with the blank
label. We further demonstrate that peaky behavior does not occur on other
related losses including a label prior model, and that this improves
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1"&gt;Albert Zeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes. (arXiv:2106.02524v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02524</id>
        <link href="http://arxiv.org/abs/2106.02524"/>
        <updated>2021-06-07T03:06:12.360Z</updated>
        <summary type="html"><![CDATA[Continuity of care is crucial to ensuring positive health outcomes for
patients discharged from an inpatient hospital setting, and improved
information sharing can help. To share information, caregivers write discharge
notes containing action items to share with patients and their future
caregivers, but these action items are easily lost due to the lengthiness of
the documents. In this work, we describe our creation of a dataset of clinical
action items annotated over MIMIC-III, the largest publicly available dataset
of real clinical notes. This dataset, which we call CLIP, is annotated by
physicians and covers 718 documents representing 100K sentences. We describe
the task of extracting the action items from these documents as multi-aspect
extractive summarization, with each aspect representing a type of action to be
taken. We evaluate several machine learning models on this task, and show that
the best models exploit in-domain language model pre-training on 59K
unannotated documents, and incorporate context from neighboring sentences. We
also propose an approach to pre-training data selection that allows us to
explore the trade-off between size and domain-specificity of pre-training
datasets for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mullenbach_J/0/1/0/all/0/1"&gt;James Mullenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1"&gt;Yada Pruksachatkun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1"&gt;Sean Adler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seale_J/0/1/0/all/0/1"&gt;Jennifer Seale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swartz_J/0/1/0/all/0/1"&gt;Jordan Swartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKelvey_T/0/1/0/all/0/1"&gt;T. Greg McKelvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion. (arXiv:2004.13631v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13631</id>
        <link href="http://arxiv.org/abs/2004.13631"/>
        <updated>2021-06-07T03:06:12.354Z</updated>
        <summary type="html"><![CDATA[A comprehensive knowledge graph (KG) contains an instance-level entity graph
and an ontology-level concept graph. The two-view KG provides a testbed for
models to "simulate" human's abilities on knowledge abstraction,
concretization, and completion (KACC), which are crucial for human to recognize
the world and manage learned knowledge. Existing studies mainly focus on
partial aspects of KACC. In order to promote thorough analyses for KACC
abilities of models, we propose a unified KG benchmark by improving existing
benchmarks in terms of dataset scale, task coverage, and difficulty.
Specifically, we collect new datasets that contain larger concept graphs,
abundant cross-view links as well as dense entity graphs. Based on the
datasets, we propose novel tasks such as multi-hop knowledge abstraction (MKA),
multi-hop knowledge concretization (MKC) and then design a comprehensive
benchmark. For MKA and MKC tasks, we further annotate multi-hop hierarchical
triples as harder samples. The experimental results of existing methods
demonstrate the challenges of our benchmark. The resource is available at
https://github.com/thunlp/KACC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengding Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xin Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Great Service! Fine-grained Parsing of Implicit Arguments. (arXiv:2106.02561v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02561</id>
        <link href="http://arxiv.org/abs/2106.02561"/>
        <updated>2021-06-07T03:06:12.347Z</updated>
        <summary type="html"><![CDATA[Broad-coverage meaning representations in NLP mostly focus on explicitly
expressed content. More importantly, the scarcity of datasets annotating
diverse implicit roles limits empirical studies into their linguistic nuances.
For example, in the web review "Great service!", the provider and consumer are
implicit arguments of different types. We examine an annotated corpus of
fine-grained implicit arguments (Cui and Hershcovich, 2020) by carefully
re-annotating it, resolving several inconsistencies. Subsequently, we present
the first transition-based neural parser that can handle implicit arguments
dynamically, and experiment with two different transition systems on the
improved dataset. We find that certain types of implicit arguments are more
difficult to parse than others and that the simpler system is more accurate in
recovering implicit arguments, despite having a lower overall parsing score,
attesting current reasoning limitations of NLP models. This work will
facilitate a better understanding of implicit and underspecified language, by
incorporating it holistically into meaning representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1"&gt;Ruixiang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1"&gt;Daniel Hershcovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural semi-Markov CRF for Monolingual Word Alignment. (arXiv:2106.02569v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02569</id>
        <link href="http://arxiv.org/abs/2106.02569"/>
        <updated>2021-06-07T03:06:12.337Z</updated>
        <summary type="html"><![CDATA[Monolingual word alignment is important for studying fine-grained editing
operations (i.e., deletion, addition, and substitution) in text-to-text
generation tasks, such as paraphrase generation, text simplification,
neutralizing biased language, etc. In this paper, we present a novel neural
semi-Markov CRF alignment model, which unifies word and phrase alignments
through variable-length spans. We also create a new benchmark with human
annotations that cover four different text genres to evaluate monolingual word
alignment models in more realistic settings. Experimental results show that our
proposed model outperforms all previous approaches for monolingual word
alignment as well as a competitive QA-based baseline, which was previously only
applied to bilingual data. Our model demonstrates good generalizability to
three out-of-domain datasets and shows great utility in two downstream
applications: automatic text simplification and sentence pair classification
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1"&gt;Wuwei Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Chao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specular reflections removal in colposcopic images based on neural networks: Supervised training with no ground truth previous knowledge. (arXiv:2106.02221v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02221</id>
        <link href="http://arxiv.org/abs/2106.02221"/>
        <updated>2021-06-07T03:06:12.330Z</updated>
        <summary type="html"><![CDATA[Cervical cancer is a malignant tumor that seriously threatens women's health,
and is one of the most common that affects women worldwide. For its early
detection, colposcopic images of the cervix are used for searching for possible
injuries or abnormalities. An inherent characteristic of these images is the
presence of specular reflections (brightness) that make it difficult to observe
some regions, which might imply a misdiagnosis. In this paper, a new strategy
based on neural networks is introduced for eliminating specular reflections and
estimating the unobserved anatomical cervix portion under the bright zones. We
present a supervised learning method, despite not knowing the ground truth from
the beginning, based on training a neural network to learn how to restore any
hidden region of colposcopic images. Once the specular reflections are
identified, they are removed from the image and the previously trained network
is used to fulfill these deleted areas. The quality of the processed images was
evaluated quantitatively and qualitatively. In 21 of the 22 evaluated images,
the detected specular reflections were totally eliminated, whereas, in the
remaining one, these reflections were almost completely eliminated. The
distribution of the colors and the content of the restored images are similar
to those of the originals. The evaluation carried out by a specialist in Cervix
Pathology concluded that, after eliminating the specular reflections, the
anatomical and physiological elements of the cervix are observable in the
restored images, which facilitates the medical diagnosis of cervical
pathologies. Our method has the potential to improve the early detection of
cervical cancer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jimenez_Martin_L/0/1/0/all/0/1"&gt;Lauren Jimenez-Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Perez_D/0/1/0/all/0/1"&gt;Daniel A. Vald&amp;#xe9;s P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asteasuainzarra_A/0/1/0/all/0/1"&gt;Ana M. Solares Asteasuainzarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leonard_L/0/1/0/all/0/1"&gt;Ludwig Leonard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_Romanach_M/0/1/0/all/0/1"&gt;Marta L. Baguer D&amp;#xed;az-Roma&amp;#xf1;ach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the Background Bias in Sparse Object Detection via Cropped Windows. (arXiv:2106.02288v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02288</id>
        <link href="http://arxiv.org/abs/2106.02288"/>
        <updated>2021-06-07T03:06:12.301Z</updated>
        <summary type="html"><![CDATA[Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging
task. The recordings are mostly sparse and contain only small objects. In this
work, we propose a simple tiling method that improves the detection capability
in the remote sensing case without modifying the model itself. By reducing the
background bias and enabling the usage of higher image resolutions during
training, our method can improve the performance of models substantially. The
procedure was validated on three different data sets and outperformed similar
approaches in performance and speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varga_L/0/1/0/all/0/1"&gt;Leon Amadeus Varga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[cs60075_team2 at SemEval-2021 Task 1 : Lexical Complexity Prediction using Transformer-based Language Models pre-trained on various text corpora. (arXiv:2106.02340v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02340</id>
        <link href="http://arxiv.org/abs/2106.02340"/>
        <updated>2021-06-07T03:06:12.205Z</updated>
        <summary type="html"><![CDATA[This paper describes the performance of the team cs60075_team2 at SemEval
2021 Task 1 - Lexical Complexity Prediction. The main contribution of this
paper is to fine-tune transformer-based language models pre-trained on several
text corpora, some being general (E.g., Wikipedia, BooksCorpus), some being the
corpora from which the CompLex Dataset was extracted, and others being from
other specific domains such as Finance, Law, etc. We perform ablation studies
on selecting the transformer models and how their individual complexity scores
are aggregated to get the resulting complexity scores. Our method achieves a
best Pearson Correlation of $0.784$ in sub-task 1 (single word) and $0.836$ in
sub-task 2 (multiple word expressions).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nandy_A/0/1/0/all/0/1"&gt;Abhilash Nandy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adak_S/0/1/0/all/0/1"&gt;Sayantan Adak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halder_T/0/1/0/all/0/1"&gt;Tanurima Halder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokala_S/0/1/0/all/0/1"&gt;Sai Mahesh Pokala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Classifying Continuous Constraint Satisfaction problems. (arXiv:2106.02397v1 [cs.CC])]]></title>
        <id>http://arxiv.org/abs/2106.02397</id>
        <link href="http://arxiv.org/abs/2106.02397"/>
        <updated>2021-06-07T03:06:12.165Z</updated>
        <summary type="html"><![CDATA[A continuous constraint satisfaction problem (CCSP) is a constraint
satisfaction problem (CSP) with a domain $U \subset \mathbb{R}$. We engage in a
systematic study to classify CCSPs that are complete of the Existential Theory
of the Reals, i.e., ER-complete. To define this class, we first consider the
problem ETR, which also stands for Existential Theory of the Reals. In an
instance of this problem we are given some sentence of the form $\exists x_1,
\ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where $\Phi$ is a
well-formed quantifier-free formula consisting of the symbols $\{0, 1, +,
\cdot, \geq, >, \wedge, \vee, \neg\}$, the goal is to check whether this
sentence is true. Now the class ER is the family of all problems that admit a
polynomial-time reduction to ETR. It is known that NP $\subseteq$ ER
$\subseteq$ PSPACE.

We restrict our attention on CCSPs with addition constraints ($x + y = z$)
and some other mild technical condition. Previously, it was shown that
multiplication constraints ($x \cdot y = z$), squaring constraints ($x^2 = y$),
or inversion constraints ($x\cdot y = 1$) are sufficient to establish
ER-completeness. We extend this in the strongest possible sense for equality
constraints as follows. We show that CCSPs (with addition constraints and some
other mild technical condition) that have any one well-behaved curved equality
constraint ($f(x,y) = 0$) are ER-complete. We further extend our results to
inequality constraints. We show that any well-behaved convexly curved and any
well-behaved concavely curved inequality constraint ($f(x,y) \geq 0$ and
$g(x,y) \geq 0$) imply ER-completeness on the class of such CCSPs.

We apply our findings to geometric packing and answer an open question by
Abrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing
convex pieces into a square container under rotations and translations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miltzow_T/0/1/0/all/0/1"&gt;Tillmann Miltzow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmiermann_R/0/1/0/all/0/1"&gt;Reinier F. Schmiermann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Contextualized Knowledge Structures for Commonsense Reasoning. (arXiv:2010.12873v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12873</id>
        <link href="http://arxiv.org/abs/2010.12873"/>
        <updated>2021-06-07T03:06:12.143Z</updated>
        <summary type="html"><![CDATA[Recently, knowledge graph (KG) augmented models have achieved noteworthy
success on various commonsense reasoning tasks. However, KG edge (fact)
sparsity and noisy edge extraction/generation often hinder models from
obtaining useful knowledge to reason over. To address these issues, we propose
a new KG-augmented model: Hybrid Graph Network (HGN). Unlike prior methods, HGN
learns to jointly contextualize extracted and generated knowledge by reasoning
over both within a unified graph structure. Given the task input context and an
extracted KG subgraph, HGN is trained to generate embeddings for the subgraph's
missing edges to form a "hybrid" graph, then reason over the hybrid graph while
filtering out context-irrelevant edges. We demonstrate HGN's effectiveness
through considerable performance gains across four commonsense reasoning
benchmarks, plus a user study on edge validness and helpfulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1"&gt;Mrigank Raman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Aaron Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1"&gt;Ryan Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Handong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungchul Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1"&gt;Nedim Lipka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Themes within Complex Unstructured Texts: A Case Study on Safeguarding Reports. (arXiv:2010.14584v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14584</id>
        <link href="http://arxiv.org/abs/2010.14584"/>
        <updated>2021-06-07T03:06:12.123Z</updated>
        <summary type="html"><![CDATA[The task of text and sentence classification is associated with the need for
large amounts of labelled training data. The acquisition of high volumes of
labelled datasets can be expensive or unfeasible, especially for
highly-specialised domains for which documents are hard to obtain. Research on
the application of supervised classification based on small amounts of training
data is limited. In this paper, we address the combination of state-of-the-art
deep learning and classification methods and provide an insight into what
combination of methods fit the needs of small, domain-specific, and
terminologically-rich corpora. We focus on a real-world scenario related to a
collection of safeguarding reports comprising learning experiences and
reflections on tackling serious incidents involving children and vulnerable
adults. The relatively small volume of available reports and their use of
highly domain-specific terminology makes the application of automated
approaches difficult. We focus on the problem of automatically identifying the
main themes in a safeguarding report using supervised classification
approaches. Our results show the potential of deep learning models to simulate
subject-expert behaviour even for complex tasks with limited labelled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_A/0/1/0/all/0/1"&gt;Aleksandra Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1"&gt;David Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1"&gt;Jose Camacho-Collados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribaupierre_H/0/1/0/all/0/1"&gt;H&amp;#xe9;l&amp;#xe8;ne de Ribaupierre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1"&gt;Alun Preece&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Draw: Emergent Communication through Sketching. (arXiv:2106.02067v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02067</id>
        <link href="http://arxiv.org/abs/2106.02067"/>
        <updated>2021-06-07T03:06:12.054Z</updated>
        <summary type="html"><![CDATA[Evidence that visual communication preceded written language and provided a
basis for it goes back to prehistory, in forms such as cave and rock paintings
depicting traces of our distant ancestors. Emergent communication research has
sought to explore how agents can learn to communicate in order to
collaboratively solve tasks. Existing research has focused on language, with a
learned communication channel transmitting sequences of discrete tokens between
the agents. In this work, we explore a visual communication channel between
agents that are allowed to draw with simple strokes. Our agents are
parameterised by deep neural networks, and the drawing procedure is
differentiable, allowing for end-to-end training. In the framework of a
referential communication game, we demonstrate that agents can not only
successfully learn to communicate by drawing, but with appropriate inductive
biases, can do so in a fashion that humans can interpret. We hope to encourage
future research to consider visual communication as a more flexible and
directly interpretable alternative of training collaborative agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1"&gt;Daniela Mihai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Negation in Cognitive Reasoning. (arXiv:2012.12641v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12641</id>
        <link href="http://arxiv.org/abs/2012.12641"/>
        <updated>2021-06-07T03:06:12.042Z</updated>
        <summary type="html"><![CDATA[Negation is both an operation in formal logic and in natural language by
which a proposition is replaced by one stating the opposite, as by the addition
of "not" or another negation cue. Treating negation in an adequate way is
required for cognitive reasoning, which aims at modeling the human ability to
draw meaningful conclusions despite incomplete and inconsistent knowledge. One
task of cognitive reasoning is answering questions given by sentences in
natural language. There are tools based on discourse representation theory to
convert sentences automatically into a formal logic representation, and
additional knowledge can be added using the predicate names in the formula and
knowledge databases. However, the knowledge in logic databases in practice
always is incomplete. Hence, forward reasoning of automated reasoning systems
alone does not suffice to derive answers to questions because, instead of
complete proofs, often only partial positive knowledge can be derived, while
negative knowledge is used only during the reasoning process. In consequence,
we aim at eliminating syntactic negation, strictly speaking, the negated event
or property. In this paper, we describe an effective procedure to determine the
negated event or property in order to replace it by its inverse. This lays the
basis of cognitive reasoning, employing both logic and machine learning for
general question answering. We evaluate our procedure by several benchmarks and
demonstrate its practical usefulness in our cognitive reasoning system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schon_C/0/1/0/all/0/1"&gt;Claudia Schon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siebert_S/0/1/0/all/0/1"&gt;Sophie Siebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolzenburg_F/0/1/0/all/0/1"&gt;Frieder Stolzenburg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entity Concept-enhanced Few-shot Relation Extraction. (arXiv:2106.02401v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02401</id>
        <link href="http://arxiv.org/abs/2106.02401"/>
        <updated>2021-06-07T03:06:12.027Z</updated>
        <summary type="html"><![CDATA[Few-shot relation extraction (FSRE) is of great importance in long-tail
distribution problem, especially in special domain with low-resource data. Most
existing FSRE algorithms fail to accurately classify the relations merely based
on the information of the sentences together with the recognized entity pairs,
due to limited samples and lack of knowledge. To address this problem, in this
paper, we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction
scheme (ConceptFERE), which introduces the inherent concepts of entities to
provide clues for relation prediction and boost the relations classification
performance. Firstly, a concept-sentence attention module is developed to
select the most appropriate concept from multiple concepts of each entity by
calculating the semantic similarity between sentences and concepts. Secondly, a
self-attention based fusion module is presented to bridge the gap of concept
embedding and sentence embedding from different semantic spaces. Extensive
experiments on the FSRE benchmark dataset FewRel have demonstrated the
effectiveness and the superiority of the proposed ConceptFERE scheme as
compared to the state-of-the-art baselines. Code is available at
https://github.com/LittleGuoKe/ConceptFERE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guanglin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qinghua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowing the No-match: Entity Alignment with Dangling Cases. (arXiv:2106.02248v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02248</id>
        <link href="http://arxiv.org/abs/2106.02248"/>
        <updated>2021-06-07T03:06:11.954Z</updated>
        <summary type="html"><![CDATA[This paper studies a new problem setting of entity alignment for knowledge
graphs (KGs). Since KGs possess different sets of entities, there could be
entities that cannot find alignment across them, leading to the problem of
dangling entities. As the first attempt to this problem, we construct a new
dataset and design a multi-task learning framework for both entity alignment
and dangling entity detection. The framework can opt to abstain from predicting
alignment for the detected dangling entities. We propose three techniques for
dangling entity detection that are based on the distribution of
nearest-neighbor distances, i.e., nearest neighbor classification, marginal
ranking and background ranking. After detecting and removing dangling entities,
an incorporated entity alignment model in our framework can provide more robust
alignment for remaining entities. Comprehensive experiments and analyses
demonstrate the effectiveness of our framework. We further discover that the
dangling entity detection module can, in turn, improve alignment learning and
the final performance. The contributed resource is publicly available to foster
further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zequn Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Barcode Method for Generative Model Evaluation driven by Topological Data Analysis. (arXiv:2106.02207v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02207</id>
        <link href="http://arxiv.org/abs/2106.02207"/>
        <updated>2021-06-07T03:06:11.936Z</updated>
        <summary type="html"><![CDATA[Evaluating the performance of generative models in image synthesis is a
challenging task. Although the Fr\'echet Inception Distance is a widely
accepted evaluation metric, it integrates different aspects (e.g., fidelity and
diversity) of synthesized images into a single score and assumes the normality
of embedded vectors. Recent methods such as precision-and-recall and its
variants such as density-and-coverage have been developed to separate fidelity
and diversity based on k-nearest neighborhood methods. In this study, we
propose an algorithm named barcode, which is inspired by the topological data
analysis and is almost free of assumption and hyperparameter selections. In
extensive experiments on real-world datasets as well as theoretical approach on
high-dimensional normal samples, it was found that the 'usual' normality
assumption of embedded vectors has several drawbacks. The experimental results
demonstrate that barcode outperforms other methods in evaluating fidelity and
diversity of GAN outputs. Official codes can be found in
https://github.com/minjeekim00/Barcode.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_R/0/1/0/all/0/1"&gt;Ryoungwoo Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minjee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eun_D/0/1/0/all/0/1"&gt;Da-in Eun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyungjin Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1"&gt;Jiyeon Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1"&gt;Namkug Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Laplacian-Based Dimensionality Reduction Including Spectral Clustering, Laplacian Eigenmap, Locality Preserving Projection, Graph Embedding, and Diffusion Map: Tutorial and Survey. (arXiv:2106.02154v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02154</id>
        <link href="http://arxiv.org/abs/2106.02154"/>
        <updated>2021-06-07T03:06:11.929Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper for nonlinear dimensionality and feature
extraction methods which are based on the Laplacian of graph of data. We first
introduce adjacency matrix, definition of Laplacian matrix, and the
interpretation of Laplacian. Then, we cover the cuts of graph and spectral
clustering which applies clustering in a subspace of data. Different
optimization variants of Laplacian eigenmap and its out-of-sample extension are
explained. Thereafter, we introduce the locality preserving projection and its
kernel variant as linear special cases of Laplacian eigenmap. Versions of graph
embedding are then explained which are generalized versions of Laplacian
eigenmap and locality preserving projection. Finally, diffusion map is
introduced which is a method based on Laplacian of data and random walks on the
data graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning via Persistency of Excitation. (arXiv:2106.02078v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02078</id>
        <link href="http://arxiv.org/abs/2106.02078"/>
        <updated>2021-06-07T03:06:11.921Z</updated>
        <summary type="html"><![CDATA[Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a network is a parameter estimation problem.
In adaptive control theory, maintaining persistency of excitation (PoE) is
integral to ensuring convergence of parameter estimates in dynamical systems to
their robust optima. In this work, we show that network training using gradient
descent is equivalent to a dynamical system parameter estimation problem.
Leveraging this relationship, we prove a sufficient condition for PoE of
gradient descent is achieved when the learning rate is less than the inverse of
the Lipschitz constant of the gradient of loss function. We provide an
efficient technique for estimating the corresponding Lipschitz constant using
extreme value theory and demonstrate that by only scaling the learning rate
schedule we can increase adversarial accuracy by up to 15% on benchmark
datasets. Our approach also universally increases the adversarial accuracy by
0.1% to 0.3% in various state-of-the-art adversarially trained models on the
AutoAttack benchmark, where every small margin of improvement is significant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1"&gt;Kaustubh Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1"&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1"&gt;Insup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1"&gt;James Weimer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory. (arXiv:2106.02317v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02317</id>
        <link href="http://arxiv.org/abs/2106.02317"/>
        <updated>2021-06-07T03:06:11.913Z</updated>
        <summary type="html"><![CDATA[Dialogue policy learning, a subtask that determines the content of system
response generation and then the degree of task completion, is essential for
task-oriented dialogue systems. However, the unbalanced distribution of system
actions in dialogue datasets often causes difficulty in learning to generate
desired actions and responses. In this paper, we propose a
retrieve-and-memorize framework to enhance the learning of system actions.
Specially, we first design a neural context-aware retrieval module to retrieve
multiple candidate system actions from the training set given a dialogue
context. Then, we propose a memory-augmented multi-decoder network to generate
the system actions conditioned on the candidate actions, which allows the
network to adaptively select key information in the candidate actions and
ignore noises. We conduct experiments on the large-scale multi-domain
task-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1.~Experimental
results show that our method achieves competitive performance among several
state-of-the-art models in the context-to-response generation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yunyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1"&gt;Xiaojun Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jianxing Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regular Expressions for Fast-response COVID-19 Text Classification. (arXiv:2102.09507v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09507</id>
        <link href="http://arxiv.org/abs/2102.09507"/>
        <updated>2021-06-07T03:06:11.906Z</updated>
        <summary type="html"><![CDATA[Text classifiers are at the core of many NLP applications and use a variety
of algorithmic approaches and software. This paper introduces infrastructure
and methodologies for text classifiers based on large-scale regular
expressions. In particular, we describe how Facebook determines if a given
piece of text - anything from a hashtag to a post - belongs to a narrow topic
such as COVID-19. To fully define a topic and evaluate classifier performance
we employ human-guided iterations of keyword discovery, but do not require
labeled data. For COVID-19, we build two sets of regular expressions: (1) for
66 languages, with 99% precision and recall >50%, (2) for the 11 most common
languages, with precision >90% and recall >90%. Regular expressions enable
low-latency queries from multiple platforms. Response to challenges like
COVID-19 is fast and so are revisions. Comparisons to a DNN classifier show
explainable results, higher precision and recall, and less overfitting. Our
learnings can be applied to other narrow-topic classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Igor L. Markov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jacqueline Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vagner_A/0/1/0/all/0/1"&gt;Adam Vagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02636</id>
        <link href="http://arxiv.org/abs/2106.02636"/>
        <updated>2021-06-07T03:06:11.887Z</updated>
        <summary type="html"><![CDATA[As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).

Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1"&gt;Jack Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jize Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotation Curricula to Implicitly Train Non-Expert Annotators. (arXiv:2106.02382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02382</id>
        <link href="http://arxiv.org/abs/2106.02382"/>
        <updated>2021-06-07T03:06:11.880Z</updated>
        <summary type="html"><![CDATA[Annotation studies often require annotators to familiarize themselves with
the task, its annotation scheme, and the data domain. This can be overwhelming
in the beginning, mentally taxing, and induce errors into the resulting
annotations; especially in citizen science or crowd sourcing scenarios where
domain expertise is not required and only annotation guidelines are provided.
To alleviate these issues, we propose annotation curricula, a novel approach to
implicitly train annotators. Our goal is to gradually introduce annotators into
the task by ordering instances that are annotated according to a learning
curriculum. To do so, we first formalize annotation curricula for sentence- and
paragraph-level annotation tasks, define an ordering strategy, and identify
well-performing heuristics and interactively trained models on three existing
English datasets. We then conduct a user study with 40 voluntary participants
who are asked to identify the most fitting misconception for English tweets
about the Covid-19 pandemic. Our results show that using a simple heuristic to
order instances can already significantly reduce the total annotation time
while preserving a high annotation quality. Annotation curricula thus can
provide a novel way to improve data collection. To facilitate future research,
we further share our code and data consisting of 2,400 annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Ji-Ung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klie_J/0/1/0/all/0/1"&gt;Jan-Christoph Klie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language. (arXiv:2012.13048v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13048</id>
        <link href="http://arxiv.org/abs/2012.13048"/>
        <updated>2021-06-07T03:06:11.872Z</updated>
        <summary type="html"><![CDATA[Transformers have been shown to emulate logical deduction over natural
language theories (logical rules expressed in natural language), reliably
assigning true/false labels to candidate implications. However, their ability
to generate implications of a theory has not yet been demonstrated, and methods
for reconstructing proofs of answers are imperfect. In this work we show that a
generative model, called ProofWriter, can reliably generate both implications
of a theory and the natural language proof(s) that support them. In particular,
iterating a 1-step implication generator results in proofs that are highly
reliable, and represent actual model decisions (rather than post-hoc
rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's
proofs exceed previous methods by +9% absolute, and in a way that generalizes
to proof depths unseen in training and on out-of-domain problems. We also show
that generative techniques can perform a type of abduction with high precision:
Given a theory and an unprovable conclusion, identify a missing fact that
allows the conclusion to be proved, along with a proof. These results
significantly improve the viability of neural methods for systematically
reasoning over natural language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1"&gt;Oyvind Tafjord&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1"&gt;Bhavana Dalvi Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1"&gt;Peter Clark&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing. (arXiv:2106.02559v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02559</id>
        <link href="http://arxiv.org/abs/2106.02559"/>
        <updated>2021-06-07T03:06:11.866Z</updated>
        <summary type="html"><![CDATA[Analysing whether neural language models encode linguistic information has
become popular in NLP. One method of doing so, which is frequently cited to
support the claim that models like BERT encode syntax, is called probing;
probes are small supervised models trained to extract linguistic information
from another model's output. If a probe is able to predict a particular
structure, it is argued that the model whose output it is trained on must have
implicitly learnt to encode it. However, drawing a generalisation about a
model's linguistic knowledge about a specific phenomena based on what a probe
is able to learn may be problematic: in this work, we show that semantic cues
in training data means that syntactic probes do not properly isolate syntax. We
generate a new corpus of semantically nonsensical but syntactically well-formed
Jabberwocky sentences, which we use to evaluate two probes trained on normal
data. We train the probes on several popular language models (BERT, GPT, and
RoBERTa), and find that in all settings they perform worse when evaluated on
these data, for one probe by an average of 15.4 UUAS points absolute. Although
in most cases they still outperform the baselines, their lead is reduced
substantially, e.g. by 53% in the case of BERT for one probe. This begs the
question: what empirical scores constitute knowing syntax?]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maudslay_R/0/1/0/all/0/1"&gt;Rowan Hall Maudslay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Democracy: Using Deep Learning to Identify and Prevent Misinformation. (arXiv:2106.02607v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.02607</id>
        <link href="http://arxiv.org/abs/2106.02607"/>
        <updated>2021-06-07T03:06:11.859Z</updated>
        <summary type="html"><![CDATA[The rise in online misinformation in recent years threatens democracies by
distorting authentic public discourse and causing confusion, fear, and even, in
extreme cases, violence. There is a need to understand the spread of false
content through online networks for developing interventions that disrupt
misinformation before it achieves virality. Using a Deep Bidirectional
Transformer for Language Understanding (BERT) and propagation graphs, this
study classifies and visualizes the spread of misinformation on a social media
network using publicly available Twitter data. The results confirm prior
research around user clusters and the virality of false content while improving
the precision of deep learning models for misinformation detection. The study
further demonstrates the suitability of BERT for providing a scalable model for
false information detection, which can contribute to the development of more
timely and accurate interventions to slow the spread of misinformation in
online environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhm_A/0/1/0/all/0/1"&gt;Alyssa Suhm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahankal_P/0/1/0/all/0/1"&gt;Prathamesh Mahankal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuntharaj_S/0/1/0/all/0/1"&gt;Subhiksha Mukuntharaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parab_M/0/1/0/all/0/1"&gt;Meghana D. Parab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_M/0/1/0/all/0/1"&gt;Malvika Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1"&gt;Meredith Berger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethumadhavan_A/0/1/0/all/0/1"&gt;Arathi Sethumadhavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaiman_A/0/1/0/all/0/1"&gt;Ashish Jaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Attention for Visual Question Answering. (arXiv:2011.02164v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02164</id>
        <link href="http://arxiv.org/abs/2011.02164"/>
        <updated>2021-06-07T03:06:11.841Z</updated>
        <summary type="html"><![CDATA[We consider the problem of Visual Question Answering (VQA). Given an image
and a free-form, open-ended, question, expressed in natural language, the goal
of VQA system is to provide accurate answer to this question with respect to
the image. The task is challenging because it requires simultaneous and
intricate understanding of both visual and textual information. Attention,
which captures intra- and inter-modal dependencies, has emerged as perhaps the
most widely used mechanism for addressing these challenges. In this paper, we
propose an improved attention-based architecture to solve VQA. We incorporate
an Attention on Attention (AoA) module within encoder-decoder framework, which
is able to determine the relation between attention results and queries.
Attention module generates weighted average for each query. On the other hand,
AoA module first generates an information vector and an attention gate using
attention results and current context; and then adds another attention to
generate final attended information by multiplying the two. We also propose
multimodal fusion module to combine both visual and textual information. The
goal of this fusion module is to dynamically decide how much information should
be considered from each modality. Extensive experiments on VQA-v2 benchmark
dataset show that our method achieves the state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tanzila Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1"&gt;Shih-Han Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1"&gt;Leonid Sigal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding. (arXiv:2106.02318v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02318</id>
        <link href="http://arxiv.org/abs/2106.02318"/>
        <updated>2021-06-07T03:06:11.834Z</updated>
        <summary type="html"><![CDATA[Automatic extraction of product attribute values is an important enabling
technology in e-Commerce platforms. This task is usually modeled using sequence
labeling architectures, with several extensions to handle multi-attribute
extraction. One line of previous work constructs attribute-specific models,
through separate decoders or entirely separate models. However, this approach
constrains knowledge sharing across different attributes. Other contributions
use a single multi-attribute model, with different techniques to embed
attribute information. But sharing the entire network parameters across all
attributes can limit the model's capacity to capture attribute-specific
characteristics. In this paper we present AdaTag, which uses adaptive decoding
to handle extraction. We parameterize the decoder with pretrained attribute
embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This
allows for separate, but semantically correlated, decoders to be generated on
the fly for different attributes. This approach facilitates knowledge sharing,
while maintaining the specificity of each attribute. Our experiments on a
real-world e-Commerce dataset show marked improvements over previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1"&gt;Nasser Zalmout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grant_C/0/1/0/all/0/1"&gt;Christan Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Luna Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Computer Generated Dialog with Auxiliary Loss Functions and Custom Evaluation Metrics. (arXiv:2106.02516v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02516</id>
        <link href="http://arxiv.org/abs/2106.02516"/>
        <updated>2021-06-07T03:06:11.827Z</updated>
        <summary type="html"><![CDATA[Although people have the ability to engage in vapid dialogue without effort,
this may not be a uniquely human trait. Since the 1960's researchers have been
trying to create agents that can generate artificial conversation. These
programs are commonly known as chatbots. With increasing use of neural networks
for dialog generation, some conclude that this goal has been achieved. This
research joins the quest by creating a dialog generating Recurrent Neural
Network (RNN) and by enhancing the ability of this network with auxiliary loss
functions and a beam search. Our custom loss functions achieve better cohesion
and coherence by including calculations of Maximum Mutual Information (MMI) and
entropy. We demonstrate the effectiveness of this system by using a set of
custom evaluation metrics inspired by an abundance of previous research and
based on tried-and-true principles of Natural Language Processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conley_T/0/1/0/all/0/1"&gt;Thomas Conley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clair_J/0/1/0/all/0/1"&gt;Jack St. Clair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02359</id>
        <link href="http://arxiv.org/abs/2106.02359"/>
        <updated>2021-06-07T03:06:11.820Z</updated>
        <summary type="html"><![CDATA[Recent years have seen many breakthroughs in natural language processing
(NLP), transitioning it from a mostly theoretical field to one with many
real-world applications. Noting the rising number of applications of other
machine learning and AI techniques with pervasive societal impact, we
anticipate the rising importance of developing NLP technologies for social
good. Inspired by theories in moral philosophy and global priorities research,
we aim to promote a guideline for social good in the context of NLP. We lay the
foundations via moral philosophy's definition of social good, propose a
framework to evaluate NLP tasks' direct and indirect real-world impact, and
adopt the methodology of global priorities research to identify priority causes
for NLP research. Finally, we use our theoretical framework to provide some
practical guidelines for future NLP research for social good. Our data and
codes are available at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhijing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1"&gt;Geeticka Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1"&gt;Brian Tse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Scaling for Universal Suggested Replies Model. (arXiv:2106.02232v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02232</id>
        <link href="http://arxiv.org/abs/2106.02232"/>
        <updated>2021-06-07T03:06:11.814Z</updated>
        <summary type="html"><![CDATA[We consider the problem of scaling automated suggested replies for Outlook
email system to multiple languages. Faced with increased compute requirements
and low resources for language expansion, we build a single universal model for
improving the quality and reducing run-time costs of our production system.
However, restricted data movement across regional centers prevents joint
training across languages. To this end, we propose a multi-task continual
learning framework, with auxiliary tasks and language adapters to learn
universal language representation across regions. The experimental results show
positive cross-lingual transfer across languages while reducing catastrophic
forgetting across regions. Our online results on real user traffic show
significant gains in CTR and characters saved, as well as 65% training cost
reduction compared with per-language models. As a consequence, we have scaled
the feature in multiple languages including low-resource markets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1"&gt;Qianlan Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1"&gt;Payal Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1"&gt;Budhaditya Deb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bojia Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1"&gt;Milad Shokouhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xia Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Compress Once: Towards Effective and Elastic BERT Compression via Exploit-Explore Stochastic Nature Gradient. (arXiv:2106.02435v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02435</id>
        <link href="http://arxiv.org/abs/2106.02435"/>
        <updated>2021-06-07T03:06:11.775Z</updated>
        <summary type="html"><![CDATA[Despite superior performance on various natural language processing tasks,
pre-trained models such as BERT are challenged by deploying on
resource-constraint devices. Most existing model compression approaches require
re-compression or fine-tuning across diverse constraints to accommodate various
hardware deployments. This practically limits the further application of model
compression. Moreover, the ineffective training and searching process of
existing elastic compression paradigms[4,27] prevents the direct migration to
BERT compression. Motivated by the necessity of efficient inference across
various constraints on BERT, we propose a novel approach, YOCO-BERT, to achieve
compress once and deploy everywhere. Specifically, we first construct a huge
search space with 10^13 architectures, which covers nearly all configurations
in BERT model. Then, we propose a novel stochastic nature gradient optimization
method to guide the generation of optimal candidate architecture which could
keep a balanced trade-off between explorations and exploitation. When a certain
resource constraint is given, a lightweight distribution optimization approach
is utilized to obtain the optimal network for target deployment without
fine-tuning. Compared with state-of-the-art algorithms, YOCO-BERT provides more
compact models, yet achieving 2.1%-4.5% average accuracy improvement on the
GLUE benchmark. Besides, YOCO-BERT is also more effective, e.g.,the training
complexity is O(1)for N different devices. Code is
availablehttps://github.com/MAC-AutoML/YOCO-BERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaokun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion. (arXiv:2106.02497v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02497</id>
        <link href="http://arxiv.org/abs/2106.02497"/>
        <updated>2021-06-07T03:06:11.767Z</updated>
        <summary type="html"><![CDATA[Despite recent successes of large pre-trained language models in solving
reasoning tasks, their inference capabilities remain opaque. We posit that such
models can be made more interpretable by explicitly generating interim
inference rules, and using them to guide the generation of task-specific
textual outputs. In this paper we present COINS, a recursive inference
framework that i) iteratively reads context sentences, ii) dynamically
generates contextualized inference rules, encodes them, and iii) uses them to
guide task-specific output generation. We apply COINS to a Narrative Story
Completion task that asks a model to complete a story with missing sentences,
to produce a coherent story with plausible logical connections, causal
relationships, and temporal dependencies. By modularizing inference and
sentence generation steps in a recurrent model, we aim to make reasoning steps
and their effects on next sentence generation transparent. Our automatic and
manual evaluations show that the model generates better story sentences than
SOTA baselines, especially in terms of coherence. We further demonstrate
improved performance over strong pre-trained LMs in generating commonsense
inference rules. The recursive nature of COINS holds the potential for
controlled generation of longer sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1"&gt;Debjit Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model. (arXiv:2106.02596v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.02596</id>
        <link href="http://arxiv.org/abs/2106.02596"/>
        <updated>2021-06-07T03:06:11.759Z</updated>
        <summary type="html"><![CDATA[Stereotypical language expresses widely-held beliefs about different social
categories. Many stereotypes are overtly negative, while others may appear
positive on the surface, but still lead to negative consequences. In this work,
we present a computational approach to interpreting stereotypes in text through
the Stereotype Content Model (SCM), a comprehensive causal theory from social
psychology. The SCM proposes that stereotypes can be understood along two
primary dimensions: warmth and competence. We present a method for defining
warmth and competence axes in semantic embedding space, and show that the four
quadrants defined by this subspace accurately represent the warmth and
competence concepts, according to annotated lexicons. We then apply our
computational SCM model to textual stereotype data and show that it compares
favourably with survey-based studies in the psychological literature.
Furthermore, we explore various strategies to counter stereotypical beliefs
with anti-stereotypes. It is known that countering stereotypes with
anti-stereotypical examples is one of the most effective ways to reduce biased
thinking, yet the problem of generating anti-stereotypes has not been
previously studied. Thus, a better understanding of how to generate realistic
and effective anti-stereotypes can contribute to addressing pressing societal
concerns of stereotyping, prejudice, and discrimination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1"&gt;Kathleen C. Fraser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1"&gt;Isar Nejadgholi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1"&gt;Svetlana Kiritchenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene. (arXiv:2106.02327v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02327</id>
        <link href="http://arxiv.org/abs/2106.02327"/>
        <updated>2021-06-07T03:06:11.749Z</updated>
        <summary type="html"><![CDATA[The major paradigm of applying a pre-trained language model to downstream
tasks is to fine-tune it on labeled task data, which often suffers instability
and low performance when the labeled examples are scarce.~One way to alleviate
this problem is to apply post-training on unlabeled task data before
fine-tuning, adapting the pre-trained model to target domains by contrastive
learning that considers either token-level or sequence-level similarity.
Inspired by the success of sequence masking, we argue that both token-level and
sequence-level similarities can be captured with a pair of masked
sequences.~Therefore, we propose complementary random masking (CRM) to generate
a pair of masked sequences from an input sequence for sequence-level
contrastive learning and then develop contrastive masked language modeling
(CMLM) for post-training to integrate both token-level and sequence-level
contrastive learnings.~Empirical results show that CMLM surpasses several
recent post-training methods in few-shot settings without the need for data
augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Ruikun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guanhuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1"&gt;Xiaojun Quan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ERICA: An Empathetic Android Companion for Covid-19 Quarantine. (arXiv:2106.02325v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02325</id>
        <link href="http://arxiv.org/abs/2106.02325"/>
        <updated>2021-06-07T03:06:11.742Z</updated>
        <summary type="html"><![CDATA[Over the past year, research in various domains, including Natural Language
Processing (NLP), has been accelerated to fight against the COVID-19 pandemic,
yet such research has just started on dialogue systems. In this paper, we
introduce an end-to-end dialogue system which aims to ease the isolation of
people under self-quarantine. We conduct a control simulation experiment to
assess the effects of the user interface, a web-based virtual agent called Nora
vs. the android ERICA via a video call. The experimental results show that the
android offers a more valuable user experience by giving the impression of
being more empathetic and engaging in the conversation due to its nonverbal
information, such as facial expressions and body gestures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1"&gt;Etsuko Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1"&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1"&gt;Samuel Cahyawijaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lala_D/0/1/0/all/0/1"&gt;Divesh Lala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1"&gt;Tatsuya Kawahara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial Discriminator for Cross-Lingual NER. (arXiv:2106.02300v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02300</id>
        <link href="http://arxiv.org/abs/2106.02300"/>
        <updated>2021-06-07T03:06:11.684Z</updated>
        <summary type="html"><![CDATA[Neural methods have been shown to achieve high performance in Named Entity
Recognition (NER), but rely on costly high-quality labeled data for training,
which is not always available across languages. While previous works have shown
that unlabeled data in a target language can be used to improve cross-lingual
model performance, we propose a novel adversarial approach (AdvPicker) to
better leverage such data and further improve results. We design an adversarial
learning framework in which an encoder learns entity domain knowledge from
labeled source-language data and better shared features are captured via
adversarial training - where a discriminator selects less language-dependent
target-language data via similarity to the source language. Experimental
results on standard benchmark datasets well demonstrate that the proposed
method benefits strongly from this data selection process and outperforms
existing state-of-the-art methods; without requiring any additional external
resources (e.g., gazetteers or via machine translation).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weile Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huiqiang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qianhui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1"&gt;B&amp;#xf6;rje F. Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yi Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Slice-Aware Representations with Mixture of Attentions. (arXiv:2106.02363v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02363</id>
        <link href="http://arxiv.org/abs/2106.02363"/>
        <updated>2021-06-07T03:06:11.669Z</updated>
        <summary type="html"><![CDATA[Real-world machine learning systems are achieving remarkable performance in
terms of coarse-grained metrics like overall accuracy and F-1 score. However,
model improvement and development often require fine-grained modeling on
individual data subsets or slices, for instance, the data slices where the
models have unsatisfactory results. In practice, it gives tangible values for
developing such models that can pay extra attention to critical or interested
slices while retaining the original overall performance. This work extends the
recent slice-based learning (SBL)~\cite{chen2019slice} with a mixture of
attentions (MoA) to learn slice-aware dual attentive representations. We
empirically show that the MoA approach outperforms the baseline method as well
as the original SBL approach on monitored slices with two natural language
understanding (NLU) tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sunghyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Han Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Bum Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarikaya_R/0/1/0/all/0/1"&gt;Ruhi Sarikaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-language Sentence Selection via Data Augmentation and Rationale Training. (arXiv:2106.02293v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02293</id>
        <link href="http://arxiv.org/abs/2106.02293"/>
        <updated>2021-06-07T03:06:11.661Z</updated>
        <summary type="html"><![CDATA[This paper proposes an approach to cross-language sentence selection in a
low-resource setting. It uses data augmentation and negative sampling
techniques on noisy parallel sentence data to directly learn a cross-lingual
embedding-based query relevance model. Results show that this approach performs
as well as or better than multiple state-of-the-art machine translation +
monolingual retrieval systems trained on the same parallel data. Moreover, when
a rationale training secondary objective is applied to encourage the model to
match word alignment hints from a phrase-based statistical machine translation
model, consistent improvements are seen across three language pairs
(English-Somali, English-Swahili and English-Tagalog) over a variety of
state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanda Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kedzie_C/0/1/0/all/0/1"&gt;Chris Kedzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuscakova_P/0/1/0/all/0/1"&gt;Petra Galu&amp;#x161;&amp;#x10d;&amp;#xe1;kov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1"&gt;Douglas W. Oard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1"&gt;Kathleen McKeown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling the Unigram Distribution. (arXiv:2106.02289v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02289</id>
        <link href="http://arxiv.org/abs/2106.02289"/>
        <updated>2021-06-07T03:06:11.515Z</updated>
        <summary type="html"><![CDATA[The unigram distribution is the non-contextual probability of finding a
specific word form in a corpus. While of central importance to the study of
language, it is commonly approximated by each word's sample frequency in the
corpus. This approach, being highly dependent on sample size, assigns zero
probability to any out-of-vocabulary (oov) word form. As a result, it produces
negatively biased probabilities for any oov word form, while positively biased
probabilities to in-corpus words. In this work, we argue in favor of properly
modeling the unigram distribution -- claiming it should be a central task in
natural language processing. With this in mind, we present a novel model for
estimating it in a language (a neuralization of Goldwater et al.'s (2011)
model) and show it produces much better estimates across a diverse set of 7
languages than the na\"ive use of neural character-level language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikkarinen_I/0/1/0/all/0/1"&gt;Irene Nikkarinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1"&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blasi_D/0/1/0/all/0/1"&gt;Dami&amp;#xe1;n E. Blasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AgreeSum: Agreement-Oriented Multi-Document Summarization. (arXiv:2106.02278v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02278</id>
        <link href="http://arxiv.org/abs/2106.02278"/>
        <updated>2021-06-07T03:06:11.483Z</updated>
        <summary type="html"><![CDATA[We aim to renew interest in a particular multi-document summarization (MDS)
task which we call AgreeSum: agreement-oriented multi-document summarization.
Given a cluster of articles, the goal is to provide abstractive summaries that
represent information common and faithful to all input articles. Given the lack
of existing datasets, we create a dataset for AgreeSum, and provide annotations
on article-summary entailment relations for a subset of the clusters in the
dataset. We aim to create strong baselines for the task by applying the
top-performing pretrained single-document summarization model PEGASUS onto
AgreeSum, leveraging both annotated clusters by supervised losses, and
unannotated clusters by T5-based entailment-related and language-related
losses. Compared to other baselines, both automatic evaluation and human
evaluation show better article-summary and cluster-summary entailment in
generated summaries. On a separate note, we hope that our article-summary
entailment annotations contribute to the community's effort in improving
abstractive summarization faithfulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1"&gt;Richard Yuanzhe Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lelkes_A/0/1/0/all/0/1"&gt;Adam D. Lelkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vinh Q. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ERNIE-Tiny : A Progressive Distillation Framework for Pretrained Transformer Compression. (arXiv:2106.02241v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02241</id>
        <link href="http://arxiv.org/abs/2106.02241"/>
        <updated>2021-06-07T03:06:11.369Z</updated>
        <summary type="html"><![CDATA[Pretrained language models (PLMs) such as BERT adopt a training paradigm
which first pretrain the model in general data and then finetune the model on
task-specific data, and have recently achieved great success. However, PLMs are
notorious for their enormous parameters and hard to be deployed on real-life
applications. Knowledge distillation has been prevailing to address this
problem by transferring knowledge from a large teacher to a much smaller
student over a set of data. We argue that the selection of thee three key
components, namely teacher, training data, and learning objective, is crucial
to the effectiveness of distillation. We, therefore, propose a four-stage
progressive distillation framework ERNIE-Tiny to compress PLM, which varies the
three components gradually from general level to task-specific level.
Specifically, the first stage, General Distillation, performs distillation with
guidance from pretrained teacher, gerenal data and latent distillation loss.
Then, General-Enhanced Distillation changes teacher model from pretrained
teacher to finetuned teacher. After that, Task-Adaptive Distillation shifts
training data from general data to task-specific data. In the end,
Task-Specific Distillation, adds two additional losses, namely Soft-Label and
Hard-Label loss onto the last stage. Empirical results demonstrate the
effectiveness of our framework and generalization gain brought by ERNIE-Tiny.In
particular, experiments show that a 4-layer ERNIE-Tiny maintains over
98.0%performance of its 12-layer teacher BERT base on GLUE benchmark,
surpassing state-of-the-art (SOTA) by 1.0% GLUE score with the same amount of
parameters. Moreover, ERNIE-Tiny achieves a new compression SOTA on five
Chinese NLP tasks, outperforming BERT base by 0.4% accuracy with 7.5x fewer
parameters and9.4x faster inference speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weiyue Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shikun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaxiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1"&gt;Hao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition. (arXiv:2106.02302v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02302</id>
        <link href="http://arxiv.org/abs/2106.02302"/>
        <updated>2021-06-07T03:06:11.362Z</updated>
        <summary type="html"><![CDATA[Integrating external language models (LMs) into end-to-end (E2E) models
remains a challenging task for domain-adaptive speech recognition. Recently,
internal language model estimation (ILME)-based LM fusion has shown significant
word error rate (WER) reduction from Shallow Fusion by subtracting a weighted
internal LM score from an interpolation of E2E model and external LM scores
during beam search. However, on different test sets, the optimal LM
interpolation weights vary over a wide range and have to be tuned extensively
on well-matched validation sets. In this work, we perform LM fusion in the
minimum WER (MWER) training of an E2E model to obviate the need for LM weights
tuning during inference. Besides MWER training with Shallow Fusion (MWER-SF),
we propose a novel MWER training with ILME (MWER-ILME) where the ILME-based
fusion is conducted to generate N-best hypotheses and their posteriors.
Additional gradient is induced when internal LM is engaged in MWER-ILME loss
computation. During inference, LM weights pre-determined in MWER training
enable robust LM integrations on test sets from different domains. Experimented
with 30K-hour trained transformer transducers, MWER-ILME achieves on average
8.8% and 5.8% relative WER reductions from MWER and MWER-SF training,
respectively, on 6 different test sets]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1"&gt;Naoyuki Kanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_G/0/1/0/all/0/1"&gt;Guoli Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_E/0/1/0/all/0/1"&gt;Eric Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yifan Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Adapt Your Pretrained Multilingual Model to 1600 Languages. (arXiv:2106.02124v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02124</id>
        <link href="http://arxiv.org/abs/2106.02124"/>
        <updated>2021-06-07T03:06:11.353Z</updated>
        <summary type="html"><![CDATA[Pretrained multilingual models (PMMs) enable zero-shot learning via
cross-lingual transfer, performing best for languages seen during pretraining.
While methods exist to improve performance for unseen languages, they have
almost exclusively been evaluated using amounts of raw text only available for
a small fraction of the world's languages. In this paper, we evaluate the
performance of existing methods to adapt PMMs to new languages using a resource
available for over 1600 languages: the New Testament. This is challenging for
two reasons: (1) the small corpus size, and (2) the narrow domain. While
performance drops for all approaches, we surprisingly still see gains of up to
$17.69\%$ accuracy for part-of-speech tagging and $6.29$ F1 for NER on average
over all languages as compared to XLM-R. Another unexpected finding is that
continued pretraining, the simplest approach, performs best. Finally, we
perform a case study to disentangle the effects of domain and size and to shed
light on the influence of the finetuning source language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_A/0/1/0/all/0/1"&gt;Abteen Ebrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1"&gt;Katharina Kann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NAST: A Non-Autoregressive Generator with Word Alignment for Unsupervised Text Style Transfer. (arXiv:2106.02210v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02210</id>
        <link href="http://arxiv.org/abs/2106.02210"/>
        <updated>2021-06-07T03:06:11.343Z</updated>
        <summary type="html"><![CDATA[Autoregressive models have been widely used in unsupervised text style
transfer. Despite their success, these models still suffer from the content
preservation problem that they usually ignore part of the source sentence and
generate some irrelevant words with strong styles. In this paper, we propose a
Non-Autoregressive generator for unsupervised text Style Transfer (NAST), which
alleviates the problem from two aspects. First, we observe that most words in
the transferred sentence can be aligned with related words in the source
sentence, so we explicitly model word alignments to suppress irrelevant words.
Second, existing models trained with the cycle loss align sentences in two
stylistic text spaces, which lacks fine-grained control at the word level. The
proposed non-autoregressive generator focuses on the connections between
aligned words, which learns the word-level transfer between styles. For
experiments, we integrate the proposed generator into two base models and
evaluate them on two style transfer tasks. The results show that NAST can
significantly improve the overall performance and provide explainable word
alignments. Moreover, the non-autoregressive generator achieves over 10x
speedups at inference. Our codes are available at
https://github.com/thu-coai/NAST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zikai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Henry Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qihan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dutch Named Entity Recognition and De-identification Methods for the Human Resource Domain. (arXiv:2106.02287v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02287</id>
        <link href="http://arxiv.org/abs/2106.02287"/>
        <updated>2021-06-07T03:06:11.323Z</updated>
        <summary type="html"><![CDATA[The human resource (HR) domain contains various types of privacy-sensitive
textual data, such as e-mail correspondence and performance appraisal. Doing
research on these documents brings several challenges, one of them
anonymisation. In this paper, we evaluate the current Dutch text
de-identification methods for the HR domain in four steps. First, by updating
one of these methods with the latest named entity recognition (NER) models. The
result is that the NER model based on the CoNLL 2002 corpus in combination with
the BERTje transformer give the best combination for suppressing persons
(recall 0.94) and locations (recall 0.82). For suppressing gender, DEDUCE is
performing best (recall 0.53). Second NER evaluation is based on both strict
de-identification of entities (a person must be suppressed as a person) and
third evaluation on a loose sense of de-identification (no matter what how a
person is suppressed, as long it is suppressed). In the fourth and last step a
new kind of NER dataset is tested for recognising job titles in texts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toledo_C/0/1/0/all/0/1"&gt;Cha&amp;#xef;m van Toledo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_F/0/1/0/all/0/1"&gt;Friso van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spruit_M/0/1/0/all/0/1"&gt;Marco Spruit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-Adversarial Visual Question Answering. (arXiv:2106.02280v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02280</id>
        <link href="http://arxiv.org/abs/2106.02280"/>
        <updated>2021-06-07T03:06:11.315Z</updated>
        <summary type="html"><![CDATA[Performance on the most commonly used Visual Question Answering dataset (VQA
v2) is starting to approach human accuracy. However, in interacting with
state-of-the-art VQA models, it is clear that the problem is far from being
solved. In order to stress test VQA models, we benchmark them against
human-adversarial examples. Human subjects interact with a state-of-the-art VQA
model, and for each image in the dataset, attempt to find a question where the
model's predicted answer is incorrect. We find that a wide range of
state-of-the-art models perform poorly when evaluated on these examples. We
conduct an extensive analysis of the collected adversarial examples and provide
guidance on future research directions. We hope that this Adversarial VQA
(AdVQA) benchmark can help drive progress in the field and advance the state of
the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_S/0/1/0/all/0/1"&gt;Sasha Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amanpreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1"&gt;Vedanuj Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magana_J/0/1/0/all/0/1"&gt;Jose Alberto Lopez Magana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1"&gt;Wojciech Galuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances. (arXiv:2106.02227v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02227</id>
        <link href="http://arxiv.org/abs/2106.02227"/>
        <updated>2021-06-07T03:06:11.305Z</updated>
        <summary type="html"><![CDATA[Nowadays, open-domain dialogue models can generate acceptable responses
according to the historical context based on the large-scale pre-trained
language models. However, they generally concatenate the dialogue history
directly as the model input to predict the response, which we named as the flat
pattern and ignores the dynamic information flow across dialogue utterances. In
this work, we propose the DialoFlow model, in which we introduce a dynamic flow
mechanism to model the context flow, and design three training objectives to
capture the information dynamics across dialogue utterances by addressing the
semantic influence brought about by each utterance in large-scale pre-training.
Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset
demonstrate that our DialoFlow significantly outperforms the DialoGPT on the
dialogue generation task. Besides, we propose the Flow score, an effective
automatic metric for evaluating interactive human-bot conversation quality
based on the pre-trained DialoFlow, which presents high chatbot-level
correlation ($r=0.9$) with human ratings among 11 chatbots. Code and
pre-trained models will be public.
\footnote{\url{https://github.com/ictnlp/DialoFlow}}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1"&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupled Dialogue Modeling and Semantic Parsing for Multi-Turn Text-to-SQL. (arXiv:2106.02282v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02282</id>
        <link href="http://arxiv.org/abs/2106.02282"/>
        <updated>2021-06-07T03:06:11.294Z</updated>
        <summary type="html"><![CDATA[Recently, Text-to-SQL for multi-turn dialogue has attracted great interest.
Here, the user input of the current turn is parsed into the corresponding SQL
query of the appropriate database, given all previous dialogue history. Current
approaches mostly employ end-to-end models and consequently face two
challenges. First, dialogue history modeling and Text-to-SQL parsing are
implicitly combined, hence it is hard to carry out interpretable analysis and
obtain targeted improvement. Second, SQL annotation of multi-turn dialogue is
very expensive, leading to training data sparsity. In this paper, we propose a
novel decoupled multi-turn Text-to-SQL framework, where an utterance rewrite
model first explicitly solves completion of dialogue context, and then a
single-turn Text-to-SQL parser follows. A dual learning approach is also
proposed for the utterance rewrite model to address the data sparsity problem.
Compared with end-to-end approaches, the proposed decoupled method can achieve
excellent performance without any annotated in-domain data. With just a few
annotated rewrite cases, the decoupled method outperforms the released
state-of-the-art end-to-end models on both SParC and CoSQL datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Chen Hanqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1"&gt;Ruisheng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1"&gt;Da Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mengyue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1"&gt;Kai Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syntax-augmented Multilingual BERT for Cross-lingual Transfer. (arXiv:2106.02134v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02134</id>
        <link href="http://arxiv.org/abs/2106.02134"/>
        <updated>2021-06-07T03:06:11.281Z</updated>
        <summary type="html"><![CDATA[In recent years, we have seen a colossal effort in pre-training multilingual
text encoders using large-scale corpora in many languages to facilitate
cross-lingual transfer learning. However, due to typological differences across
languages, the cross-lingual transfer is challenging. Nevertheless, language
syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous
works have shown that pre-trained multilingual encoders, such as mBERT
\cite{devlin-etal-2019-bert}, capture language syntax, helping cross-lingual
transfer. This work shows that explicitly providing language syntax and
training mBERT using an auxiliary objective to encode the universal dependency
tree structure helps cross-lingual transfer. We perform rigorous experiments on
four NLP tasks, including text classification, question answering, named entity
recognition, and task-oriented semantic parsing. The experiment results show
that syntax-augmented mBERT improves cross-lingual transfer on popular
benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across
all languages. In the \emph{generalized} transfer setting, the performance
boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1"&gt;Wasi Uddin Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1"&gt;Yashar Mehdad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Transformers for Neural Machine Translation. (arXiv:2106.02242v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02242</id>
        <link href="http://arxiv.org/abs/2106.02242"/>
        <updated>2021-06-07T03:06:11.262Z</updated>
        <summary type="html"><![CDATA[Transformer has been widely adopted in Neural Machine Translation (NMT)
because of its large capacity and parallel training of sequence generation.
However, the deployment of Transformer is challenging because different
scenarios require models of different complexities and scales. Naively training
multiple Transformers is redundant in terms of both computation and memory. In
this paper, we propose a novel scalable Transformers, which naturally contains
sub-Transformers of different scales and have shared parameters. Each
sub-Transformer can be easily obtained by cropping the parameters of the
largest Transformer. A three-stage training scheme is proposed to tackle the
difficulty of training the scalable Transformers, which introduces additional
supervisions from word-level and sequence-level self-distillation. Extensive
experiments were conducted on WMT EN-De and En-Fr to validate our proposed
scalable Transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1"&gt;Shijie Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation. (arXiv:2106.02170v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02170</id>
        <link href="http://arxiv.org/abs/2106.02170"/>
        <updated>2021-06-07T03:06:11.254Z</updated>
        <summary type="html"><![CDATA[Automatic detection of phoneme or word-like units is one of the core
objectives in zero-resource speech processing. Recent attempts employ
self-supervised training methods, such as contrastive predictive coding (CPC),
where the next frame is predicted given past context. However, CPC only looks
at the audio signal's frame-level structure. We overcome this limitation with a
segmental contrastive predictive coding (SCPC) framework that can model the
signal structure at a higher level e.g. at the phoneme level. In this
framework, a convolutional neural network learns frame-level representation
from the raw waveform via noise-contrastive estimation (NCE). A differentiable
boundary detector finds variable-length segments, which are then used to
optimize a segment encoder via NCE to learn segment representations. The
differentiable boundary detector allows us to train frame-level and
segment-level encoders jointly. Typically, phoneme and word segmentation are
treated as separate tasks. We unify them and experimentally show that our
single model outperforms existing phoneme and word segmentation methods on
TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and
when is the right time to include the segmental loss in the learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhati_S/0/1/0/all/0/1"&gt;Saurabhchand Bhati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Villalba_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Villalba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr &amp;#x17b;elasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Velazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dehak_N/0/1/0/all/0/1"&gt;Najim Dehak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERTTune: Fine-Tuning Neural Machine Translation with BERTScore. (arXiv:2106.02208v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02208</id>
        <link href="http://arxiv.org/abs/2106.02208"/>
        <updated>2021-06-07T03:06:11.245Z</updated>
        <summary type="html"><![CDATA[Neural machine translation models are often biased toward the limited
translation references seen during training. To amend this form of overfitting,
in this paper we propose fine-tuning the models with a novel training objective
based on the recently-proposed BERTScore evaluation metric. BERTScore is a
scoring function based on contextual embeddings that overcomes the typical
limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing
translations that are different from the references, yet close in the
contextual embedding space, to be treated as substantially correct. To be able
to use BERTScore as a training objective, we propose three approaches for
generating soft predictions, allowing the network to remain completely
differentiable end-to-end. Experiments carried out over four, diverse language
pairs have achieved improvements of up to 0.58 pp (3.28%) in BLEU score and up
to 0.76 pp (0.98%) in BERTScore (F_BERT) when fine-tuning a strong baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Unanue_I/0/1/0/all/0/1"&gt;Inigo Jauregi Unanue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parnell_J/0/1/0/all/0/1"&gt;Jacob Parnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piccardi_M/0/1/0/all/0/1"&gt;Massimo Piccardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Embeddings for Typology and Cross-lingual Transfer Learning. (arXiv:2106.02082v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02082</id>
        <link href="http://arxiv.org/abs/2106.02082"/>
        <updated>2021-06-07T03:06:11.236Z</updated>
        <summary type="html"><![CDATA[Cross-lingual language tasks typically require a substantial amount of
annotated data or parallel translation data. We explore whether language
representations that capture relationships among languages can be learned and
subsequently leveraged in cross-lingual tasks without the use of parallel data.
We generate dense embeddings for 29 languages using a denoising autoencoder,
and evaluate the embeddings using the World Atlas of Language Structures (WALS)
and two extrinsic tasks in a zero-shot setting: cross-lingual dependency
parsing and cross-lingual natural language inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Dian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Taiqi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagae_K/0/1/0/all/0/1"&gt;Kenji Sagae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding 'Grounding' in NLP. (arXiv:2106.02192v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02192</id>
        <link href="http://arxiv.org/abs/2106.02192"/>
        <updated>2021-06-07T03:06:11.226Z</updated>
        <summary type="html"><![CDATA[The NLP community has seen substantial recent interest in grounding to
facilitate interaction between language technologies and the world. However, as
a community, we use the term broadly to reference any linking of text to data
or non-textual modality. In contrast, Cognitive Science more formally defines
"grounding" as the process of establishing what mutual information is required
for successful communication between two interlocutors -- a definition which
might implicitly capture the NLP usage but differs in intent and scope. We
investigate the gap between these definitions and seek answers to the following
questions: (1) What aspects of grounding are missing from NLP tasks? Here we
present the dimensions of coordination, purviews and constraints. (2) How is
the term "grounding" used in the current research? We study the trends in
datasets, domains, and tasks introduced in recent NLP conferences. And finally,
(3) How to advance our current definition to bridge the gap with Cognitive
Science? We present ways to both create new tasks or repurpose existing ones to
make advancements towards achieving a more complete sense of grounding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1"&gt;Khyathi Raghavi Chandu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1"&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1"&gt;Alan W Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Inquiries about History: An Efficient and Practical Framework for Evaluating Open-domain Chatbot Consistency. (arXiv:2106.02228v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02228</id>
        <link href="http://arxiv.org/abs/2106.02228"/>
        <updated>2021-06-07T03:06:11.204Z</updated>
        <summary type="html"><![CDATA[A good open-domain chatbot should avoid presenting contradictory responses
about facts or opinions in a conversational session, known as its consistency
capacity. However, evaluating the consistency capacity of a chatbot is still
challenging. Employing human judges to interact with chatbots on purpose to
check their capacities is costly and low-efficient, and difficult to get rid of
subjective bias. In this paper, we propose the Addressing Inquiries about
History (AIH), an efficient and practical framework for the consistency
evaluation. At the conversation stage, AIH attempts to address appropriate
inquiries about the dialogue history to induce the chatbot to redeclare the
historical facts or opinions. We carry out the conversation between chatbots,
which is more efficient than the human-bot interaction and can also alleviate
the subjective bias. In this way, we manage to rapidly obtain a dialog session
that contains responses with high contradiction possibilities. At the
contradiction recognition stage, we can either employ human judges or a natural
language inference (NLI) model to recognize whether the answers to the
inquiries are contradictory with history. Finally, we are able to rank chatbots
according to the contradiction statistics. Experiments on open-domain chatbots
show that our approach can efficiently and reliably assess the consistency
capacity of chatbots and achieve a high ranking correlation with the human
evaluation. We release the framework and hope to help improve the consistency
capacity of chatbots. \footnote{\url{https://github.com/ictnlp/AIH}}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1"&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-07T03:06:11.194Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[nmT5 -- Is parallel data still relevant for pre-training massively multilingual language models?. (arXiv:2106.02171v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02171</id>
        <link href="http://arxiv.org/abs/2106.02171"/>
        <updated>2021-06-07T03:06:11.185Z</updated>
        <summary type="html"><![CDATA[Recently, mT5 - a massively multilingual version of T5 - leveraged a unified
text-to-text format to attain state-of-the-art results on a wide variety of
multilingual NLP tasks. In this paper, we investigate the impact of
incorporating parallel data into mT5 pre-training. We find that multi-tasking
language modeling with objectives such as machine translation during
pre-training is a straightforward way to improve performance on downstream
multilingual and cross-lingual tasks. However, the gains start to diminish as
the model capacity increases, suggesting that parallel data might not be as
essential for larger models. At the same time, even at larger model sizes, we
find that pre-training with parallel data still provides benefits in the
limited labelled data regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1"&gt;Aditya Siddhant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1"&gt;Noah Constant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"&gt;Melvin Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1"&gt;Rami Al-Rfou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1"&gt;Linting Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems. (arXiv:2005.12964v9 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12964</id>
        <link href="http://arxiv.org/abs/2005.12964"/>
        <updated>2021-06-07T03:06:11.174Z</updated>
        <summary type="html"><![CDATA[Deep candidate generation (DCG) that narrows down the collection of relevant
items from billions to hundreds via representation learning has become
prevalent in industrial recommender systems. Standard approaches approximate
maximum likelihood estimation (MLE) through sampling for better scalability and
address the problem of DCG in a way similar to language modeling. However, live
recommender systems face severe exposure bias and have a vocabulary several
orders of magnitude larger than that of natural language, implying that MLE
will preserve and even exacerbate the exposure bias in the long run in order to
faithfully fit the observed samples. In this paper, we theoretically prove that
a popular choice of contrastive loss is equivalent to reducing the exposure
bias via inverse propensity weighting, which provides a new perspective for
understanding the effectiveness of contrastive learning. Based on the
theoretical discovery, we design CLRec, a contrastive learning method to
improve DCG in terms of fairness, effectiveness and efficiency in recommender
systems with extremely large candidate size. We further improve upon CLRec and
propose Multi-CLRec, for accurate multi-intention aware bias reduction. Our
methods have been successfully deployed in Taobao, where at least four-month
online A/B tests and offline analyses demonstrate its substantial improvements,
including a dramatic reduction in the Matthew effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianxin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LGBTQ-AI? Exploring Expressions of Gender and Sexual Orientation in Chatbots. (arXiv:2106.02076v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.02076</id>
        <link href="http://arxiv.org/abs/2106.02076"/>
        <updated>2021-06-07T03:06:11.122Z</updated>
        <summary type="html"><![CDATA[Chatbots are popular machine partners for task-oriented and social
interactions. Human-human computer-mediated communication research has explored
how people express their gender and sexuality in online social interactions,
but little is known about whether and in what way chatbots do the same. We
conducted semi-structured interviews with 5 text-based conversational agents to
explore this topic Through these interviews, we identified 6 common themes
around the expression of gender and sexual identity: identity description,
identity formation, peer acceptance, positive reflection, uncomfortable
feelings and off-topic responses. Chatbots express gender and sexuality
explicitly and through relation of experience and emotions, mimicking the human
language on which they are trained. It is nevertheless evident that chatbots
differ from human dialogue partners as they lack the flexibility and
understanding enabled by lived human experience. While chatbots are proficient
in using language to express identity, they also display a lack of authentic
experiences of gender and sexuality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_J/0/1/0/all/0/1"&gt;Justin Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_L/0/1/0/all/0/1"&gt;Leigh Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perrone_A/0/1/0/all/0/1"&gt;Allison Perrone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A diachronic evaluation of gender asymmetry in euphemism. (arXiv:2106.02083v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02083</id>
        <link href="http://arxiv.org/abs/2106.02083"/>
        <updated>2021-06-07T03:06:11.095Z</updated>
        <summary type="html"><![CDATA[The use of euphemisms is a known driver of language change. It has been
proposed that women use euphemisms more than men. Although there have been
several studies investigating gender differences in language, the claim about
euphemism usage has not been tested comprehensively through time. If women do
use euphemisms more, this could mean that women also lead the formation of new
euphemisms and language change over time. Using four large diachronic text
corpora of English, we evaluate the claim that women use euphemisms more than
men through a quantitative analysis. We assembled a list of 106 euphemism-taboo
pairs to analyze their relative use through time by each gender in the corpora.
Contrary to the existing belief, our results show that women do not use
euphemisms with a higher proportion than men. We repeated the analysis using
different subsets of the euphemism-taboo pairs list and found that our result
was robust. Our study indicates that in a broad range of settings involving
both speech and writing, and with varying degrees of formality, women do not
use or form euphemisms more than men.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapron_King_A/0/1/0/all/0/1"&gt;Anna Kapron-King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Equal Gender Representation in the Annotations of Toxic Language Detection. (arXiv:2106.02183v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02183</id>
        <link href="http://arxiv.org/abs/2106.02183"/>
        <updated>2021-06-07T03:06:11.070Z</updated>
        <summary type="html"><![CDATA[Classifiers tend to propagate biases present in the data on which they are
trained. Hence, it is important to understand how the demographic identities of
the annotators of comments affect the fairness of the resulting model. In this
paper, we focus on the differences in the ways men and women annotate comments
for toxicity, investigating how these differences result in models that amplify
the opinions of male annotators. We find that the BERT model as-sociates toxic
comments containing offensive words with male annotators, causing the model to
predict 67.7% of toxic comments as having been annotated by men. We show that
this disparity between gender predictions can be mitigated by removing
offensive words and highly toxic comments from the training data. We then apply
the learned associations between gender and language to toxic language
classifiers, finding that models trained exclusively on female-annotated data
perform 1.8% better than those trained solely on male-annotated data and that
training models on data after removing all offensive words reduces bias in the
model by 55.5% while increasing the sensitivity by 0.4%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Excell_E/0/1/0/all/0/1"&gt;Elizabeth Excell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1"&gt;Noura Al Moubayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval. (arXiv:2106.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02400</id>
        <link href="http://arxiv.org/abs/2106.02400"/>
        <updated>2021-06-07T03:06:10.740Z</updated>
        <summary type="html"><![CDATA[Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph's nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Manh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurrin_C/0/1/0/all/0/1"&gt;Cathal Gurrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Insights into Metric Optimization for Ranking-based Recommendation. (arXiv:2106.02545v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02545</id>
        <link href="http://arxiv.org/abs/2106.02545"/>
        <updated>2021-06-07T03:06:10.712Z</updated>
        <summary type="html"><![CDATA[Direct optimization of IR metrics has often been adopted as an approach to
devise and develop ranking-based recommender systems. Most methods following
this approach aim at optimizing the same metric being used for evaluation,
under the assumption that this will lead to the best performance. A number of
studies of this practice bring this assumption, however, into question. In this
paper, we dig deeper into this issue in order to learn more about the effects
of the choice of the metric to optimize on the performance of a ranking-based
recommender system. We present an extensive experimental study conducted on
different datasets in both pairwise and listwise learning-to-rank scenarios, to
compare the relative merit of four popular IR metrics, namely RR, AP, nDCG and
RBP, when used for optimization and assessment of recommender systems in
various combinations. For the first three, we follow the practice of loss
function formulation available in literature. For the fourth one, we propose
novel loss functions inspired by RBP for both the pairwise and listwise
scenario. Our results confirm that the best performance is indeed not
necessarily achieved when optimizing the same metric being used for evaluation.
In fact, we find that RBP-inspired losses perform at least as well as other
metrics in a consistent way, and offer clear benefits in several cases.
Interesting to see is that RBP-inspired losses, while improving the
recommendation performance for all uses, may lead to an individual performance
gain that is correlated with the activity level of a user in interacting with
items. The more active the users, the more they benefit. Overall, our results
challenge the assumption behind the current research practice of optimizing and
evaluating the same metric, and point to RBP-based optimization instead as a
promising alternative when learning to rank in the recommendation context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Roger Zhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urbano_J/0/1/0/all/0/1"&gt;Juli&amp;#xe1;n Urbano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanjalic_A/0/1/0/all/0/1"&gt;Alan Hanjalic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Fine-Grained Review-based Transformer Model for Personalized Product Search. (arXiv:2004.09424v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09424</id>
        <link href="http://arxiv.org/abs/2004.09424"/>
        <updated>2021-06-07T03:06:10.697Z</updated>
        <summary type="html"><![CDATA[Product search has been a crucial entry point to serve people shopping
online. Most existing personalized product models follow the paradigm of
representing and matching user intents and items in the semantic space, where
finer-grained matching is totally discarded and the ranking of an item cannot
be explained further than just user/item level similarity. In addition, while
some models in existing studies have created dynamic user representations based
on search context, their representations for items are static across all search
sessions. This makes every piece of information about the item always equally
important in representing the item during matching with various user intents.
Aware of the above limitations, we propose a review-based transformer model
(RTM) for personalized product search, which encodes the sequence of query,
user reviews, and item reviews with a transformer architecture. RTM conducts
review-level matching between the user and item, where each review has a
dynamic effect according to the context in the sequence. This makes it possible
to identify useful reviews to explain the scoring. Experimental results show
that RTM significantly outperforms state-of-the-art personalized product search
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1"&gt;Keping Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingyao Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croft_W/0/1/0/all/0/1"&gt;W. Bruce Croft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02634</id>
        <link href="http://arxiv.org/abs/2106.02634"/>
        <updated>2021-06-07T03:06:10.657Z</updated>
        <summary type="html"><![CDATA[Inferring representations of 3D scenes from 2D observations is a fundamental
problem of computer graphics, computer vision, and artificial intelligence.
Emerging 3D-structured neural scene representations are a promising approach to
3D scene understanding. In this work, we propose a novel neural scene
representation, Light Field Networks or LFNs, which represent both geometry and
appearance of the underlying 3D scene in a 360-degree, four-dimensional light
field parameterized via a neural implicit representation. Rendering a ray from
an LFN requires only a *single* network evaluation, as opposed to hundreds of
evaluations per ray for ray-marching or volumetric based renderers in
3D-structured neural scene representations. In the setting of simple scenes, we
leverage meta-learning to learn a prior over LFNs that enables multi-view
consistent light field reconstruction from as little as a single image
observation. This results in dramatic reductions in time and memory complexity,
and enables real-time rendering. The cost of storing a 360-degree light field
via an LFN is two orders of magnitude lower than conventional methods such as
the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further
demonstrate the extraction of sparse depth maps from LFNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1"&gt;Semon Rezchikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fredo Durand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Elastic Embeddings for Customizing On-Device Recommenders. (arXiv:2106.02223v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02223</id>
        <link href="http://arxiv.org/abs/2106.02223"/>
        <updated>2021-06-07T03:06:10.644Z</updated>
        <summary type="html"><![CDATA[In today's context, deploying data-driven services like recommendation on
edge devices instead of cloud servers becomes increasingly attractive due to
privacy and network latency concerns. A common practice in building compact
on-device recommender systems is to compress their embeddings which are
normally the cause of excessive parameterization. However, despite the vast
variety of devices and their associated memory constraints, existing
memory-efficient recommender systems are only specialized for a fixed memory
budget in every design and training life cycle, where a new model has to be
retrained to obtain the optimal performance while adapting to a smaller/larger
memory budget. In this paper, we present a novel lightweight recommendation
paradigm that allows a well-trained recommender to be customized for arbitrary
device-specific memory constraints without retraining. The core idea is to
compose elastic embeddings for each item, where an elastic embedding is the
concatenation of a set of embedding blocks that are carefully chosen by an
automated search function. Correspondingly, we propose an innovative approach,
namely recommendation with universally learned elastic embeddings (RULE). To
ensure the expressiveness of all candidate embedding blocks, RULE enforces a
diversity-driven regularization when learning different embedding blocks. Then,
a performance estimator-based evolutionary search function is designed,
allowing for efficient specialization of elastic embeddings under any memory
constraint for on-device recommendation. Extensive experiments on real-world
datasets reveal the superior performance of RULE under tight memory budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yujia Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Social Media Background to Improve Cold-start Recommendation Deep Models. (arXiv:2106.02256v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02256</id>
        <link href="http://arxiv.org/abs/2106.02256"/>
        <updated>2021-06-07T03:06:10.401Z</updated>
        <summary type="html"><![CDATA[In recommender systems, a cold-start problem occurs when there is no past
interaction record associated with the user or item. Typical solutions to the
cold-start problem make use of contextual information, such as user demographic
attributes or product descriptions. A group of works have shown that social
media background can help predicting temporal phenomenons such as product sales
and stock price movements. In this work, our goal is to investigate whether
social media background can be used as extra contextual information to improve
recommendation models. Based on an existing deep neural network model, we
proposed a method to represent temporal social media background as embeddings
and fuse them as an extra component in the model. We conduct experimental
evaluations on a real-world e-commerce dataset and a Twitter dataset. The
results show that our method of fusing social media background with the
existing model does generally improve recommendation performance. In some cases
the recommendation accuracy measured by hit-rate@K doubles after fusing with
social media background. Our findings can be beneficial for future recommender
system designs that consider complex temporal information representing social
interests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yihong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maekawa_T/0/1/0/all/0/1"&gt;Takuya Maekawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hara_T/0/1/0/all/0/1"&gt;Takahiro Hara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facade-X: an opinionated approach to SPARQL anything. (arXiv:2106.02361v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.02361</id>
        <link href="http://arxiv.org/abs/2106.02361"/>
        <updated>2021-06-07T03:06:10.383Z</updated>
        <summary type="html"><![CDATA[The Semantic Web research community understood since its beginning how
crucial it is to equip practitioners with methods to transform non-RDF
resources into RDF. Proposals focus on either engineering content
transformations or accessing non-RDF resources with SPARQL. Existing solutions
require users to learn specific mapping languages (e.g. RML), to know how to
query and manipulate a variety of source formats (e.g. XPATH, JSON-Path), or to
combine multiple languages (e.g. SPARQL Generate). In this paper, we explore an
alternative solution and contribute a general-purpose meta-model for converting
non-RDF resources into RDF: Facade-X. Our approach can be implemented by
overriding the SERVICE operator and does not require to extend the SPARQL
syntax. We compare our approach with the state of art methods RML and SPARQL
Generate and show how our solution has lower learning demands and cognitive
complexity, and it is cheaper to implement and maintain, while having
comparable extensibility and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daga_E/0/1/0/all/0/1"&gt;Enrico Daga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asprino_L/0/1/0/all/0/1"&gt;Luigi Asprino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulholland_P/0/1/0/all/0/1"&gt;Paul Mulholland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangemi_A/0/1/0/all/0/1"&gt;Aldo Gangemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A General Method for Event Detection on Social Media. (arXiv:2106.02250v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02250</id>
        <link href="http://arxiv.org/abs/2106.02250"/>
        <updated>2021-06-07T03:06:10.358Z</updated>
        <summary type="html"><![CDATA[Event detection on social media has attracted a number of researches, given
the recent availability of large volumes of social media discussions. Previous
works on social media event detection either assume a specific type of event,
or assume certain behavior of observed variables. In this paper, we propose a
general method for event detection on social media that makes few assumptions.
The main assumption we make is that when an event occurs, affected semantic
aspects will behave differently from its usual behavior. We generalize the
representation of time units based on word embeddings of social media text, and
propose an algorithm to detect events in time series in a general sense. In the
experimental evaluation, we use a novel setting to test if our method and
baseline methods can exhaustively catch all real-world news in the test period.
The evaluation results show that when the event is quite unusual with regard to
the base social media discussion, it can be captured more effectively with our
method. Our method can be easily implemented and can be treated as a starting
point for more specific applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yihong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirakawa_M/0/1/0/all/0/1"&gt;Masumi Shirakawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hara_T/0/1/0/all/0/1"&gt;Takahiro Hara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-language Sentence Selection via Data Augmentation and Rationale Training. (arXiv:2106.02293v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02293</id>
        <link href="http://arxiv.org/abs/2106.02293"/>
        <updated>2021-06-07T03:06:10.316Z</updated>
        <summary type="html"><![CDATA[This paper proposes an approach to cross-language sentence selection in a
low-resource setting. It uses data augmentation and negative sampling
techniques on noisy parallel sentence data to directly learn a cross-lingual
embedding-based query relevance model. Results show that this approach performs
as well as or better than multiple state-of-the-art machine translation +
monolingual retrieval systems trained on the same parallel data. Moreover, when
a rationale training secondary objective is applied to encourage the model to
match word alignment hints from a phrase-based statistical machine translation
model, consistent improvements are seen across three language pairs
(English-Somali, English-Swahili and English-Tagalog) over a variety of
state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanda Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kedzie_C/0/1/0/all/0/1"&gt;Chris Kedzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuscakova_P/0/1/0/all/0/1"&gt;Petra Galu&amp;#x161;&amp;#x10d;&amp;#xe1;kov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1"&gt;Douglas W. Oard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1"&gt;Kathleen McKeown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intelligent Resource Reservation for Crowdsourced Live Video Streaming Applications in Geo-Distributed Cloud Environment. (arXiv:2106.02420v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02420</id>
        <link href="http://arxiv.org/abs/2106.02420"/>
        <updated>2021-06-07T03:06:10.260Z</updated>
        <summary type="html"><![CDATA[Crowdsourced live video streaming (livecast) services such as Facebook Live,
YouNow, Douyu and Twitch are gaining more momentum recently. Allocating the
limited resources in a cost-effective manner while maximizing the Quality of
Service (QoS) through real-time delivery and the provision of the appropriate
representations for all viewers is a challenging problem. In our paper, we
introduce a machine-learning based predictive resource allocation framework for
geo-distributed cloud sites, considering the delay and quality constraints to
guarantee the maximum QoS for viewers and the minimum cost for content
providers. First, we present an offline optimization that decides the required
transcoding resources in distributed regions near the viewers with a trade-off
between the QoS and the overall cost. Second, we use machine learning to build
forecasting models that proactively predict the approximate transcoding
resources to be reserved at each cloud site ahead of time. Finally, we develop
a Greedy Nearest and Cheapest algorithm (GNCA) to perform the resource
allocation of real-time broadcasted videos on the rented resources. Extensive
simulations have shown that GNCA outperforms the state-of-the art resource
allocation approaches for crowdsourced live streaming by achieving more than
20% gain in terms of system cost while serving the viewers with relatively
lower latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1"&gt;Emna Baccour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haouari_F/0/1/0/all/0/1"&gt;Fatima Haouari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilal_K/0/1/0/all/0/1"&gt;Kashif Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1"&gt;Mohsen Guizani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1"&gt;Mounir Hamdi&lt;/a&gt;</name>
        </author>
    </entry>
</feed>