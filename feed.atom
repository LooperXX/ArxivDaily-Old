<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-05-27T03:45:28.761Z</updated>
    <generator>osmosfeed 1.9.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?. (arXiv:2105.04949v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04949</id>
        <link href="http://arxiv.org/abs/2105.04949"/>
        <updated>2021-05-27T01:32:29.916Z</updated>
        <summary type="html"><![CDATA[Analogies play a central role in human commonsense reasoning. The ability to
recognize analogies such as "eye is to seeing what ear is to hearing",
sometimes referred to as analogical proportions, shape how we structure
knowledge and understand language. Surprisingly, however, the task of
identifying such analogies has not yet received much attention in the language
model era. In this paper, we analyze the capabilities of transformer-based
language models on this unsupervised task, using benchmarks obtained from
educational settings, as well as more commonly used datasets. We find that
off-the-shelf language models can identify analogies to a certain extent, but
struggle with abstract and complex relations, and results are highly sensitive
to model architecture and hyperparameters. Overall the best results were
obtained with GPT-2 and RoBERTa, while configurations using BERT were not able
to outperform word embedding models. Our results raise important questions for
future work about how, and to what extent, pre-trained language models capture
knowledge about abstract semantic relations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1"&gt;Asahi Ushio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1"&gt;Luis Espinosa-Anke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1"&gt;Steven Schockaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1"&gt;Jose Camacho-Collados&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NukeLM: Pre-Trained and Fine-Tuned Language Models for the Nuclear and Energy Domains. (arXiv:2105.12192v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12192</id>
        <link href="http://arxiv.org/abs/2105.12192"/>
        <updated>2021-05-27T01:32:29.896Z</updated>
        <summary type="html"><![CDATA[Natural language processing (NLP) tasks (text classification, named entity
recognition, etc.) have seen revolutionary improvements over the last few
years. This is due to language models such as BERT that achieve deep knowledge
transfer by using a large pre-trained model, then fine-tuning the model on
specific tasks. The BERT architecture has shown even better performance on
domain-specific tasks when the model is pre-trained using domain-relevant
texts. Inspired by these recent advancements, we have developed NukeLM, a
nuclear-domain language model pre-trained on 1.5 million abstracts from the
U.S. Department of Energy Office of Scientific and Technical Information (OSTI)
database. This NukeLM model is then fine-tuned for the classification of
research articles into either binary classes (related to the nuclear fuel cycle
[NFC] or not) or multiple categories related to the subject of the article. We
show that continued pre-training of a BERT-style architecture prior to
fine-tuning yields greater performance on both article classification tasks.
This information is critical for properly triaging manuscripts, a necessary
task for better understanding citation networks that publish in the nuclear
space, and for uncovering new areas of research in the nuclear (or
nuclear-relevant) domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burke_L/0/1/0/all/0/1"&gt;Lee Burke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pazdernik_K/0/1/0/all/0/1"&gt;Karl Pazdernik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortin_D/0/1/0/all/0/1"&gt;Daniel Fortin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1"&gt;Benjamin Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goychayev_R/0/1/0/all/0/1"&gt;Rustam Goychayev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattingly_J/0/1/0/all/0/1"&gt;John Mattingly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerated Gradient Tracking over Time-varying Graphs for Decentralized Optimization. (arXiv:2104.02596v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02596</id>
        <link href="http://arxiv.org/abs/2104.02596"/>
        <updated>2021-05-27T01:32:29.890Z</updated>
        <summary type="html"><![CDATA[Decentralized optimization over time-varying graphs has been increasingly
common in modern machine learning with massive data stored on millions of
mobile devices, such as in federated learning. This paper revisits the widely
used accelerated gradient tracking and extends it to time-varying graphs. We
prove the $O((\frac{\gamma}{1-\sigma_{\gamma}})^2\sqrt{\frac{L}{\epsilon}})$
and
$O((\frac{\gamma}{1-\sigma_{\gamma}})^{1.5}\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})$
complexities for the practical single loop accelerated gradient tracking over
time-varying graphs when the problems are nonstrongly convex and strongly
convex, respectively, where $\gamma$ and $\sigma_{\gamma}$ are two common
constants charactering the network connectivity, $\epsilon$ is the desired
precision, and $L$ and $\mu$ are the smoothness and strong convexity constants,
respectively. Our complexities improve significantly over the ones of
$O(\frac{1}{\epsilon^{5/7}})$ and
$O((\frac{L}{\mu})^{5/7}\frac{1}{(1-\sigma)^{1.5}}\log\frac{1}{\epsilon})$,
respectively, which were proved in the original literature only for static
graphs, where $\frac{1}{1-\sigma}$ equals $\frac{\gamma}{1-\sigma_{\gamma}}$
when the network is time-invariant. When combining with a multiple consensus
subroutine, the dependence on the network connectivity constants can be further
improved to $O(1)$ and $O(\frac{\gamma}{1-\sigma_{\gamma}})$ for the
computation and communication complexities, respectively. When the network is
static, by employing the Chebyshev acceleration, our complexities exactly match
the lower bounds without hiding any poly-logarithmic factor for both
nonstrongly convex and strongly convex problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_H/0/1/0/all/0/1"&gt;Huan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ModelPS: An Interactive and Collaborative Platform for Editing Pre-trained Models at Scale. (arXiv:2105.08275v2 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08275</id>
        <link href="http://arxiv.org/abs/2105.08275"/>
        <updated>2021-05-27T01:32:29.881Z</updated>
        <summary type="html"><![CDATA[AI engineering has emerged as a crucial discipline to democratize deep neural
network (DNN) models among software developers with a diverse background. In
particular, altering these DNN models in the deployment stage posits a
tremendous challenge. In this research, we propose and develop a low-code
solution, ModelPS (an acronym for "Model Photoshop"), to enable and empower
collaborative DNN model editing and intelligent model serving. The ModelPS
solution embodies two transformative features: 1) a user-friendly web interface
for a developer team to share and edit DNN models pictorially, in a low-code
fashion, and 2) a model genie engine in the backend to aid developers in
customizing model editing configurations for given deployment requirements or
constraints. Our case studies with a wide range of deep learning (DL) models
show that the system can tremendously reduce both development and communication
overheads with improved productivity. The code has been released as an
open-source package at GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huaizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shanshan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yonggang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yong Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Optical Learning Operator. (arXiv:2012.12404v2 [physics.optics] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12404</id>
        <link href="http://arxiv.org/abs/2012.12404"/>
        <updated>2021-05-27T01:32:29.872Z</updated>
        <summary type="html"><![CDATA[Today's heavy machine learning tasks are fueled by large datasets. Computing
is performed with power hungry processors whose performance is ultimately
limited by the data transfer to and from memory. Optics is one of the powerful
means of communicating and processing information and there is intense current
interest in optical information processing for realizing high-speed
computations. Here we present and experimentally demonstrate an optical
computing framework based on spatiotemporal effects in multimode fibers for a
range of learning tasks from classifying COVID-19 X-ray lung images and speech
recognition to predicting age from face images. The presented framework
overcomes the energy scaling problem of existing systems without compromising
speed. We leveraged simultaneous, linear, and nonlinear interaction of spatial
modes as a computation engine. We numerically and experimentally showed the
ability of the method to execute several different tasks with accuracy
comparable to a digital implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Tegin_U/0/1/0/all/0/1"&gt;U&amp;#x11f;ur Te&amp;#x11f;in&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yildirim_M/0/1/0/all/0/1"&gt;Mustafa Y&amp;#x131;ld&amp;#x131;r&amp;#x131;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Oguz_I/0/1/0/all/0/1"&gt;&amp;#x130;lker O&amp;#x11f;uz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Moser_C/0/1/0/all/0/1"&gt;Christophe Moser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Psaltis_D/0/1/0/all/0/1"&gt;Demetri Psaltis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models. (arXiv:2102.10440v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10440</id>
        <link href="http://arxiv.org/abs/2102.10440"/>
        <updated>2021-05-27T01:32:29.854Z</updated>
        <summary type="html"><![CDATA[While probabilistic models are an important tool for studying causality,
doing so suffers from the intractability of inference. As a step towards
tractable causal models, we consider the problem of learning interventional
distributions using sum-product networks (SPNs) that are over-parameterized by
gate functions, e.g., neural networks. Providing an arbitrarily intervened
causal graph as input, effectively subsuming Pearl's do-operator, the gate
function predicts the parameters of the SPN. The resulting interventional SPNs
are motivated and illustrated by a structural causal model themed around
personal health. Our empirical evaluation on three benchmark data sets as well
as a synthetic health data set clearly demonstrates that interventional SPNs
indeed are both expressive in modelling and flexible in adapting to the
interventions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1"&gt;Matej Ze&amp;#x10d;evi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1"&gt;Devendra Singh Dhami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanam_A/0/1/0/all/0/1"&gt;Athresh Karanam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1"&gt;Sriraam Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Normalizing Flows for Deep Gaussian Processes. (arXiv:2104.08472v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08472</id>
        <link href="http://arxiv.org/abs/2104.08472"/>
        <updated>2021-05-27T01:32:29.838Z</updated>
        <summary type="html"><![CDATA[Deep Gaussian processes (DGPs), a hierarchical composition of GP models, have
successfully boosted the expressive power of their single-layer counterpart.
However, it is impossible to perform exact inference in DGPs, which has
motivated the recent development of variational inference-based methods.
Unfortunately, either these methods yield a biased posterior belief or it is
difficult to evaluate their convergence. This paper introduces a new approach
for specifying flexible, arbitrarily complex, and scalable approximate
posterior distributions. The posterior distribution is constructed through a
normalizing flow (NF) which transforms a simple initial probability into a more
complex one through a sequence of invertible transformations. Moreover, a novel
convolutional normalizing flow (CNF) is developed to improve the time
efficiency and capture dependency between layers. Empirical evaluation shows
that CNF DGP outperforms the state-of-the-art approximation methods for DGPs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Haibin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dapeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yizhou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1"&gt;Bryan Kian Hsiang Low&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1"&gt;Patrick Jaillet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Active Deep Learning: From Model-driven to Data-driven. (arXiv:2101.09933v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09933</id>
        <link href="http://arxiv.org/abs/2101.09933"/>
        <updated>2021-05-27T01:32:29.833Z</updated>
        <summary type="html"><![CDATA[Which samples should be labelled in a large data set is one of the most
important problems for trainingof deep learning. So far, a variety of active
sample selection strategies related to deep learning havebeen proposed in many
literatures. We defined them as Active Deep Learning (ADL) only if
theirpredictor is deep model, where the basic learner is called as predictor
and the labeling schemes iscalled selector. In this survey, three fundamental
factors in selector designation were summarized. Wecategory ADL into
model-driven ADL and data-driven ADL, by whether its selector is model-drivenor
data-driven. The different characteristics of the two major type of ADL were
addressed in indetail respectively. Furthermore, different sub-classes of
data-driven and model-driven ADL are alsosummarized and discussed emphatically.
The advantages and disadvantages between data-driven ADLand model-driven ADL
are thoroughly analyzed. We pointed out that, with the development of
deeplearning, the selector in ADL also is experiencing the stage from
model-driven to data-driven. Finally,we make discussion on ADL about its
uncertainty, explanatory, foundations of cognitive science etc.and survey on
the trend of ADL from model-driven to data-driven.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lizhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1"&gt;Guojin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lei Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning Methods for Structure-Guided Processing Path Optimization. (arXiv:2009.09706v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09706</id>
        <link href="http://arxiv.org/abs/2009.09706"/>
        <updated>2021-05-27T01:32:29.811Z</updated>
        <summary type="html"><![CDATA[A major goal of materials design is to find material structures with desired
properties and in a second step to find a processing path to reach one of these
structures. In this paper, we propose and investigate a deep reinforcement
learning approach for the optimization of processing paths. The goal is to find
optimal processing paths in the material structure space that lead to
target-structures, which have been identified beforehand to result in desired
material properties. As the relation between properties and structures is
generally non-unique, typically a whole set of target-structures can be
identified, that lead to desired properties. Our proposed method optimizes
processing paths from a start structure to one of these equivalent
target-structures. The algorithm learns to find near-optimal paths by
interacting with the structure-generating process. It is guided by structure
descriptors as process state features and a reward signal, which is formulated
based on a distance function in the structure space. The model-free
reinforcement learning algorithm learns through trial and error while
interacting with the process and does not rely on a priori sampled processing
data. We instantiate and evaluate the proposed methods by optimizing paths of a
generic metal forming process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dornheim_J/0/1/0/all/0/1"&gt;Johannes Dornheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morand_L/0/1/0/all/0/1"&gt;Lukas Morand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeitvogel_S/0/1/0/all/0/1"&gt;Samuel Zeitvogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iraki_T/0/1/0/all/0/1"&gt;Tarek Iraki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Link_N/0/1/0/all/0/1"&gt;Norbert Link&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helm_D/0/1/0/all/0/1"&gt;Dirk Helm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEAR: Sketching BFGS Algorithm for Ultra-High Dimensional Feature Selection in Sublinear Memory. (arXiv:2010.13829v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13829</id>
        <link href="http://arxiv.org/abs/2010.13829"/>
        <updated>2021-05-27T01:32:29.805Z</updated>
        <summary type="html"><![CDATA[We consider feature selection for applications in machine learning where the
dimensionality of the data is so large that it exceeds the working memory of
the (local) computing machine. Unfortunately, current large-scale sketching
algorithms show poor memory-accuracy trade-off due to the irreversible
collision and accumulation of the stochastic gradient noise in the sketched
domain. Here, we develop a second-order ultra-high dimensional feature
selection algorithm, called BEAR, which avoids the extra collisions by storing
the second-order gradients in the celebrated Broyden-Fletcher-Goldfarb-Shannon
(BFGS) algorithm in Count Sketch, a sublinear memory data structure from the
streaming literature. Experiments on real-world data sets demonstrate that BEAR
requires up to three orders of magnitude less memory space to achieve the same
classification accuracy compared to the first-order sketching algorithms.
Theoretical analysis proves convergence of BEAR with rate O(1/t) in t
iterations of the sketched algorithm. Our algorithm reveals an unexplored
advantage of second-order optimization for memory-constrained sketching of
models trained on ultra-high dimensional data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghazadeh_A/0/1/0/all/0/1"&gt;Amirali Aghazadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vipul Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DeWeese_A/0/1/0/all/0/1"&gt;Alex DeWeese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyluoglu_O/0/1/0/all/0/1"&gt;O. Ozan Koyluoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction. (arXiv:2101.11427v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11427</id>
        <link href="http://arxiv.org/abs/2101.11427"/>
        <updated>2021-05-27T01:32:29.783Z</updated>
        <summary type="html"><![CDATA[Traditional industrial recommenders are usually trained on a single business
domain and then serve for this domain. However, in large commercial platforms,
it is often the case that the recommenders need to make click-through rate
(CTR) predictions for multiple business domains. Different domains have
overlapping user groups and items. Thus, there exist commonalities. Since the
specific user groups have disparity and the user behaviors may change in
various business domains, there also have distinctions. The distinctions result
in domain-specific data distributions, making it hard for a single shared model
to work well on all domains. To learn an effective and efficient CTR model to
handle multiple domains simultaneously, we present Star Topology Adaptive
Recommender (STAR). Concretely, STAR has the star topology, which consists of
the shared centered parameters and domain-specific parameters. The shared
parameters are applied to learn commonalities of all domains, and the
domain-specific parameters capture domain distinction for more refined
prediction. Given requests from different business domains, STAR can adapt its
parameters conditioned on the domain characteristics. The experimental result
from production data validates the superiority of the proposed STAR model.
Since 2020, STAR has been deployed in the display advertising system of
Alibaba, obtaining averaging 8.0% improvement on CTR and 6.0% on RPM (Revenue
Per Mille).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1"&gt;Xiang-Rong Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liqin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Guorui Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xinyao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Binding Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1"&gt;Qiang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Siran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jingshan Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1"&gt;Hongbo Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Route via Theory-Guided Residual Network. (arXiv:2105.08279v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08279</id>
        <link href="http://arxiv.org/abs/2105.08279"/>
        <updated>2021-05-27T01:32:29.767Z</updated>
        <summary type="html"><![CDATA[The heavy traffic and related issues have always been concerns for modern
cities. With the help of deep learning and reinforcement learning, people have
proposed various policies to solve these traffic-related problems, such as
smart traffic signal control systems and taxi dispatching systems. People
usually validate these policies in a city simulator, since directly applying
them in the real city introduces real cost. However, these policies validated
in the city simulator may fail in the real city if the simulator is
significantly different from the real world. To tackle this problem, we need to
build a real-like traffic simulation system. Therefore, in this paper, we
propose to learn the human routing model, which is one of the most essential
part in the traffic simulator. This problem has two major challenges. First,
human routing decisions are determined by multiple factors, besides the common
time and distance factor. Second, current historical routes data usually covers
just a small portion of vehicles, due to privacy and device availability
issues. To address these problems, we propose a theory-guided residual network
model, where the theoretical part can emphasize the general principles for
human routing decisions (e.g., fastest route), and the residual part can
capture drivable condition preferences (e.g., local road or highway). Since the
theoretical part is composed of traditional shortest path algorithms that do
not need data to train, our residual network can learn human routing models
from limited data. We have conducted extensive experiments on multiple
real-world datasets to show the superior performance of our model, especially
with small data. Besides, we have also illustrated why our model is better at
recovering real routes through case studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guanjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenhui Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hop-Count Based Self-Supervised Anomaly Detection on Attributed Networks. (arXiv:2104.07917v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07917</id>
        <link href="http://arxiv.org/abs/2104.07917"/>
        <updated>2021-05-27T01:32:29.761Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed an upsurge of interest in the problem of anomaly
detection on attributed networks due to its importance in both research and
practice. Although various approaches have been proposed to solve this problem,
two major limitations exist: (1) unsupervised approaches usually work much less
efficiently due to the lack of supervisory signal, and (2) existing anomaly
detection methods only use local contextual information to detect anomalous
nodes, e.g., one- or two-hop information, but ignore the global contextual
information. Since anomalous nodes differ from normal nodes in structures and
attributes, it is intuitive that the distance between anomalous nodes and their
neighbors should be larger than that between normal nodes and their neighbors
if we remove the edges connecting anomalous and normal nodes. Thus, hop counts
based on both global and local contextual information can be served as the
indicators of anomaly. Motivated by this intuition, we propose a hop-count
based model (HCM) to detect anomalies by modeling both local and global
contextual information. To make better use of hop counts for anomaly
identification, we propose to use hop counts prediction as a self-supervised
task. We design two anomaly scores based on the hop counts prediction via HCM
model to identify anomalies. Besides, we employ Bayesian learning to train HCM
model for capturing uncertainty in learned parameters and avoiding overfitting.
Extensive experiments on real-world attributed networks demonstrate that our
proposed model is effective in anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tianjin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1"&gt;Yulong Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1"&gt;Vlado Menkovski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Priors in Bayesian Deep Learning: A Review. (arXiv:2105.06868v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06868</id>
        <link href="http://arxiv.org/abs/2105.06868"/>
        <updated>2021-05-27T01:32:29.756Z</updated>
        <summary type="html"><![CDATA[While the choice of prior is one of the most critical parts of the Bayesian
inference workflow, recent Bayesian deep learning models have often fallen back
on vague priors, such as standard Gaussians. In this review, we highlight the
importance of prior choices for Bayesian deep learning and present an overview
of different priors that have been proposed for (deep) Gaussian processes,
variational autoencoders, and Bayesian neural networks. We also outline
different methods of learning priors for these models from data. We hope to
motivate practitioners in Bayesian deep learning to think more carefully about
the prior specification for their models and to provide them with some
inspiration in this regard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation Capabilities of Wasserstein Generative Adversarial Networks. (arXiv:2103.10060v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10060</id>
        <link href="http://arxiv.org/abs/2103.10060"/>
        <updated>2021-05-27T01:32:29.751Z</updated>
        <summary type="html"><![CDATA[In this paper, we study Wasserstein Generative Adversarial Networks (WGANs)
using GroupSort neural networks as discriminators. We show that the error bound
for the approximation of target distribution depends on both the width/depth
(capacity) of generators and discriminators, as well as the number of samples
in training. A quantified generalization bound is established for Wasserstein
distance between the generated distribution and the target distribution.
According to our theoretical results, WGANs have higher requirement for the
capacity of discriminators than that of generators, which is consistent with
some existing theories. More importantly, overly deep and wide (high capacity)
generators may cause worse results (after training) than low capacity
generators if discriminators are not strong enough. Numerical results on the
synthetic data (swiss roll) and MNIST data confirm our theoretical results, and
demonstrate that the performance by using GroupSort neural networks as
discriminators is better than that of the original WGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yihang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingjie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael K. Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards creativity characterization of generative models via group-based subset scanning. (arXiv:2104.00479v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00479</id>
        <link href="http://arxiv.org/abs/2104.00479"/>
        <updated>2021-05-27T01:32:29.746Z</updated>
        <summary type="html"><![CDATA[Deep generative models, such as Variational Autoencoders (VAEs), have been
employed widely in computational creativity research. However, such models
discourage out-of-distribution generation to avoid spurious sample generation,
limiting their creativity. Thus, incorporating research on human creativity
into generative deep learning techniques presents an opportunity to make their
outputs more compelling and human-like. As we see the emergence of generative
models directed to creativity research, a need for machine learning-based
surrogate metrics to characterize creative output from these models is
imperative. We propose group-based subset scanning to quantify, detect, and
characterize creative processes by detecting a subset of anomalous
node-activations in the hidden layers of generative models. Our experiments on
original, typically decoded, and "creatively decoded" (Das et al 2020) image
datasets reveal that the proposed subset scores distribution is more useful for
detecting creative processes in the activation space rather than the pixel
space. Further, we found that creative samples generate larger subsets of
anomalies than normal or non-creative samples across datasets. The node
activations highlighted during the creative decoding process are different from
those responsible for normal sample generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1"&gt;Celia Cintas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quanz_B/0/1/0/all/0/1"&gt;Brian Quanz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1"&gt;Skyler Speakman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akinwande_V/0/1/0/all/0/1"&gt;Victor Akinwande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Provable Robustness of Quantum Classification via Quantum Hypothesis Testing. (arXiv:2009.10064v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10064</id>
        <link href="http://arxiv.org/abs/2009.10064"/>
        <updated>2021-05-27T01:32:29.731Z</updated>
        <summary type="html"><![CDATA[Quantum machine learning models have the potential to offer speedups and
better predictive accuracy compared to their classical counterparts. However,
these quantum algorithms, like their classical counterparts, have been shown to
also be vulnerable to input perturbations, in particular for classification
problems. These can arise either from noisy implementations or, as a worst-case
type of noise, adversarial attacks. In order to develop defence mechanisms and
to better understand the reliability of these algorithms, it is crucial to
understand their robustness properties in presence of natural noise sources or
adversarial manipulation. From the observation that measurements involved in
quantum classification algorithms are naturally probabilistic, we uncover and
formalize a fundamental link between binary quantum hypothesis testing and
provably robust quantum classification. This link leads to a tight robustness
condition which puts constraints on the amount of noise a classifier can
tolerate, independent of whether the noise source is natural or adversarial.
Based on this result, we develop practical protocols to optimally certify
robustness. Finally, since this is a robustness condition against worst-case
types of noise, our result naturally extends to scenarios where the noise
source is known. Thus, we also provide a framework to study the reliability of
quantum classification protocols beyond the adversarial, worst-case noise
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Weber_M/0/1/0/all/0/1"&gt;Maurice Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nana Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhikuan Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Community Detection Approaches: From Statistical Modeling to Deep Representation. (arXiv:2101.01669v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01669</id>
        <link href="http://arxiv.org/abs/2101.01669"/>
        <updated>2021-05-27T01:32:29.726Z</updated>
        <summary type="html"><![CDATA[Community detection, a fundamental task for network analysis, aims to
partition a network into multiple sub-structures to help reveal their latent
functions. Community detection has been extensively studied in and broadly
applied to many real-world network problems. Classical approaches to community
detection typically utilize probabilistic graphical models and adopt a variety
of prior knowledge to infer community structures. As the problems that network
methods try to solve and the network data to be analyzed become increasingly
more sophisticated, new approaches have also been proposed and developed,
particularly those that utilize deep learning and convert networked data into
low dimensional representation. Despite all the recent advancement, there is
still a lack of insightful understanding of the theoretical and methodological
underpinning of community detection, which will be critically important for
future development of the area of network analysis. In this paper, we develop
and present a unified architecture of network community-finding methods to
characterize the state-of-the-art of the field of community detection.
Specifically, we provide a comprehensive review of the existing community
detection methods and introduce a new taxonomy that divides the existing
methods into two categories, namely probabilistic graphical model and deep
learning. We then discuss in detail the main idea behind each method in the two
categories. Furthermore, to promote future development of community detection,
we release several benchmark datasets from several problem domains and
highlight their applications to various network analysis tasks. We conclude
with discussions of the challenges of the field and suggestions of possible
directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1"&gt;Di Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhizhi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_P/0/1/0/all/0/1"&gt;Pengfei Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weixiong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limitations of Autoregressive Models and Their Alternatives. (arXiv:2010.11939v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11939</id>
        <link href="http://arxiv.org/abs/2010.11939"/>
        <updated>2021-05-27T01:32:29.720Z</updated>
        <summary type="html"><![CDATA[Standard autoregressive language models perform only polynomial-time
computation to compute the probability of the next symbol. While this is
attractive, it means they cannot model distributions whose next-symbol
probability is hard to compute. Indeed, they cannot even model them well enough
to solve associated easy decision problems for which an engineer might want to
consult a language model. These limitations apply no matter how much
computation and data are used to train the model, unless the model is given
access to oracle parameters that grow superpolynomially in sequence length.

Thus, simply training larger autoregressive language models is not a panacea
for NLP. Alternatives include energy-based models (which give up efficient
sampling) and latent-variable autoregressive models (which give up efficient
scoring of a given string). Both are powerful enough to escape the above
limitations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chu-Cheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaech_A/0/1/0/all/0/1"&gt;Aaron Jaech&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1"&gt;Matthew R. Gormley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1"&gt;Jason Eisner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction. (arXiv:2101.11427v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11427</id>
        <link href="http://arxiv.org/abs/2101.11427"/>
        <updated>2021-05-27T01:32:29.715Z</updated>
        <summary type="html"><![CDATA[Traditional industrial recommenders are usually trained on a single business
domain and then serve for this domain. However, in large commercial platforms,
it is often the case that the recommenders need to make click-through rate
(CTR) predictions for multiple business domains. Different domains have
overlapping user groups and items. Thus, there exist commonalities. Since the
specific user groups have disparity and the user behaviors may change in
various business domains, there also have distinctions. The distinctions result
in domain-specific data distributions, making it hard for a single shared model
to work well on all domains. To learn an effective and efficient CTR model to
handle multiple domains simultaneously, we present Star Topology Adaptive
Recommender (STAR). Concretely, STAR has the star topology, which consists of
the shared centered parameters and domain-specific parameters. The shared
parameters are applied to learn commonalities of all domains, and the
domain-specific parameters capture domain distinction for more refined
prediction. Given requests from different business domains, STAR can adapt its
parameters conditioned on the domain characteristics. The experimental result
from production data validates the superiority of the proposed STAR model.
Since 2020, STAR has been deployed in the display advertising system of
Alibaba, obtaining averaging 8.0% improvement on CTR and 6.0% on RPM (Revenue
Per Mille).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1"&gt;Xiang-Rong Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liqin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Guorui Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xinyao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Binding Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1"&gt;Qiang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Siran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jingshan Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1"&gt;Hongbo Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deeper Look at Discounting Mismatch in Actor-Critic Algorithms. (arXiv:2010.01069v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01069</id>
        <link href="http://arxiv.org/abs/2010.01069"/>
        <updated>2021-05-27T01:32:29.710Z</updated>
        <summary type="html"><![CDATA[We investigate the discounting mismatch in actor-critic algorithm
implementations from a representation learning perspective. Theoretically,
actor-critic algorithms usually have discounting for both actor and critic,
i.e., there is a $\gamma^t$ term in the actor update for the transition
observed at time $t$ in a trajectory and the critic is a discounted value
function. Practitioners, however, usually ignore the discounting ($\gamma^t$)
for the actor while using a discounted critic. We investigate this mismatch in
two scenarios. In the first scenario, we consider optimizing an undiscounted
objective $(\gamma = 1)$ where $\gamma^t$ disappears naturally $(1^t = 1)$. We
then propose to interpret the discounting in critic in terms of a
bias-variance-representation trade-off and provide supporting empirical
results. In the second scenario, we consider optimizing a discounted objective
($\gamma < 1$) and propose to interpret the omission of the discounting in the
actor update from an auxiliary task perspective and provide supporting
empirical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shangtong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1"&gt;Romain Laroche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seijen_H/0/1/0/all/0/1"&gt;Harm van Seijen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Combes_R/0/1/0/all/0/1"&gt;Remi Tachet des Combes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Vocabulary Entities in Link Prediction. (arXiv:2105.12524v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12524</id>
        <link href="http://arxiv.org/abs/2105.12524"/>
        <updated>2021-05-27T01:32:29.694Z</updated>
        <summary type="html"><![CDATA[Knowledge graph embedding techniques are key to making knowledge graphs
amenable to the plethora of machine learning approaches based on vector
representations. Link prediction is often used as a proxy to evaluate the
quality of these embeddings. Given that the creation of benchmarks for link
prediction is a time-consuming endeavor, most work on the subject matter uses
only a few benchmarks. As benchmarks are crucial for the fair comparison of
algorithms, ensuring their quality is tantamount to providing a solid ground
for developing better solutions to link prediction and ipso facto embedding
knowledge graphs. First studies of benchmarks pointed to limitations pertaining
to information leaking from the development to the test fragments of some
benchmark datasets. We spotted a further common limitation of three of the
benchmarks commonly used for evaluating link prediction approaches:
out-of-vocabulary entities in the test and validation sets. We provide an
implementation of an approach for spotting and removing such entities and
provide corrected versions of the datasets WN18RR, FB15K-237, and YAGO3-10. Our
experiments on the corrected versions of WN18RR, FB15K-237, and YAGO3-10
suggest that the measured performance of state-of-the-art approaches is altered
significantly with p-values <1%, <1.4%, and <1%, respectively. Overall,
state-of-the-art approaches gain on average absolute $3.29 \pm 0.24\%$ in all
metrics on WN18RR. This means that some of the conclusions achieved in previous
works might need to be revisited. We provide an open-source implementation of
our experiments and corrected datasets at at
https://github.com/dice-group/OOV-In-Link-Prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1"&gt;Caglar Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1"&gt;Axel-Cyrille Ngonga Ngomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CASA: A Bridge Between Gradient of Policy Improvement and Policy Evaluation. (arXiv:2105.03923v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03923</id>
        <link href="http://arxiv.org/abs/2105.03923"/>
        <updated>2021-05-27T01:32:29.689Z</updated>
        <summary type="html"><![CDATA[This paper introduces a novel design of model-free reinforcement learning,
CASA, Critic AS an Actor. CASA follows the actor-critic framework that
estimates state-value, state-action-value and policy simultaneously. We prove
that CASA integrates a consistent path for the policy evaluation and the policy
improvement, which completely eliminates the gradient conflict between the
policy improvement and the policy evaluation. The policy evaluation is
equivalent to a compensational policy improvement, which alleviates the
function approximation error, and is also equivalent to an entropy-regularized
policy improvement, which prevents the policy from being trapped into a
suboptimal solution. Building on this design, an expectation-correct Doubly
Robust Trace is introduced to learn state-value and state-action-value, and the
convergence is guaranteed. Our experiments show that the design achieves
State-Of-The-Art on Arcade Learning Environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Changnan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Haosen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiajun Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shihong Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IntelliCAT: Intelligent Machine Translation Post-Editing with Quality Estimation and Translation Suggestion. (arXiv:2105.12172v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12172</id>
        <link href="http://arxiv.org/abs/2105.12172"/>
        <updated>2021-05-27T01:32:29.684Z</updated>
        <summary type="html"><![CDATA[We present IntelliCAT, an interactive translation interface with neural
models that streamline the post-editing process on machine translation output.
We leverage two quality estimation (QE) models at different granularities:
sentence-level QE, to predict the quality of each machine-translated sentence,
and word-level QE, to locate the parts of the machine-translated sentence that
need correction. Additionally, we introduce a novel translation suggestion
model conditioned on both the left and right contexts, providing alternatives
for specific words or phrases for correction. Finally, with word alignments,
IntelliCAT automatically preserves the original document's styles in the
translated document. The experimental results show that post-editing based on
the proposed QE and translation suggestions can significantly improve
translation quality. Furthermore, a user study reveals that three features
provided in IntelliCAT significantly accelerate the post-editing task,
achieving a 52.9\% speedup in translation time compared to translating from
scratch. The interface is publicly available at
https://intellicat.beringlab.com/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dongjun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1"&gt;Junhyeong Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Heesoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1"&gt;Jaemin Jo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth. (arXiv:2105.04550v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04550</id>
        <link href="http://arxiv.org/abs/2105.04550"/>
        <updated>2021-05-27T01:32:29.678Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have been studied through the lens of expressive
power and generalization. However, their optimization properties are less well
understood. We take the first step towards analyzing GNN training by studying
the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that
despite the non-convexity of training, convergence to a global minimum at a
linear rate is guaranteed under mild assumptions that we validate on real-world
graphs. Second, we study what may affect the GNNs' training speed. Our results
show that the training of GNNs is implicitly accelerated by skip connections,
more depth, and/or a good label distribution. Empirical results confirm that
our theoretical results for linearized GNNs align with the training behavior of
nonlinear GNNs. Our results provide the first theoretical support for the
success of GNNs with skip connections in terms of optimization, and suggest
that deep GNNs with skip connections would be promising in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Keyulu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mozhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1"&gt;Stefanie Jegelka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning representations with end-to-end models for improved remaining useful life prognostics. (arXiv:2104.05049v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05049</id>
        <link href="http://arxiv.org/abs/2104.05049"/>
        <updated>2021-05-27T01:32:29.673Z</updated>
        <summary type="html"><![CDATA[The remaining Useful Life (RUL) of equipment is defined as the duration
between the current time and its failure. An accurate and reliable prognostic
of the remaining useful life provides decision-makers with valuable information
to adopt an appropriate maintenance strategy to maximize equipment utilization
and avoid costly breakdowns. In this work, we propose an end-to-end deep
learning model based on multi-layer perceptron and long short-term memory
layers (LSTM) to predict the RUL. After normalization of all data, inputs are
fed directly to an MLP layers for feature learning, then to an LSTM layer to
capture temporal dependencies, and finally to other MLP layers for RUL
prognostic. The proposed architecture is tested on the NASA commercial modular
aero-propulsion system simulation (C-MAPSS) dataset. Despite its simplicity
with respect to other recently proposed models, the model developed outperforms
them with a significant decrease in the competition score and in the root mean
square error score between the predicted and the gold value of the RUL. In this
paper, we will discuss how the proposed end-to-end model is able to achieve
such good results and compare it to other deep learning and state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaoub_A/0/1/0/all/0/1"&gt;Alaaeddine Chaoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voisin_A/0/1/0/all/0/1"&gt;Alexandre Voisin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerisara_C/0/1/0/all/0/1"&gt;Christophe Cerisara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iung_B/0/1/0/all/0/1"&gt;Beno&amp;#xee;t Iung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptable Automation with Modular Deep Reinforcement Learning and Policy Transfer. (arXiv:2012.01934v1 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2012.01934</id>
        <link href="http://arxiv.org/abs/2012.01934"/>
        <updated>2021-05-27T01:32:29.657Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep Reinforcement Learning (RL) have created
unprecedented opportunities for intelligent automation, where a machine can
autonomously learn an optimal policy for performing a given task. However,
current deep RL algorithms predominantly specialize in a narrow range of tasks,
are sample inefficient, and lack sufficient stability, which in turn hinder
their industrial adoption. This article tackles this limitation by developing
and testing a Hyper-Actor Soft Actor-Critic (HASAC) RL framework based on the
notions of task modularization and transfer learning. The goal of the proposed
HASAC is to enhance the adaptability of an agent to new tasks by transferring
the learned policies of former tasks to the new task via a "hyper-actor". The
HASAC framework is tested on a new virtual robotic manipulation benchmark,
Meta-World. Numerical experiments show superior performance by HASAC over
state-of-the-art deep RL algorithms in terms of reward value, success rate, and
task completion time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raziei_Z/0/1/0/all/0/1"&gt;Zohreh Raziei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moghaddam_M/0/1/0/all/0/1"&gt;Mohsen Moghaddam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prosodic segmentation for parsing spoken dialogue. (arXiv:2105.12667v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12667</id>
        <link href="http://arxiv.org/abs/2105.12667"/>
        <updated>2021-05-27T01:32:29.651Z</updated>
        <summary type="html"><![CDATA[Parsing spoken dialogue poses unique difficulties, including disfluencies and
unmarked boundaries between sentence-like units. Previous work has shown that
prosody can help with parsing disfluent speech (Tran et al. 2018), but has
assumed that the input to the parser is already segmented into sentence-like
units (SUs), which isn't true in existing speech applications. We investigate
how prosody affects a parser that receives an entire dialogue turn as input (a
turn-based model), instead of gold standard pre-segmented SUs (an SU-based
model). In experiments on the English Switchboard corpus, we find that when
using transcripts alone, the turn-based model has trouble segmenting SUs,
leading to worse parse performance than the SU-based model. However, prosody
can effectively replace gold standard SU boundaries: with prosody, the
turn-based model performs as well as the SU-based model (90.79 vs. 90.65 F1
score, respectively), despite performing two tasks (SU segmentation and
parsing) rather than one (parsing alone). Analysis shows that pitch and
intensity features are the most important for this corpus, since they allow the
model to correctly distinguish an SU boundary from a speech disfluency -- a
distinction that the model otherwise struggles to make.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_E/0/1/0/all/0/1"&gt;Elizabeth Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1"&gt;Mark Steedman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1"&gt;Sharon Goldwater&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disambiguation of weak supervision with exponential convergence rates. (arXiv:2102.02789v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02789</id>
        <link href="http://arxiv.org/abs/2102.02789"/>
        <updated>2021-05-27T01:32:29.645Z</updated>
        <summary type="html"><![CDATA[Machine learning approached through supervised learning requires expensive
annotation of data. This motivates weakly supervised learning, where data are
annotated with incomplete yet discriminative information. In this paper, we
focus on partial labelling, an instance of weak supervision where, from a given
input, we are given a set of potential targets. We review a disambiguation
principle to recover full supervision from weak supervision, and propose an
empirical disambiguation algorithm. We prove exponential convergence rates of
our algorithm under classical learnability assumptions, and we illustrate the
usefulness of our method on practical examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1"&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[REPAINT: Knowledge Transfer in Deep Reinforcement Learning. (arXiv:2011.11827v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11827</id>
        <link href="http://arxiv.org/abs/2011.11827"/>
        <updated>2021-05-27T01:32:29.637Z</updated>
        <summary type="html"><![CDATA[Accelerating learning processes for complex tasks by leveraging previously
learned tasks has been one of the most challenging problems in reinforcement
learning, especially when the similarity between source and target tasks is
low. This work proposes REPresentation And INstance Transfer (REPAINT)
algorithm for knowledge transfer in deep reinforcement learning. REPAINT not
only transfers the representation of a pre-trained teacher policy in the
on-policy learning, but also uses an advantage-based experience selection
approach to transfer useful samples collected following the teacher policy in
the off-policy learning. Our experimental results on several benchmark tasks
show that REPAINT significantly reduces the total training time in generic
cases of task similarity. In particular, when the source tasks are dissimilar
to, or sub-tasks of, the target tasks, REPAINT outperforms other baselines in
both training-time reduction and asymptotic performance of return scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Yunzhe Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genc_S/0/1/0/all/0/1"&gt;Sahika Genc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1"&gt;Jonathan Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mallya_S/0/1/0/all/0/1"&gt;Sunil Mallya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Accuracy of Early-est Earthquake Magnitude Estimates with an LSTM Neural Network: A Preliminary Analysis. (arXiv:2104.05712v2 [physics.geo-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05712</id>
        <link href="http://arxiv.org/abs/2104.05712"/>
        <updated>2021-05-27T01:32:29.630Z</updated>
        <summary type="html"><![CDATA[This report presents a preliminary analysis of an LSTM neural network
designed to predict the accuracy of magnitude estimates computed by Early-est
during the first minutes after an earthquake occurs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Nazaria_M/0/1/0/all/0/1"&gt;Massimo Nazaria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning. (arXiv:2008.12260v2 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12260</id>
        <link href="http://arxiv.org/abs/2008.12260"/>
        <updated>2021-05-27T01:32:29.604Z</updated>
        <summary type="html"><![CDATA[Pollux improves scheduling performance in deep learning (DL) clusters by
adaptively co-optimizing inter-dependent factors both at the per-job level and
at the cluster-wide level. Most existing schedulers expect users to specify the
number of resources for each job, often leading to inefficient resource use.
Some recent schedulers choose job resources for users, but do so without
awareness of how DL training can be re-optimized to better utilize the provided
resources.

Pollux simultaneously considers both aspects. By monitoring the status of
each job during training, Pollux models how their goodput (a novel metric we
introduce that combines system throughput with statistical efficiency) would
change by adding or removing resources. Leveraging these information, Pollux
dynamically (re-)assigns resources to improve cluster-wide goodput, while
respecting fairness and continually optimizing each DL job to better utilize
those resources.

In experiments with real DL jobs and with trace-driven simulations, Pollux
reduces average job completion times by 37-50% relative to state-of-the-art DL
schedulers, even when they are provided with ideal resource and training
configurations for every job. Pollux promotes fairness among DL jobs competing
for resources based on a more meaningful measure of useful job progress, and
reveals a new opportunity for reducing DL cost in cloud environments. Pollux is
implemented and publicly available as part of an open-source project at
https://github.com/petuum/adaptdl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_A/0/1/0/all/0/1"&gt;Aurick Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_S/0/1/0/all/0/1"&gt;Sang Keun Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanya_S/0/1/0/all/0/1"&gt;Suhas Jayaram Subramanya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1"&gt;Willie Neiswanger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_Q/0/1/0/all/0/1"&gt;Qirong Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganger_G/0/1/0/all/0/1"&gt;Gregory R. Ganger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric P. Xing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding How Over-Parametrization Leads to Acceleration: A case of learning a single teacher neuron. (arXiv:2010.01637v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01637</id>
        <link href="http://arxiv.org/abs/2010.01637"/>
        <updated>2021-05-27T01:32:29.568Z</updated>
        <summary type="html"><![CDATA[Over-parametrization has become a popular technique in deep learning. It is
observed that by over-parametrization, a larger neural network needs a fewer
training iterations than a smaller one to achieve a certain level of
performance -- namely, over-parametrization leads to acceleration in
optimization. However, despite that over-parametrization is widely used
nowadays, little theory is available to explain the acceleration due to
over-parametrization. In this paper, we propose understanding it by studying a
simple problem first. Specifically, we consider the setting that there is a
single teacher neuron with quadratic activation, where over-parametrization is
realized by having multiple student neurons learn the data generated from the
teacher neuron. We provably show that over-parametrization helps the iterate
generated by gradient descent to enter the neighborhood of a global optimal
solution that achieves zero testing error faster. On the other hand, we also
point out an issue regarding the necessity of over-parametrization and study
how the scaling of the output neurons affects the convergence time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun-Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1"&gt;Jacob Abernethy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterization of Excess Risk for Locally Strongly Convex Population Risk. (arXiv:2012.02456v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02456</id>
        <link href="http://arxiv.org/abs/2012.02456"/>
        <updated>2021-05-27T01:32:29.553Z</updated>
        <summary type="html"><![CDATA[We establish upper bounds for the expected excess risk of models trained by
proper iterative algorithms which approximate the global minima (resp. local
minima) under convex (resp. non-convex) loss functions. In contrast to the
existing bounds, our results are not limited to a specific algorithm e.g.,
stochastic gradient descent, and the bounds remain small when the sample size
$n$ is large for an arbitrary number of iterations. In concrete, after a
certain number of iterations, the bound under convex loss functions is of order
$\tilde{\mathcal{O}}(1/n)$. Under non-convex loss functions with $d$ model
parameters such that $d/n$ is smaller than a threshold independent of $n$, the
order of $\tilde{\mathcal{O}}(1/n)$ can be maintained if the empirical risk has
no spurious local minima with high probability. The bound becomes
$\tilde{\mathcal{O}}(1/\sqrt{n})$ if we discard the assumption on the empirical
local minima. Technically, we assume the Hessian of the population risk is
non-degenerate at each local minima. Under this and some other mild smoothness
and boundedness assumptions, we establish our results via algorithmic stability
\citep{bousquet2002stability} and characterization of the empirical risk
landscape. Our bounds are dimensional insensitive and fast converges to zero as
$n$ goes to infinity. These underscore that with locally strongly convex
population risk, the models trained by proper iterative algorithms generalize
well on unseen data even when the loss function is non-convex and $d$ is large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1"&gt;Mingyang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhi-Ming Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Aqueous Solubility of Organic Molecules Using Deep Learning Models with Varied Molecular Representations. (arXiv:2105.12638v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2105.12638</id>
        <link href="http://arxiv.org/abs/2105.12638"/>
        <updated>2021-05-27T01:32:29.417Z</updated>
        <summary type="html"><![CDATA[Determining the aqueous solubility of molecules is a vital step in many
pharmaceutical, environmental, and energy storage applications. Despite efforts
made over decades, there are still challenges associated with developing a
solubility prediction model with satisfactory accuracy for many of these
applications. The goal of this study is to develop a general model capable of
predicting the solubility of a broad range of organic molecules. Using the
largest currently available solubility dataset, we implement deep
learning-based models to predict solubility from molecular structure and
explore several different molecular representations including molecular
descriptors, simplified molecular-input line-entry system (SMILES) strings,
molecular graphs, and three-dimensional (3D) atomic coordinates using four
different neural network architectures - fully connected neural networks
(FCNNs), recurrent neural networks (RNNs), graph neural networks (GNNs), and
SchNet. We find that models using molecular descriptors achieve the best
performance, with GNN models also achieving good performance. We perform
extensive error analysis to understand the molecular properties that influence
model performance, perform feature analysis to understand which information
about molecular structure is most valuable for prediction, and perform a
transfer learning and data size study to understand the impact of data
availability on model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Panapitiya_G/0/1/0/all/0/1"&gt;Gihan Panapitiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Girard_M/0/1/0/all/0/1"&gt;Michael Girard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Hollas_A/0/1/0/all/0/1"&gt;Aaron Hollas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Murugesan_V/0/1/0/all/0/1"&gt;Vijay Murugesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Saldanha_E/0/1/0/all/0/1"&gt;Emily Saldanha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Testing Product Distributions: A Closer Look. (arXiv:2012.14632v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14632</id>
        <link href="http://arxiv.org/abs/2012.14632"/>
        <updated>2021-05-27T01:32:29.411Z</updated>
        <summary type="html"><![CDATA[We study the problems of identity and closeness testing of $n$-dimensional
product distributions. Prior works by Canonne, Diakonikolas, Kane and Stewart
(COLT 2017) and Daskalakis and Pan (COLT 2017) have established tight sample
complexity bounds for non-tolerant testing over a binary alphabet: given two
product distributions $P$ and $Q$ over a binary alphabet, distinguish between
the cases $P = Q$ and $d_{\mathrm{TV}}(P, Q) > \epsilon$. We build on this
prior work to give a more comprehensive map of the complexity of testing of
product distributions by investigating tolerant testing with respect to several
natural distance measures and over an arbitrary alphabet. Our study gives a
fine-grained understanding of how the sample complexity of tolerant testing
varies with the distance measures for product distributions. In addition, we
also extend one of our upper bounds on product distributions to bounded-degree
Bayes nets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1"&gt;Arnab Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1"&gt;Sutanu Gayen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandasamy_S/0/1/0/all/0/1"&gt;Saravanan Kandasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinodchandran_N/0/1/0/all/0/1"&gt;N. V. Vinodchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoReCon: Neural Architecture Search-based Reconstruction for Data-free Compression. (arXiv:2105.12151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12151</id>
        <link href="http://arxiv.org/abs/2105.12151"/>
        <updated>2021-05-27T01:32:29.396Z</updated>
        <summary type="html"><![CDATA[Data-free compression raises a new challenge because the original training
dataset for a pre-trained model to be compressed is not available due to
privacy or transmission issues. Thus, a common approach is to compute a
reconstructed training dataset before compression. The current reconstruction
methods compute the reconstructed training dataset with a generator by
exploiting information from the pre-trained model. However, current
reconstruction methods focus on extracting more information from the
pre-trained model but do not leverage network engineering. This work is the
first to consider network engineering as an approach to design the
reconstruction method. Specifically, we propose the AutoReCon method, which is
a neural architecture search-based reconstruction method. In the proposed
AutoReCon method, the generator architecture is designed automatically given
the pre-trained model for reconstruction. Experimental results show that using
generators discovered by the AutoRecon method always improve the performance of
data-free compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1"&gt;Baozhou Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofstee_P/0/1/0/all/0/1"&gt;Peter Hofstee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peltenburg_J/0/1/0/all/0/1"&gt;Johan Peltenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jinho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alars_Z/0/1/0/all/0/1"&gt;Zaid Alars&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attack Framework on Graph Embedding Models with Limited Knowledge. (arXiv:2105.12419v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12419</id>
        <link href="http://arxiv.org/abs/2105.12419"/>
        <updated>2021-05-27T01:32:29.391Z</updated>
        <summary type="html"><![CDATA[With the success of the graph embedding model in both academic and industry
areas, the robustness of graph embedding against adversarial attack inevitably
becomes a crucial problem in graph learning. Existing works usually perform the
attack in a white-box fashion: they need to access the predictions/labels to
construct their adversarial loss. However, the inaccessibility of
predictions/labels makes the white-box attack impractical to a real graph
learning system. This paper promotes current frameworks in a more general and
flexible sense -- we demand to attack various kinds of graph embedding models
with black-box driven. We investigate the theoretical connections between graph
signal processing and graph embedding models and formulate the graph embedding
model as a general graph signal process with a corresponding graph filter.
Therefore, we design a generalized adversarial attacker: GF-Attack. Without
accessing any labels and model predictions, GF-Attack can perform the attack
directly on the graph filter in a black-box fashion. We further prove that
GF-Attack can perform an effective attack without knowing the number of layers
of graph embedding models. To validate the generalization of GF-Attack, we
construct the attacker on four popular graph embedding models. Extensive
experiments validate the effectiveness of GF-Attack on several benchmark
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Heng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tingyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenbing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honglei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1"&gt;Peng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenwu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Marius: Learning Massive Graph Embeddings on a Single Machine. (arXiv:2101.08358v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08358</id>
        <link href="http://arxiv.org/abs/2101.08358"/>
        <updated>2021-05-27T01:32:29.385Z</updated>
        <summary type="html"><![CDATA[We propose a new framework for computing the embeddings of large-scale graphs
on a single machine. A graph embedding is a fixed length vector representation
for each node (and/or edge-type) in a graph and has emerged as the de-facto
approach to apply modern machine learning on graphs. We identify that current
systems for learning the embeddings of large-scale graphs are bottlenecked by
data movement, which results in poor resource utilization and inefficient
training. These limitations require state-of-the-art systems to distribute
training across multiple machines. We propose Marius, a system for efficient
training of graph embeddings that leverages partition caching and buffer-aware
data orderings to minimize disk access and interleaves data movement with
computation to maximize utilization. We compare Marius against two
state-of-the-art industrial systems on a diverse array of benchmarks. We
demonstrate that Marius achieves the same level of accuracy but is up to one
order of magnitude faster. We also show that Marius can scale training to
datasets an order of magnitude beyond a single machine's GPU and CPU memory
capacity, enabling training of configurations with more than a billion edges
and 550 GB of total parameters on a single machine with 16 GB of GPU memory and
64 GB of CPU memory. Marius is open-sourced at www.marius-project.org.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohoney_J/0/1/0/all/0/1"&gt;Jason Mohoney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waleffe_R/0/1/0/all/0/1"&gt;Roger Waleffe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yiheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1"&gt;Theodoros Rekatsinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1"&gt;Shivaram Venkataraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Act Safely with Limited Exposure and Almost Sure Certainty. (arXiv:2105.08748v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08748</id>
        <link href="http://arxiv.org/abs/2105.08748"/>
        <updated>2021-05-27T01:32:29.380Z</updated>
        <summary type="html"><![CDATA[This paper aims to put forward the concept that learning to take safe actions
in unknown environments, even with probability one guarantees, can be achieved
without the need for an unbounded number of exploratory trials, provided that
one is willing to navigate trade-offs between optimality, level of exposure to
unsafe events, and the maximum detection time of unsafe actions. We illustrate
this concept in two complementary settings. We first focus on the canonical
multi-armed bandit problem and seek to study the intrinsic trade-offs of
learning safety in the presence of uncertainty. Under mild assumptions on
sufficient exploration, we provide an algorithm that provably detects all
unsafe machines in an (expected) finite number of rounds. The analysis also
unveils a trade-off between the number of rounds needed to secure the
environment and the probability of discarding safe machines. We then consider
the problem of finding optimal policies for a Markov Decision Process (MDP)
with almost sure constraints. We show that the (action) value function
satisfies a barrier-based decomposition which allows for the identification of
feasible policies independently of the reward process. Using this
decomposition, we develop a Barrier-learning algorithm, that identifies such
unsafe state-action pairs in a finite expected number of steps. Our analysis
further highlights a trade-off between the time lag for the underlying MDP
necessary to detect unsafe actions, and the level of exposure to unsafe events.
Simulations corroborate our theoretical findings, further illustrating the
aforementioned trade-offs, and suggesting that safety constraints can further
speed up the learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Castellano_A/0/1/0/all/0/1"&gt;Agustin Castellano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Min_H/0/1/0/all/0/1"&gt;Hancheng Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bazerque_J/0/1/0/all/0/1"&gt;Juan Bazerque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mallada_E/0/1/0/all/0/1"&gt;Enrique Mallada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pattern Detection in the Activation Space for Identifying Synthesized Content. (arXiv:2105.12479v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12479</id>
        <link href="http://arxiv.org/abs/2105.12479"/>
        <updated>2021-05-27T01:32:29.374Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have recently achieved unprecedented
success in photo-realistic image synthesis from low-dimensional random noise.
The ability to synthesize high-quality content at a large scale brings
potential risks as the generated samples may lead to misinformation that can
create severe social, political, health, and business hazards. We propose
SubsetGAN to identify generated content by detecting a subset of anomalous
node-activations in the inner layers of pre-trained neural networks. These
nodes, as a group, maximize a non-parametric measure of divergence away from
the expected distribution of activations created from real data. This enable us
to identify synthesised images without prior knowledge of their distribution.
SubsetGAN efficiently scores subsets of nodes and returns the group of nodes
within the pre-trained classifier that contributed to the maximum score. The
classifier can be a general fake classifier trained over samples from multiple
sources or the discriminator network from different GANs. Our approach shows
consistently higher detection power than existing detection methods across
several state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different
proportions of generated content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1"&gt;Celia Cintas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1"&gt;Skyler Speakman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1"&gt;Girmaw Abebe Tadesse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akinwande_V/0/1/0/all/0/1"&gt;Victor Akinwande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McFowland_E/0/1/0/all/0/1"&gt;Edward McFowland III&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weldemariam_K/0/1/0/all/0/1"&gt;Komminist Weldemariam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimNet: Learning Reactive Self-driving Simulations from Real-world Observations. (arXiv:2105.12332v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.12332</id>
        <link href="http://arxiv.org/abs/2105.12332"/>
        <updated>2021-05-27T01:32:29.369Z</updated>
        <summary type="html"><![CDATA[In this work, we present a simple end-to-end trainable machine learning
system capable of realistically simulating driving experiences. This can be
used for the verification of self-driving system performance without relying on
expensive and time-consuming road testing. In particular, we frame the
simulation problem as a Markov Process, leveraging deep neural networks to
model both state distribution and transition function. These are trainable
directly from the existing raw observations without the need for any
handcrafting in the form of plant or kinematic models. All that is needed is a
dataset of historical traffic episodes. Our formulation allows the system to
construct never seen scenes that unfold realistically reacting to the
self-driving car's behaviour. We train our system directly from 1,000 hours of
driving logs and measure both realism, reactivity of the simulation as the two
key properties of the simulation. At the same time, we apply the method to
evaluate the performance of a recently proposed state-of-the-art ML planning
system trained from human driving logs. We discover this planning system is
prone to previously unreported causal confusion issues that are difficult to
test by non-reactive simulation. To the best of our knowledge, this is the
first work that directly merges highly realistic data-driven simulations with a
closed-loop evaluation for self-driving vehicles. We make the data, code, and
pre-trained models publicly available to further stimulate simulation
development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bergamini_L/0/1/0/all/0/1"&gt;Luca Bergamini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yawei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheel_O/0/1/0/all/0/1"&gt;Oliver Scheel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chih Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1"&gt;Luca Del Pero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1"&gt;Blazej Osinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1"&gt;Hugo Grimmett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1"&gt;Peter Ondruska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation. (arXiv:2104.05801v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05801</id>
        <link href="http://arxiv.org/abs/2104.05801"/>
        <updated>2021-05-27T01:32:29.351Z</updated>
        <summary type="html"><![CDATA[With the recent advances of open-domain story generation, the lack of
reliable automatic evaluation metrics becomes an increasingly imperative issue
that hinders the fast development of story generation. According to conducted
researches in this regard, learnable evaluation metrics have promised more
accurate assessments by having higher correlations with human judgments. A
critical bottleneck of obtaining a reliable learnable evaluation metric is the
lack of high-quality training data for classifiers to efficiently distinguish
plausible and implausible machine-generated stories. Previous works relied on
\textit{heuristically manipulated} plausible examples to mimic possible system
drawbacks such as repetition, contradiction, or irrelevant content in the text
level, which can be \textit{unnatural} and \textit{oversimplify} the
characteristics of implausible machine-generated stories. We propose to tackle
these issues by generating a more comprehensive set of implausible stories
using {\em plots}, which are structured representations of controllable factors
used to generate stories. Since these plots are compact and structured, it is
easier to manipulate them to generate text with targeted undesirable
properties, while at the same time maintain the grammatical correctness and
naturalness of the generated sentences. To improve the quality of generated
implausible stories, we further apply the adversarial filtering procedure
presented by \citet{zellers2018swag} to select a more nuanced set of
implausible texts. Experiments show that the evaluation metrics trained on our
generated data result in more reliable automatic assessments that correlate
remarkably better with human judgments compared to the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1"&gt;Sarik Ghazarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zixi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+SM_A/0/1/0/all/0/1"&gt;Akash SM&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1"&gt;Ralph Weischedel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1"&gt;Aram Galstyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lenient Regret and Good-Action Identification in Gaussian Process Bandits. (arXiv:2102.05793v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05793</id>
        <link href="http://arxiv.org/abs/2102.05793"/>
        <updated>2021-05-27T01:32:29.344Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of Gaussian process (GP) bandits under
relaxed optimization criteria stating that any function value above a certain
threshold is "good enough". On the theoretical side, we study various {\em
lenient regret} notions in which all near-optimal actions incur zero penalty,
and provide upper bounds on the lenient regret for GP-UCB and an elimination
algorithm, circumventing the usual $O(\sqrt{T})$ term (with time horizon $T$)
resulting from zooming extremely close towards the function maximum. In
addition, we complement these upper bounds with algorithm-independent lower
bounds. On the practical side, we consider the problem of finding a single
"good action" according to a known pre-specified threshold, and introduce
several good-action identification algorithms that exploit knowledge of the
threshold. We experimentally find that such algorithms can often find a good
action faster than standard optimization-based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cai_X/0/1/0/all/0/1"&gt;Xu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gomes_S/0/1/0/all/0/1"&gt;Selwyn Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scarlett_J/0/1/0/all/0/1"&gt;Jonathan Scarlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Transparent Application of Machine Learning in Video Processing. (arXiv:2105.12700v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.12700</id>
        <link href="http://arxiv.org/abs/2105.12700"/>
        <updated>2021-05-27T01:32:29.339Z</updated>
        <summary type="html"><![CDATA[Machine learning techniques for more efficient video compression and video
enhancement have been developed thanks to breakthroughs in deep learning. The
new techniques, considered as an advanced form of Artificial Intelligence (AI),
bring previously unforeseen capabilities. However, they typically come in the
form of resource-hungry black-boxes (overly complex with little transparency
regarding the inner workings). Their application can therefore be unpredictable
and generally unreliable for large-scale use (e.g. in live broadcast). The aim
of this work is to understand and optimise learned models in video processing
applications so systems that incorporate them can be used in a more trustworthy
manner. In this context, the presented work introduces principles for
simplification of learned models targeting improved transparency in
implementing machine learning for video production and distribution
applications. These principles are demonstrated on video compression examples,
showing how bitrate savings and reduced complexity can be achieved by
simplifying relevant deep learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1"&gt;Luka Murn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blanch_M/0/1/0/all/0/1"&gt;Marc Gorriz Blanch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Santamaria_M/0/1/0/all/0/1"&gt;Maria Santamaria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rivera_F/0/1/0/all/0/1"&gt;Fiona Rivera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1"&gt;Marta Mrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Survey on Community Detection with Deep Learning. (arXiv:2105.12584v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.12584</id>
        <link href="http://arxiv.org/abs/2105.12584"/>
        <updated>2021-05-27T01:32:29.327Z</updated>
        <summary type="html"><![CDATA[A community reveals the features and connections of its members that are
different from those in other communities in a network. Detecting communities
is of great significance in network analysis. Despite the classical spectral
clustering and statistical inference methods, we notice a significant
development of deep learning techniques for community detection in recent years
with their advantages in handling high dimensional network data. Hence, a
comprehensive overview of community detection's latest progress through deep
learning is timely to both academics and practitioners. This survey devises and
proposes a new taxonomy covering different categories of the state-of-the-art
methods, including deep learning-based models upon deep neural networks, deep
nonnegative matrix factorization and deep sparse filtering. The main category,
i.e., deep neural networks, is further divided into convolutional networks,
graph attention networks, generative adversarial networks and autoencoders. The
survey also summarizes the popular benchmark data sets, model evaluation
metrics, and open-source implementations to address experimentation settings.
We then discuss the practical applications of community detection in various
domains and point to implementation scenarios. Finally, we outline future
directions by suggesting challenging topics in this fast-growing deep learning
field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xing Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1"&gt;Shan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fanzhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wenbin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paris_C/0/1/0/all/0/1"&gt;Cecile Paris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1"&gt;Surya Nepal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1"&gt;Di Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1"&gt;Quan Z. Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Literature Review on Federated Machine Learning: From A Software Engineering Perspective. (arXiv:2007.11354v8 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11354</id>
        <link href="http://arxiv.org/abs/2007.11354"/>
        <updated>2021-05-27T01:32:29.322Z</updated>
        <summary type="html"><![CDATA[Federated learning is an emerging machine learning paradigm where clients
train models locally and formulate a global model based on the local model
updates. To identify the state-of-the-art in federated learning and explore how
to develop federated learning systems, we perform a systematic literature
review from a software engineering perspective, based on 231 primary studies.
Our data synthesis covers the lifecycle of federated learning system
development that includes background understanding, requirement analysis,
architecture design, implementation, and evaluation. We highlight and summarise
the findings from the results, and identify future trends to encourage
researchers to advance their current work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Sin Kit Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qinghua Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1"&gt;Hye-Young Paik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Liming Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Gaussian Processes on Discrete Domains. (arXiv:1810.10368v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.10368</id>
        <link href="http://arxiv.org/abs/1810.10368"/>
        <updated>2021-05-27T01:32:29.317Z</updated>
        <summary type="html"><![CDATA[Kernel methods on discrete domains have shown great promise for many
challenging data types, for instance, biological sequence data and molecular
structure data. Scalable kernel methods like Support Vector Machines may offer
good predictive performances but do not intrinsically provide uncertainty
estimates. In contrast, probabilistic kernel methods like Gaussian Processes
offer uncertainty estimates in addition to good predictive performance but fall
short in terms of scalability. While the scalability of Gaussian processes can
be improved using sparse inducing point approximations, the selection of these
inducing points remains challenging. We explore different techniques for
selecting inducing points on discrete domains, including greedy selection,
determinantal point processes, and simulated annealing. We find that simulated
annealing, which can select inducing points that are not in the training set,
can perform competitively with support vector machines and full Gaussian
processes on synthetic data, as well as on challenging real-world DNA sequence
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dresdner_G/0/1/0/all/0/1"&gt;Gideon Dresdner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Strathmann_H/0/1/0/all/0/1"&gt;Heiko Strathmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1"&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-rank matrix completion theory via Plucker coordinates. (arXiv:2004.12430v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.12430</id>
        <link href="http://arxiv.org/abs/2004.12430"/>
        <updated>2021-05-27T01:32:29.312Z</updated>
        <summary type="html"><![CDATA[Despite the popularity of low-rank matrix completion, the majority of its
theory has been developed under the assumption of random observation patterns,
whereas very little is known about the practically relevant case of non-random
patterns. Specifically, a fundamental yet largely open question is to describe
patterns that allow for unique or finitely many completions. This paper
provides two such families of patterns for any rank. A key to achieving this is
a novel formulation of low-rank matrix completion in terms of Plucker
coordinates, the latter a traditional tool in computer vision. This connection
is of potential significance to a wide family of matrix and subspace learning
problems with incomplete data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsakiris_M/0/1/0/all/0/1"&gt;Manolis C. Tsakiris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Value Functions. (arXiv:2105.12204v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2105.12204</id>
        <link href="http://arxiv.org/abs/2105.12204"/>
        <updated>2021-05-27T01:32:29.307Z</updated>
        <summary type="html"><![CDATA[The relationship between safety and optimality in control is not well
understood, and they are often seen as important yet conflicting objectives.
There is a pressing need to formalize this relationship, especially given the
growing prominence of learning-based methods. Indeed, it is common practice in
reinforcement learning to simply modify reward functions by penalizing
failures, with the penalty treated as a mere heuristic. We rigorously examine
this relationship, and formalize the requirements for safe value functions:
value functions that are both optimal for a given task, and enforce safety. We
reveal the structure of this relationship through a proof of strong duality,
showing that there always exists a finite penalty that induces a safe value
function. This penalty is not unique, but upper-unbounded: larger penalties do
not harm optimality. Although it is often not possible to compute the minimum
required penalty, we reveal clear structure of how the penalty, rewards,
discount factor, and dynamics interact. This insight suggests practical,
theory-guided heuristics to design reward functions for control problems where
safety is important.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Massiani_P/0/1/0/all/0/1"&gt;Pierre-Fran&amp;#xe7;ois Massiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heim_S/0/1/0/all/0/1"&gt;Steve Heim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Solowjow_F/0/1/0/all/0/1"&gt;Friedrich Solowjow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Trimpe_S/0/1/0/all/0/1"&gt;Sebastian Trimpe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Online Political Advertisements. (arXiv:2105.04047v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04047</id>
        <link href="http://arxiv.org/abs/2105.04047"/>
        <updated>2021-05-27T01:32:29.301Z</updated>
        <summary type="html"><![CDATA[Online political advertising is a central aspect of modern election
campaigning for influencing public opinion. Computational analysis of political
ads is of utmost importance in political science to understand the
characteristics of digital campaigning. It is also important in computational
linguistics to study features of political discourse and communication on a
large scale. In this work, we present the first computational study on online
political ads with the aim to (1) infer the political ideology of an ad
sponsor; and (2) identify whether the sponsor is an official political party or
a third-party organization. We develop two new large datasets for the two tasks
consisting of ads from the U.S.. Evaluation results show that our approach that
combines textual and visual information from pre-trained neural models
outperforms a state-of-the-art method for generic commercial ad classification.
Finally, we provide an in-depth analysis of the limitations of our
best-performing models and linguistic analysis to study the characteristics of
political ads discourse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_D/0/1/0/all/0/1"&gt;Danae S&amp;#xe1;nchez Villegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mokaram_S/0/1/0/all/0/1"&gt;Saeid Mokaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1"&gt;Nikolaos Aletras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CBANet: Towards Complexity and Bitrate Adaptive Deep Image Compression using a Single Network. (arXiv:2105.12386v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.12386</id>
        <link href="http://arxiv.org/abs/2105.12386"/>
        <updated>2021-05-27T01:32:29.296Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new deep image compression framework called
Complexity and Bitrate Adaptive Network (CBANet), which aims to learn one
single network to support variable bitrate coding under different computational
complexity constraints. In contrast to the existing state-of-the-art learning
based image compression frameworks that only consider the rate-distortion
trade-off without introducing any constraint related to the computational
complexity, our CBANet considers the trade-off between the rate and distortion
under dynamic computational complexity constraints. Specifically, to decode the
images with one single decoder under various computational complexity
constraints, we propose a new multi-branch complexity adaptive module, in which
each branch only takes a small portion of the computational budget of the
decoder. The reconstructed images with different visual qualities can be
readily generated by using different numbers of branches. Furthermore, to
achieve variable bitrate decoding with one single decoder, we propose a bitrate
adaptive module to project the representation from a base bitrate to the
expected representation at a target bitrate for transmission. Then it will
project the transmitted representation at the target bitrate back to that at
the base bitrate for the decoding process. The proposed bit adaptive module can
significantly reduce the storage requirement for deployment platforms. As a
result, our CBANet enables one single codec to support multiple bitrate
decoding under various computational complexity constraints. Comprehensive
experiments on two benchmark datasets demonstrate the effectiveness of our
CBANet for deep image compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jinyang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guo Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers. (arXiv:2105.12628v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12628</id>
        <link href="http://arxiv.org/abs/2105.12628"/>
        <updated>2021-05-27T01:32:29.291Z</updated>
        <summary type="html"><![CDATA[We propose Predict then Interpolate (PI), a simple algorithm for learning
correlations that are stable across environments. The algorithm follows from
the intuition that when using a classifier trained on one environment to make
predictions on examples from another environment, its mistakes are informative
as to which correlations are unstable. In this work, we prove that by
interpolating the distributions of the correct predictions and the wrong
predictions, we can uncover an oracle distribution where the unstable
correlation vanishes. Since the oracle interpolation coefficients are not
accessible, we use group distributionally robust optimization to minimize the
worst-case risk across all such interpolations. We evaluate our method on both
text classification and image classification. Empirical results demonstrate
that our algorithm is able to learn robust classifiers (outperforms IRM by
23.85% on synthetic environments and 12.41% on natural environments). Our code
and data are available at https://github.com/YujiaBao/Predict-then-Interpolate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yujia Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Contextual Deep Network Based Multimodal Pedestrian Detection For Autonomous Driving. (arXiv:2105.12713v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12713</id>
        <link href="http://arxiv.org/abs/2105.12713"/>
        <updated>2021-05-27T01:32:29.286Z</updated>
        <summary type="html"><![CDATA[Pedestrian Detection is the most critical module of an Autonomous Driving
system. Although a camera is commonly used for this purpose, its quality
degrades severely in low-light night time driving scenarios. On the other hand,
the quality of a thermal camera image remains unaffected in similar conditions.
This paper proposes an end-to-end multimodal fusion model for pedestrian
detection using RGB and thermal images. Its novel spatio-contextual deep
network architecture is capable of exploiting the multimodal input efficiently.
It consists of two distinct deformable ResNeXt-50 encoders for feature
extraction from the two modalities. Fusion of these two encoded features takes
place inside a multimodal feature embedding module (MuFEm) consisting of
several groups of a pair of Graph Attention Network and a feature fusion unit.
The output of the last feature fusion unit of MuFEm is subsequently passed to
two CRFs for their spatial refinement. Further enhancement of the features is
achieved by applying channel-wise attention and extraction of contextual
information with the help of four RNNs traversing in four different directions.
Finally, these feature maps are used by a single-stage decoder to generate the
bounding box of each pedestrian and the score map. We have performed extensive
experiments of the proposed framework on three publicly available multimodal
pedestrian detection benchmark datasets, namely KAIST, CVC-14, and UTokyo. The
results on each of them improved the respective state-of-the-art performance. A
short video giving an overview of this work along with its qualitative results
can be seen at https://youtu.be/FDJdSifuuCs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_K/0/1/0/all/0/1"&gt;Kinjal Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Arindam Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sudip Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Ujjwal Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Probabilistic Pruning: A general framework for hardware-constrained pruning at different granularities. (arXiv:2105.12686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12686</id>
        <link href="http://arxiv.org/abs/2105.12686"/>
        <updated>2021-05-27T01:32:29.267Z</updated>
        <summary type="html"><![CDATA[Unstructured neural network pruning algorithms have achieved impressive
compression rates. However, the resulting - typically irregular - sparse
matrices hamper efficient hardware implementations, leading to additional
memory usage and complex control logic that diminishes the benefits of
unstructured pruning. This has spurred structured coarse-grained pruning
solutions that prune entire filters or even layers, enabling efficient
implementation at the expense of reduced flexibility. Here we propose a
flexible new pruning mechanism that facilitates pruning at different
granularities (weights, kernels, filters/feature maps), while retaining
efficient memory organization (e.g. pruning exactly k-out-of-n weights for
every output neuron, or pruning exactly k-out-of-n kernels for every feature
map). We refer to this algorithm as Dynamic Probabilistic Pruning (DPP). DPP
leverages the Gumbel-softmax relaxation for differentiable k-out-of-n sampling,
facilitating end-to-end optimization. We show that DPP achieves competitive
compression rates and classification accuracy when pruning common deep learning
models trained on different benchmark datasets for image classification.
Relevantly, the non-magnitude-based nature of DPP allows for joint optimization
of pruning and weight quantization in order to even further compress the
network, which we show as well. Finally, we propose novel information theoretic
metrics that show the confidence and pruning diversity of pruning masks within
a layer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Carabarin_L/0/1/0/all/0/1"&gt;Lizeth Gonzalez-Carabarin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huijben_I/0/1/0/all/0/1"&gt;Iris A.M. Huijben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1"&gt;Bastiaan S. Veeling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_A/0/1/0/all/0/1"&gt;Alexandre Schmid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1"&gt;Ruud J.G. van Sloun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads from Aerial Imagery. (arXiv:2005.02264v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02264</id>
        <link href="http://arxiv.org/abs/2005.02264"/>
        <updated>2021-05-27T01:32:29.260Z</updated>
        <summary type="html"><![CDATA[Monitoring of land cover and land use is crucial in natural resources
management. Automatic visual mapping can carry enormous economic value for
agriculture, forestry, or public administration. Satellite or aerial images
combined with computer vision and deep learning enable precise assessment and
can significantly speed up change detection. Aerial imagery usually provides
images with much higher pixel resolution than satellite data allowing more
detailed mapping. However, there is still a lack of aerial datasets made for
the segmentation, covering rural areas with a resolution of tens centimeters
per pixel, manual fine labels, and highly publicly important environmental
instances like buildings, woods, water, or roads.

Here we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for
semantic segmentation. We collected images of 216.27 sq. km rural areas across
Poland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per
pixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine
annotated four following classes of objects: buildings, woodlands, water, and
roads. Additionally, we report simple benchmark results, achieving 85.56% of
mean intersection over union on the test set. It proves that the automatic
mapping of land cover is possible with a relatively small, cost-efficient,
RGB-only dataset. The dataset is publicly available at https://landcover.ai]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boguszewski_A/0/1/0/all/0/1"&gt;Adrian Boguszewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batorski_D/0/1/0/all/0/1"&gt;Dominik Batorski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziemba_Jankowska_N/0/1/0/all/0/1"&gt;Natalia Ziemba-Jankowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dziedzic_T/0/1/0/all/0/1"&gt;Tomasz Dziedzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zambrzycka_A/0/1/0/all/0/1"&gt;Anna Zambrzycka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[mvlearn: Multiview Machine Learning in Python. (arXiv:2005.11890v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.11890</id>
        <link href="http://arxiv.org/abs/2005.11890"/>
        <updated>2021-05-27T01:32:29.238Z</updated>
        <summary type="html"><![CDATA[As data are generated more and more from multiple disparate sources,
multiview data sets, where each sample has features in distinct views, have
ballooned in recent years. However, no comprehensive package exists that
enables non-specialists to use these methods easily. mvlearn is a Python
library which implements the leading multiview machine learning methods. Its
simple API closely follows that of scikit-learn for increased ease-of-use. The
package can be installed from Python Package Index (PyPI) and the conda package
manager and is released under the MIT open-source license. The documentation,
detailed examples, and all releases are available at
https://mvlearn.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Perry_R/0/1/0/all/0/1"&gt;Ronan Perry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mischler_G/0/1/0/all/0/1"&gt;Gavin Mischler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Guo_R/0/1/0/all/0/1"&gt;Richard Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_T/0/1/0/all/0/1"&gt;Theodore Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chang_A/0/1/0/all/0/1"&gt;Alexander Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Koul_A/0/1/0/all/0/1"&gt;Arman Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Franz_C/0/1/0/all/0/1"&gt;Cameron Franz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Richard_H/0/1/0/all/0/1"&gt;Hugo Richard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carmichael_I/0/1/0/all/0/1"&gt;Iain Carmichael&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1"&gt;Pierre Ablin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gramfort_A/0/1/0/all/0/1"&gt;Alexandre Gramfort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1"&gt;Joshua T. Vogelstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Properties of Deep Residual Networks. (arXiv:2105.12245v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12245</id>
        <link href="http://arxiv.org/abs/2105.12245"/>
        <updated>2021-05-27T01:32:29.231Z</updated>
        <summary type="html"><![CDATA[Residual networks (ResNets) have displayed impressive results in pattern
recognition and, recently, have garnered considerable theoretical interest due
to a perceived link with neural ordinary differential equations (neural ODEs).
This link relies on the convergence of network weights to a smooth function as
the number of layers increases. We investigate the properties of weights
trained by stochastic gradient descent and their scaling with network depth
through detailed numerical experiments. We observe the existence of scaling
regimes markedly different from those assumed in neural ODE literature.
Depending on certain features of the network architecture, such as the
smoothness of the activation function, one may obtain an alternative ODE limit,
a stochastic differential equation or neither of these. These findings cast
doubts on the validity of the neural ODE model as an adequate asymptotic
description of deep ResNets and point to an alternative class of differential
equations as a better description of the deep network limit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1"&gt;Alain-Sam Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cont_R/0/1/0/all/0/1"&gt;Rama Cont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossier_A/0/1/0/all/0/1"&gt;Alain Rossier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renyuan Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face Image Quality Assessment: A Literature Survey. (arXiv:2009.01103v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.01103</id>
        <link href="http://arxiv.org/abs/2009.01103"/>
        <updated>2021-05-27T01:32:29.221Z</updated>
        <summary type="html"><![CDATA[The performance of face analysis and recognition systems depends on the
quality of the acquired face data, which is influenced by numerous factors.
Automatically assessing the quality of face data in terms of biometric utility
can thus be useful to detect low-quality data and make decisions accordingly.
This survey provides an overview of the face image quality assessment
literature, which predominantly focuses on single visible wavelength face image
input. A trend towards deep learning based methods is observed, including
notable conceptual differences among the recent approaches, such as the
integration of quality assessment into face recognition models. Besides image
selection, face image quality assessment can also be used in a variety of other
application scenarios, which are discussed herein. Open issues and challenges
are pointed out, i.a. highlighting the importance of comparability for
algorithm evaluations, and the challenge for future work to create deep
learning approaches that are interpretable in addition to providing accurate
utility predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schlett_T/0/1/0/all/0/1"&gt;Torsten Schlett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1"&gt;Christian Rathgeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henniger_O/0/1/0/all/0/1"&gt;Olaf Henniger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1"&gt;Javier Galbally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Julian Fierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1"&gt;Christoph Busch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards an IMU-based Pen Online Handwriting Recognizer. (arXiv:2105.12434v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12434</id>
        <link href="http://arxiv.org/abs/2105.12434"/>
        <updated>2021-05-27T01:32:29.208Z</updated>
        <summary type="html"><![CDATA[Most online handwriting recognition systems require the use of specific
writing surfaces to extract positional data. In this paper we present a online
handwriting recognition system for word recognition which is based on inertial
measurement units (IMUs) for digitizing text written on paper. This is obtained
by means of a sensor-equipped pen that provides acceleration, angular velocity,
and magnetic forces streamed via Bluetooth. Our model combines convolutional
and bidirectional LSTM networks, and is trained with the Connectionist Temporal
Classification loss that allows the interpretation of raw sensor data into
words without the need of sequence segmentation. We use a dataset of words
collected using multiple sensor-enhanced pens and evaluate our model on
distinct test sets of seen and unseen words achieving a character error rate of
17.97% and 17.08%, respectively, without the use of a dictionary or language
model]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1"&gt;Mohamad Wehbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamann_T/0/1/0/all/0/1"&gt;Tim Hamann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barth_J/0/1/0/all/0/1"&gt;Jens Barth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaempf_P/0/1/0/all/0/1"&gt;Peter Kaempf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1"&gt;Dario Zanca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1"&gt;Bjoern Eskofier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial robustness against multiple $l_p$-threat models at the price of one and how to quickly fine-tune robust models to another threat model. (arXiv:2105.12508v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12508</id>
        <link href="http://arxiv.org/abs/2105.12508"/>
        <updated>2021-05-27T01:32:29.194Z</updated>
        <summary type="html"><![CDATA[Adversarial training (AT) in order to achieve adversarial robustness wrt
single $l_p$-threat models has been discussed extensively. However, for
safety-critical systems adversarial robustness should be achieved wrt all
$l_p$-threat models simultaneously. In this paper we develop a simple and
efficient training scheme to achieve adversarial robustness against the union
of $l_p$-threat models. Our novel $l_1+l_\infty$-AT scheme is based on
geometric considerations of the different $l_p$-balls and costs as much as
normal adversarial training against a single $l_p$-threat model. Moreover, we
show that using our $l_1+l_\infty$-AT scheme one can fine-tune with just 3
epochs any $l_p$-robust model (for $p \in \{1,2,\infty\}$) and achieve multiple
norm adversarial robustness. In this way we boost the previous state-of-the-art
reported for multiple-norm robustness by more than $6\%$ on CIFAR-10 and report
up to our knowledge the first ImageNet models with multiple norm robustness.
Moreover, we study the general transfer of adversarial robustness between
different threat models and in this way boost the previous SOTA
$l_1$-robustness on CIFAR-10 by almost $10\%$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1"&gt;Francesco Croce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FEDS -- Filtered Edit Distance Surrogate. (arXiv:2103.04635v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04635</id>
        <link href="http://arxiv.org/abs/2103.04635"/>
        <updated>2021-05-27T01:32:29.188Z</updated>
        <summary type="html"><![CDATA[This paper proposes a procedure to train a scene text recognition model using
a robust learned surrogate of edit distance. The proposed method borrows from
self-paced learning and filters out the training examples that are hard for the
surrogate. The filtering is performed by judging the quality of the
approximation, using a ramp function, enabling end-to-end training. Following
the literature, the experiments are conducted in a post-tuning setup, where a
trained scene text recognition model is tuned using the learned surrogate of
edit distance. The efficacy is demonstrated by improvements on various
challenging scene text datasets such as IIIT-5K, SVT, ICDAR, SVTP, and CUTE.
The proposed method provides an average improvement of $11.2 \%$ on total edit
distance and an error reduction of $9.5\%$ on accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_Y/0/1/0/all/0/1"&gt;Yash Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Jiri Matas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Style Similarity as Feedback for Product Design. (arXiv:2105.12256v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12256</id>
        <link href="http://arxiv.org/abs/2105.12256"/>
        <updated>2021-05-27T01:32:29.172Z</updated>
        <summary type="html"><![CDATA[Matching and recommending products is beneficial for both customers and
companies. With the rapid increase in home goods e-commerce, there is an
increasing demand for quantitative methods for providing such recommendations
for millions of products. This approach is facilitated largely by online stores
such as Amazon and Wayfair, in which the goal is to maximize overall sales.
Instead of focusing on overall sales, we take a product design perspective, by
employing big-data analysis for determining the design qualities of a highly
recommended product. Specifically, we focus on the visual style compatibility
of such products. We build off previous work which implemented a style-based
similarity metric for thousands of furniture products. Using analysis and
visualization, we extract attributes of furniture products that are highly
compatible style-wise. We propose a designer in-the-loop workflow that mirrors
methods of displaying similar products to consumers browsing e-commerce
websites. Our findings are useful when designing new products, since they
provide insight regarding what furniture will be strongly compatible across
multiple styles, and hence, more likely to be recommended.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_M/0/1/0/all/0/1"&gt;Mathew Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_T/0/1/0/all/0/1"&gt;Tomer Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ataer_Cansizoglu_E/0/1/0/all/0/1"&gt;Esra Ataer-Cansizoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jae-Woo Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using the Overlapping Score to Improve Corruption Benchmarks. (arXiv:2105.12357v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12357</id>
        <link href="http://arxiv.org/abs/2105.12357"/>
        <updated>2021-05-27T01:32:29.167Z</updated>
        <summary type="html"><![CDATA[Neural Networks are sensitive to various corruptions that usually occur in
real-world applications such as blurs, noises, low-lighting conditions, etc. To
estimate the robustness of neural networks to these common corruptions, we
generally use a group of modeled corruptions gathered into a benchmark.
Unfortunately, no objective criterion exists to determine whether a benchmark
is representative of a large diversity of independent corruptions. In this
paper, we propose a metric called corruption overlapping score, which can be
used to reveal flaws in corruption benchmarks. Two corruptions overlap when the
robustnesses of neural networks to these corruptions are correlated. We argue
that taking into account overlappings between corruptions can help to improve
existing benchmarks or build better ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laugros_A/0/1/0/all/0/1"&gt;Alfred Laugros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caplier_A/0/1/0/all/0/1"&gt;Alice Caplier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ospici_M/0/1/0/all/0/1"&gt;Matthieu Ospici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Robustness of Machine Reading Comprehension Models. (arXiv:2004.14004v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.14004</id>
        <link href="http://arxiv.org/abs/2004.14004"/>
        <updated>2021-05-27T01:32:29.161Z</updated>
        <summary type="html"><![CDATA[Machine Reading Comprehension (MRC) is an important testbed for evaluating
models' natural language understanding (NLU) ability. There has been rapid
progress in this area, with new models achieving impressive performance on
various benchmarks. However, existing benchmarks only evaluate models on
in-domain test sets without considering their robustness under test-time
perturbations or adversarial attacks. To fill this important gap, we construct
AdvRACE (Adversarial RACE), a new model-agnostic benchmark for evaluating the
robustness of MRC models under four different types of adversarial attacks,
including our novel distractor extraction and generation attacks. We show that
state-of-the-art (SOTA) models are vulnerable to all of these attacks. We
conclude that there is substantial room for building more robust MRC models and
our benchmark can help motivate and measure progress in this area. We release
our data and code at https://github.com/NoviScl/AdvRACE .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Chenglei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziqing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yiming Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wentao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shijin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning. (arXiv:2012.15022v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15022</id>
        <link href="http://arxiv.org/abs/2012.15022"/>
        <updated>2021-05-27T01:32:29.156Z</updated>
        <summary type="html"><![CDATA[Pre-trained Language Models (PLMs) have shown superior performance on various
downstream Natural Language Processing (NLP) tasks. However, conventional
pre-training objectives do not explicitly model relational facts in text, which
are crucial for textual understanding. To address this issue, we propose a
novel contrastive learning framework ERICA to obtain a deep understanding of
the entities and their relations in text. Specifically, we define two novel
pre-training tasks to better understand entities and relations: (1) the entity
discrimination task to distinguish which tail entity can be inferred by the
given head entity and relation; (2) the relation discrimination task to
distinguish whether two relations are close or not semantically, which involves
complex relational reasoning. Experimental results demonstrate that ERICA can
improve typical PLMs (BERT and RoBERTa) on several language understanding
tasks, including relation extraction, entity typing and question answering,
especially under low-resource settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yujia Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yankai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1"&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Occlusion Aware Kernel Correlation Filter Tracker using RGB-D. (arXiv:2105.12161v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12161</id>
        <link href="http://arxiv.org/abs/2105.12161"/>
        <updated>2021-05-27T01:32:29.151Z</updated>
        <summary type="html"><![CDATA[Unlike deep learning which requires large training datasets, correlation
filter-based trackers like Kernelized Correlation Filter (KCF) uses implicit
properties of tracked images (circulant matrices) for training in real-time.
Despite their practical application in tracking, a need for a better
understanding of the fundamentals associated with KCF in terms of
theoretically, mathematically, and experimentally exists. This thesis first
details the workings prototype of the tracker and investigates its
effectiveness in real-time applications and supporting visualizations. We
further address some of the drawbacks of the tracker in cases of occlusions,
scale changes, object rotation, out-of-view and model drift with our novel
RGB-D Kernel Correlation tracker. We also study the use of particle filters to
improve trackers' accuracy. Our results are experimentally evaluated using a)
standard dataset and b) real-time using the Microsoft Kinect V2 sensor. We
believe this work will set the basis for a better understanding of the
effectiveness of kernel-based correlation filter trackers and to further define
some of its possible advantages in tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1"&gt;Srishti Yadav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blurs Make Results Clearer: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness. (arXiv:2105.12639v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12639</id>
        <link href="http://arxiv.org/abs/2105.12639"/>
        <updated>2021-05-27T01:32:29.136Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks (BNNs) have shown success in the areas of
uncertainty estimation and robustness. However, a crucial challenge prohibits
their use in practice: Bayesian NNs require a large number of predictions to
produce reliable results, leading to a significant increase in computational
cost. To alleviate this issue, we propose spatial smoothing, a method that
ensembles neighboring feature map points of CNNs. By simply adding a few blur
layers to the models, we empirically show that the spatial smoothing improves
accuracy, uncertainty estimation, and robustness of BNNs across a whole range
of ensemble sizes. In particular, BNNs incorporating the spatial smoothing
achieve high predictive performance merely with a handful of ensembles.
Moreover, this method also can be applied to canonical deterministic neural
networks to improve the performances. A number of evidences suggest that the
improvements can be attributed to the smoothing and flattening of the loss
landscape. In addition, we provide a fundamental explanation for prior works -
namely, global average pooling, pre-activation, and ReLU6 - by addressing to
them as special cases of the spatial smoothing. These not only enhance
accuracy, but also improve uncertainty estimation and robustness by making the
loss landscape smoother in the same manner as the spatial smoothing. The code
is available at https://github.com/xxxnell/spatial-smoothing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1"&gt;Namuk Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Songkuk Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Operator Autoencoders: Learning Physical Operations on Encoded Molecular Graphs. (arXiv:2105.12295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12295</id>
        <link href="http://arxiv.org/abs/2105.12295"/>
        <updated>2021-05-27T01:32:29.131Z</updated>
        <summary type="html"><![CDATA[Molecular dynamics simulations produce data with complex nonlinear dynamics.
If the timestep behavior of such a dynamic system can be represented by a
linear operator, future states can be inferred directly without expensive
simulations. The use of an autoencoder in combination with a physical timestep
operator allows both the relevant structural characteristics of the molecular
graphs and the underlying physics of the system to be isolated during the
training process. In this work, we develop a pipeline for establishing
graph-structured representations of time-series volumetric data from molecular
dynamics simulations. We then train an autoencoder to find nonlinear mappings
to a latent space where future timesteps can be predicted through application
of a linear operator trained in tandem with the autoencoder. Increasing the
dimensionality of the autoencoder output is shown to improve the accuracy of
the physical timestep operator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoke_W/0/1/0/all/0/1"&gt;Willis Hoke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shea_D/0/1/0/all/0/1"&gt;Daniel Shea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casey_S/0/1/0/all/0/1"&gt;Stephen Casey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting invasive ductal carcinoma using a Reinforcement Sample Learning Strategy using Deep Learning. (arXiv:2105.12564v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12564</id>
        <link href="http://arxiv.org/abs/2105.12564"/>
        <updated>2021-05-27T01:32:29.127Z</updated>
        <summary type="html"><![CDATA[Invasive ductal carcinoma is a prevalent, potentially deadly disease
associated with a high rate of morbidity and mortality. Its malignancy is the
second leading cause of death from cancer in women. The mammogram is an
extremely useful resource for mass detection and invasive ductal carcinoma
diagnosis. We are proposing a method for Invasive ductal carcinoma that will
use convolutional neural networks (CNN) on mammograms to assist radiologists in
diagnosing the disease. Due to the varying image clarity and structure of
certain mammograms, it is difficult to observe major cancer characteristics
such as microcalcification and mass, and it is often difficult to interpret and
diagnose these attributes. The aim of this study is to establish a novel method
for fully automated feature extraction and classification in invasive ductal
carcinoma computer-aided diagnosis (CAD) systems. This article presents a tumor
classification algorithm that makes novel use of convolutional neural networks
on breast mammogram images to increase feature extraction and training speed.
The algorithm makes two contributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1"&gt;Rushabh Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curiosity Killed or Incapacitated the Cat and the Asymptotically Optimal Agent. (arXiv:2006.03357v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03357</id>
        <link href="http://arxiv.org/abs/2006.03357"/>
        <updated>2021-05-27T01:32:29.122Z</updated>
        <summary type="html"><![CDATA[Reinforcement learners are agents that learn to pick actions that lead to
high reward. Ideally, the value of a reinforcement learner's policy approaches
optimality--where the optimal informed policy is the one which maximizes
reward. Unfortunately, we show that if an agent is guaranteed to be
"asymptotically optimal" in any (stochastically computable) environment, then
subject to an assumption about the true environment, this agent will be either
"destroyed" or "incapacitated" with probability 1. Much work in reinforcement
learning uses an ergodicity assumption to avoid this problem. Often, doing
theoretical research under simplifying assumptions prepares us to provide
practical solutions even in the absence of those assumptions, but the
ergodicity assumption in reinforcement learning may have led us entirely astray
in preparing safe and effective exploration strategies for agents in dangerous
environments. Rather than assuming away the problem, we present an agent,
Mentee, with the modest guarantee of approaching the performance of a mentor,
doing safe exploration instead of reckless exploration. Critically, Mentee's
exploration probability depends on the expected information gain from
exploring. In a simple non-ergodic environment with a weak mentor, we find
Mentee outperforms existing asymptotically optimal agents and its mentor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_M/0/1/0/all/0/1"&gt;Michael K. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catt_E/0/1/0/all/0/1"&gt;Elliot Catt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1"&gt;Marcus Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing ECG Monitoring Healthcare System with Federated Transfer Learning and Explainable AI. (arXiv:2105.12497v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12497</id>
        <link href="http://arxiv.org/abs/2105.12497"/>
        <updated>2021-05-27T01:32:29.117Z</updated>
        <summary type="html"><![CDATA[Deep learning play a vital role in classifying different arrhythmias using
the electrocardiography (ECG) data. Nevertheless, training deep learning models
normally requires a large amount of data and it can lead to privacy concerns.
Unfortunately, a large amount of healthcare data cannot be easily collected
from a single silo. Additionally, deep learning models are like black-box, with
no explainability of the predicted results, which is often required in clinical
healthcare. This limits the application of deep learning in real-world health
systems. In this paper, we design a new explainable artificial intelligence
(XAI) based deep learning framework in a federated setting for ECG-based
healthcare applications. The federated setting is used to solve issues such as
data availability and privacy concerns. Furthermore, the proposed framework
setting effectively classifies arrhythmia's using an autoencoder and a
classifier, both based on a convolutional neural network (CNN). Additionally,
we propose an XAI-based module on top of the proposed classifier to explain the
classification results, which help clinical practitioners make quick and
reliable decisions. The proposed framework was trained and tested using the
MIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98%
for arrhythmia detection using noisy and clean data, respectively, with
five-fold cross-validation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1"&gt;Ali Raza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1"&gt;Kim Phuc Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehl_L/0/1/0/all/0/1"&gt;Ludovic Koehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shujun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Repulsive Prototypes for Adversarial Robustness. (arXiv:2105.12427v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12427</id>
        <link href="http://arxiv.org/abs/2105.12427"/>
        <updated>2021-05-27T01:32:29.111Z</updated>
        <summary type="html"><![CDATA[While many defences against adversarial examples have been proposed, finding
robust machine learning models is still an open problem. The most compelling
defence to date is adversarial training and consists of complementing the
training data set with adversarial examples. Yet adversarial training severely
impacts training time and depends on finding representative adversarial
samples. In this paper we propose to train models on output spaces with large
class separation in order to gain robustness without adversarial training. We
introduce a method to partition the output space into class prototypes with
large separation and train models to preserve it. Experimental results shows
that models trained with these prototypes -- which we call deep repulsive
prototypes -- gain robustness competitive with adversarial training, while also
preserving more accuracy on natural samples. Moreover, the models are more
resilient to large perturbation sizes. For example, we obtained over 50%
robustness for CIFAR-10, with 92% accuracy on natural samples and over 20%
robustness for CIFAR-100, with 71% accuracy on natural samples without
adversarial training. For both data sets, the models preserved robustness
against large perturbations better than adversarially trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Serban_A/0/1/0/all/0/1"&gt;Alex Serban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poll_E/0/1/0/all/0/1"&gt;Erik Poll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Visser_J/0/1/0/all/0/1"&gt;Joost Visser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-Sample Analysis of Off-Policy Natural Actor-Critic with Linear Function Approximation. (arXiv:2105.12540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12540</id>
        <link href="http://arxiv.org/abs/2105.12540"/>
        <updated>2021-05-27T01:32:29.096Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop a novel variant of off-policy natural actor-critic
algorithm with linear function approximation and we establish a sample
complexity of $\mathcal{O}(\epsilon^{-3})$, outperforming all the previously
known convergence bounds of such algorithms. In order to overcome the
divergence due to deadly triad in off-policy policy evaluation under function
approximation, we develop a critic that employs $n$-step TD-learning algorithm
with a properly chosen $n$. We present finite-sample convergence bounds on this
critic under both constant and diminishing step sizes, which are of independent
interest. Furthermore, we develop a variant of natural policy gradient under
function approximation, with an improved convergence rate of $\mathcal{O}(1/T)$
after $T$ iterations. Combining the finite sample error bounds of actor and the
critic, we obtain the $\mathcal{O}(\epsilon^{-3})$ sample complexity. We derive
our sample complexity bounds solely based on the assumption that the behavior
policy sufficiently explores all the states and actions, which is a much
lighter assumption compared to the related literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zaiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khodadadian_S/0/1/0/all/0/1"&gt;Sajad Khodadadian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1"&gt;Siva Theja Maguluri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Equivalence between Bayesian Priors and Penalties in Variational Inference. (arXiv:2002.00178v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00178</id>
        <link href="http://arxiv.org/abs/2002.00178"/>
        <updated>2021-05-27T01:32:29.091Z</updated>
        <summary type="html"><![CDATA[In machine learning, it is common to optimize the parameters of a
probabilistic model, modulated by an ad hoc regularization term that penalizes
some values of the parameters. Regularization terms appear naturally in
Variational Inference (VI), a tractable way to approximate Bayesian posteriors:
the loss to optimize contains a Kullback--Leibler divergence term between the
approximate posterior and a Bayesian prior. We fully characterize which
regularizers can arise this way, and provide a systematic way to compute the
corresponding prior. This viewpoint also provides a prediction for useful
values of the regularization factor in neural networks. We apply this framework
to regularizers such as L2, L1 or group-Lasso.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolinski_P/0/1/0/all/0/1"&gt;Pierre Wolinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charpiat_G/0/1/0/all/0/1"&gt;Guillaume Charpiat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ollivier_Y/0/1/0/all/0/1"&gt;Yann Ollivier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Precision Hardware Architectures Meet Recommendation Model Inference at Scale. (arXiv:2105.12676v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12676</id>
        <link href="http://arxiv.org/abs/2105.12676"/>
        <updated>2021-05-27T01:32:29.085Z</updated>
        <summary type="html"><![CDATA[Tremendous success of machine learning (ML) and the unabated growth in ML
model complexity motivated many ML-specific designs in both CPU and accelerator
architectures to speed up the model inference. While these architectures are
diverse, highly optimized low-precision arithmetic is a component shared by
most. Impressive compute throughputs are indeed often exhibited by these
architectures on benchmark ML models. Nevertheless, production models such as
recommendation systems important to Facebook's personalization services are
demanding and complex: These systems must serve billions of users per month
responsively with low latency while maintaining high prediction accuracy,
notwithstanding computations with many tens of billions parameters per
inference. Do these low-precision architectures work well with our production
recommendation systems? They do. But not without significant effort. We share
in this paper our search strategies to adapt reference recommendation models to
low-precision hardware, our optimization of low-precision compute kernels, and
the design and development of tool chain so as to maintain our models' accuracy
throughout their lifespan during which topic trends and users' interests
inevitably evolve. Practicing these low-precision technologies helped us save
datacenter capacities while deploying models with up to 5X complexity that
would otherwise not be deployed on traditional general-purpose CPUs. We believe
these lessons from the trenches promote better co-design between hardware
architecture and software engineering and advance the state of the art of ML in
industry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhaoxia/0/1/0/all/0/1"&gt;Zhaoxia&lt;/a&gt; (Summer) &lt;a href="http://arxiv.org/find/cs/1/au:+Deng/0/1/0/all/0/1"&gt;Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jongsoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1"&gt;Ping Tak Peter Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haixin Liu&lt;/a&gt;, Jie (Amy) &lt;a href="http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1"&gt;Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuen_H/0/1/0/all/0/1"&gt;Hector Yuen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khudia_D/0/1/0/all/0/1"&gt;Daya Khudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiaohan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1"&gt;Ellie Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_D/0/1/0/all/0/1"&gt;Dhruv Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1"&gt;Raghuraman Krishnamoorthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Carole-Jean Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadathur_S/0/1/0/all/0/1"&gt;Satish Nadathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Changkyu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumov_M/0/1/0/all/0/1"&gt;Maxim Naumov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naghshineh_S/0/1/0/all/0/1"&gt;Sam Naghshineh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smelyanskiy_M/0/1/0/all/0/1"&gt;Mikhail Smelyanskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collective Learning. (arXiv:1912.02580v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.02580</id>
        <link href="http://arxiv.org/abs/1912.02580"/>
        <updated>2021-05-27T01:32:29.078Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce the concept of collective learning (CL) which
exploits the notion of collective intelligence in the field of distributed
semi-supervised learning. The proposed framework draws inspiration from the
learning behavior of human beings, who alternate phases involving
collaboration, confrontation and exchange of views with other consisting of
studying and learning on their own. On this regard, CL comprises two main
phases: a self-training phase in which learning is performed on local private
(labeled) data only and a collective training phase in which proxy-labels are
assigned to shared (unlabeled) data by means of a consensus-based algorithm. In
the considered framework, heterogeneous systems can be connected over the same
network, each with different computational capabilities and resources and
everyone in the network may take advantage of the cooperation and will
eventually reach higher performance with respect to those it can reach on its
own. An extensive experimental campaign on an image classification problem
emphasizes the properties of CL by analyzing the performance achieved by the
cooperating agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1"&gt;Francesco Farina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Part Segmentation through Disentangling Appearance and Shape. (arXiv:2105.12405v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12405</id>
        <link href="http://arxiv.org/abs/2105.12405"/>
        <updated>2021-05-27T01:32:29.073Z</updated>
        <summary type="html"><![CDATA[We study the problem of unsupervised discovery and segmentation of object
parts, which, as an intermediate local representation, are capable of finding
intrinsic object structure and providing more explainable recognition results.
Recent unsupervised methods have greatly relaxed the dependency on annotated
data which are costly to obtain, but still rely on additional information such
as object segmentation mask or saliency map. To remove such a dependency and
further improve the part segmentation performance, we develop a novel approach
by disentangling the appearance and shape representations of object parts
followed with reconstruction losses without using additional object mask
information. To avoid degenerated solutions, a bottleneck block is designed to
squeeze and expand the appearance representation, leading to a more effective
disentanglement between geometry and appearance. Combined with a
self-supervised part classification loss and an improved geometry concentration
constraint, we can segment more consistent parts with semantic meanings.
Comprehensive experiments on a wide variety of objects such as face, bird, and
PASCAL VOC objects demonstrate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shilong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Standard Development Activities on Video Coding for Machines. (arXiv:2105.12653v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12653</id>
        <link href="http://arxiv.org/abs/2105.12653"/>
        <updated>2021-05-27T01:32:29.055Z</updated>
        <summary type="html"><![CDATA[In recent years, video data has dominated internet traffic and becomes one of
the major data formats. With the emerging 5G and internet of things (IoT)
technologies, more and more videos are generated by edge devices, sent across
networks, and consumed by machines. The volume of video consumed by machine is
exceeding the volume of video consumed by humans. Machine vision tasks include
object detection, segmentation, tracking, and other machine-based applications,
which are quite different from those for human consumption. On the other hand,
due to large volumes of video data, it is essential to compress video before
transmission. Thus, efficient video coding for machines (VCM) has become an
important topic in academia and industry. In July 2019, the international
standardization organization, i.e., MPEG, created an Ad-Hoc group named VCM to
study the requirements for potential standardization work. In this paper, we
will address the recent development activities in the MPEG VCM group.
Specifically, we will first provide an overview of the MPEG VCM group including
use cases, requirements, processing pipelines, plan for potential VCM
standards, followed by the evaluation framework including machine-vision tasks,
dataset, evaluation metrics, and anchor generation. We then introduce
technology solutions proposed so far and discuss the recent responses to the
Call for Evidence issued by MPEG VCM group.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaozhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafie_M/0/1/0/all/0/1"&gt;Manouchehr Rafie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Curcio_I/0/1/0/all/0/1"&gt;Igor Curcio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Parameters of Structural Causal Models. (arXiv:2105.12697v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12697</id>
        <link href="http://arxiv.org/abs/2105.12697"/>
        <updated>2021-05-27T01:32:29.044Z</updated>
        <summary type="html"><![CDATA[In recent years there has been a lot of focus on adversarial attacks,
especially on deep neural networks. Here, we argue that they are more general
in nature and can easily affect a larger class of models, e.g., any
differentiable perturbed optimizers. We further show that such attacks can be
determined by the hidden confounders in a domain, thus drawing a novel
connection between such attacks and causality. Establishing this causal
perspective is characterized by the influence of the structural causal model's
data generating process on the subsequent optimization thereby exhibiting
intriguing parameters of the former. We reveal the existence of such parameters
for three combinatorial optimization problems, namely linear assignment,
shortest path and a real world problem of energy systems. Our empirical
examination also unveils worrisome consequences of these attacks on
differentiable perturbed optimizers thereby highlighting the criticality of our
findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1"&gt;Matej Ze&amp;#x10d;evi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1"&gt;Devendra Singh Dhami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Optimal Transport Embedding: Provable Wasserstein classification for certain rigid transformations and perturbations. (arXiv:2008.09165v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09165</id>
        <link href="http://arxiv.org/abs/2008.09165"/>
        <updated>2021-05-27T01:32:29.037Z</updated>
        <summary type="html"><![CDATA[Discriminating between distributions is an important problem in a number of
scientific fields. This motivated the introduction of Linear Optimal
Transportation (LOT), which embeds the space of distributions into an
$L^2$-space. The transform is defined by computing the optimal transport of
each distribution to a fixed reference distribution, and has a number of
benefits when it comes to speed of computation and to determining
classification boundaries. In this paper, we characterize a number of settings
in which LOT embeds families of distributions into a space in which they are
linearly separable. This is true in arbitrary dimension, and for families of
distributions generated through perturbations of shifts and scalings of a fixed
distribution.We also prove conditions under which the $L^2$ distance of the LOT
embedding between two distributions in arbitrary dimension is nearly isometric
to Wasserstein-2 distance between those distributions. This is of significant
computational benefit, as one must only compute $N$ optimal transport maps to
define the $N^2$ pairwise distances between $N$ distributions. We demonstrate
the benefits of LOT on a number of distribution classification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Moosmuller_C/0/1/0/all/0/1"&gt;Caroline Moosm&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cloninger_A/0/1/0/all/0/1"&gt;Alexander Cloninger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[There and Back Again: Unraveling the Variational Auto-Encoder. (arXiv:1912.10309v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.10309</id>
        <link href="http://arxiv.org/abs/1912.10309"/>
        <updated>2021-05-27T01:32:29.031Z</updated>
        <summary type="html"><![CDATA[We prove that the evidence lower bound (ELBO) employed by variational
auto-encoders (VAEs) admits non-trivial solutions having constant posterior
variances under certain mild conditions, removing the need to learn variances
in the encoder. The proof follows from an unexpected journey through an array
of topics: the closed form optimal decoder for Gaussian VAEs, a proof that the
decoder is always smooth, a proof that the ELBO at its stationary points is
equal to the exact log evidence, and the posterior variance is merely part of a
stochastic estimator of the decoder Hessian. The penalty incurred from using a
constant posterior variance is small under mild conditions, and otherwise
discourages large variations in the decoder Hessian. From here we derive a
simplified formulation of the ELBO as an expectation over a batch, which we
call the Batch Information Lower Bound (BILBO). Despite the use of Gaussians,
our analysis is broadly applicable -- it extends to any likelihood function
that induces a Riemannian metric. Regarding learned likelihoods, we show that
the ELBO is optimal in the limit as the likelihood variances approach zero,
where it is equivalent to the change of variables formulation employed in
normalizing flow networks. Standard optimization procedures are unstable in
this limit, so we propose a bounded Gaussian likelihood that is invariant to
the scale of the data using a measure of the aggregate information in a batch,
which we call Bounded Aggregate Information Sampling (BAGGINS). Combining the
two formulations, we construct VAE networks with only half the outputs of
ordinary VAEs (no learned variances), yielding improved ELBO scores and scale
invariance in experiments. As we perform our analyses irrespective of any
particular network architecture, our reformulations may apply to any VAE
implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fyffe_G/0/1/0/all/0/1"&gt;Graham Fyffe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. (arXiv:2105.12485v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12485</id>
        <link href="http://arxiv.org/abs/2105.12485"/>
        <updated>2021-05-27T01:32:29.025Z</updated>
        <summary type="html"><![CDATA[Source code can be parsed into the abstract syntax tree (AST) based on
defined syntax rules. However, in pre-training, little work has considered the
incorporation of tree structure into the learning process. In this paper, we
present TreeBERT, a tree-based pre-trained model for improving programming
language-oriented generation tasks. To utilize tree structure, TreeBERT
represents the AST corresponding to the code as a set of composition paths and
introduces node position embedding. The model is trained by tree masked
language modeling (TMLM) and node order prediction (NOP) with a hybrid
objective. TMLM uses a novel masking strategy designed according to the tree's
characteristics to help the model understand the AST and infer the missing
semantics of the AST. With NOP, TreeBERT extracts the syntactical structure by
learning the order constraints of nodes in AST. We pre-trained TreeBERT on
datasets covering multiple programming languages. On code summarization and
code documentation tasks, TreeBERT outperforms other pre-trained models and
state-of-the-art models designed for these tasks. Furthermore, TreeBERT
performs well when transferred to the pre-trained unseen programming language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xue Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhuoran Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1"&gt;Chen Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lei Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating dynamicity of transportation network with multi-weight traffic graph convolutional network for traffic forecasting. (arXiv:1909.07105v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.07105</id>
        <link href="http://arxiv.org/abs/1909.07105"/>
        <updated>2021-05-27T01:32:29.007Z</updated>
        <summary type="html"><![CDATA[Traffic forecasting problem remains a challenging task in the intelligent
transportation system due to its spatio-temporal complexity. Although temporal
dependency has been well studied and discussed, spatial dependency is
relatively less explored due to its large variations, especially in the urban
environment. In this study, a novel graph convolutional network model,
Multi-Weight Traffic Graph Convolutional (MW-TGC) network, is proposed and
applied to two urban networks with contrasting geometric constraints. The model
conducts graph convolution operations on speed data with multi-weighted
adjacency matrices to combine the features, including speed limit, distance,
and angle. The spatially isolated dimension reduction operation is conducted on
the combined features to learn the dependencies among the features and reduce
the size of the output to a computationally feasible level. The output of
multi-weight graph convolution is applied to the sequence-to-sequence model
with Long Short-Term Memory units to learn temporal dependencies. When applied
to two urban sites, urban-core and urban-mix, MW-TGC network not only
outperformed the comparative models in both sites but also reduced variance in
the heterogeneous urban-mix network. We conclude that MW-TGC network can
provide a robust traffic forecasting performance across the variations in
spatial complexity, which can be a strong advantage in urban traffic
forecasting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yuyol Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yoon_Y/0/1/0/all/0/1"&gt;Yoonjin Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smile Like You Mean It: Driving Animatronic Robotic Face with Learned Models. (arXiv:2105.12724v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.12724</id>
        <link href="http://arxiv.org/abs/2105.12724"/>
        <updated>2021-05-27T01:32:28.820Z</updated>
        <summary type="html"><![CDATA[Ability to generate intelligent and generalizable facial expressions is
essential for building human-like social robots. At present, progress in this
field is hindered by the fact that each facial expression needs to be
programmed by humans. In order to adapt robot behavior in real time to
different situations that arise when interacting with human subjects, robots
need to be able to train themselves without requiring human labels, as well as
make fast action decisions and generalize the acquired knowledge to diverse and
new contexts. We addressed this challenge by designing a physical animatronic
robotic face with soft skin and by developing a vision-based self-supervised
learning framework for facial mimicry. Our algorithm does not require any
knowledge of the robot's kinematic model, camera calibration or predefined
expression set. By decomposing the learning process into a generative model and
an inverse model, our framework can be trained using a single motor babbling
dataset. Comprehensive evaluations show that our method enables accurate and
diverse face mimicry across diverse human subjects. The project website is at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Boyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yuhang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lianfeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1"&gt;Sara Cummings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1"&gt;Hod Lipson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Submodular Kernels for Efficient Rankings. (arXiv:2105.12356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12356</id>
        <link href="http://arxiv.org/abs/2105.12356"/>
        <updated>2021-05-27T01:32:28.814Z</updated>
        <summary type="html"><![CDATA[Many algorithms for ranked data become computationally intractable as the
number of objects grows due to complex geometric structure induced by rankings.
An additional challenge is posed by partial rankings, i.e. rankings in which
the preference is only known for a subset of all objects. For these reasons,
state-of-the-art methods cannot scale to real-world applications, such as
recommender systems. We address this challenge by exploiting geometric
structure of ranked data and additional available information about the objects
to derive a submodular kernel for ranking. The submodular kernel combines the
efficiency of submodular optimization with the theoretical properties of
kernel-based methods. We demonstrate that the submodular kernel drastically
reduces the computational cost compared to state-of-the-art kernels and scales
well to large datasets while attaining good empirical performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conserva_M/0/1/0/all/0/1"&gt;Michelangelo Conserva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deisenroth_M/0/1/0/all/0/1"&gt;Marc Peter Deisenroth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1"&gt;K S Sesh Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SizeNet: Object Recognition via Object Real Size-based Convolutional Networks. (arXiv:2105.06188v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06188</id>
        <link href="http://arxiv.org/abs/2105.06188"/>
        <updated>2021-05-27T01:32:28.808Z</updated>
        <summary type="html"><![CDATA[Inspired by the conclusion that humans choose the visual cortex regions
corresponding to the real size of an object to analyze its features when
identifying objects in the real world, this paper presents a framework,
SizeNet, which is based on both the real sizes and features of objects to solve
object recognition problems. SizeNet was used for object recognition
experiments on the homemade Rsize dataset, and was compared with the
state-of-the-art methods AlexNet, VGG-16, Inception V3, Resnet-18, and
DenseNet-121. The results showed that SizeNet provides much higher accuracy
rates for object recognition than the other algorithms. SizeNet can solve the
two problems of correctly recognizing objects with highly similar features but
real sizes that are obviously different from each other, and correctly
distinguishing a target object from interference objects whose real sizes are
obviously different from the target object. This is because SizeNet recognizes
objects based not only on their features, but also on their real size. The real
size of an object can help exclude the interference object's categories whose
real size ranges do not match the real size of the object, which greatly
reduces the object's categories' number in the label set used for the
downstream object recognition based on object features. SizeNet is of great
significance for studying the interpretable computer vision. Our code and
dataset will thus be made public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaofei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhong Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Probabilistic Pruning: A general framework for hardware-constrained pruning at different granularities. (arXiv:2105.12686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12686</id>
        <link href="http://arxiv.org/abs/2105.12686"/>
        <updated>2021-05-27T01:32:28.803Z</updated>
        <summary type="html"><![CDATA[Unstructured neural network pruning algorithms have achieved impressive
compression rates. However, the resulting - typically irregular - sparse
matrices hamper efficient hardware implementations, leading to additional
memory usage and complex control logic that diminishes the benefits of
unstructured pruning. This has spurred structured coarse-grained pruning
solutions that prune entire filters or even layers, enabling efficient
implementation at the expense of reduced flexibility. Here we propose a
flexible new pruning mechanism that facilitates pruning at different
granularities (weights, kernels, filters/feature maps), while retaining
efficient memory organization (e.g. pruning exactly k-out-of-n weights for
every output neuron, or pruning exactly k-out-of-n kernels for every feature
map). We refer to this algorithm as Dynamic Probabilistic Pruning (DPP). DPP
leverages the Gumbel-softmax relaxation for differentiable k-out-of-n sampling,
facilitating end-to-end optimization. We show that DPP achieves competitive
compression rates and classification accuracy when pruning common deep learning
models trained on different benchmark datasets for image classification.
Relevantly, the non-magnitude-based nature of DPP allows for joint optimization
of pruning and weight quantization in order to even further compress the
network, which we show as well. Finally, we propose novel information theoretic
metrics that show the confidence and pruning diversity of pruning masks within
a layer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Carabarin_L/0/1/0/all/0/1"&gt;Lizeth Gonzalez-Carabarin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huijben_I/0/1/0/all/0/1"&gt;Iris A.M. Huijben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1"&gt;Bastiaan S. Veeling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_A/0/1/0/all/0/1"&gt;Alexandre Schmid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1"&gt;Ruud J.G. van Sloun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SB-GCN: Structured BREP Graph Convolutional Network for Automatic Mating of CAD Assemblies. (arXiv:2105.12238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12238</id>
        <link href="http://arxiv.org/abs/2105.12238"/>
        <updated>2021-05-27T01:32:28.786Z</updated>
        <summary type="html"><![CDATA[Assembly modeling is a core task of computer aided design (CAD), comprising
around one third of the work in a CAD workflow. Optimizing this process
therefore represents a huge opportunity in the design of a CAD system, but
current research of assembly based modeling is not directly applicable to
modern CAD systems because it eschews the dominant data structure of modern
CAD: parametric boundary representations (BREPs). CAD assembly modeling defines
assemblies as a system of pairwise constraints, called mates, between parts,
which are defined relative to BREP topology rather than in world coordinates
common to existing work. We propose SB-GCN, a representation learning scheme on
BREPs that retains the topological structure of parts, and use these learned
representations to predict CAD type mates. To train our system, we compiled the
first large scale dataset of BREP CAD assemblies, which we are releasing along
with benchmark mate prediction tasks. Finally, we demonstrate the compatibility
of our model with an existing commercial CAD system by building a tool that
assists users in mate creation by suggesting mate completions, with 72.2%
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_B/0/1/0/all/0/1"&gt;Benjamin Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hildreth_D/0/1/0/all/0/1"&gt;Dalton Hildreth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Duowen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baran_I/0/1/0/all/0/1"&gt;Ilya Baran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1"&gt;Vova Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1"&gt;Adriana Schulz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09008</id>
        <link href="http://arxiv.org/abs/2105.09008"/>
        <updated>2021-05-27T01:32:28.780Z</updated>
        <summary type="html"><![CDATA[In the paper of ExquisiteNetV1, the ability of classification of
ExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and
better model ExquisiteNetV2. We conduct many experiments to evaluate its
performance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known
models on 15 credible datasets under the same condition. According to the
experimental results, ExquisiteNetV2 gets the highest classification accuracy
over half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts
of parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing
speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jou_S/0/1/0/all/0/1"&gt;Shyh Yaw Jou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chung Yen Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GeomCA: Geometric Evaluation of Data Representations. (arXiv:2105.12486v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12486</id>
        <link href="http://arxiv.org/abs/2105.12486"/>
        <updated>2021-05-27T01:32:28.774Z</updated>
        <summary type="html"><![CDATA[Evaluating the quality of learned representations without relying on a
downstream task remains one of the challenges in representation learning. In
this work, we present Geometric Component Analysis (GeomCA) algorithm that
evaluates representation spaces based on their geometric and topological
properties. GeomCA can be applied to representations of any dimension,
independently of the model that generated them. We demonstrate its
applicability by analyzing representations obtained from a variety of
scenarios, such as contrastive learning models, generative models and
supervised learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poklukar_P/0/1/0/all/0/1"&gt;Petra Poklukar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varava_A/0/1/0/all/0/1"&gt;Anastasia Varava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1"&gt;Danica Kragic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring dual information in distance metric learning for clustering. (arXiv:2105.12703v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12703</id>
        <link href="http://arxiv.org/abs/2105.12703"/>
        <updated>2021-05-27T01:32:28.769Z</updated>
        <summary type="html"><![CDATA[Distance metric learning algorithms aim to appropriately measure similarities
and distances between data points. In the context of clustering, metric
learning is typically applied with the assist of side-information provided by
experts, most commonly expressed in the form of cannot-link and must-link
constraints. In this setting, distance metric learning algorithms move closer
pairs of data points involved in must-link constraints, while pairs of points
involved in cannot-link constraints are moved away from each other. For these
algorithms to be effective, it is important to use a distance metric that
matches the expert knowledge, beliefs, and expectations, and the
transformations made to stick to the side-information should preserve
geometrical properties of the dataset. Also, it is interesting to filter the
constraints provided by the experts to keep only the most useful and reject
those that can harm the clustering process. To address these issues, we propose
to exploit the dual information associated with the pairwise constraints of the
semi-supervised clustering problem. Experiments clearly show that distance
metric learning algorithms benefit from integrating this dual information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Randel_R/0/1/0/all/0/1"&gt;Rodrigo Randel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aloise_D/0/1/0/all/0/1"&gt;Daniel Aloise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1"&gt;Alain Hertz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers. (arXiv:2105.12628v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12628</id>
        <link href="http://arxiv.org/abs/2105.12628"/>
        <updated>2021-05-27T01:32:28.763Z</updated>
        <summary type="html"><![CDATA[We propose Predict then Interpolate (PI), a simple algorithm for learning
correlations that are stable across environments. The algorithm follows from
the intuition that when using a classifier trained on one environment to make
predictions on examples from another environment, its mistakes are informative
as to which correlations are unstable. In this work, we prove that by
interpolating the distributions of the correct predictions and the wrong
predictions, we can uncover an oracle distribution where the unstable
correlation vanishes. Since the oracle interpolation coefficients are not
accessible, we use group distributionally robust optimization to minimize the
worst-case risk across all such interpolations. We evaluate our method on both
text classification and image classification. Empirical results demonstrate
that our algorithm is able to learn robust classifiers (outperforms IRM by
23.85% on synthetic environments and 12.41% on natural environments). Our code
and data are available at https://github.com/YujiaBao/Predict-then-Interpolate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yujia Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Successive Convex Approximation Based Off-Policy Optimization for Constrained Reinforcement Learning. (arXiv:2105.12545v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12545</id>
        <link href="http://arxiv.org/abs/2105.12545"/>
        <updated>2021-05-27T01:32:28.747Z</updated>
        <summary type="html"><![CDATA[We propose a successive convex approximation based off-policy optimization
(SCAOPO) algorithm to solve the general constrained reinforcement learning
problem, which is formulated as a constrained Markov decision process (CMDP) in
the context of average cost. The SCAOPO is based on solving a sequence of
convex objective/feasibility optimization problems obtained by replacing the
objective and constraint functions in the original problems with convex
surrogate functions. At each iteration, the convex surrogate problem can be
efficiently solved by Lagrange dual method even the policy is parameterized by
a high-dimensional function. Moreover, the SCAOPO enables to reuse old
experiences from previous updates, thereby significantly reducing the
implementation cost when deployed in the real-world engineering systems that
need to online learn the environment. In spite of the time-varying state
distribution and the stochastic bias incurred by the off-policy learning, the
SCAOPO with a feasible initial point can still provably converge to a
Karush-Kuhn-Tucker (KKT) point of the original problem almost surely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1"&gt;Chang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;An Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1"&gt;Wu Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimationand 3D Object Detection. (arXiv:2103.03977v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03977</id>
        <link href="http://arxiv.org/abs/2103.03977"/>
        <updated>2021-05-27T01:32:28.740Z</updated>
        <summary type="html"><![CDATA[The ability to accurately detect and localize objects is recognized as being
the most important for the perception of self-driving cars. From 2D to 3D
object detection, the most difficult is to determine the distance from the
ego-vehicle to objects. Expensive technology like LiDAR can provide a precise
and accurate depth information, so most studies have tended to focus on this
sensor showing a performance gap between LiDAR-based methods and camera-based
methods. Although many authors have investigated how to fuse LiDAR with RGB
cameras, as far as we know there are no studies to fuse LiDAR and stereo in a
deep neural network for the 3D object detection task. This paper presents
SLS-Fusion, a new approach to fuse data from 4-beam LiDAR and a stereo camera
via a neural network for depth estimation to achieve better dense depth maps
and thereby improves 3D object detection performance. Since 4-beam LiDAR is
cheaper than the well-known 64-beam LiDAR, this approach is also classified as
a low-cost sensors-based method. Through evaluation on the KITTI benchmark, it
is shown that the proposed method significantly improves depth estimation
performance compared to a baseline method. Also, when applying it to 3D object
detection, a new state of the art on low-cost sensor based method is achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_N/0/1/0/all/0/1"&gt;Nguyen Anh Minh Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duthon_P/0/1/0/all/0/1"&gt;Pierre Duthon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoudour_L/0/1/0/all/0/1"&gt;Louahdi Khoudour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crouzil_A/0/1/0/all/0/1"&gt;Alain Crouzil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velastin_S/0/1/0/all/0/1"&gt;Sergio A. Velastin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances. (arXiv:2105.12221v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12221</id>
        <link href="http://arxiv.org/abs/2105.12221"/>
        <updated>2021-05-27T01:32:28.733Z</updated>
        <summary type="html"><![CDATA[We study how permutation symmetries in overparameterized multi-layer neural
networks generate `symmetry-induced' critical points. Assuming a network with $
L $ layers of minimal widths $ r_1^*, \ldots, r_{L-1}^* $ reaches a zero-loss
minimum at $ r_1^*! \cdots r_{L-1}^*! $ isolated points that are permutations
of one another, we show that adding one extra neuron to each layer is
sufficient to connect all these previously discrete minima into a single
manifold. For a two-layer overparameterized network of width $ r^*+ h =: m $ we
explicitly describe the manifold of global minima: it consists of $ T(r^*, m) $
affine subspaces of dimension at least $ h $ that are connected to one another.
For a network of width $m$, we identify the number $G(r,m)$ of affine subspaces
containing only symmetry-induced critical points that are related to the
critical points of a smaller network of width $r<r^*$. Via a combinatorial
analysis, we derive closed-form formulas for $ T $ and $ G $ and show that the
number of symmetry-induced critical subspaces dominates the number of affine
subspaces forming the global minima manifold in the mildly overparameterized
regime (small $ h $) and vice versa in the vastly overparameterized regime ($h
\gg r^*$). Our results provide new insights into the minimization of the
non-convex loss function of overparameterized neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simsek_B/0/1/0/all/0/1"&gt;Berfin &amp;#x15e;im&amp;#x15f;ek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ged_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Ged&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacot_A/0/1/0/all/0/1"&gt;Arthur Jacot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spadaro_F/0/1/0/all/0/1"&gt;Francesco Spadaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hongler_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Hongler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1"&gt;Wulfram Gerstner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brea_J/0/1/0/all/0/1"&gt;Johanni Brea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth. (arXiv:2105.04550v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04550</id>
        <link href="http://arxiv.org/abs/2105.04550"/>
        <updated>2021-05-27T01:32:28.728Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have been studied through the lens of expressive
power and generalization. However, their optimization properties are less well
understood. We take the first step towards analyzing GNN training by studying
the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that
despite the non-convexity of training, convergence to a global minimum at a
linear rate is guaranteed under mild assumptions that we validate on real-world
graphs. Second, we study what may affect the GNNs' training speed. Our results
show that the training of GNNs is implicitly accelerated by skip connections,
more depth, and/or a good label distribution. Empirical results confirm that
our theoretical results for linearized GNNs align with the training behavior of
nonlinear GNNs. Our results provide the first theoretical support for the
success of GNNs with skip connections in terms of optimization, and suggest
that deep GNNs with skip connections would be promising in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Keyulu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mozhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1"&gt;Stefanie Jegelka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards an IMU-based Pen Online Handwriting Recognizer. (arXiv:2105.12434v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12434</id>
        <link href="http://arxiv.org/abs/2105.12434"/>
        <updated>2021-05-27T01:32:28.722Z</updated>
        <summary type="html"><![CDATA[Most online handwriting recognition systems require the use of specific
writing surfaces to extract positional data. In this paper we present a online
handwriting recognition system for word recognition which is based on inertial
measurement units (IMUs) for digitizing text written on paper. This is obtained
by means of a sensor-equipped pen that provides acceleration, angular velocity,
and magnetic forces streamed via Bluetooth. Our model combines convolutional
and bidirectional LSTM networks, and is trained with the Connectionist Temporal
Classification loss that allows the interpretation of raw sensor data into
words without the need of sequence segmentation. We use a dataset of words
collected using multiple sensor-enhanced pens and evaluate our model on
distinct test sets of seen and unseen words achieving a character error rate of
17.97% and 17.08%, respectively, without the use of a dictionary or language
model]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1"&gt;Mohamad Wehbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamann_T/0/1/0/all/0/1"&gt;Tim Hamann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barth_J/0/1/0/all/0/1"&gt;Jens Barth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaempf_P/0/1/0/all/0/1"&gt;Peter Kaempf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1"&gt;Dario Zanca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1"&gt;Bjoern Eskofier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local, global and scale-dependent node roles. (arXiv:2105.12598v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.12598</id>
        <link href="http://arxiv.org/abs/2105.12598"/>
        <updated>2021-05-27T01:32:28.717Z</updated>
        <summary type="html"><![CDATA[This paper re-examines the concept of node equivalences like structural
equivalence or automorphic equivalence, which have originally emerged in social
network analysis to characterize the role an actor plays within a social
system, but have since then been of independent interest for graph-based
learning tasks. Traditionally, such exact node equivalences have been defined
either in terms of the one hop neighborhood of a node, or in terms of the
global graph structure. Here we formalize exact node roles with a
scale-parameter, describing up to what distance the ego network of a node
should be considered when assigning node roles - motivated by the idea that
there can be local roles of a node that should not be determined by nodes
arbitrarily far away in the network. We present numerical experiments that show
how already "shallow" roles of depth 3 or 4 carry sufficient information to
perform node classification tasks with high accuracy. These findings
corroborate the success of recent graph-learning approaches that compute
approximate node roles in terms of embeddings, by nonlinearly aggregating node
features in an (un)supervised manner over relatively small neighborhood sizes.
Indeed, based on our ideas we can construct a shallow classifier achieving on
par results with recent graph neural network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scholkemper_M/0/1/0/all/0/1"&gt;Michael Scholkemper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1"&gt;Michael T. Schaub&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Representation Learning for Imitation with Contrastive Fourier Features. (arXiv:2105.12272v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12272</id>
        <link href="http://arxiv.org/abs/2105.12272"/>
        <updated>2021-05-27T01:32:28.701Z</updated>
        <summary type="html"><![CDATA[In imitation learning, it is common to learn a behavior policy to match an
unknown target policy via max-likelihood training on a collected set of target
demonstrations. In this work, we consider using offline experience datasets -
potentially far from the target distribution - to learn low-dimensional state
representations that provably accelerate the sample-efficiency of downstream
imitation learning. A central challenge in this setting is that the unknown
target policy itself may not exhibit low-dimensional behavior, and so there is
a potential for the representation learning objective to alias states in which
the target policy acts differently. Circumventing this challenge, we derive a
representation learning objective which provides an upper bound on the
performance difference between the target policy and a lowdimensional policy
trained with max-likelihood, and this bound is tight regardless of whether the
target policy itself exhibits low-dimensional structure. Moving to the
practicality of our method, we show that our objective can be implemented as
contrastive learning, in which the transition dynamics are approximated by
either an implicit energy-based model or, in some special cases, an implicit
linear model with representations given by random Fourier features. Experiments
on both tabular environments and high-dimensional Atari games provide
quantitative evidence for the practical benefits of our proposed objective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1"&gt;Ofir Nachum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mengjiao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The "given data" paradigm undermines both cultures. (arXiv:2105.12478v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.12478</id>
        <link href="http://arxiv.org/abs/2105.12478"/>
        <updated>2021-05-27T01:32:28.696Z</updated>
        <summary type="html"><![CDATA[Breiman organizes "Statistical modeling: The two cultures" around a simple
visual. Data, to the far right, are compelled into a "black box" with an arrow
and then catapulted left by a second arrow, having been transformed into an
output. Breiman then posits two interpretations of this visual as encapsulating
a distinction between two cultures in statistics. The divide, he argues is
about what happens in the "black box." In this comment, I argue for a broader
perspective on statistics and, in doing so, elevate questions from "before" and
"after" the box as fruitful areas for statistical innovation and practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+McCormick_T/0/1/0/all/0/1"&gt;Tyler McCormick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Nonparametric Reinforcement Learning in LTE and Wi-Fi Coexistence. (arXiv:2105.12249v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12249</id>
        <link href="http://arxiv.org/abs/2105.12249"/>
        <updated>2021-05-27T01:32:28.691Z</updated>
        <summary type="html"><![CDATA[With the formation of next generation wireless communication, a growing
number of new applications like internet of things, autonomous car, and drone
is crowding the unlicensed spectrum. Licensed network such as LTE also comes to
the unlicensed spectrum for better providing high-capacity contents with low
cost. However, LTE was not designed for sharing spectrum with others. A
cooperation center for these networks is costly because they possess
heterogeneous properties and everyone can enter and leave the spectrum
unrestrictedly, so the design will be challenging. Since it is infeasible to
incorporate potentially infinite scenarios with one unified design, an
alternative solution is to let each network learn its own coexistence policy.
Previous solutions only work on fixed scenarios. In this work a reinforcement
learning algorithm is presented to cope with the coexistence between Wi-Fi and
LTE-LAA agents in 5 GHz unlicensed spectrum. The coexistence problem was
modeled as a Dec-POMDP and Bayesian approach was adopted for policy learning
with nonparametric prior to accommodate the uncertainty of policy for different
agents. A fairness measure was introduced in the reward function to encourage
fair sharing between agents. The reinforcement learning was turned into an
optimization problem by transforming the value function as likelihood and
variational inference for posterior approximation. Simulation results
demonstrate that this algorithm can reach high value with compact policy
representations, and stay computationally efficient when applying to agent set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shih_P/0/1/0/all/0/1"&gt;Po-Kan Shih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning. (arXiv:2105.12722v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12722</id>
        <link href="http://arxiv.org/abs/2105.12722"/>
        <updated>2021-05-27T01:32:28.685Z</updated>
        <summary type="html"><![CDATA[The objective of this work is to segment any arbitrary structures of interest
(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D
segmentation). We show that high accuracy can be achieved by simply propagating
the 2D slice segmentation with an affinity matrix between consecutive slices,
which can be learnt in a self-supervised manner, namely slice reconstruction.
Specifically, we compare the proposed framework, termed as Sli2Vol, with
supervised approaches and two other unsupervised/ self-supervised slice
registration approaches, on 8 public datasets (both CT and MRI scans), spanning
9 different SOIs. Without any parameter-tuning, the same model achieves
superior performance with Dice scores (0-100 scale) of over 80 for most of the
benchmarks, including the ones that are unseen during training. Our results
show generalizability of the proposed approach across data from different
machines and with different SOIs: a major use case of semi-automatic
segmentation methods where fully supervised approaches would normally struggle.
The source code will be made publicly available at
https://github.com/pakheiyeung/Sli2Vol.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_P/0/1/0/all/0/1"&gt;Pak-Hei Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1"&gt;Ana I.L. Namburete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weidi Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating the Uncertainty of Neural Network Forecasts for Influenza Prevalence Using Web Search Activity. (arXiv:2105.12433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12433</id>
        <link href="http://arxiv.org/abs/2105.12433"/>
        <updated>2021-05-27T01:32:28.680Z</updated>
        <summary type="html"><![CDATA[Influenza is an infectious disease with the potential to become a pandemic,
and hence, forecasting its prevalence is an important undertaking for planning
an effective response. Research has found that web search activity can be used
to improve influenza models. Neural networks (NN) can provide state-of-the-art
forecasting accuracy but do not commonly incorporate uncertainty in their
estimates, something essential for using them effectively during decision
making. In this paper, we demonstrate how Bayesian Neural Networks (BNNs) can
be used to both provide a forecast and a corresponding uncertainty without
significant loss in forecasting accuracy compared to traditional NNs. Our
method accounts for two sources of uncertainty: data and model uncertainty,
arising due to measurement noise and model specification, respectively.
Experiments are conducted using 14 years of data for England, assessing the
model's accuracy over the last 4 flu seasons in this dataset. We evaluate the
performance of different models including competitive baselines with
conventional metrics as well as error functions that incorporate uncertainty
estimates. Our empirical analysis indicates that considering both sources of
uncertainty simultaneously is superior to considering either one separately. We
also show that a BNN with recurrent layers that models both sources of
uncertainty yields superior accuracy for these metrics for forecasting horizons
greater than 7 days.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1"&gt;Michael Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayes_P/0/1/0/all/0/1"&gt;Peter Hayes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cox_I/0/1/0/all/0/1"&gt;Ingemar J. Cox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampos_V/0/1/0/all/0/1"&gt;Vasileios Lampos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social-IWSTCNN: A Social Interaction-Weighted Spatio-Temporal Convolutional Neural Network for Pedestrian Trajectory Prediction in Urban Traffic Scenarios. (arXiv:2105.12436v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12436</id>
        <link href="http://arxiv.org/abs/2105.12436"/>
        <updated>2021-05-27T01:32:28.665Z</updated>
        <summary type="html"><![CDATA[Pedestrian trajectory prediction in urban scenarios is essential for
automated driving. This task is challenging because the behavior of pedestrians
is influenced by both their own history paths and the interactions with others.
Previous research modeled these interactions with pooling mechanisms or
aggregating with hand-crafted attention weights. In this paper, we present the
Social Interaction-Weighted Spatio-Temporal Convolutional Neural Network
(Social-IWSTCNN), which includes both the spatial and the temporal features. We
propose a novel design, namely the Social Interaction Extractor, to learn the
spatial and social interaction features of pedestrians. Most previous works
used ETH and UCY datasets which include five scenes but do not cover urban
traffic scenarios extensively for training and evaluation. In this paper, we
use the recently released large-scale Waymo Open Dataset in urban traffic
scenarios, which includes 374 urban training scenes and 76 urban testing scenes
to analyze the performance of our proposed algorithm in comparison to the
state-of-the-art (SOTA) models. The results show that our algorithm outperforms
SOTA algorithms such as Social-LSTM, Social-GAN, and Social-STGCNN on both
Average Displacement Error (ADE) and Final Displacement Error (FDE).
Furthermore, our Social-IWSTCNN is 54.8 times faster in data pre-processing
speed, and 4.7 times faster in total test speed than the current best SOTA
algorithm Social-STGCNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1"&gt;Christian Berger&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Dozza_M/0/1/0/all/0/1"&gt;Marco Dozza&lt;/a&gt; (2) ((1) Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden, (2) Department of Maritime Sciences and Mechanics, Chalmers University of Technology, Gothenburg, Sweden)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling. (arXiv:2105.12441v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12441</id>
        <link href="http://arxiv.org/abs/2105.12441"/>
        <updated>2021-05-27T01:32:28.659Z</updated>
        <summary type="html"><![CDATA[Since 2014 transfer learning has become the key driver for the improvement of
spatial saliency prediction; however, with stagnant progress in the last 3-5
years. We conduct a large-scale transfer learning study which tests different
ImageNet backbones, always using the same read out architecture and learning
protocol adopted from DeepGaze II. By replacing the VGG19 backbone of DeepGaze
II with ResNet50 features we improve the performance on saliency prediction
from 78% to 85%. However, as we continue to test better ImageNet models as
backbones (such as EfficientNetB5) we observe no additional improvement on
saliency prediction. By analyzing the backbones further, we find that
generalization to other datasets differs substantially, with models being
consistently overconfident in their fixation predictions. We show that by
combining multiple backbones in a principled manner a good confidence
calibration on unseen datasets can be achieved. This yields a significant leap
in benchmark performance in and out-of-domain with a 15 percent point
improvement over DeepGaze II to 93% on MIT1003, marking a new state of the art
on the MIT/Tuebingen Saliency Benchmark in all available metrics (AUC: 88.3%,
sAUC: 79.4%, CC: 82.4%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kummerer_M/0/1/0/all/0/1"&gt;Matthias K&amp;#xfc;mmerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1"&gt;Ori Press&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Basic and Depression Specific Emotion Identification in Tweets: Multi-label Classification Experiments. (arXiv:2105.12364v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12364</id>
        <link href="http://arxiv.org/abs/2105.12364"/>
        <updated>2021-05-27T01:32:28.653Z</updated>
        <summary type="html"><![CDATA[In this paper, we present empirical analysis on basic and depression specific
multi-emotion mining in Tweets with the help of state of the art multi-label
classifiers. We choose our basic emotions from a hybrid emotion model
consisting of the common emotions from four highly regarded psychological
models of emotions. Moreover, we augment that emotion model with new emotion
categories because of their importance in the analysis of depression. Most of
those additional emotions have not been used in previous emotion mining
research. Our experimental analyses show that a cost sensitive RankSVM
algorithm and a Deep Learning model are both robust, measured by both Macro
F-measures and Micro F-measures. This suggests that these algorithms are
superior in addressing the widely known data imbalance problem in multi-label
learning. Moreover, our application of Deep Learning performs the best, giving
it an edge in modeling deep semantic features of our extended emotional
categories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chenyang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Domain-Oblivious Approach for Learning Concise Representations of Filtered Topological Spaces. (arXiv:2105.12208v1 [cs.CG])]]></title>
        <id>http://arxiv.org/abs/2105.12208</id>
        <link href="http://arxiv.org/abs/2105.12208"/>
        <updated>2021-05-27T01:32:28.647Z</updated>
        <summary type="html"><![CDATA[Persistence diagrams have been widely used to quantify the underlying
features of filtered topological spaces in data visualization. In many
applications, computing distances between diagrams is essential; however,
computing these distances has been challenging due to the computational cost.
In this paper, we propose a persistence diagram hashing framework that learns a
binary code representation of persistence diagrams, which allows for fast
computation of distances. This framework is built upon a generative adversarial
network (GAN) with a diagram distance loss function to steer the learning
process. Instead of attempting to transform diagrams into vectorized
representations, we hash diagrams into binary codes, which have natural
advantages in large-scale tasks. The training of this model is domain-oblivious
in that it can be computed purely from synthetic, randomly created diagrams. As
a consequence, our proposed method is directly applicable to various datasets
without the need of retraining the model. These binary codes, when compared
using fast Hamming distance, better maintain topological similarity properties
between datasets than other vectorized representations. To evaluate this
method, we apply our framework to the problem of diagram clustering and we
compare the quality and performance of our approach to the state-of-the-art. In
addition, we show the scalability of our approach on a dataset with 10k
persistence diagrams, which is not possible with current techniques. Moreover,
our experimental results demonstrate that our method is significantly faster
with less memory usage, while retaining comparable or better quality
comparisons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yu Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fasy_B/0/1/0/all/0/1"&gt;Brittany Terese Fasy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenk_C/0/1/0/all/0/1"&gt;Carola Wenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Summa_B/0/1/0/all/0/1"&gt;Brian Summa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Sensitive Visualization of Deep Learning Natural Language Processing Models. (arXiv:2105.12202v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12202</id>
        <link href="http://arxiv.org/abs/2105.12202"/>
        <updated>2021-05-27T01:32:28.641Z</updated>
        <summary type="html"><![CDATA[The introduction of Transformer neural networks has changed the landscape of
Natural Language Processing (NLP) during the last years. So far, none of the
visualization systems has yet managed to examine all the facets of the
Transformers. This gave us the motivation of the current work. We propose a new
NLP Transformer context-sensitive visualization method that leverages existing
NLP tools to find the most significant groups of tokens (words) that have the
greatest effect on the output, thus preserving some context from the original
text. First, we use a sentence-level dependency parser to highlight promising
word groups. The dependency parser creates a tree of relationships between the
words in the sentence. Next, we systematically remove adjacent and non-adjacent
tuples of \emph{n} tokens from the input text, producing several new texts with
those tokens missing. The resulting texts are then passed to a pre-trained BERT
model. The classification output is compared with that of the full text, and
the difference in the activation strength is recorded. The modified texts that
produce the largest difference in the target classification output neuron are
selected, and the combination of removed words are then considered to be the
most influential on the model's output. Finally, the most influential word
combinations are visualized in a heatmap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1"&gt;Andrew Dunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inkpen_D/0/1/0/all/0/1"&gt;Diana Inkpen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andonie_R/0/1/0/all/0/1"&gt;R&amp;#x103;zvan Andonie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quotient Space-Based Keyword Retrieval in Sponsored Search. (arXiv:2105.12371v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.12371</id>
        <link href="http://arxiv.org/abs/2105.12371"/>
        <updated>2021-05-27T01:32:28.626Z</updated>
        <summary type="html"><![CDATA[Synonymous keyword retrieval has become an important problem for sponsored
search ever since major search engines relax the exact match product's matching
requirement to a synonymous level. Since the synonymous relations between
queries and keywords are quite scarce, the traditional information retrieval
framework is inefficient in this scenario. In this paper, we propose a novel
quotient space-based retrieval framework to address this problem. Considering
the synonymy among keywords as a mathematical equivalence relation, we can
compress the synonymous keywords into one representative, and the corresponding
quotient space would greatly reduce the size of the keyword repository. Then an
embedding-based retrieval is directly conducted between queries and the keyword
representatives. To mitigate the semantic gap of the quotient space-based
retrieval, a single semantic siamese model is utilized to detect both the
keyword--keyword and query-keyword synonymous relations. The experiments show
that with our quotient space-based retrieval method, the synonymous keyword
retrieving performance can be greatly improved in terms of memory cost and
recall efficiency. This method has been successfully implemented in Baidu's
online sponsored search system and has yielded a significant improvement in
revenue.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1"&gt;Yijiang Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chaobing Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;YanFeng Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certainty Equivalent Quadratic Control for Markov Jump Systems. (arXiv:2105.12358v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2105.12358</id>
        <link href="http://arxiv.org/abs/2105.12358"/>
        <updated>2021-05-27T01:32:28.620Z</updated>
        <summary type="html"><![CDATA[Real-world control applications often involve complex dynamics subject to
abrupt changes or variations. Markov jump linear systems (MJS) provide a rich
framework for modeling such dynamics. Despite an extensive history, theoretical
understanding of parameter sensitivities of MJS control is somewhat lacking.
Motivated by this, we investigate robustness aspects of certainty equivalent
model-based optimal control for MJS with quadratic cost function. Given the
uncertainty in the system matrices and in the Markov transition matrix is
bounded by $\epsilon$ and $\eta$ respectively, robustness results are
established for (i) the solution to coupled Riccati equations and (ii) the
optimal cost, by providing explicit perturbation bounds which decay as
$\mathcal{O}(\epsilon + \eta)$ and $\mathcal{O}((\epsilon + \eta)^2)$
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhe Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sattar_Y/0/1/0/all/0/1"&gt;Yahya Sattar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tarzanagh_D/0/1/0/all/0/1"&gt;Davoud Ataee Tarzanagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Balzano_L/0/1/0/all/0/1"&gt;Laura Balzano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Oymak_S/0/1/0/all/0/1"&gt;Samet Oymak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ozay_N/0/1/0/all/0/1"&gt;Necmiye Ozay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Detect Fortified Areas. (arXiv:2105.12385v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12385</id>
        <link href="http://arxiv.org/abs/2105.12385"/>
        <updated>2021-05-27T01:32:28.611Z</updated>
        <summary type="html"><![CDATA[High resolution data models like grid terrain models made from LiDAR data are
a prerequisite for modern day Geographic Information Systems applications.
Besides providing the foundation for the very accurate digital terrain models,
LiDAR data is also extensively used to classify which parts of the considered
surface comprise relevant elements like water, buildings and vegetation. In
this paper we consider the problem of classifying which areas of a given
surface are fortified by for instance, roads, sidewalks, parking spaces, paved
driveways and terraces. We consider using LiDAR data and orthophotos, combined
and alone, to show how well the modern machine learning algorithms Gradient
Boosted Trees and Convolutional Neural Networks are able to detect fortified
areas on large real world data. The LiDAR data features, in particular the
intensity feature that measures the signal strength of the return, that we
consider in this project are heavily dependent on the actual LiDAR sensor that
made the measurement. This is highly problematic, in particular for the
generalisation capability of pattern matching algorithms, as this means that
data features for test data may be very different from the data the model is
trained on. We propose an algorithmic solution to this problem by designing a
neural net embedding architecture that transforms data from all the different
sensor systems into a new common representation that works as well as if the
training data and test data originated from the same sensor. The final
algorithm result has an accuracy above 96 percent, and an AUC score above 0.99.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gronlund_A/0/1/0/all/0/1"&gt;Allan Gr&amp;#xf8;nlund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tranberg_J/0/1/0/all/0/1"&gt;Jonas Tranberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks. (arXiv:2105.12395v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.12395</id>
        <link href="http://arxiv.org/abs/2105.12395"/>
        <updated>2021-05-27T01:32:28.605Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the performance of variants of well-known
Convolutional Neural Network (CNN) architectures on different audio tasks. We
show that tuning the Receptive Field (RF) of CNNs is crucial to their
generalization. An insufficient RF limits the CNN's ability to fit the training
data. In contrast, CNNs with an excessive RF tend to over-fit the training data
and fail to generalize to unseen testing data. As state-of-the-art CNN
architectures-in computer vision and other domains-tend to go deeper in terms
of number of layers, their RF size increases and therefore they degrade in
performance in several audio classification and tagging tasks. We study
well-known CNN architectures and how their building blocks affect their
receptive field. We propose several systematic approaches to control the RF of
CNNs and systematically test the resulting architectures on different audio
classification and tagging tasks and datasets. The experiments show that
regularizing the RF of CNNs using our proposed approaches can drastically
improve the generalization of models, out-performing complex architectures and
pre-trained models on larger datasets. The proposed CNNs achieve
state-of-the-art results in multiple tasks, from acoustic scene classification
to emotion and theme detection in music to instrument recognition, as
demonstrated by top ranks in several pertinent challenges (DCASE, MediaEval).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koutini_K/0/1/0/all/0/1"&gt;Khaled Koutini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eghbal_zadeh_H/0/1/0/all/0/1"&gt;Hamid Eghbal-zadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1"&gt;Gerhard Widmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[There and Back Again: Self-supervised Multispectral Correspondence Estimation. (arXiv:2103.10768v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10768</id>
        <link href="http://arxiv.org/abs/2103.10768"/>
        <updated>2021-05-27T01:32:28.598Z</updated>
        <summary type="html"><![CDATA[Across a wide range of applications, from autonomous vehicles to medical
imaging, multi-spectral images provide an opportunity to extract additional
information not present in color images. One of the most important steps in
making this information readily available is the accurate estimation of dense
correspondences between different spectra.

Due to the nature of cross-spectral images, most correspondence solving
techniques for the visual domain are simply not applicable. Furthermore, most
cross-spectral techniques utilize spectra-specific characteristics to perform
the alignment. In this work, we aim to address the dense correspondence
estimation problem in a way that generalizes to more than one spectrum. We do
this by introducing a novel cycle-consistency metric that allows us to
self-supervise. This, combined with our spectra-agnostic loss functions, allows
us to train the same network across multiple spectra.

We demonstrate our approach on the challenging task of dense RGB-FIR
correspondence estimation. We also show the performance of our unmodified
network on the cases of RGB-NIR and RGB-RGB, where we achieve higher accuracy
than similar self-supervised approaches. Our work shows that cross-spectral
correspondence estimation can be solved in a common framework that learns to
generalize alignment across spectra.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Walters_C/0/1/0/all/0/1"&gt;Celyn Walters&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1"&gt;Oscar Mendez&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"&gt;Mark Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt; (1) ((1) CVSSP, University of Surrey)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ikshana: A Theory of Human Scene Understanding Mechanism. (arXiv:2101.10837v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10837</id>
        <link href="http://arxiv.org/abs/2101.10837"/>
        <updated>2021-05-27T01:32:28.592Z</updated>
        <summary type="html"><![CDATA[In recent years, deep neural networks (DNNs) achieved state-of-the-art
performance on many computer vision tasks. However, the one typical drawback of
these DNNs is the requirement of massive labeled data. Even though few-shot
learning methods addressed this problem through metric-learning and
meta-learning techniques, in this work, we address this problem from a
neuroscience perspective. We propose a theory named Ikshana, to explain the
functioning of the human brain, while humans understand an image. By following
the Ikshana theory, we propose a novel neural-inspired CNN architecture named
IkshanaNet for semantic segmentation. The empirical results demonstrate the
effectiveness of our method on few data samples, outperforming several
baselines, on the Cityscapes and the CamVid benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daliparthi_V/0/1/0/all/0/1"&gt;Venkata Satya Sai Ajay Daliparthi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What data do we need for training an AV motion planner?. (arXiv:2105.12337v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.12337</id>
        <link href="http://arxiv.org/abs/2105.12337"/>
        <updated>2021-05-27T01:32:28.578Z</updated>
        <summary type="html"><![CDATA[We investigate what grade of sensor data is required for training an
imitation-learning-based AV planner on human expert demonstration.
Machine-learned planners are very hungry for training data, which is usually
collected using vehicles equipped with the same sensors used for autonomous
operation. This is costly and non-scalable. If cheaper sensors could be used
for collection instead, data availability would go up, which is crucial in a
field where data volume requirements are large and availability is small. We
present experiments using up to 1000 hours worth of expert demonstration and
find that training with 10x lower-quality data outperforms 1x AV-grade data in
terms of planner performance. The important implication of this is that cheaper
sensors can indeed be used. This serves to improve data access and democratize
the field of imitation-based motion planning. Alongside this, we perform a
sensitivity analysis of planner performance as a function of perception range,
field-of-view, accuracy, and data volume, and the reason why lower-quality data
still provide good planning results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Platinsky_L/0/1/0/all/0/1"&gt;Lukas Platinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speichert_S/0/1/0/all/0/1"&gt;Stefanie Speichert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1"&gt;Blazej Osinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheel_O/0/1/0/all/0/1"&gt;Oliver Scheel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yawei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1"&gt;Hugo Grimmett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1"&gt;Luca del Pero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1"&gt;Peter Ondruska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A data-driven approach to beating SAA out-of-sample. (arXiv:2105.12342v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2105.12342</id>
        <link href="http://arxiv.org/abs/2105.12342"/>
        <updated>2021-05-27T01:32:28.572Z</updated>
        <summary type="html"><![CDATA[While solutions of Distributionally Robust Optimization (DRO) problems can
sometimes have a higher out-of-sample expected reward than the Sample Average
Approximation (SAA), there is no guarantee. In this paper, we introduce the
class of Distributionally Optimistic Optimization (DOO) models, and show that
it is always possible to "beat" SAA out-of-sample if we consider not just
worst-case (DRO) models but also best-case (DOO) ones. We also show, however,
that this comes at a cost: Optimistic solutions are more sensitive to model
error than either worst-case or SAA optimizers, and hence are less robust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gotoh_J/0/1/0/all/0/1"&gt;Jun-ya Gotoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kim_M/0/1/0/all/0/1"&gt;Michael Jong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lim_A/0/1/0/all/0/1"&gt;Andrew E.B. Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Speech Enhancement Systems with Noisy Speech Datasets. (arXiv:2105.12315v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2105.12315</id>
        <link href="http://arxiv.org/abs/2105.12315"/>
        <updated>2021-05-27T01:32:28.567Z</updated>
        <summary type="html"><![CDATA[Recently, deep neural network (DNN)-based speech enhancement (SE) systems
have been used with great success. During training, such systems require clean
speech data - ideally, in large quantity with a variety of acoustic conditions,
many different speaker characteristics and for a given sampling rate (e.g.,
48kHz for fullband SE). However, obtaining such clean speech data is not
straightforward - especially, if only considering publicly available datasets.
At the same time, a lot of material for automatic speech recognition (ASR) with
the desired acoustic/speaker/sampling rate characteristics is publicly
available except being clean, i.e., it also contains background noise as this
is even often desired in order to have ASR systems that are noise-robust.
Hence, using such data to train SE systems is not straightforward. In this
paper, we propose two improvements to train SE systems on noisy speech data.
First, we propose several modifications of the loss functions, which make them
robust against noisy speech targets. In particular, computing the median over
the sample axis before averaging over time-frequency bins allows to use such
data. Furthermore, we propose a noise augmentation scheme for mixture-invariant
training (MixIT), which allows using it also in such scenarios. For our
experiments, we use the Mozilla Common Voice dataset and we show that using our
robust loss function improves PESQ by up to 0.19 compared to a system trained
in the traditional way. Similarly, for MixIT we can see an improvement of up to
0.27 in PESQ when using our proposed noise augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saito_K/0/1/0/all/0/1"&gt;Koichi Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Uhlich_S/0/1/0/all/0/1"&gt;Stefan Uhlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fabbro_G/0/1/0/all/0/1"&gt;Giorgio Fabbro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mitsufuji_Y/0/1/0/all/0/1"&gt;Yuki Mitsufuji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using the Overlapping Score to Improve Corruption Benchmarks. (arXiv:2105.12357v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12357</id>
        <link href="http://arxiv.org/abs/2105.12357"/>
        <updated>2021-05-27T01:32:28.562Z</updated>
        <summary type="html"><![CDATA[Neural Networks are sensitive to various corruptions that usually occur in
real-world applications such as blurs, noises, low-lighting conditions, etc. To
estimate the robustness of neural networks to these common corruptions, we
generally use a group of modeled corruptions gathered into a benchmark.
Unfortunately, no objective criterion exists to determine whether a benchmark
is representative of a large diversity of independent corruptions. In this
paper, we propose a metric called corruption overlapping score, which can be
used to reveal flaws in corruption benchmarks. Two corruptions overlap when the
robustnesses of neural networks to these corruptions are correlated. We argue
that taking into account overlappings between corruptions can help to improve
existing benchmarks or build better ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laugros_A/0/1/0/all/0/1"&gt;Alfred Laugros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caplier_A/0/1/0/all/0/1"&gt;Alice Caplier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ospici_M/0/1/0/all/0/1"&gt;Matthieu Ospici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Density estimation: an inflation-deflation approach. (arXiv:2105.12152v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12152</id>
        <link href="http://arxiv.org/abs/2105.12152"/>
        <updated>2021-05-27T01:32:28.548Z</updated>
        <summary type="html"><![CDATA[Normalizing Flows (NFs) are universal density estimators based on Neuronal
Networks. However, this universality is limited: the density's support needs to
be diffeomorphic to a Euclidean space. In this paper, we propose a novel method
to overcome this limitation without sacrificing universality. The proposed
method inflates the data manifold by adding noise in the normal space, trains
an NF on this inflated manifold, and, finally, deflates the learned density.
Our main result provides sufficient conditions on the manifold and the specific
choice of noise under which the corresponding estimator is exact. Our method
has the same computational complexity as NFs and does not require computing an
inverse flow. We also show that, if the embedding dimension is much larger than
the manifold dimension, noise in the normal space can be well approximated by
Gaussian noise. This allows to use our method for approximating arbitrary
densities on non-flat manifolds provided that the manifold dimension is known.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horvat_C/0/1/0/all/0/1"&gt;Christian Horvat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_J/0/1/0/all/0/1"&gt;Jean-Pascal Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Sensing of Urban Waterlogging. (arXiv:2103.05927v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05927</id>
        <link href="http://arxiv.org/abs/2103.05927"/>
        <updated>2021-05-27T01:32:28.542Z</updated>
        <summary type="html"><![CDATA[In the monsoon season, sudden flood events occur frequently in urban areas,
which hamper the social and economic activities and may threaten the
infrastructure and lives. The use of an efficient large-scale waterlogging
sensing and information system can provide valuable real-time disaster
information to facilitate disaster management and enhance awareness of the
general public to alleviate losses during and after flood disasters. Therefore,
in this study, a visual sensing approach driven by deep neural networks and
information and communication technology was developed to provide an end-to-end
mechanism to realize waterlogging sensing and event-location mapping. The use
of a deep sensing system in the monsoon season in Taiwan was demonstrated, and
waterlogging events were predicted on the island-wide scale. The system could
sense approximately 2379 vision sources through an internet of video things
framework and transmit the event-location information in 5 min. The proposed
approach can sense waterlogging events at a national scale and provide an
efficient and highly scalable alternative to conventional waterlogging sensing
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shi-Wei Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jyh-Horng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jo-Yu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1"&gt;Chien-Hao Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Meng-Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fang-Pang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Self Supervised Learning: the BT, the HSIC, and the VICReg. (arXiv:2105.12247v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12247</id>
        <link href="http://arxiv.org/abs/2105.12247"/>
        <updated>2021-05-27T01:32:28.537Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning and pre-training strategies have developed over the
last few years especially for Convolutional Neural Networks (CNNs). Recently
application of such methods can also be noticed for Graph Neural Networks
(GNNs). In this paper, we have used a graph based self-supervised learning
strategy with different loss functions (Barlow Twins[ 7], HSIC[ 4], VICReg[ 1])
which have shown promising results when applied with CNNs previously. We have
also proposed a hybrid loss function combining the advantages of VICReg and
HSIC and called it as VICRegHSIC. The performance of these aforementioned
methods have been compared when applied to two different datasets namely MUTAG
and PROTEINS. Moreover, the impact of different batch sizes, projector
dimensions and data augmentation strategies have also been explored. The
results are preliminary and we will be continuing to explore with other
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1"&gt;Sayan Nag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks. (arXiv:2105.12374v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12374</id>
        <link href="http://arxiv.org/abs/2105.12374"/>
        <updated>2021-05-27T01:32:28.531Z</updated>
        <summary type="html"><![CDATA[Continual learning is essential for all real-world applications, as frozen
pre-trained models cannot effectively deal with non-stationary data
distributions. The purpose of this study is to review the state-of-the-art
methods that allow continuous learning of computational models over time. We
primarily focus on the learning algorithms that perform continuous learning in
an online fashion from considerably large (or infinite) sequential data and
require substantially low computational and memory resources. We critically
analyze the key challenges associated with continual learning for autonomous
real-world systems and compare current methods in terms of computations,
memory, and network/model complexity. We also briefly describe the
implementations of continuous learning algorithms under three main autonomous
systems, i.e., self-driving vehicles, unmanned aerial vehicles, and robotics.
The learning methods of these autonomous systems and their strengths and
limitations are extensively explored in this article.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaheen_K/0/1/0/all/0/1"&gt;Khadija Shaheen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanif_M/0/1/0/all/0/1"&gt;Muhammad Abdullah Hanif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_O/0/1/0/all/0/1"&gt;Osman Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1"&gt;Muhammad Shafique&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content Disentanglement for Semantically Consistent Synthetic-to-Real Domain Adaptation. (arXiv:2105.08704v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08704</id>
        <link href="http://arxiv.org/abs/2105.08704"/>
        <updated>2021-05-27T01:32:28.526Z</updated>
        <summary type="html"><![CDATA[Synthetic data generation is an appealing approach to generate novel traffic
scenarios in autonomous driving. However, deep learning techniques trained
solely on synthetic data encounter dramatic performance drops when they are
tested on real data. Such performance drop is commonly attributed to the domain
gap between real and synthetic data. Domain adaptation methods have been
applied to mitigate the aforementioned domain gap. These methods achieve
visually appealing results, but the translated samples usually introduce
semantic inconsistencies. In this work, we propose a new, unsupervised,
end-to-end domain adaptation network architecture that enables semantically
consistent domain adaptation between synthetic and real data. We evaluate our
architecture on the downstream task of semantic segmentation and show that our
method achieves superior performance compared to the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keser_M/0/1/0/all/0/1"&gt;Mert Keser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savkin_A/0/1/0/all/0/1"&gt;Artem Savkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No-reference Screen Content Image Quality Assessment with Unsupervised Domain Adaptation. (arXiv:2008.08561v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08561</id>
        <link href="http://arxiv.org/abs/2008.08561"/>
        <updated>2021-05-27T01:32:28.521Z</updated>
        <summary type="html"><![CDATA[In this paper, we quest the capability of transferring the quality of natural
scene images to the images that are not acquired by optical cameras (e.g.,
screen content images, SCIs), rooted in the widely accepted view that the human
visual system has adapted and evolved through the perception of natural
environment. Here, we develop the first unsupervised domain adaptation based no
reference quality assessment method for SCIs, leveraging rich subjective
ratings of the natural images (NIs). In general, it is a non-trivial task to
directly transfer the quality prediction model from NIs to a new type of
content (i.e., SCIs) that holds dramatically different statistical
characteristics. Inspired by the transferability of pair-wise relationship, the
proposed quality measure operates based on the philosophy of improving the
transferability and discriminability simultaneously. In particular, we
introduce three types of losses which complementarily and explicitly regularize
the feature space of ranking in a progressive manner. Regarding feature
discriminatory capability enhancement, we propose a center based loss to
rectify the classifier and improve its prediction capability not only for
source domain (NI) but also the target domain (SCI). For feature discrepancy
minimization, the maximum mean discrepancy (MMD) is imposed on the extracted
ranking features of NIs and SCIs. Furthermore, to further enhance the feature
diversity, we introduce the correlation penalization between different feature
dimensions, leading to the features with lower rank and higher diversity.
Experiments show that our method can achieve higher performance on different
source-target settings based on a light-weight convolution neural network. The
proposed method also sheds light on learning quality assessment measures for
unseen application-specific content without the cumbersome and costing
subjective evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Baoliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hongfei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shiqi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time, Deep Synthetic Aperture Sonar (SAS) Autofocus. (arXiv:2103.10312v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10312</id>
        <link href="http://arxiv.org/abs/2103.10312"/>
        <updated>2021-05-27T01:32:28.515Z</updated>
        <summary type="html"><![CDATA[Synthetic aperture sonar (SAS) requires precise time-of-flight measurements
of the transmitted/received waveform to produce well-focused imagery. It is not
uncommon for errors in these measurements to be present resulting in image
defocusing. To overcome this, an \emph{autofocus} algorithm is employed as a
post-processing step after image reconstruction to improve image focus. A
particular class of these algorithms can be framed as a sharpness/contrast
metric-based optimization. To improve convergence, a hand-crafted weighting
function to remove "bad" areas of the image is sometimes applied to the
image-under-test before the optimization procedure. Additionally, dozens of
iterations are necessary for convergence which is a large compute burden for
low size, weight, and power (SWaP) systems. We propose a deep learning
technique to overcome these limitations and implicitly learn the weighting
function in a data-driven manner. Our proposed method, which we call Deep
Autofocus, uses features from the single-look-complex (SLC) to estimate the
phase correction which is applied in $k$-space. Furthermore, we train our
algorithm on batches of training imagery so that during deployment, only a
single iteration of our method is sufficient to autofocus. We show results
demonstrating the robustness of our technique by comparing our results to four
commonly used image sharpness metrics. Our results demonstrate Deep Autofocus
can produce imagery perceptually better than common iterative techniques but at
a lower computational cost. We conclude that Deep Autofocus can provide a more
favorable cost-quality trade-off than alternatives with significant potential
of future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gerg_I/0/1/0/all/0/1"&gt;Isaac D. Gerg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1"&gt;Vishal Monga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Convex Formulation of Robust One-hidden-layer Neural Network Training. (arXiv:2105.12237v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12237</id>
        <link href="http://arxiv.org/abs/2105.12237"/>
        <updated>2021-05-27T01:32:28.509Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that the training of a one-hidden-layer, scalar-output
fully-connected ReLU neural network can be reformulated as a finite-dimensional
convex program. Unfortunately, the scale of such a convex program grows
exponentially in data size. In this work, we prove that a stochastic procedure
with a linear complexity well approximates the exact formulation. Moreover, we
derive a convex optimization approach to efficiently solve the "adversarial
training" problem, which trains neural networks that are robust to adversarial
input perturbations. Our method can be applied to binary classification and
regression, and provides an alternative to the current adversarial training
methods, such as Fast Gradient Sign Method (FGSM) and Projected Gradient
Descent (PGD). We demonstrate in experiments that the proposed method achieves
a noticeably better adversarial robustness and performance than the existing
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yatong Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_T/0/1/0/all/0/1"&gt;Tanmay Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gai_Y/0/1/0/all/0/1"&gt;Yu Gai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1"&gt;Somayeh Sojoudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Nonlinearity Coefficient -- A Practical Guide to Neural Architecture Design. (arXiv:2105.12210v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12210</id>
        <link href="http://arxiv.org/abs/2105.12210"/>
        <updated>2021-05-27T01:32:28.497Z</updated>
        <summary type="html"><![CDATA[In essence, a neural network is an arbitrary differentiable, parametrized
function. Choosing a neural network architecture for any task is as complex as
searching the space of those functions. For the last few years, 'neural
architecture design' has been largely synonymous with 'neural architecture
search' (NAS), i.e. brute-force, large-scale search. NAS has yielded
significant gains on practical tasks. However, NAS methods end up searching for
a local optimum in architecture space in a small neighborhood around
architectures that often go back decades, based on CNN or LSTM.

In this work, we present a different and complementary approach to
architecture design, which we term 'zero-shot architecture design' (ZSAD). We
develop methods that can predict, without any training, whether an archi…]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Philipp_G/0/1/0/all/0/1"&gt;George Philipp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SG-PALM: a Fast Physically Interpretable Tensor Graphical Model. (arXiv:2105.12271v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.12271</id>
        <link href="http://arxiv.org/abs/2105.12271"/>
        <updated>2021-05-27T01:32:28.486Z</updated>
        <summary type="html"><![CDATA[We propose a new graphical model inference procedure, called SG-PALM, for
learning conditional dependency structure of high-dimensional tensor-variate
data. Unlike most other tensor graphical models the proposed model is
interpretable and computationally scalable to high dimension. Physical
interpretability follows from the Sylvester generative (SG) model on which
SG-PALM is based: the model is exact for any observation process that is a
solution of a partial differential equation of Poisson type. Scalability
follows from the fast proximal alternating linearized minimization (PALM)
procedure that SG-PALM uses during training. We establish that SG-PALM
converges linearly (i.e., geometric convergence rate) to a global optimum of
its objective function. We demonstrate the scalability and accuracy of SG-PALM
for an important but challenging climate prediction problem: spatio-temporal
forecasting of solar flares from multimodal imaging data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hero_A/0/1/0/all/0/1"&gt;Alfred Hero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable UAV Collision Avoidance using Deep Reinforcement Learning. (arXiv:2105.12254v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.12254</id>
        <link href="http://arxiv.org/abs/2105.12254"/>
        <updated>2021-05-27T01:32:28.454Z</updated>
        <summary type="html"><![CDATA[The major components of any successful autonomous flight system are task
completion and collision avoidance. Most deep learning algorithms are
successful while executing these aspects under the environment and conditions
in which they have been trained. However, they fail when subjected to novel
environments. In this paper we present autonomous UAV flight using Deep
Reinforcement Learning augmented with Self-Attention Models that can
effectively reason when subjected to varying inputs. In addition to their
reasoning ability, they also are interpretable which enables it to be used
under real-world conditions. We have tested our algorithm under different
weather and environments and found it to be robust compared to conventional
Deep Reinforcement Learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1"&gt;Deepak-George Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olshanskyi_D/0/1/0/all/0/1"&gt;Daniil Olshanskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krueger_K/0/1/0/all/0/1"&gt;Karter Krueger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1"&gt;Ali Jannesari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Pruning using Adaptive Exemplar Filters. (arXiv:2101.07985v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07985</id>
        <link href="http://arxiv.org/abs/2101.07985"/>
        <updated>2021-05-27T01:32:28.448Z</updated>
        <summary type="html"><![CDATA[Popular network pruning algorithms reduce redundant information by optimizing
hand-crafted models, and may cause suboptimal performance and long time in
selecting filters. We innovatively introduce adaptive exemplar filters to
simplify the algorithm design, resulting in an automatic and efficient pruning
approach called EPruner. Inspired by the face recognition community, we use a
message passing algorithm Affinity Propagation on the weight matrices to obtain
an adaptive number of exemplars, which then act as the preserved filters.
EPruner breaks the dependency on the training data in determining the
"important" filters and allows the CPU implementation in seconds, an order of
magnitude faster than GPU based SOTAs. Moreover, we show that the weights of
exemplars provide a better initialization for the fine-tuning. On VGGNet-16,
EPruner achieves a 76.34%-FLOPs reduction by removing 88.80% parameters, with
0.06% accuracy improvement on CIFAR-10. In ResNet-152, EPruner achieves a
65.12%-FLOPs reduction by removing 64.18% parameters, with only 0.71% top-5
accuracy loss on ILSVRC-2012. Our code can be available at
https://github.com/lmbxmu/EPruner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaojie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongjian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Bipedal Robot Locomotion from Human Movement. (arXiv:2105.12277v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.12277</id>
        <link href="http://arxiv.org/abs/2105.12277"/>
        <updated>2021-05-27T01:32:28.442Z</updated>
        <summary type="html"><![CDATA[Teaching an anthropomorphic robot from human example offers the opportunity
to impart humanlike qualities on its movement. In this work we present a
reinforcement learning based method for teaching a real world bipedal robot to
perform movements directly from human motion capture data. Our method
seamlessly transitions from training in a simulation environment to executing
on a physical robot without requiring any real world training iterations or
offline steps. To overcome the disparity in joint configurations between the
robot and the motion capture actor, our method incorporates motion re-targeting
into the training process. Domain randomization techniques are used to
compensate for the differences between the simulated and physical systems. We
demonstrate our method on an internally developed humanoid robot with movements
ranging from a dynamic walk cycle to complex balancing and waving. Our
controller preserves the style imparted by the motion capture data and exhibits
graceful failure modes resulting in safe operation for the robot. This work was
performed for research purposes only.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1"&gt;Michael Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bashkirov_S/0/1/0/all/0/1"&gt;Sergey Bashkirov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rico_J/0/1/0/all/0/1"&gt;Javier Fernandez Rico&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toriyama_I/0/1/0/all/0/1"&gt;Ike Toriyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyada_N/0/1/0/all/0/1"&gt;Naoyuki Miyada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yanagisawa_H/0/1/0/all/0/1"&gt;Hideki Yanagisawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishizuka_K/0/1/0/all/0/1"&gt;Kensaku Ishizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rank-one matrix estimation: analytic time evolution of gradient descent dynamics. (arXiv:2105.12257v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.12257</id>
        <link href="http://arxiv.org/abs/2105.12257"/>
        <updated>2021-05-27T01:32:28.436Z</updated>
        <summary type="html"><![CDATA[We consider a rank-one symmetric matrix corrupted by additive noise. The
rank-one matrix is formed by an $n$-component unknown vector on the sphere of
radius $\sqrt{n}$, and we consider the problem of estimating this vector from
the corrupted matrix in the high dimensional limit of $n$ large, by gradient
descent for a quadratic cost function on the sphere. Explicit formulas for the
whole time evolution of the overlap between the estimator and unknown vector,
as well as the cost, are rigorously derived. In the long time limit we recover
the well known spectral phase transition, as a function of the signal-to-noise
ratio. The explicit formulas also allow to point out interesting transient
features of the time evolution. Our analysis technique is based on recent
progress in random matrix theory and uses local versions of the semi-circle
law.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bodin_A/0/1/0/all/0/1"&gt;Antoine Bodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Macris_N/0/1/0/all/0/1"&gt;Nicolas Macris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Value Iteration for Continuous Control Tasks. (arXiv:2105.12189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12189</id>
        <link href="http://arxiv.org/abs/2105.12189"/>
        <updated>2021-05-27T01:32:28.431Z</updated>
        <summary type="html"><![CDATA[When transferring a control policy from simulation to a physical system, the
policy needs to be robust to variations in the dynamics to perform well.
Commonly, the optimal policy overfits to the approximate model and the
corresponding state-distribution, often resulting in failure to trasnfer
underlying distributional shifts. In this paper, we present Robust Fitted Value
Iteration, which uses dynamic programming to compute the optimal value function
on the compact state domain and incorporates adversarial perturbations of the
system dynamics. The adversarial perturbations encourage a optimal policy that
is robust to changes in the dynamics. Utilizing the continuous-time perspective
of reinforcement learning, we derive the optimal perturbations for the states,
actions, observations and model parameters in closed-form. Notably, the
resulting algorithm does not require discretization of states or actions.
Therefore, the optimal adversarial perturbations can be efficiently
incorporated in the min-max value function update. We apply the resulting
algorithm to the physical Furuta pendulum and cartpole. By changing the masses
of the systems we evaluate the quantitative and qualitative performance across
different model parameters. We show that robust value iteration is more robust
compared to deep reinforcement learning algorithm and the non-robust version of
the algorithm. Videos of the experiments are shown at
https://sites.google.com/view/rfvi]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lutter_M/0/1/0/all/0/1"&gt;Michael Lutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1"&gt;Animesh Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Block Dense Weighted Networks with Augmented Degree Correction. (arXiv:2105.12290v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.12290</id>
        <link href="http://arxiv.org/abs/2105.12290"/>
        <updated>2021-05-27T01:32:28.413Z</updated>
        <summary type="html"><![CDATA[Dense networks with weighted connections often exhibit a community like
structure, where although most nodes are connected to each other, different
patterns of edge weights may emerge depending on each node's community
membership. We propose a new framework for generating and estimating dense
weighted networks with potentially different connectivity patterns across
different communities. The proposed model relies on a particular class of
functions which map individual node characteristics to the edges connecting
those nodes, allowing for flexibility while requiring a small number of
parameters relative to the number of edges. By leveraging the estimation
techniques, we also develop a bootstrap methodology for generating new networks
on the same set of vertices, which may be useful in circumstances where
multiple data sets cannot be collected. Performance of these methods are
analyzed in theory, simulations, and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Leinwand_B/0/1/0/all/0/1"&gt;Benjamin Leinwand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pipiras_V/0/1/0/all/0/1"&gt;Vladas Pipiras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias in Machine Learning Software: Why? How? What to do?. (arXiv:2105.12195v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12195</id>
        <link href="http://arxiv.org/abs/2105.12195"/>
        <updated>2021-05-27T01:32:28.407Z</updated>
        <summary type="html"><![CDATA[Increasingly, software is making autonomous decisions in case of criminal
sentencing, approving credit cards, hiring employees, and so on. Some of these
decisions show bias and adversely affect certain social groups (e.g. those
defined by sex, race, age, marital status). Many prior works on bias mitigation
take the following form: change the data or learners in multiple ways, then see
if any of that improves fairness. Perhaps a better approach is to postulate
root causes of bias and then applying some resolution strategy. This paper
postulates that the root causes of bias are the prior decisions that affect-
(a) what data was selected and (b) the labels assigned to those examples. Our
Fair-SMOTE algorithm removes biased labels; and rebalances internal
distributions such that based on sensitive attribute, examples are equal in
both positive and negative classes. On testing, it was seen that this method
was just as effective at reducing bias as prior approaches. Further, models
generated via Fair-SMOTE achieve higher performance (measured in terms of
recall and F1) than other state-of-the-art fairness improvement algorithms. To
the best of our knowledge, measured in terms of number of analyzed learners and
datasets, this study is one of the largest studies on bias mitigation yet
presented in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_J/0/1/0/all/0/1"&gt;Joymallya Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1"&gt;Suvodeep Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1"&gt;Tim Menzies&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monocular Instance Motion Segmentation for Autonomous Driving: KITTI InstanceMotSeg Dataset and Multi-task Baseline. (arXiv:2008.07008v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07008</id>
        <link href="http://arxiv.org/abs/2008.07008"/>
        <updated>2021-05-27T01:32:28.256Z</updated>
        <summary type="html"><![CDATA[Moving object segmentation is a crucial task for autonomous vehicles as it
can be used to segment objects in a class agnostic manner based on their motion
cues. It enables the detection of unseen objects during training (e.g., moose
or a construction truck) based on their motion and independent of their
appearance. Although pixel-wise motion segmentation has been studied in
autonomous driving literature, it has been rarely addressed at the instance
level, which would help separate connected segments of moving objects leading
to better trajectory planning. As the main issue is the lack of large public
datasets, we create a new InstanceMotSeg dataset comprising of 12.9K samples
improving upon our KITTIMoSeg dataset. In addition to providing instance level
annotations, we have added 4 additional classes which is crucial for studying
class agnostic motion segmentation. We adapt YOLACT and implement a
motion-based class agnostic instance segmentation model which would act as a
baseline for the dataset. We also extend it to an efficient multi-task model
which additionally provides semantic instance segmentation sharing the encoder.
The model then learns separate prototype coefficients within the class agnostic
and semantic heads providing two independent paths of object detection for
redundant safety. To obtain real-time performance, we study different efficient
encoders and obtain 39 fps on a Titan Xp GPU using MobileNetV2 with an
improvement of 10% mAP relative to the baseline. Our model improves the
previous state of the art motion segmentation method by 3.3%. The dataset and
qualitative results video are shared in our website at
https://sites.google.com/view/instancemotseg/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewaisha_M/0/1/0/all/0/1"&gt;Mahmoud Ewaisha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siam_M/0/1/0/all/0/1"&gt;Mennatullah Siam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1"&gt;Hazem Rashed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdy_W/0/1/0/all/0/1"&gt;Waleed Hamdy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helmi_M/0/1/0/all/0/1"&gt;Muhammad Helmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmad El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accurate Camouflaged Object Detection via Mixture Convolution and Interactive Fusion. (arXiv:2101.05687v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05687</id>
        <link href="http://arxiv.org/abs/2101.05687"/>
        <updated>2021-05-27T01:32:28.250Z</updated>
        <summary type="html"><![CDATA[Camouflaged object detection (COD), which aims to identify the objects that
conceal themselves into the surroundings, has recently drawn increasing
research efforts in the field of computer vision. In practice, the success of
deep learning based COD is mainly determined by two key factors, including (i)
A significantly large receptive field, which provides rich context information,
and (ii) An effective fusion strategy, which aggregates the rich multi-level
features for accurate COD. Motivated by these observations, in this paper, we
propose a novel deep learning based COD approach, which integrates the large
receptive field and effective feature fusion into a unified framework.
Specifically, we first extract multi-level features from a backbone network.
The resulting features are then fed to the proposed dual-branch mixture
convolution modules, each of which utilizes multiple asymmetric convolutional
layers and two dilated convolutional layers to extract rich context features
from a large receptive field. Finally, we fuse the features using
specially-designed multi-level interactive fusion modules, each of which
employs an attention mechanism along with feature interaction for effective
feature fusion. Our method detects camouflaged objects with an effective fusion
strategy, which aggregates the rich context information from a large receptive
field. All of these designs meet the requirements of COD well, allowing the
accurate detection of camouflaged objects. Extensive experiments on widely-used
benchmark datasets demonstrate that our method is capable of accurately
detecting camouflaged objects and outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1"&gt;Bo Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1"&gt;Mingchen Zhuge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_H/0/1/0/all/0/1"&gt;Hongbo Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Geng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-based Vehicle Speed Estimation: A Survey. (arXiv:2101.06159v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06159</id>
        <link href="http://arxiv.org/abs/2101.06159"/>
        <updated>2021-05-27T01:32:28.244Z</updated>
        <summary type="html"><![CDATA[The need to accurately estimate the speed of road vehicles is becoming
increasingly important for at least two main reasons. First, the number of
speed cameras installed worldwide has been growing in recent years, as the
introduction and enforcement of appropriate speed limits is considered one of
the most effective means to increase the road safety. Second, traffic
monitoring and forecasting in road networks plays a fundamental role to enhance
traffic, emissions and energy consumption in smart cities, being the speed of
the vehicles one of the most relevant parameters of the traffic state. Among
the technologies available for the accurate detection of vehicle speed, the use
of vision-based systems brings great challenges to be solved, but also great
potential advantages, such as the drastic reduction of costs due to the absence
of expensive range sensors, and the possibility of identifying vehicles
accurately. This paper provides a review of vision-based vehicle speed
estimation. We describe the terminology, the application domains, and propose a
complete taxonomy of a large selection of works that categorizes all stages
involved. An overview of performance evaluation metrics and available datasets
is provided. Finally, we discuss current limitations and future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Llorca_D/0/1/0/all/0/1"&gt;David Fern&amp;#xe1;ndez Llorca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1"&gt;Antonio Hern&amp;#xe1;ndez Mart&amp;#xed;nez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daza_I/0/1/0/all/0/1"&gt;Iv&amp;#xe1;n Garc&amp;#xed;a Daza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Cohort Generalizability of Deep and Conventional Machine Learning for MRI-based Diagnosis and Prediction of Alzheimer's Disease. (arXiv:2012.08769v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08769</id>
        <link href="http://arxiv.org/abs/2012.08769"/>
        <updated>2021-05-27T01:32:28.228Z</updated>
        <summary type="html"><![CDATA[This work validates the generalizability of MRI-based classification of
Alzheimer's disease (AD) patients and controls (CN) to an external data set and
to the task of prediction of conversion to AD in individuals with mild
cognitive impairment (MCI). We used a conventional support vector machine (SVM)
and a deep convolutional neural network (CNN) approach based on structural MRI
scans that underwent either minimal pre-processing or more extensive
pre-processing into modulated gray matter (GM) maps. Classifiers were optimized
and evaluated using cross-validation in the ADNI (334 AD, 520 CN). Trained
classifiers were subsequently applied to predict conversion to AD in ADNI MCI
patients (231 converters, 628 non-converters) and in the independent Health-RI
Parelsnoer data set. From this multi-center study representing a tertiary
memory clinic population, we included 199 AD patients, 139 participants with
subjective cognitive decline, 48 MCI patients converting to dementia, and 91
MCI patients who did not convert to dementia. AD-CN classification based on
modulated GM maps resulted in a similar AUC for SVM (0.940) and CNN (0.933).
Application to conversion prediction in MCI yielded significantly higher
performance for SVM (0.756) than for CNN (0.742). In external validation,
performance was slightly decreased. For AD-CN, it again gave similar AUCs for
SVM (0.896) and CNN (0.876). For prediction in MCI, performances decreased for
both SVM (0.665) and CNN (0.702). Both with SVM and CNN, classification based
on modulated GM maps significantly outperformed classification based on
minimally processed images. Deep and conventional classifiers performed equally
well for AD classification and their performance decreased only slightly when
applied to the external cohort. We expect that this work on external validation
contributes towards translation of machine learning to clinical practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bron_E/0/1/0/all/0/1"&gt;Esther E. Bron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1"&gt;Stefan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Papma_J/0/1/0/all/0/1"&gt;Janne M. Papma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiskoot_L/0/1/0/all/0/1"&gt;Lize C. Jiskoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Venkatraghavan_V/0/1/0/all/0/1"&gt;Vikram Venkatraghavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Linders_J/0/1/0/all/0/1"&gt;Jara Linders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aalten_P/0/1/0/all/0/1"&gt;Pauline Aalten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deyn_P/0/1/0/all/0/1"&gt;Peter Paul De Deyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Biessels_G/0/1/0/all/0/1"&gt;Geert Jan Biessels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Claassen_J/0/1/0/all/0/1"&gt;Jurgen A.H.R. Claassen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Middelkoop_H/0/1/0/all/0/1"&gt;Huub A.M. Middelkoop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smits_M/0/1/0/all/0/1"&gt;Marion Smits&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niessen_W/0/1/0/all/0/1"&gt;Wiro J. Niessen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Swieten_J/0/1/0/all/0/1"&gt;John C. van Swieten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Flier_W/0/1/0/all/0/1"&gt;Wiesje M. van der Flier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramakers_I/0/1/0/all/0/1"&gt;Inez H.G.B. Ramakers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lugt_A/0/1/0/all/0/1"&gt;Aad van der Lugt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polka Lines: Learning Structured Illumination and Reconstruction for Active Stereo. (arXiv:2011.13117v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13117</id>
        <link href="http://arxiv.org/abs/2011.13117"/>
        <updated>2021-05-27T01:32:28.221Z</updated>
        <summary type="html"><![CDATA[Active stereo cameras that recover depth from structured light captures have
become a cornerstone sensor modality for 3D scene reconstruction and
understanding tasks across application domains. Existing active stereo cameras
project a pseudo-random dot pattern on object surfaces to extract disparity
independently of object texture. Such hand-crafted patterns are designed in
isolation from the scene statistics, ambient illumination conditions, and the
reconstruction method. In this work, we propose the first method to jointly
learn structured illumination and reconstruction, parameterized by a
diffractive optical element and a neural network, in an end-to-end fashion. To
this end, we introduce a novel differentiable image formation model for active
stereo, relying on both wave and geometric optics, and a novel trinocular
reconstruction network. The jointly optimized pattern, which we dub "Polka
Lines," together with the reconstruction network, achieve state-of-the-art
active-stereo depth estimates across imaging conditions. We validate the
proposed method in simulation and on a hardware prototype, and show that our
method outperforms existing active stereo systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1"&gt;Seung-Hwan Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1"&gt;Felix Heide&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure Preserving Stain Normalization of Histopathology Images Using Self-Supervised Semantic Guidance. (arXiv:2008.02101v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02101</id>
        <link href="http://arxiv.org/abs/2008.02101"/>
        <updated>2021-05-27T01:32:28.188Z</updated>
        <summary type="html"><![CDATA[Although generative adversarial network (GAN) based style transfer is state
of the art in histopathology color-stain normalization, they do not explicitly
integrate structural information of tissues. We propose a self-supervised
approach to incorporate semantic guidance into a GAN based stain normalization
framework and preserve detailed structural information. Our method does not
require manual segmentation maps which is a significant advantage over existing
methods. We integrate semantic information at different layers between a
pre-trained semantic network and the stain color normalization network. The
proposed scheme outperforms other color normalization methods leading to better
classification and segmentation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1"&gt;Dwarikanath Mahapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bozorgtabar_B/0/1/0/all/0/1"&gt;Behzad Bozorgtabar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thiran_J/0/1/0/all/0/1"&gt;Jean-Philippe Thiran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoDiNet: Path Distribution Modeling with Consistency and Diversity for Dynamic Routing. (arXiv:2005.14439v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.14439</id>
        <link href="http://arxiv.org/abs/2005.14439"/>
        <updated>2021-05-27T01:32:28.183Z</updated>
        <summary type="html"><![CDATA[Dynamic routing networks, aimed at finding the best routing paths in the
networks, have achieved significant improvements to neural networks in terms of
accuracy and efficiency. In this paper, we see dynamic routing networks in a
fresh light, formulating a routing method as a mapping from a sample space to a
routing space. From the perspective of space mapping, prevalent methods of
dynamic routing didn't consider how inference paths would be distributed in the
routing space. Thus, we propose a novel method, termed CoDiNet, to model the
relationship between a sample space and a routing space by regularizing the
distribution of routing paths with the properties of consistency and diversity.
Specifically, samples with similar semantics should be mapped into the same
area in routing space, while those with dissimilar semantics should be mapped
into different areas. Moreover, we design a customizable dynamic routing
module, which can strike a balance between accuracy and efficiency. When
deployed upon ResNet models, our method achieves higher performance and
effectively reduces average computational cost on four widely used datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huanyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zequn Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Songyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Giving Commands to a Self-driving Car: A Multimodal Reasoner for Visual Grounding. (arXiv:2003.08717v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08717</id>
        <link href="http://arxiv.org/abs/2003.08717"/>
        <updated>2021-05-27T01:32:28.177Z</updated>
        <summary type="html"><![CDATA[We propose a new spatial memory module and a spatial reasoner for the Visual
Grounding (VG) task. The goal of this task is to find a certain object in an
image based on a given textual query. Our work focuses on integrating the
regions of a Region Proposal Network (RPN) into a new multi-step reasoning
model which we have named a Multimodal Spatial Region Reasoner (MSRR). The
introduced model uses the object regions from an RPN as initialization of a 2D
spatial memory and then implements a multi-step reasoning process scoring each
region according to the query, hence why we call it a multimodal reasoner. We
evaluate this new model on challenging datasets and our experiments show that
our model that jointly reasons over the object regions of the image and words
of the query largely improves accuracy compared to current state-of-the-art
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deruyttere_T/0/1/0/all/0/1"&gt;Thierry Deruyttere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collell_G/0/1/0/all/0/1"&gt;Guillem Collell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1"&gt;Marie-Francine Moens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregating Nested Transformers. (arXiv:2105.12723v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12723</id>
        <link href="http://arxiv.org/abs/2105.12723"/>
        <updated>2021-05-27T01:32:28.171Z</updated>
        <summary type="html"><![CDATA[Although hierarchical structures are popular in recent vision transformers,
they require sophisticated designs and massive datasets to work well. In this
work, we explore the idea of nesting basic local transformers on
non-overlapping image blocks and aggregating them in a hierarchical manner. We
find that the block aggregation function plays a critical role in enabling
cross-block non-local information communication. This observation leads us to
design a simplified architecture with minor code changes upon the original
vision transformer and obtains improved performance compared to existing
methods. Our empirical results show that the proposed method NesT converges
faster and requires much less training data to achieve good generalization. For
example, a NesT with 68M parameters trained on ImageNet for 100/300 epochs
achieves $82.3\%/83.8\%$ accuracy evaluated on $224\times 224$ image size,
outperforming previous methods with up to $57\%$ parameter reduction. Training
a NesT with 6M parameters from scratch on CIFAR10 achieves $96\%$ accuracy
using a single GPU, setting a new state of the art for vision transformers.
Beyond image classification, we extend the key idea to image generation and
show NesT leads to a strong decoder that is 8$\times$ faster than previous
transformer based generators. Furthermore, we also propose a novel method for
visually interpreting the learned model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zizhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Long Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1"&gt;Tomas Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighing Features of Lung and Heart Regions for Thoracic Disease Classification. (arXiv:2105.12430v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.12430</id>
        <link href="http://arxiv.org/abs/2105.12430"/>
        <updated>2021-05-27T01:32:28.155Z</updated>
        <summary type="html"><![CDATA[Chest X-rays are the most commonly available and affordable radiological
examination for screening thoracic diseases. According to the domain knowledge
of screening chest X-rays, the pathological information usually lay on the lung
and heart regions. However, it is costly to acquire region-level annotation in
practice, and model training mainly relies on image-level class labels in a
weakly supervised manner, which is highly challenging for computer-aided chest
X-ray screening. To address this issue, some methods have been proposed
recently to identify local regions containing pathological information, which
is vital for thoracic disease classification. Inspired by this, we propose a
novel deep learning framework to explore discriminative information from lung
and heart regions. We design a feature extractor equipped with a multi-scale
attention module to learn global attention maps from global images. To exploit
disease-specific cues effectively, we locate lung and heart regions containing
pathological information by a well-trained pixel-wise segmentation model to
generate binarization masks. By introducing element-wise logical AND operator
on the learned global attention maps and the binarization masks, we obtain
local attention maps in which pixels are $1$ for lung and heart region and $0$
for other regions. By zeroing features of non-lung and heart regions in
attention maps, we can effectively exploit their disease-specific cues in lung
and heart regions. Compared to existing methods fusing global and local
features, we adopt feature weighting to avoid weakening visual cues unique to
lung and heart regions. Evaluated by the benchmark split on the publicly
available chest X-ray14 dataset, the comprehensive experiments show that our
method achieves superior performance compared to the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiansheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yitian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yuguang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Nonlinearity Coefficient -- A Practical Guide to Neural Architecture Design. (arXiv:2105.12210v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12210</id>
        <link href="http://arxiv.org/abs/2105.12210"/>
        <updated>2021-05-27T01:32:28.149Z</updated>
        <summary type="html"><![CDATA[In essence, a neural network is an arbitrary differentiable, parametrized
function. Choosing a neural network architecture for any task is as complex as
searching the space of those functions. For the last few years, 'neural
architecture design' has been largely synonymous with 'neural architecture
search' (NAS), i.e. brute-force, large-scale search. NAS has yielded
significant gains on practical tasks. However, NAS methods end up searching for
a local optimum in architecture space in a small neighborhood around
architectures that often go back decades, based on CNN or LSTM.

In this work, we present a different and complementary approach to
architecture design, which we term 'zero-shot architecture design' (ZSAD). We
develop methods that can predict, without any training, whether an archi…]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Philipp_G/0/1/0/all/0/1"&gt;George Philipp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-aware Cross-level Fusion Network for Camouflaged Object Detection. (arXiv:2105.12555v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12555</id>
        <link href="http://arxiv.org/abs/2105.12555"/>
        <updated>2021-05-27T01:32:28.143Z</updated>
        <summary type="html"><![CDATA[Camouflaged object detection (COD) is a challenging task due to the low
boundary contrast between the object and its surroundings. In addition, the
appearance of camouflaged objects varies significantly, e.g., object size and
shape, aggravating the difficulties of accurate COD. In this paper, we propose
a novel Context-aware Cross-level Fusion Network (C2F-Net) to address the
challenging COD task. Specifically, we propose an Attention-induced Cross-level
Fusion Module (ACFM) to integrate the multi-level features with informative
attention coefficients. The fused features are then fed to the proposed
Dual-branch Global Context Module (DGCM), which yields multi-scale feature
representations for exploiting rich global context information. In C2F-Net, the
two modules are conducted on high-level features using a cascaded manner.
Extensive experiments on three widely used benchmark datasets demonstrate that
our C2F-Net is an effective COD model and outperforms state-of-the-art models
remarkably. Our code is publicly available at:
https://github.com/thograce/C2FNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yujia Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Geng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nian Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge Detection for Satellite Images without Deep Networks. (arXiv:2105.12633v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12633</id>
        <link href="http://arxiv.org/abs/2105.12633"/>
        <updated>2021-05-27T01:32:28.137Z</updated>
        <summary type="html"><![CDATA[Satellite imagery is widely used in many application sectors, including
agriculture, navigation, and urban planning. Frequently, satellite imagery
involves both large numbers of images as well as high pixel counts, making
satellite datasets computationally expensive to analyze. Recent approaches to
satellite image analysis have largely emphasized deep learning methods. Though
extremely powerful, deep learning has some drawbacks, including the requirement
of specialized computing hardware and a high reliance on training data. When
dealing with large satellite datasets, the cost of both computational resources
and training data annotation may be prohibitive.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abraham_J/0/1/0/all/0/1"&gt;Joshua Abraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wloka_C/0/1/0/all/0/1"&gt;Calden Wloka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Longitudinal Pooling & Consistency Regularization to Model Disease Progression from MRIs. (arXiv:2003.13958v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.13958</id>
        <link href="http://arxiv.org/abs/2003.13958"/>
        <updated>2021-05-27T01:32:28.132Z</updated>
        <summary type="html"><![CDATA[Many neurological diseases are characterized by gradual deterioration of
brain structure and function. Large longitudinal MRI datasets have revealed
such deterioration, in part, by applying machine and deep learning to predict
diagnosis. A popular approach is to apply Convolutional Neural Networks (CNN)
to extract informative features from each visit of the longitudinal MRI and
then use those features to classify each visit via Recurrent Neural Networks
(RNNs). Such modeling neglects the progressive nature of the disease, which may
result in clinically implausible classifications across visits. To avoid this
issue, we propose to combine features across visits by coupling feature
extraction with a novel longitudinal pooling layer and enforce consistency of
the classification across visits in line with disease progression. We evaluate
the proposed method on the longitudinal structural MRIs from three neuroimaging
datasets: Alzheimer's Disease Neuroimaging Initiative (ADNI, N=404), a dataset
composed of 274 normal controls and 329 patients with Alcohol Use Disorder
(AUD), and 255 youths from the National Consortium on Alcohol and
NeuroDevelopment in Adolescence (NCANDA). In all three experiments our method
is superior to other widely used approaches for longitudinal classification
thus making a unique contribution towards more accurate tracking of the impact
of conditions on the brain. The code is available at
https://github.com/ouyangjiahong/longitudinal-pooling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ouyang_J/0/1/0/all/0/1"&gt;Jiahong Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sullivan_E/0/1/0/all/0/1"&gt;Edith V Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pfefferbaum_A/0/1/0/all/0/1"&gt;Adolf Pfefferbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tapert_S/0/1/0/all/0/1"&gt;Susan F. Tapert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Biological Locomotion in Video: A Computational Approach. (arXiv:2105.12661v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12661</id>
        <link href="http://arxiv.org/abs/2105.12661"/>
        <updated>2021-05-27T01:32:28.113Z</updated>
        <summary type="html"><![CDATA[Animals locomote for various reasons: to search for food, find suitable
habitat, pursue prey, escape from predators, or seek a mate. The grand scale of
biodiversity contributes to the great locomotory design and mode diversity.
Various creatures make use of legs, wings, fins and other means to move through
the world. In this report, we refer to the locomotion of general biological
species as biolocomotion. We present a computational approach to detect
biolocomotion in unprocessed video.

Significantly, the motion exhibited by the body parts of a biological entity
to navigate through an environment can be modeled by a combination of an
overall positional advance with an overlaid asymmetric oscillatory pattern, a
distinctive signature that tends to be absent in non-biological objects in
locomotion. We exploit this key trait of positional advance with asymmetric
oscillation along with differences in an object's common motion (extrinsic
motion) and localized motion of its parts (intrinsic motion) to detect
biolocomotion. An algorithm is developed to measure the presence of these
traits in tracked objects to determine if they correspond to a biological
entity in locomotion. An alternative algorithm, based on generic features
combined with learning is assembled out of components from allied areas of
investigation, also is presented as a basis of comparison.

A novel biolocomotion dataset encompassing a wide range of moving biological
and non-biological objects in natural settings is provided. Also, biolocomotion
annotations to an extant camouflage animals dataset are provided. Quantitative
results indicate that the proposed algorithm considerably outperforms the
alternative approach, supporting the hypothesis that biolocomotion can be
detected reliably based on its distinct signature of positional advance with
asymmetric oscillation and extrinsic/intrinsic motion dissimilarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;Soo Min Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1"&gt;Richard P. Wildes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Resolution Information Also Matters: Learning Multi-Resolution Representations for Person Re-Identification. (arXiv:2105.12684v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12684</id>
        <link href="http://arxiv.org/abs/2105.12684"/>
        <updated>2021-05-27T01:32:28.107Z</updated>
        <summary type="html"><![CDATA[As a prevailing task in video surveillance and forensics field, person
re-identification (re-ID) aims to match person images captured from
non-overlapped cameras. In unconstrained scenarios, person images often suffer
from the resolution mismatch problem, i.e., \emph{Cross-Resolution Person
Re-ID}. To overcome this problem, most existing methods restore low resolution
(LR) images to high resolution (HR) by super-resolution (SR). However, they
only focus on the HR feature extraction and ignore the valid information from
original LR images. In this work, we explore the influence of resolutions on
feature extraction and develop a novel method for cross-resolution person re-ID
called \emph{\textbf{M}ulti-Resolution \textbf{R}epresentations \textbf{J}oint
\textbf{L}earning} (\textbf{MRJL}). Our method consists of a Resolution
Reconstruction Network (RRN) and a Dual Feature Fusion Network (DFFN). The RRN
uses an input image to construct a HR version and a LR version with an encoder
and two decoders, while the DFFN adopts a dual-branch structure to generate
person representations from multi-resolution images. Comprehensive experiments
on five benchmarks verify the superiority of the proposed MRJL over the
relevent state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weisi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandran_A/0/1/0/all/0/1"&gt;Arun Chandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_X/0/1/0/all/0/1"&gt;Xuan Jing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey. (arXiv:2105.12694v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12694</id>
        <link href="http://arxiv.org/abs/2105.12694"/>
        <updated>2021-05-27T01:32:28.101Z</updated>
        <summary type="html"><![CDATA[Weakly-Supervised Object Detection (WSOD) and Localization (WSOL), i.e.,
detecting multiple and single instances with bounding boxes in an image using
image-level labels, are long-standing and challenging tasks in the CV
community. With the success of deep neural networks in object detection, both
WSOD and WSOL have received unprecedented attention. Hundreds of WSOD and WSOL
methods and numerous techniques have been proposed in the deep learning era. To
this end, in this paper, we consider WSOL is a sub-task of WSOD and provide a
comprehensive survey of the recent achievements of WSOD. Specifically, we
firstly describe the formulation and setting of the WSOD, including the
background, challenges, basic framework. Meanwhile, we summarize and analyze
all advanced techniques and training tricks for improving detection
performance. Then, we introduce the widely-used datasets and evaluation metrics
of WSOD. Lastly, we discuss the future directions of WSOD. We believe that
these summaries can help pave a way for future research on WSOD and WSOL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_F/0/1/0/all/0/1"&gt;Feifei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1"&gt;Jian Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1"&gt;Wei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1"&gt;Shaoning Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1"&gt;Lu Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yueting Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jun Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anticipating human actions by correlating past with the future with Jaccard similarity measures. (arXiv:2105.12414v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12414</id>
        <link href="http://arxiv.org/abs/2105.12414"/>
        <updated>2021-05-27T01:32:28.095Z</updated>
        <summary type="html"><![CDATA[We propose a framework for early action recognition and anticipation by
correlating past features with the future using three novel similarity measures
called Jaccard vector similarity, Jaccard cross-correlation and Jaccard
Frobenius inner product over covariances. Using these combinations of novel
losses and using our framework, we obtain state-of-the-art results for early
action recognition in UCF101 and JHMDB datasets by obtaining 91.7 % and 83.5 %
accuracy respectively for an observation percentage of 20. Similarly, we obtain
state-of-the-art results for Epic-Kitchen55 and Breakfast datasets for action
anticipation by obtaining 20.35 and 41.8 top-1 accuracy respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1"&gt;Basura Fernando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herath_S/0/1/0/all/0/1"&gt;Samitha Herath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangled Face Attribute Editing via Instance-Aware Latent Space Search. (arXiv:2105.12660v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12660</id>
        <link href="http://arxiv.org/abs/2105.12660"/>
        <updated>2021-05-27T01:32:28.089Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that a rich set of semantic directions exist in the
latent space of Generative Adversarial Networks (GANs), which enables various
facial attribute editing applications. However, existing methods may suffer
poor attribute variation disentanglement, leading to unwanted change of other
attributes when altering the desired one. The semantic directions used by
existing methods are at attribute level, which are difficult to model complex
attribute correlations, especially in the presence of attribute distribution
bias in GAN's training set. In this paper, we propose a novel framework (IALS)
that performs Instance-Aware Latent-Space Search to find semantic directions
for disentangled attribute editing. The instance information is injected by
leveraging the supervision from a set of attribute classifiers evaluated on the
input images. We further propose a Disentanglement-Transformation (DT) metric
to quantify the attribute transformation and disentanglement efficacy and find
the optimal control factor between attribute-level and instance-specific
directions based on it. Experimental results on both GAN-generated and
real-world images collectively show that our method outperforms
state-of-the-art methods proposed recently by a wide margin. Code is available
at https://github.com/yxuhan/IALS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yuxuan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaolong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Ying Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers. (arXiv:2105.12628v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12628</id>
        <link href="http://arxiv.org/abs/2105.12628"/>
        <updated>2021-05-27T01:32:28.072Z</updated>
        <summary type="html"><![CDATA[We propose Predict then Interpolate (PI), a simple algorithm for learning
correlations that are stable across environments. The algorithm follows from
the intuition that when using a classifier trained on one environment to make
predictions on examples from another environment, its mistakes are informative
as to which correlations are unstable. In this work, we prove that by
interpolating the distributions of the correct predictions and the wrong
predictions, we can uncover an oracle distribution where the unstable
correlation vanishes. Since the oracle interpolation coefficients are not
accessible, we use group distributionally robust optimization to minimize the
worst-case risk across all such interpolations. We evaluate our method on both
text classification and image classification. Empirical results demonstrate
that our algorithm is able to learn robust classifiers (outperforms IRM by
23.85% on synthetic environments and 12.41% on natural environments). Our code
and data are available at https://github.com/YujiaBao/Predict-then-Interpolate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yujia Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Self Supervised Learning: the BT, the HSIC, and the VICReg. (arXiv:2105.12247v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12247</id>
        <link href="http://arxiv.org/abs/2105.12247"/>
        <updated>2021-05-27T01:32:28.067Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning and pre-training strategies have developed over the
last few years especially for Convolutional Neural Networks (CNNs). Recently
application of such methods can also be noticed for Graph Neural Networks
(GNNs). In this paper, we have used a graph based self-supervised learning
strategy with different loss functions (Barlow Twins[ 7], HSIC[ 4], VICReg[ 1])
which have shown promising results when applied with CNNs previously. We have
also proposed a hybrid loss function combining the advantages of VICReg and
HSIC and called it as VICRegHSIC. The performance of these aforementioned
methods have been compared when applied to two different datasets namely MUTAG
and PROTEINS. Moreover, the impact of different batch sizes, projector
dimensions and data augmentation strategies have also been explored. The
results are preliminary and we will be continuing to explore with other
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1"&gt;Sayan Nag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blurs Make Results Clearer: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness. (arXiv:2105.12639v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12639</id>
        <link href="http://arxiv.org/abs/2105.12639"/>
        <updated>2021-05-27T01:32:28.061Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks (BNNs) have shown success in the areas of
uncertainty estimation and robustness. However, a crucial challenge prohibits
their use in practice: Bayesian NNs require a large number of predictions to
produce reliable results, leading to a significant increase in computational
cost. To alleviate this issue, we propose spatial smoothing, a method that
ensembles neighboring feature map points of CNNs. By simply adding a few blur
layers to the models, we empirically show that the spatial smoothing improves
accuracy, uncertainty estimation, and robustness of BNNs across a whole range
of ensemble sizes. In particular, BNNs incorporating the spatial smoothing
achieve high predictive performance merely with a handful of ensembles.
Moreover, this method also can be applied to canonical deterministic neural
networks to improve the performances. A number of evidences suggest that the
improvements can be attributed to the smoothing and flattening of the loss
landscape. In addition, we provide a fundamental explanation for prior works -
namely, global average pooling, pre-activation, and ReLU6 - by addressing to
them as special cases of the spatial smoothing. These not only enhance
accuracy, but also improve uncertainty estimation and robustness by making the
loss landscape smoother in the same manner as the spatial smoothing. The code
is available at https://github.com/xxxnell/spatial-smoothing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1"&gt;Namuk Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Songkuk Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Calibrate Your Event Camera. (arXiv:2105.12362v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12362</id>
        <link href="http://arxiv.org/abs/2105.12362"/>
        <updated>2021-05-27T01:32:28.056Z</updated>
        <summary type="html"><![CDATA[We propose a generic event camera calibration framework using image
reconstruction. Instead of relying on blinking LED patterns or external
screens, we show that neural-network-based image reconstruction is well suited
for the task of intrinsic and extrinsic calibration of event cameras. The
advantage of our proposed approach is that we can use standard calibration
patterns that do not rely on active illumination. Furthermore, our approach
enables the possibility to perform extrinsic calibration between frame-based
and event-based sensors without additional complexity. Both simulation and
real-world experiments indicate that calibration through image reconstruction
is accurate under common distortion models and a wide variety of distortion
parameters]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muglikar_M/0/1/0/all/0/1"&gt;Manasi Muglikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1"&gt;Mathias Gehrig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1"&gt;Daniel Gehrig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1"&gt;Davide Scaramuzza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Style Similarity as Feedback for Product Design. (arXiv:2105.12256v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12256</id>
        <link href="http://arxiv.org/abs/2105.12256"/>
        <updated>2021-05-27T01:32:28.049Z</updated>
        <summary type="html"><![CDATA[Matching and recommending products is beneficial for both customers and
companies. With the rapid increase in home goods e-commerce, there is an
increasing demand for quantitative methods for providing such recommendations
for millions of products. This approach is facilitated largely by online stores
such as Amazon and Wayfair, in which the goal is to maximize overall sales.
Instead of focusing on overall sales, we take a product design perspective, by
employing big-data analysis for determining the design qualities of a highly
recommended product. Specifically, we focus on the visual style compatibility
of such products. We build off previous work which implemented a style-based
similarity metric for thousands of furniture products. Using analysis and
visualization, we extract attributes of furniture products that are highly
compatible style-wise. We propose a designer in-the-loop workflow that mirrors
methods of displaying similar products to consumers browsing e-commerce
websites. Our findings are useful when designing new products, since they
provide insight regarding what furniture will be strongly compatible across
multiple styles, and hence, more likely to be recommended.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_M/0/1/0/all/0/1"&gt;Mathew Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_T/0/1/0/all/0/1"&gt;Tomer Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ataer_Cansizoglu_E/0/1/0/all/0/1"&gt;Esra Ataer-Cansizoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jae-Woo Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What data do we need for training an AV motion planner?. (arXiv:2105.12337v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.12337</id>
        <link href="http://arxiv.org/abs/2105.12337"/>
        <updated>2021-05-27T01:32:28.044Z</updated>
        <summary type="html"><![CDATA[We investigate what grade of sensor data is required for training an
imitation-learning-based AV planner on human expert demonstration.
Machine-learned planners are very hungry for training data, which is usually
collected using vehicles equipped with the same sensors used for autonomous
operation. This is costly and non-scalable. If cheaper sensors could be used
for collection instead, data availability would go up, which is crucial in a
field where data volume requirements are large and availability is small. We
present experiments using up to 1000 hours worth of expert demonstration and
find that training with 10x lower-quality data outperforms 1x AV-grade data in
terms of planner performance. The important implication of this is that cheaper
sensors can indeed be used. This serves to improve data access and democratize
the field of imitation-based motion planning. Alongside this, we perform a
sensitivity analysis of planner performance as a function of perception range,
field-of-view, accuracy, and data volume, and the reason why lower-quality data
still provide good planning results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Platinsky_L/0/1/0/all/0/1"&gt;Lukas Platinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speichert_S/0/1/0/all/0/1"&gt;Stefanie Speichert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1"&gt;Blazej Osinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheel_O/0/1/0/all/0/1"&gt;Oliver Scheel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yawei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1"&gt;Hugo Grimmett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1"&gt;Luca del Pero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1"&gt;Peter Ondruska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhance to Read Better: An Improved Generative Adversarial Network for Handwritten Document Image Enhancement. (arXiv:2105.12710v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12710</id>
        <link href="http://arxiv.org/abs/2105.12710"/>
        <updated>2021-05-27T01:32:28.027Z</updated>
        <summary type="html"><![CDATA[Handwritten document images can be highly affected by degradation for
different reasons: Paper ageing, daily-life scenarios (wrinkles, dust, etc.),
bad scanning process and so on. These artifacts raise many readability issues
for current Handwritten Text Recognition (HTR) algorithms and severely devalue
their efficiency. In this paper, we propose an end to end architecture based on
Generative Adversarial Networks (GANs) to recover the degraded documents into a
clean and readable form. Unlike the most well-known document binarization
methods, which try to improve the visual quality of the degraded document, the
proposed architecture integrates a handwritten text recognizer that promotes
the generated document image to be more readable. To the best of our knowledge,
this is the first work to use the text information while binarizing handwritten
documents. Extensive experiments conducted on degraded Arabic and Latin
handwritten documents demonstrate the usefulness of integrating the recognizer
within the GAN architecture, which improves both the visual quality and the
readability of the degraded document images. Moreover, we outperform the state
of the art in H-DIBCO 2018 challenge, after fine tuning our pre-trained model
with synthetically degraded Latin handwritten images, on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jemni_S/0/1/0/all/0/1"&gt;Sana Khamekhem Jemni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1"&gt;Mohamed Ali Souibgui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1"&gt;Yousri Kessentini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1"&gt;Alicia Forn&amp;#xe9;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Video Summarization via Multi-source Features. (arXiv:2105.12532v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12532</id>
        <link href="http://arxiv.org/abs/2105.12532"/>
        <updated>2021-05-27T01:32:28.021Z</updated>
        <summary type="html"><![CDATA[Video summarization aims at generating a compact yet representative visual
summary that conveys the essence of the original video. The advantage of
unsupervised approaches is that they do not require human annotations to learn
the summarization capability and generalize to a wider range of domains.
Previous work relies on the same type of deep features, typically based on a
model pre-trained on ImageNet data. Therefore, we propose the incorporation of
multiple feature sources with chunk and stride fusion to provide more
information about the visual content. For a comprehensive evaluation on the two
benchmarks TVSum and SumMe, we compare our method with four state-of-the-art
approaches. Two of these approaches were implemented by ourselves to reproduce
the reported results. Our evaluation shows that we obtain state-of-the-art
results on both datasets, while also highlighting the shortcomings of previous
work with regard to the evaluation methodology. Finally, we perform error
analysis on videos for the two benchmark datasets to summarize and spot the
factors that lead to misclassifications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kanafani_H/0/1/0/all/0/1"&gt;Hussain Kanafani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghauri_J/0/1/0/all/0/1"&gt;Junaid Ahmed Ghauri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1"&gt;Sherzod Hakimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Transparent Application of Machine Learning in Video Processing. (arXiv:2105.12700v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.12700</id>
        <link href="http://arxiv.org/abs/2105.12700"/>
        <updated>2021-05-27T01:32:28.016Z</updated>
        <summary type="html"><![CDATA[Machine learning techniques for more efficient video compression and video
enhancement have been developed thanks to breakthroughs in deep learning. The
new techniques, considered as an advanced form of Artificial Intelligence (AI),
bring previously unforeseen capabilities. However, they typically come in the
form of resource-hungry black-boxes (overly complex with little transparency
regarding the inner workings). Their application can therefore be unpredictable
and generally unreliable for large-scale use (e.g. in live broadcast). The aim
of this work is to understand and optimise learned models in video processing
applications so systems that incorporate them can be used in a more trustworthy
manner. In this context, the presented work introduces principles for
simplification of learned models targeting improved transparency in
implementing machine learning for video production and distribution
applications. These principles are demonstrated on video compression examples,
showing how bitrate savings and reduced complexity can be achieved by
simplifying relevant deep learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1"&gt;Luka Murn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blanch_M/0/1/0/all/0/1"&gt;Marc Gorriz Blanch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Santamaria_M/0/1/0/all/0/1"&gt;Maria Santamaria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rivera_F/0/1/0/all/0/1"&gt;Fiona Rivera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1"&gt;Marta Mrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pattern Detection in the Activation Space for Identifying Synthesized Content. (arXiv:2105.12479v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12479</id>
        <link href="http://arxiv.org/abs/2105.12479"/>
        <updated>2021-05-27T01:32:28.010Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have recently achieved unprecedented
success in photo-realistic image synthesis from low-dimensional random noise.
The ability to synthesize high-quality content at a large scale brings
potential risks as the generated samples may lead to misinformation that can
create severe social, political, health, and business hazards. We propose
SubsetGAN to identify generated content by detecting a subset of anomalous
node-activations in the inner layers of pre-trained neural networks. These
nodes, as a group, maximize a non-parametric measure of divergence away from
the expected distribution of activations created from real data. This enable us
to identify synthesised images without prior knowledge of their distribution.
SubsetGAN efficiently scores subsets of nodes and returns the group of nodes
within the pre-trained classifier that contributed to the maximum score. The
classifier can be a general fake classifier trained over samples from multiple
sources or the discriminator network from different GANs. Our approach shows
consistently higher detection power than existing detection methods across
several state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different
proportions of generated content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1"&gt;Celia Cintas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1"&gt;Skyler Speakman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1"&gt;Girmaw Abebe Tadesse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akinwande_V/0/1/0/all/0/1"&gt;Victor Akinwande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McFowland_E/0/1/0/all/0/1"&gt;Edward McFowland III&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weldemariam_K/0/1/0/all/0/1"&gt;Komminist Weldemariam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KLIEP-based Density Ratio Estimation for Semantically Consistent Synthetic to Real Images Adaptation in Urban Traffic Scenes. (arXiv:2105.12549v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12549</id>
        <link href="http://arxiv.org/abs/2105.12549"/>
        <updated>2021-05-27T01:32:28.005Z</updated>
        <summary type="html"><![CDATA[Synthetic data has been applied in many deep learning based computer vision
tasks. Limited performance of algorithms trained solely on synthetic data has
been approached with domain adaptation techniques such as the ones based on
generative adversarial framework. We demonstrate how adversarial training alone
can introduce semantic inconsistencies in translated images. To tackle this
issue we propose density prematching strategy using KLIEP-based density ratio
estimation procedure. Finally, we show that aforementioned strategy improves
quality of translated images of underlying method and their usability for the
semantic segmentation task in the context of autonomous driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Savkin_A/0/1/0/all/0/1"&gt;Artem Savkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Advantages of Multiple Stereo Vision Camera Designs for Autonomous Drone Navigation. (arXiv:2105.12691v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.12691</id>
        <link href="http://arxiv.org/abs/2105.12691"/>
        <updated>2021-05-27T01:32:27.987Z</updated>
        <summary type="html"><![CDATA[In this work we showcase the design and assessment of the performance of a
multi-camera UAV, when coupled with state-of-the-art planning and mapping
algorithms for autonomous navigation. The system leverages state-of-the-art
receding horizon exploration techniques for Next-Best-View (NBV) planning with
3D and semantic information, provided by a reconfigurable multi stereo camera
system. We employ our approaches in an autonomous drone-based inspection task
and evaluate them in an autonomous exploration and mapping scenario. We discuss
the advantages and limitations of using multi stereo camera flying systems, and
the trade-off between number of cameras and mapping performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Figueiredo_R/0/1/0/all/0/1"&gt;Rui Pimentel de Figueiredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_J/0/1/0/all/0/1"&gt;Jakob Grimm Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fevre_J/0/1/0/all/0/1"&gt;Jonas Le Fevre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brandao_M/0/1/0/all/0/1"&gt;Martim Brand&amp;#xe3;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kayacan_E/0/1/0/all/0/1"&gt;Erdal Kayacan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smile Like You Mean It: Driving Animatronic Robotic Face with Learned Models. (arXiv:2105.12724v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.12724</id>
        <link href="http://arxiv.org/abs/2105.12724"/>
        <updated>2021-05-27T01:32:27.982Z</updated>
        <summary type="html"><![CDATA[Ability to generate intelligent and generalizable facial expressions is
essential for building human-like social robots. At present, progress in this
field is hindered by the fact that each facial expression needs to be
programmed by humans. In order to adapt robot behavior in real time to
different situations that arise when interacting with human subjects, robots
need to be able to train themselves without requiring human labels, as well as
make fast action decisions and generalize the acquired knowledge to diverse and
new contexts. We addressed this challenge by designing a physical animatronic
robotic face with soft skin and by developing a vision-based self-supervised
learning framework for facial mimicry. Our algorithm does not require any
knowledge of the robot's kinematic model, camera calibration or predefined
expression set. By decomposing the learning process into a generative model and
an inverse model, our framework can be trained using a single motor babbling
dataset. Comprehensive evaluations show that our method enables accurate and
diverse face mimicry across diverse human subjects. The project website is at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Boyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yuhang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lianfeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1"&gt;Sara Cummings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1"&gt;Hod Lipson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning. (arXiv:2105.12722v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12722</id>
        <link href="http://arxiv.org/abs/2105.12722"/>
        <updated>2021-05-27T01:32:27.975Z</updated>
        <summary type="html"><![CDATA[The objective of this work is to segment any arbitrary structures of interest
(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D
segmentation). We show that high accuracy can be achieved by simply propagating
the 2D slice segmentation with an affinity matrix between consecutive slices,
which can be learnt in a self-supervised manner, namely slice reconstruction.
Specifically, we compare the proposed framework, termed as Sli2Vol, with
supervised approaches and two other unsupervised/ self-supervised slice
registration approaches, on 8 public datasets (both CT and MRI scans), spanning
9 different SOIs. Without any parameter-tuning, the same model achieves
superior performance with Dice scores (0-100 scale) of over 80 for most of the
benchmarks, including the ones that are unseen during training. Our results
show generalizability of the proposed approach across data from different
machines and with different SOIs: a major use case of semi-automatic
segmentation methods where fully supervised approaches would normally struggle.
The source code will be made publicly available at
https://github.com/pakheiyeung/Sli2Vol.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_P/0/1/0/all/0/1"&gt;Pak-Hei Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1"&gt;Ana I.L. Namburete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weidi Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling. (arXiv:2105.12441v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12441</id>
        <link href="http://arxiv.org/abs/2105.12441"/>
        <updated>2021-05-27T01:32:27.969Z</updated>
        <summary type="html"><![CDATA[Since 2014 transfer learning has become the key driver for the improvement of
spatial saliency prediction; however, with stagnant progress in the last 3-5
years. We conduct a large-scale transfer learning study which tests different
ImageNet backbones, always using the same read out architecture and learning
protocol adopted from DeepGaze II. By replacing the VGG19 backbone of DeepGaze
II with ResNet50 features we improve the performance on saliency prediction
from 78% to 85%. However, as we continue to test better ImageNet models as
backbones (such as EfficientNetB5) we observe no additional improvement on
saliency prediction. By analyzing the backbones further, we find that
generalization to other datasets differs substantially, with models being
consistently overconfident in their fixation predictions. We show that by
combining multiple backbones in a principled manner a good confidence
calibration on unseen datasets can be achieved. This yields a significant leap
in benchmark performance in and out-of-domain with a 15 percent point
improvement over DeepGaze II to 93% on MIT1003, marking a new state of the art
on the MIT/Tuebingen Saliency Benchmark in all available metrics (AUC: 88.3%,
sAUC: 79.4%, CC: 82.4%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kummerer_M/0/1/0/all/0/1"&gt;Matthias K&amp;#xfc;mmerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1"&gt;Ori Press&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social-IWSTCNN: A Social Interaction-Weighted Spatio-Temporal Convolutional Neural Network for Pedestrian Trajectory Prediction in Urban Traffic Scenarios. (arXiv:2105.12436v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12436</id>
        <link href="http://arxiv.org/abs/2105.12436"/>
        <updated>2021-05-27T01:32:27.963Z</updated>
        <summary type="html"><![CDATA[Pedestrian trajectory prediction in urban scenarios is essential for
automated driving. This task is challenging because the behavior of pedestrians
is influenced by both their own history paths and the interactions with others.
Previous research modeled these interactions with pooling mechanisms or
aggregating with hand-crafted attention weights. In this paper, we present the
Social Interaction-Weighted Spatio-Temporal Convolutional Neural Network
(Social-IWSTCNN), which includes both the spatial and the temporal features. We
propose a novel design, namely the Social Interaction Extractor, to learn the
spatial and social interaction features of pedestrians. Most previous works
used ETH and UCY datasets which include five scenes but do not cover urban
traffic scenarios extensively for training and evaluation. In this paper, we
use the recently released large-scale Waymo Open Dataset in urban traffic
scenarios, which includes 374 urban training scenes and 76 urban testing scenes
to analyze the performance of our proposed algorithm in comparison to the
state-of-the-art (SOTA) models. The results show that our algorithm outperforms
SOTA algorithms such as Social-LSTM, Social-GAN, and Social-STGCNN on both
Average Displacement Error (ADE) and Final Displacement Error (FDE).
Furthermore, our Social-IWSTCNN is 54.8 times faster in data pre-processing
speed, and 4.7 times faster in total test speed than the current best SOTA
algorithm Social-STGCNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1"&gt;Christian Berger&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Dozza_M/0/1/0/all/0/1"&gt;Marco Dozza&lt;/a&gt; (2) ((1) Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden, (2) Department of Maritime Sciences and Mechanics, Chalmers University of Technology, Gothenburg, Sweden)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FINNger -- Applying artificial intelligence to ease math learning for children. (arXiv:2105.12281v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12281</id>
        <link href="http://arxiv.org/abs/2105.12281"/>
        <updated>2021-05-27T01:32:27.948Z</updated>
        <summary type="html"><![CDATA[Kids have an amazing capacity to use modern electronic devices such as
tablets, smartphones, etc. This has been incredibly boosted by the ease of
access of these devices given the expansion of such devices through the world,
reaching even third world countries. Also, it is well known that children tend
to have difficulty learning some subjects at pre-school. We as a society focus
extensively on alphabetization, but in the end, children end up having
differences in another essential area: Mathematics. With this work, we create
the basis for an intuitive application that could join the fact that children
have a lot of ease when using such technological applications, trying to shrink
the gap between a fun and enjoyable activity with something that will improve
the children knowledge and ability to understand concepts when in a low age, by
using a novel convolutional neural network to achieve so, named FINNger.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Audibert_R/0/1/0/all/0/1"&gt;Rafael Baldasso Audibert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maschio_V/0/1/0/all/0/1"&gt;Vinicius Marinho Maschio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting invasive ductal carcinoma using a Reinforcement Sample Learning Strategy using Deep Learning. (arXiv:2105.12564v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12564</id>
        <link href="http://arxiv.org/abs/2105.12564"/>
        <updated>2021-05-27T01:32:27.943Z</updated>
        <summary type="html"><![CDATA[Invasive ductal carcinoma is a prevalent, potentially deadly disease
associated with a high rate of morbidity and mortality. Its malignancy is the
second leading cause of death from cancer in women. The mammogram is an
extremely useful resource for mass detection and invasive ductal carcinoma
diagnosis. We are proposing a method for Invasive ductal carcinoma that will
use convolutional neural networks (CNN) on mammograms to assist radiologists in
diagnosing the disease. Due to the varying image clarity and structure of
certain mammograms, it is difficult to observe major cancer characteristics
such as microcalcification and mass, and it is often difficult to interpret and
diagnose these attributes. The aim of this study is to establish a novel method
for fully automated feature extraction and classification in invasive ductal
carcinoma computer-aided diagnosis (CAD) systems. This article presents a tumor
classification algorithm that makes novel use of convolutional neural networks
on breast mammogram images to increase feature extraction and training speed.
The algorithm makes two contributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1"&gt;Rushabh Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling. (arXiv:2104.06967v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06967</id>
        <link href="http://arxiv.org/abs/2104.06967"/>
        <updated>2021-05-27T01:32:27.936Z</updated>
        <summary type="html"><![CDATA[A vital step towards the widespread adoption of neural retrieval models is
their resource efficiency throughout the training, indexing and query
workflows. The neural IR community made great advancements in training
effective dual-encoder dense retrieval (DR) models recently. A dense text
retrieval model uses a single vector representation per query and passage to
score a match, which enables low-latency first stage retrieval with a nearest
neighbor search. Increasingly common, training approaches require enormous
compute power, as they either conduct negative passage sampling out of a
continuously updating refreshing index or require very large batch sizes for
in-batch negative sampling. Instead of relying on more compute capability, we
introduce an efficient topic-aware query and balanced margin sampling
technique, called TAS-Balanced. We cluster queries once before training and
sample queries out of a cluster per batch. We train our lightweight 6-layer DR
model with a novel dual-teacher supervision that combines pairwise and in-batch
negative teachers. Our method is trainable on a single consumer-grade GPU in
under 48 hours (as opposed to a common configuration of 8x V100s). We show that
our TAS-Balanced training method achieves state-of-the-art low-latency (64ms
per query) results on two TREC Deep Learning Track query sets. Evaluated on
NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by
11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces
the first dense retriever that outperforms every other method on recall at any
cutoff on TREC-DL and allows more resource intensive re-ranking models to
operate on fewer passages to improve results further.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1"&gt;Sebastian Hofst&amp;#xe4;tter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Sheng-Chieh Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jheng-Hong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1"&gt;Allan Hanbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial robustness against multiple $l_p$-threat models at the price of one and how to quickly fine-tune robust models to another threat model. (arXiv:2105.12508v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12508</id>
        <link href="http://arxiv.org/abs/2105.12508"/>
        <updated>2021-05-27T01:32:27.929Z</updated>
        <summary type="html"><![CDATA[Adversarial training (AT) in order to achieve adversarial robustness wrt
single $l_p$-threat models has been discussed extensively. However, for
safety-critical systems adversarial robustness should be achieved wrt all
$l_p$-threat models simultaneously. In this paper we develop a simple and
efficient training scheme to achieve adversarial robustness against the union
of $l_p$-threat models. Our novel $l_1+l_\infty$-AT scheme is based on
geometric considerations of the different $l_p$-balls and costs as much as
normal adversarial training against a single $l_p$-threat model. Moreover, we
show that using our $l_1+l_\infty$-AT scheme one can fine-tune with just 3
epochs any $l_p$-robust model (for $p \in \{1,2,\infty\}$) and achieve multiple
norm adversarial robustness. In this way we boost the previous state-of-the-art
reported for multiple-norm robustness by more than $6\%$ on CIFAR-10 and report
up to our knowledge the first ImageNet models with multiple norm robustness.
Moreover, we study the general transfer of adversarial robustness between
different threat models and in this way boost the previous SOTA
$l_1$-robustness on CIFAR-10 by almost $10\%$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1"&gt;Francesco Croce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DialogSum: A Real-Life Scenario Dialogue Summarization Dataset. (arXiv:2105.06762v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06762</id>
        <link href="http://arxiv.org/abs/2105.06762"/>
        <updated>2021-05-27T01:32:27.923Z</updated>
        <summary type="html"><![CDATA[Proposal of large-scale datasets has facilitated research on deep neural
models for news summarization. Deep learning can also be potentially useful for
spoken dialogue summarization, which can benefit a range of real-life scenarios
including customer service management and medication tracking. To this end, we
propose DialogSum, a large-scale labeled dialogue summarization dataset. We
conduct empirical analysis on DialogSum using state-of-the-art neural
summarizers. Experimental results show unique challenges in dialogue
summarization, such as spoken terms, special discourse structures, coreferences
and ellipsis, pragmatics and social commonsense, which require specific
representation learning technologies to better deal with.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yulong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Domain Experts Collaborative Learning: Multi-Source Domain Generalization For Person Re-Identification. (arXiv:2105.12355v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12355</id>
        <link href="http://arxiv.org/abs/2105.12355"/>
        <updated>2021-05-27T01:32:27.917Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed significant progress in person re-identification
(ReID). However, current ReID approaches suffer from considerable performance
degradation when the test target domains exhibit different characteristics from
the training ones, known as the domain shift problem. To make ReID more
practical and generalizable, we formulate person re-identification as a Domain
Generalization (DG) problem and propose a novel training framework, named
Multiple Domain Experts Collaborative Learning (MD-ExCo). Specifically, the
MD-ExCo consists of a universal expert and several domain experts. Each domain
expert focuses on learning from a specific domain, and periodically
communicates with other domain experts to regulate its learning strategy in the
meta-learning manner to avoid overfitting. Besides, the universal expert
gathers knowledge from the domain experts, and also provides supervision to
them as feedback. Extensive experiments on DG-ReID benchmarks show that our
MD-ExCo outperforms the state-of-the-art methods by a large margin, showing its
ability to improve the generalization capability of the ReID models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shijie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dapeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1"&gt;Rui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haobin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shixiang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jinguo Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Detect Fortified Areas. (arXiv:2105.12385v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12385</id>
        <link href="http://arxiv.org/abs/2105.12385"/>
        <updated>2021-05-27T01:32:27.901Z</updated>
        <summary type="html"><![CDATA[High resolution data models like grid terrain models made from LiDAR data are
a prerequisite for modern day Geographic Information Systems applications.
Besides providing the foundation for the very accurate digital terrain models,
LiDAR data is also extensively used to classify which parts of the considered
surface comprise relevant elements like water, buildings and vegetation. In
this paper we consider the problem of classifying which areas of a given
surface are fortified by for instance, roads, sidewalks, parking spaces, paved
driveways and terraces. We consider using LiDAR data and orthophotos, combined
and alone, to show how well the modern machine learning algorithms Gradient
Boosted Trees and Convolutional Neural Networks are able to detect fortified
areas on large real world data. The LiDAR data features, in particular the
intensity feature that measures the signal strength of the return, that we
consider in this project are heavily dependent on the actual LiDAR sensor that
made the measurement. This is highly problematic, in particular for the
generalisation capability of pattern matching algorithms, as this means that
data features for test data may be very different from the data the model is
trained on. We propose an algorithmic solution to this problem by designing a
neural net embedding architecture that transforms data from all the different
sensor systems into a new common representation that works as well as if the
training data and test data originated from the same sensor. The final
algorithm result has an accuracy above 96 percent, and an AUC score above 0.99.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gronlund_A/0/1/0/all/0/1"&gt;Allan Gr&amp;#xf8;nlund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tranberg_J/0/1/0/all/0/1"&gt;Jonas Tranberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimNet: Learning Reactive Self-driving Simulations from Real-world Observations. (arXiv:2105.12332v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.12332</id>
        <link href="http://arxiv.org/abs/2105.12332"/>
        <updated>2021-05-27T01:32:27.895Z</updated>
        <summary type="html"><![CDATA[In this work, we present a simple end-to-end trainable machine learning
system capable of realistically simulating driving experiences. This can be
used for the verification of self-driving system performance without relying on
expensive and time-consuming road testing. In particular, we frame the
simulation problem as a Markov Process, leveraging deep neural networks to
model both state distribution and transition function. These are trainable
directly from the existing raw observations without the need for any
handcrafting in the form of plant or kinematic models. All that is needed is a
dataset of historical traffic episodes. Our formulation allows the system to
construct never seen scenes that unfold realistically reacting to the
self-driving car's behaviour. We train our system directly from 1,000 hours of
driving logs and measure both realism, reactivity of the simulation as the two
key properties of the simulation. At the same time, we apply the method to
evaluate the performance of a recently proposed state-of-the-art ML planning
system trained from human driving logs. We discover this planning system is
prone to previously unreported causal confusion issues that are difficult to
test by non-reactive simulation. To the best of our knowledge, this is the
first work that directly merges highly realistic data-driven simulations with a
closed-loop evaluation for self-driving vehicles. We make the data, code, and
pre-trained models publicly available to further stimulate simulation
development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bergamini_L/0/1/0/all/0/1"&gt;Luca Bergamini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yawei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheel_O/0/1/0/all/0/1"&gt;Oliver Scheel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chih Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1"&gt;Luca Del Pero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1"&gt;Blazej Osinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1"&gt;Hugo Grimmett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1"&gt;Peter Ondruska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Permutation invariance and uncertainty in multitemporal image super-resolution. (arXiv:2105.12409v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.12409</id>
        <link href="http://arxiv.org/abs/2105.12409"/>
        <updated>2021-05-27T01:32:27.889Z</updated>
        <summary type="html"><![CDATA[Recent advances have shown how deep neural networks can be extremely
effective at super-resolving remote sensing imagery, starting from a
multitemporal collection of low-resolution images. However, existing models
have neglected the issue of temporal permutation, whereby the temporal ordering
of the input images does not carry any relevant information for the
super-resolution task and causes such models to be inefficient with the, often
scarce, ground truth data that available for training. Thus, models ought not
to learn feature extractors that rely on temporal ordering. In this paper, we
show how building a model that is fully invariant to temporal permutation
significantly improves performance and data efficiency. Moreover, we study how
to quantify the uncertainty of the super-resolved image so that the final user
is informed on the local quality of the product. We show how uncertainty
correlates with temporal variation in the series, and how quantifying it
further improves model performance. Experiments on the Proba-V challenge
dataset show significant improvements over the state of the art without the
need for self-ensembling, as well as improved data efficiency, reaching the
performance of the challenge winner with just 25% of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Valsesia_D/0/1/0/all/0/1"&gt;Diego Valsesia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Magli_E/0/1/0/all/0/1"&gt;Enrico Magli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Sign Language Translation with Monolingual Data by Sign Back-Translation. (arXiv:2105.12397v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12397</id>
        <link href="http://arxiv.org/abs/2105.12397"/>
        <updated>2021-05-27T01:32:27.882Z</updated>
        <summary type="html"><![CDATA[Despite existing pioneering works on sign language translation (SLT), there
is a non-trivial obstacle, i.e., the limited quantity of parallel sign-text
data. To tackle this parallel data bottleneck, we propose a sign
back-translation (SignBT) approach, which incorporates massive spoken language
texts into SLT training. With a text-to-gloss translation model, we first
back-translate the monolingual text to its gloss sequence. Then, the paired
sign sequence is generated by splicing pieces from an estimated gloss-to-sign
bank at the feature level. Finally, the synthetic parallel data serves as a
strong supplement for the end-to-end training of the encoder-decoder SLT
framework.

To promote the SLT research, we further contribute CSL-Daily, a large-scale
continuous SLT dataset. It provides both spoken language translations and
gloss-level annotations. The topic revolves around people's daily lives (e.g.,
travel, shopping, medical care), the most likely SLT application scenario.
Extensive experimental results and analysis of SLT methods are reported on
CSL-Daily. With the proposed sign back-translation method, we obtain a
substantial improvement over previous state-of-the-art SLT methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wengang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1"&gt;Weizhen Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1"&gt;Junfu Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Private Recommender Systems: How Can Users Build Their Own Fair Recommender Systems without Log Data?. (arXiv:2105.12353v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.12353</id>
        <link href="http://arxiv.org/abs/2105.12353"/>
        <updated>2021-05-27T01:32:27.868Z</updated>
        <summary type="html"><![CDATA[Fairness is an important property in data-mining applications, including
recommender systems. In this work, we investigate a case where users of a
recommender system need (or want) to be fair to a protected group of items. For
example, in a job market, the user is the recruiter, an item is the job seeker,
and the protected attribute is gender or race. Even if recruiters want to use a
fair talent recommender system, the platform may not provide a fair recommender
system, or recruiters may not be able to ascertain whether the recommender
system's algorithm is fair. In this case, recruiters cannot utilize the
recommender system, or they may become unfair to job seekers. In this work, we
propose methods to enable the users to build their own fair recommender
systems. Our methods can generate fair recommendations even when the platform
does not (or cannot) provide fair recommender systems. The key challenge is
that a user does not have access to the log data of other users or the latent
representations of items. This restriction prohibits us from adopting existing
methods, which are designed for platforms. The main idea is that a user has
access to unfair recommendations provided by the platform. Our methods leverage
the outputs of an unfair recommender system to construct a new fair recommender
system. We empirically validate that our proposed method improves fairness
substantially without harming much performance of the original unfair system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1"&gt;Ryoma Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SB-GCN: Structured BREP Graph Convolutional Network for Automatic Mating of CAD Assemblies. (arXiv:2105.12238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12238</id>
        <link href="http://arxiv.org/abs/2105.12238"/>
        <updated>2021-05-27T01:32:27.861Z</updated>
        <summary type="html"><![CDATA[Assembly modeling is a core task of computer aided design (CAD), comprising
around one third of the work in a CAD workflow. Optimizing this process
therefore represents a huge opportunity in the design of a CAD system, but
current research of assembly based modeling is not directly applicable to
modern CAD systems because it eschews the dominant data structure of modern
CAD: parametric boundary representations (BREPs). CAD assembly modeling defines
assemblies as a system of pairwise constraints, called mates, between parts,
which are defined relative to BREP topology rather than in world coordinates
common to existing work. We propose SB-GCN, a representation learning scheme on
BREPs that retains the topological structure of parts, and use these learned
representations to predict CAD type mates. To train our system, we compiled the
first large scale dataset of BREP CAD assemblies, which we are releasing along
with benchmark mate prediction tasks. Finally, we demonstrate the compatibility
of our model with an existing commercial CAD system by building a tool that
assists users in mate creation by suggesting mate completions, with 72.2%
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_B/0/1/0/all/0/1"&gt;Benjamin Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hildreth_D/0/1/0/all/0/1"&gt;Dalton Hildreth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Duowen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baran_I/0/1/0/all/0/1"&gt;Ilya Baran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1"&gt;Vova Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1"&gt;Adriana Schulz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance Analysis of a Foreground Segmentation Neural Network Model. (arXiv:2105.12311v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12311</id>
        <link href="http://arxiv.org/abs/2105.12311"/>
        <updated>2021-05-27T01:32:27.855Z</updated>
        <summary type="html"><![CDATA[In recent years the interest in segmentation has been growing, being used in
a wide range of applications such as fraud detection, anomaly detection in
public health and intrusion detection. We present an ablation study of
FgSegNet_v2, analysing its three stages: (i) Encoder, (ii) Feature Pooling
Module and (iii) Decoder. The result of this study is a proposal of a variation
of the aforementioned method that surpasses state of the art results. Three
datasets are used for testing: CDNet2014, SBI2015 and CityScapes. In CDNet2014
we got an overall improvement compared to the state of the art, mainly in the
LowFrameRate subset. The presented approach is promising as it produces
comparable results with the state of the art (SBI2015 and Cityscapes datasets)
in very different conditions, such as different lighting conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morais_J/0/1/0/all/0/1"&gt;Joel Tom&amp;#xe1;s Morais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_A/0/1/0/all/0/1"&gt;Ant&amp;#xf3;nio Ramires Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Leite Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faria_B/0/1/0/all/0/1"&gt;Bruno Faria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Guided Instance-Aware Network for Depth Completion and Enhancement. (arXiv:2105.12186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12186</id>
        <link href="http://arxiv.org/abs/2105.12186"/>
        <updated>2021-05-27T01:32:27.849Z</updated>
        <summary type="html"><![CDATA[Depth completion aims at inferring a dense depth image from sparse depth
measurement since glossy, transparent or distant surface cannot be scanned
properly by the sensor. Most of existing methods directly interpolate the
missing depth measurements based on pixel-wise image content and the
corresponding neighboring depth values. Consequently, this leads to blurred
boundaries or inaccurate structure of object. To address these problems, we
propose a novel self-guided instance-aware network (SG-IANet) that: (1) utilize
self-guided mechanism to extract instance-level features that is needed for
depth restoration, (2) exploit the geometric and context information into
network learning to conform to the underlying constraints for edge clarity and
structure consistency, (3) regularize the depth estimation and mitigate the
impact of noise by instance-aware learning, and (4) train with synthetic data
only by domain randomization to bridge the reality gap. Extensive experiments
on synthetic and real world dataset demonstrate that our proposed method
outperforms previous works. Further ablation studies give more insights into
the proposed method and demonstrate the generalization capability of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhongzhen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fengjia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1"&gt;Guoyi Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiajie Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Model-Driven Variational Network for Deformable Image Registration. (arXiv:2105.12227v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12227</id>
        <link href="http://arxiv.org/abs/2105.12227"/>
        <updated>2021-05-27T01:32:27.843Z</updated>
        <summary type="html"><![CDATA[Data-driven deep learning approaches to image registration can be less
accurate than conventional iterative approaches, especially when training data
is limited. To address this whilst retaining the fast inference speed of deep
learning, we propose VR-Net, a novel cascaded variational network for
unsupervised deformable image registration. Using the variable splitting
optimization scheme, we first convert the image registration problem,
established in a generic variational framework, into two sub-problems, one with
a point-wise, closed-form solution while the other one is a denoising problem.
We then propose two neural layers (i.e. warping layer and intensity consistency
layer) to model the analytical solution and a residual U-Net to formulate the
denoising problem (i.e. generalized denoising layer). Finally, we cascade the
warping layer, intensity consistency layer, and generalized denoising layer to
form the VR-Net. Extensive experiments on three (two 2D and one 3D) cardiac
magnetic resonance imaging datasets show that VR-Net outperforms
state-of-the-art deep learning methods on registration accuracy, while
maintains the fast inference speed of deep learning and the data-efficiency of
variational model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xi Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thorley_A/0/1/0/all/0/1"&gt;Alexander Thorley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1"&gt;Huaqi Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Linlin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Styles_I/0/1/0/all/0/1"&gt;Iain B Styles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hyung Jin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ales Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marvao_A/0/1/0/all/0/1"&gt;Antonio de Marvao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ORegan_D/0/1/0/all/0/1"&gt;Declan P. O&amp;#x27;Regan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinming Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Occlusion Aware Kernel Correlation Filter Tracker using RGB-D. (arXiv:2105.12161v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12161</id>
        <link href="http://arxiv.org/abs/2105.12161"/>
        <updated>2021-05-27T01:32:27.836Z</updated>
        <summary type="html"><![CDATA[Unlike deep learning which requires large training datasets, correlation
filter-based trackers like Kernelized Correlation Filter (KCF) uses implicit
properties of tracked images (circulant matrices) for training in real-time.
Despite their practical application in tracking, a need for a better
understanding of the fundamentals associated with KCF in terms of
theoretically, mathematically, and experimentally exists. This thesis first
details the workings prototype of the tracker and investigates its
effectiveness in real-time applications and supporting visualizations. We
further address some of the drawbacks of the tracker in cases of occlusions,
scale changes, object rotation, out-of-view and model drift with our novel
RGB-D Kernel Correlation tracker. We also study the use of particle filters to
improve trackers' accuracy. Our results are experimentally evaluated using a)
standard dataset and b) real-time using the Microsoft Kinect V2 sensor. We
believe this work will set the basis for a better understanding of the
effectiveness of kernel-based correlation filter trackers and to further define
some of its possible advantages in tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1"&gt;Srishti Yadav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSGAN++: Robust Detail-Preserving Makeup Transfer and Removal. (arXiv:2105.12324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12324</id>
        <link href="http://arxiv.org/abs/2105.12324"/>
        <updated>2021-05-27T01:32:27.826Z</updated>
        <summary type="html"><![CDATA[In this paper, we address the makeup transfer and removal tasks
simultaneously, which aim to transfer the makeup from a reference image to a
source image and remove the makeup from the with-makeup image respectively.
Existing methods have achieved much advancement in constrained scenarios, but
it is still very challenging for them to transfer makeup between images with
large pose and expression differences, or handle makeup details like blush on
cheeks or highlight on the nose. In addition, they are hardly able to control
the degree of makeup during transferring or to transfer a specified part in the
input face. In this work, we propose the PSGAN++, which is capable of
performing both detail-preserving makeup transfer and effective makeup removal.
For makeup transfer, PSGAN++ uses a Makeup Distill Network to extract makeup
information, which is embedded into spatial-aware makeup matrices. We also
devise an Attentive Makeup Morphing module that specifies how the makeup in the
source image is morphed from the reference image, and a makeup detail loss to
supervise the model within the selected makeup detail area. On the other hand,
for makeup removal, PSGAN++ applies an Identity Distill Network to embed the
identity information from with-makeup images into identity matrices. Finally,
the obtained makeup/identity matrices are fed to a Style Transfer Network that
is able to edit the feature maps to achieve makeup transfer or removal. To
evaluate the effectiveness of our PSGAN++, we collect a Makeup Transfer In the
Wild dataset that contains images with diverse poses and expressions and a
Makeup Transfer High-Resolution dataset that contains high-resolution images.
Experiments demonstrate that PSGAN++ not only achieves state-of-the-art results
with fine makeup details even in cases of large pose/expression differences but
also can perform partial or degree-controllable makeup transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Si Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wentao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chen Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Ran He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Shuicheng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation. (arXiv:2104.05801v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05801</id>
        <link href="http://arxiv.org/abs/2104.05801"/>
        <updated>2021-05-27T01:32:27.697Z</updated>
        <summary type="html"><![CDATA[With the recent advances of open-domain story generation, the lack of
reliable automatic evaluation metrics becomes an increasingly imperative issue
that hinders the fast development of story generation. According to conducted
researches in this regard, learnable evaluation metrics have promised more
accurate assessments by having higher correlations with human judgments. A
critical bottleneck of obtaining a reliable learnable evaluation metric is the
lack of high-quality training data for classifiers to efficiently distinguish
plausible and implausible machine-generated stories. Previous works relied on
\textit{heuristically manipulated} plausible examples to mimic possible system
drawbacks such as repetition, contradiction, or irrelevant content in the text
level, which can be \textit{unnatural} and \textit{oversimplify} the
characteristics of implausible machine-generated stories. We propose to tackle
these issues by generating a more comprehensive set of implausible stories
using {\em plots}, which are structured representations of controllable factors
used to generate stories. Since these plots are compact and structured, it is
easier to manipulate them to generate text with targeted undesirable
properties, while at the same time maintain the grammatical correctness and
naturalness of the generated sentences. To improve the quality of generated
implausible stories, we further apply the adversarial filtering procedure
presented by \citet{zellers2018swag} to select a more nuanced set of
implausible texts. Experiments show that the evaluation metrics trained on our
generated data result in more reliable automatic assessments that correlate
remarkably better with human judgments compared to the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1"&gt;Sarik Ghazarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zixi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+SM_A/0/1/0/all/0/1"&gt;Akash SM&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1"&gt;Ralph Weischedel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1"&gt;Aram Galstyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mind Reading at Work: Cooperation without common ground. (arXiv:2105.01949v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01949</id>
        <link href="http://arxiv.org/abs/2105.01949"/>
        <updated>2021-05-27T01:32:27.682Z</updated>
        <summary type="html"><![CDATA[As Stefan Kopp and Nicole Kramer say in their recent paper[Frontiers in
Psychology 12 (2021) 597], despite some very impressive demonstrations over the
last decade or so, we still don't know how how to make a computer have a half
decent conversation with a human. They argue that the capabilities required to
do this include incremental joint co-construction and mentalizing. Although
agreeing whole heartedly with their statement of the problem, this paper argues
for a different approach to the solution based on the "new" AI of situated
action.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1"&gt;Peter Wallis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The interplay between language similarity and script on a novel multi-layer Algerian dialect corpus. (arXiv:2105.07400v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07400</id>
        <link href="http://arxiv.org/abs/2105.07400"/>
        <updated>2021-05-27T01:32:27.667Z</updated>
        <summary type="html"><![CDATA[Recent years have seen a rise in interest for cross-lingual transfer between
languages with similar typology, and between languages of various scripts.
However, the interplay between language similarity and difference in script on
cross-lingual transfer is a less studied problem. We explore this interplay on
cross-lingual transfer for two supervised tasks, namely part-of-speech tagging
and sentiment analysis. We introduce a newly annotated corpus of Algerian
user-generated comments comprising parallel annotations of Algerian written in
Latin, Arabic, and code-switched scripts, as well as annotations for sentiment
and topic categories. We perform baseline experiments by fine-tuning
multi-lingual language models. We further explore the effect of script vs.
language similarity in cross-lingual transfer by fine-tuning multi-lingual
models on languages which are a) typologically distinct, but use the same
script, b) typologically similar, but use a distinct script, or c) are
typologically similar and use the same script. We find there is a delicate
relationship between script and typology for part-of-speech, while sentiment
analysis is less sensitive.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Touileb_S/0/1/0/all/0/1"&gt;Samia Touileb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_J/0/1/0/all/0/1"&gt;Jeremy Barnes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem. (arXiv:2012.15329v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15329</id>
        <link href="http://arxiv.org/abs/2012.15329"/>
        <updated>2021-05-27T01:32:27.661Z</updated>
        <summary type="html"><![CDATA[Car-focused navigation services are based on turns and distances of named
streets, whereas navigation instructions naturally used by humans are centered
around physical objects called landmarks. We present a neural model that takes
OpenStreetMap representations as input and learns to generate navigation
instructions that contain visible and salient landmarks from human natural
language instructions. Routes on the map are encoded in a location- and
rotation-invariant graph representation that is decoded into natural language
instructions. Our work is based on a novel dataset of 7,672 crowd-sourced
instances that have been verified by human navigation in Street View. Our
evaluation shows that the navigation instructions generated by our system have
similar properties as human-generated instructions, and lead to successful
human navigation in Street View.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schumann_R/0/1/0/all/0/1"&gt;Raphael Schumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1"&gt;Stefan Riezler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?. (arXiv:2105.04949v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04949</id>
        <link href="http://arxiv.org/abs/2105.04949"/>
        <updated>2021-05-27T01:32:27.644Z</updated>
        <summary type="html"><![CDATA[Analogies play a central role in human commonsense reasoning. The ability to
recognize analogies such as "eye is to seeing what ear is to hearing",
sometimes referred to as analogical proportions, shape how we structure
knowledge and understand language. Surprisingly, however, the task of
identifying such analogies has not yet received much attention in the language
model era. In this paper, we analyze the capabilities of transformer-based
language models on this unsupervised task, using benchmarks obtained from
educational settings, as well as more commonly used datasets. We find that
off-the-shelf language models can identify analogies to a certain extent, but
struggle with abstract and complex relations, and results are highly sensitive
to model architecture and hyperparameters. Overall the best results were
obtained with GPT-2 and RoBERTa, while configurations using BERT were not able
to outperform word embedding models. Our results raise important questions for
future work about how, and to what extent, pre-trained language models capture
knowledge about abstract semantic relations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1"&gt;Asahi Ushio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1"&gt;Luis Espinosa-Anke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1"&gt;Steven Schockaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1"&gt;Jose Camacho-Collados&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embed2Detect: Temporally Clustered Embedded Words for Event Detection in Social Media. (arXiv:2006.05908v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05908</id>
        <link href="http://arxiv.org/abs/2006.05908"/>
        <updated>2021-05-27T01:32:27.638Z</updated>
        <summary type="html"><![CDATA[Social media is becoming a primary medium to discuss what is happening around
the world. Therefore, the data generated by social media platforms contain rich
information which describes the ongoing events. Further, the timeliness
associated with these data is capable of facilitating immediate insights.
However, considering the dynamic nature and high volume of data production in
social media data streams, it is impractical to filter the events manually and
therefore, automated event detection mechanisms are invaluable to the
community. Apart from a few notable exceptions, most previous research on
automated event detection have focused only on statistical and syntactical
features in data and lacked the involvement of underlying semantics which are
important for effective information retrieval from text since they represent
the connections between words and their meanings. In this paper, we propose a
novel method termed Embed2Detect for event detection in social media by
combining the characteristics in word embeddings and hierarchical agglomerative
clustering. The adoption of word embeddings gives Embed2Detect the capability
to incorporate powerful semantical features into event detection and overcome a
major limitation inherent in previous approaches. We experimented our method on
two recent real social media data sets which represent the sports and political
domain and also compared the results to several state-of-the-art methods. The
obtained results show that Embed2Detect is capable of effective and efficient
event detection and it outperforms the recent event detection methods. For the
sports data set, Embed2Detect achieved 27% higher F-measure than the
best-performed baseline and for the political data set, it was an increase of
29%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1"&gt;Hansi Hettiarachchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adedoyin_Olowe_M/0/1/0/all/0/1"&gt;Mariam Adedoyin-Olowe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhogal_J/0/1/0/all/0/1"&gt;Jagdev Bhogal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaber_M/0/1/0/all/0/1"&gt;Mohamed Medhat Gaber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Construction of Sememe Knowledge Bases via Dictionaries. (arXiv:2105.12585v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12585</id>
        <link href="http://arxiv.org/abs/2105.12585"/>
        <updated>2021-05-27T01:32:27.627Z</updated>
        <summary type="html"><![CDATA[A sememe is defined as the minimum semantic unit in linguistics. Sememe
knowledge bases (SKBs), which comprise words annotated with sememes, enable
sememes to be applied to natural language processing. So far a large body of
research has showcased the unique advantages and effectiveness of SKBs in
various tasks. However, most languages have no SKBs, and manual construction of
SKBs is time-consuming and labor-intensive. To tackle this challenge, we
propose a simple and fully automatic method of building an SKB via an existing
dictionary. We use this method to build an English SKB and a French SKB, and
conduct comprehensive evaluations from both intrinsic and extrinsic
perspectives. Experimental results demonstrate that the automatically built
English SKB is even superior to HowNet, the most widely used SKB that takes
decades to build manually. And both the English and French SKBs can bring
obvious performance enhancement in multiple downstream tasks. All the code and
data of this paper (except the copyrighted dictionaries) can be obtained at
https://github.com/thunlp/DictSKB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yangyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fengyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing The Factual Accuracy of Generated Text. (arXiv:1905.13322v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.13322</id>
        <link href="http://arxiv.org/abs/1905.13322"/>
        <updated>2021-05-27T01:32:27.618Z</updated>
        <summary type="html"><![CDATA[We propose a model-based metric to estimate the factual accuracy of generated
text that is complementary to typical scoring schemes like ROUGE
(Recall-Oriented Understudy for Gisting Evaluation) and BLEU (Bilingual
Evaluation Understudy). We introduce and release a new large-scale dataset
based on Wikipedia and Wikidata to train relation classifiers and end-to-end
fact extraction models. The end-to-end models are shown to be able to extract
complete sets of facts from datasets with full pages of text. We then analyse
multiple models that estimate factual accuracy on a Wikipedia text
summarization task, and show their efficacy compared to ROUGE and other
model-free variants by conducting a human evaluation study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goodrich_B/0/1/0/all/0/1"&gt;Ben Goodrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1"&gt;Vinay Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1"&gt;Mohammad Saleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peter J Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Precision Hardware Architectures Meet Recommendation Model Inference at Scale. (arXiv:2105.12676v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12676</id>
        <link href="http://arxiv.org/abs/2105.12676"/>
        <updated>2021-05-27T01:32:27.606Z</updated>
        <summary type="html"><![CDATA[Tremendous success of machine learning (ML) and the unabated growth in ML
model complexity motivated many ML-specific designs in both CPU and accelerator
architectures to speed up the model inference. While these architectures are
diverse, highly optimized low-precision arithmetic is a component shared by
most. Impressive compute throughputs are indeed often exhibited by these
architectures on benchmark ML models. Nevertheless, production models such as
recommendation systems important to Facebook's personalization services are
demanding and complex: These systems must serve billions of users per month
responsively with low latency while maintaining high prediction accuracy,
notwithstanding computations with many tens of billions parameters per
inference. Do these low-precision architectures work well with our production
recommendation systems? They do. But not without significant effort. We share
in this paper our search strategies to adapt reference recommendation models to
low-precision hardware, our optimization of low-precision compute kernels, and
the design and development of tool chain so as to maintain our models' accuracy
throughout their lifespan during which topic trends and users' interests
inevitably evolve. Practicing these low-precision technologies helped us save
datacenter capacities while deploying models with up to 5X complexity that
would otherwise not be deployed on traditional general-purpose CPUs. We believe
these lessons from the trenches promote better co-design between hardware
architecture and software engineering and advance the state of the art of ML in
industry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhaoxia/0/1/0/all/0/1"&gt;Zhaoxia&lt;/a&gt; (Summer) &lt;a href="http://arxiv.org/find/cs/1/au:+Deng/0/1/0/all/0/1"&gt;Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jongsoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1"&gt;Ping Tak Peter Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haixin Liu&lt;/a&gt;, Jie (Amy) &lt;a href="http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1"&gt;Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuen_H/0/1/0/all/0/1"&gt;Hector Yuen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khudia_D/0/1/0/all/0/1"&gt;Daya Khudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiaohan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1"&gt;Ellie Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_D/0/1/0/all/0/1"&gt;Dhruv Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1"&gt;Raghuraman Krishnamoorthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Carole-Jean Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadathur_S/0/1/0/all/0/1"&gt;Satish Nadathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Changkyu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumov_M/0/1/0/all/0/1"&gt;Maxim Naumov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naghshineh_S/0/1/0/all/0/1"&gt;Sam Naghshineh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smelyanskiy_M/0/1/0/all/0/1"&gt;Mikhail Smelyanskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deception detection in text and its relation to the cultural dimension of individualism/collectivism. (arXiv:2105.12530v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12530</id>
        <link href="http://arxiv.org/abs/2105.12530"/>
        <updated>2021-05-27T01:32:27.589Z</updated>
        <summary type="html"><![CDATA[Deception detection is a task with many applications both in direct physical
and in computer-mediated communication. Our focus is on automatic deception
detection in text across cultures. We view culture through the prism of the
individualism/collectivism dimension and we approximate culture by using
country as a proxy. Having as a starting point recent conclusions drawn from
the social psychology discipline, we explore if differences in the usage of
specific linguistic features of deception across cultures can be confirmed and
attributed to norms in respect to the individualism/collectivism divide. We
also investigate if a universal feature set for cross-cultural text deception
detection tasks exists. We evaluate the predictive power of different feature
sets and approaches. We create culture/language-aware classifiers by
experimenting with a wide range of n-gram features based on phonology,
morphology and syntax, other linguistic cues like word and phoneme counts,
pronouns use, etc., and token embeddings. We conducted our experiments over 11
datasets from 5 languages i.e., English, Dutch, Russian, Spanish and Romanian,
from six countries (US, Belgium, India, Russia, Mexico and Romania), and we
applied two classification methods i.e, logistic regression and fine-tuned BERT
models. The results showed that our task is fairly complex and demanding. There
are indications that some linguistic cues of deception have cultural origins,
and are consistent in the context of diverse domains and dataset settings for
the same language. This is more evident for the usage of pronouns and the
expression of sentiment in deceptive language. The results of this work show
that the automatic deception detection across cultures and languages cannot be
handled in a unified manner, and that such approaches should be augmented with
knowledge about cultural differences and the domains of interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papantoniou_K/0/1/0/all/0/1"&gt;Katerina Papantoniou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadakos_P/0/1/0/all/0/1"&gt;Panagiotis Papadakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patkos_T/0/1/0/all/0/1"&gt;Theodore Patkos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flouris_G/0/1/0/all/0/1"&gt;Giorgos Flouris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1"&gt;Ion Androutsopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plexousakis_D/0/1/0/all/0/1"&gt;Dimitris Plexousakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Morphology Dataset and Models for Multiple Languages, from the Large to the Endangered. (arXiv:2105.12428v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12428</id>
        <link href="http://arxiv.org/abs/2105.12428"/>
        <updated>2021-05-27T01:32:27.583Z</updated>
        <summary type="html"><![CDATA[We train neural models for morphological analysis, generation and
lemmatization for morphologically rich languages. We present a method for
automatically extracting substantially large amount of training data from FSTs
for 22 languages, out of which 17 are endangered. The neural models follow the
same tagset as the FSTs in order to make it possible to use them as fallback
systems together with the FSTs. The source code, models and datasets have been
released on Zenodo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1"&gt;Mika H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Partanen_N/0/1/0/all/0/1"&gt;Niko Partanen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueter_J/0/1/0/all/0/1"&gt;Jack Rueter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1"&gt;Khalid Alnajjar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It is rotating leaders who build the swarm: social network determinants of growth for healthcare virtual communities of practice. (arXiv:2105.12659v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.12659</id>
        <link href="http://arxiv.org/abs/2105.12659"/>
        <updated>2021-05-27T01:32:27.577Z</updated>
        <summary type="html"><![CDATA[Purpose: The purpose of this paper is to identify the factors influencing the
growth of healthcare virtual communities of practice (VCoPs) through a
seven-year longitudinal study conducted using metrics from social-network and
semantic analysis. By studying online communication along the three dimensions
of social interactions (connectivity, interactivity and language use), the
authors aim to provide VCoP managers with valuable insights to improve the
success of their communities. Design/methodology/approach: Communications over
a period of seven years (April 2008 to April 2015) and between 14,000 members
of 16 different healthcare VCoPs coexisting on the same web platform were
analysed. Multilevel regression models were used to reveal the main
determinants of community growth over time. Independent variables were derived
from social network and semantic analysis measures. Findings: Results show that
structural and content-based variables predict the growth of the community.
Progressively, more people will join a community if its structure is more
centralised, leaders are more dynamic (they rotate more) and the language used
in the posts is less complex. Research limitations/implications: The available
data set included one Web platform and a limited number of control variables.
To consolidate the findings of the present study, the experiment should be
replicated on other healthcare VCoPs. Originality/value: The study provides
useful recommendations for setting up and nurturing the growth of professional
communities, considering, at the same time, the interaction patterns among the
community members, the dynamic evolution of these interactions and the use of
language. New analytical tools are presented, together with the use of
innovative interaction metrics, that can significantly influence community
growth, such as rotating leadership.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antonacci_G/0/1/0/all/0/1"&gt;G. Antonacci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_A/0/1/0/all/0/1"&gt;A. Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1"&gt;P. Gloor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Makes a Good Summary? Investigating the Focus of Automatic Summarization in an Educational Context. (arXiv:2012.07619v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07619</id>
        <link href="http://arxiv.org/abs/2012.07619"/>
        <updated>2021-05-27T01:32:27.571Z</updated>
        <summary type="html"><![CDATA[Automatic text summarization has enjoyed great progress over the last years.
However, there is little research that investigates whether the current
research focus adheres to users' needs. Importantly, these needs are dependent
on the envisioned target group of the generated summaries. One such important
target group is formed by students, due to their usage of summaries in their
study activities. For this reason, we investigate students' needs regarding
automatically generated summaries by means of a survey amongst university
students and find that the current direction of the field does not fully align
with their needs. Motivated by our findings, we formulate three groups of
implications that together help us formulate a renewed perspective on future
research on automatic summarization. First, the educational domain requires a
broader perspective on automatic summarization, beyond the approaches that are
currently the standard. We illustrate how we can expand these approaches
regarding the input material, the purpose of the summaries and their potential
format and we define requirements for datasets that can facilitate these
research directions. Second, we propose a methodology to evaluate the
usefulness of a summary based on the identified needs of a target group. Third,
in more general terms, we hope that our survey will be reused to investigate
the needs of different user groups of automatically generated summaries to
broaden our perspective even further.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1"&gt;Maartje ter Hoeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1"&gt;Julia Kiseleva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embed2Detect: Temporally Clustered Embedded Words for Event Detection in Social Media. (arXiv:2006.05908v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05908</id>
        <link href="http://arxiv.org/abs/2006.05908"/>
        <updated>2021-05-27T01:32:27.565Z</updated>
        <summary type="html"><![CDATA[Social media is becoming a primary medium to discuss what is happening around
the world. Therefore, the data generated by social media platforms contain rich
information which describes the ongoing events. Further, the timeliness
associated with these data is capable of facilitating immediate insights.
However, considering the dynamic nature and high volume of data production in
social media data streams, it is impractical to filter the events manually and
therefore, automated event detection mechanisms are invaluable to the
community. Apart from a few notable exceptions, most previous research on
automated event detection have focused only on statistical and syntactical
features in data and lacked the involvement of underlying semantics which are
important for effective information retrieval from text since they represent
the connections between words and their meanings. In this paper, we propose a
novel method termed Embed2Detect for event detection in social media by
combining the characteristics in word embeddings and hierarchical agglomerative
clustering. The adoption of word embeddings gives Embed2Detect the capability
to incorporate powerful semantical features into event detection and overcome a
major limitation inherent in previous approaches. We experimented our method on
two recent real social media data sets which represent the sports and political
domain and also compared the results to several state-of-the-art methods. The
obtained results show that Embed2Detect is capable of effective and efficient
event detection and it outperforms the recent event detection methods. For the
sports data set, Embed2Detect achieved 27% higher F-measure than the
best-performed baseline and for the political data set, it was an increase of
29%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1"&gt;Hansi Hettiarachchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adedoyin_Olowe_M/0/1/0/all/0/1"&gt;Mariam Adedoyin-Olowe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhogal_J/0/1/0/all/0/1"&gt;Jagdev Bhogal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaber_M/0/1/0/all/0/1"&gt;Mohamed Medhat Gaber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Set2setRank: Collaborative Set to Set Ranking for Implicit Feedback based Recommendation. (arXiv:2105.07377v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07377</id>
        <link href="http://arxiv.org/abs/2105.07377"/>
        <updated>2021-05-27T01:32:27.548Z</updated>
        <summary type="html"><![CDATA[As users often express their preferences with binary behavior data~(implicit
feedback), such as clicking items or buying products, implicit feedback based
Collaborative Filtering~(CF) models predict the top ranked items a user might
like by leveraging implicit user-item interaction data. For each user, the
implicit feedback is divided into two sets: an observed item set with limited
observed behaviors, and a large unobserved item set that is mixed with negative
item behaviors and unknown behaviors. Given any user preference prediction
model, researchers either designed ranking based optimization goals or relied
on negative item mining techniques for better optimization. Despite the
performance gain of these implicit feedback based models, the recommendation
results are still far from satisfactory due to the sparsity of the observed
item set for each user. To this end, in this paper, we explore the unique
characteristics of the implicit feedback and propose Set2setRank framework for
recommendation. The optimization criteria of Set2setRank are two folds: First,
we design an item to an item set comparison that encourages each observed item
from the sampled observed set is ranked higher than any unobserved item from
the sampled unobserved set. Second, we model set level comparison that
encourages a margin between the distance summarized from the observed item set
and the most "hard" unobserved item from the sampled negative set. Further, an
adaptive sampling technique is designed to implement these two goals. We have
to note that our proposed framework is model-agnostic and can be easily applied
to most recommendation prediction approaches, and is time efficient in
practice. Finally, extensive experiments on three real-world datasets
demonstrate the superiority of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Le Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1"&gt;Richang Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of detecting clinical trial elements in exploration of COVID-19 literature. (arXiv:2105.12261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12261</id>
        <link href="http://arxiv.org/abs/2105.12261"/>
        <updated>2021-05-27T01:32:27.540Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic has driven ever-greater demand for tools which enable
efficient exploration of biomedical literature. Although semi-structured
information resulting from concept recognition and detection of the defining
elements of clinical trials (e.g. PICO criteria) has been commonly used to
support literature search, the contributions of this abstraction remain poorly
understood, especially in relation to text-based retrieval. In this study, we
compare the results retrieved by a standard search engine with those filtered
using clinically-relevant concepts and their relations. With analysis based on
the annotations from the TREC-COVID shared task, we obtain quantitative as well
as qualitative insights into characteristics of relational and concept-based
literature exploration. Most importantly, we find that the relational concept
selection filters the original retrieved collection in a way that decreases the
proportion of unjudged documents and increases the precision, which means that
the user is likely to be exposed to a larger number of relevant documents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suster_S/0/1/0/all/0/1"&gt;Simon &amp;#x160;uster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1"&gt;Karin Verspoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1"&gt;Timothy Baldwin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1"&gt;Jey Han Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yepes_A/0/1/0/all/0/1"&gt;Antonio Jimeno Yepes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_D/0/1/0/all/0/1"&gt;David Martinez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otmakhova_Y/0/1/0/all/0/1"&gt;Yulia Otmakhova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger. (arXiv:2105.12400v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12400</id>
        <link href="http://arxiv.org/abs/2105.12400"/>
        <updated>2021-05-27T01:32:27.532Z</updated>
        <summary type="html"><![CDATA[Backdoor attacks are a kind of insidious security threat against machine
learning models. After being injected with a backdoor in training, the victim
model will produce adversary-specified outputs on the inputs embedded with
predesigned triggers but behave properly on normal inputs during inference. As
a sort of emergent attack, backdoor attacks in natural language processing
(NLP) are investigated insufficiently. As far as we know, almost all existing
textual backdoor attack methods insert additional contents into normal samples
as triggers, which causes the trigger-embedded samples to be detected and the
backdoor attacks to be blocked without much effort. In this paper, we propose
to use syntactic structure as the trigger in textual backdoor attacks. We
conduct extensive experiments to demonstrate that the syntactic trigger-based
attack method can achieve comparable attack performance (almost 100\% success
rate) to the insertion-based methods but possesses much higher invisibility and
stronger resistance to defenses. These results also reveal the significant
insidiousness and harmfulness of textual backdoor attacks. All the code and
data of this paper can be obtained at https://github.com/thunlp/HiddenKiller.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mukai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yangyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yasheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGPT: Semantic Graphs based Pre-training for Aspect-based Sentiment Analysis. (arXiv:2105.12305v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12305</id>
        <link href="http://arxiv.org/abs/2105.12305"/>
        <updated>2021-05-27T01:32:27.525Z</updated>
        <summary type="html"><![CDATA[Previous studies show effective of pre-trained language models for sentiment
analysis. However, most of these studies ignore the importance of sentimental
information for pre-trained models.Therefore, we fully investigate the
sentimental information for pre-trained models and enhance pre-trained language
models with semantic graphs for sentiment analysis.In particular, we introduce
Semantic Graphs based Pre-training(SGPT) using semantic graphs to obtain
synonym knowledge for aspect-sentiment pairs and similar aspect/sentiment
terms.We then optimize the pre-trained language model with the semantic
graphs.Empirical studies on several downstream tasks show that proposed model
outperforms strong pre-trained baselines. The results also show the
effectiveness of proposed semantic graphs for pre-trained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yong Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhongqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1"&gt;Rong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haihong Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Sensitive Visualization of Deep Learning Natural Language Processing Models. (arXiv:2105.12202v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12202</id>
        <link href="http://arxiv.org/abs/2105.12202"/>
        <updated>2021-05-27T01:32:27.510Z</updated>
        <summary type="html"><![CDATA[The introduction of Transformer neural networks has changed the landscape of
Natural Language Processing (NLP) during the last years. So far, none of the
visualization systems has yet managed to examine all the facets of the
Transformers. This gave us the motivation of the current work. We propose a new
NLP Transformer context-sensitive visualization method that leverages existing
NLP tools to find the most significant groups of tokens (words) that have the
greatest effect on the output, thus preserving some context from the original
text. First, we use a sentence-level dependency parser to highlight promising
word groups. The dependency parser creates a tree of relationships between the
words in the sentence. Next, we systematically remove adjacent and non-adjacent
tuples of \emph{n} tokens from the input text, producing several new texts with
those tokens missing. The resulting texts are then passed to a pre-trained BERT
model. The classification output is compared with that of the full text, and
the difference in the activation strength is recorded. The modified texts that
produce the largest difference in the target classification output neuron are
selected, and the combination of removed words are then considered to be the
most influential on the model's output. Finally, the most influential word
combinations are visualized in a heatmap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1"&gt;Andrew Dunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inkpen_D/0/1/0/all/0/1"&gt;Diana Inkpen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andonie_R/0/1/0/all/0/1"&gt;R&amp;#x103;zvan Andonie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling. (arXiv:2104.06967v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06967</id>
        <link href="http://arxiv.org/abs/2104.06967"/>
        <updated>2021-05-27T01:32:27.495Z</updated>
        <summary type="html"><![CDATA[A vital step towards the widespread adoption of neural retrieval models is
their resource efficiency throughout the training, indexing and query
workflows. The neural IR community made great advancements in training
effective dual-encoder dense retrieval (DR) models recently. A dense text
retrieval model uses a single vector representation per query and passage to
score a match, which enables low-latency first stage retrieval with a nearest
neighbor search. Increasingly common, training approaches require enormous
compute power, as they either conduct negative passage sampling out of a
continuously updating refreshing index or require very large batch sizes for
in-batch negative sampling. Instead of relying on more compute capability, we
introduce an efficient topic-aware query and balanced margin sampling
technique, called TAS-Balanced. We cluster queries once before training and
sample queries out of a cluster per batch. We train our lightweight 6-layer DR
model with a novel dual-teacher supervision that combines pairwise and in-batch
negative teachers. Our method is trainable on a single consumer-grade GPU in
under 48 hours (as opposed to a common configuration of 8x V100s). We show that
our TAS-Balanced training method achieves state-of-the-art low-latency (64ms
per query) results on two TREC Deep Learning Track query sets. Evaluated on
NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by
11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces
the first dense retriever that outperforms every other method on recall at any
cutoff on TREC-DL and allows more resource intensive re-ranking models to
operate on fewer passages to improve results further.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1"&gt;Sebastian Hofst&amp;#xe4;tter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Sheng-Chieh Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jheng-Hong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1"&gt;Allan Hanbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking. (arXiv:2105.12306v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12306</id>
        <link href="http://arxiv.org/abs/2105.12306"/>
        <updated>2021-05-27T01:32:27.487Z</updated>
        <summary type="html"><![CDATA[Chinese Spell Checking (CSC) aims to detect and correct erroneous characters
for user-generated text in the Chinese language. Most of the Chinese spelling
errors are misused semantically, phonetically or graphically similar
characters. Previous attempts noticed this phenomenon and try to use the
similarity for this task. However, these methods use either heuristics or
handcrafted confusion sets to predict the correct character. In this paper, we
propose a Chinese spell checker called ReaLiSe, by directly leveraging the
multimodal information of the Chinese characters. The ReaLiSe model tackles the
CSC task by (1) capturing the semantic, phonetic and graphic information of the
input characters, and (2) selectively mixing the information in these
modalities to predict the correct output. Experiments on the SIGHAN benchmarks
show that the proposed model outperforms strong baselines by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Heng-Da Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhongli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qingyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zizhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yunbo Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heyan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xian-Ling Mao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Medical Entity Retrieval without Annotation: Learning From Rich Knowledge Graph Semantics. (arXiv:2105.12682v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12682</id>
        <link href="http://arxiv.org/abs/2105.12682"/>
        <updated>2021-05-27T01:32:27.480Z</updated>
        <summary type="html"><![CDATA[Medical entity retrieval is an integral component for understanding and
communicating information across various health systems. Current approaches
tend to work well on specific medical domains but generalize poorly to unseen
sub-specialties. This is of increasing concern under a public health crisis as
new medical conditions and drug treatments come to light frequently. Zero-shot
retrieval is challenging due to the high degree of ambiguity and variability in
medical corpora, making it difficult to build an accurate similarity measure
between mentions and concepts. Medical knowledge graphs (KG), however, contain
rich semantics including large numbers of synonyms as well as its curated
graphical structures. To take advantage of this valuable information, we
propose a suite of learning tasks designed for training efficient zero-shot
entity retrieval models. Without requiring any human annotation, our knowledge
graph enriched architecture significantly outperforms common zero-shot
benchmarks including BM25 and Clinical BERT with 7% to 30% higher recall across
multiple major medical ontologies, such as UMLS, SNOMED, and ICD-10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1"&gt;Luyang Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winestock_C/0/1/0/all/0/1"&gt;Christopher Winestock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1"&gt;Parminder Bhatia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Giving Commands to a Self-driving Car: A Multimodal Reasoner for Visual Grounding. (arXiv:2003.08717v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08717</id>
        <link href="http://arxiv.org/abs/2003.08717"/>
        <updated>2021-05-27T01:32:27.473Z</updated>
        <summary type="html"><![CDATA[We propose a new spatial memory module and a spatial reasoner for the Visual
Grounding (VG) task. The goal of this task is to find a certain object in an
image based on a given textual query. Our work focuses on integrating the
regions of a Region Proposal Network (RPN) into a new multi-step reasoning
model which we have named a Multimodal Spatial Region Reasoner (MSRR). The
introduced model uses the object regions from an RPN as initialization of a 2D
spatial memory and then implements a multi-step reasoning process scoring each
region according to the query, hence why we call it a multimodal reasoner. We
evaluate this new model on challenging datasets and our experiments show that
our model that jointly reasons over the object regions of the image and words
of the query largely improves accuracy compared to current state-of-the-art
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deruyttere_T/0/1/0/all/0/1"&gt;Thierry Deruyttere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collell_G/0/1/0/all/0/1"&gt;Guillem Collell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1"&gt;Marie-Francine Moens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization. (arXiv:2105.12544v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12544</id>
        <link href="http://arxiv.org/abs/2105.12544"/>
        <updated>2021-05-27T01:32:27.466Z</updated>
        <summary type="html"><![CDATA[Current dialogue summarization systems usually encode the text with a number
of general semantic features (e.g., keywords and topics) to gain more powerful
dialogue modeling capabilities. However, these features are obtained via
open-domain toolkits that are dialog-agnostic or heavily relied on human
annotations. In this paper, we show how DialoGPT, a pre-trained model for
conversational response generation, can be developed as an unsupervised
dialogue annotator, which takes advantage of dialogue background knowledge
encoded in DialoGPT. We apply DialoGPT to label three types of features on two
dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non
pre-trained models as our summarizes. Experimental results show that our
proposed method can obtain remarkable improvements on both datasets and
achieves new state-of-the-art performance on the SAMSum dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xiachong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xiaocheng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1"&gt;Libo Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1"&gt;Bing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Makes a Good Summary? Investigating the Focus of Automatic Summarization in an Educational Context. (arXiv:2012.07619v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07619</id>
        <link href="http://arxiv.org/abs/2012.07619"/>
        <updated>2021-05-27T01:32:27.445Z</updated>
        <summary type="html"><![CDATA[Automatic text summarization has enjoyed great progress over the last years.
However, there is little research that investigates whether the current
research focus adheres to users' needs. Importantly, these needs are dependent
on the envisioned target group of the generated summaries. One such important
target group is formed by students, due to their usage of summaries in their
study activities. For this reason, we investigate students' needs regarding
automatically generated summaries by means of a survey amongst university
students and find that the current direction of the field does not fully align
with their needs. Motivated by our findings, we formulate three groups of
implications that together help us formulate a renewed perspective on future
research on automatic summarization. First, the educational domain requires a
broader perspective on automatic summarization, beyond the approaches that are
currently the standard. We illustrate how we can expand these approaches
regarding the input material, the purpose of the summaries and their potential
format and we define requirements for datasets that can facilitate these
research directions. Second, we propose a methodology to evaluate the
usefulness of a summary based on the identified needs of a target group. Third,
in more general terms, we hope that our survey will be reused to investigate
the needs of different user groups of automatically generated summaries to
broaden our perspective even further.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1"&gt;Maartje ter Hoeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1"&gt;Julia Kiseleva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Route via Theory-Guided Residual Network. (arXiv:2105.08279v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08279</id>
        <link href="http://arxiv.org/abs/2105.08279"/>
        <updated>2021-05-27T01:32:27.437Z</updated>
        <summary type="html"><![CDATA[The heavy traffic and related issues have always been concerns for modern
cities. With the help of deep learning and reinforcement learning, people have
proposed various policies to solve these traffic-related problems, such as
smart traffic signal control systems and taxi dispatching systems. People
usually validate these policies in a city simulator, since directly applying
them in the real city introduces real cost. However, these policies validated
in the city simulator may fail in the real city if the simulator is
significantly different from the real world. To tackle this problem, we need to
build a real-like traffic simulation system. Therefore, in this paper, we
propose to learn the human routing model, which is one of the most essential
part in the traffic simulator. This problem has two major challenges. First,
human routing decisions are determined by multiple factors, besides the common
time and distance factor. Second, current historical routes data usually covers
just a small portion of vehicles, due to privacy and device availability
issues. To address these problems, we propose a theory-guided residual network
model, where the theoretical part can emphasize the general principles for
human routing decisions (e.g., fastest route), and the residual part can
capture drivable condition preferences (e.g., local road or highway). Since the
theoretical part is composed of traditional shortest path algorithms that do
not need data to train, our residual network can learn human routing models
from limited data. We have conducted extensive experiments on multiple
real-world datasets to show the superior performance of our model, especially
with small data. Besides, we have also illustrated why our model is better at
recovering real routes through case studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guanjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenhui Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The statistical advantage of automatic NLG metrics at the system level. (arXiv:2105.12437v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12437</id>
        <link href="http://arxiv.org/abs/2105.12437"/>
        <updated>2021-05-27T01:32:27.430Z</updated>
        <summary type="html"><![CDATA[Estimating the expected output quality of generation systems is central to
NLG. This paper qualifies the notion that automatic metrics are not as good as
humans in estimating system-level quality. Statistically, humans are unbiased,
high variance estimators, while metrics are biased, low variance estimators. We
compare these estimators by their error in pairwise prediction (which
generation system is better?) using the bootstrap. Measuring this error is
complicated: predictions are evaluated against noisy, human predicted labels
instead of the ground truth, and metric predictions fluctuate based on the test
sets they were calculated on. By applying a bias-variance-noise decomposition,
we adjust this error to a noise-free, infinite test set setting. Our analysis
compares the adjusted error of metrics to humans and a derived, perfect
segment-level annotator, both of which are unbiased estimators dependent on the
number of judgments collected. In MT, we identify two settings where metrics
outperform humans due to a statistical advantage in variance: when the number
of human judgments used is small, and when the quality difference between
compared systems is small. The data and code to reproduce our analyses are
available at https://github.com/johntzwei/metric-statistical-advantage .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Johnny Tian-Zheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Robin Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weight Distillation: Transferring the Knowledge in Neural Network Parameters. (arXiv:2009.09152v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09152</id>
        <link href="http://arxiv.org/abs/2009.09152"/>
        <updated>2021-05-27T01:32:27.423Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation has been proven to be effective in model acceleration
and compression. It allows a small network to learn to generalize in the same
way as a large network. Recent successes in pre-training suggest the
effectiveness of transferring model parameters. Inspired by this, we
investigate methods of model acceleration and compression in another line of
research. We propose Weight Distillation to transfer the knowledge in the large
network parameters through a parameter generator. Our experiments on WMT16
En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks show that weight
distillation can train a small network that is 1.88~2.94x faster than the large
network but with competitive performance. With the same sized small network,
weight distillation can outperform knowledge distillation by 0.51~1.82 BLEU
points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Ye Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1"&gt;Quan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Tong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jingbo Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word Embedding Transformation for Robust Unsupervised Bilingual Lexicon Induction. (arXiv:2105.12297v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12297</id>
        <link href="http://arxiv.org/abs/2105.12297"/>
        <updated>2021-05-27T01:32:27.416Z</updated>
        <summary type="html"><![CDATA[Great progress has been made in unsupervised bilingual lexicon induction
(UBLI) by aligning the source and target word embeddings independently trained
on monolingual corpora. The common assumption of most UBLI models is that the
embedding spaces of two languages are approximately isomorphic. Therefore the
performance is bound by the degree of isomorphism, especially on etymologically
and typologically distant languages. To address this problem, we propose a
transformation-based method to increase the isomorphism. Embeddings of two
languages are made to match with each other by rotating and scaling. The method
does not require any form of supervision and can be applied to any language
pair. On a benchmark data set of bilingual lexicon induction, our approach can
achieve competitive or superior performance compared to state-of-the-art
methods, with particularly strong results being found on distant languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1"&gt;Hailong Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiejun Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Pronoun Resolution via Masked Noun-Phrase Prediction. (arXiv:2105.12392v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12392</id>
        <link href="http://arxiv.org/abs/2105.12392"/>
        <updated>2021-05-27T01:32:27.399Z</updated>
        <summary type="html"><![CDATA[In this work, we propose Masked Noun-Phrase Prediction (MNPP), a pre-training
strategy to tackle pronoun resolution in a fully unsupervised setting. Firstly,
We evaluate our pre-trained model on various pronoun resolution datasets
without any finetuning. Our method outperforms all previous unsupervised
methods on all datasets by large margins. Secondly, we proceed to a few-shot
setting where we finetune our pre-trained model on WinoGrande-S and XS. Our
method outperforms RoBERTa-large baseline with large margins, meanwhile,
achieving a higher AUC score after further finetuning on the remaining three
official splits of WinoGrande.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1"&gt;Ming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1"&gt;Pratyay Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1"&gt;Chitta Baral&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limitations of Autoregressive Models and Their Alternatives. (arXiv:2010.11939v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11939</id>
        <link href="http://arxiv.org/abs/2010.11939"/>
        <updated>2021-05-27T01:32:27.390Z</updated>
        <summary type="html"><![CDATA[Standard autoregressive language models perform only polynomial-time
computation to compute the probability of the next symbol. While this is
attractive, it means they cannot model distributions whose next-symbol
probability is hard to compute. Indeed, they cannot even model them well enough
to solve associated easy decision problems for which an engineer might want to
consult a language model. These limitations apply no matter how much
computation and data are used to train the model, unless the model is given
access to oracle parameters that grow superpolynomially in sequence length.

Thus, simply training larger autoregressive language models is not a panacea
for NLP. Alternatives include energy-based models (which give up efficient
sampling) and latent-variable autoregressive models (which give up efficient
scoring of a given string). Both are powerful enough to escape the above
limitations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chu-Cheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaech_A/0/1/0/all/0/1"&gt;Aaron Jaech&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1"&gt;Matthew R. Gormley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1"&gt;Jason Eisner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition. (arXiv:2105.12708v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12708</id>
        <link href="http://arxiv.org/abs/2105.12708"/>
        <updated>2021-05-27T01:32:27.383Z</updated>
        <summary type="html"><![CDATA[Loanwords, such as Anglicisms, are a challenge in German speech recognition.
Due to their irregular pronunciation compared to native German words,
automatically generated pronunciation dictionaries often include faulty phoneme
sequences for Anglicisms. In this work, we propose a multitask
sequence-to-sequence approach for grapheme-to-phoneme conversion to improve the
phonetization of Anglicisms. We extended a grapheme-to-phoneme model with a
classifier to distinguish Anglicisms from native German words. With this
approach, the model learns to generate pronunciations differently depending on
the classification result. We used our model to create supplementary Anglicism
pronunciation dictionaries that are added to an existing German speech
recognition model. Tested on a dedicated Anglicism evaluation set, we improved
the recognition of Anglicisms compared to a baseline model, reducing the word
error rate by 1 % and the Anglicism error rate by 3 %. We show that multitask
learning can help solving the challenge of loanwords in German speech
recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pritzen_J/0/1/0/all/0/1"&gt;Julia Pritzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gref_M/0/1/0/all/0/1"&gt;Michael Gref&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1"&gt;Christoph Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuhlke_D/0/1/0/all/0/1"&gt;Dietlind Z&amp;#xfc;hlke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SentEmojiBot: Empathising Conversations Generation with Emojis. (arXiv:2105.12399v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12399</id>
        <link href="http://arxiv.org/abs/2105.12399"/>
        <updated>2021-05-27T01:32:27.374Z</updated>
        <summary type="html"><![CDATA[The increasing use of dialogue agents makes it extremely desirable for them
to understand and acknowledge the implied emotions to respond like humans with
empathy. Chatbots using traditional techniques analyze emotions based on the
context and meaning of the text and lack the understanding of emotions
expressed through face. Emojis representing facial expressions present a
promising way to express emotions. However, none of the AI systems utilizes
emojis for empathetic conversation generation. We propose, SentEmojiBot, based
on the SentEmoji dataset, to generate empathetic conversations with a
combination of emojis and text. Evaluation metrics show that the BERT-based
model outperforms the vanilla transformer model. A user study indicates that
the dialogues generated by our model were understandable and adding emojis
improved empathetic traits in conversations by 9.8%]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_A/0/1/0/all/0/1"&gt;Akhilesh Ravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_A/0/1/0/all/0/1"&gt;Amit Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1"&gt;Jainish Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dholakia_J/0/1/0/all/0/1"&gt;Jatin Dholakia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1"&gt;Naman Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Optimization of Tokenization and Downstream Model. (arXiv:2105.12410v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12410</id>
        <link href="http://arxiv.org/abs/2105.12410"/>
        <updated>2021-05-27T01:32:27.362Z</updated>
        <summary type="html"><![CDATA[Since traditional tokenizers are isolated from a downstream task and model,
they cannot output an appropriate tokenization depending on the task and model,
although recent studies imply that the appropriate tokenization improves the
performance. In this paper, we propose a novel method to find an appropriate
tokenization to a given downstream model by jointly optimizing a tokenizer and
the model. The proposed method has no restriction except for using loss values
computed by the downstream model to train the tokenizer, and thus, we can apply
the proposed method to any NLP task. Moreover, the proposed method can be used
to explore the appropriate tokenization for an already trained model as
post-processing. Therefore, the proposed method is applicable to various
situations. We evaluated whether our method contributes to improving
performance on text classification in three languages and machine translation
in eight language pairs. Experimental results show that our proposed method
improves the performance by determining appropriate tokenizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hiraoka_T/0/1/0/all/0/1"&gt;Tatsuya Hiraoka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1"&gt;Sho Takase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchiumi_K/0/1/0/all/0/1"&gt;Kei Uchiumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keyaki_A/0/1/0/all/0/1"&gt;Atsushi Keyaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1"&gt;Naoaki Okazaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Sign Language Translation with Monolingual Data by Sign Back-Translation. (arXiv:2105.12397v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12397</id>
        <link href="http://arxiv.org/abs/2105.12397"/>
        <updated>2021-05-27T01:32:27.352Z</updated>
        <summary type="html"><![CDATA[Despite existing pioneering works on sign language translation (SLT), there
is a non-trivial obstacle, i.e., the limited quantity of parallel sign-text
data. To tackle this parallel data bottleneck, we propose a sign
back-translation (SignBT) approach, which incorporates massive spoken language
texts into SLT training. With a text-to-gloss translation model, we first
back-translate the monolingual text to its gloss sequence. Then, the paired
sign sequence is generated by splicing pieces from an estimated gloss-to-sign
bank at the feature level. Finally, the synthetic parallel data serves as a
strong supplement for the end-to-end training of the encoder-decoder SLT
framework.

To promote the SLT research, we further contribute CSL-Daily, a large-scale
continuous SLT dataset. It provides both spoken language translations and
gloss-level annotations. The topic revolves around people's daily lives (e.g.,
travel, shopping, medical care), the most likely SLT application scenario.
Extensive experimental results and analysis of SLT methods are reported on
CSL-Daily. With the proposed sign back-translation method, we obtain a
substantial improvement over previous state-of-the-art SLT methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wengang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1"&gt;Weizhen Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1"&gt;Junfu Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LMMS Reloaded: Transformer-based Sense Embeddings for Disambiguation and Beyond. (arXiv:2105.12449v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12449</id>
        <link href="http://arxiv.org/abs/2105.12449"/>
        <updated>2021-05-27T01:32:27.328Z</updated>
        <summary type="html"><![CDATA[Distributional semantics based on neural approaches is a cornerstone of
Natural Language Processing, with surprising connections to human meaning
representation as well. Recent Transformer-based Language Models have proven
capable of producing contextual word representations that reliably convey
sense-specific information, simply as a product of self-supervision. Prior work
has shown that these contextual representations can be used to accurately
represent large sense inventories as sense embeddings, to the extent that a
distance-based solution to Word Sense Disambiguation (WSD) tasks outperforms
models trained specifically for the task. Still, there remains much to
understand on how to use these Neural Language Models (NLMs) to produce sense
embeddings that can better harness each NLM's meaning representation abilities.
In this work we introduce a more principled approach to leverage information
from all layers of NLMs, informed by a probing analysis on 14 NLM variants. We
also emphasize the versatility of these sense embeddings in contrast to
task-specific models, applying them on several sense-related tasks, besides
WSD, while demonstrating improved performance using our proposed approach over
prior work focused on sense embeddings. Finally, we discuss unexpected findings
regarding layer and model performance variations, and potential applications
for downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loureiro_D/0/1/0/all/0/1"&gt;Daniel Loureiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorge_A/0/1/0/all/0/1"&gt;Al&amp;#xed;pio M&amp;#xe1;rio Jorge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1"&gt;Jose Camacho-Collados&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation. (arXiv:2105.12523v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12523</id>
        <link href="http://arxiv.org/abs/2105.12523"/>
        <updated>2021-05-27T01:32:27.318Z</updated>
        <summary type="html"><![CDATA[Recently, token-level adaptive training has achieved promising improvement in
machine translation, where the cross-entropy loss function is adjusted by
assigning different training weights to different tokens, in order to alleviate
the token imbalance problem. However, previous approaches only use static word
frequency information in the target language without considering the source
language, which is insufficient for bilingual tasks like machine translation.
In this paper, we propose a novel bilingual mutual information (BMI) based
adaptive objective, which measures the learning difficulty for each target
token from the perspective of bilingualism, and assigns an adaptive weight
accordingly to improve token-level adaptive training. This method assigns
larger training weights to tokens with higher BMI, so that easy tokens are
updated with coarse granularity while difficult tokens are updated with fine
granularity. Experimental results on WMT14 English-to-German and WMT19
Chinese-to-English demonstrate the superiority of our approach compared with
the Transformer baseline and previous token-level adaptive training approaches.
Further analyses confirm that our method can improve the lexical diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yangyifan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yijin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiajun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of detecting clinical trial elements in exploration of COVID-19 literature. (arXiv:2105.12261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12261</id>
        <link href="http://arxiv.org/abs/2105.12261"/>
        <updated>2021-05-27T01:32:26.995Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic has driven ever-greater demand for tools which enable
efficient exploration of biomedical literature. Although semi-structured
information resulting from concept recognition and detection of the defining
elements of clinical trials (e.g. PICO criteria) has been commonly used to
support literature search, the contributions of this abstraction remain poorly
understood, especially in relation to text-based retrieval. In this study, we
compare the results retrieved by a standard search engine with those filtered
using clinically-relevant concepts and their relations. With analysis based on
the annotations from the TREC-COVID shared task, we obtain quantitative as well
as qualitative insights into characteristics of relational and concept-based
literature exploration. Most importantly, we find that the relational concept
selection filters the original retrieved collection in a way that decreases the
proportion of unjudged documents and increases the precision, which means that
the user is likely to be exposed to a larger number of relevant documents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suster_S/0/1/0/all/0/1"&gt;Simon &amp;#x160;uster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1"&gt;Karin Verspoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1"&gt;Timothy Baldwin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1"&gt;Jey Han Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yepes_A/0/1/0/all/0/1"&gt;Antonio Jimeno Yepes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_D/0/1/0/all/0/1"&gt;David Martinez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otmakhova_Y/0/1/0/all/0/1"&gt;Yulia Otmakhova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quotient Space-Based Keyword Retrieval in Sponsored Search. (arXiv:2105.12371v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.12371</id>
        <link href="http://arxiv.org/abs/2105.12371"/>
        <updated>2021-05-27T01:32:26.961Z</updated>
        <summary type="html"><![CDATA[Synonymous keyword retrieval has become an important problem for sponsored
search ever since major search engines relax the exact match product's matching
requirement to a synonymous level. Since the synonymous relations between
queries and keywords are quite scarce, the traditional information retrieval
framework is inefficient in this scenario. In this paper, we propose a novel
quotient space-based retrieval framework to address this problem. Considering
the synonymy among keywords as a mathematical equivalence relation, we can
compress the synonymous keywords into one representative, and the corresponding
quotient space would greatly reduce the size of the keyword repository. Then an
embedding-based retrieval is directly conducted between queries and the keyword
representatives. To mitigate the semantic gap of the quotient space-based
retrieval, a single semantic siamese model is utilized to detect both the
keyword--keyword and query-keyword synonymous relations. The experiments show
that with our quotient space-based retrieval method, the synonymous keyword
retrieving performance can be greatly improved in terms of memory cost and
recall efficiency. This method has been successfully implemented in Baidu's
online sponsored search system and has yielded a significant improvement in
revenue.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1"&gt;Yijiang Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chaobing Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;YanFeng Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Climate Action During COVID-19 Recovery and Beyond: A Twitter Text Mining Study. (arXiv:2105.12190v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.12190</id>
        <link href="http://arxiv.org/abs/2105.12190"/>
        <updated>2021-05-27T01:32:26.941Z</updated>
        <summary type="html"><![CDATA[The Coronavirus pandemic created a global crisis that prompted immediate
large-scale action, including economic shutdowns and mobility restrictions.
These actions have had devastating effects on the economy, but some positive
effects on the environment. As the world recovers from the pandemic, we ask the
following question: What is the public attitude towards climate action during
COVID-19 recovery and beyond? We answer this question by analyzing discussions
on the Twitter social media platform. We find that most discussions support
climate action and point out lessons learned during pandemic response that can
shape future climate policy, although skeptics continue to have a presence.
Additionally, concerns arise in the context of climate action during the
pandemic, such as mitigating the risk of COVID-19 transmission on public
transit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parsa_M/0/1/0/all/0/1"&gt;Mohammad S. Parsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1"&gt;Lukasz Golab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keshav_S/0/1/0/all/0/1"&gt;Srinivasan Keshav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Transparent Application of Machine Learning in Video Processing. (arXiv:2105.12700v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.12700</id>
        <link href="http://arxiv.org/abs/2105.12700"/>
        <updated>2021-05-27T01:32:26.907Z</updated>
        <summary type="html"><![CDATA[Machine learning techniques for more efficient video compression and video
enhancement have been developed thanks to breakthroughs in deep learning. The
new techniques, considered as an advanced form of Artificial Intelligence (AI),
bring previously unforeseen capabilities. However, they typically come in the
form of resource-hungry black-boxes (overly complex with little transparency
regarding the inner workings). Their application can therefore be unpredictable
and generally unreliable for large-scale use (e.g. in live broadcast). The aim
of this work is to understand and optimise learned models in video processing
applications so systems that incorporate them can be used in a more trustworthy
manner. In this context, the presented work introduces principles for
simplification of learned models targeting improved transparency in
implementing machine learning for video production and distribution
applications. These principles are demonstrated on video compression examples,
showing how bitrate savings and reduced complexity can be achieved by
simplifying relevant deep learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1"&gt;Luka Murn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blanch_M/0/1/0/all/0/1"&gt;Marc Gorriz Blanch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Santamaria_M/0/1/0/all/0/1"&gt;Maria Santamaria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rivera_F/0/1/0/all/0/1"&gt;Fiona Rivera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1"&gt;Marta Mrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Initializing ReLU networks in an expressive subspace of weights. (arXiv:2103.12499v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12499</id>
        <link href="http://arxiv.org/abs/2103.12499"/>
        <updated>2021-05-26T01:22:12.162Z</updated>
        <summary type="html"><![CDATA[Using a mean-field theory of signal propagation, we analyze the evolution of
correlations between two signals propagating forward through a deep ReLU
network with correlated weights. Signals become highly correlated in deep ReLU
networks with uncorrelated weights. We show that ReLU networks with
anti-correlated weights can avoid this fate and have a chaotic phase where the
signal correlations saturate below unity. Consistent with this analysis, we
find that networks initialized with anti-correlated weights can train faster
(in a teacher-student setting) by taking advantage of the increased
expressivity in the chaotic phase. Combining this with a previously proposed
strategy of using an asymmetric initialization to reduce dead node probability,
we propose an initialization scheme that allows faster training and learning
than the best-known initializations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1"&gt;Dayal Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sreejith_G/0/1/0/all/0/1"&gt;G J Sreejith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoRT: Complementary Rankings from Transformers. (arXiv:2010.10252v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10252</id>
        <link href="http://arxiv.org/abs/2010.10252"/>
        <updated>2021-05-26T01:22:12.020Z</updated>
        <summary type="html"><![CDATA[Many recent approaches towards neural information retrieval mitigate their
computational costs by using a multi-stage ranking pipeline. In the first
stage, a number of potentially relevant candidates are retrieved using an
efficient retrieval model such as BM25. Although BM25 has proven decent
performance as a first-stage ranker, it tends to miss relevant passages. In
this context we propose CoRT, a simple neural first-stage ranking model that
leverages contextual representations from pretrained language models such as
BERT to complement term-based ranking functions while causing no significant
delay at query time. Using the MS MARCO dataset, we show that CoRT
significantly increases the candidate recall by complementing BM25 with missing
candidates. Consequently, we find subsequent re-rankers achieve superior
results with less candidates. We further demonstrate that passage retrieval
using CoRT can be realized with surprisingly low latencies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wrzalik_M/0/1/0/all/0/1"&gt;Marco Wrzalik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krechel_D/0/1/0/all/0/1"&gt;Dirk Krechel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analogical Proportions. (arXiv:2006.02854v6 [cs.LO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02854</id>
        <link href="http://arxiv.org/abs/2006.02854"/>
        <updated>2021-05-26T01:22:12.005Z</updated>
        <summary type="html"><![CDATA[Analogy-making is at the core of human intelligence and creativity with
applications to such diverse tasks as commonsense reasoning, learning, language
acquisition, and story telling. This paper contributes to the foundations of
artificial general intelligence by introducing from first principles an
abstract algebraic framework of analogical proportions of the form `$a$ is to
$b$ what $c$ is to $d$' in the general setting of universal algebra. This
enables us to compare mathematical objects possibly across different domains in
a uniform way which is crucial for AI-systems. The main idea is to define
solutions to analogical equations in terms of maximal sets of algebraic
justifications, which amounts to deriving abstract terms of concrete elements
from a `known' source domain which can then be instantiated in an `unknown'
target domain to obtain analogous elements. It turns out that our notion of
analogical proportions has appealing mathematical properties. For example, we
show that analogical proportions preserve functional dependencies across
different domains, which is desirable. We study Lepage's axioms of analogical
proportions and argue why we disagree with his symmetry, central permutation,
strong reflexivity, and strong determinism axioms. We compare our framework
with two prominent and recently introduced frameworks of analogical proportions
from the literature in the concrete domains of sets and numbers, and we show
that in each case we either disagree with the notion from the literature
justified by some plausible counter-example or we can show that our model
yields strictly more reasonable solutions. This provides evidence for its
applicability. In a broader sense, this paper is a first step towards a theory
of analogical reasoning and learning systems with potential applications to
fundamental AI-problems like commonsense reasoning and computational learning
and creativity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1"&gt;Christian Anti&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T-SVD Based Non-convex Tensor Completion and Robust Principal Component Analysis. (arXiv:1904.10165v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.10165</id>
        <link href="http://arxiv.org/abs/1904.10165"/>
        <updated>2021-05-26T01:22:11.999Z</updated>
        <summary type="html"><![CDATA[Tensor completion and robust principal component analysis have been widely
used in machine learning while the key problem relies on the minimization of a
tensor rank that is very challenging. A common way to tackle this difficulty is
to approximate the tensor rank with the $\ell_1-$norm of singular values based
on its Tensor Singular Value Decomposition (T-SVD). Besides, the sparsity of a
tensor is also measured by its $\ell_1-$norm. However, the $\ell_1$ penalty is
essentially biased and thus the result will deviate. In order to sidestep the
bias, we propose a novel non-convex tensor rank surrogate function and a novel
non-convex sparsity measure. In this new setting by using the concavity instead
of the convexity, a majorization minimization algorithm is further designed for
tensor completion and robust principal component analysis. Furthermore, we
analyze its theoretical properties. Finally, the experiments on natural and
hyperspectral images demonstrate the efficacy and efficiency of our proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jinwen Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-individual Recognition of Emotions by a Dynamic Entropy based on Pattern Learning with EEG features. (arXiv:2009.12525v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12525</id>
        <link href="http://arxiv.org/abs/2009.12525"/>
        <updated>2021-05-26T01:22:11.994Z</updated>
        <summary type="html"><![CDATA[Use of the electroencephalogram (EEG) and machine learning approaches to
recognize emotions can facilitate affective human computer interactions.
However, the type of EEG data constitutes an obstacle for cross-individual EEG
feature modelling and classification. To address this issue, we propose a
deep-learning framework denoted as a dynamic entropy-based pattern learning
(DEPL) to abstract informative indicators pertaining to the neurophysiological
features among multiple individuals. DEPL enhanced the capability of
representations generated by a deep convolutional neural network by modelling
the interdependencies between the cortical locations of dynamical entropy based
features. The effectiveness of the DEPL has been validated with two public
databases, commonly referred to as the DEAP and MAHNOB-HCI multimodal tagging
databases. Specifically, the leave one subject out training and testing
paradigm has been applied. Numerous experiments on EEG emotion recognition
demonstrate that the proposed DEPL is superior to those traditional machine
learning (ML) methods, and could learn between electrode dependencies w.r.t.
different emotions, which is meaningful for developing the effective
human-computer interaction systems by adapting to human emotions in the real
world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1"&gt;Xiaolong Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zhong Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Auto-Regressive Gaussian Processes for Continual Learning. (arXiv:2006.05468v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05468</id>
        <link href="http://arxiv.org/abs/2006.05468"/>
        <updated>2021-05-26T01:22:11.988Z</updated>
        <summary type="html"><![CDATA[Through sequential construction of posteriors on observing data online,
Bayes' theorem provides a natural framework for continual learning. We develop
Variational Auto-Regressive Gaussian Processes (VAR-GPs), a principled
posterior updating mechanism to solve sequential tasks in continual learning.
By relying on sparse inducing point approximations for scalable posteriors, we
propose a novel auto-regressive variational distribution which reveals two
fruitful connections to existing results in Bayesian inference, expectation
propagation and orthogonal inducing points. Mean predictive entropy estimates
show VAR-GPs prevent catastrophic forgetting, which is empirically supported by
strong performance on modern continual learning benchmarks against competitive
baselines. A thorough ablation study demonstrates the efficacy of our modeling
choices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kapoor_S/0/1/0/all/0/1"&gt;Sanyam Kapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1"&gt;Theofanis Karaletsos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bui_T/0/1/0/all/0/1"&gt;Thang D. Bui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HERS: Homomorphically Encrypted Representation Search. (arXiv:2003.12197v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12197</id>
        <link href="http://arxiv.org/abs/2003.12197"/>
        <updated>2021-05-26T01:22:11.983Z</updated>
        <summary type="html"><![CDATA[We present a method to search for a probe (or query) image representation
against a large gallery in the encrypted domain. We require that the probe and
gallery images be represented in terms of a fixed-length representation, which
is typical for representations obtained from learned networks. Our encryption
scheme is agnostic to how the fixed-length representation is obtained and can
therefore be applied to any fixed-length representation in any application
domain. Our method, dubbed HERS (Homomorphically Encrypted Representation
Search), operates by (i) compressing the representation towards its estimated
intrinsic dimensionality with minimal loss of accuracy (ii) encrypting the
compressed representation using the proposed fully homomorphic encryption
scheme, and (iii) efficiently searching against a gallery of encrypted
representations directly in the encrypted domain, without decrypting them.
Numerical results on large galleries of face, fingerprint, and object datasets
such as ImageNet show that, for the first time, accurate and fast image search
within the encrypted domain is feasible at scale (500 seconds; $275\times$
speed up over state-of-the-art for encrypted search against a gallery of 100
million).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1"&gt;Joshua J. Engelsma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Anil K. Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1"&gt;Vishnu Naresh Boddeti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Manifold Neighborhood size for Nonlinear Analysis of LIBS Amino Acid Spectra. (arXiv:2105.12089v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12089</id>
        <link href="http://arxiv.org/abs/2105.12089"/>
        <updated>2021-05-26T01:22:11.977Z</updated>
        <summary type="html"><![CDATA[Classification and identification of amino acids in aqueous solutions is
important in the study of biomacromolecules. Laser Induced Breakdown
Spectroscopy (LIBS) uses high energy laser-pulses for ablation of chemical
compounds whose radiated spectra are captured and recorded to reveal molecular
structure. Spectral peaks and noise from LIBS are impacted by experimental
protocols. Current methods for LIBS spectral analysis achieves promising
results using PCA, a linear method. It is well-known that the underlying
physical processes behind LIBS are highly nonlinear. Our work set out to
understand the impact of LIBS spectra on suitable neighborhood size over which
to consider pattern phenomena, if nonlinear methods capture pattern phenomena
with increased efficacy, and how they improve classification and identification
of compounds. We analyzed four amino acids, polysaccharide, and a control
group, water. We developed an information theoretic method for measurement of
LIBS energy spectra, implemented manifold methods for nonlinear dimensionality
reduction, and found while clustering results were not statistically
significantly different, nonlinear methods lead to increased classification
accuracy. Moreover, our approach uncovered the contribution of micro-wells
(experimental protocol) in LIBS spectra. To the best of our knowledge, ours is
the first application of Manifold methods to LIBS amino-acid analysis in the
research literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush K. Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holness_G/0/1/0/all/0/1"&gt;Gary Holness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivakumar_P/0/1/0/all/0/1"&gt;Poopalasingam Sivakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markushin_Y/0/1/0/all/0/1"&gt;Yuri Markushin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melikechi_N/0/1/0/all/0/1"&gt;Noureddine Melikechi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Upper Limit of Decaying Rate with Respect to Frequency in Deep Neural Network. (arXiv:2105.11675v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11675</id>
        <link href="http://arxiv.org/abs/2105.11675"/>
        <updated>2021-05-26T01:22:11.971Z</updated>
        <summary type="html"><![CDATA[Deep neural network (DNN) usually learns the target function from low to high
frequency, which is called frequency principle or spectral bias. This frequency
principle sheds light on a high-frequency curse of DNNs -- difficult to learn
high-frequency information. Inspired by the frequency principle, a series of
works are devoted to develop algorithms for overcoming the high-frequency
curse. A natural question arises: what is the upper limit of the decaying rate
w.r.t. frequency when one trains a DNN? In this work, our theory, confirmed by
numerical experiments, suggests that there is a critical decaying rate w.r.t.
frequency in DNN training. Below the upper limit of the decaying rate, the DNN
interpolates the training data by a function with a certain regularity.
However, above the upper limit, the DNN interpolates the training data by a
trivial function, i.e., a function is only non-zero at training data points.
Our results indicate a better way to overcome the high-frequency curse is to
design a proper pre-condition approach to shift high-frequency information to
low-frequency one, which coincides with several previous developed algorithms
for fast learning high-frequency information. More importantly, this work
rigorously proves that the high-frequency curse is an intrinsic difficulty of
DNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhi-Qin John Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yaoyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Honest-but-Curious Nets: Sensitive Attributes of Private Inputs can be Secretly Coded into the Entropy of Classifiers' Outputs. (arXiv:2105.12049v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12049</id>
        <link href="http://arxiv.org/abs/2105.12049"/>
        <updated>2021-05-26T01:22:11.965Z</updated>
        <summary type="html"><![CDATA[It is known that deep neural networks, trained for the classification of a
non-sensitive target attribute, can reveal sensitive attributes of their input
data; through features of different granularity extracted by the classifier.
We, taking a step forward, show that deep classifiers can be trained to
secretly encode a sensitive attribute of users' input data, at inference time,
into the classifier's outputs for the target attribute. An attack that works
even if users have a white-box view of the classifier, and can keep all
internal representations hidden except for the classifier's estimation of the
target attribute. We introduce an information-theoretical formulation of such
adversaries and present efficient empirical implementations for training
honest-but-curious (HBC) classifiers based on this formulation: deep models
that can be accurate in predicting the target attribute, but also can utilize
their outputs to secretly encode a sensitive attribute. Our evaluations on
several tasks in real-world datasets show that a semi-trusted server can build
a classifier that is not only perfectly honest but also accurately curious. Our
work highlights a vulnerability that can be exploited by malicious machine
learning service providers to attack their user's privacy in several seemingly
safe scenarios; such as encrypted inferences, computations at the edge, or
private knowledge distillation. We conclude by showing the difficulties in
distinguishing between standard and HBC classifiers and discussing potential
proactive defenses against this vulnerability of deep classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malekzadeh_M/0/1/0/all/0/1"&gt;Mohammad Malekzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borovykh_A/0/1/0/all/0/1"&gt;Anastasia Borovykh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz G&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Generative Prior with Latent Space Sparsity Constraints. (arXiv:2105.11956v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11956</id>
        <link href="http://arxiv.org/abs/2105.11956"/>
        <updated>2021-05-26T01:22:11.948Z</updated>
        <summary type="html"><![CDATA[We address the problem of compressed sensing using a deep generative prior
model and consider both linear and learned nonlinear sensing mechanisms, where
the nonlinear one involves either a fully connected neural network or a
convolutional neural network. Recently, it has been argued that the
distribution of natural images do not lie in a single manifold but rather lie
in a union of several submanifolds. We propose a sparsity-driven latent space
sampling (SDLSS) framework and develop a proximal meta-learning (PML) algorithm
to enforce sparsity in the latent space. SDLSS allows the range-space of the
generator to be considered as a union-of-submanifolds. We also derive the
sample complexity bounds within the SDLSS framework for the linear measurement
model. The results demonstrate that for a higher degree of compression, the
SDLSS method is more efficient than the state-of-the-art method. We first
consider a comparison between linear and nonlinear sensing mechanisms on
Fashion-MNIST dataset and show that the learned nonlinear version is superior
to the linear one. Subsequent comparisons with the deep compressive sensing
(DCS) framework proposed in the literature are reported. We also consider the
effect of the dimension of the latent space and the sparsity factor in
validating the SDLSS framework. Performance quantification is carried out by
employing three objective metrics: peak signal-to-noise ratio (PSNR),
structural similarity index metric (SSIM), and reconstruction error (RE).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killedar_V/0/1/0/all/0/1"&gt;Vinayak Killedar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokala_P/0/1/0/all/0/1"&gt;Praveen Kumar Pokala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seelamantula_C/0/1/0/all/0/1"&gt;Chandra Sekhar Seelamantula&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator. (arXiv:2105.11589v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11589</id>
        <link href="http://arxiv.org/abs/2105.11589"/>
        <updated>2021-05-26T01:22:11.936Z</updated>
        <summary type="html"><![CDATA[Interactive robots navigating photo-realistic environments face challenges
underlying vision-and-language navigation (VLN), but in addition, they need to
be trained to handle the dynamic nature of dialogue. However, research in
Cooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts
with a guide in natural language in order to reach a goal, treats the dialogue
history as a VLN-style static instruction. In this paper, we present VISITRON,
a navigator better suited to the interactive regime inherent to CVDN by being
trained to: i) identify and associate object-level concepts and semantics
between the environment and dialogue history, ii) identify when to interact vs.
navigate via imitation learning of a binary classification head. We perform
extensive ablations with VISITRON to gain empirical insights and improve
performance on CVDN. VISITRON is competitive with models on the static CVDN
leaderboard. We also propose a generalized interactive regime to fine-tune and
evaluate VISITRON and future such models with pre-trained guides for
adaptability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Ayush Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Karthik Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1"&gt;Robinson Piramuthu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1"&gt;Gokhan T&amp;#xfc;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1"&gt;Dilek Hakkani-T&amp;#xfc;r&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoMAE: A Multi-factor Hierarchical Framework for Empathetic Response Generation. (arXiv:2105.08316v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08316</id>
        <link href="http://arxiv.org/abs/2105.08316"/>
        <updated>2021-05-26T01:22:11.921Z</updated>
        <summary type="html"><![CDATA[The capacity of empathy is crucial to the success of open-domain dialog
systems. Due to its nature of multi-dimensionality, there are various factors
that relate to empathy expression, such as communication mechanism, dialog act
and emotion. However, existing methods for empathetic response generation
usually either consider only one empathy factor or ignore the hierarchical
relationships between different factors, leading to a weak ability of empathy
modeling. In this paper, we propose a multi-factor hierarchical framework,
CoMAE, for empathetic response generation, which models the above three key
factors of empathy expression in a hierarchical way. We show experimentally
that our CoMAE-based model can generate more empathetic responses than previous
methods. We also highlight the importance of hierarchical modeling of different
factors through both the empirical analysis on a real-life corpus and the
extensive experiments. Our codes and used data are available at
https://github.com/chujiezheng/CoMAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chujie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1"&gt;Yongcai Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaGCN:Adaptive Boosting Algorithm for Graph Convolutional Networks on Imbalanced Node Classification. (arXiv:2105.11625v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11625</id>
        <link href="http://arxiv.org/abs/2105.11625"/>
        <updated>2021-05-26T01:22:11.905Z</updated>
        <summary type="html"><![CDATA[The Graph Neural Network (GNN) has achieved remarkable success in graph data
representation. However, the previous work only considered the ideal balanced
dataset, and the practical imbalanced dataset was rarely considered, which, on
the contrary, is of more significance for the application of GNN. Traditional
methods such as resampling, reweighting and synthetic samples that deal with
imbalanced datasets are no longer applicable in GNN. Ensemble models can handle
imbalanced datasets better compared with single estimator. Besides, ensemble
learning can achieve higher estimation accuracy and has better reliability
compared with the single estimator. In this paper, we propose an ensemble model
called AdaGCN, which uses a Graph Convolutional Network (GCN) as the base
estimator during adaptive boosting. In AdaGCN, a higher weight will be set for
the training samples that are not properly classified by the previous
classifier, and transfer learning is used to reduce computational cost and
increase fitting capability. Experiments show that the AdaGCN model we proposed
achieves better performance than GCN, GraphSAGE, GAT, N-GCN and the most of
advanced reweighting and resampling methods on synthetic imbalanced datasets,
with an average improvement of 4.3%. Our model also improves state-of-the-art
baselines on all of the challenging node classification tasks we consider:
Cora, Citeseer, Pubmed, and NELL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;S. Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;L. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;J. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting malware threat intelligence using KGs. (arXiv:2102.05571v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05571</id>
        <link href="http://arxiv.org/abs/2102.05571"/>
        <updated>2021-05-26T01:22:11.900Z</updated>
        <summary type="html"><![CDATA[Large amounts of threat intelligence information about malware attacks are
available in disparate, typically unstructured, formats. Knowledge graphs can
capture this information and its context using RDF triples represented by
entities and relations. Sparse or inaccurate threat information, however, leads
to challenges such as incomplete or erroneous triples. Generic information
extraction (IE) models used to populate the knowledge graph cannot fully
guarantee domain-specific context. This paper proposes a system to generate a
Malware Knowledge Graph called MalKG, the first open-source automated knowledge
graph for malware threat intelligence. MalKG dataset (MT40K\footnote{ Anonymous
GitHub link: https://github.com/malkg-researcher/MalKG}) contains approximately
40,000 triples generated from 27,354 unique entities and 34 relations. For
ground truth, we manually curate a knowledge graph called MT3K, with 3,027
triples generated from 5,741 unique entities and 22 relations. We demonstrate
the intelligence prediction of MalKG using two use cases. Predicting malware
threat information using the benchmark model achieves 80.4 for the hits@10
metric (predicts the top 10 options for an information class), and 0.75 for the
MRR (mean reciprocal rank). We also propose an automated, contextual framework
for information extraction, both manually and automatically, at the sentence
level from 1,100 malware threat reports and from the common vulnerabilities and
exposures (CVE) database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_N/0/1/0/all/0/1"&gt;Nidhi Rastogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Sharmishtha Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christian_R/0/1/0/all/0/1"&gt;Ryan Christian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gridley_J/0/1/0/all/0/1"&gt;Jared Gridley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1"&gt;Mohammad Zaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1"&gt;Alex Gittens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1"&gt;Charu Aggarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid and Automated Machine Learning Approaches for Oil Fields Development: the Case Study of Volve Field, North Sea. (arXiv:2103.02598v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02598</id>
        <link href="http://arxiv.org/abs/2103.02598"/>
        <updated>2021-05-26T01:22:11.893Z</updated>
        <summary type="html"><![CDATA[The paper describes the usage of intelligent approaches for field development
tasks that may assist a decision-making process. We focused on the problem of
wells location optimization and two tasks within it: improving the quality of
oil production estimation and estimation of reservoir characteristics for
appropriate wells allocation and parametrization, using machine learning
methods. For oil production estimation, we implemented and investigated the
quality of forecasting models: physics-based, pure data-driven, and hybrid one.
The CRMIP model was chosen as a physics-based approach. We compare it with the
machine learning and hybrid methods in a frame of oil production forecasting
task. In the investigation of reservoir characteristics for wells location
choice, we automated the seismic analysis using evolutionary identification of
convolutional neural network for the reservoir detection. The Volve oil field
dataset was used as a case study to conduct the experiments. The implemented
approaches can be used to analyze different oil fields or adapted to similar
physics-related problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikitin_N/0/1/0/all/0/1"&gt;Nikolay O. Nikitin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Revin_I/0/1/0/all/0/1"&gt;Ilia Revin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hvatov_A/0/1/0/all/0/1"&gt;Alexander Hvatov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vychuzhanin_P/0/1/0/all/0/1"&gt;Pavel Vychuzhanin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1"&gt;Anna V. Kalyuzhnaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On learning parametric distributions from quantized samples. (arXiv:2105.12019v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2105.12019</id>
        <link href="http://arxiv.org/abs/2105.12019"/>
        <updated>2021-05-26T01:22:11.888Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning parametric distributions from their
quantized samples in a network. Specifically, $n$ agents or sensors observe
independent samples of an unknown parametric distribution; and each of them
uses $k$ bits to describe its observed sample to a central processor whose goal
is to estimate the unknown distribution. First, we establish a generalization
of the well-known van Trees inequality to general $L_p$-norms, with $p > 1$, in
terms of Generalized Fisher information. Then, we develop minimax lower bounds
on the estimation error for two losses: general $L_p$-norms and the related
Wasserstein loss from optimal transport.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarbu_S/0/1/0/all/0/1"&gt;Septimia Sarbu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaidi_A/0/1/0/all/0/1"&gt;Abdellatif Zaidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Planetary Transit Candidates in TESS Full-Frame Image Light Curves via Convolutional Neural Networks. (arXiv:2101.10919v2 [astro-ph.EP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10919</id>
        <link href="http://arxiv.org/abs/2101.10919"/>
        <updated>2021-05-26T01:22:11.883Z</updated>
        <summary type="html"><![CDATA[The Transiting Exoplanet Survey Satellite (TESS) mission measured light from
stars in ~75% of the sky throughout its two year primary mission, resulting in
millions of TESS 30-minute cadence light curves to analyze in the search for
transiting exoplanets. To search this vast data trove for transit signals, we
aim to provide an approach that is both computationally efficient and produces
highly performant predictions. This approach minimizes the required human
search effort. We present a convolutional neural network, which we train to
identify planetary transit signals and dismiss false positives. To make a
prediction for a given light curve, our network requires no prior transit
parameters identified using other methods. Our network performs inference on a
TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large
scale archival searches. We present 181 new planet candidates identified by our
network, which pass subsequent human vetting designed to rule out false
positives. Our neural network model is additionally provided as open-source
code for public use and extension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Olmschenk_G/0/1/0/all/0/1"&gt;Greg Olmschenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Silva_S/0/1/0/all/0/1"&gt;Stela Ishitani Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rau_G/0/1/0/all/0/1"&gt;Gioia Rau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Barry_R/0/1/0/all/0/1"&gt;Richard K. Barry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kruse_E/0/1/0/all/0/1"&gt;Ethan Kruse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Cacciapuoti_L/0/1/0/all/0/1"&gt;Luca Cacciapuoti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kostov_V/0/1/0/all/0/1"&gt;Veselin Kostov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Powell_B/0/1/0/all/0/1"&gt;Brian P. Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Wyrwas_E/0/1/0/all/0/1"&gt;Edward Wyrwas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Schnittman_J/0/1/0/all/0/1"&gt;Jeremy D. Schnittman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Barclay_T/0/1/0/all/0/1"&gt;Thomas Barclay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Power-grid stability predictions using transferable machine learning. (arXiv:2105.07562v2 [physics.soc-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07562</id>
        <link href="http://arxiv.org/abs/2105.07562"/>
        <updated>2021-05-26T01:22:11.875Z</updated>
        <summary type="html"><![CDATA[Complex network analyses have provided clues to improve power-grid stability
with the help of numerical models. The high computational cost of numerical
simulations, however, has inhibited the approach especially when it deals with
the dynamic properties of power grids such as frequency synchronization. In
this study, we investigate machine learning techniques to estimate the
stability of power grid synchronization. We test three different machine
learning algorithms -- random forest, support vector machine, and artificial
neural network -- training them with two different types of synthetic power
grids consisting of homogeneous and heterogeneous input-power distribution,
respectively. We find that the three machine learning models better predict the
synchronization stability of power-grid nodes when they are trained with the
heterogeneous input-power distribution than the homogeneous one. With the
real-world power grids of Great Britain, Spain, France, and Germany, we also
demonstrate that the machine learning algorithms trained on synthetic power
grids are transferable to the stability prediction of the real-world power
grids, which implies the prospective applicability of machine learning
techniques on power-grid studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Yang_S/0/1/0/all/0/1"&gt;Seong-Gyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kim_B/0/1/0/all/0/1"&gt;Beom Jun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Son_S/0/1/0/all/0/1"&gt;Seung-Woo Son&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kim_H/0/1/0/all/0/1"&gt;Heetae Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RNNoise-Ex: Hybrid Speech Enhancement System based on RNN and Spectral Features. (arXiv:2105.11813v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.11813</id>
        <link href="http://arxiv.org/abs/2105.11813"/>
        <updated>2021-05-26T01:22:11.857Z</updated>
        <summary type="html"><![CDATA[Recent interest in exploiting Deep Learning techniques for Noise Suppression,
has led to the creation of Hybrid Denoising Systems that combine classic Signal
Processing with Deep Learning. In this paper, we concentrated our efforts on
extending the RNNoise denoising system (arXiv:1709.08243) with the inclusion of
complementary features during the training phase. We present a comprehensive
explanation of the set-up process of a modified system and present the
comparative results derived from a performance evaluation analysis, using a
reference version of RNNoise as control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doumanidis_C/0/1/0/all/0/1"&gt;Constantine C. Doumanidis&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Anagnostou_C/0/1/0/all/0/1"&gt;Christina Anagnostou&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Arvaniti_E/0/1/0/all/0/1"&gt;Evangelia-Sofia Arvaniti&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Papadopoulou_A/0/1/0/all/0/1"&gt;Anthi Papadopoulou&lt;/a&gt; (1) ((1) Aristotle University of Thessaloniki)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Reward Functions in Q-Learning Applied to a Cart Position Problem. (arXiv:2105.11617v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11617</id>
        <link href="http://arxiv.org/abs/2105.11617"/>
        <updated>2021-05-26T01:22:11.852Z</updated>
        <summary type="html"><![CDATA[Growing advancements in reinforcement learning has led to advancements in
control theory. Reinforcement learning has effectively solved the inverted
pendulum problem and more recently the double inverted pendulum problem. In
reinforcement learning, our agents learn by interacting with the control system
with the goal of maximizing rewards. In this paper, we explore three such
reward functions in the cart position problem. This paper concludes that a
discontinuous reward function that gives non-zero rewards to agents only if
they are within a given distance from the desired position gives the best
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1"&gt;Amartya Mukherjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Schemes for Finding Near-Stationary Points of Convex Finite-Sums. (arXiv:2105.12062v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2105.12062</id>
        <link href="http://arxiv.org/abs/2105.12062"/>
        <updated>2021-05-26T01:22:11.846Z</updated>
        <summary type="html"><![CDATA[The problem of finding near-stationary points in convex optimization has not
been adequately studied yet, unlike other optimality measures such as
minimizing function value. Even in the deterministic case, the optimal method
(OGM-G, due to Kim and Fessler (2021)) has just been discovered recently. In
this work, we conduct a systematic study of the algorithmic techniques in
finding near-stationary points of convex finite-sums. Our main contributions
are several algorithmic discoveries: (1) we discover a memory-saving variant of
OGM-G based on the performance estimation problem approach (Drori and Teboulle,
2014); (2) we design a new accelerated SVRG variant that can simultaneously
achieve fast rates for both minimizing gradient norm and function value; (3) we
propose an adaptively regularized accelerated SVRG variant, which does not
require the knowledge of some unknown initial constants and achieves
near-optimal complexities. We put an emphasis on the simplicity and
practicality of the new schemes, which could facilitate future developments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kaiwen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tian_L/0/1/0/all/0/1"&gt;Lai Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+So_A/0/1/0/all/0/1"&gt;Anthony Man-Cho So&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Cheng_J/0/1/0/all/0/1"&gt;James Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAP-GAN: Towards Adversarial Robustness with Cycle-consistent Attentional Purification. (arXiv:2102.07304v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07304</id>
        <link href="http://arxiv.org/abs/2102.07304"/>
        <updated>2021-05-26T01:22:11.840Z</updated>
        <summary type="html"><![CDATA[Adversarial attack is aimed at fooling the target classifier with
imperceptible perturbation. Adversarial examples, which are carefully crafted
with a malicious purpose, can lead to erroneous predictions, resulting in
catastrophic accidents. To mitigate the effects of adversarial attacks, we
propose a novel purification model called CAP-GAN. CAP-GAN takes account of the
idea of pixel-level and feature-level consistency to achieve reasonable
purification under cycle-consistent learning. Specifically, we utilize the
guided attention module and knowledge distillation to convey meaningful
information to the purification model. Once a model is fully trained, inputs
would be projected into the purification model and transformed into clean-like
images. We vary the capacity of the adversary to argue the robustness against
various types of attack strategies. On the CIFAR-10 dataset, CAP-GAN
outperforms other pre-processing based defenses under both black-box and
white-box settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Mingu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Trung Quang Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Seungju Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daeyoung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Molecule Edit Graph Attention Network: Modeling Chemical Reactions as Sequences of Graph Edits. (arXiv:2006.15426v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15426</id>
        <link href="http://arxiv.org/abs/2006.15426"/>
        <updated>2021-05-26T01:22:11.835Z</updated>
        <summary type="html"><![CDATA[The central challenge in automated synthesis planning is to be able to
generate and predict outcomes of a diverse set of chemical reactions. In
particular, in many cases, the most likely synthesis pathway cannot be applied
due to additional constraints, which requires proposing alternative chemical
reactions. With this in mind, we present Molecule Edit Graph Attention Network
(MEGAN), an end-to-end encoder-decoder neural model. MEGAN is inspired by
models that express a chemical reaction as a sequence of graph edits, akin to
the arrow pushing formalism. We extend this model to retrosynthesis prediction
(predicting substrates given the product of a chemical reaction) and scale it
up to large datasets. We argue that representing the reaction as a sequence of
edits enables MEGAN to efficiently explore the space of plausible chemical
reactions, maintaining the flexibility of modeling the reaction in an
end-to-end fashion, and achieving state-of-the-art accuracy in standard
benchmarks. Code and trained models are made available online at
https://github.com/molecule-one/megan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sacha_M/0/1/0/all/0/1"&gt;Miko&amp;#x142;aj Sacha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blaz_M/0/1/0/all/0/1"&gt;Miko&amp;#x142;aj B&amp;#x142;a&amp;#x17c;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byrski_P/0/1/0/all/0/1"&gt;Piotr Byrski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabrowski_Tumanski_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; D&amp;#x105;browski-Tuma&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chrominski_M/0/1/0/all/0/1"&gt;Miko&amp;#x142;aj Chromi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loska_R/0/1/0/all/0/1"&gt;Rafa&amp;#x142; Loska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wlodarczyk_Pruszynski_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; W&amp;#x142;odarczyk-Pruszy&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jastrzebski_S/0/1/0/all/0/1"&gt;Stanis&amp;#x142;aw Jastrz&amp;#x119;bski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Consistency of Decision Trees in High Dimensions. (arXiv:2104.13881v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13881</id>
        <link href="http://arxiv.org/abs/2104.13881"/>
        <updated>2021-05-26T01:22:11.819Z</updated>
        <summary type="html"><![CDATA[This paper shows that decision trees constructed with Classification and
Regression Trees (CART) methodology are universally consistent in an additive
model context, even when the number of predictor variables scales exponentially
with the sample size, under certain $1$-norm sparsity constraints. The
consistency is universal in the sense that there are no a priori assumptions on
the distribution of the predictor variables. Amazingly, this adaptivity to
(approximate or exact) sparsity is achieved with a single tree, as opposed to
what might be expected for an ensemble. Finally, we show that these qualitative
properties of individual trees are inherited by Breiman's random forests.
Another surprise is that consistency holds even when the "mtry" tuning
parameter vanishes as a fraction of the number of predictor variables, thus
speeding up computation of the forest. A key step in the analysis is the
establishment of an oracle inequality, which precisely characterizes the
goodness-of-fit and complexity tradeoff for a misspecified model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Klusowski_J/0/1/0/all/0/1"&gt;Jason M. Klusowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalised Inverse Reinforcement Learning Framework. (arXiv:2105.11812v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11812</id>
        <link href="http://arxiv.org/abs/2105.11812"/>
        <updated>2021-05-26T01:22:11.812Z</updated>
        <summary type="html"><![CDATA[The gloabal objective of inverse Reinforcement Learning (IRL) is to estimate
the unknown cost function of some MDP base on observed trajectories generated
by (approximate) optimal policies. The classical approach consists in tuning
this cost function so that associated optimal trajectories (that minimise the
cumulative discounted cost, i.e. the classical RL loss) are 'similar' to the
observed ones. Prior contributions focused on penalising degenerate solutions
and improving algorithmic scalability. Quite orthogonally to them, we question
the pertinence of characterising optimality with respect to the cumulative
discounted cost as it induces an implicit bias against policies with longer
mixing times. State of the art value based RL algorithms circumvent this issue
by solving for the fixed point of the Bellman optimality operator, a stronger
criterion that is not well defined for the inverse problem. To alleviate this
bias in IRL, we introduce an alternative training loss that puts more weights
on future states which yields a reformulation of the (maximum entropy) IRL
problem. The algorithms we devised exhibit enhanced performances (and similar
tractability) than off-the-shelf ones in multiple OpenAI gym environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jarboui_F/0/1/0/all/0/1"&gt;Firas Jarboui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1"&gt;Vianney Perchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Hierarchical Agglomerative Clustering to Billion-sized Datasets. (arXiv:2105.11653v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11653</id>
        <link href="http://arxiv.org/abs/2105.11653"/>
        <updated>2021-05-26T01:22:11.806Z</updated>
        <summary type="html"><![CDATA[Hierarchical Agglomerative Clustering (HAC) is one of the oldest but still
most widely used clustering methods. However, HAC is notoriously hard to scale
to large data sets as the underlying complexity is at least quadratic in the
number of data points and many algorithms to solve HAC are inherently
sequential. In this paper, we propose {Reciprocal Agglomerative Clustering
(RAC)}, a distributed algorithm for HAC, that uses a novel strategy to
efficiently merge clusters in parallel. We prove theoretically that RAC
recovers the exact solution of HAC. Furthermore, under clusterability and
balancedness assumption we show provable speedups in total runtime due to the
parallelism. We also show that these speedups are achievable for certain
probabilistic data models. In extensive experiments, we show that this
parallelism is achieved on real world data sets and that the proposed RAC
algorithm can recover the HAC hierarchy on billions of data points connected by
trillions of edges in less than an hour.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sumengen_B/0/1/0/all/0/1"&gt;Baris Sumengen&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1"&gt;Anand Rajagopalan&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Citovsky_G/0/1/0/all/0/1"&gt;Gui Citovsky&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Simcha_D/0/1/0/all/0/1"&gt;David Simcha&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1"&gt;Olivier Bachem&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1"&gt;Pradipta Mitra&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Blasiak_S/0/1/0/all/0/1"&gt;Sam Blasiak&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mason Liang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt; (1) ((1) Google Research, (2) 0x Labs)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning-based bias transfer for overcoming laboratory differences of microscopic images. (arXiv:2105.11765v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11765</id>
        <link href="http://arxiv.org/abs/2105.11765"/>
        <updated>2021-05-26T01:22:11.801Z</updated>
        <summary type="html"><![CDATA[The automated analysis of medical images is currently limited by technical
and biological noise and bias. The same source tissue can be represented by
vastly different images if the image acquisition or processing protocols vary.
For an image analysis pipeline, it is crucial to compensate such biases to
avoid misinterpretations. Here, we evaluate, compare, and improve existing
generative model architectures to overcome domain shifts for immunofluorescence
(IF) and Hematoxylin and Eosin (H&E) stained microscopy images. To determine
the performance of the generative models, the original and transformed images
were segmented or classified by deep neural networks that were trained only on
images of the target bias. In the scope of our analysis, U-Net cycleGANs
trained with an additional identity and an MS-SSIM-based loss and Fixed-Point
GANs trained with an additional structure loss led to the best results for the
IF and H&E stained samples, respectively. Adapting the bias of the samples
significantly improved the pixel-level segmentation for human kidney glomeruli
and podocytes and improved the classification accuracy for human prostate
biopsies by up to 14%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Thebille_A/0/1/0/all/0/1"&gt;Ann-Katrin Thebille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dietrich_E/0/1/0/all/0/1"&gt;Esther Dietrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klaus_M/0/1/0/all/0/1"&gt;Martin Klaus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gernhold_L/0/1/0/all/0/1"&gt;Lukas Gernhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lennartz_M/0/1/0/all/0/1"&gt;Maximilian Lennartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuppe_C/0/1/0/all/0/1"&gt;Christoph Kuppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kramann_R/0/1/0/all/0/1"&gt;Rafael Kramann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huber_T/0/1/0/all/0/1"&gt;Tobias B. Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sauter_G/0/1/0/all/0/1"&gt;Guido Sauter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Puelles_V/0/1/0/all/0/1"&gt;Victor G. Puelles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zimmermann_M/0/1/0/all/0/1"&gt;Marina Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bonn_S/0/1/0/all/0/1"&gt;Stefan Bonn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Black-Box Optimization Challenge via Learning Search Space Partition for Local Bayesian Optimization. (arXiv:2012.10335v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10335</id>
        <link href="http://arxiv.org/abs/2012.10335"/>
        <updated>2021-05-26T01:22:11.795Z</updated>
        <summary type="html"><![CDATA[Black-box optimization is one of the vital tasks in machine learning, since
it approximates real-world conditions, in that we do not always know all the
properties of a given system, up to knowing almost nothing but the results.
This paper describes our approach to solving the black-box optimization
challenge at NeurIPS 2020 through learning search space partition for local
Bayesian optimization. We describe the task of the challenge as well as our
algorithm for low budget optimization that we named \texttt{SPBOpt}. We
optimize the hyper-parameters of our algorithm for the competition finals using
multi-task Bayesian optimization on results from the first two evaluation
settings. Our approach has ranked third in the competition finals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sazanovich_M/0/1/0/all/0/1"&gt;Mikita Sazanovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolskaya_A/0/1/0/all/0/1"&gt;Anastasiya Nikolskaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belousov_Y/0/1/0/all/0/1"&gt;Yury Belousov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shpilman_A/0/1/0/all/0/1"&gt;Aleksei Shpilman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08850</id>
        <link href="http://arxiv.org/abs/2102.08850"/>
        <updated>2021-05-26T01:22:11.774Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has recently seen tremendous success in self-supervised
learning. So far, however, it is largely unclear why the learned
representations generalize so effectively to a large variety of downstream
tasks. We here prove that feedforward models trained with objectives belonging
to the commonly used InfoNCE family learn to implicitly invert the underlying
generative model of the observed data. While the proofs make certain
statistical assumptions about the generative model, we observe empirically that
our findings hold even if these assumptions are severely violated. Our theory
highlights a fundamental connection between contrastive learning, generative
modeling, and nonlinear independent component analysis, thereby furthering our
understanding of the learned representations as well as providing a theoretical
foundation to derive more effective contrastive losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1"&gt;Yash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1"&gt;Steffen Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistent regression when oblivious outliers overwhelm. (arXiv:2009.14774v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.14774</id>
        <link href="http://arxiv.org/abs/2009.14774"/>
        <updated>2021-05-26T01:22:11.765Z</updated>
        <summary type="html"><![CDATA[We consider a robust linear regression model $y=X\beta^* + \eta$, where an
adversary oblivious to the design $X\in \mathbb{R}^{n\times d}$ may choose
$\eta$ to corrupt all but an $\alpha$ fraction of the observations $y$ in an
arbitrary way. Prior to our work, even for Gaussian $X$, no estimator for
$\beta^*$ was known to be consistent in this model except for quadratic sample
size $n \gtrsim (d/\alpha)^2$ or for logarithmic inlier fraction $\alpha\ge
1/\log n$. We show that consistent estimation is possible with nearly linear
sample size and inverse-polynomial inlier fraction. Concretely, we show that
the Huber loss estimator is consistent for every sample size $n=
\omega(d/\alpha^2)$ and achieves an error rate of $O(d/\alpha^2n)^{1/2}$. Both
bounds are optimal (up to constant factors). Our results extend to designs far
beyond the Gaussian case and only require the column span of $X$ to not contain
approximately sparse vectors). (similar to the kind of assumption commonly made
about the kernel space for compressed sensing). We provide two technically
similar proofs. One proof is phrased in terms of strong convexity, extending
work of [Tsakonas et al.'14], and particularly short. The other proof
highlights a connection between the Huber loss estimator and high-dimensional
median computations. In the special case of Gaussian designs, this connection
leads us to a strikingly simple algorithm based on computing coordinate-wise
medians that achieves optimal guarantees in nearly-linear time, and that can
exploit sparsity of $\beta^*$. The model studied here also captures
heavy-tailed noise distributions that may not even have a first moment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+dOrsi_T/0/1/0/all/0/1"&gt;Tommaso d&amp;#x27;Orsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novikov_G/0/1/0/all/0/1"&gt;Gleb Novikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steurer_D/0/1/0/all/0/1"&gt;David Steurer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matching Targets Across Domains with RADON, the Re-Identification Across Domain Network. (arXiv:2105.12056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12056</id>
        <link href="http://arxiv.org/abs/2105.12056"/>
        <updated>2021-05-26T01:22:11.757Z</updated>
        <summary type="html"><![CDATA[We present a novel convolutional neural network that learns to match images
of an object taken from different viewpoints or by different optical sensors.
Our Re-Identification Across Domain Network (RADON) scores pairs of input
images from different domains on similarity. Our approach extends previous work
on Siamese networks and modifies them to more challenging use cases, including
low- and no-shot learning, in which few images of a specific target are
available for training. RADON shows strong performance on cross-view vehicle
matching and cross-domain person identification in a no-shot learning
environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burgess_C/0/1/0/all/0/1"&gt;Cassandra Burgess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neisinger_C/0/1/0/all/0/1"&gt;Cordelia Neisinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinner_R/0/1/0/all/0/1"&gt;Rafael Dinner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Transformer and LSTM to Kalman Filter with EM algorithm for state estimation. (arXiv:2105.00250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00250</id>
        <link href="http://arxiv.org/abs/2105.00250"/>
        <updated>2021-05-26T01:22:11.752Z</updated>
        <summary type="html"><![CDATA[Kalman Filter requires the true parameters of the model and solves optimal
state estimation recursively. Expectation Maximization (EM) algorithm is
applicable for estimating the parameters of the model that are not available
before Kalman filtering, which is EM-KF algorithm. To improve the preciseness
of EM-KF algorithm, the author presents a state estimation method by combining
the Long-Short Term Memory network (LSTM), Transformer and EM-KF algorithm in
the framework of Encoder-Decoder in Sequence to Sequence (seq2seq). Simulation
on a linear mobile robot model demonstrates that the new method is more
accurate. Source code of this paper is available at
https://github.com/zshicode/Deep-Learning-Based-State-Estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhuangwei Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BATS: A Spectral Biclustering Approach to Single Document Topic Modeling and Segmentation. (arXiv:2008.02218v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02218</id>
        <link href="http://arxiv.org/abs/2008.02218"/>
        <updated>2021-05-26T01:22:11.746Z</updated>
        <summary type="html"><![CDATA[Existing topic modeling and text segmentation methodologies generally require
large datasets for training, limiting their capabilities when only small
collections of text are available. In this work, we reexamine the inter-related
problems of "topic identification" and "text segmentation" for sparse document
learning, when there is a single new text of interest. In developing a
methodology to handle single documents, we face two major challenges. First is
sparse information: with access to only one document, we cannot train
traditional topic models or deep learning algorithms. Second is significant
noise: a considerable portion of words in any single document will produce only
noise and not help discern topics or segments. To tackle these issues, we
design an unsupervised, computationally efficient methodology called BATS:
Biclustering Approach to Topic modeling and Segmentation. BATS leverages three
key ideas to simultaneously identify topics and segment text: (i) a new
mechanism that uses word order information to reduce sample complexity, (ii) a
statistically sound graph-based biclustering technique that identifies latent
structures of words and sentences, and (iii) a collection of effective
heuristics that remove noise words and award important words to further improve
performance. Experiments on four datasets show that our approach outperforms
several state-of-the-art baselines when considering topic coherence, topic
diversity, segmentation, and runtime comparison metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qiong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_A/0/1/0/all/0/1"&gt;Adam Hare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sirui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1"&gt;Yuwei Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1"&gt;Christopher G. Brinton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanhua Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Reachability Analysis of Closed-Loop Systems with Neural Network Controllers. (arXiv:2101.01815v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01815</id>
        <link href="http://arxiv.org/abs/2101.01815"/>
        <updated>2021-05-26T01:22:11.740Z</updated>
        <summary type="html"><![CDATA[Neural Networks (NNs) can provide major empirical performance improvements
for robotic systems, but they also introduce challenges in formally analyzing
those systems' safety properties. In particular, this work focuses on
estimating the forward reachable set of closed-loop systems with NN
controllers. Recent work provides bounds on these reachable sets, yet the
computationally efficient approaches provide overly conservative bounds (thus
cannot be used to verify useful properties), whereas tighter methods are too
intensive for online computation. This work bridges the gap by formulating a
convex optimization problem for reachability analysis for closed-loop systems
with NN controllers. While the solutions are less tight than prior semidefinite
program-based methods, they are substantially faster to compute, and some of
the available computation time can be used to refine the bounds through input
set partitioning, which more than overcomes the tightness gap. The proposed
framework further considers systems with measurement and process noise, thus
being applicable to realistic systems with uncertainty. Finally, numerical
comparisons show $10\times$ reduction in conservatism in $\frac{1}{2}$ of the
computation time compared to the state-of-the-art, and the ability to handle
various sources of uncertainty is highlighted on a quadrotor model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Everett_M/0/1/0/all/0/1"&gt;Michael Everett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Habibi_G/0/1/0/all/0/1"&gt;Golnaz Habibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+How_J/0/1/0/all/0/1"&gt;Jonathan P. How&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperparameter Selection for Imitation Learning. (arXiv:2105.12034v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12034</id>
        <link href="http://arxiv.org/abs/2105.12034"/>
        <updated>2021-05-26T01:22:11.722Z</updated>
        <summary type="html"><![CDATA[We address the issue of tuning hyperparameters (HPs) for imitation learning
algorithms in the context of continuous-control, when the underlying reward
function of the demonstrating expert cannot be observed at any time. The vast
literature in imitation learning mostly considers this reward function to be
available for HP selection, but this is not a realistic setting. Indeed, would
this reward function be available, it could then directly be used for policy
training and imitation would not be necessary. To tackle this mostly ignored
problem, we propose a number of possible proxies to the external reward. We
evaluate them in an extensive empirical study (more than 10'000 agents across 9
environments) and make practical recommendations for selecting HPs. Our results
show that while imitation learning algorithms are sensitive to HP choices, it
is often possible to select good enough HPs through a proxy to the reward
function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hussenot_L/0/1/0/all/0/1"&gt;Leonard Hussenot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andrychowicz_M/0/1/0/all/0/1"&gt;Marcin Andrychowicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_D/0/1/0/all/0/1"&gt;Damien Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dadashi_R/0/1/0/all/0/1"&gt;Robert Dadashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raichuk_A/0/1/0/all/0/1"&gt;Anton Raichuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stafiniak_L/0/1/0/all/0/1"&gt;Lukasz Stafiniak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girgin_S/0/1/0/all/0/1"&gt;Sertan Girgin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marinier_R/0/1/0/all/0/1"&gt;Raphael Marinier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momchev_N/0/1/0/all/0/1"&gt;Nikola Momchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramos_S/0/1/0/all/0/1"&gt;Sabela Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orsini_M/0/1/0/all/0/1"&gt;Manu Orsini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1"&gt;Olivier Bachem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1"&gt;Matthieu Geist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1"&gt;Olivier Pietquin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A unified framework based on graph consensus term for multi-view learning. (arXiv:2105.11781v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11781</id>
        <link href="http://arxiv.org/abs/2105.11781"/>
        <updated>2021-05-26T01:22:11.716Z</updated>
        <summary type="html"><![CDATA[In recent years, multi-view learning technologies for various applications
have attracted a surge of interest. Due to more compatible and complementary
information from multiple views, existing multi-view methods could achieve more
promising performance than conventional single-view methods in most situations.
However, there are still no sufficient researches on the unified framework in
existing multi-view works. Meanwhile, how to efficiently integrate multi-view
information is still full of challenges. In this paper, we propose a novel
multi-view learning framework, which aims to leverage most existing graph
embedding works into a unified formula via introducing the graph consensus
term. In particular, our method explores the graph structure in each view
independently to preserve the diversity property of graph embedding methods.
Meanwhile, we choose heterogeneous graphs to construct the graph consensus term
to explore the correlations among multiple views jointly. To this end, the
diversity and complementary information among different views could be
simultaneously considered. Furthermore, the proposed framework is utilized to
implement the multi-view extension of Locality Linear Embedding, named
Multi-view Locality Linear Embedding (MvLLE), which could be efficiently solved
by applying the alternating optimization strategy. Empirical validations
conducted on six benchmark datasets can show the effectiveness of our proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiangzhu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chonghui Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSIT-Free Federated Edge Learning via Reconfigurable Intelligent Surface. (arXiv:2102.10749v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10749</id>
        <link href="http://arxiv.org/abs/2102.10749"/>
        <updated>2021-05-26T01:22:11.710Z</updated>
        <summary type="html"><![CDATA[We study over-the-air model aggregation in federated edge learning (FEEL)
systems, where channel state information at the transmitters (CSIT) is assumed
to be unavailable. We leverage the reconfigurable intelligent surface (RIS)
technology to align the cascaded channel coefficients for CSIT-free model
aggregation. To this end, we jointly optimize the RIS and the receiver by
minimizing the aggregation error under the channel alignment constraint. We
then develop a difference-of-convex algorithm for the resulting non-convex
optimization. Numerical experiments on image classification show that the
proposed method is able to achieve a similar learning accuracy as the
state-of-the-art CSIT-based solution, demonstrating the efficiency of our
approach in combating the lack of CSIT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaojun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ying-Jun Angela Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SLOE: A Faster Method for Statistical Inference in High-Dimensional Logistic Regression. (arXiv:2103.12725v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12725</id>
        <link href="http://arxiv.org/abs/2103.12725"/>
        <updated>2021-05-26T01:22:11.705Z</updated>
        <summary type="html"><![CDATA[Logistic regression remains one of the most widely used tools in applied
statistics, machine learning and data science. However, in moderately
high-dimensional problems, where the number of features $d$ is a non-negligible
fraction of the sample size $n$, the logistic regression maximum likelihood
estimator (MLE), and statistical procedures based the large-sample
approximation of its distribution, behave poorly. Recently, Sur and Cand\`es
(2019) showed that these issues can be corrected by applying a new
approximation of the MLE's sampling distribution in this high-dimensional
regime. Unfortunately, these corrections are difficult to implement in
practice, because they require an estimate of the \emph{signal strength}, which
is a function of the underlying parameters $\beta$ of the logistic regression.
To address this issue, we propose SLOE, a fast and straightforward approach to
estimate the signal strength in logistic regression. The key insight of SLOE is
that the Sur and Cand\`es (2019) correction can be reparameterized in terms of
the \emph{corrupted signal strength}, which is only a function of the estimated
parameters $\widehat \beta$. We propose an estimator for this quantity, prove
that it is consistent in the relevant high-dimensional regime, and show that
dimensionality correction using SLOE is accurate in finite samples. Compared to
the existing ProbeFrontier heuristic, SLOE is conceptually simpler and orders
of magnitude faster, making it suitable for routine use. We demonstrate the
importance of routine dimensionality correction in the Heart Disease dataset
from the UCI repository, and a genomics application using data from the UK
Biobank. We provide an open source package for this method, available at
\url{https://github.com/google-research/sloe-logistic}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yadlowsky_S/0/1/0/all/0/1"&gt;Steve Yadlowsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yun_T/0/1/0/all/0/1"&gt;Taedong Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+McLean_C/0/1/0/all/0/1"&gt;Cory McLean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+DAmour_A/0/1/0/all/0/1"&gt;Alexander D&amp;#x27;Amour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Efficient Multilevel Clustering via Wasserstein Distances. (arXiv:1909.08787v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.08787</id>
        <link href="http://arxiv.org/abs/1909.08787"/>
        <updated>2021-05-26T01:22:11.699Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach to the problem of multilevel clustering, which
aims to simultaneously partition data in each group and discover grouping
patterns among groups in a potentially large hierarchically structured corpus
of data. Our method involves a joint optimization formulation over several
spaces of discrete probability measures, which are endowed with Wasserstein
distance metrics. We propose several variants of this problem, which admit fast
optimization algorithms, by exploiting the connection to the problem of finding
Wasserstein barycenters. Consistency properties are established for the
estimates of both local and global clusters. Finally, experimental results with
both synthetic and real data are presented to demonstrate the flexibility and
scalability of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Huynh_V/0/1/0/all/0/1"&gt;Viet Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dam_N/0/1/0/all/0/1"&gt;Nhan Dam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_X/0/1/0/all/0/1"&gt;XuanLong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yurochkin_M/0/1/0/all/0/1"&gt;Mikhail Yurochkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bui_H/0/1/0/all/0/1"&gt;Hung Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Phung_a/0/1/0/all/0/1"&gt;and Dinh Phung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Need of Removing Last Releases of Data When Using or Validating Defect Prediction Models. (arXiv:2003.14376v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.14376</id>
        <link href="http://arxiv.org/abs/2003.14376"/>
        <updated>2021-05-26T01:22:11.683Z</updated>
        <summary type="html"><![CDATA[To develop and train defect prediction models, researchers rely on datasets
in which a defect is attributed to an artifact, e.g., a class of a given
release. However, the creation of such datasets is far from being perfect. It
can happen that a defect is discovered several releases after its introduction:
this phenomenon has been called "dormant defects". This means that, if we
observe today the status of a class in its current version, it can be
considered as defect-free while this is not the case. We call "snoring" the
noise consisting of such classes, affected by dormant defects only. We
conjecture that the presence of snoring negatively impacts the classifiers'
accuracy and their evaluation. Moreover, earlier releases likely contain more
snoring classes than older releases, thus, removing the most recent releases
from a dataset could reduce the snoring effect and improve the accuracy of
classifiers. In this paper we investigate the impact of the snoring noise on
classifiers' accuracy and their evaluation, and the effectiveness of a possible
countermeasure consisting in removing the last releases of data. We analyze the
accuracy of 15 machine learning defect prediction classifiers on data from more
than 4,000 bugs and 600 releases of 19 open source projects from the Apache
ecosystem. Our results show that, on average across projects: (i) the presence
of snoring decreases the recall of defect prediction classifiers; (ii)
evaluations affected by snoring are likely unable to identify the best
classifiers, and (iii) removing data from recent releases helps to
significantly improve the accuracy of the classifiers. On summary, this paper
provides insights on how to create a software defect dataset by mitigating the
effect of snoring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahluwalia_A/0/1/0/all/0/1"&gt;Aalok Ahluwalia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Penta_M/0/1/0/all/0/1"&gt;Massimiliano Di Penta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falessi_D/0/1/0/all/0/1"&gt;Davide Falessi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with Co-designed Compressed Neural Networks. (arXiv:2010.12861v2 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12861</id>
        <link href="http://arxiv.org/abs/2010.12861"/>
        <updated>2021-05-26T01:22:11.675Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) play a key role in deep learning
applications. However, the large storage overheads and the substantial
computation cost of CNNs are problematic in hardware accelerators.
Computing-in-memory (CIM) architecture has demonstrated great potential to
effectively compute large-scale matrix-vector multiplication. However, the
intensive multiply and accumulation (MAC) operations executed at the crossbar
array and the limited capacity of CIM macros remain bottlenecks for further
improvement of energy efficiency and throughput. To reduce computation costs,
network pruning and quantization are two widely studied compression methods to
shrink the model size. However, most of the model compression algorithms can
only be implemented in digital-based CNN accelerators. For implementation in a
static random access memory (SRAM) CIM-based accelerator, the model compression
algorithm must consider the hardware limitations of CIM macros, such as the
number of word lines and bit lines that can be turned on at the same time, as
well as how to map the weight to the SRAM CIM macro. In this study, a software
and hardware co-design approach is proposed to design an SRAM CIM-based CNN
accelerator and an SRAM CIM-aware model compression algorithm. To lessen the
high-precision MAC required by batch normalization (BN), a quantization
algorithm that can fuse BN into the weights is proposed. Furthermore, to reduce
the number of network parameters, a sparsity algorithm that considers a CIM
architecture is proposed. Last, MARS, a CIM-based CNN accelerator that can
utilize multiple SRAM CIM macros as processing units and support a sparsity
neural network, is proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sie_S/0/1/0/all/0/1"&gt;Syuan-Hao Sie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jye-Luen Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Ren Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chih-Cheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Chih-Cheng Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1"&gt;Meng-Fan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Kea-Tiong Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey on Semi-, Self- and Unsupervised Learning for Image Classification. (arXiv:2002.08721v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08721</id>
        <link href="http://arxiv.org/abs/2002.08721"/>
        <updated>2021-05-26T01:22:11.670Z</updated>
        <summary type="html"><![CDATA[While deep learning strategies achieve outstanding results in computer vision
tasks, one issue remains: The current strategies rely heavily on a huge amount
of labeled data. In many real-world problems, it is not feasible to create such
an amount of labeled training data. Therefore, it is common to incorporate
unlabeled data into the training process to reach equal results with fewer
labels. Due to a lot of concurrent research, it is difficult to keep track of
recent developments. In this survey, we provide an overview of often used ideas
and methods in image classification with fewer labels. We compare 34 methods in
detail based on their performance and their commonly used ideas rather than a
fine-grained taxonomy. In our analysis, we identify three major trends that
lead to future research opportunities. 1. State-of-the-art methods are
scaleable to real-world applications in theory but issues like class imbalance,
robustness, or fuzzy labels are not considered. 2. The degree of supervision
which is needed to achieve comparable results to the usage of all labels is
decreasing and therefore methods need to be extended to settings with a
variable number of classes. 3. All methods share some common ideas but we
identify clusters of methods that do not share many ideas. We show that
combining ideas from different clusters can lead to better performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1"&gt;Lars Schmarje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1"&gt;Monty Santarossa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1"&gt;Simon-Martin Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1"&gt;Reinhard Koch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FSOCO: The Formula Student Objects in Context Dataset. (arXiv:2012.07139v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07139</id>
        <link href="http://arxiv.org/abs/2012.07139"/>
        <updated>2021-05-26T01:22:11.663Z</updated>
        <summary type="html"><![CDATA[This paper presents the FSOCO dataset, a collaborative dataset for
vision-based cone detection systems in Formula Student Driverless competitions.
It contains human annotated ground truth labels for both bounding boxes and
instance-wise segmentation masks. The data buy-in philosophy of FSOCO asks
student teams to contribute to the database first before being granted access
ensuring continuous growth. By providing clear labeling guidelines and tools
for a sophisticated raw image selection, new annotations are guaranteed to meet
the desired quality. The effectiveness of the approach is shown by comparing
prediction results of a network trained on FSOCO and its unregulated
predecessor. The FSOCO dataset can be found at fsoco-dataset.com.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dodel_D/0/1/0/all/0/1"&gt;David Dodel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schotz_M/0/1/0/all/0/1"&gt;Michael Sch&amp;#xf6;tz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vodisch_N/0/1/0/all/0/1"&gt;Niclas V&amp;#xf6;disch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems. (arXiv:2105.11558v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11558</id>
        <link href="http://arxiv.org/abs/2105.11558"/>
        <updated>2021-05-26T01:22:11.657Z</updated>
        <summary type="html"><![CDATA[We consider the setting of vector valued non-linear dynamical systems
$X_{t+1} = \phi(A^* X_t) + \eta_t$, where $\eta_t$ is unbiased noise and $\phi
: \mathbb{R} \to \mathbb{R}$ is a known link function that satisfies certain
{\em expansivity property}. The goal is to learn $A^*$ from a single trajectory
$X_1,\cdots,X_T$ of {\em dependent or correlated} samples. While the problem is
well-studied in the linear case, where $\phi$ is identity, with optimal error
rates even for non-mixing systems, existing results in the non-linear case hold
only for mixing systems. In this work, we improve existing results for learning
nonlinear systems in a number of ways: a) we provide the first offline
algorithm that can learn non-linear dynamical systems without the mixing
assumption, b) we significantly improve upon the sample complexity of existing
results for mixing systems, c) in the much harder one-pass, streaming setting
we study a SGD with Reverse Experience Replay ($\mathsf{SGD-RER}$) method, and
demonstrate that for mixing systems, it achieves the same sample complexity as
our offline algorithm, d) we justify the expansivity assumption by showing that
for the popular ReLU link function -- a non-expansive but easy to learn link
function with i.i.d. samples -- any method would require exponentially many
samples (with respect to dimension of $X_t$) from the dynamical system. We
validate our results via. simulations and demonstrate that a naive application
of SGD can be highly sub-optimal. Indeed, our work demonstrates that for
correlated data, specialized methods designed for the dependency structure in
data can significantly outperform standard SGD based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Suhas S Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraj_D/0/1/0/all/0/1"&gt;Dheeraj Nagaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Making Few-Shot Learning Stronger. (arXiv:2105.11904v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11904</id>
        <link href="http://arxiv.org/abs/2105.11904"/>
        <updated>2021-05-26T01:22:11.638Z</updated>
        <summary type="html"><![CDATA[Few-shot learning has been proposed and rapidly emerging as a viable means
for completing various tasks. Many few-shot models have been widely used for
relation learning tasks. However, each of these models has a shortage of
capturing a certain aspect of semantic features, for example, CNN on long-range
dependencies part, Transformer on local features. It is difficult for a single
model to adapt to various relation learning, which results in the high variance
problem. Ensemble strategy could be competitive on improving the accuracy of
few-shot relation extraction and mitigating high variance risks. This paper
explores an ensemble approach to reduce the variance and introduces fine-tuning
and feature attention strategies to calibrate relation-level features. Results
on several few-shot relation learning tasks show that our model significantly
outperforms the previous state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qing Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongbin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1"&gt;Wen Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1"&gt;Zhihua Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Federated Learning Approach to Anomaly Detection in Smart Buildings. (arXiv:2010.10293v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10293</id>
        <link href="http://arxiv.org/abs/2010.10293"/>
        <updated>2021-05-26T01:22:11.633Z</updated>
        <summary type="html"><![CDATA[Internet of Things (IoT) sensors in smart buildings are becoming increasingly
ubiquitous, making buildings more livable, energy efficient, and sustainable.
These devices sense the environment and generate multivariate temporal data of
paramount importance for detecting anomalies and improving the prediction of
energy usage in smart buildings. However, detecting these anomalies in
centralized systems is often plagued by a huge delay in response time. To
overcome this issue, we formulate the anomaly detection problem in a federated
learning setting by leveraging the multi-task learning paradigm, which aims at
solving multiple tasks simultaneously while taking advantage of the
similarities and differences across tasks. We propose a novel privacy-by-design
federated learning model using a stacked long short-time memory (LSTM) model,
and we demonstrate that it is more than twice as fast during training
convergence compared to the centralized LSTM. The effectiveness of our
federated learning approach is demonstrated on three real-world datasets
generated by the IoT production system at General Electric Current smart
building, achieving state-of-the-art performance compared to baseline methods
in both classification and regression tasks. Our experimental results
demonstrate the effectiveness of the proposed framework in reducing the overall
training cost without compromising the prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sater_R/0/1/0/all/0/1"&gt;Raed Abdel Sater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1"&gt;A. Ben Hamza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiBS: Differentiable Bayesian Structure Learning. (arXiv:2105.11839v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11839</id>
        <link href="http://arxiv.org/abs/2105.11839"/>
        <updated>2021-05-26T01:22:11.627Z</updated>
        <summary type="html"><![CDATA[Bayesian structure learning allows inferring Bayesian network structure from
data while reasoning about the epistemic uncertainty -- a key element towards
enabling active causal discovery and designing interventions in real world
systems. In this work, we propose a general, fully differentiable framework for
Bayesian structure learning (DiBS) that operates in the continuous space of a
latent probabilistic graph representation. Building on recent advances in
variational inference, we use DiBS to devise an efficient method for
approximating posteriors over structural models. Contrary to existing work,
DiBS is agnostic to the form of the local conditional distributions and allows
for joint posterior inference of both the graph structure and the conditional
distribution parameters. This makes our method directly applicable to posterior
inference of nonstandard Bayesian network models, e.g., with nonlinear
dependencies encoded by neural networks. In evaluations on simulated and
real-world data, DiBS significantly outperforms related approaches to joint
posterior inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lorch_L/0/1/0/all/0/1"&gt;Lars Lorch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rothfuss_J/0/1/0/all/0/1"&gt;Jonas Rothfuss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Regression Activation Maps For Lesion Segmentation in CT scans of COVID-19 patients. (arXiv:2105.11748v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11748</id>
        <link href="http://arxiv.org/abs/2105.11748"/>
        <updated>2021-05-26T01:22:11.621Z</updated>
        <summary type="html"><![CDATA[Automatic lesion segmentation on thoracic CT enables rapid quantitative
analysis of lung involvement in COVID- 19 infections. Obtaining voxel-level
annotations for training segmentation networks is prohibitively expensive.
Therefore we propose a weakly-supervised segmentation method based on dense
regression activation maps (dRAM). Most advanced weakly supervised segmentation
approaches exploit class activation maps (CAMs) to localize objects generated
from high-level semantic features at a coarse resolution. As a result, CAMs
provide coarse outlines that do not align precisely with the object
segmentations. Instead, we exploit dense features from a segmentation network
to compute dense regression activation maps (dRAMs) for preserving local
details. During training, dRAMs are pooled lobe-wise to regress the per-lobe
lesion percentage. In such a way, the network achieves additional information
regarding the lesion quantification in comparison with the classification
approach. Furthermore, we refine dRAMs based on an attention module and dense
conditional random field trained together with the main regression task. The
refined dRAMs are served as the pseudo labels for training a final segmentation
network. When evaluated on 69 CT scans, our method substantially improves the
intersection over union from 0.335 in the CAM-based weakly supervised
segmentation method to 0.495.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weiyi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jacobs_C/0/1/0/all/0/1"&gt;Colin Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1"&gt;Bram van Ginneken&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11866</id>
        <link href="http://arxiv.org/abs/2105.11866"/>
        <updated>2021-05-26T01:22:11.615Z</updated>
        <summary type="html"><![CDATA[Factorization machine (FM) is a prevalent approach to modeling pairwise
(second-order) feature interactions when dealing with high-dimensional sparse
data. However, on the one hand, FM fails to capture higher-order feature
interactions suffering from combinatorial expansion, on the other hand, taking
into account interaction between every pair of features may introduce noise and
degrade prediction accuracy. To solve the problems, we propose a novel approach
Graph Factorization Machine (GraphFM) by naturally representing features in the
graph structure. In particular, a novel mechanism is designed to select the
beneficial feature interactions and formulate them as edges between features.
Then our proposed model which integrates the interaction function of FM into
the feature aggregation strategy of Graph Neural Network (GNN), can model
arbitrary-order feature interactions on the graph-structured features by
stacking layers. Experimental results on several real-world datasets has
demonstrated the rationality and effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zeyu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias-Robust Bayesian Optimization via Dueling Bandit. (arXiv:2105.11802v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.11802</id>
        <link href="http://arxiv.org/abs/2105.11802"/>
        <updated>2021-05-26T01:22:11.602Z</updated>
        <summary type="html"><![CDATA[We consider Bayesian optimization in settings where observations can be
adversarially biased, for example by an uncontrolled hidden confounder. Our
first contribution is a reduction of the confounded setting to the dueling
bandit model. Then we propose a novel approach for dueling bandits based on
information-directed sampling (IDS). Thereby, we obtain the first efficient
kernelized algorithm for dueling bandits that comes with cumulative regret
guarantees. Our analysis further generalizes a previously proposed
semi-parametric linear bandit model to non-linear reward functions, and
uncovers interesting links to doubly-robust estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kirschner_J/0/1/0/all/0/1"&gt;Johannes Kirschner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Reducing Biases in Combining Multiple Experts Online. (arXiv:1908.07009v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.07009</id>
        <link href="http://arxiv.org/abs/1908.07009"/>
        <updated>2021-05-26T01:22:11.597Z</updated>
        <summary type="html"><![CDATA[In many real life situations, including job and loan applications,
gatekeepers must make justified and fair real-time decisions about a person's
fitness for a particular opportunity. In this paper, we aim to accomplish
approximate group fairness in an online stochastic decision-making process,
where the fairness metric we consider is equalized odds. Our work follows from
the classical learning-from-experts scheme, assuming a finite set of
classifiers (human experts, rules, options, etc) that cannot be modified. We
run separate instances of the algorithm for each label class as well as
sensitive groups, where the probability of choosing each instance is optimized
for both fairness and regret. Our theoretical results show that approximately
equalized odds can be achieved without sacrificing much regret. We also
demonstrate the performance of the algorithm on real data sets commonly used by
the fairness community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramirez_I/0/1/0/all/0/1"&gt;Ivan Ramirez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuesta_Infante_A/0/1/0/all/0/1"&gt;Alfredo Cuesta-Infante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1"&gt;Kalyan Veeramachaneni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Explainable Multi-Party Learning: A Contrastive Knowledge Sharing Framework. (arXiv:2104.06670v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06670</id>
        <link href="http://arxiv.org/abs/2104.06670"/>
        <updated>2021-05-26T01:22:11.590Z</updated>
        <summary type="html"><![CDATA[Multi-party learning provides solutions for training joint models with
decentralized data under legal and practical constraints. However, traditional
multi-party learning approaches are confronted with obstacles such as system
heterogeneity, statistical heterogeneity, and incentive design. How to deal
with these challenges and further improve the efficiency and performance of
multi-party learning has become an urgent problem to be solved. In this paper,
we propose a novel contrastive multi-party learning framework for knowledge
refinement and sharing with an accountable incentive mechanism. Since the
existing naive model parameter averaging method is contradictory to the
learning paradigm of neural networks, we simulate the process of human
cognition and communication, and analogy multi-party learning as a many-to-one
knowledge sharing problem. The approach is capable of integrating the acquired
explicit knowledge of each client in a transparent manner without privacy
disclosure, and it reduces the dependence on data distribution and
communication environments. The proposed scheme achieves significant
improvement in model performance in a variety of scenarios, as we demonstrated
through experiments on several real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yuan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiawei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Maoguo Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1"&gt;A. K. Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Uncertainty in Deep Spatiotemporal Forecasting. (arXiv:2105.11982v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.11982</id>
        <link href="http://arxiv.org/abs/2105.11982"/>
        <updated>2021-05-26T01:22:11.576Z</updated>
        <summary type="html"><![CDATA[Deep learning is gaining increasing popularity for spatiotemporal
forecasting. However, prior works have mostly focused on point estimates
without quantifying the uncertainty of the predictions. In high stakes domains,
being able to generate probabilistic forecasts with confidence intervals is
critical to risk assessment and decision making. Hence, a systematic study of
uncertainty quantification (UQ) methods for spatiotemporal forecasting is
missing in the community. In this paper, we describe two types of
spatiotemporal forecasting problems: regular grid-based and graph-based. Then
we analyze UQ methods from both the Bayesian and the frequentist point of view,
casting in a unified framework via statistical decision theory. Through
extensive experiments on real-world road network traffic, epidemics, and air
quality forecasting tasks, we reveal the statistical and computational
trade-offs for different UQ methods: Bayesian methods are typically more robust
in mean prediction, while confidence levels obtained from frequentist methods
provide more extensive coverage over data variations. Computationally, quantile
regression type methods are cheaper for a single confidence interval but
require re-training for different intervals. Sampling based methods generate
samples that can form multiple confidence intervals, albeit at a higher
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongxia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Liyao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1"&gt;Xinyue Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinazzi_M/0/1/0/all/0/1"&gt;Matteo Chinazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vespignani_A/0/1/0/all/0/1"&gt;Alessandro Vespignani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi-An Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rose Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Utterance partitioning for speaker recognition: an experimental review and analysis with new findings under GMM-SVM framework. (arXiv:2105.11728v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11728</id>
        <link href="http://arxiv.org/abs/2105.11728"/>
        <updated>2021-05-26T01:22:11.563Z</updated>
        <summary type="html"><![CDATA[The performance of speaker recognition system is highly dependent on the
amount of speech used in enrollment and test. This work presents a detailed
experimental review and analysis of the GMM-SVM based speaker recognition
system in presence of duration variability. This article also reports a
comparison of the performance of GMM-SVM classifier with its precursor
technique Gaussian mixture model-universal background model (GMM-UBM)
classifier in presence of duration variability. The goal of this research work
is not to propose a new algorithm for improving speaker recognition performance
in presence of duration variability. However, the main focus of this work is on
utterance partitioning (UP), a commonly used strategy to compensate the
duration variability issue. We have analysed in detailed the impact of training
utterance partitioning in speaker recognition performance under GMM-SVM
framework. We further investigate the reason why the utterance partitioning is
important for boosting speaker recognition performance. We have also shown in
which case the utterance partitioning could be useful and where not. Our study
has revealed that utterance partitioning does not reduce the data imbalance
problem of the GMM-SVM classifier as claimed in earlier study. Apart from
these, we also discuss issues related to the impact of parameters such as
number of Gaussians, supervector length, amount of splitting required for
obtaining better performance in short and long duration test conditions from
speech duration perspective. We have performed the experiments with telephone
speech from POLYCOST corpus consisting of 130 speakers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sen_N/0/1/0/all/0/1"&gt;Nirmalya Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahidullah_M/0/1/0/all/0/1"&gt;Md Sahidullah&lt;/a&gt; (MULTISPEECH), &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_H/0/1/0/all/0/1"&gt;Hemant Patil&lt;/a&gt; (DA-IICT), &lt;a href="http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1"&gt;Shyamal Kumar das Mandal&lt;/a&gt; (IIT Kharagpur), &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1"&gt;Sreenivasa Krothapalli Rao&lt;/a&gt; (IIT Kharagpur), &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_T/0/1/0/all/0/1"&gt;Tapan Kumar Basu&lt;/a&gt; (IIT Kharagpur)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for Key Information Extraction from Documents. (arXiv:2105.11672v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11672</id>
        <link href="http://arxiv.org/abs/2105.11672"/>
        <updated>2021-05-26T01:22:11.550Z</updated>
        <summary type="html"><![CDATA[Recent grid-based document representations like BERTgrid allow the
simultaneous encoding of the textual and layout information of a document in a
2D feature map so that state-of-the-art image segmentation and/or object
detection models can be straightforwardly leveraged to extract key information
from documents. However, such methods have not achieved comparable performance
to state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK
yet. In this paper, we propose a new multi-modal backbone network by
concatenating a BERTgrid to an intermediate layer of a CNN model, where the
input of CNN is a document image and the BERTgrid is a grid of word embeddings,
to generate a more powerful grid-based document representation, named
ViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal
backbone network are trained jointly. Our experimental results demonstrate that
this joint training strategy improves significantly the representation ability
of ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction
approach has achieved state-of-the-art performance on real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weihong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qifang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhuoyao Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1"&gt;Kai Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1"&gt;Qin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1"&gt;Qiang Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principal Component Hierarchy for Sparse Quadratic Programs. (arXiv:2105.12022v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2105.12022</id>
        <link href="http://arxiv.org/abs/2105.12022"/>
        <updated>2021-05-26T01:22:11.543Z</updated>
        <summary type="html"><![CDATA[We propose a novel approximation hierarchy for cardinality-constrained,
convex quadratic programs that exploits the rank-dominating eigenvectors of the
quadratic matrix. Each level of approximation admits a min-max characterization
whose objective function can be optimized over the binary variables
analytically, while preserving convexity in the continuous variables.
Exploiting this property, we propose two scalable optimization algorithms,
coined as the "best response" and the "dual program", that can efficiently
screen the potential indices of the nonzero elements of the original program.
We show that the proposed methods are competitive with the existing screening
methods in the current sparse regression literature, and it is particularly
fast on instances with high number of measurements in experiments with both
synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Vreugdenhil_R/0/1/0/all/0/1"&gt;Robbie Vreugdenhil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Viet Anh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Eftekhari_A/0/1/0/all/0/1"&gt;Armin Eftekhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Esfahani_P/0/1/0/all/0/1"&gt;Peyman Mohajerin Esfahani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Mismatch Trade-offs in LMMSE Estimation. (arXiv:2105.11964v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2105.11964</id>
        <link href="http://arxiv.org/abs/2105.11964"/>
        <updated>2021-05-26T01:22:11.527Z</updated>
        <summary type="html"><![CDATA[We consider a linear minimum mean squared error (LMMSE) estimation framework
with model mismatch where the assumed model order is smaller than that of the
underlying linear system which generates the data used in the estimation
process. By modelling the regressors of the underlying system as random
variables, we analyze the average behaviour of the mean squared error (MSE).
Our results quantify how the MSE depends on the interplay between the number of
samples and the number of parameters in the underlying system and in the
assumed model. In particular, if the number of samples is not sufficiently
large, neither increasing the number of samples nor the assumed model
complexity is sufficient to guarantee a performance improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hellkvist_M/0/1/0/all/0/1"&gt;Martin Hellkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozcelikkale_A/0/1/0/all/0/1"&gt;Ay&amp;#xe7;a &amp;#xd6;z&amp;#xe7;elikkale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deconfounded Score Method: Scoring DAGs with Dense Unobserved Confounding. (arXiv:2103.15106v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15106</id>
        <link href="http://arxiv.org/abs/2103.15106"/>
        <updated>2021-05-26T01:22:11.463Z</updated>
        <summary type="html"><![CDATA[Unobserved confounding is one of the greatest challenges for causal
discovery. The case in which unobserved variables have a widespread effect on
many of the observed ones is particularly difficult because most pairs of
variables are conditionally dependent given any other subset, rendering the
causal effect unidentifiable. In this paper we show that beyond conditional
independencies, under the principle of independent mechanisms, unobserved
confounding in this setting leaves a statistical footprint in the observed data
distribution that allows for disentangling spurious and causal effects. Using
this insight, we demonstrate that a sparse linear Gaussian directed acyclic
graph among observed variables may be recovered approximately and propose an
adjusted score-based causal discovery algorithm that may be implemented with
general purpose solvers and scales to high-dimensional problems. We find, in
addition, that despite the conditions we pose to guarantee causal recovery,
performance in practice is robust to large deviations in model assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bellot_A/0/1/0/all/0/1"&gt;Alexis Bellot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a method to anticipate dark matter signals with deep learning at the LHC. (arXiv:2105.12018v1 [hep-ph])]]></title>
        <id>http://arxiv.org/abs/2105.12018</id>
        <link href="http://arxiv.org/abs/2105.12018"/>
        <updated>2021-05-26T01:22:11.460Z</updated>
        <summary type="html"><![CDATA[We study several simplified dark matter (DM) models and their signatures at
the LHC using neural networks. We focus on the usual monojet plus missing
transverse energy channel, but to train the algorithms we organize the data in
2D histograms instead of event-by-event arrays. This results in a large
performance boost to distinguish between standard model (SM) only and SM plus
new physics signals. We use the kinematic monojet features as input data which
allow us to describe families of models with a single data sample. We found
that the neural network performance does not depend on the simulated number of
background events if they are presented as a function of $S/\sqrt{B}$, where
$S$ and $B$ are the number of signal and background events per histogram,
respectively. This provides flexibility to the method, since testing a
particular model in that case only requires knowing the new physics monojet
cross section. Furthermore, we also discuss the network performance under
incorrect assumptions about the true DM nature. Finally, we propose multimodel
classifiers to search and identify new signals in a more general way, for the
next LHC run.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Arganda_E/0/1/0/all/0/1"&gt;Ernesto Arganda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Medina_A/0/1/0/all/0/1"&gt;Anibal D. Medina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Perez_A/0/1/0/all/0/1"&gt;Andres D. Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Szynkman_A/0/1/0/all/0/1"&gt;Alejandro Szynkman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrusion Detection System in Smart Home Network Using Bidirectional LSTM and Convolutional Neural Networks Hybrid Model. (arXiv:2105.12096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12096</id>
        <link href="http://arxiv.org/abs/2105.12096"/>
        <updated>2021-05-26T01:22:11.452Z</updated>
        <summary type="html"><![CDATA[Internet of Things (IoT) allowed smart homes to improve the quality and the
comfort of our daily lives. However, these conveniences introduced several
security concerns that increase rapidly. IoT devices, smart home hubs, and
gateway raise various security risks. The smart home gateways act as a
centralized point of communication between the IoT devices, which can create a
backdoor into network data for hackers. One of the common and effective ways to
detect such attacks is intrusion detection in the network traffic. In this
paper, we proposed an intrusion detection system (IDS) to detect anomalies in a
smart home network using a bidirectional long short-term memory (BiLSTM) and
convolutional neural network (CNN) hybrid model. The BiLSTM recurrent behavior
provides the intrusion detection model to preserve the learned information
through time, and the CNN extracts perfectly the data features. The proposed
model can be applied to any smart home network gateway.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elsayed_N/0/1/0/all/0/1"&gt;Nelly Elsayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaghloul_Z/0/1/0/all/0/1"&gt;Zaghloul Saad Zaghloul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azumah_S/0/1/0/all/0/1"&gt;Sylvia Worlali Azumah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengcheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectrum Correction: Acoustic Scene Classification with Mismatched Recording Devices. (arXiv:2105.11856v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.11856</id>
        <link href="http://arxiv.org/abs/2105.11856"/>
        <updated>2021-05-26T01:22:11.433Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms, when trained on audio recordings from a limited
set of devices, may not generalize well to samples recorded using other devices
with different frequency responses. In this work, a relatively straightforward
method is introduced to address this problem. Two variants of the approach are
presented. First requires aligned examples from multiple devices, the second
approach alleviates this requirement. This method works for both time and
frequency domain representations of audio recordings. Further, a relation to
standardization and Cepstral Mean Subtraction is analysed. The proposed
approach becomes effective even when very few examples are provided. This
method was developed during the Detection and Classification of Acoustic Scenes
and Events (DCASE) 2019 challenge and won the 1st place in the scenario with
mis-matched recording devices with the accuracy of 75%. Source code for the
experiments can be found online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kosmider_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Ko&amp;#x15b;mider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHAFF: Fast and consistent SHApley eFfect estimates via random Forests. (arXiv:2105.11724v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.11724</id>
        <link href="http://arxiv.org/abs/2105.11724"/>
        <updated>2021-05-26T01:22:11.429Z</updated>
        <summary type="html"><![CDATA[Interpretability of learning algorithms is crucial for applications involving
critical decisions, and variable importance is one of the main interpretation
tools. Shapley effects are now widely used to interpret both tree ensembles and
neural networks, as they can efficiently handle dependence and interactions in
the data, as opposed to most other variable importance measures. However,
estimating Shapley effects is a challenging task, because of the computational
complexity and the conditional expectation estimates. Accordingly, existing
Shapley algorithms have flaws: a costly running time, or a bias when input
variables are dependent. Therefore, we introduce SHAFF, SHApley eFfects via
random Forests, a fast and accurate Shapley effect estimate, even when input
variables are dependent. We show SHAFF efficiency through both a theoretical
analysis of its consistency, and the practical performance improvements over
competitors with extensive experiments. An implementation of SHAFF in C++ and R
is available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Benard_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment B&amp;#xe9;nard&lt;/a&gt; (LPSM), &lt;a href="http://arxiv.org/find/stat/1/au:+Biau_G/0/1/0/all/0/1"&gt;G&amp;#xe9;rard Biau&lt;/a&gt; (LPSM), &lt;a href="http://arxiv.org/find/stat/1/au:+Veiga_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien da Veiga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scornet_E/0/1/0/all/0/1"&gt;Erwan Scornet&lt;/a&gt; (CMAP)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modulation Front-End for Music Audio Tagging. (arXiv:2105.11836v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.11836</id>
        <link href="http://arxiv.org/abs/2105.11836"/>
        <updated>2021-05-26T01:22:11.402Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks have been extensively explored in the task of
automatic music tagging. The problem can be approached by using either
engineered time-frequency features or raw audio as input. Modulation filter
bank representations that have been actively researched as a basis for timbre
perception have the potential to facilitate the extraction of perceptually
salient features. We explore end-to-end learned front-ends for audio
representation learning, ModNet and SincModNet, that incorporate a temporal
modulation processing block. The structure is effectively analogous to a
modulation filter bank, where the FIR filter center frequencies are learned in
a data-driven manner. The expectation is that a perceptually motivated filter
bank can provide a useful representation for identifying music features. Our
experimental results provide a fully visualisable and interpretable front-end
temporal modulation decomposition of raw audio. We evaluate the performance of
our model against the state-of-the-art of music tagging on the MagnaTagATune
dataset. We analyse the impact on performance for particular tags when
time-frequency bands are subsampled by the modulation filters at a
progressively reduced rate. We demonstrate that modulation filtering provides
promising results for music tagging and feature representation, without using
extensive musical domain knowledge in the design of this front-end.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vahidi_C/0/1/0/all/0/1"&gt;Cyrus Vahidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1"&gt;Charalampos Saitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trajectory Modeling via Random Utility Inverse Reinforcement Learning. (arXiv:2105.12092v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.12092</id>
        <link href="http://arxiv.org/abs/2105.12092"/>
        <updated>2021-05-26T01:22:11.397Z</updated>
        <summary type="html"><![CDATA[We consider the problem of modeling trajectories of drivers in a road network
from the perspective of inverse reinforcement learning. As rational agents,
drivers are trying to maximize some reward function unknown to an external
observer as they make up their trajectories. We apply the concept of random
utility from microeconomic theory to model the unknown reward function as a
function of observable features plus an error term which represents features
known only to the driver. We develop a parameterized generative model for the
trajectories based on a random utility Markov decision process formulation of
drivers decisions. We show that maximum entropy inverse reinforcement learning
is a particular case of our proposed formulation when we assume a Gumbel
density function for the unobserved reward error terms. We illustrate Bayesian
inference on model parameters through a case study with real trajectory data
from a large city obtained from sensors placed on sparsely distributed points
on the street network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pitombeira_Neto_A/0/1/0/all/0/1"&gt;Anselmo R. Pitombeira-Neto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_H/0/1/0/all/0/1"&gt;Helano P. Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_T/0/1/0/all/0/1"&gt;Ticiana L. Coelho da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Antonio F. de Macedo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization. (arXiv:2105.12002v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12002</id>
        <link href="http://arxiv.org/abs/2105.12002"/>
        <updated>2021-05-26T01:22:11.317Z</updated>
        <summary type="html"><![CDATA[The Lottery Ticket Hypothesis suggests that an over-parametrized network
consists of "lottery tickets", and training a certain collection of them (i.e.,
a subnetwork) can match the performance of the full model. In this paper, we
study such a collection of tickets, which is referred to as "winning tickets",
in extremely over-parametrized models, e.g., pre-trained language models. We
observe that at certain compression ratios, generalization performance of the
winning tickets can not only match, but also exceed that of the full model. In
particular, we observe a phase transition phenomenon: As the compression ratio
increases, generalization performance of the winning tickets first improves
then deteriorates after a certain threshold. We refer to the tickets on the
threshold as "super tickets". We further show that the phase transition is task
and model dependent -- as model size becomes larger and training data set
becomes smaller, the transition becomes more pronounced. Our experiments on the
GLUE benchmark show that the super tickets improve single task fine-tuning by
$0.9$ points on BERT-base and $1.0$ points on BERT-large, in terms of
task-average score. We also demonstrate that adaptively sharing the super
tickets across tasks benefits multi-task learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1"&gt;Simiao Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minshuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical power for cluster analysis. (arXiv:2003.00381v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00381</id>
        <link href="http://arxiv.org/abs/2003.00381"/>
        <updated>2021-05-26T01:22:11.307Z</updated>
        <summary type="html"><![CDATA[Cluster algorithms are increasingly popular in biomedical research due to
their compelling ability to identify discrete subgroups in data, and their
increasing accessibility in mainstream software. While guidelines exist for
algorithm selection and outcome evaluation, there are no firmly established
ways of computing a priori statistical power for cluster analysis. Here, we
estimated power and accuracy for common analysis pipelines through simulation.
We varied subgroup size, number, separation (effect size), and covariance
structure. We then subjected generated datasets to dimensionality reduction
(none, multidimensional scaling, or UMAP) and cluster algorithms (k-means,
agglomerative hierarchical clustering with Ward or average linkage and
Euclidean or cosine distance, HDBSCAN). Finally, we compared the statistical
power of discrete (k-means), "fuzzy" (c-means), and finite mixture modelling
approaches (which include latent profile and latent class analysis). We found
that outcomes were driven by large effect sizes or the accumulation of many
smaller effects across features, and were unaffected by differences in
covariance structure. Sufficient statistical power was achieved with relatively
small samples (N=20 per subgroup), provided cluster separation is large
({\Delta}=4). Fuzzy clustering provided a more parsimonious and powerful
alternative for identifying separable multivariate normal distributions,
particularly those with slightly lower centroid separation ({\Delta}=3).
Overall, we recommend that researchers 1) only apply cluster analysis when
large subgroup separation is expected, 2) aim for sample sizes of N=20 to N=30
per expected subgroup, 3) use multidimensional scaling to improve cluster
separation, and 4) use fuzzy clustering or finite mixture modelling approaches
that are more powerful and more parsimonious with partially overlapping
multivariate normal distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dalmaijer_E/0/1/0/all/0/1"&gt;E. S. Dalmaijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nord_C/0/1/0/all/0/1"&gt;C. L. Nord&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Astle_D/0/1/0/all/0/1"&gt;D. E. Astle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Based Link Prediction between Human Phenotypes and Genes. (arXiv:2105.11989v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.11989</id>
        <link href="http://arxiv.org/abs/2105.11989"/>
        <updated>2021-05-26T01:22:11.306Z</updated>
        <summary type="html"><![CDATA[Background The learning of genotype-phenotype associations and history of
human disease by doing detailed and precise analysis of phenotypic
abnormalities can be defined as deep phenotyping. To understand and detect this
interaction between phenotype and genotype is a fundamental step when
translating precision medicine to clinical practice. The recent advances in the
field of machine learning is efficient to predict these interactions between
abnormal human phenotypes and genes.

Methods In this study, we developed a framework to predict links between
human phenotype ontology (HPO) and genes. The annotation data from the
heterogeneous knowledge resources i.e., orphanet, is used to parse human
phenotype-gene associations. To generate the embeddings for the nodes (HPO &
genes), an algorithm called node2vec was used. It performs node sampling on
this graph based on random walks, then learns features over these sampled nodes
to generate embeddings. These embeddings were used to perform the downstream
task to predict the presence of the link between these nodes using 5 different
supervised machine learning algorithms.

Results: The downstream link prediction task shows that the Gradient Boosting
Decision Tree based model (LightGBM) achieved an optimal AUROC 0.904 and AUCPR
0.784. In addition, LightGBM achieved an optimal weighted F1 score of 0.87.
Compared to the other 4 methods LightGBM is able to find more accurate
interaction/link between human phenotype & gene pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1"&gt;Rushabh Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yanhui Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibration and Uncertainty Quantification of Bayesian Convolutional Neural Networks for Geophysical Applications. (arXiv:2105.12115v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12115</id>
        <link href="http://arxiv.org/abs/2105.12115"/>
        <updated>2021-05-26T01:22:11.300Z</updated>
        <summary type="html"><![CDATA[Deep neural networks offer numerous potential applications across geoscience,
for example, one could argue that they are the state-of-the-art method for
predicting faults in seismic datasets. In quantitative reservoir
characterization workflows, it is common to incorporate the uncertainty of
predictions thus such subsurface models should provide calibrated probabilities
and the associated uncertainties in their predictions. It has been shown that
popular Deep Learning-based models are often miscalibrated, and due to their
deterministic nature, provide no means to interpret the uncertainty of their
predictions. We compare three different approaches to obtaining probabilistic
models based on convolutional neural networks in a Bayesian formalism, namely
Deep Ensembles, Concrete Dropout, and Stochastic Weight Averaging-Gaussian
(SWAG). These methods are consistently applied to fault detection case studies
where Deep Ensembles use independently trained models to provide fault
probabilities, Concrete Dropout represents an extension to the popular Dropout
technique to approximate Bayesian neural networks, and finally, we apply SWAG,
a recent method that is based on the Bayesian inference equivalence of
mini-batch Stochastic Gradient Descent. We provide quantitative results in
terms of model calibration and uncertainty representation, as well as
qualitative results on synthetic and real seismic datasets. Our results show
that the approximate Bayesian methods, Concrete Dropout and SWAG, both provide
well-calibrated predictions and uncertainty attributes at a lower computational
cost when compared to the baseline Deep Ensemble approach. The resulting
uncertainties also offer a possibility to further improve the model performance
as well as enhancing the interpretability of the models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mosser_L/0/1/0/all/0/1"&gt;Lukas Mosser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeini_E/0/1/0/all/0/1"&gt;Ehsan Zabihi Naeini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep neural network enabled corrective source term approach to hybrid analysis and modeling. (arXiv:2105.11521v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2105.11521</id>
        <link href="http://arxiv.org/abs/2105.11521"/>
        <updated>2021-05-26T01:22:11.291Z</updated>
        <summary type="html"><![CDATA[Hybrid Analysis and Modeling (HAM) is an emerging modeling paradigm which
aims to combine physics-based modeling (PBM) and data-driven modeling (DDM) to
create generalizable, trustworthy, accurate, computationally efficient and
self-evolving models. Here, we introduce, justify and demonstrate a novel
approach to HAM -- the Corrective Source Term Approach (CoSTA) -- which
augments the governing equation of a PBM model with a corrective source term
generated by a deep neural network (DNN). In a series of numerical experiments
on one-dimensional heat diffusion, CoSTA is generally found to outperform
comparable DDM and PBM models in terms of accuracy -- often reducing predictive
errors by several orders of magnitude -- while also generalizing better than
pure DDM. Due to its flexible but solid theoretical foundation, CoSTA provides
a modular framework for leveraging novel developments within both PBM and DDM,
and due to the interpretability of the DNN-generated source term within the PBM
paradigm, CoSTA can be a potential door-opener for data-driven techniques to
enter high-stakes applications previously reserved for pure PBM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blakseth_S/0/1/0/all/0/1"&gt;Sindre Stenen Blakseth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasheed_A/0/1/0/all/0/1"&gt;Adil Rasheed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kvamsdal_T/0/1/0/all/0/1"&gt;Trond Kvamsdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+San_O/0/1/0/all/0/1"&gt;Omer San&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropic Out-of-Distribution Detection. (arXiv:1908.05569v13 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.05569</id>
        <link href="http://arxiv.org/abs/1908.05569"/>
        <updated>2021-05-26T01:22:11.291Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution (OOD) detection approaches usually present special
requirements (e.g., hyperparameter validation, collection of outlier data) and
produce side effects (e.g., classification accuracy drop, slower
energy-inefficient inferences). We argue that these issues are a consequence of
the SoftMax loss anisotropy and disagreement with the maximum entropy
principle. Thus, we propose the IsoMax loss and the entropic score. The
seamless drop-in replacement of the SoftMax loss by IsoMax loss requires
neither additional data collection nor hyperparameter validation. The trained
models do not exhibit classification accuracy drop and produce fast
energy-efficient inferences. Moreover, our experiments show that training
neural networks with IsoMax loss significantly improves their OOD detection
performance. The IsoMax loss exhibits state-of-the-art performance under the
mentioned conditions (fast energy-efficient inference, no classification
accuracy drop, no collection of outlier data, and no hyperparameter
validation), which we call the seamless OOD detection task. In future work,
current OOD detection methods may replace the SoftMax loss with the IsoMax loss
to improve their performance on the commonly studied non-seamless OOD detection
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1"&gt;Tsang Ing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1"&gt;Cleber Zanchettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Adriano L. I. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend Forecasting. (arXiv:2105.11826v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11826</id>
        <link href="http://arxiv.org/abs/2105.11826"/>
        <updated>2021-05-26T01:22:11.290Z</updated>
        <summary type="html"><![CDATA[This companion paper supports the replication of the fashion trend
forecasting experiments with the KERN (Knowledge Enhanced Recurrent Network)
method that we presented in the ICMR 2020. We provide an artifact that allows
the replication of the experiments using a Python implementation. The artifact
is easy to deploy with simple installation, training and evaluation. We
reproduce the experiments conducted in the original paper and obtain similar
performance as previously reported. The replication results of the experiments
support the main claims in the original paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yunshan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yujuan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1"&gt;Lizi Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1"&gt;Wai Keung Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1"&gt;Jinyoung Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1"&gt;Hong-Han Shuai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Sampling Density for Nonparametric Regression. (arXiv:2105.11990v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11990</id>
        <link href="http://arxiv.org/abs/2105.11990"/>
        <updated>2021-05-26T01:22:11.290Z</updated>
        <summary type="html"><![CDATA[We propose a novel active learning strategy for regression, which is
model-agnostic, robust against model mismatch, and interpretable. Assuming that
a small number of initial samples are available, we derive the optimal training
density that minimizes the generalization error of local polynomial smoothing
(LPS) with its kernel bandwidth tuned locally: We adopt the mean integrated
squared error (MISE) as a generalization criterion, and use the asymptotic
behavior of the MISE as well as thelocally optimal bandwidths (LOB) -- the
bandwidth function that minimizes MISE in the asymptotic limit. The asymptotic
expression of our objective then reveals the dependence of the MISE on the
training density, enabling analytic minimization. As a result, we obtain the
optimal training density in a closed-form. The almost model-free nature of our
approach should encode raw properties of the target problem, and thus provide a
robust and model-agnostic active learning strategy. Furthermore, the obtained
training density factorizes the influence of local function complexity, noise
leveland test density in a transparent and interpretable way. We validate our
theory in numerical simulations, and show that the proposed active learning
method outperforms the existing state-of-the-art model-agnostic approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panknin_D/0/1/0/all/0/1"&gt;Danny Panknin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1"&gt;Shinichi Nakajima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus Robert M&amp;#xfc;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Subspace Learning for Dimensionality Reduction to Improve Classification Accuracy in Large Data Sets. (arXiv:2105.12005v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12005</id>
        <link href="http://arxiv.org/abs/2105.12005"/>
        <updated>2021-05-26T01:22:11.290Z</updated>
        <summary type="html"><![CDATA[Manifold learning is used for dimensionality reduction, with the goal of
finding a projection subspace to increase and decrease the inter- and
intraclass variances, respectively. However, a bottleneck for subspace learning
methods often arises from the high dimensionality of datasets. In this paper, a
hierarchical approach is proposed to scale subspace learning methods, with the
goal of improving classification in large datasets by a range of 3% to 10%.
Different combinations of methods are studied. We assess the proposed method on
five publicly available large datasets, for different eigen-value based
subspace learning methods such as linear discriminant analysis, principal
component analysis, generalized discriminant analysis, and reconstruction
independent component analysis. To further examine the effect of the proposed
method on various classification methods, we fed the generated result to linear
discriminant analysis, quadratic linear analysis, k-nearest neighbor, and
random forest classifiers. The resulting classification accuracies are compared
to show the effectiveness of the hierarchical approach, reporting results of an
average of 5% increase in classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poorheravi_P/0/1/0/all/0/1"&gt;Parisa Abdolrahim Poorheravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaudet_V/0/1/0/all/0/1"&gt;Vincent Gaudet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring Hierarchical Mixture Structures: A Bayesian Nonparametric Approach. (arXiv:1905.05022v6 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.05022</id>
        <link href="http://arxiv.org/abs/1905.05022"/>
        <updated>2021-05-26T01:22:11.290Z</updated>
        <summary type="html"><![CDATA[This paper focuses on the problem of hierarchical non-overlapping clustering
of a dataset. In such a clustering, each data item is associated with exactly
one leaf node and each internal node is associated with all the data items
stored in the sub-tree beneath it, so that each level of the hierarchy
corresponds to a partition of the dataset. We develop a novel Bayesian
nonparametric method combining the nested Chinese Restaurant Process (nCRP) and
the Hierarchical Dirichlet Process (HDP). Compared with other existing Bayesian
approaches, our solution tackles data with complex latent mixture features
which has not been previously explored in the literature. We discuss the
details of the model and the inference procedure. Furthermore, experiments on
three datasets show that our method achieves solid empirical results in
comparison with existing algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weipeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Laitonjam_N/0/1/0/all/0/1"&gt;Nishma Laitonjam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Piao_G/0/1/0/all/0/1"&gt;Guangyuan Piao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hurley_N/0/1/0/all/0/1"&gt;Neil Hurley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Public Transportation Demand Analysis: A Case Study of Metropolitan Lagos. (arXiv:2105.11816v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11816</id>
        <link href="http://arxiv.org/abs/2105.11816"/>
        <updated>2021-05-26T01:22:11.289Z</updated>
        <summary type="html"><![CDATA[Modelling, simulation, and forecasting offer a means of facilitating better
planning and decision-making. These quantitative approaches can add value
beyond traditional methods that do not rely on data and are particularly
relevant for public transportation. Lagos is experiencing rapid urbanization
and currently has a population of just under 15 million. Both long waiting
times and uncertain travel times has driven many people to acquire their own
vehicle or use alternative modes of transport. This has significantly increased
the number of vehicles on the roads leading to even more traffic and greater
traffic congestion. This paper investigates urban travel demand in Lagos and
explores passenger dynamics in time and space. Using individual commuter trip
data from tickets purchased from the Lagos State Bus Rapid Transit (BRT), the
demand patterns through the hours of the day, days of the week and bus stations
are analysed. This study aims to quantify demand from actual passenger trips
and estimate the impact that dynamic scheduling could have on passenger waiting
times. Station segmentation is provided to cluster stations by their demand
characteristics in order to tailor specific bus schedules. Intra-day public
transportation demand in Lagos BRT is analysed and predictions are compared.
Simulations using fixed and dynamic bus scheduling demonstrate that the average
waiting time could be reduced by as much as 80%. The load curves, insights and
the approach developed will be useful for informing policymaking in Lagos and
similar African cities facing the challenges of rapid urbanization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_O/0/1/0/all/0/1"&gt;Ozioma Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McSharry_P/0/1/0/all/0/1"&gt;Patrick McSharry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGD with Coordinate Sampling: Theory and Practice. (arXiv:2105.11818v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.11818</id>
        <link href="http://arxiv.org/abs/2105.11818"/>
        <updated>2021-05-26T01:22:11.289Z</updated>
        <summary type="html"><![CDATA[While classical forms of stochastic gradient descent algorithm treat the
different coordinates in the same way, a framework allowing for adaptive (non
uniform) coordinate sampling is developed to leverage structure in data. In a
non-convex setting and including zeroth order gradient estimate, almost sure
convergence as well as non-asymptotic bounds are established. Within the
proposed framework, we develop an algorithm, MUSKETEER, based on a
reinforcement strategy: after collecting information on the noisy gradients, it
samples the most promising coordinate (all for one); then it moves along the
one direction yielding an important decrease of the objective (one for all).
Numerical experiments on both synthetic and real data examples confirm the
effectiveness of MUSKETEER in large scale problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Leluc_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Leluc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Portier_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Portier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FILTRA: Rethinking Steerable CNN by Filter Transform. (arXiv:2105.11636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11636</id>
        <link href="http://arxiv.org/abs/2105.11636"/>
        <updated>2021-05-26T01:22:11.288Z</updated>
        <summary type="html"><![CDATA[Steerable CNN imposes the prior knowledge of transformation invariance or
equivariance in the network architecture to enhance the the network robustness
on geometry transformation of data and reduce overfitting. It has been an
intuitive and widely used technique to construct a steerable filter by
augmenting a filter with its transformed copies in the past decades, which is
named as filter transform in this paper. Recently, the problem of steerable CNN
has been studied from aspect of group representation theory, which reveals the
function space structure of a steerable kernel function. However, it is not yet
clear on how this theory is related to the filter transform technique. In this
paper, we show that kernel constructed by filter transform can also be
interpreted in the group representation theory. This interpretation help
complete the puzzle of steerable CNN theory and provides a novel and simple
approach to implement steerable convolution operators. Experiments are executed
on multiple datasets to verify the feasibility of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gim Hee Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HINT: Hierarchical Invertible Neural Transport for Density Estimation and Bayesian Inference. (arXiv:1905.10687v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.10687</id>
        <link href="http://arxiv.org/abs/1905.10687"/>
        <updated>2021-05-26T01:22:11.288Z</updated>
        <summary type="html"><![CDATA[Many recent invertible neural architectures are based on coupling block
designs where variables are divided in two subsets which serve as inputs of an
easily invertible (usually affine) triangular transformation. While such a
transformation is invertible, its Jacobian is very sparse and thus may lack
expressiveness. This work presents a simple remedy by noting that subdivision
and (affine) coupling can be repeated recursively within the resulting subsets,
leading to an efficiently invertible block with dense, triangular Jacobian. By
formulating our recursive coupling scheme via a hierarchical architecture, HINT
allows sampling from a joint distribution p(y,x) and the corresponding
posterior p(x|y) using a single invertible network. We evaluate our method on
some standard data sets and benchmark its full power for density estimation and
Bayesian inference on a novel data set of 2D shapes in Fourier
parameterization, which enables consistent visualization of samples for
different dimensionalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kruse_J/0/1/0/all/0/1"&gt;Jakob Kruse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Detommaso_G/0/1/0/all/0/1"&gt;Gianluca Detommaso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kothe_U/0/1/0/all/0/1"&gt;Ullrich K&amp;#xf6;the&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scheichl_R/0/1/0/all/0/1"&gt;Robert Scheichl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Space Exploration For Planning Initial Benthic AUV Surveys. (arXiv:2105.11598v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.11598</id>
        <link href="http://arxiv.org/abs/2105.11598"/>
        <updated>2021-05-26T01:22:11.287Z</updated>
        <summary type="html"><![CDATA[Special-purpose Autonomous Underwater Vehicles (AUVs) are utilised for
benthic (seafloor) surveys, where the vehicle collects optical imagery of near
the seafloor. Due to the small-sensor footprint of the cameras and the vast
areas to be surveyed, these AUVs can not feasibly full coverage of areas larger
than a few tens of thousands of square meters. Therefore AUV paths which sample
sparsely, yet effectively, the survey areas are necessary. Broad scale acoustic
bathymetric data is ready available over large areas, and often is a useful
prior of seafloor cover. As such, prior bathymetry can be used to guide AUV
data collection. This research proposes methods for planning initial AUV
surveys that efficiently explore a feature space representation of the
bathymetry, in order to sample from a diverse set of bathymetric terrain. This
will enable the AUV to visit areas that likely contain unique habitats and are
representative of the entire survey site. The suitability of these methods to
plan AUV surveys is evaluated based on the coverage of the feature space and
also the ability to visit all classes of benthic habitat on the initial dive.
This is a valuable tool for AUV surveys as it increases the utility of initial
dives. It also delivers a comprehensive training set to learn a relationship
between acoustic bathymetry and visually-derived seafloor classifications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shields_J/0/1/0/all/0/1"&gt;Jackson Shields&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pizarro_O/0/1/0/all/0/1"&gt;Oscar Pizarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_S/0/1/0/all/0/1"&gt;Stefan B. Williams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring Temporal Logic Properties from Data using Boosted Decision Trees. (arXiv:2105.11508v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.11508</id>
        <link href="http://arxiv.org/abs/2105.11508"/>
        <updated>2021-05-26T01:22:11.284Z</updated>
        <summary type="html"><![CDATA[Many autonomous systems, such as robots and self-driving cars, involve
real-time decision making in complex environments, and require prediction of
future outcomes from limited data. Moreover, their decisions are increasingly
required to be interpretable to humans for safe and trustworthy co-existence.
This paper is a first step towards interpretable learning-based robot control.
We introduce a novel learning problem, called incremental formula and predictor
learning, to generate binary classifiers with temporal logic structure from
time-series data. The classifiers are represented as pairs of Signal Temporal
Logic (STL) formulae and predictors for their satisfaction. The incremental
property provides prediction of labels for prefix signals that are revealed
over time. We propose a boosted decision-tree algorithm that leverages weak,
but computationally inexpensive, learners to increase prediction and runtime
performance. The effectiveness and classification accuracy of our algorithms
are evaluated on autonomous-driving and naval surveillance case studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aasi_E/0/1/0/all/0/1"&gt;Erfan Aasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasile_C/0/1/0/all/0/1"&gt;Cristian Ioan Vasile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahreinian_M/0/1/0/all/0/1"&gt;Mahroo Bahreinian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belta_C/0/1/0/all/0/1"&gt;Calin Belta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pan-sharpening via High-pass Modification Convolutional Neural Network. (arXiv:2105.11576v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11576</id>
        <link href="http://arxiv.org/abs/2105.11576"/>
        <updated>2021-05-26T01:22:11.279Z</updated>
        <summary type="html"><![CDATA[Most existing deep learning-based pan-sharpening methods have several widely
recognized issues, such as spectral distortion and insufficient spatial texture
enhancement, we propose a novel pan-sharpening convolutional neural network
based on a high-pass modification block. Different from existing methods, the
proposed block is designed to learn the high-pass information, leading to
enhance spatial information in each band of the multi-spectral-resolution
images. To facilitate the generation of visually appealing pan-sharpened
images, we propose a perceptual loss function and further optimize the model
based on high-level features in the near-infrared space. Experiments
demonstrate the superior performance of the proposed method compared to the
state-of-the-art pan-sharpening methods, both quantitatively and qualitatively.
The proposed model is open-sourced at https://github.com/jiaming-wang/HMB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhenfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruiqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiayi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Least-Squares ReLU Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Law. (arXiv:2105.11627v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2105.11627</id>
        <link href="http://arxiv.org/abs/2105.11627"/>
        <updated>2021-05-26T01:22:11.262Z</updated>
        <summary type="html"><![CDATA[We introduced the least-squares ReLU neural network (LSNN) method for solving
the linear advection-reaction problem with discontinuous solution and showed
that the method outperforms mesh-based numerical methods in terms of the number
of degrees of freedom. This paper studies the LSNN method for scalar nonlinear
hyperbolic conservation law. The method is a discretization of an equivalent
least-squares (LS) formulation in the set of neural network functions with the
ReLU activation function. Evaluation of the LS functional is done by using
numerical integration and conservative finite volume scheme. Numerical results
of some test problems show that the method is capable of approximating the
discontinuous interface of the underlying problem automatically through the
free breaking lines of the ReLU neural network. Moreover, the method does not
exhibit the common Gibbs phenomena along the discontinuous interface.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhiqiang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingshuang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liu_M/0/1/0/all/0/1"&gt;Min Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Machine Learning-Based Modeling of Semiconductor Devices by Data Self-Augmentation. (arXiv:2105.11453v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11453</id>
        <link href="http://arxiv.org/abs/2105.11453"/>
        <updated>2021-05-26T01:22:11.256Z</updated>
        <summary type="html"><![CDATA[In the electronics industry, introducing Machine Learning (ML)-based
techniques can enhance Technology Computer-Aided Design (TCAD) methods.
However, the performance of ML models is highly dependent on their training
datasets. Particularly in the semiconductor industry, given the fact that the
fabrication process of semiconductor devices is complicated and expensive, it
is of great difficulty to obtain datasets with sufficient size and good
quality. In this paper, we propose a strategy for improving ML-based device
modeling by data self-augmentation using variational autoencoder-based
techniques, where initially only a few experimental data points are required
and TCAD tools are not essential. Taking a deep neural network-based prediction
task of the Ohmic resistance value in Gallium Nitride devices as an example, we
apply our proposed strategy to augment data points and achieve a reduction in
the mean absolute error of predicting the experimental results by up to 70%.
The proposed method could be easily modified for different tasks, rendering it
of high interest to the semiconductor industry in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zeheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leon_R/0/1/0/all/0/1"&gt;Ross C. C. Leon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laucht_A/0/1/0/all/0/1"&gt;Arne Laucht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAP-GAN: Towards Adversarial Robustness with Cycle-consistent Attentional Purification. (arXiv:2102.07304v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07304</id>
        <link href="http://arxiv.org/abs/2102.07304"/>
        <updated>2021-05-26T01:22:11.250Z</updated>
        <summary type="html"><![CDATA[Adversarial attack is aimed at fooling the target classifier with
imperceptible perturbation. Adversarial examples, which are carefully crafted
with a malicious purpose, can lead to erroneous predictions, resulting in
catastrophic accidents. To mitigate the effects of adversarial attacks, we
propose a novel purification model called CAP-GAN. CAP-GAN takes account of the
idea of pixel-level and feature-level consistency to achieve reasonable
purification under cycle-consistent learning. Specifically, we utilize the
guided attention module and knowledge distillation to convey meaningful
information to the purification model. Once a model is fully trained, inputs
would be projected into the purification model and transformed into clean-like
images. We vary the capacity of the adversary to argue the robustness against
various types of attack strategies. On the CIFAR-10 dataset, CAP-GAN
outperforms other pre-processing based defenses under both black-box and
white-box settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Mingu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Trung Quang Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Seungju Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daeyoung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Damage Mapping with InSAR Coherence Time Series. (arXiv:2105.11544v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2105.11544</id>
        <link href="http://arxiv.org/abs/2105.11544"/>
        <updated>2021-05-26T01:22:11.245Z</updated>
        <summary type="html"><![CDATA[Satellite remote sensing is playing an increasing role in the rapid mapping
of damage after natural disasters. In particular, synthetic aperture radar
(SAR) can image the Earth's surface and map damage in all weather conditions,
day and night. However, current SAR damage mapping methods struggle to separate
damage from other changes in the Earth's surface. In this study, we propose a
novel approach to damage mapping, combining deep learning with the full time
history of SAR observations of an impacted region in order to detect anomalous
variations in the Earth's surface properties due to a natural disaster. We
quantify Earth surface change using time series of Interferometric SAR
coherence, then use a recurrent neural network (RNN) as a probabilistic anomaly
detector on these coherence time series. The RNN is first trained on pre-event
coherence time series, and then forecasts a probability distribution of the
coherence between pre- and post-event SAR images. The difference between the
forecast and observed co-event coherence provides a measure of the confidence
in the identification of damage. The method allows the user to choose a damage
detection threshold that is customized for each location, based on the local
behavior of coherence through time before the event. We apply this method to
calculate estimates of damage for three earthquakes using multi-year time
series of Sentinel-1 SAR acquisitions. Our approach shows good agreement with
observed damage and quantitative improvement compared to using pre- to co-event
coherence loss as a damage proxy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Stephenson_O/0/1/0/all/0/1"&gt;Oliver L. Stephenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kohne_T/0/1/0/all/0/1"&gt;Tobias K&amp;#xf6;hne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhan_E/0/1/0/all/0/1"&gt;Eric Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Cahill_B/0/1/0/all/0/1"&gt;Brent E. Cahill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yun_S/0/1/0/all/0/1"&gt;Sang-Ho Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ross_Z/0/1/0/all/0/1"&gt;Zachary E. Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Simons_M/0/1/0/all/0/1"&gt;Mark Simons&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Connect the Dots: In Situ 4D Seismic Monitoring of CO$_2$ Storage with Spatio-temporal CNNs. (arXiv:2105.11622v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2105.11622</id>
        <link href="http://arxiv.org/abs/2105.11622"/>
        <updated>2021-05-26T01:22:11.227Z</updated>
        <summary type="html"><![CDATA[4D seismic imaging has been widely used in CO$_2$ sequestration projects to
monitor the fluid flow in the volumetric subsurface region that is not sampled
by wells. Ideally, real-time monitoring and near-future forecasting would
provide site operators with great insights to understand the dynamics of the
subsurface reservoir and assess any potential risks. However, due to obstacles
such as high deployment cost, availability of acquisition equipment, exclusion
zones around surface structures, only very sparse seismic imaging data can be
obtained during monitoring. That leads to an unavoidable and growing knowledge
gap over time. The operator needs to understand the fluid flow throughout the
project lifetime and the seismic data are only available at a limited number of
times, this is insufficient for understanding the reservoir behavior. To
overcome those challenges, we have developed spatio-temporal
neural-network-based models that can produce high-fidelity interpolated or
extrapolated images effectively and efficiently. Specifically, our models are
built on an autoencoder, and incorporate the long short-term memory (LSTM)
structure with a new loss function regularized by optical flow. We validate the
performance of our models using real 4D post-stack seismic imaging data
acquired at the Sleipner CO$_2$ sequestration field. We employ two different
strategies in evaluating our models. Numerically, we compare our models with
different baseline approaches using classic pixel-based metrics. We also
conduct a blind survey and collect a total of 20 responses from domain experts
to evaluate the quality of data generated by our models. Via both numerical and
expert evaluation, we conclude that our models can produce high-quality 2D/3D
seismic imaging data at a reasonable cost, offering the possibility of
real-time monitoring or even near-future forecasting of the CO$_2$ storage
reservoir.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shihang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xitong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wohlberg_B/0/1/0/all/0/1"&gt;Brendt Wohlberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Symons_N/0/1/0/all/0/1"&gt;Neill Symons&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Youzuo Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Test Suite Generation for Key-Points Detection DNNs using Many-Objective Search (Experience Paper). (arXiv:2012.06511v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06511</id>
        <link href="http://arxiv.org/abs/2012.06511"/>
        <updated>2021-05-26T01:22:11.222Z</updated>
        <summary type="html"><![CDATA[Automatically detecting the positions of key-points (e.g., facial key-points
or finger key-points) in an image is an essential problem in many applications,
such as driver's gaze detection and drowsiness detection in automated driving
systems. With the recent advances of Deep Neural Networks (DNNs), Key-Points
detection DNNs (KP-DNNs) have been increasingly employed for that purpose.
Nevertheless, KP-DNN testing and validation have remained a challenging problem
because KP-DNNs predict many independent key-points at the same time -- where
each individual key-point may be critical in the targeted application -- and
images can vary a great deal according to many factors.

In this paper, we present an approach to automatically generate test data for
KP-DNNs using many-objective search. In our experiments, focused on facial
key-points detection DNNs developed for an industrial automotive application,
we show that our approach can generate test suites to severely mispredict, on
average, more than 93% of all key-points. In comparison, random search-based
test data generation can only severely mispredict 41% of them. Many of these
mispredictions, however, are not avoidable and should not therefore be
considered failures. We also empirically compare state-of-the-art,
many-objective search algorithms and their variants, tailored for test suite
generation. Furthermore, we investigate and demonstrate how to learn specific
conditions, based on image characteristics (e.g., head posture and skin color),
that lead to severe mispredictions. Such conditions serve as a basis for risk
analysis or DNN retraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haq_F/0/1/0/all/0/1"&gt;Fitash Ul Haq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1"&gt;Donghwan Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Briand_L/0/1/0/all/0/1"&gt;Lionel C. Briand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stifter_T/0/1/0/all/0/1"&gt;Thomas Stifter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Teachable Autonomous Agents. (arXiv:2105.11977v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11977</id>
        <link href="http://arxiv.org/abs/2105.11977"/>
        <updated>2021-05-26T01:22:11.214Z</updated>
        <summary type="html"><![CDATA[Autonomous discovery and direct instruction are two extreme sources of
learning in children, but educational sciences have shown that intermediate
approaches such as assisted discovery or guided play resulted in better
acquisition of skills. When turning to Artificial Intelligence, the above
dichotomy is translated into the distinction between autonomous agents which
learn in isolation and interactive learning agents which can be taught by
social partners but generally lack autonomy. In between should stand teachable
autonomous agents: agents learning from both internal and teaching signals to
benefit from the higher efficiency of assisted discovery. Such agents could
learn on their own in the real world, but non-expert users could drive their
learning behavior towards their expectations. More fundamentally, combining
both capabilities might also be a key step towards general intelligence. In
this paper we elucidate obstacles along this research line. First, we build on
a seminal work of Bruner to extract relevant features of the assisted discovery
processes. Second, we describe current research on autotelic agents, i.e.
agents equipped with forms of intrinsic motivations that enable them to
represent, self-generate and pursue their own goals. We argue that autotelic
capabilities are paving the way towards teachable and autonomous agents.
Finally, we adopt a social learning perspective on tutoring interactions and we
highlight some components that are currently missing to autotelic agents before
they can be taught by ordinary people using natural pedagogy, and we provide a
list of specific research questions that emerge from this perspective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1"&gt;Olivier Sigaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caselles_Dupre_H/0/1/0/all/0/1"&gt;Hugo Caselles-Dupr&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Colas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akakzia_A/0/1/0/all/0/1"&gt;Ahmed Akakzia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1"&gt;Pierre-Yves Oudeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chetouani_M/0/1/0/all/0/1"&gt;Mohamed Chetouani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Quantum Hopfield Associative Memory Implemented on an Actual Quantum Processor. (arXiv:2105.11590v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2105.11590</id>
        <link href="http://arxiv.org/abs/2105.11590"/>
        <updated>2021-05-26T01:22:11.213Z</updated>
        <summary type="html"><![CDATA[In this work, we present a Quantum Hopfield Associative Memory (QHAM) and
demonstrate its capabilities in simulation and hardware using IBM Quantum
Experience. The QHAM is based on a quantum neuron design which can be utilized
for many different machine learning applications and can be implemented on real
quantum hardware without requiring mid-circuit measurement or reset operations.
We analyze the accuracy of the neuron and the full QHAM considering hardware
errors via simulation with hardware noise models as well as with implementation
on the 15-qubit ibmq_16_melbourne device. The quantum neuron and the QHAM are
shown to be resilient to noise and require low qubit and time overhead. We
benchmark the QHAM by testing its effective memory capacity against qubit- and
circuit-level errors and demonstrate its capabilities in the NISQ-era of
quantum hardware. This demonstration of the first functional QHAM to be
implemented in NISQ-era quantum hardware is a significant step in machine
learning at the leading edge of quantum computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Miller_N/0/1/0/all/0/1"&gt;Nathan Eli Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Mukhopadhyay_S/0/1/0/all/0/1"&gt;Saibal Mukhopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Cross Validation Losses for Gaussian Process Models. (arXiv:2105.11535v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.11535</id>
        <link href="http://arxiv.org/abs/2105.11535"/>
        <updated>2021-05-26T01:22:11.212Z</updated>
        <summary type="html"><![CDATA[We introduce a simple and scalable method for training Gaussian process (GP)
models that exploits cross-validation and nearest neighbor truncation. To
accommodate binary and multi-class classification we leverage P\`olya-Gamma
auxiliary variables and variational inference. In an extensive empirical
comparison with a number of alternative methods for scalable GP regression and
classification, we find that our method offers fast training and excellent
predictive performance. We argue that the good predictive performance can be
traced to the non-parametric nature of the resulting predictive distributions
as well as to the cross-validation loss, which provides robustness against
model mis-specification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jankowiak_M/0/1/0/all/0/1"&gt;Martin Jankowiak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pleiss_G/0/1/0/all/0/1"&gt;Geoff Pleiss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Geometry-Informed Deep Learning Framework for Ultra-Sparse 3D Tomographic Image Reconstruction. (arXiv:2105.11692v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11692</id>
        <link href="http://arxiv.org/abs/2105.11692"/>
        <updated>2021-05-26T01:22:11.212Z</updated>
        <summary type="html"><![CDATA[Deep learning affords enormous opportunities to augment the armamentarium of
biomedical imaging, albeit its design and implementation have potential flaws.
Fundamentally, most deep learning models are driven entirely by data without
consideration of any prior knowledge, which dramatically increases the
complexity of neural networks and limits the application scope and model
generalizability. Here we establish a geometry-informed deep learning framework
for ultra-sparse 3D tomographic image reconstruction. We introduce a novel
mechanism for integrating geometric priors of the imaging system. We
demonstrate that the seamless inclusion of known priors is essential to enhance
the performance of 3D volumetric computed tomography imaging with ultra-sparse
sampling. The study opens new avenues for data-driven biomedical imaging and
promises to provide substantially improved imaging tools for various clinical
imaging and image-guided interventions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Liyue Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capaldi_D/0/1/0/all/0/1"&gt;Dante Capaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1"&gt;John Pauly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1"&gt;Lei Xing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FNAS: Uncertainty-Aware Fast Neural Architecture Search. (arXiv:2105.11694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11694</id>
        <link href="http://arxiv.org/abs/2105.11694"/>
        <updated>2021-05-26T01:22:11.212Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL)-based neural architecture search (NAS) generally
guarantees better convergence yet suffers from the requirement of huge
computational resources compared with gradient-based approaches, due to the
rollout bottleneck -- exhaustive training for each sampled generation on proxy
tasks. In this paper, we propose a general pipeline to accelerate the
convergence of the rollout process as well as the RL process in NAS. It is
motivated by the interesting observation that both the architecture and the
parameter knowledge can be transferred between different experiments and even
different tasks. We first introduce an uncertainty-aware critic (value
function) in Proximal Policy Optimization (PPO) to utilize the architecture
knowledge in previous experiments, which stabilizes the training process and
reduces the searching time by 4 times. Further, an architecture knowledge pool
together with a block similarity function is proposed to utilize parameter
knowledge and reduces the searching time by 2 times. It is the first to
introduce block-level weight sharing in RLbased NAS. The block similarity
function guarantees a 100% hitting ratio with strict fairness. Besides, we show
that a simply designed off-policy correction factor used in "replay buffer" in
RL optimization can further reduce half of the searching time. Experiments on
the Mobile Neural Architecture Search (MNAS) search space show the proposed
Fast Neural Architecture Search (FNAS) accelerates standard RL-based NAS
process by ~10x (e.g. ~256 2x2 TPUv2 x days / 20,000 GPU x hour -> 2,000 GPU x
hour for MNAS), and guarantees better performance on various vision tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yangting Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Boxiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1"&gt;Guanglu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Jointed Feature Fusion Framework for Photoacoustic Reconstruction. (arXiv:2012.02472v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02472</id>
        <link href="http://arxiv.org/abs/2012.02472"/>
        <updated>2021-05-26T01:22:11.211Z</updated>
        <summary type="html"><![CDATA[Photoacoustic (PA) computed tomography (PACT) reconstructs the initial
pressure distribution from raw PA signals. The standard reconstruction of
medical image could cause the artifacts due to interferences or ill-posed
setup. Recently, deep learning has been used to reconstruct the PA image with
ill-posed conditions. Most works remove the artifacts from image domain, and
compensate the limited-view from dataset. In this paper, we propose a jointed
feature fusion framework (JEFF-Net) based on deep learning to reconstruct the
PA image using limited-view data. The cross-domain features from limited-view
position-wise data and the reconstructed image are fused by a backtracked
supervision. Specifically, our results could generate superior performance,
whose artifacts are drastically reduced in the output compared to ground-truth
(full-view reconstructed result). In this paper, a quarter position-wise data
(32 channels) is fed into model, which outputs another 3-quarters-view data (96
channels). Moreover, two novel losses are designed to restrain the artifacts by
sufficiently manipulating superposed data. The numerical and in-vivo results
have demonstrated the superior performance of our method to reconstruct the
full-view image without artifacts. Finally, quantitative evaluations show that
our proposed method outperformed the ground-truth in some metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1"&gt;Hengrong Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Changchun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Fei Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GCNBoost: Artwork Classification by Label Propagation through a Knowledge Graph. (arXiv:2105.11852v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11852</id>
        <link href="http://arxiv.org/abs/2105.11852"/>
        <updated>2021-05-26T01:22:11.211Z</updated>
        <summary type="html"><![CDATA[The rise of digitization of cultural documents offers large-scale contents,
opening the road for development of AI systems in order to preserve, search,
and deliver cultural heritage. To organize such cultural content also means to
classify them, a task that is very familiar to modern computer science.
Contextual information is often the key to structure such real world data, and
we propose to use it in form of a knowledge graph. Such a knowledge graph,
combined with content analysis, enhances the notion of proximity between
artworks so it improves the performances in classification tasks. In this
paper, we propose a novel use of a knowledge graph, that is constructed on
annotated data and pseudo-labeled data. With label propagation, we boost
artwork classification by training a model using a graph convolutional network,
relying on the relationships between entities of the knowledge graph. Following
a transductive learning framework, our experiments show that relying on a
knowledge graph modeling the relations between labeled data and unlabeled data
allows to achieve state-of-the-art results on multiple classification tasks on
a dataset of paintings, and on a dataset of Buddha statues. Additionally, we
show state-of-the-art results for the difficult case of dealing with unbalanced
data, with the limitation of disregarding classes with extremely low degrees in
the knowledge graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaigh_C/0/1/0/all/0/1"&gt;Cheikh Brahim El Vaigh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1"&gt;Noa Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renoust_B/0/1/0/all/0/1"&gt;Benjamin Renoust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1"&gt;Chenhui Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1"&gt;Yuta Nakashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagahara_H/0/1/0/all/0/1"&gt;Hajime Nagahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Convolutional Kernel Networks for Airline Crew Scheduling. (arXiv:2105.11646v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11646</id>
        <link href="http://arxiv.org/abs/2105.11646"/>
        <updated>2021-05-26T01:22:11.205Z</updated>
        <summary type="html"><![CDATA[Motivated by the needs from an airline crew scheduling application, we
introduce structured convolutional kernel networks (Struct-CKN), which combine
CKNs from Mairal et al. (2014) in a structured prediction framework that
supports constraints on the outputs. CKNs are a particular kind of
convolutional neural networks that approximate a kernel feature map on training
data, thus combining properties of deep learning with the non-parametric
flexibility of kernel methods. Extending CKNs to structured outputs allows us
to obtain useful initial solutions on a flight-connection dataset that can be
further refined by an airline crew scheduling solver. More specifically, we use
a flight-based network modeled as a general conditional random field capable of
incorporating local constraints in the learning process. Our experiments
demonstrate that this approach yields significant improvements for the
large-scale crew pairing problem (50,000 flights per month) over standard
approaches, reducing the solution cost by 17% (a gain of millions of dollars)
and the cost of global constraints by 97%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yaakoubi_Y/0/1/0/all/0/1"&gt;Yassine Yaakoubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soumis_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Soumis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1"&gt;Simon Lacoste-Julien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Fairness-aware Learning Under Sample Selection Bias. (arXiv:2105.11570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11570</id>
        <link href="http://arxiv.org/abs/2105.11570"/>
        <updated>2021-05-26T01:22:11.198Z</updated>
        <summary type="html"><![CDATA[The underlying assumption of many machine learning algorithms is that the
training data and test data are drawn from the same distributions. However, the
assumption is often violated in real world due to the sample selection bias
between the training and test data. Previous research works focus on reweighing
biased training data to match the test data and then building classification
models on the reweighed training data. However, how to achieve fairness in the
built classification models is under-explored. In this paper, we propose a
framework for robust and fair learning under sample selection bias. Our
framework adopts the reweighing estimation approach for bias correction and the
minimax robust estimation approach for achieving robustness on prediction
accuracy. Moreover, during the minimax optimization, the fairness is achieved
under the worst case, which guarantees the model's fairness on test data. We
further develop two algorithms to handle sample selection bias when test data
is both available and unavailable. We conduct experiments on two real-world
datasets and the experimental results demonstrate its effectiveness in terms of
both utility and fairness metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1"&gt;Wei Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xintao Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LENs: a Python library for Logic Explained Networks. (arXiv:2105.11697v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11697</id>
        <link href="http://arxiv.org/abs/2105.11697"/>
        <updated>2021-05-26T01:22:11.198Z</updated>
        <summary type="html"><![CDATA[LENs is a Python module integrating a variety of state-of-the-art approaches
to provide logic explanations from neural networks. This package focuses on
bringing these methods to non-specialists. It has minimal dependencies and it
is distributed under the Apache 2.0 licence allowing both academic and
commercial use. Source code and documentation can be downloaded from the github
repository: https://github.com/pietrobarbiero/logic_explainer_networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1"&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1"&gt;Gabriele Ciravegna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1"&gt;Dobrik Georgiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1"&gt;Franscesco Giannini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Autoencoder-Based Error-Bounded Compression for Scientific Data. (arXiv:2105.11730v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11730</id>
        <link href="http://arxiv.org/abs/2105.11730"/>
        <updated>2021-05-26T01:22:11.198Z</updated>
        <summary type="html"><![CDATA[Error-bounded lossy compression is becoming an indispensable technique for
the success of today's scientific projects with vast volumes of data produced
during the simulations or instrument data acquisitions. Not only can it
significantly reduce data size, but it also can control the compression errors
based on user-specified error bounds. Autoencoder (AE) models have been widely
used in image compression, but few AE-based compression approaches support
error-bounding features, which are highly required by scientific applications.
To address this issue, we explore using convolutional autoencoders to improve
error-bounded lossy compression for scientific data, with the following three
key contributions. (1) We provide an in-depth investigation of the
characteristics of various autoencoder models and develop an error-bounded
autoencoder-based framework in terms of the SZ model. (2) We optimize the
compression quality for main stages in our designed AE-based error-bounded
compression framework, fine-tuning the block sizes and latent sizes and also
optimizing the compression efficiency of latent vectors. (3) We evaluate our
proposed solution using five real-world scientific datasets and comparing them
with six other related works. Experiments show that our solution exhibits a
very competitive compression quality from among all the compressors in our
tests. In absolute terms, it can obtain a much better compression quality (100%
~ 800% improvement in compression ratio with the same data distortion) compared
with SZ2.1 and ZFP in cases with a high compression ratio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1"&gt;Sheng Di&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Sian Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dingwen Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zizhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cappello_F/0/1/0/all/0/1"&gt;Franck Cappello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Model-based Off-policy Reinforcement Learning for Eco-Driving in Connected and Automated Hybrid Electric Vehicles. (arXiv:2105.11640v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11640</id>
        <link href="http://arxiv.org/abs/2105.11640"/>
        <updated>2021-05-26T01:22:11.197Z</updated>
        <summary type="html"><![CDATA[Connected and Automated Hybrid Electric Vehicles have the potential to reduce
fuel consumption and travel time in real-world driving conditions. The
eco-driving problem seeks to design optimal speed and power usage profiles
based upon look-ahead information from connectivity and advanced mapping
features. Recently, Deep Reinforcement Learning (DRL) has been applied to the
eco-driving problem. While the previous studies synthesize simulators and
model-free DRL to reduce online computation, this work proposes a Safe
Off-policy Model-Based Reinforcement Learning algorithm for the eco-driving
problem. The advantages over the existing literature are three-fold. First, the
combination of off-policy learning and the use of a physics-based model
improves the sample efficiency. Second, the training does not require any
extrinsic rewarding mechanism for constraint satisfaction. Third, the
feasibility of trajectory is guaranteed by using a safe set approximated by
deep generative models.

The performance of the proposed method is benchmarked against a baseline
controller representing human drivers, a previously designed model-free DRL
strategy, and the wait-and-see optimal solution. In simulation, the proposed
algorithm leads to a policy with a higher average speed and a better fuel
economy compared to the model-free agent. Compared to the baseline controller,
the learned strategy reduces the fuel consumption by more than 21\% while
keeping the average speed comparable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhaoxuan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pivaro_N/0/1/0/all/0/1"&gt;Nicola Pivaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Shobhit Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canova_M/0/1/0/all/0/1"&gt;Marcello Canova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Constrained Deep Learning Approaches for Inverse Problems. (arXiv:2105.12033v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.12033</id>
        <link href="http://arxiv.org/abs/2105.12033"/>
        <updated>2021-05-26T01:22:11.197Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL), in particular deep neural networks (DNN), by design is
purely data-driven and in general does not require physics. This is the
strength of DL but also one of its key limitations when applied to science and
engineering problems in which underlying physical properties (such as
stability, conservation, and positivity) and desired accuracy need to be
achieved. DL methods in their original forms are not capable of respecting the
underlying mathematical models or achieving desired accuracy even in big-data
regimes. On the other hand, many data-driven science and engineering problems,
such as inverse problems, typically have limited experimental or observational
data, and DL would overfit the data in this case. Leveraging information
encoded in the underlying mathematical models, we argue, not only compensates
missing information in low data regimes but also provides opportunities to
equip DL methods with the underlying physics and hence obtaining higher
accuracy. This short communication introduces several model-constrained DL
approaches (including both feed-forward DNN and autoencoders) that are capable
of learning not only information hidden in the training data but also in the
underlying mathematical models to solve inverse problems. We present and
provide intuitions for our formulations for general nonlinear problems. For
linear inverse problems and linear networks, the first order optimality
conditions show that our model-constrained DL approaches can learn information
encoded in the underlying mathematical models, and thus can produce consistent
or equivalent inverse solutions, while naive purely data-based counterparts
cannot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hai V. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bui_Thanh_T/0/1/0/all/0/1"&gt;Tan Bui-Thanh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Scalable Verification of RL-Driven Systems. (arXiv:2105.11931v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11931</id>
        <link href="http://arxiv.org/abs/2105.11931"/>
        <updated>2021-05-26T01:22:11.196Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have gained significant popularity in recent
years, becoming the state of the art in a variety of domains. In particular,
deep reinforcement learning (DRL) has recently been employed to train DNNs that
act as control policies for various types of real-world systems. In this work,
we present the whiRL 2.0 tool, which implements a new approach for verifying
complex properties of interest for such DRL systems. To demonstrate the
benefits of whiRL 2.0, we apply it to case studies from the communication
networks domain that have recently been used to motivate formal verification of
DRL systems, and which exhibit characteristics that are conducive for scalable
verification. We propose techniques for performing k-induction and automated
invariant inference on such systems, and use these techniques for proving
safety and liveness properties of interest that were previously impossible to
verify due to the scalability barriers of prior approaches. Furthermore, we
show how our proposed techniques provide insights into the inner workings and
the generalizability of DRL systems. whiRL 2.0 is publicly available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amir_G/0/1/0/all/0/1"&gt;Guy Amir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schapira_M/0/1/0/all/0/1"&gt;Michael Schapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1"&gt;Guy Katz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Network Based VC Investment Success Prediction. (arXiv:2105.11537v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.11537</id>
        <link href="http://arxiv.org/abs/2105.11537"/>
        <updated>2021-05-26T01:22:11.193Z</updated>
        <summary type="html"><![CDATA[Predicting the start-ups that will eventually succeed is essentially
important for the venture capital business and worldwide policy makers,
especially at an early stage such that rewards can possibly be exponential.

Though various empirical studies and data-driven modeling work have been
done, the predictive power of the complex networks of stakeholders including
venture capital investors, start-ups, and start-ups' managing members has not
been thoroughly explored. We design an incremental representation learning
mechanism and a sequential learning model, utilizing the network structure
together with the rich attributes of the nodes. In general, our method achieves
the state-of-the-art prediction performance on a comprehensive dataset of
global venture capital investments and surpasses human investors by large
margins. Specifically, it excels at predicting the outcomes for start-ups in
industries such as healthcare and IT. Meanwhile, we shed light on impacts on
start-up success from observable factors including gender, education, and
networking, which can be of value for practitioners as well as policy makers
when they screen ventures of high growth potentials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Shiwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1"&gt;Shuai Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1"&gt;Kaihao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kunpeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1"&gt;Suting Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1"&gt;Qing Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jinjie Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Label Encoding for Boundary Discontinuity Free Rotation Detection. (arXiv:2011.09670v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09670</id>
        <link href="http://arxiv.org/abs/2011.09670"/>
        <updated>2021-05-26T01:22:11.188Z</updated>
        <summary type="html"><![CDATA[Rotation detection serves as a fundamental building block in many visual
applications involving aerial image, scene text, and face etc. Differing from
the dominant regression-based approaches for orientation estimation, this paper
explores a relatively less-studied methodology based on classification. The
hope is to inherently dismiss the boundary discontinuity issue as encountered
by the regression-based detectors. We propose new techniques to push its
frontier in two aspects: i) new encoding mechanism: the design of two Densely
Coded Labels (DCL) for angle classification, to replace the Sparsely Coded
Label (SCL) in existing classification-based detectors, leading to three times
training speed increase as empirically observed across benchmarks, further with
notable improvement in detection accuracy; ii) loss re-weighting: we propose
Angle Distance and Aspect Ratio Sensitive Weighting (ADARSW), which improves
the detection accuracy especially for square-like objects, by making DCL-based
detectors sensitive to angular distance and object's aspect ratio. Extensive
experiments and visual analysis on large-scale public datasets for aerial
images i.e. DOTA, UCAS-AOD, HRSC2016, as well as scene text dataset ICDAR2015
and MLT, show the effectiveness of our approach. The source code is available
at https://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflow and is also
integrated in our open source rotation detection benchmark:
https://github.com/yangxue0827/RotationDetection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xue Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Liping Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yue Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wentao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Performance Analysis of 3D Face Alignment. (arXiv:2004.06550v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.06550</id>
        <link href="http://arxiv.org/abs/2004.06550"/>
        <updated>2021-05-26T01:22:11.172Z</updated>
        <summary type="html"><![CDATA[We address the problem of analyzing the performance of 3D face alignment
(3DFA) algorithms. Traditionally, performance analysis relies on carefully
annotated datasets. Here, these annotations correspond to the 3D coordinates of
a set of pre-defined facial landmarks. However, this annotation process, be it
manual or automatic, is rarely error-free, which strongly biases the analysis.
In contrast, we propose a fully unsupervised methodology based on robust
statistics and a parametric confidence test. We revisit the problem of robust
estimation of the rigid transformation between two point sets and we describe
two algorithms, one based on a mixture between a Gaussian and a uniform
distribution, and another one based on the generalized Student's
t-distribution. We show that these methods are robust to up to 50% outliers,
which makes them suitable for mapping a face, from an unknown pose to a frontal
pose, in the presence of facial expressions and occlusions. Using these methods
in conjunction with large datasets of face images, we build a statistical
frontal facial model and an associated parametric confidence metric, eventually
used for performance analysis. We empirically show that the proposed pipeline
is neither method-biased nor data-biased, and that it can be used to assess
both the performance of 3DFA algorithms and the accuracy of annotations of face
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1"&gt;Mostafa Sadeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guy_S/0/1/0/all/0/1"&gt;Sylvain Guy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raison_A/0/1/0/all/0/1"&gt;Adrien Raison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1"&gt;Xavier Alameda-Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1"&gt;Radu Horaud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Based Sleep Phases Classification for Resource Constraint Environments. (arXiv:2105.11452v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2105.11452</id>
        <link href="http://arxiv.org/abs/2105.11452"/>
        <updated>2021-05-26T01:22:11.167Z</updated>
        <summary type="html"><![CDATA[Sleep is restoration process of the body. The efficiency of this restoration
process is directly correlated to the amount of time spent at each sleep phase.
Hence, automatic tracking of sleep via wearable devices has attracted both the
researchers and industry. Current state-of-the-art sleep tracking solutions are
memory and processing greedy and they require cloud or mobile phone
connectivity. We propose a memory efficient sleep tracking architecture which
can work in the embedded environment without needing any cloud or mobile phone
connection. In this study, a novel architecture is proposed that consists of a
feature extraction and Artificial Neural Networks based stacking classifier.
Besides, we discussed how to tackle with sequential nature of the sleep staging
for the memory constraint environments through the proposed framework. To
verify the system, a dataset is collected from 24 different subjects for 31
nights with a wrist worn device having 3-axis accelerometer (ACC) and
photoplethysmogram (PPG) sensors. Over the collected dataset, the proposed
classification architecture achieves 20\% and 14\% better F1 scores than its
competitors. Apart from the superior performance, proposed architecture is a
promising solution for resource constraint embedded systems by allocating only
4.2 kilobytes of memory (RAM).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kopru_B/0/1/0/all/0/1"&gt;Berkay K&amp;#xf6;pr&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aslan_M/0/1/0/all/0/1"&gt;Murat Aslan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kholmatov_A/0/1/0/all/0/1"&gt;Alisher Kholmatov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Principal Component Analysis Using a Novel Kernel Related with the L1-Norm. (arXiv:2105.11634v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11634</id>
        <link href="http://arxiv.org/abs/2105.11634"/>
        <updated>2021-05-26T01:22:11.161Z</updated>
        <summary type="html"><![CDATA[We consider a family of vector dot products that can be implemented using
sign changes and addition operations only. The dot products are
energy-efficient as they avoid the multiplication operation entirely. Moreover,
the dot products induce the $\ell_1$-norm, thus providing robustness to
impulsive noise. First, we analytically prove that the dot products yield
symmetric, positive semi-definite generalized covariance matrices, thus
enabling principal component analysis (PCA). Moreover, the generalized
covariance matrices can be constructed in an Energy Efficient (EEF) manner due
to the multiplication-free property of the underlying vector products. We
present image reconstruction examples in which our EEF PCA method result in the
highest peak signal-to-noise ratios compared to the ordinary $\ell_2$-PCA and
the recursive $\ell_1$-PCA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1"&gt;Hongyi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badawi_D/0/1/0/all/0/1"&gt;Diaa Badawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyuncu_E/0/1/0/all/0/1"&gt;Erdem Koyuncu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1"&gt;A. Enis Cetin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks and End-to-End Learning for Audio Compression. (arXiv:2105.11681v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11681</id>
        <link href="http://arxiv.org/abs/2105.11681"/>
        <updated>2021-05-26T01:22:11.155Z</updated>
        <summary type="html"><![CDATA[Recent achievements in end-to-end deep learning have encouraged the
exploration of tasks dealing with highly structured data with unified deep
network models. Having such models for compressing audio signals has been
challenging since it requires discrete representations that are not easy to
train with end-to-end backpropagation. In this paper, we present an end-to-end
deep learning approach that combines recurrent neural networks (RNNs) within
the training strategy of variational autoencoders (VAEs) with a binary
representation of the latent space. We apply a reparametrization trick for the
Bernoulli distribution for the discrete representations, which allows smooth
backpropagation. In addition, our approach allows the separation of the encoder
and decoder, which is necessary for compression tasks. To our best knowledge,
this is the first end-to-end learning for a single audio compression model with
RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rim_D/0/1/0/all/0/1"&gt;Daniela N. Rim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_I/0/1/0/all/0/1"&gt;Inseon Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Heeyoul Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08850</id>
        <link href="http://arxiv.org/abs/2102.08850"/>
        <updated>2021-05-26T01:22:11.140Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has recently seen tremendous success in self-supervised
learning. So far, however, it is largely unclear why the learned
representations generalize so effectively to a large variety of downstream
tasks. We here prove that feedforward models trained with objectives belonging
to the commonly used InfoNCE family learn to implicitly invert the underlying
generative model of the observed data. While the proofs make certain
statistical assumptions about the generative model, we observe empirically that
our findings hold even if these assumptions are severely violated. Our theory
highlights a fundamental connection between contrastive learning, generative
modeling, and nonlinear independent component analysis, thereby furthering our
understanding of the learned representations as well as providing a theoretical
foundation to derive more effective contrastive losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1"&gt;Yash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1"&gt;Steffen Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IGO-QNN: Quantum Neural Network Architecture for Inductive Grover Oracularization. (arXiv:2105.11603v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2105.11603</id>
        <link href="http://arxiv.org/abs/2105.11603"/>
        <updated>2021-05-26T01:22:11.134Z</updated>
        <summary type="html"><![CDATA[We propose a novel paradigm of integration of Grover's algorithm in a machine
learning framework: the inductive Grover oracular quantum neural network
(IGO-QNN). The model defines a variational quantum circuit with hidden layers
of parameterized quantum neurons densely connected via entangle synapses to
encode a dynamic Grover's search oracle that can be trained from a set of
database-hit training examples. This widens the range of problem applications
of Grover's unstructured search algorithm to include the vast majority of
problems lacking analytic descriptions of solution verifiers, allowing for
quadratic speed-up in unstructured search for the set of search problems with
relationships between input and output spaces that are tractably underivable
deductively. This generalization of Grover's oracularization may prove
particularly effective in deep reinforcement learning, computer vision, and,
more generally, as a feature vector classifier at the top of an existing model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Areeq I. Hasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Explainable AI and Uncertainty Quantification to Enhance Trustability. (arXiv:2105.11828v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.11828</id>
        <link href="http://arxiv.org/abs/2105.11828"/>
        <updated>2021-05-26T01:22:11.128Z</updated>
        <summary type="html"><![CDATA[After the tremendous advances of deep learning and other AI methods, more
attention is flowing into other properties of modern approaches, such as
interpretability, fairness, etc. combined in frameworks like Responsible AI.
Two research directions, namely Explainable AI and Uncertainty Quantification
are becoming more and more important, but have been so far never combined and
jointly explored. In this paper, I show how both research areas provide
potential for combination, why more research should be done in this direction
and how this would lead to an increase in trustability in AI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seuss_D/0/1/0/all/0/1"&gt;Dominik Seu&amp;#xdf;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending the Abstraction of Personality Types based on MBTI with Machine Learning and Natural Language Processing. (arXiv:2105.11798v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11798</id>
        <link href="http://arxiv.org/abs/2105.11798"/>
        <updated>2021-05-26T01:22:11.123Z</updated>
        <summary type="html"><![CDATA[A data-centric approach with Natural Language Processing (NLP) to predict
personality types based on the MBTI (an introspective self-assessment
questionnaire that indicates different psychological preferences about how
people perceive the world and make decisions) through systematic enrichment of
text representation, based on the domain of the area, under the generation of
features based on three types of analysis: sentimental, grammatical and
aspects. The experimentation had a robust baseline of stacked models, with
premature optimization of hyperparameters through grid search, with gradual
feedback, for each of the four classifiers (dichotomies) of MBTI. The results
showed that attention to the data iteration loop focused on quality,
explanatory power and representativeness for the abstraction of more
relevant/important resources for the studied phenomenon made it possible to
improve the evaluation metrics results more quickly and less costly than
complex models such as the LSTM or state of the art ones as BERT, as well as
the importance of these results by comparisons made from various perspectives.
In addition, the study demonstrated a broad spectrum for the evolution and
deepening of the task and possible approaches for a greater extension of the
abstraction of personality types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Basto_C/0/1/0/all/0/1"&gt;Carlos Basto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Hyperparameter Tuning Through Visualization and Inference. (arXiv:2105.11516v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2105.11516</id>
        <link href="http://arxiv.org/abs/2105.11516"/>
        <updated>2021-05-26T01:22:11.116Z</updated>
        <summary type="html"><![CDATA[For deep learning practitioners, hyperparameter tuning for optimizing model
performance can be a computationally expensive task. Though visualization can
help practitioners relate hyperparameter settings to overall model performance,
significant manual inspection is still required to guide the hyperparameter
settings in the next batch of experiments. In response, we present a
streamlined visualization system enabling deep learning practitioners to more
efficiently explore, tune, and optimize hyperparameters in a batch of
experiments. A key idea is to directly suggest more optimal hyperparameter
values using a predictive mechanism. We then integrate this mechanism with
current visualization practices for deep learning. Moreover, an analysis on the
variance in a selected performance metric in the context of the model
hyperparameters shows the impact that certain hyperparameters have on the
performance metric. We evaluate the tool with a user study on deep learning
model builders, finding that our participants have little issue adopting the
tool and working with it as part of their workflow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1"&gt;Hyekang Joo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1"&gt;Calvin Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_I/0/1/0/all/0/1"&gt;Ishan Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battle_L/0/1/0/all/0/1"&gt;Leilani Battle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are Labels Always Necessary for Classifier Accuracy Evaluation?. (arXiv:2007.02915v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02915</id>
        <link href="http://arxiv.org/abs/2007.02915"/>
        <updated>2021-05-26T01:22:11.110Z</updated>
        <summary type="html"><![CDATA[To calculate the model accuracy on a computer vision task, e.g., object
recognition, we usually require a test set composing of test samples and their
ground truth labels. Whilst standard usage cases satisfy this requirement, many
real-world scenarios involve unlabeled test data, rendering common model
evaluation methods infeasible. We investigate this important and under-explored
problem, Automatic model Evaluation (AutoEval). Specifically, given a labeled
training set and a classifier, we aim to estimate the classification accuracy
on unlabeled test datasets. We construct a meta-dataset: a dataset comprised of
datasets generated from the original images via various transformations such as
rotation, background substitution, foreground scaling, etc. As the
classification accuracy of the model on each sample (dataset) is known from the
original dataset labels, our task can be solved via regression. Using the
feature statistics to represent the distribution of a sample dataset, we can
train regression models (e.g., a regression neural network) to predict model
performance. Using synthetic meta-dataset and real-world datasets in training
and testing, respectively, we report a reasonable and promising prediction of
the model accuracy. We also provide insights into the application scope,
limitation, and potential future direction of AutoEval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1"&gt;Weijian Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropic Out-of-Distribution Detection. (arXiv:1908.05569v13 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.05569</id>
        <link href="http://arxiv.org/abs/1908.05569"/>
        <updated>2021-05-26T01:22:11.104Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution (OOD) detection approaches usually present special
requirements (e.g., hyperparameter validation, collection of outlier data) and
produce side effects (e.g., classification accuracy drop, slower
energy-inefficient inferences). We argue that these issues are a consequence of
the SoftMax loss anisotropy and disagreement with the maximum entropy
principle. Thus, we propose the IsoMax loss and the entropic score. The
seamless drop-in replacement of the SoftMax loss by IsoMax loss requires
neither additional data collection nor hyperparameter validation. The trained
models do not exhibit classification accuracy drop and produce fast
energy-efficient inferences. Moreover, our experiments show that training
neural networks with IsoMax loss significantly improves their OOD detection
performance. The IsoMax loss exhibits state-of-the-art performance under the
mentioned conditions (fast energy-efficient inference, no classification
accuracy drop, no collection of outlier data, and no hyperparameter
validation), which we call the seamless OOD detection task. In future work,
current OOD detection methods may replace the SoftMax loss with the IsoMax loss
to improve their performance on the commonly studied non-seamless OOD detection
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1"&gt;Tsang Ing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1"&gt;Cleber Zanchettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Adriano L. I. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The 5th AI City Challenge. (arXiv:2104.12233v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12233</id>
        <link href="http://arxiv.org/abs/2104.12233"/>
        <updated>2021-05-26T01:22:11.098Z</updated>
        <summary type="html"><![CDATA[The AI City Challenge was created with two goals in mind: (1) pushing the
boundaries of research and development in intelligent video analysis for
smarter cities use cases, and (2) assessing tasks where the level of
performance is enough to cause real-world adoption. Transportation is a segment
ripe for such adoption. The fifth AI City Challenge attracted 305 participating
teams across 38 countries, who leveraged city-scale real traffic data and
high-quality synthetic data to compete in five challenge tracks. Track 1
addressed video-based automatic vehicle counting, where the evaluation being
conducted on both algorithmic effectiveness and computational efficiency. Track
2 addressed city-scale vehicle re-identification with augmented synthetic data
to substantially increase the training set for the task. Track 3 addressed
city-scale multi-target multi-camera vehicle tracking. Track 4 addressed
traffic anomaly detection. Track 5 was a new track addressing vehicle retrieval
using natural language descriptions. The evaluation system shows a general
leader board of all submitted results, and a public leader board of results
limited to the contest participation rules, where teams are not allowed to use
external data in their work. The public leader board shows results more close
to real-world situations where annotated data is limited. Results show the
promise of AI in Smarter Transportation. State-of-the-art performance for some
tasks shows that these technologies are ready for adoption in real-world
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naphade_M/0/1/0/all/0/1"&gt;Milind Naphade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1"&gt;David C. Anastasiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1"&gt;Ming-Ching Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yue Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1"&gt;Pranamesh Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1"&gt;Christian E. Lopez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Anuj Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1"&gt;Qi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1"&gt;Vitaly Ablavsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1"&gt;Stan Sclaroff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[There is no data like more data -- current status of machine learning datasets in remote sensing. (arXiv:2105.11726v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11726</id>
        <link href="http://arxiv.org/abs/2105.11726"/>
        <updated>2021-05-26T01:22:11.061Z</updated>
        <summary type="html"><![CDATA[Annotated datasets have become one of the most crucial preconditions for the
development and evaluation of machine learning-based methods designed for the
automated interpretation of remote sensing data. In this paper, we review the
historic development of such datasets, discuss their features based on a few
selected examples, and address open issues for future developments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1"&gt;Michael Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1"&gt;Seyed Ali Ahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansch_R/0/1/0/all/0/1"&gt;Ronny H&amp;#xe4;nsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training. (arXiv:2105.11686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11686</id>
        <link href="http://arxiv.org/abs/2105.11686"/>
        <updated>2021-05-26T01:22:11.053Z</updated>
        <summary type="html"><![CDATA[It is important to study what implicit regularization is imposed on the loss
function during the training that leads over-parameterized neural networks
(NNs) to good performance on real dataset. Empirically, existing works have
shown that weights of NNs condense on isolated orientations with small
initialization. The condensation implies that the NN learns features from the
training data and is effectively a much smaller network. In this work, we show
that the singularity of the activation function at original point is a key
factor to understanding the condensation at initial training stage. Our
experiments suggest that the maximal number of condensed orientations is twice
of the singularity order. Our theoretical analysis confirms experiments for two
cases, one is for the first-order singularity activation function and the other
is for the one-dimensional input. This work takes a step towards understanding
how small initialization implicitly leads NNs to condensation at initial
training, which is crucial to understand the training and the learning of deep
NNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhi-Qin John Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hanxu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yaoyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FSOCO: The Formula Student Objects in Context Dataset. (arXiv:2012.07139v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07139</id>
        <link href="http://arxiv.org/abs/2012.07139"/>
        <updated>2021-05-26T01:22:11.044Z</updated>
        <summary type="html"><![CDATA[This paper presents the FSOCO dataset, a collaborative dataset for
vision-based cone detection systems in Formula Student Driverless competitions.
It contains human annotated ground truth labels for both bounding boxes and
instance-wise segmentation masks. The data buy-in philosophy of FSOCO asks
student teams to contribute to the database first before being granted access
ensuring continuous growth. By providing clear labeling guidelines and tools
for a sophisticated raw image selection, new annotations are guaranteed to meet
the desired quality. The effectiveness of the approach is shown by comparing
prediction results of a network trained on FSOCO and its unregulated
predecessor. The FSOCO dataset can be found at fsoco-dataset.com.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dodel_D/0/1/0/all/0/1"&gt;David Dodel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schotz_M/0/1/0/all/0/1"&gt;Michael Sch&amp;#xf6;tz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vodisch_N/0/1/0/all/0/1"&gt;Niclas V&amp;#xf6;disch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Small and large scale critical infrastructures detection based on deep learning using high resolution orthogonal images. (arXiv:2105.11844v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11844</id>
        <link href="http://arxiv.org/abs/2105.11844"/>
        <updated>2021-05-26T01:22:11.034Z</updated>
        <summary type="html"><![CDATA[The detection of critical infrastructures is of high importance in several
fields such as security, anomaly detection, land use planning and land use
change detection. However, critical infrastructures detection in aerial and
satellite images is still a challenge as each one has completely different size
and requires different spacial resolution to be identified correctly.
Heretofore, there are no special datasets for training critical infrastructures
detectors. This paper presents a smart dataset as well as a
resolution-independent critical infrastructure detection system. In particular,
guided by the performance of the detection model, we built a dataset organized
into two scales, small and large scale, and designed a two-stage deep learning
detection of different scale critical infrastructures (DetDSCI) methodology in
ortho-images. DetDSCI methodology first determines the input image zoom level
using a classification model, then analyses the input image with the
appropriate scale detection model. Our experiments show that DetDSCI
methodology achieves up to 37,53% F1 improvement with respect to the baseline
detector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Francisco_P/0/1/0/all/0/1"&gt;P&amp;#xe9;rez-Hern&amp;#xe1;ndez Francisco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jose_R/0/1/0/all/0/1"&gt;Rodr&amp;#xed;guez-Ortega Jos&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yassir_B/0/1/0/all/0/1"&gt;Benhammou Yassir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francisco_H/0/1/0/all/0/1"&gt;Herrera Francisco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siham_T/0/1/0/all/0/1"&gt;Tabik Siham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey on Semi-, Self- and Unsupervised Learning for Image Classification. (arXiv:2002.08721v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08721</id>
        <link href="http://arxiv.org/abs/2002.08721"/>
        <updated>2021-05-26T01:22:11.032Z</updated>
        <summary type="html"><![CDATA[While deep learning strategies achieve outstanding results in computer vision
tasks, one issue remains: The current strategies rely heavily on a huge amount
of labeled data. In many real-world problems, it is not feasible to create such
an amount of labeled training data. Therefore, it is common to incorporate
unlabeled data into the training process to reach equal results with fewer
labels. Due to a lot of concurrent research, it is difficult to keep track of
recent developments. In this survey, we provide an overview of often used ideas
and methods in image classification with fewer labels. We compare 34 methods in
detail based on their performance and their commonly used ideas rather than a
fine-grained taxonomy. In our analysis, we identify three major trends that
lead to future research opportunities. 1. State-of-the-art methods are
scaleable to real-world applications in theory but issues like class imbalance,
robustness, or fuzzy labels are not considered. 2. The degree of supervision
which is needed to achieve comparable results to the usage of all labels is
decreasing and therefore methods need to be extended to settings with a
variable number of classes. 3. All methods share some common ideas but we
identify clusters of methods that do not share many ideas. We show that
combining ideas from different clusters can lead to better performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1"&gt;Lars Schmarje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1"&gt;Monty Santarossa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1"&gt;Simon-Martin Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1"&gt;Reinhard Koch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep Spiking Neural Networks. (arXiv:2105.11654v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2105.11654</id>
        <link href="http://arxiv.org/abs/2105.11654"/>
        <updated>2021-05-26T01:22:11.021Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural
networks, have attracted great attentions from researchers and industry. The
most efficient way to train deep SNNs is through ANN-SNN conversion. However,
the conversion usually suffers from accuracy loss and long inference time,
which impede the practical application of SNN. In this paper, we theoretically
analyze ANN-SNN conversion and derive sufficient conditions of the optimal
conversion. To better correlate ANN-SNN and get greater accuracy, we propose
Rate Norm Layer to replace the ReLU activation function in source ANN training,
enabling direct conversion from a trained ANN to an SNN. Moreover, we propose
an optimal fit curve to quantify the fit between the activation value of source
ANN and the actual firing rate of target SNN. We show that the inference time
can be reduced by optimizing the upper bound of the fit curve in the revised
ANN to achieve fast inference. Our theory can explain the existing work on fast
reasoning and get better results. The experimental results show that the
proposed method achieves near loss less conversion with VGG-16,
PreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster
reasoning performance under 0.265x energy consumption of the typical method.
The code is available at
https://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jianhao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesizing Optical and SAR Imagery From Land Cover Maps and Auxiliary Raster Data. (arXiv:2011.11314v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11314</id>
        <link href="http://arxiv.org/abs/2011.11314"/>
        <updated>2021-05-26T01:22:11.004Z</updated>
        <summary type="html"><![CDATA[We synthesize both optical RGB and synthetic aperture radar (SAR) remote
sensing images from land cover maps and auxiliary raster data using generative
adversarial networks (GANs). In remote sensing, many types of data, such as
digital elevation models (DEMs) or precipitation maps, are often not reflected
in land cover maps but still influence image content or structure. Including
such data in the synthesis process increases the quality of the generated
images and exerts more control on their characteristics. Spatially adaptive
normalization layers fuse both inputs and are applied to a full-blown generator
architecture consisting of encoder and decoder to take full advantage of the
information content in the auxiliary raster data. Our method successfully
synthesizes medium (10 m) and high (1 m) resolution images when trained with
the corresponding data set. We show the advantage of data fusion of land cover
maps and auxiliary information using mean intersection over unions (mIoUs),
pixel accuracy, and Fr\'echet inception distances (FIDs) using pretrained U-Net
segmentation models. Handpicked images exemplify how fusing information avoids
ambiguities in the synthesized images. By slightly editing the input, our
method can be used to synthesize realistic changes, i.e., raising the water
levels. The source code is available at https://github.com/gbaier/rs_img_synth
and we published the newly created high-resolution dataset at
https://ieee-dataport.org/open-access/geonrw.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baier_G/0/1/0/all/0/1"&gt;Gerald Baier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deschemps_A/0/1/0/all/0/1"&gt;Antonin Deschemps&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1"&gt;Michael Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1"&gt;Naoto Yokoya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction and Description of Near-Future Activities in Video. (arXiv:1908.00943v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.00943</id>
        <link href="http://arxiv.org/abs/1908.00943"/>
        <updated>2021-05-26T01:22:10.979Z</updated>
        <summary type="html"><![CDATA[Most of the existing works on human activity analysis focus on recognition or
early recognition of the activity labels from complete or partial observations.
Similarly, almost all of the existing video captioning approaches focus on the
observed events in videos. Predicting the labels and the captions of future
activities where no frames of the predicted activities have been observed is a
challenging problem, with important applications that require anticipatory
response. In this work, we propose a system that can infer the labels and the
captions of a sequence of future activities. Our proposed network for label
prediction of a future activity sequence has three branches where the first
branch takes visual features from the objects present in the scene, the second
branch takes observed sequential activity features, and the third branch
captures the last observed activity features. The predicted labels and the
observed scene context are then mapped to meaningful captions using a
sequence-to-sequence learning-based method. Experiments on four challenging
activity analysis datasets and a video description dataset demonstrate that our
label prediction approach achieves comparable performance with the
state-of-the-arts and our captioning framework outperform the
state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmud_T/0/1/0/all/0/1"&gt;Tahmida Mahmud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Billah_M/0/1/0/all/0/1"&gt;Mohammad Billah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Mahmudul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KnowSR: Knowledge Sharing among Homogeneous Agents in Multi-agent Reinforcement Learning. (arXiv:2105.11611v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.11611</id>
        <link href="http://arxiv.org/abs/2105.11611"/>
        <updated>2021-05-26T01:22:10.972Z</updated>
        <summary type="html"><![CDATA[Recently, deep reinforcement learning (RL) algorithms have made great
progress in multi-agent domain. However, due to characteristics of RL, training
for complex tasks would be resource-intensive and time-consuming. To meet this
challenge, mutual learning strategy between homogeneous agents is essential,
which is under-explored in previous studies, because most existing methods do
not consider to use the knowledge of agent models. In this paper, we present an
adaptation method of the majority of multi-agent reinforcement learning (MARL)
algorithms called KnowSR which takes advantage of the differences in learning
between agents. We employ the idea of knowledge distillation (KD) to share
knowledge among agents to shorten the training phase. To empirically
demonstrate the robustness and effectiveness of KnowSR, we performed extensive
experiments on state-of-the-art MARL algorithms in collaborative and
competitive scenarios. The results demonstrate that KnowSR outperforms recently
reported methodologies, emphasizing the importance of the proposed knowledge
sharing for MARL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zijian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kele Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1"&gt;Bo Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huaimin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1"&gt;Hongda Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Unpaired Depth Enhancement and Super-Resolution in the Wild. (arXiv:2105.12038v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12038</id>
        <link href="http://arxiv.org/abs/2105.12038"/>
        <updated>2021-05-26T01:22:10.966Z</updated>
        <summary type="html"><![CDATA[Depth maps captured with commodity sensors are often of low quality and
resolution; these maps need to be enhanced to be used in many applications.
State-of-the-art data-driven methods of depth map super-resolution rely on
registered pairs of low- and high-resolution depth maps of the same scenes.
Acquisition of real-world paired data requires specialized setups. Another
alternative, generating low-resolution maps from high-resolution maps by
subsampling, adding noise and other artificial degradation methods, does not
fully capture the characteristics of real-world low-resolution images. As a
consequence, supervised learning methods trained on such artificial paired data
may not perform well on real-world low-resolution inputs. We consider an
approach to depth map enhancement based on learning from unpaired data. While
many techniques for unpaired image-to-image translation have been proposed,
most are not directly applicable to depth maps. We propose an unpaired learning
method for simultaneous depth enhancement and super-resolution, which is based
on a learnable degradation model and surface normal estimates as features to
produce more accurate depth maps. We demonstrate that our method outperforms
existing unpaired methods and performs on par with paired methods on a new
benchmark for unpaired learning that we developed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safin_A/0/1/0/all/0/1"&gt;Aleksandr Safin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1"&gt;Maxim Kan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drobyshev_N/0/1/0/all/0/1"&gt;Nikita Drobyshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voynov_O/0/1/0/all/0/1"&gt;Oleg Voynov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1"&gt;Alexey Artemov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1"&gt;Alexander Filippov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1"&gt;Denis Zorin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification. (arXiv:2104.14528v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14528</id>
        <link href="http://arxiv.org/abs/2104.14528"/>
        <updated>2021-05-26T01:22:10.966Z</updated>
        <summary type="html"><![CDATA[Existing deep learning methods for diagnosis of gastric cancer commonly use
convolutional neural networks (CNN). Recently, the Visual Transformer (VT) has
attracted a major attention because of its performance and efficiency, but its
applications are mostly in the field of computer vision. In this paper, a
multi-scale visual transformer model, referred to as GasHis-Transformer, is
proposed for gastric histopathology image classification (GHIC), which enables
the automatic classification of microscopic gastric images into abnormal and
normal cases. The GasHis-Transformer model consists of two key modules: a
global information module (GIM) and a local information module (LIM) to extract
pathological features effectively. In our experiments, a public hematoxylin and
eosin (H&E) stained gastric histopathology dataset with 280 abnormal or normal
images using the GasHis-Transformer model is applied to estimate precision,
recall, F1-score, and accuracy on the testing set as 98.0%, 100.0%, 96.0% and
98.0% respectively. Furthermore, a critical study is conducted to evaluate the
robustness of GasHis-Transformer according to add ten different noises
including adversarial attack and traditional image noise. In addition, a
clinically meaningful study is executed to test the gastric cancer
identification of GasHis-Transformerwith 420 abnormal images and achieves 96.2%
accuracy. Finally, a comparative study is performed to test the
generalizability with both H&E and Immunohistochemical (IHC) stained images on
a lymphoma image dataset, a breast cancer dataset and a cervical cancer
dataset, producing comparable F1-scores (85.6%, 82.8% and 65.7%, respectively)
and accuracy (83.9%, 89.4% and 65.7%, respectively) respectively. In
conclusion, GasHis-Transformerdemonstrates a high classification performance
and shows its significant potential in histopathology image analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changhao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yudong Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1"&gt;Yueyang Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tab.IAIS: Flexible Table Recognition and Semantic Interpretation System. (arXiv:2105.11879v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11879</id>
        <link href="http://arxiv.org/abs/2105.11879"/>
        <updated>2021-05-26T01:22:10.960Z</updated>
        <summary type="html"><![CDATA[Table extraction is an important but still unsolved problem. In this paper,
we introduce a flexible end-to-end table extraction system. We develop two
rule-based algorithms that perform the complete table recognition process and
support the most frequent table formats found in the scientific literature.
Moreover, to incorporate the extraction of semantic information into the table
recognition process, we develop a graph-based table interpretation method. We
conduct extensive experiments on the challenging table recognition benchmarks
ICDAR 2013 and ICDAR 2019. Our table recognition approach achieves results
competitive with state-of-the-art approaches. Moreover, our complete
information extraction system exhibited a high F1 score of 0.7380 proving the
utility of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Namysl_M/0/1/0/all/0/1"&gt;Marcin Namysl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esser_A/0/1/0/all/0/1"&gt;Alexander M. Esser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1"&gt;Joachim K&amp;#xf6;hler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Learning with Part Discovery and Augmentation from Unlabeled Images. (arXiv:2105.11874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11874</id>
        <link href="http://arxiv.org/abs/2105.11874"/>
        <updated>2021-05-26T01:22:10.959Z</updated>
        <summary type="html"><![CDATA[Few-shot learning is a challenging task since only few instances are given
for recognizing an unseen class. One way to alleviate this problem is to
acquire a strong inductive bias via meta-learning on similar tasks. In this
paper, we show that such inductive bias can be learned from a flat collection
of unlabeled images, and instantiated as transferable representations among
seen and unseen classes. Specifically, we propose a novel part-based
self-supervised representation learning scheme to learn transferable
representations by maximizing the similarity of an image to its discriminative
part. To mitigate the overfitting in few-shot classification caused by data
scarcity, we further propose a part augmentation strategy by retrieving extra
images from a base dataset. We conduct systematic studies on miniImageNet and
tieredImageNet benchmarks. Remarkably, our method yields impressive results,
outperforming the previous best unsupervised methods by 7.74% and 9.24% under
5-way 1-shot and 5-way 5-shot settings, which are comparable with
state-of-the-art supervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wentao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Chenyang Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zilei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VSGM -- Enhance robot task understanding ability through visual semantic graph. (arXiv:2105.08959v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08959</id>
        <link href="http://arxiv.org/abs/2105.08959"/>
        <updated>2021-05-26T01:22:10.958Z</updated>
        <summary type="html"><![CDATA[In recent years, developing AI for robotics has raised much attention. The
interaction of vision and language of robots is particularly difficult. We
consider that giving robots an understanding of visual semantics and language
semantics will improve inference ability. In this paper, we propose a novel
method-VSGM (Visual Semantic Graph Memory), which uses the semantic graph to
obtain better visual image features, improve the robot's visual understanding
ability. By providing prior knowledge of the robot and detecting the objects in
the image, it predicts the correlation between the attributes of the object and
the objects and converts them into a graph-based representation; and mapping
the object in the image to be a top-down egocentric map. Finally, the important
object features of the current task are extracted by Graph Neural Networks. The
method proposed in this paper is verified in the ALFRED (Action Learning From
Realistic Environments and Directives) dataset. In this dataset, the robot
needs to perform daily indoor household tasks following the required language
instructions. After the model is added to the VSGM, the task success rate can
be improved by 6~10%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1"&gt;Cheng Yu Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1"&gt;Mu-Chun Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11804</id>
        <link href="http://arxiv.org/abs/2105.11804"/>
        <updated>2021-05-26T01:22:10.943Z</updated>
        <summary type="html"><![CDATA[Few-Shot Learning (FSL) algorithms have made substantial progress in learning
novel concepts with just a handful of labelled data. To classify query
instances from novel classes encountered at test-time, they only require a
support set composed of a few labelled samples. FSL benchmarks commonly assume
that those queries come from the same distribution as instances in the support
set. However, in a realistic set-ting, data distribution is plausibly subject
to change, a situation referred to as Distribution Shift (DS). The present work
addresses the new and challenging problem of Few-Shot Learning under
Support/Query Shift (FSQS) i.e., when support and query instances are sampled
from related but different distributions. Our contributions are the following.
First, we release a testbed for FSQS, including datasets, relevant baselines
and a protocol for a rigorous and reproducible evaluation. Second, we observe
that well-established FSL algorithms unsurprisingly suffer from a considerable
drop in accuracy when facing FSQS, stressing the significance of our study.
Finally, we show that transductive algorithms can limit the inopportune effect
of DS. In particular, we study both the role of Batch-Normalization and Optimal
Transport (OT) in aligning distributions, bridging Unsupervised Domain
Adaptation with FSL. This results in a new method that efficiently combines OT
with the celebrated Prototypical Networks. We bring compelling experiments
demonstrating the advantage of our method. Our work opens an exciting line of
research by providing a testbed and strong baselines. Our code is available at
https://github.com/ebennequin/meta-domain-shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1"&gt;Etienne Bennequin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1"&gt;Victor Bouvier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1"&gt;Myriam Tami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1"&gt;Antoine Toubhans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Hudelot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning of Relative Position Regression for One-Shot Object Localization in 3D Medical Images. (arXiv:2012.07043v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07043</id>
        <link href="http://arxiv.org/abs/2012.07043"/>
        <updated>2021-05-26T01:22:10.934Z</updated>
        <summary type="html"><![CDATA[Deep learning networks have shown promising performance for accurate object
localization in medial images, but require large amount of annotated data for
supervised training, which is expensive and expertise burdensome. To address
this problem, we present a one-shot framework for organ and landmark
localization in volumetric medical images, which does not need any annotation
during the training stage and could be employed to locate any landmarks or
organs in test images given a support (reference) image during the inference
stage. Our main idea comes from that tissues and organs from different human
bodies have a similar relative position and context. Therefore, we could
predict the relative positions of their non-local patches, thus locate the
target organ. Our framework is composed of three parts: (1) A projection
network trained to predict the 3D offset between any two patches from the same
volume, where human annotations are not required. In the inference stage, it
takes one given landmark in a reference image as a support patch and predicts
the offset from a random patch to the corresponding landmark in the test
(query) volume. (2) A coarse-to-fine framework contains two projection
networks, providing more accurate localization of the target. (3) Based on the
coarse-to-fine model, we transfer the organ boundingbox (B-box) detection to
locating six extreme points along x, y and z directions in the query volume.
Experiments on multi-organ localization from head-and-neck (HaN) CT volumes
showed that our method acquired competitive performance in real time, which is
more accurate and 10^5 times faster than template matching methods with the
same setting. Code is available: https://github.com/LWHYC/RPR-Loc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1"&gt;Wenhui Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1"&gt;Ran Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Hao Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaoting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guotai Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DTNN: Energy-efficient Inference with Dendrite Tree Inspired Neural Networks for Edge Vision Applications. (arXiv:2105.11848v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11848</id>
        <link href="http://arxiv.org/abs/2105.11848"/>
        <updated>2021-05-26T01:22:10.924Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNN) have achieved remarkable success in computer
vision (CV). However, training and inference of DNN models are both memory and
computation intensive, incurring significant overhead in terms of energy
consumption and silicon area. In particular, inference is much more
cost-sensitive than training because training can be done offline with powerful
platforms, while inference may have to be done on battery powered devices with
constrained form factors, especially for mobile or edge vision applications. In
order to accelerate DNN inference, model quantization was proposed. However
previous works only focus on the quantization rate without considering the
efficiency of operations. In this paper, we propose Dendrite-Tree based Neural
Network (DTNN) for energy-efficient inference with table lookup operations
enabled by activation quantization. In DTNN both costly weight access and
arithmetic computations are eliminated for inference. We conducted experiments
on various kinds of DNN models such as LeNet-5, MobileNet, VGG, and ResNet with
different datasets, including MNIST, Cifar10/Cifar100, SVHN, and ImageNet. DTNN
achieved significant energy saving (19.4X and 64.9X improvement on ResNet-18
and VGG-11 with ImageNet, respectively) with negligible loss of accuracy. To
further validate the effectiveness of DTNN and compare with state-of-the-art
low energy implementation for edge vision, we design and implement DTNN based
MLP image classifiers using off-the-shelf FPGAs. The results show that DTNN on
the FPGA, with higher accuracy, could achieve orders of magnitude better energy
consumption and latency compared with the state-of-the-art low energy
approaches reported that use ASIC chips.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1"&gt;Wai Teng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Matthew Kay Fei Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1"&gt;Chuping Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1"&gt;Weng-Fai Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1"&gt;Rick Goh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoRSAI: A System for Robust Interpretation of CT Scans of COVID-19 Patients Using Deep Learning. (arXiv:2105.11863v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11863</id>
        <link href="http://arxiv.org/abs/2105.11863"/>
        <updated>2021-05-26T01:22:10.924Z</updated>
        <summary type="html"><![CDATA[Analysis of chest CT scans can be used in detecting parts of lungs that are
affected by infectious diseases such as COVID-19.Determining the volume of
lungs affected by lesions is essential for formulating treatment
recommendations and prioritizingpatients by severity of the disease. In this
paper we adopted an approach based on using an ensemble of deep
convolutionalneural networks for segmentation of slices of lung CT scans. Using
our models we are able to segment the lesions, evaluatepatients dynamics,
estimate relative volume of lungs affected by lesions and evaluate the lung
damage stage. Our modelswere trained on data from different medical centers. We
compared predictions of our models with those of six experiencedradiologists
and our segmentation model outperformed most of them. On the task of
classification of disease severity, ourmodel outperformed all the radiologists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Avetisian_M/0/1/0/all/0/1"&gt;Manvel Avetisian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burenko_I/0/1/0/all/0/1"&gt;Ilya Burenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Egorov_K/0/1/0/all/0/1"&gt;Konstantin Egorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kokh_V/0/1/0/all/0/1"&gt;Vladimir Kokh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nesterov_A/0/1/0/all/0/1"&gt;Aleksandr Nesterov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nikolaev_A/0/1/0/all/0/1"&gt;Aleksandr Nikolaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ponomarchuk_A/0/1/0/all/0/1"&gt;Alexander Ponomarchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sokolova_E/0/1/0/all/0/1"&gt;Elena Sokolova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alex Tuzhilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Umerenkov_D/0/1/0/all/0/1"&gt;Dmitry Umerenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Least-Squares ReLU Neural Network (LSNN) Method For Linear Advection-Reaction Equation. (arXiv:2105.11632v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2105.11632</id>
        <link href="http://arxiv.org/abs/2105.11632"/>
        <updated>2021-05-26T01:22:10.922Z</updated>
        <summary type="html"><![CDATA[This paper studies least-squares ReLU neural network method for solving the
linear advection-reaction problem with discontinuous solution. The method is a
discretization of an equivalent least-squares formulation in the set of neural
network functions with the ReLU activation function. The method is capable of
approximating the discontinuous interface of the underlying problem
automatically through the free hyper-planes of the ReLU neural network and,
hence, outperforms mesh-based numerical methods in terms of the number of
degrees of freedom. Numerical results of some benchmark test problems show that
the method can not only approximate the solution with the least number of
parameters, but also avoid the common Gibbs phenomena along the discontinuous
interface. Moreover, a three-layer ReLU neural network is necessary and
sufficient in order to well approximate a discontinuous solution with an
interface in $\mathbb{R}^2$ that is not a straight line.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhiqiang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingshuang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liu_M/0/1/0/all/0/1"&gt;Min Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Perturbed Prox-Preconditioned SPIDER algorithm for EM-based large scale learning. (arXiv:2105.11732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11732</id>
        <link href="http://arxiv.org/abs/2105.11732"/>
        <updated>2021-05-26T01:22:10.916Z</updated>
        <summary type="html"><![CDATA[Incremental Expectation Maximization (EM) algorithms were introduced to
design EM for the large scale learning framework by avoiding the full data set
to be processed at each iteration. Nevertheless, these algorithms all assume
that the conditional expectations of the sufficient statistics are explicit. In
this paper, we propose a novel algorithm named Perturbed Prox-Preconditioned
SPIDER (3P-SPIDER), which builds on the Stochastic Path Integral Differential
EstimatoR EM (SPIDER-EM) algorithm. The 3P-SPIDER algorithm addresses many
intractabilities of the E-step of EM; it also deals with non-smooth
regularization and convex constraint set. Numerical experiments show that
3P-SPIDER outperforms other incremental EM methods and discuss the role of some
design parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fort_G/0/1/0/all/0/1"&gt;Gersende Fort&lt;/a&gt; (IMT), &lt;a href="http://arxiv.org/find/cs/1/au:+Moulines_E/0/1/0/all/0/1"&gt;Eric Moulines&lt;/a&gt; (X-DEP-MATHAPP, XPOP)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Local Kernels Formulation of Mutual Information with Application to Active Post-Seismic Building Damage Inference. (arXiv:2105.11492v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11492</id>
        <link href="http://arxiv.org/abs/2105.11492"/>
        <updated>2021-05-26T01:22:10.900Z</updated>
        <summary type="html"><![CDATA[The abundance of training data is not guaranteed in various supervised
learning applications. One of these situations is the post-earthquake regional
damage assessment of buildings. Querying the damage label of each building
requires a thorough inspection by experts, and thus, is an expensive task. A
practical approach is to sample the most informative buildings in a sequential
learning scheme. Active learning methods recommend the most informative cases
that are able to maximally reduce the generalization error. The information
theoretic measure of mutual information (MI) is one of the most effective
criteria to evaluate the effectiveness of the samples in a pool-based sample
selection scenario. However, the computational complexity of the standard MI
algorithm prevents the utilization of this method on large datasets. A local
kernels strategy was proposed to reduce the computational costs, but the
adaptability of the kernels to the observed labels was not considered in the
original formulation of this strategy. In this article, an adaptive local
kernels methodology is developed that allows for the conformability of the
kernels to the observed output data while enhancing the computational
complexity of the standard MI algorithm. The proposed algorithm is developed to
work on a Gaussian process regression (GPR) framework, where the kernel
hyperparameters are updated after each label query using the maximum likelihood
estimation. In the sequential learning procedure, the updated hyperparameters
can be used in the MI kernel matrices to improve the sample suggestion
performance. The advantages are demonstrated on a simulation of the 2018
Anchorage, AK, earthquake. It is shown that while the proposed algorithm
enables GPR to reach acceptable performance with fewer training data, the
computational demands remain lower than the standard local kernels strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheibani_M/0/1/0/all/0/1"&gt;Mohamadreza Sheibani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_G/0/1/0/all/0/1"&gt;Ge Ou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning and Curriculum Learning in Sokoban. (arXiv:2105.11702v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.11702</id>
        <link href="http://arxiv.org/abs/2105.11702"/>
        <updated>2021-05-26T01:22:10.887Z</updated>
        <summary type="html"><![CDATA[Transfer learning can speed up training in machine learning and is regularly
used in classification tasks. It reuses prior knowledge from other tasks to
pre-train networks for new tasks. In reinforcement learning, learning actions
for a behavior policy that can be applied to new environments is still a
challenge, especially for tasks that involve much planning. Sokoban is a
challenging puzzle game. It has been used widely as a benchmark in
planning-based reinforcement learning. In this paper, we show how prior
knowledge improves learning in Sokoban tasks. We find that reusing feature
representations learned previously can accelerate learning new, more complex,
instances. In effect, we show how curriculum learning, from simple to complex
tasks, works in Sokoban. Furthermore, feature representations learned in
simpler instances are more general, and thus lead to positive transfers towards
more complex tasks, but not vice versa. We have also studied which part of the
knowledge is most important for transfer to succeed, and identify which layers
should be used for pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1"&gt;Mike Preuss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1"&gt;Aske Plaat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reversible Adversarial Attack based on Reversible Image Transformation. (arXiv:1911.02360v7 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.02360</id>
        <link href="http://arxiv.org/abs/1911.02360"/>
        <updated>2021-05-26T01:22:10.854Z</updated>
        <summary type="html"><![CDATA[In order to prevent illegal or unauthorized access of image data such as
human faces and ensure legitimate users can use authorization-protected data,
reversible adversarial attack technique is rise. Reversible adversarial
examples (RAE) get both attack capability and reversibility at the same time.
However, the existing technique can not meet application requirements because
of serious distortion and failure of image recovery when adversarial
perturbations get strong. In this paper, we take advantage of Reversible Image
Transformation technique to generate RAE and achieve reversible adversarial
attack. Experimental results show that proposed RAE generation scheme can
ensure imperceptible image distortion and the original image can be
reconstructed error-free. What's more, both the attack ability and the image
quality are not limited by the perturbation amplitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zhaoxia Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperspectral Image Denoising with Log-Based Robust PCA. (arXiv:2105.11927v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11927</id>
        <link href="http://arxiv.org/abs/2105.11927"/>
        <updated>2021-05-26T01:22:10.840Z</updated>
        <summary type="html"><![CDATA[It is a challenging task to remove heavy and mixed types of noise from
Hyperspectral images (HSIs). In this paper, we propose a novel nonconvex
approach to RPCA for HSI denoising, which adopts the log-determinant rank
approximation and a novel $\ell_{2,\log}$ norm, to restrict the low-rank or
column-wise sparse properties for the component matrices, respectively.For the
$\ell_{2,\log}$-regularized shrinkage problem, we develop an efficient,
closed-form solution, which is named $\ell_{2,\log}$-shrinkage operator, which
can be generally used in other problems. Extensive experiments on both
simulated and real HSIs demonstrate the effectiveness of the proposed method in
denoising HSIs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yongyong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1"&gt;Qiang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chong Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Graph Representation Learning via Topology Transformations. (arXiv:2105.11689v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11689</id>
        <link href="http://arxiv.org/abs/2105.11689"/>
        <updated>2021-05-26T01:22:10.790Z</updated>
        <summary type="html"><![CDATA[We present the Topology Transformation Equivariant Representation learning, a
general paradigm of self-supervised learning for node representations of graph
data to enable the wide applicability of Graph Convolutional Neural Networks
(GCNNs). We formalize the proposed model from an information-theoretic
perspective, by maximizing the mutual information between topology
transformations and node representations before and after the transformations.
We derive that maximizing such mutual information can be relaxed to minimizing
the cross entropy between the applied topology transformation and its
estimation from node representations. In particular, we seek to sample a subset
of node pairs from the original graph and flip the edge connectivity between
each pair to transform the graph topology. Then, we self-train a representation
encoder to learn node representations by reconstructing the topology
transformations from the feature representations of the original and
transformed graphs. In experiments, we apply the proposed model to the
downstream node and graph classification tasks, and results show that the
proposed method outperforms the state-of-the-art unsupervised approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1"&gt;Guo-Jun Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Asymmetric Actor-Critic for Partially Observable Reinforcement Learning. (arXiv:2105.11674v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11674</id>
        <link href="http://arxiv.org/abs/2105.11674"/>
        <updated>2021-05-26T01:22:10.788Z</updated>
        <summary type="html"><![CDATA[In partially observable reinforcement learning, offline training gives access
to latent information which is not available during online training and/or
execution, such as the system state. Asymmetric actor-critic methods exploit
such information by training a history-based policy via a state-based critic.
However, many asymmetric methods lack theoretical foundation, and are only
evaluated on limited domains. We examine the theory of asymmetric actor-critic
methods which use state-based critics, and expose fundamental issues which
undermine the validity of a common variant, and its ability to address high
partial observability. We propose an unbiased asymmetric actor-critic variant
which is able to exploit state information while remaining theoretically sound,
maintaining the validity of the policy gradient theorem, and introducing no
bias and relatively low variance into the training process. An empirical
evaluation performed on domains which exhibit significant partial observability
confirms our analysis, and shows the unbiased asymmetric actor-critic converges
to better policies and/or faster than symmetric actor-critic and standard
asymmetric actor-critic baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baisero_A/0/1/0/all/0/1"&gt;Andrea Baisero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1"&gt;Christopher Amato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Embedding Search for Quantum Machine Learning. (arXiv:2105.11853v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2105.11853</id>
        <link href="http://arxiv.org/abs/2105.11853"/>
        <updated>2021-05-26T01:22:10.761Z</updated>
        <summary type="html"><![CDATA[This paper introduces a novel quantum embedding search algorithm (QES,
pronounced as "quest"), enabling search for optimal quantum embedding design
for a specific dataset of interest. First, we establish the connection between
the structures of quantum embedding and the representations of directed
multi-graphs, enabling a well-defined search space. Second, we instigate the
entanglement level to reduce the cardinality of the search space to a feasible
size for practical implementations. Finally, we mitigate the cost of evaluating
the true loss function by using surrogate models via sequential model-based
optimization. We demonstrate the feasibility of our proposed approach on
synthesis and Iris datasets, which empirically shows that found quantum
embedding architecture by QES outperforms manual designs whereas achieving
comparable performance to classical machine learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Nam Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kwang-Chen Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search. (arXiv:2105.11871v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11871</id>
        <link href="http://arxiv.org/abs/2105.11871"/>
        <updated>2021-05-26T01:22:10.751Z</updated>
        <summary type="html"><![CDATA[Recent breakthroughs of Neural Architecture Search (NAS) extend the field's
research scope towards a broader range of vision tasks and more diversified
search spaces. While existing NAS methods mostly design architectures on a
single task, algorithms that look beyond single-task search are surging to
pursue a more efficient and universal solution across various tasks. Many of
them leverage transfer learning and seek to preserve, reuse, and refine network
design knowledge to achieve higher efficiency in future tasks. However, the
enormous computational cost and experiment complexity of cross-task NAS are
imposing barriers for valuable research in this direction. Existing NAS
benchmarks all focus on one type of vision task, i.e., classification. In this
work, we propose TransNAS-Bench-101, a benchmark dataset containing network
performance across seven tasks, covering classification, regression,
pixel-level prediction, and self-supervised tasks. This diversity provides
opportunities to transfer NAS methods among tasks and allows for more complex
transfer schemes to evolve. We explore two fundamentally different types of
search space: cell-level search space and macro-level search space. With 7,352
backbones evaluated on seven tasks, 51,464 trained models with detailed
training information are provided. With TransNAS-Bench-101, we hope to
encourage the advent of exceptional NAS algorithms that raise cross-task search
efficiency and generalizability to the next level. Our dataset file will be
available at Mindspore, VEGA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1"&gt;Yawen Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zewei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experimenting with Knowledge Distillation techniques for performing Brain Tumor Segmentation. (arXiv:2105.11486v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11486</id>
        <link href="http://arxiv.org/abs/2105.11486"/>
        <updated>2021-05-26T01:22:10.725Z</updated>
        <summary type="html"><![CDATA[Multi-modal magnetic resonance imaging (MRI) is a crucial method for
analyzing the human brain. It is usually used for diagnosing diseases and for
making valuable decisions regarding the treatments - for instance, checking for
gliomas in the human brain. With varying degrees of severity and detection,
properly diagnosing gliomas is one of the most daunting and significant
analysis tasks in modern-day medicine. Our primary focus is on working with
different approaches to perform the segmentation of brain tumors in multimodal
MRI scans. Now, the quantity, variability of the data used for training has
always been considered to be crucial for developing excellent models. Hence, we
also want to experiment with Knowledge Distillation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nalwade_A/0/1/0/all/0/1"&gt;Ashwin Nalwade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kisa_J/0/1/0/all/0/1"&gt;Jackie Kisa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Descriptive Clustering. (arXiv:2105.11549v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11549</id>
        <link href="http://arxiv.org/abs/2105.11549"/>
        <updated>2021-05-26T01:22:10.722Z</updated>
        <summary type="html"><![CDATA[Recent work on explainable clustering allows describing clusters when the
features are interpretable. However, much modern machine learning focuses on
complex data such as images, text, and graphs where deep learning is used but
the raw features of data are not interpretable. This paper explores a novel
setting for performing clustering on complex data while simultaneously
generating explanations using interpretable tags. We propose deep descriptive
clustering that performs sub-symbolic representation learning on complex data
while generating explanations based on symbolic data. We form good clusters by
maximizing the mutual information between empirical distribution on the inputs
and the induced clustering labels for clustering objectives. We generate
explanations by solving an integer linear programming that generates concise
and orthogonal descriptions for each cluster. Finally, we allow the explanation
to inform better clustering by proposing a novel pairwise loss with
self-generated constraints to maximize the clustering and explanation module's
consistency. Experimental results on public data demonstrate that our model
outperforms competitive baselines in clustering performance while offering
high-quality cluster-level explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongjing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davidson_I/0/1/0/all/0/1"&gt;Ian Davidson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Recognition in Horses with Convolutional Neural Networks. (arXiv:2105.11953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11953</id>
        <link href="http://arxiv.org/abs/2105.11953"/>
        <updated>2021-05-26T01:22:10.692Z</updated>
        <summary type="html"><![CDATA[Creating intelligent systems capable of recognizing emotions is a difficult
task, especially when looking at emotions in animals. This paper describes the
process of designing a "proof of concept" system to recognize emotions in
horses. This system is formed by two elements, a detector and a model. The
detector is a faster region-based convolutional neural network that detects
horses in an image. The second one, the model, is a convolutional neural
network that predicts the emotion of those horses. These two models were
trained with multiple images of horses until they achieved high accuracy in
their tasks, creating therefore the desired system. 400 images of horses were
used to train both the detector and the model while 80 were used to validate
the system. Once the two components were validated they were combined into a
testable system that would detect equine emotions based on established
behavioral ethograms indicating emotional affect through head, neck, ear,
muzzle, and eye position. The system showed an accuracy of between 69% and 74%
on the validation set, demonstrating that it is possible to predict emotions in
animals using autonomous intelligent systems. It is a first "proof of concept"
approach that can be enhanced in many ways. Such a system has multiple
applications including further studies in the growing field of animal emotions
as well as in the veterinary field to determine the physical welfare of horses
or other livestock.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Corujo_L/0/1/0/all/0/1"&gt;Luis A. Corujo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1"&gt;Peter A. Gloor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kieson_E/0/1/0/all/0/1"&gt;Emily Kieson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised learning of images with strong rotational disorder: assembling nanoparticle libraries. (arXiv:2105.11475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11475</id>
        <link href="http://arxiv.org/abs/2105.11475"/>
        <updated>2021-05-26T01:22:10.680Z</updated>
        <summary type="html"><![CDATA[The proliferation of optical, electron, and scanning probe microscopies gives
rise to large volumes of imaging data of objects as diversified as cells,
bacteria, pollen, to nanoparticles and atoms and molecules. In most cases, the
experimental data streams contain images having arbitrary rotations and
translations within the image. At the same time, for many cases, small amounts
of labeled data are available in the form of prior published results, image
collections, and catalogs, or even theoretical models. Here we develop an
approach that allows generalizing from a small subset of labeled data with a
weak orientational disorder to a large unlabeled dataset with a much stronger
orientational (and positional) disorder, i.e., it performs a classification of
image data given a small number of examples even in the presence of a
distribution shift between the labeled and unlabeled parts. This approach is
based on the semi-supervised rotationally invariant variational autoencoder
(ss-rVAE) model consisting of the encoder-decoder "block" that learns a
rotationally (and translationally) invariant continuous latent representation
of data and a classifier that encodes data into a finite number of discrete
classes. The classifier part of the trained ss-rVAE inherits the rotational
(and translational) invariances and can be deployed independently of the other
parts of the model. The performance of the ss-rVAE is illustrated using the
synthetic data sets with known factors of variation. We further demonstrate its
application for experimental data sets of nanoparticles, creating nanoparticle
libraries and disentangling the representations defining the physical factors
of variation in the data. The code reproducing the results is available at
https://github.com/ziatdinovmax/Semi-Supervised-VAE-nanoparticles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziatdinov_M/0/1/0/all/0/1"&gt;Maxim Ziatdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaman_M/0/1/0/all/0/1"&gt;Muammer Yusuf Yaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongtao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ginger_D/0/1/0/all/0/1"&gt;David Ginger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinin_S/0/1/0/all/0/1"&gt;Sergei V. Kalinin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TRACE: A Differentiable Approach to Line-level Stroke Recovery for Offline Handwritten Text. (arXiv:2105.11559v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11559</id>
        <link href="http://arxiv.org/abs/2105.11559"/>
        <updated>2021-05-26T01:22:10.676Z</updated>
        <summary type="html"><![CDATA[Stroke order and velocity are helpful features in the fields of signature
verification, handwriting recognition, and handwriting synthesis. Recovering
these features from offline handwritten text is a challenging and well-studied
problem. We propose a new model called TRACE (Trajectory Recovery by an
Adaptively-trained Convolutional Encoder). TRACE is a differentiable approach
that uses a convolutional recurrent neural network (CRNN) to infer temporal
stroke information from long lines of offline handwritten text with many
characters and dynamic time warping (DTW) to align predictions and ground truth
points. TRACE is perhaps the first system to be trained end-to-end on entire
lines of text of arbitrary width and does not require the use of dynamic
exemplars. Moreover, the system does not require images to undergo any
pre-processing, nor do the predictions require any post-processing.
Consequently, the recovered trajectory is differentiable and can be used as a
loss function for other tasks, including synthesizing offline handwritten text.

We demonstrate that temporal stroke information recovered by TRACE from
offline data can be used for handwriting synthesis and establish the first
benchmarks for a stroke trajectory recovery system trained on the IAM online
handwriting dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Archibald_T/0/1/0/all/0/1"&gt;Taylor Archibald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poggemann_M/0/1/0/all/0/1"&gt;Mason Poggemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Aaron Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_T/0/1/0/all/0/1"&gt;Tony Martinez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Organized Variational Autoencoders (Self-VAE) for Learned Image Compression. (arXiv:2105.12107v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.12107</id>
        <link href="http://arxiv.org/abs/2105.12107"/>
        <updated>2021-05-26T01:22:10.659Z</updated>
        <summary type="html"><![CDATA[In end-to-end optimized learned image compression, it is standard practice to
use a convolutional variational autoencoder with generalized divisive
normalization (GDN) to transform images into a latent space. Recently,
Operational Neural Networks (ONNs) that learn the best non-linearity from a set
of alternatives, and their self-organized variants, Self-ONNs, that approximate
any non-linearity via Taylor series have been proposed to address the
limitations of convolutional layers and a fixed nonlinear activation. In this
paper, we propose to replace the convolutional and GDN layers in the
variational autoencoder with self-organized operational layers, and propose a
novel self-organized variational autoencoder (Self-VAE) architecture that
benefits from stronger non-linearity. The experimental results demonstrate that
the proposed Self-VAE yields improvements in both rate-distortion performance
and perceptual image quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yilmaz_M/0/1/0/all/0/1"&gt;M. Ak&amp;#x131;n Y&amp;#x131;lmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Keles_O/0/1/0/all/0/1"&gt;Onur Kele&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guven_H/0/1/0/all/0/1"&gt;Hilal G&amp;#xfc;ven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tekalp_A/0/1/0/all/0/1"&gt;A. Murat Tekalp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Malik_J/0/1/0/all/0/1"&gt;Junaid Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan K&amp;#x131;ranyaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAS-MEF: Multi-exposure image fusion based on principal component analysis, adaptive well-exposedness and saliency map. (arXiv:2105.11809v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11809</id>
        <link href="http://arxiv.org/abs/2105.11809"/>
        <updated>2021-05-26T01:22:10.657Z</updated>
        <summary type="html"><![CDATA[High dynamic range (HDR) imaging enables to immortalize natural scenes
similar to the way that they are perceived by human observers. With regular low
dynamic range (LDR) capture/display devices, significant details may not be
preserved in images due to the huge dynamic range of natural scenes. To
minimize the information loss and produce high quality HDR-like images for LDR
screens, this study proposes an efficient multi-exposure fusion (MEF) approach
with a simple yet effective weight extraction method relying on principal
component analysis, adaptive well-exposedness and saliency maps. These weight
maps are later refined through a guided filter and the fusion is carried out by
employing a pyramidal decomposition. Experimental comparisons with existing
techniques demonstrate that the proposed method produces very strong
statistical and visual results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karakaya_D/0/1/0/all/0/1"&gt;Diclehan Karakaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulucan_O/0/1/0/all/0/1"&gt;Oguzhan Ulucan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turkan_M/0/1/0/all/0/1"&gt;Mehmet Turkan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Small and large scale critical infrastructures detection based on deep learning using high resolution orthogonal images. (arXiv:2105.11844v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11844</id>
        <link href="http://arxiv.org/abs/2105.11844"/>
        <updated>2021-05-26T01:22:10.650Z</updated>
        <summary type="html"><![CDATA[The detection of critical infrastructures is of high importance in several
fields such as security, anomaly detection, land use planning and land use
change detection. However, critical infrastructures detection in aerial and
satellite images is still a challenge as each one has completely different size
and requires different spacial resolution to be identified correctly.
Heretofore, there are no special datasets for training critical infrastructures
detectors. This paper presents a smart dataset as well as a
resolution-independent critical infrastructure detection system. In particular,
guided by the performance of the detection model, we built a dataset organized
into two scales, small and large scale, and designed a two-stage deep learning
detection of different scale critical infrastructures (DetDSCI) methodology in
ortho-images. DetDSCI methodology first determines the input image zoom level
using a classification model, then analyses the input image with the
appropriate scale detection model. Our experiments show that DetDSCI
methodology achieves up to 37,53% F1 improvement with respect to the baseline
detector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Francisco_P/0/1/0/all/0/1"&gt;P&amp;#xe9;rez-Hern&amp;#xe1;ndez Francisco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jose_R/0/1/0/all/0/1"&gt;Rodr&amp;#xed;guez-Ortega Jos&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yassir_B/0/1/0/all/0/1"&gt;Benhammou Yassir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francisco_H/0/1/0/all/0/1"&gt;Herrera Francisco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siham_T/0/1/0/all/0/1"&gt;Tabik Siham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Few-shot Learning with Weakly-supervised Object Localization. (arXiv:2105.11715v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11715</id>
        <link href="http://arxiv.org/abs/2105.11715"/>
        <updated>2021-05-26T01:22:10.617Z</updated>
        <summary type="html"><![CDATA[Few-shot learning often involves metric learning-based classifiers, which
predict the image label by comparing the distance between the extracted feature
vector and class representations. However, applying global pooling in the
backend of the feature extractor may not produce an embedding that correctly
focuses on the class object. In this work, we propose a novel framework that
generates class representations by extracting features from class-relevant
regions of the images. Given only a few exemplary images with image-level
labels, our framework first localizes the class objects by spatially
decomposing the similarity between the images and their class prototypes. Then,
enhanced class representations are achieved from the localization results. We
also propose a loss function to enhance distinctions of the refined features.
Our method outperforms the baseline few-shot model in miniImageNet and
tieredImageNet benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koo_I/0/1/0/all/0/1"&gt;Inyong Koo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1"&gt;Minki Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Changick Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Generative Prior with Latent Space Sparsity Constraints. (arXiv:2105.11956v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11956</id>
        <link href="http://arxiv.org/abs/2105.11956"/>
        <updated>2021-05-26T01:22:10.617Z</updated>
        <summary type="html"><![CDATA[We address the problem of compressed sensing using a deep generative prior
model and consider both linear and learned nonlinear sensing mechanisms, where
the nonlinear one involves either a fully connected neural network or a
convolutional neural network. Recently, it has been argued that the
distribution of natural images do not lie in a single manifold but rather lie
in a union of several submanifolds. We propose a sparsity-driven latent space
sampling (SDLSS) framework and develop a proximal meta-learning (PML) algorithm
to enforce sparsity in the latent space. SDLSS allows the range-space of the
generator to be considered as a union-of-submanifolds. We also derive the
sample complexity bounds within the SDLSS framework for the linear measurement
model. The results demonstrate that for a higher degree of compression, the
SDLSS method is more efficient than the state-of-the-art method. We first
consider a comparison between linear and nonlinear sensing mechanisms on
Fashion-MNIST dataset and show that the learned nonlinear version is superior
to the linear one. Subsequent comparisons with the deep compressive sensing
(DCS) framework proposed in the literature are reported. We also consider the
effect of the dimension of the latent space and the sparsity factor in
validating the SDLSS framework. Performance quantification is carried out by
employing three objective metrics: peak signal-to-noise ratio (PSNR),
structural similarity index metric (SSIM), and reconstruction error (RE).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killedar_V/0/1/0/all/0/1"&gt;Vinayak Killedar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokala_P/0/1/0/all/0/1"&gt;Praveen Kumar Pokala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seelamantula_C/0/1/0/all/0/1"&gt;Chandra Sekhar Seelamantula&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review on Indoor RGB-D Semantic Segmentation with Deep Convolutional Neural Networks. (arXiv:2105.11925v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11925</id>
        <link href="http://arxiv.org/abs/2105.11925"/>
        <updated>2021-05-26T01:22:10.610Z</updated>
        <summary type="html"><![CDATA[Many research works focus on leveraging the complementary geometric
information of indoor depth sensors in vision tasks performed by deep
convolutional neural networks, notably semantic segmentation. These works deal
with a specific vision task known as "RGB-D Indoor Semantic Segmentation". The
challenges and resulting solutions of this task differ from its standard RGB
counterpart. This results in a new active research topic. The objective of this
paper is to introduce the field of Deep Convolutional Neural Networks for RGB-D
Indoor Semantic Segmentation. This review presents the most popular public
datasets, proposes a categorization of the strategies employed by recent
contributions, evaluates the performance of the current state-of-the-art, and
discusses the remaining challenges and promising directions for future works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barchid_S/0/1/0/all/0/1"&gt;Sami Barchid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mennesson_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Mennesson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1"&gt;Chaabane Dj&amp;#xe9;raba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attack Driven Data Augmentation for Accurate And Robust Medical Image Segmentation. (arXiv:2105.12106v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.12106</id>
        <link href="http://arxiv.org/abs/2105.12106"/>
        <updated>2021-05-26T01:22:10.602Z</updated>
        <summary type="html"><![CDATA[Segmentation is considered to be a very crucial task in medical image
analysis. This task has been easier since deep learning models have taken over
with its high performing behavior. However, deep learning models dependency on
large data proves it to be an obstacle in medical image analysis because of
insufficient data samples. Several data augmentation techniques have been used
to mitigate this problem. We propose a new augmentation method by introducing
adversarial learning attack techniques, specifically Fast Gradient Sign Method
(FGSM). Furthermore, We have also introduced the concept of Inverse FGSM
(InvFGSM), which works in the opposite manner of FGSM for the data
augmentation. This two approaches worked together to improve the segmentation
accuracy, as well as helped the model to gain robustness against adversarial
attacks. The overall analysis of experiments indicates a novel use of
adversarial machine learning along with robustness enhancement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pervin_M/0/1/0/all/0/1"&gt;Mst. Tasnim Pervin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_L/0/1/0/all/0/1"&gt;Linmi Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huq_A/0/1/0/all/0/1"&gt;Aminul Huq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_Z/0/1/0/all/0/1"&gt;Zuoxiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huo_L/0/1/0/all/0/1"&gt;Li Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Making Few-Shot Learning Stronger. (arXiv:2105.11904v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11904</id>
        <link href="http://arxiv.org/abs/2105.11904"/>
        <updated>2021-05-26T01:22:10.594Z</updated>
        <summary type="html"><![CDATA[Few-shot learning has been proposed and rapidly emerging as a viable means
for completing various tasks. Many few-shot models have been widely used for
relation learning tasks. However, each of these models has a shortage of
capturing a certain aspect of semantic features, for example, CNN on long-range
dependencies part, Transformer on local features. It is difficult for a single
model to adapt to various relation learning, which results in the high variance
problem. Ensemble strategy could be competitive on improving the accuracy of
few-shot relation extraction and mitigating high variance risks. This paper
explores an ensemble approach to reduce the variance and introduces fine-tuning
and feature attention strategies to calibrate relation-level features. Results
on several few-shot relation learning tasks show that our model significantly
outperforms the previous state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qing Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongbin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1"&gt;Wen Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1"&gt;Zhihua Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SRH-Net: Stacked Recurrent Hourglass Network for Stereo Matching. (arXiv:2105.11587v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11587</id>
        <link href="http://arxiv.org/abs/2105.11587"/>
        <updated>2021-05-26T01:22:10.588Z</updated>
        <summary type="html"><![CDATA[The cost aggregation strategy shows a crucial role in learning-based stereo
matching tasks, where 3D convolutional filters obtain state of the art but
require intensive computation resources, while 2D operations need less GPU
memory but are sensitive to domain shift. In this paper, we decouple the 4D
cubic cost volume used by 3D convolutional filters into sequential cost maps
along the direction of disparity instead of dealing with it at once by
exploiting a recurrent cost aggregation strategy. Furthermore, a novel
recurrent module, Stacked Recurrent Hourglass (SRH), is proposed to process
each cost map. Our hourglass network is constructed based on Gated Recurrent
Units (GRUs) and down/upsampling layers, which provides GRUs larger receptive
fields. Then two hourglass networks are stacked together, while multi-scale
information is processed by skip connections to enhance the performance of the
pipeline in textureless areas. The proposed architecture is implemented in an
end-to-end pipeline and evaluated on public datasets, which reduces GPU memory
consumption by up to 56.1\% compared with PSMNet using stacked hourglass 3D
CNNs without the degradation of accuracy. Then, we further demonstrate the
scalability of the proposed method on several high-resolution pairs, while
previously learned approaches often fail due to the memory constraint. The code
is released at \url{https://github.com/hongzhidu/SRHNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1"&gt;Hongzhi Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yanbiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jigui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoRSAI: A System for Robust Interpretation of CT Scans of COVID-19 Patients Using Deep Learning. (arXiv:2105.11863v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11863</id>
        <link href="http://arxiv.org/abs/2105.11863"/>
        <updated>2021-05-26T01:22:10.573Z</updated>
        <summary type="html"><![CDATA[Analysis of chest CT scans can be used in detecting parts of lungs that are
affected by infectious diseases such as COVID-19.Determining the volume of
lungs affected by lesions is essential for formulating treatment
recommendations and prioritizingpatients by severity of the disease. In this
paper we adopted an approach based on using an ensemble of deep
convolutionalneural networks for segmentation of slices of lung CT scans. Using
our models we are able to segment the lesions, evaluatepatients dynamics,
estimate relative volume of lungs affected by lesions and evaluate the lung
damage stage. Our modelswere trained on data from different medical centers. We
compared predictions of our models with those of six experiencedradiologists
and our segmentation model outperformed most of them. On the task of
classification of disease severity, ourmodel outperformed all the radiologists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Avetisian_M/0/1/0/all/0/1"&gt;Manvel Avetisian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burenko_I/0/1/0/all/0/1"&gt;Ilya Burenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Egorov_K/0/1/0/all/0/1"&gt;Konstantin Egorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kokh_V/0/1/0/all/0/1"&gt;Vladimir Kokh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nesterov_A/0/1/0/all/0/1"&gt;Aleksandr Nesterov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nikolaev_A/0/1/0/all/0/1"&gt;Aleksandr Nikolaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ponomarchuk_A/0/1/0/all/0/1"&gt;Alexander Ponomarchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sokolova_E/0/1/0/all/0/1"&gt;Elena Sokolova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alex Tuzhilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Umerenkov_D/0/1/0/all/0/1"&gt;Dmitry Umerenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of GraphSum's Attention Weights to Improve the Explainability of Multi-Document Summarization. (arXiv:2105.11908v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11908</id>
        <link href="http://arxiv.org/abs/2105.11908"/>
        <updated>2021-05-26T01:22:10.567Z</updated>
        <summary type="html"><![CDATA[Modern multi-document summarization (MDS) methods are based on transformer
architectures. They generate state of the art summaries, but lack
explainability. We focus on graph-based transformer models for MDS as they
gained recent popularity. We aim to improve the explainability of the
graph-based MDS by analyzing their attention weights. In a graph-based MDS such
as GraphSum, vertices represent the textual units, while the edges form some
similarity graph over the units. We compare GraphSum's performance utilizing
different textual units, i. e., sentences versus paragraphs, on two news
benchmark datasets, namely WikiSum and MultiNews. Our experiments show that
paragraph-level representations provide the best summarization performance.
Thus, we subsequently focus oAnalysisn analyzing the paragraph-level attention
weights of GraphSum's multi-heads and decoding layers in order to improve the
explainability of a transformer-based MDS model. As a reference metric, we
calculate the ROUGE scores between the input paragraphs and each sentence in
the generated summary, which indicate source origin information via text
similarity. We observe a high correlation between the attention weights and
this reference metric, especially on the the later decoding layers of the
transformer architecture. Finally, we investigate if the generated summaries
follow a pattern of positional bias by extracting which paragraph provided the
most information for each generated summary. Our results show that there is a
high correlation between the position in the summary and the source origin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hickmann_M/0/1/0/all/0/1"&gt;M. Lautaro Hickmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wurzberger_F/0/1/0/all/0/1"&gt;Fabian Wurzberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoxhalli_M/0/1/0/all/0/1"&gt;Megi Hoxhalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lochner_A/0/1/0/all/0/1"&gt;Arne Lochner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tollich_J/0/1/0/all/0/1"&gt;Jessica T&amp;#xf6;llich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1"&gt;Ansgar Scherp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Regression Activation Maps For Lesion Segmentation in CT scans of COVID-19 patients. (arXiv:2105.11748v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11748</id>
        <link href="http://arxiv.org/abs/2105.11748"/>
        <updated>2021-05-26T01:22:10.561Z</updated>
        <summary type="html"><![CDATA[Automatic lesion segmentation on thoracic CT enables rapid quantitative
analysis of lung involvement in COVID- 19 infections. Obtaining voxel-level
annotations for training segmentation networks is prohibitively expensive.
Therefore we propose a weakly-supervised segmentation method based on dense
regression activation maps (dRAM). Most advanced weakly supervised segmentation
approaches exploit class activation maps (CAMs) to localize objects generated
from high-level semantic features at a coarse resolution. As a result, CAMs
provide coarse outlines that do not align precisely with the object
segmentations. Instead, we exploit dense features from a segmentation network
to compute dense regression activation maps (dRAMs) for preserving local
details. During training, dRAMs are pooled lobe-wise to regress the per-lobe
lesion percentage. In such a way, the network achieves additional information
regarding the lesion quantification in comparison with the classification
approach. Furthermore, we refine dRAMs based on an attention module and dense
conditional random field trained together with the main regression task. The
refined dRAMs are served as the pseudo labels for training a final segmentation
network. When evaluated on 69 CT scans, our method substantially improves the
intersection over union from 0.335 in the CAM-based weakly supervised
segmentation method to 0.495.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weiyi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jacobs_C/0/1/0/all/0/1"&gt;Colin Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1"&gt;Bram van Ginneken&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-view 3D Reconstruction of a Texture-less Smooth Surface of Unknown Generic Reflectance. (arXiv:2105.11599v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11599</id>
        <link href="http://arxiv.org/abs/2105.11599"/>
        <updated>2021-05-26T01:22:10.556Z</updated>
        <summary type="html"><![CDATA[Recovering the 3D geometry of a purely texture-less object with generally
unknown surface reflectance (e.g. non-Lambertian) is regarded as a challenging
task in multi-view reconstruction. The major obstacle revolves around
establishing cross-view correspondences where photometric constancy is
violated. This paper proposes a simple and practical solution to overcome this
challenge based on a co-located camera-light scanner device. Unlike existing
solutions, we do not explicitly solve for correspondence. Instead, we argue the
problem is generally well-posed by multi-view geometrical and photometric
constraints, and can be solved from a small number of input views. We formulate
the reconstruction task as a joint energy minimization over the surface
geometry and reflectance. Despite this energy is highly non-convex, we develop
an optimization algorithm that robustly recovers globally optimal shape and
reflectance even from a random initialization. Extensive experiments on both
simulated and real data have validated our method, and possible future
extensions are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Ziang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongdong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1"&gt;Yuta Asano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yinqiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1"&gt;Imari Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applying physics-based loss functions to neural networks for improved generalizability in mechanics problems. (arXiv:2105.00075v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00075</id>
        <link href="http://arxiv.org/abs/2105.00075"/>
        <updated>2021-05-26T01:22:10.550Z</updated>
        <summary type="html"><![CDATA[Physics-Informed Machine Learning (PIML) has gained momentum in the last 5
years with scientists and researchers aiming to utilize the benefits afforded
by advances in machine learning, particularly in deep learning. With large
scientific data sets with rich spatio-temporal data and high-performance
computing providing large amounts of data to be inferred and interpreted, the
task of PIML is to ensure that these predictions, categorizations, and
inferences are enforced by, and conform to the limits imposed by physical laws.
In this work a new approach to utilizing PIML is discussed that deals with the
use of physics-based loss functions. While typical usage of physical equations
in the loss function requires complex layers of derivatives and other functions
to ensure that the known governing equation is satisfied, here we show that a
similar level of enforcement can be found by implementing more simpler loss
functions on specific kinds of output data. The generalizability that this
approach affords is shown using examples of simple mechanical models that can
be thought of as sufficiently simplified surrogate models for a wide class of
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Raymond_S/0/1/0/all/0/1"&gt;Samuel J. Raymond&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Camarillo_D/0/1/0/all/0/1"&gt;David B. Camarillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HIN-RNN: A Graph Representation Learning Neural Network for Fraudster Group Detection With No Handcrafted Features. (arXiv:2105.11602v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11602</id>
        <link href="http://arxiv.org/abs/2105.11602"/>
        <updated>2021-05-26T01:22:10.534Z</updated>
        <summary type="html"><![CDATA[Social reviews are indispensable resources for modern consumers' decision
making. For financial gain, companies pay fraudsters preferably in groups to
demote or promote products and services since consumers are more likely to be
misled by a large number of similar reviews from groups. Recent approaches on
fraudster group detection employed handcrafted features of group behaviors
without considering the semantic relation between reviews from the reviewers in
a group. In this paper, we propose the first neural approach, HIN-RNN, a
Heterogeneous Information Network (HIN) Compatible RNN for fraudster group
detection that requires no handcrafted features. HIN-RNN provides a unifying
architecture for representation learning of each reviewer, with the initial
vector as the sum of word embeddings of all review text written by the same
reviewer, concatenated by the ratio of negative reviews. Given a co-review
network representing reviewers who have reviewed the same items with the same
ratings and the reviewers' vector representation, a collaboration matrix is
acquired through HIN-RNN training. The proposed approach is confirmed to be
effective with marked improvement over state-of-the-art approaches on both the
Yelp (22% and 12% in terms of recall and F1-value, respectively) and Amazon (4%
and 2% in terms of recall and F1-value, respectively) datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shehnepoor_S/0/1/0/all/0/1"&gt;Saeedreza Shehnepoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Togneri_R/0/1/0/all/0/1"&gt;Roberto Togneri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Compact Single Image Super-Resolution via Contrastive Self-distillation. (arXiv:2105.11683v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11683</id>
        <link href="http://arxiv.org/abs/2105.11683"/>
        <updated>2021-05-26T01:22:10.529Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) are highly successful for
super-resolution (SR) but often require sophisticated architectures with heavy
memory cost and computational overhead, significantly restricts their practical
deployments on resource-limited devices. In this paper, we proposed a novel
contrastive self-distillation (CSD) framework to simultaneously compress and
accelerate various off-the-shelf SR models. In particular, a channel-splitting
super-resolution network can first be constructed from a target teacher network
as a compact student network. Then, we propose a novel contrastive loss to
improve the quality of SR images and PSNR/SSIM via explicit knowledge transfer.
Extensive experiments demonstrate that the proposed CSD scheme effectively
compresses and accelerates several standard SR models such as EDSR, RCAN and
CARN. Code is available at https://github.com/Booooooooooo/CSD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shaohui Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1"&gt;Yanyun Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haiyan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizhong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for Key Information Extraction from Documents. (arXiv:2105.11672v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11672</id>
        <link href="http://arxiv.org/abs/2105.11672"/>
        <updated>2021-05-26T01:22:10.523Z</updated>
        <summary type="html"><![CDATA[Recent grid-based document representations like BERTgrid allow the
simultaneous encoding of the textual and layout information of a document in a
2D feature map so that state-of-the-art image segmentation and/or object
detection models can be straightforwardly leveraged to extract key information
from documents. However, such methods have not achieved comparable performance
to state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK
yet. In this paper, we propose a new multi-modal backbone network by
concatenating a BERTgrid to an intermediate layer of a CNN model, where the
input of CNN is a document image and the BERTgrid is a grid of word embeddings,
to generate a more powerful grid-based document representation, named
ViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal
backbone network are trained jointly. Our experimental results demonstrate that
this joint training strategy improves significantly the representation ability
of ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction
approach has achieved state-of-the-art performance on real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weihong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qifang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhuoyao Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1"&gt;Kai Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1"&gt;Qin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1"&gt;Qiang Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Adversarial Learning via Sparsifying Front Ends. (arXiv:1810.10625v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.10625</id>
        <link href="http://arxiv.org/abs/1810.10625"/>
        <updated>2021-05-26T01:22:10.517Z</updated>
        <summary type="html"><![CDATA[It is by now well-known that small adversarial perturbations can induce
classification errors in deep neural networks. In this paper, we take a
bottom-up signal processing perspective to this problem and show that a
systematic exploitation of sparsity in natural data is a promising tool for
defense. For linear classifiers, we show that a sparsifying front end is
provably effective against $\ell_{\infty}$-bounded attacks, reducing output
distortion due to the attack by a factor of roughly $K/N$ where $N$ is the data
dimension and $K$ is the sparsity level. We then extend this concept to deep
networks, showing that a "locally linear" model can be used to develop a
theoretical foundation for crafting attacks and defenses. We also devise
attacks based on the locally linear model that outperform the well-known FGSM
attack. We supplement our theoretical results with experiments on the MNIST and
CIFAR-10 datasets, showing the efficacy of the proposed sparsity-based defense
schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gopalakrishnan_S/0/1/0/all/0/1"&gt;Soorya Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marzi_Z/0/1/0/all/0/1"&gt;Zhinus Marzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cekic_M/0/1/0/all/0/1"&gt;Metehan Cekic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Madhow_U/0/1/0/all/0/1"&gt;Upamanyu Madhow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pedarsani_R/0/1/0/all/0/1"&gt;Ramtin Pedarsani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taxonomy of academic plagiarism methods. (arXiv:2105.12068v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12068</id>
        <link href="http://arxiv.org/abs/2105.12068"/>
        <updated>2021-05-26T01:22:10.511Z</updated>
        <summary type="html"><![CDATA[The article gives an overview of the plagiarism domain, with focus on
academic plagiarism. The article defines plagiarism, explains the origin of the
term, as well as plagiarism related terms. It identifies the extent of the
plagiarism domain and then focuses on the plagiarism subdomain of text
documents, for which it gives an overview of current classifications and
taxonomies and then proposes a more comprehensive classification according to
several criteria: their origin and purpose, technical implementation,
consequence, complexity of detection and according to the number of linguistic
sources. The article suggests the new classification of academic plagiarism,
describes sorts and methods of plagiarism, types and categories, approaches and
phases of plagiarism detection, the classification of methods and algorithms
for plagiarism detection. The title of the article explicitly targets the
academic community, but it is sufficiently general and interdisciplinary, so it
can be useful for many other professionals like software developers, linguists
and librarians.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vrbanec_T/0/1/0/all/0/1"&gt;Tedo Vrbanec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mestrovic_A/0/1/0/all/0/1"&gt;Ana Mestrovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accounting for Unobserved Confounding in Domain Generalization. (arXiv:2007.10653v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10653</id>
        <link href="http://arxiv.org/abs/2007.10653"/>
        <updated>2021-05-26T01:22:10.495Z</updated>
        <summary type="html"><![CDATA[The ability to generalize from observed to new related environments is
central to any form of reliable machine learning, yet most methods fail when
moving beyond i.i.d data. This work argues that in some cases the reason lies
in a misapreciation of the causal structure in data; and in particular due to
the influence of unobserved confounders which void many of the invariances and
principles of minimum error between environments presently used for the problem
of domain generalization. This observation leads us to study generalization in
the context of a broader class of interventions in an underlying causal model
(including changes in observed, unobserved and target variable distributions)
and to connect this causal intuition with an explicit distributionally robust
optimization problem. From this analysis derives a new proposal for model
learning with explicit generalization guarantees that is based on the partial
equality of error derivatives with respect to model parameters. We demonstrate
the empirical performance of our approach on healthcare data from different
modalities, including image, speech and tabular data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bellot_A/0/1/0/all/0/1"&gt;Alexis Bellot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation of Photovoltaic Module Cells in Uncalibrated Electroluminescence Images. (arXiv:1806.06530v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1806.06530</id>
        <link href="http://arxiv.org/abs/1806.06530"/>
        <updated>2021-05-26T01:22:10.488Z</updated>
        <summary type="html"><![CDATA[High resolution electroluminescence (EL) images captured in the infrared
spectrum allow to visually and non-destructively inspect the quality of
photovoltaic (PV) modules. Currently, however, such a visual inspection
requires trained experts to discern different kinds of defects, which is
time-consuming and expensive. Automated segmentation of cells is therefore a
key step in automating the visual inspection workflow. In this work, we propose
a robust automated segmentation method for extraction of individual solar cells
from EL images of PV modules. This enables controlled studies on large amounts
of data to understanding the effects of module degradation over time-a process
not yet fully understood. The proposed method infers in several steps a
high-level solar module representation from low-level edge features. An
important step in the algorithm is to formulate the segmentation problem in
terms of lens calibration by exploiting the plumbline constraint. We evaluate
our method on a dataset of various solar modules types containing a total of
408 solar cells with various defects. Our method robustly solves this task with
a median weighted Jaccard index of 94.47% and an $F_1$ score of 97.62%, both
indicating a very high similarity between automatically segmented and ground
truth solar cell masks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deitsch_S/0/1/0/all/0/1"&gt;Sergiu Deitsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buerhop_Lutz_C/0/1/0/all/0/1"&gt;Claudia Buerhop-Lutz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sovetkin_E/0/1/0/all/0/1"&gt;Evgenii Sovetkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steland_A/0/1/0/all/0/1"&gt;Ansgar Steland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallwitz_F/0/1/0/all/0/1"&gt;Florian Gallwitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riess_C/0/1/0/all/0/1"&gt;Christian Riess&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of GraphSum's Attention Weights to Improve the Explainability of Multi-Document Summarization. (arXiv:2105.11908v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11908</id>
        <link href="http://arxiv.org/abs/2105.11908"/>
        <updated>2021-05-26T01:22:10.481Z</updated>
        <summary type="html"><![CDATA[Modern multi-document summarization (MDS) methods are based on transformer
architectures. They generate state of the art summaries, but lack
explainability. We focus on graph-based transformer models for MDS as they
gained recent popularity. We aim to improve the explainability of the
graph-based MDS by analyzing their attention weights. In a graph-based MDS such
as GraphSum, vertices represent the textual units, while the edges form some
similarity graph over the units. We compare GraphSum's performance utilizing
different textual units, i. e., sentences versus paragraphs, on two news
benchmark datasets, namely WikiSum and MultiNews. Our experiments show that
paragraph-level representations provide the best summarization performance.
Thus, we subsequently focus oAnalysisn analyzing the paragraph-level attention
weights of GraphSum's multi-heads and decoding layers in order to improve the
explainability of a transformer-based MDS model. As a reference metric, we
calculate the ROUGE scores between the input paragraphs and each sentence in
the generated summary, which indicate source origin information via text
similarity. We observe a high correlation between the attention weights and
this reference metric, especially on the the later decoding layers of the
transformer architecture. Finally, we investigate if the generated summaries
follow a pattern of positional bias by extracting which paragraph provided the
most information for each generated summary. Our results show that there is a
high correlation between the position in the summary and the source origin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hickmann_M/0/1/0/all/0/1"&gt;M. Lautaro Hickmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wurzberger_F/0/1/0/all/0/1"&gt;Fabian Wurzberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoxhalli_M/0/1/0/all/0/1"&gt;Megi Hoxhalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lochner_A/0/1/0/all/0/1"&gt;Arne Lochner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tollich_J/0/1/0/all/0/1"&gt;Jessica T&amp;#xf6;llich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1"&gt;Ansgar Scherp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Imitation Learning for Robot Tasks with Sparse and Delayed Rewards. (arXiv:2010.06962v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06962</id>
        <link href="http://arxiv.org/abs/2010.06962"/>
        <updated>2021-05-26T01:22:10.474Z</updated>
        <summary type="html"><![CDATA[The application of reinforcement learning (RL) in robotic control is still
limited in the environments with sparse and delayed rewards. In this paper, we
propose a practical self-imitation learning method named Self-Imitation
Learning with Constant Reward (SILCR). Instead of requiring hand-defined
immediate rewards from environments, our method assigns the immediate rewards
at each timestep with constant values according to their final episodic
rewards. In this way, even if the dense rewards from environments are
unavailable, every action taken by the agents would be guided properly. We
demonstrate the effectiveness of our method in some challenging continuous
robotics control tasks in MuJoCo simulation and the results show that our
method significantly outperforms the alternative methods in tasks with sparse
and delayed rewards. Even compared with alternatives with dense rewards
available, our method achieves competitive performance. The ablation
experiments also show the stability and reproducibility of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhixin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mengxiang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiamMOT: Siamese Multi-Object Tracking. (arXiv:2105.11595v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11595</id>
        <link href="http://arxiv.org/abs/2105.11595"/>
        <updated>2021-05-26T01:22:10.468Z</updated>
        <summary type="html"><![CDATA[In this paper, we focus on improving online multi-object tracking (MOT). In
particular, we introduce a region-based Siamese Multi-Object Tracking network,
which we name SiamMOT. SiamMOT includes a motion model that estimates the
instance's movement between two frames such that detected instances are
associated. To explore how the motion modelling affects its tracking
capability, we present two variants of Siamese tracker, one that implicitly
models motion and one that models it explicitly. We carry out extensive
quantitative experiments on three different MOT datasets: MOT17, TAO-person and
Caltech Roadside Pedestrians, showing the importance of motion modelling for
MOT and the ability of SiamMOT to substantially outperform the
state-of-the-art. Finally, SiamMOT also outperforms the winners of ACM MM'20
HiEve Grand Challenge on HiEve dataset. Moreover, SiamMOT is efficient, and it
runs at 17 FPS for 720P videos on a single modern GPU. Codes are available in
\url{https://github.com/amazon-research/siam-mot}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shuai_B/0/1/0/all/0/1"&gt;Bing Shuai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berneshawi_A/0/1/0/all/0/1"&gt;Andrew Berneshawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1"&gt;Davide Modolo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1"&gt;Joseph Tighe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Centimeter-Wave Free-Space Time-of-Flight Imaging. (arXiv:2105.11606v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11606</id>
        <link href="http://arxiv.org/abs/2105.11606"/>
        <updated>2021-05-26T01:22:10.454Z</updated>
        <summary type="html"><![CDATA[Depth cameras are emerging as a cornerstone modality with diverse
applications that directly or indirectly rely on measured depth, including
personal devices, robotics, and self-driving vehicles. Although time-of-flight
(ToF) methods have fueled these applications, the precision and robustness of
ToF methods is limited by relying on photon time-tagging or modulation after
photo-conversion. Successful optical modulation approaches have been restricted
fiber-coupled modulation with large coupling losses or interferometric
modulation with sub-cm range, and the precision gap between interferometric
methods and ToF methods is more than three orders of magnitudes. In this work,
we close this gap and propose a computational imaging method for all-optical
free-space correlation before photo-conversion that achieves micron-scale depth
resolution with robustness to surface reflectance and ambient light with
conventional silicon intensity sensors. To this end, we solve two technical
challenges: modulating at GHz rates and computational phase unwrapping. We
propose an imaging approach with resonant polarization modulators and devise a
novel optical dual-pass frequency-doubling which achieves high modulation
contrast at more than 10GHz. At the same time, centimeter-wave modulation
together with a small modulation bandwidth render existing phase unwrapping
methods ineffective. We tackle this problem with a neural phase unwrapping
method that exploits that adjacent wraps are often highly correlated. We
validate the proposed method in simulation and experimentally, where it
achieves micron-scale depth precision. We demonstrate precise depth sensing
independently of surface texture and ambient light and compare against existing
analog demodulation methods, which we outperform across all tested scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1"&gt;Seung-Hwan Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walsh_N/0/1/0/all/0/1"&gt;Noah Walsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chugunov_I/0/1/0/all/0/1"&gt;Ilya Chugunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zheng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1"&gt;Felix Heide&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TIPCB: A Simple but Effective Part-based Convolutional Baseline for Text-based Person Search. (arXiv:2105.11628v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11628</id>
        <link href="http://arxiv.org/abs/2105.11628"/>
        <updated>2021-05-26T01:22:10.448Z</updated>
        <summary type="html"><![CDATA[Text-based person search is a sub-task in the field of image retrieval, which
aims to retrieve target person images according to a given textual description.
The significant feature gap between two modalities makes this task very
challenging. Many existing methods attempt to utilize local alignment to
address this problem in the fine-grained level. However, most relevant methods
introduce additional models or complicated training and evaluation strategies,
which are hard to use in realistic scenarios. In order to facilitate the
practical application, we propose a simple but effective end-to-end learning
framework for text-based person search named TIPCB (i.e., Text-Image Part-based
Convolutional Baseline). Firstly, a novel dual-path local alignment network
structure is proposed to extract visual and textual local representations, in
which images are segmented horizontally and texts are aligned adaptively. Then,
we propose a multi-stage cross-modal matching strategy, which eliminates the
modality gap from three feature levels, including low level, local level and
global level. Extensive experiments are conducted on the widely-used benchmark
dataset (CUHK-PEDES) and verify that our method outperforms the
state-of-the-art methods by 3.69%, 2.95% and 2.31% in terms of Top-1, Top-5 and
Top-10. Our code has been released in https://github.com/OrangeYHChen/TIPCB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yujiang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhenxing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuhui Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruili Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Space Targeted Attacks by Statistic Alignment. (arXiv:2105.11645v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11645</id>
        <link href="http://arxiv.org/abs/2105.11645"/>
        <updated>2021-05-26T01:22:10.440Z</updated>
        <summary type="html"><![CDATA[By adding human-imperceptible perturbations to images, DNNs can be easily
fooled. As one of the mainstream methods, feature space targeted attacks
perturb images by modulating their intermediate feature maps, for the
discrepancy between the intermediate source and target features is minimized.
However, the current choice of pixel-wise Euclidean Distance to measure the
discrepancy is questionable because it unreasonably imposes a
spatial-consistency constraint on the source and target features. Intuitively,
an image can be categorized as "cat" no matter the cat is on the left or right
of the image. To address this issue, we propose to measure this discrepancy
using statistic alignment. Specifically, we design two novel approaches called
Pair-wise Alignment Attack and Global-wise Alignment Attack, which attempt to
measure similarities between feature maps by high-order statistics with
translation invariance. Furthermore, we systematically analyze the layer-wise
transferability with varied difficulties to obtain highly reliable attacks.
Extensive experiments verify the effectiveness of our proposed method, and it
outperforms the state-of-the-art algorithms by a large margin. Our code is
publicly available at https://github.com/yaya-cheng/PAA-GAA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianli Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yaya Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qilong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jingkuan Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Enhancing Ground Surface Detection from Sparse Lidar Point Cloud. (arXiv:2105.11649v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.11649</id>
        <link href="http://arxiv.org/abs/2105.11649"/>
        <updated>2021-05-26T01:22:10.434Z</updated>
        <summary type="html"><![CDATA[Ground surface detection in point cloud is widely used as a key module in
autonomous driving systems. Different from previous approaches which are mostly
developed for lidars with high beam resolution, e.g. Velodyne HDL-64, this
paper proposes ground detection techniques applicable to much sparser point
cloud captured by lidars with low beam resolution, e.g. Velodyne VLP-16. The
approach is based on the RANSAC scheme of plane fitting. Inlier verification
for plane hypotheses is enhanced by exploiting the point-wise tangent, which is
a local feature available to compute regardless of the density of lidar beams.
Ground surface which is not perfectly planar is fitted by multiple
(specifically 4 in our implementation) disjoint plane regions. By assuming
these plane regions to be rectanglar and exploiting the integral image
technique, our approach approximately finds the optimal region partition and
plane hypotheses under the RANSAC scheme with real-time computational
complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Dual Sampling Module for Fine-Grained Semantic Segmentation. (arXiv:2105.11657v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11657</id>
        <link href="http://arxiv.org/abs/2105.11657"/>
        <updated>2021-05-26T01:22:10.427Z</updated>
        <summary type="html"><![CDATA[Representation of semantic context and local details is the essential issue
for building modern semantic segmentation models. However, the
interrelationship between semantic context and local details is not well
explored in previous works. In this paper, we propose a Dynamic Dual Sampling
Module (DDSM) to conduct dynamic affinity modeling and propagate semantic
context to local details, which yields a more discriminative representation.
Specifically, a dynamic sampling strategy is used to sparsely sample
representative pixels and channels in the higher layer, forming adaptive
compact support for each pixel and channel in the lower layer. The sampled
features with high semantics are aggregated according to the affinities and
then propagated to detailed lower-layer features, leading to a fine-grained
segmentation result with well-preserved boundaries. Experiment results on both
Cityscapes and Camvid datasets validate the effectiveness and efficiency of the
proposed approach. Code and models will be available at
\url{x3https://github.com/Fantasticarl/DDSM}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangtai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yanran Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yunhai Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Regression with Distributed Learning: A Generalization Error Perspective. (arXiv:2101.09001v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09001</id>
        <link href="http://arxiv.org/abs/2101.09001"/>
        <updated>2021-05-26T01:22:10.419Z</updated>
        <summary type="html"><![CDATA[Distributed learning provides an attractive framework for scaling the
learning task by sharing the computational load over multiple nodes in a
network. Here, we investigate the performance of distributed learning for
large-scale linear regression where the model parameters, i.e., the unknowns,
are distributed over the network. We adopt a statistical learning approach. In
contrast to works that focus on the performance on the training data, we focus
on the generalization error, i.e., the performance on unseen data. We provide
high-probability bounds on the generalization error for both isotropic and
correlated Gaussian data as well as sub-gaussian data. These results reveal the
dependence of the generalization performance on the partitioning of the model
over the network. In particular, our results show that the generalization error
of the distributed solution can be substantially higher than that of the
centralized solution even when the error on the training data is at the same
level for both the centralized and distributed approaches. Our numerical
results illustrate the performance with both real-world image data as well as
synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hellkvist_M/0/1/0/all/0/1"&gt;Martin Hellkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ozcelikkale_A/0/1/0/all/0/1"&gt;Ay&amp;#xe7;a &amp;#xd6;z&amp;#xe7;elikkale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ahlen_A/0/1/0/all/0/1"&gt;Anders Ahl&amp;#xe9;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blockchain Assisted Decentralized Federated Learning (BLADE-FL): Performance Analysis and Resource Allocation. (arXiv:2101.06905v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06905</id>
        <link href="http://arxiv.org/abs/2101.06905"/>
        <updated>2021-05-26T01:22:10.401Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL), as a distributed machine learning paradigm, promotes
personal privacy by local data processing at each client. However, relying on a
centralized server for model aggregation, standard FL is vulnerable to server
malfunctions, untrustworthy server, and external attacks. To address this
issue, we propose a decentralized FL framework by integrating blockchain into
FL, namely, blockchain assisted decentralized federated learning (BLADE-FL). In
a round of the proposed BLADE-FL, each client broadcasts the trained model to
other clients, aggregates its own model with received ones, and then competes
to generate a block before its local training of the next round. We evaluate
the learning performance of BLADE-FL, and develop an upper bound on the global
loss function. Then we verify that this bound is convex with respect to the
number of overall aggregation rounds K, and optimize the computing resource
allocation for minimizing the upper bound. We also note that there is a
critical problem of training deficiency, caused by lazy clients who plagiarize
others' trained models and add artificial noises to disguise their cheating
behaviors. Focusing on this problem, we explore the impact of lazy clients on
the learning performance of BLADE-FL, and characterize the relationship among
the optimal K, the learning parameters, and the proportion of lazy clients.
Based on MNIST and Fashion-MNIST datasets, we show that the experimental
results are consistent with the analytical ones. To be specific, the gap
between the developed upper bound and experimental results is lower than 5%,
and the optimized K based on the upper bound can effectively minimize the loss
function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yumeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1"&gt;Kang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1"&gt;Long Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affine Transport for Sim-to-Real Domain Adaptation. (arXiv:2105.11739v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.11739</id>
        <link href="http://arxiv.org/abs/2105.11739"/>
        <updated>2021-05-26T01:22:10.394Z</updated>
        <summary type="html"><![CDATA[Sample-efficient domain adaptation is an open problem in robotics. In this
paper, we present affine transport -- a variant of optimal transport, which
models the mapping between state transition distributions between the source
and target domains with an affine transformation. First, we derive the affine
transport framework; then, we extend the basic framework with Procrustes
alignment to model arbitrary affine transformations. We evaluate the method in
a number of OpenAI Gym sim-to-sim experiments with simulation environments, as
well as on a sim-to-real domain adaptation task of a robot hitting a hockeypuck
such that it slides and stops at a target position. In each experiment, we
evaluate the results when transferring between each pair of dynamics domains.
The results show that affine transport can significantly reduce the model
adaptation error in comparison to using the original, non-adapted dynamics
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mallasto_A/0/1/0/all/0/1"&gt;Anton Mallasto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arndt_K/0/1/0/all/0/1"&gt;Karol Arndt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1"&gt;Markus Heinonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1"&gt;Ville Kyrki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibration and Uncertainty Quantification of Bayesian Convolutional Neural Networks for Geophysical Applications. (arXiv:2105.12115v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12115</id>
        <link href="http://arxiv.org/abs/2105.12115"/>
        <updated>2021-05-26T01:22:10.388Z</updated>
        <summary type="html"><![CDATA[Deep neural networks offer numerous potential applications across geoscience,
for example, one could argue that they are the state-of-the-art method for
predicting faults in seismic datasets. In quantitative reservoir
characterization workflows, it is common to incorporate the uncertainty of
predictions thus such subsurface models should provide calibrated probabilities
and the associated uncertainties in their predictions. It has been shown that
popular Deep Learning-based models are often miscalibrated, and due to their
deterministic nature, provide no means to interpret the uncertainty of their
predictions. We compare three different approaches to obtaining probabilistic
models based on convolutional neural networks in a Bayesian formalism, namely
Deep Ensembles, Concrete Dropout, and Stochastic Weight Averaging-Gaussian
(SWAG). These methods are consistently applied to fault detection case studies
where Deep Ensembles use independently trained models to provide fault
probabilities, Concrete Dropout represents an extension to the popular Dropout
technique to approximate Bayesian neural networks, and finally, we apply SWAG,
a recent method that is based on the Bayesian inference equivalence of
mini-batch Stochastic Gradient Descent. We provide quantitative results in
terms of model calibration and uncertainty representation, as well as
qualitative results on synthetic and real seismic datasets. Our results show
that the approximate Bayesian methods, Concrete Dropout and SWAG, both provide
well-calibrated predictions and uncertainty attributes at a lower computational
cost when compared to the baseline Deep Ensemble approach. The resulting
uncertainties also offer a possibility to further improve the model performance
as well as enhancing the interpretability of the models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mosser_L/0/1/0/all/0/1"&gt;Lukas Mosser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeini_E/0/1/0/all/0/1"&gt;Ehsan Zabihi Naeini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Estimation of the Gradient of the Log-Likelihood for a Class of Continuous-Time State-Space Models. (arXiv:2105.11522v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.11522</id>
        <link href="http://arxiv.org/abs/2105.11522"/>
        <updated>2021-05-26T01:22:10.381Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider static parameter estimation for a class of
continuous-time state-space models. Our goal is to obtain an unbiased estimate
of the gradient of the log-likelihood (score function), which is an estimate
that is unbiased even if the stochastic processes involved in the model must be
discretized in time. To achieve this goal, we apply a \emph{doubly randomized
scheme} (see, e.g.,~\cite{ub_mcmc, ub_grad}), that involves a novel coupled
conditional particle filter (CCPF) on the second level of randomization
\cite{jacob2}. Our novel estimate helps facilitate the application of
gradient-based estimation algorithms, such as stochastic-gradient Langevin
descent. We illustrate our methodology in the context of stochastic gradient
descent (SGD) in several numerical examples and compare with the Rhee \& Glynn
estimator \cite{rhee,vihola}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ballesio_M/0/1/0/all/0/1"&gt;Marco Ballesio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jasra_A/0/1/0/all/0/1"&gt;Ajay Jasra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search. (arXiv:2105.11871v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11871</id>
        <link href="http://arxiv.org/abs/2105.11871"/>
        <updated>2021-05-26T01:22:10.365Z</updated>
        <summary type="html"><![CDATA[Recent breakthroughs of Neural Architecture Search (NAS) extend the field's
research scope towards a broader range of vision tasks and more diversified
search spaces. While existing NAS methods mostly design architectures on a
single task, algorithms that look beyond single-task search are surging to
pursue a more efficient and universal solution across various tasks. Many of
them leverage transfer learning and seek to preserve, reuse, and refine network
design knowledge to achieve higher efficiency in future tasks. However, the
enormous computational cost and experiment complexity of cross-task NAS are
imposing barriers for valuable research in this direction. Existing NAS
benchmarks all focus on one type of vision task, i.e., classification. In this
work, we propose TransNAS-Bench-101, a benchmark dataset containing network
performance across seven tasks, covering classification, regression,
pixel-level prediction, and self-supervised tasks. This diversity provides
opportunities to transfer NAS methods among tasks and allows for more complex
transfer schemes to evolve. We explore two fundamentally different types of
search space: cell-level search space and macro-level search space. With 7,352
backbones evaluated on seven tasks, 51,464 trained models with detailed
training information are provided. With TransNAS-Bench-101, we hope to
encourage the advent of exceptional NAS algorithms that raise cross-task search
efficiency and generalizability to the next level. Our dataset file will be
available at Mindspore, VEGA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1"&gt;Yawen Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zewei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep High-Resolution Representation Learning for Cross-Resolution Person Re-identification. (arXiv:2105.11722v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11722</id>
        <link href="http://arxiv.org/abs/2105.11722"/>
        <updated>2021-05-26T01:22:10.359Z</updated>
        <summary type="html"><![CDATA[Person re-identification (re-ID) tackles the problem of matching person
images with the same identity from different cameras. In practical
applications, due to the differences in camera performance and distance between
cameras and persons of interest, captured person images usually have various
resolutions. We name this problem as Cross-Resolution Person Re-identification
which brings a great challenge for matching correctly. In this paper, we
propose a Deep High-Resolution Pseudo-Siamese Framework (PS-HRNet) to solve the
above problem. Specifically, in order to restore the resolution of
low-resolution images and make reasonable use of different channel information
of feature maps, we introduce and innovate VDSR module with channel attention
(CA) mechanism, named as VDSR-CA. Then we reform the HRNet by designing a novel
representation head to extract discriminating features, named as HRNet-ReID. In
addition, a pseudo-siamese framework is constructed to reduce the difference of
feature distributions between low-resolution images and high-resolution images.
The experimental results on five cross-resolution person datasets verify the
effectiveness of our proposed approach. Compared with the state-of-the-art
methods, our proposed PS-HRNet improves 3.4\%, 6.2\%, 2.5\%,1.1\% and 4.2\% at
Rank-1 on MLR-Market-1501, MLR-CUHK03, MLR-VIPeR, MLR-DukeMTMC-reID, and CAVIAR
datasets, respectively. Our code is available at
\url{https://github.com/zhguoqing}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yu Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhicheng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuhui Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shengyong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FILTRA: Rethinking Steerable CNN by Filter Transform. (arXiv:2105.11636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11636</id>
        <link href="http://arxiv.org/abs/2105.11636"/>
        <updated>2021-05-26T01:22:10.353Z</updated>
        <summary type="html"><![CDATA[Steerable CNN imposes the prior knowledge of transformation invariance or
equivariance in the network architecture to enhance the the network robustness
on geometry transformation of data and reduce overfitting. It has been an
intuitive and widely used technique to construct a steerable filter by
augmenting a filter with its transformed copies in the past decades, which is
named as filter transform in this paper. Recently, the problem of steerable CNN
has been studied from aspect of group representation theory, which reveals the
function space structure of a steerable kernel function. However, it is not yet
clear on how this theory is related to the filter transform technique. In this
paper, we show that kernel constructed by filter transform can also be
interpreted in the group representation theory. This interpretation help
complete the puzzle of steerable CNN theory and provides a novel and simple
approach to implement steerable convolution operators. Experiments are executed
on multiple datasets to verify the feasibility of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gim Hee Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Scale-consistent Depth Learning from Video. (arXiv:2105.11610v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11610</id>
        <link href="http://arxiv.org/abs/2105.11610"/>
        <updated>2021-05-26T01:22:10.347Z</updated>
        <summary type="html"><![CDATA[We propose a monocular depth estimator SC-Depth, which requires only
unlabelled videos for training and enables the scale-consistent prediction at
inference time. Our contributions include: (i) we propose a geometry
consistency loss, which penalizes the inconsistency of predicted depths between
adjacent views; (ii) we propose a self-discovered mask to automatically
localize moving objects that violate the underlying static scene assumption and
cause noisy signals during training; (iii) we demonstrate the efficacy of each
component with a detailed ablation study and show high-quality depth estimation
results in both KITTI and NYUv2 datasets. Moreover, thanks to the capability of
scale-consistent prediction, we show that our monocular-trained deep networks
are readily integrated into the ORB-SLAM2 system for more robust and accurate
tracking. The proposed hybrid Pseudo-RGBD SLAM shows compelling results in
KITTI, and it generalizes well to the KAIST dataset without additional
training. Finally, we provide several demos for qualitative evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jia-Wang Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1"&gt;Huangying Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Naiyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhichao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Le Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1"&gt;Ian Reid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Mobile GUI: from Pixel-Words to Screen-Sentences. (arXiv:2105.11941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11941</id>
        <link href="http://arxiv.org/abs/2105.11941"/>
        <updated>2021-05-26T01:22:10.340Z</updated>
        <summary type="html"><![CDATA[The ubiquity of mobile phones makes mobile GUI understanding an important
task. Most previous works in this domain require human-created metadata of
screens (e.g. View Hierarchy) during inference, which unfortunately is often
not available or reliable enough for GUI understanding. Inspired by the
impressive success of Transformers in NLP tasks, targeting for purely
vision-based GUI understanding, we extend the concepts of Words/Sentence to
Pixel-Words/Screen-Sentence, and propose a mobile GUI understanding
architecture: Pixel-Words to Screen-Sentence (PW2SS). In analogy to the
individual Words, we define the Pixel-Words as atomic visual components (text
and graphic components), which are visually consistent and semantically clear
across screenshots of a large variety of design styles. The Pixel-Words
extracted from a screenshot are aggregated into Screen-Sentence with a Screen
Transformer proposed to model their relations. Since the Pixel-Words are
defined as atomic visual components, the ambiguity between their visual
appearance and semantics is dramatically reduced. We are able to make use of
metadata available in training data to auto-generate high-quality annotations
for Pixel-Words. A dataset, RICO-PW, of screenshots with Pixel-Words
annotations is built based on the public RICO dataset, which will be released
to help to address the lack of high-quality training data in this area. We
train a detector to extract Pixel-Words from screenshots on this dataset and
achieve metadata-free GUI understanding during inference. We conduct
experiments and show that Pixel-Words can be well extracted on RICO-PW and well
generalized to a new dataset, P2S-UI, collected by ourselves. The effectiveness
of PW2SS is further verified in the GUI understanding tasks including relation
prediction, clickability prediction, screen retrieval, and app type
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jingwen Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuwang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sam Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliard_G/0/1/0/all/0/1"&gt;Grayson Hilliard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matching Targets Across Domains with RADON, the Re-Identification Across Domain Network. (arXiv:2105.12056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12056</id>
        <link href="http://arxiv.org/abs/2105.12056"/>
        <updated>2021-05-26T01:22:10.323Z</updated>
        <summary type="html"><![CDATA[We present a novel convolutional neural network that learns to match images
of an object taken from different viewpoints or by different optical sensors.
Our Re-Identification Across Domain Network (RADON) scores pairs of input
images from different domains on similarity. Our approach extends previous work
on Siamese networks and modifies them to more challenging use cases, including
low- and no-shot learning, in which few images of a specific target are
available for training. RADON shows strong performance on cross-view vehicle
matching and cross-domain person identification in a no-shot learning
environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burgess_C/0/1/0/all/0/1"&gt;Cassandra Burgess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neisinger_C/0/1/0/all/0/1"&gt;Cordelia Neisinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinner_R/0/1/0/all/0/1"&gt;Rafael Dinner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Filter Sketch for Network Pruning. (arXiv:2001.08514v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.08514</id>
        <link href="http://arxiv.org/abs/2001.08514"/>
        <updated>2021-05-26T01:22:10.316Z</updated>
        <summary type="html"><![CDATA[We propose a novel network pruning approach by information preserving of
pre-trained network weights (filters). Network pruning with the information
preserving is formulated as a matrix sketch problem, which is efficiently
solved by the off-the-shelf Frequent Direction method. Our approach, referred
to as FilterSketch, encodes the second-order information of pre-trained
weights, which enables the representation capacity of pruned networks to be
recovered with a simple fine-tuning procedure. FilterSketch requires neither
training from scratch nor data-driven iterative optimization, leading to a
several-orders-of-magnitude reduction of time cost in the optimization of
pruning. Experiments on CIFAR-10 show that FilterSketch reduces 63.3% of FLOPs
and prunes 59.9% of network parameters with negligible accuracy cost for
ResNet-110. On ILSVRC-2012, it reduces 45.5% of FLOPs and removes 43.0% of
parameters with only 0.69% accuracy drop for ResNet-50. Our code and pruned
models can be found at https://github.com/lmbxmu/FilterSketch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Liujuan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaojie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts. (arXiv:2012.09165v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09165</id>
        <link href="http://arxiv.org/abs/2012.09165"/>
        <updated>2021-05-26T01:22:10.306Z</updated>
        <summary type="html"><![CDATA[The rapid progress in 3D scene understanding has come with growing demand for
data; however, collecting and annotating 3D scenes (e.g. point clouds) are
notoriously hard. For example, the number of scenes (e.g. indoor rooms) that
can be accessed and scanned might be limited; even given sufficient data,
acquiring 3D labels (e.g. instance masks) requires intensive human labor. In
this paper, we explore data-efficient learning for 3D point cloud. As a first
step towards this direction, we propose Contrastive Scene Contexts, a 3D
pre-training method that makes use of both point-level correspondences and
spatial contexts in a scene. Our method achieves state-of-the-art results on a
suite of benchmarks where training data or labels are scarce. Our study reveals
that exhaustive labelling of 3D point clouds might be unnecessary; and
remarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89%
(instance segmentation) and 96% (semantic segmentation) of the baseline
performance that uses full annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Ji Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1"&gt;Benjamin Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1"&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Saining Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security in Next Generation Mobile Payment Systems: A Comprehensive Survey. (arXiv:2105.12097v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.12097</id>
        <link href="http://arxiv.org/abs/2105.12097"/>
        <updated>2021-05-26T01:22:10.300Z</updated>
        <summary type="html"><![CDATA[Cash payment is still king in several markets, accounting for more than 90\
of the payments in almost all the developing countries. The usage of mobile
phones is pretty ordinary in this present era. Mobile phones have become an
inseparable friend for many users, serving much more than just communication
tools. Every subsequent person is heavily relying on them due to multifaceted
usage and affordability. Every person wants to manage his/her daily
transactions and related issues by using his/her mobile phone. With the rise
and advancements of mobile-specific security, threats are evolving as well. In
this paper, we provide a survey of various security models for mobile phones.
We explore multiple proposed models of the mobile payment system (MPS), their
technologies and comparisons, payment methods, different security mechanisms
involved in MPS, and provide analysis of the encryption technologies,
authentication methods, and firewall in MPS. We also present current challenges
and future directions of mobile phone security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1"&gt;Waqas Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasool_A/0/1/0/all/0/1"&gt;Amir Rasool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nebhen_J/0/1/0/all/0/1"&gt;Jamel Nebhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahzad_F/0/1/0/all/0/1"&gt;Faisal Shahzad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+RehmanJaved_A/0/1/0/all/0/1"&gt;Abdul RehmanJaved&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadekallu_T/0/1/0/all/0/1"&gt;Thippa Reddy Gadekallu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalil_Z/0/1/0/all/0/1"&gt;Zunera Jalil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11804</id>
        <link href="http://arxiv.org/abs/2105.11804"/>
        <updated>2021-05-26T01:22:10.294Z</updated>
        <summary type="html"><![CDATA[Few-Shot Learning (FSL) algorithms have made substantial progress in learning
novel concepts with just a handful of labelled data. To classify query
instances from novel classes encountered at test-time, they only require a
support set composed of a few labelled samples. FSL benchmarks commonly assume
that those queries come from the same distribution as instances in the support
set. However, in a realistic set-ting, data distribution is plausibly subject
to change, a situation referred to as Distribution Shift (DS). The present work
addresses the new and challenging problem of Few-Shot Learning under
Support/Query Shift (FSQS) i.e., when support and query instances are sampled
from related but different distributions. Our contributions are the following.
First, we release a testbed for FSQS, including datasets, relevant baselines
and a protocol for a rigorous and reproducible evaluation. Second, we observe
that well-established FSL algorithms unsurprisingly suffer from a considerable
drop in accuracy when facing FSQS, stressing the significance of our study.
Finally, we show that transductive algorithms can limit the inopportune effect
of DS. In particular, we study both the role of Batch-Normalization and Optimal
Transport (OT) in aligning distributions, bridging Unsupervised Domain
Adaptation with FSL. This results in a new method that efficiently combines OT
with the celebrated Prototypical Networks. We bring compelling experiments
demonstrating the advantage of our method. Our work opens an exciting line of
research by providing a testbed and strong baselines. Our code is available at
https://github.com/ebennequin/meta-domain-shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1"&gt;Etienne Bennequin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1"&gt;Victor Bouvier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1"&gt;Myriam Tami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1"&gt;Antoine Toubhans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Hudelot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T-SVD Based Non-convex Tensor Completion and Robust Principal Component Analysis. (arXiv:1904.10165v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.10165</id>
        <link href="http://arxiv.org/abs/1904.10165"/>
        <updated>2021-05-26T01:22:10.288Z</updated>
        <summary type="html"><![CDATA[Tensor completion and robust principal component analysis have been widely
used in machine learning while the key problem relies on the minimization of a
tensor rank that is very challenging. A common way to tackle this difficulty is
to approximate the tensor rank with the $\ell_1-$norm of singular values based
on its Tensor Singular Value Decomposition (T-SVD). Besides, the sparsity of a
tensor is also measured by its $\ell_1-$norm. However, the $\ell_1$ penalty is
essentially biased and thus the result will deviate. In order to sidestep the
bias, we propose a novel non-convex tensor rank surrogate function and a novel
non-convex sparsity measure. In this new setting by using the concavity instead
of the convexity, a majorization minimization algorithm is further designed for
tensor completion and robust principal component analysis. Furthermore, we
analyze its theoretical properties. Finally, the experiments on natural and
hyperspectral images demonstrate the efficacy and efficiency of our proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jinwen Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Action Proposal Generation with Transformers. (arXiv:2105.12043v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12043</id>
        <link href="http://arxiv.org/abs/2105.12043"/>
        <updated>2021-05-26T01:22:10.268Z</updated>
        <summary type="html"><![CDATA[Transformer networks are effective at modeling long-range contextual
information and have recently demonstrated exemplary performance in the natural
language processing domain. Conventionally, the temporal action proposal
generation (TAPG) task is divided into two main sub-tasks: boundary prediction
and proposal confidence prediction, which rely on the frame-level dependencies
and proposal-level relationships separately. To capture the dependencies at
different levels of granularity, this paper intuitively presents a unified
temporal action proposal generation framework with original Transformers,
called TAPG Transformer, which consists of a Boundary Transformer and a
Proposal Transformer. Specifically, the Boundary Transformer captures long-term
temporal dependencies to predict precise boundary information and the Proposal
Transformer learns the rich inter-proposal relationships for reliable
confidence evaluation. Extensive experiments are conducted on two popular
benchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG
Transformer outperforms state-of-the-art methods. Equipped with the existing
action classifier, our method achieves remarkable performance on the temporal
action localization task. Codes and models will be available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lining Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haosen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Hongxun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hujie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HERS: Homomorphically Encrypted Representation Search. (arXiv:2003.12197v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12197</id>
        <link href="http://arxiv.org/abs/2003.12197"/>
        <updated>2021-05-26T01:22:10.262Z</updated>
        <summary type="html"><![CDATA[We present a method to search for a probe (or query) image representation
against a large gallery in the encrypted domain. We require that the probe and
gallery images be represented in terms of a fixed-length representation, which
is typical for representations obtained from learned networks. Our encryption
scheme is agnostic to how the fixed-length representation is obtained and can
therefore be applied to any fixed-length representation in any application
domain. Our method, dubbed HERS (Homomorphically Encrypted Representation
Search), operates by (i) compressing the representation towards its estimated
intrinsic dimensionality with minimal loss of accuracy (ii) encrypting the
compressed representation using the proposed fully homomorphic encryption
scheme, and (iii) efficiently searching against a gallery of encrypted
representations directly in the encrypted domain, without decrypting them.
Numerical results on large galleries of face, fingerprint, and object datasets
such as ImageNet show that, for the first time, accurate and fast image search
within the encrypted domain is feasible at scale (500 seconds; $275\times$
speed up over state-of-the-art for encrypted search against a gallery of 100
million).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1"&gt;Joshua J. Engelsma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Anil K. Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1"&gt;Vishnu Naresh Boddeti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Recognition in Horses with Convolutional Neural Networks. (arXiv:2105.11953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11953</id>
        <link href="http://arxiv.org/abs/2105.11953"/>
        <updated>2021-05-26T01:22:10.256Z</updated>
        <summary type="html"><![CDATA[Creating intelligent systems capable of recognizing emotions is a difficult
task, especially when looking at emotions in animals. This paper describes the
process of designing a "proof of concept" system to recognize emotions in
horses. This system is formed by two elements, a detector and a model. The
detector is a faster region-based convolutional neural network that detects
horses in an image. The second one, the model, is a convolutional neural
network that predicts the emotion of those horses. These two models were
trained with multiple images of horses until they achieved high accuracy in
their tasks, creating therefore the desired system. 400 images of horses were
used to train both the detector and the model while 80 were used to validate
the system. Once the two components were validated they were combined into a
testable system that would detect equine emotions based on established
behavioral ethograms indicating emotional affect through head, neck, ear,
muzzle, and eye position. The system showed an accuracy of between 69% and 74%
on the validation set, demonstrating that it is possible to predict emotions in
animals using autonomous intelligent systems. It is a first "proof of concept"
approach that can be enhanced in many ways. Such a system has multiple
applications including further studies in the growing field of animal emotions
as well as in the veterinary field to determine the physical welfare of horses
or other livestock.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Corujo_L/0/1/0/all/0/1"&gt;Luis A. Corujo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1"&gt;Peter A. Gloor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kieson_E/0/1/0/all/0/1"&gt;Emily Kieson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12085</id>
        <link href="http://arxiv.org/abs/2105.12085"/>
        <updated>2021-05-26T01:22:10.250Z</updated>
        <summary type="html"><![CDATA[Long-range and short-range temporal modeling are two complementary and
crucial aspects of video recognition. Most of the state-of-the-arts focus on
short-range spatio-temporal modeling and then average multiple snippet-level
predictions to yield the final video-level prediction. Thus, their video-level
prediction does not consider spatio-temporal features of how video evolves
along the temporal dimension. In this paper, we introduce a novel Dynamic
Segment Aggregation (DSA) module to capture relationship among snippets. To be
more specific, we attempt to generate a dynamic kernel for a convolutional
operation to aggregate long-range temporal information among adjacent snippets
adaptively. The DSA module is an efficient plug-and-play module and can be
combined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform
powerful long-range modeling with minimal overhead. The final video
architecture, coined as DSANet. We conduct extensive experiments on several
video recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,
Something-Something V1 and ActivityNet) to show its superiority. Our proposed
DSA module is shown to benefit various video recognition models significantly.
For example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is
improved from 74.9% to 78.2% on Kinetics-400. Codes will be available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhikang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1"&gt;Mingde Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zichao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yifeng Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism. (arXiv:2003.03955v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.03955</id>
        <link href="http://arxiv.org/abs/2003.03955"/>
        <updated>2021-05-26T01:22:10.244Z</updated>
        <summary type="html"><![CDATA[Food retrieval is an important task to perform analysis of food-related
information, where we are interested in retrieving relevant information about
the queried food item such as ingredients, cooking instructions, etc. In this
paper, we investigate cross-modal retrieval between food images and cooking
recipes. The goal is to learn an embedding of images and recipes in a common
feature space, such that the corresponding image-recipe embeddings lie close to
one another. Two major challenges in addressing this problem are 1) large
intra-variance and small inter-variance across cross-modal food data; and 2)
difficulties in obtaining discriminative recipe representations. To address
these two problems, we propose Semantic-Consistent and Attention-based Networks
(SCAN), which regularize the embeddings of the two modalities through aligning
output semantic probabilities. Besides, we exploit a self-attention mechanism
to improve the embedding of recipes. We evaluate the performance of the
proposed method on the large-scale Recipe1M dataset, and show that we can
outperform several state-of-the-art cross-modal retrieval strategies for food
images and cooking recipes by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahoo_D/0/1/0/all/0/1"&gt;Doyen Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chenghao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1"&gt;Ke Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achananuparp_P/0/1/0/all/0/1"&gt;Palakorn Achananuparp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1"&gt;Ee-peng Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1"&gt;Steven C. H. Hoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Geometry-Informed Deep Learning Framework for Ultra-Sparse 3D Tomographic Image Reconstruction. (arXiv:2105.11692v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11692</id>
        <link href="http://arxiv.org/abs/2105.11692"/>
        <updated>2021-05-26T01:22:10.219Z</updated>
        <summary type="html"><![CDATA[Deep learning affords enormous opportunities to augment the armamentarium of
biomedical imaging, albeit its design and implementation have potential flaws.
Fundamentally, most deep learning models are driven entirely by data without
consideration of any prior knowledge, which dramatically increases the
complexity of neural networks and limits the application scope and model
generalizability. Here we establish a geometry-informed deep learning framework
for ultra-sparse 3D tomographic image reconstruction. We introduce a novel
mechanism for integrating geometric priors of the imaging system. We
demonstrate that the seamless inclusion of known priors is essential to enhance
the performance of 3D volumetric computed tomography imaging with ultra-sparse
sampling. The study opens new avenues for data-driven biomedical imaging and
promises to provide substantially improved imaging tools for various clinical
imaging and image-guided interventions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Liyue Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capaldi_D/0/1/0/all/0/1"&gt;Dante Capaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1"&gt;John Pauly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1"&gt;Lei Xing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FNAS: Uncertainty-Aware Fast Neural Architecture Search. (arXiv:2105.11694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11694</id>
        <link href="http://arxiv.org/abs/2105.11694"/>
        <updated>2021-05-26T01:22:10.213Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL)-based neural architecture search (NAS) generally
guarantees better convergence yet suffers from the requirement of huge
computational resources compared with gradient-based approaches, due to the
rollout bottleneck -- exhaustive training for each sampled generation on proxy
tasks. In this paper, we propose a general pipeline to accelerate the
convergence of the rollout process as well as the RL process in NAS. It is
motivated by the interesting observation that both the architecture and the
parameter knowledge can be transferred between different experiments and even
different tasks. We first introduce an uncertainty-aware critic (value
function) in Proximal Policy Optimization (PPO) to utilize the architecture
knowledge in previous experiments, which stabilizes the training process and
reduces the searching time by 4 times. Further, an architecture knowledge pool
together with a block similarity function is proposed to utilize parameter
knowledge and reduces the searching time by 2 times. It is the first to
introduce block-level weight sharing in RLbased NAS. The block similarity
function guarantees a 100% hitting ratio with strict fairness. Besides, we show
that a simply designed off-policy correction factor used in "replay buffer" in
RL optimization can further reduce half of the searching time. Experiments on
the Mobile Neural Architecture Search (MNAS) search space show the proposed
Fast Neural Architecture Search (FNAS) accelerates standard RL-based NAS
process by ~10x (e.g. ~256 2x2 TPUv2 x days / 20,000 GPU x hour -> 2,000 GPU x
hour for MNAS), and guarantees better performance on various vision tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yangting Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Boxiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1"&gt;Guanglu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GCNBoost: Artwork Classification by Label Propagation through a Knowledge Graph. (arXiv:2105.11852v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11852</id>
        <link href="http://arxiv.org/abs/2105.11852"/>
        <updated>2021-05-26T01:22:10.200Z</updated>
        <summary type="html"><![CDATA[The rise of digitization of cultural documents offers large-scale contents,
opening the road for development of AI systems in order to preserve, search,
and deliver cultural heritage. To organize such cultural content also means to
classify them, a task that is very familiar to modern computer science.
Contextual information is often the key to structure such real world data, and
we propose to use it in form of a knowledge graph. Such a knowledge graph,
combined with content analysis, enhances the notion of proximity between
artworks so it improves the performances in classification tasks. In this
paper, we propose a novel use of a knowledge graph, that is constructed on
annotated data and pseudo-labeled data. With label propagation, we boost
artwork classification by training a model using a graph convolutional network,
relying on the relationships between entities of the knowledge graph. Following
a transductive learning framework, our experiments show that relying on a
knowledge graph modeling the relations between labeled data and unlabeled data
allows to achieve state-of-the-art results on multiple classification tasks on
a dataset of paintings, and on a dataset of Buddha statues. Additionally, we
show state-of-the-art results for the difficult case of dealing with unbalanced
data, with the limitation of disregarding classes with extremely low degrees in
the knowledge graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaigh_C/0/1/0/all/0/1"&gt;Cheikh Brahim El Vaigh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1"&gt;Noa Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renoust_B/0/1/0/all/0/1"&gt;Benjamin Renoust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1"&gt;Chenhui Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1"&gt;Yuta Nakashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagahara_H/0/1/0/all/0/1"&gt;Hajime Nagahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Frequency aware Perceptual Image Enhancement. (arXiv:2105.11711v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11711</id>
        <link href="http://arxiv.org/abs/2105.11711"/>
        <updated>2021-05-26T01:22:10.194Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a novel deep neural network suitable for
multi-scale analysis and propose efficient model-agnostic methods that help the
network extract information from high-frequency domains to reconstruct clearer
images. Our model can be applied to multi-scale image enhancement problems
including denoising, deblurring and single image super-resolution. Experiments
on SIDD, Flickr2K, DIV2K, and REDS datasets show that our method achieves
state-of-the-art performance on each task. Furthermore, we show that our model
can overcome the over-smoothing problem commonly observed in existing
PSNR-oriented methods and generate more natural high-resolution images by
applying adversarial training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roh_H/0/1/0/all/0/1"&gt;Hyungmin Roh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Myungjoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MBIC -- A Media Bias Annotation Dataset Including Annotator Characteristics. (arXiv:2105.11910v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11910</id>
        <link href="http://arxiv.org/abs/2105.11910"/>
        <updated>2021-05-26T01:22:10.188Z</updated>
        <summary type="html"><![CDATA[Many people consider news articles to be a reliable source of information on
current events. However, due to the range of factors influencing news agencies,
such coverage may not always be impartial. Media bias, or slanted news
coverage, can have a substantial impact on public perception of events, and,
accordingly, can potentially alter the beliefs and views of the public. The
main data gap in current research on media bias detection is a robust,
representative, and diverse dataset containing annotations of biased words and
sentences. In particular, existing datasets do not control for the individual
background of annotators, which may affect their assessment and, thus,
represents critical information for contextualizing their annotations. In this
poster, we present a matrix-based methodology to crowdsource such data using a
self-developed annotation platform. We also present MBIC (Media Bias Including
Characteristics) - the first sample of 1,700 statements representing various
media bias instances. The statements were reviewed by ten annotators each and
contain labels for media bias identification both on the word and sentence
level. MBIC is the first available dataset about media bias reporting detailed
information on annotator characteristics and their individual background. The
current dataset already significantly extends existing data in this domain
providing unique and more reliable insights into the perception of bias. In
future, we will further extend it both with respect to the number of articles
and annotators per article.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1"&gt;T. Spinde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudnitckaia_L/0/1/0/all/0/1"&gt;L. Rudnitckaia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1"&gt;K. Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1"&gt;F. Hamborg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1"&gt;B. Gipp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1"&gt;K. Donnay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning-based bias transfer for overcoming laboratory differences of microscopic images. (arXiv:2105.11765v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11765</id>
        <link href="http://arxiv.org/abs/2105.11765"/>
        <updated>2021-05-26T01:22:10.173Z</updated>
        <summary type="html"><![CDATA[The automated analysis of medical images is currently limited by technical
and biological noise and bias. The same source tissue can be represented by
vastly different images if the image acquisition or processing protocols vary.
For an image analysis pipeline, it is crucial to compensate such biases to
avoid misinterpretations. Here, we evaluate, compare, and improve existing
generative model architectures to overcome domain shifts for immunofluorescence
(IF) and Hematoxylin and Eosin (H&E) stained microscopy images. To determine
the performance of the generative models, the original and transformed images
were segmented or classified by deep neural networks that were trained only on
images of the target bias. In the scope of our analysis, U-Net cycleGANs
trained with an additional identity and an MS-SSIM-based loss and Fixed-Point
GANs trained with an additional structure loss led to the best results for the
IF and H&E stained samples, respectively. Adapting the bias of the samples
significantly improved the pixel-level segmentation for human kidney glomeruli
and podocytes and improved the classification accuracy for human prostate
biopsies by up to 14%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Thebille_A/0/1/0/all/0/1"&gt;Ann-Katrin Thebille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dietrich_E/0/1/0/all/0/1"&gt;Esther Dietrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klaus_M/0/1/0/all/0/1"&gt;Martin Klaus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gernhold_L/0/1/0/all/0/1"&gt;Lukas Gernhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lennartz_M/0/1/0/all/0/1"&gt;Maximilian Lennartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuppe_C/0/1/0/all/0/1"&gt;Christoph Kuppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kramann_R/0/1/0/all/0/1"&gt;Rafael Kramann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huber_T/0/1/0/all/0/1"&gt;Tobias B. Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sauter_G/0/1/0/all/0/1"&gt;Guido Sauter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Puelles_V/0/1/0/all/0/1"&gt;Victor G. Puelles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zimmermann_M/0/1/0/all/0/1"&gt;Marina Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bonn_S/0/1/0/all/0/1"&gt;Stefan Bonn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polarimetric Spatio-Temporal Light Transport Probing. (arXiv:2105.11609v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11609</id>
        <link href="http://arxiv.org/abs/2105.11609"/>
        <updated>2021-05-26T01:22:10.165Z</updated>
        <summary type="html"><![CDATA[Light can undergo complex interactions with multiple scene surfaces of
different material types before being reflected towards a detector. During this
transport, every surface reflection and propagation is encoded in the
properties of the photons that ultimately reach the detector, including travel
time, direction, intensity, wavelength and polarization. Conventional imaging
systems capture intensity by integrating over all other dimensions of the light
into a single quantity, hiding this rich scene information in the accumulated
measurements. Existing methods can untangle these into their spatial and
temporal dimensions, fueling geometric scene understanding. However, examining
polarimetric material properties jointly with geometric properties is an open
challenge that could enable unprecedented capabilities beyond geometric
understanding, allowing to incorporate material-dependent semantics. In this
work, we propose a computational light-transport imaging method that captures
the spatially- and temporally-resolved complete polarimetric response of a
scene. Our method hinges on a novel 7D tensor theory of light transport. We
discover low-rank structures in the polarimetric tensor dimension and propose a
data-driven rotating ellipsometry method that learns to exploit redundancy of
the polarimetric structures. We instantiate our theory in two imaging
prototypes: spatio-polarimetric imaging and coaxial temporal-polarimetric
imaging. This allows us to decompose scene light transport into temporal,
spatial, and complete polarimetric dimensions that unveil scene properties
hidden to conventional methods. We validate the applicability of our method on
diverse tasks, including shape reconstruction with subsurface scattering,
seeing through scattering medium, untangling multi-bounce light transport,
breaking metamerism with polarization, and spatio-polarimetric decomposition of
crystals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1"&gt;Seung-Hwan Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1"&gt;Felix Heide&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering. (arXiv:2105.11776v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11776</id>
        <link href="http://arxiv.org/abs/2105.11776"/>
        <updated>2021-05-26T01:22:10.158Z</updated>
        <summary type="html"><![CDATA[Knowledge retrieval and reasoning are two key stages in multi-hop question
answering (QA) at web scale. Existing approaches suffer from low confidence
when retrieving evidence facts to fill the knowledge gap and lack transparent
reasoning process. In this paper, we propose a new framework to exploit more
valid facts while obtaining explainability for multi-hop QA by dynamically
constructing a semantic graph and reasoning over it. We employ Abstract Meaning
Representation (AMR) as semantic graph representation. Our framework contains
three new ideas: (a) {\tt AMR-SG}, an AMR-based Semantic Graph, constructed by
candidate fact AMRs to uncover any hop relations among question, answer and
multiple facts. (b) A novel path-based fact analytics approach exploiting {\tt
AMR-SG} to extract active facts from a large fact pool to answer questions. (c)
A fact-level relation modeling leveraging graph convolution network (GCN) to
guide the reasoning process. Results on two scientific multi-hop QA datasets
show that we can surpass recent approaches including those using additional
knowledge graphs while maintaining high explainability on OpenBookQA and
achieve a new state-of-the-art result on ARC-Challenge in a computationally
practicable setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weiwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huihui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimates of maize plant density from UAV RGB images using Faster-RCNN detection model: impact of the spatial resolution. (arXiv:2105.11857v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11857</id>
        <link href="http://arxiv.org/abs/2105.11857"/>
        <updated>2021-05-26T01:22:10.151Z</updated>
        <summary type="html"><![CDATA[Early-stage plant density is an essential trait that determines the fate of a
genotype under given environmental conditions and management practices. The use
of RGB images taken from UAVs may replace traditional visual counting in fields
with improved throughput, accuracy and access to plant localization. However,
high-resolution (HR) images are required to detect small plants present at
early stages. This study explores the impact of image ground sampling distance
(GSD) on the performances of maize plant detection at 3-5 leaves stage using
Faster-RCNN. Data collected at HR (GSD=0.3cm) over 6 contrasted sites were used
for model training. Two additional sites with images acquired both at high and
low (GSD=0.6cm) resolution were used for model evaluation. Results show that
Faster-RCNN achieved very good plant detection and counting (rRMSE=0.08)
performances when native HR images are used both for training and validation.
Similarly, good performances were observed (rRMSE=0.11) when the model is
trained over synthetic low-resolution (LR) images obtained by down-sampling the
native training HR images, and applied to the synthetic LR validation images.
Conversely, poor performances are obtained when the model is trained on a given
spatial resolution and applied to another spatial resolution. Training on a mix
of HR and LR images allows to get very good performances on the native HR
(rRMSE=0.06) and synthetic LR (rRMSE=0.10) images. However, very low
performances are still observed over the native LR images (rRMSE=0.48), mainly
due to the poor quality of the native LR images. Finally, an advanced
super-resolution method based on GAN (generative adversarial network) that
introduces additional textural information derived from the native HR images
was applied to the native LR validation images. Results show some significant
improvement (rRMSE=0.22) compared to bicubic up-sampling approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Velumani_K/0/1/0/all/0/1"&gt;Kaaviya Velumani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Lozano_R/0/1/0/all/0/1"&gt;Raul Lopez-Lozano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madec_S/0/1/0/all/0/1"&gt;Simon Madec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gillet_J/0/1/0/all/0/1"&gt;Joss Gillet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comar_A/0/1/0/all/0/1"&gt;Alexis Comar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baret_F/0/1/0/all/0/1"&gt;Frederic Baret&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Monocular Depth Estimation with Sparse Supervision on Mobile. (arXiv:2105.12053v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12053</id>
        <link href="http://arxiv.org/abs/2105.12053"/>
        <updated>2021-05-26T01:22:10.142Z</updated>
        <summary type="html"><![CDATA[Monocular (relative or metric) depth estimation is a critical task for
various applications, such as autonomous vehicles, augmented reality and image
editing. In recent years, with the increasing availability of mobile devices,
accurate and mobile-friendly depth models have gained importance. Increasingly
accurate models typically require more computational resources, which inhibits
the use of such models on mobile devices. The mobile use case is arguably the
most unrestricted one, which requires highly accurate yet mobile-friendly
architectures. Therefore, we try to answer the following question: How can we
improve a model without adding further complexity (i.e. parameters)? Towards
this end, we systematically explore the design space of a relative depth
estimation model from various dimensions and we show, with key design choices
and ablation studies, even an existing architecture can reach highly
competitive performance to the state of the art, with a fraction of the
complexity. Our study spans an in-depth backbone model selection process,
knowledge distillation, intermediate predictions, model pruning and loss
rebalancing. We show that our model, using only DIW as the supervisory dataset,
achieves 0.1156 WHDR on DIW with 2.6M parameters and reaches 37 FPS on a mobile
GPU, without pruning or hardware-specific optimization. A pruned version of our
model achieves 0.1208 WHDR on DIW with 1M parameters and reaches 44 FPS on a
mobile GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yucel_M/0/1/0/all/0/1"&gt;Mehmet Kerim Yucel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimaridou_V/0/1/0/all/0/1"&gt;Valia Dimaridou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drosou_A/0/1/0/all/0/1"&gt;Anastasios Drosou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saa_Garriga_A/0/1/0/all/0/1"&gt;Albert Sa&amp;#xe0;-Garriga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SBEVNet: End-to-End Deep Stereo Layout Estimation. (arXiv:2105.11705v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11705</id>
        <link href="http://arxiv.org/abs/2105.11705"/>
        <updated>2021-05-26T01:22:10.129Z</updated>
        <summary type="html"><![CDATA[Accurate layout estimation is crucial for planning and navigation in robotics
applications, such as self-driving. In this paper, we introduce the Stereo
Bird's Eye ViewNetwork (SBEVNet), a novel supervised end-to-end framework for
estimation of bird's eye view layout from a pair of stereo images. Although our
network reuses some of the building blocks from the state-of-the-art deep
learning networks for disparity estimation, we show that explicit depth
estimation is neither sufficient nor necessary. Instead, the learning of a good
internal bird's eye view feature representation is effective for layout
estimation. Specifically, we first generate a disparity feature volume using
the features of the stereo images and then project it to the bird's eye view
coordinates. This gives us coarse-grained information about the scene
structure. We also apply inverse perspective mapping (IPM) to map the input
images and their features to the bird's eye view. This gives us fine-grained
texture information. Concatenating IPM features with the projected feature
volume creates a rich bird's eye view representation which is useful for
spatial reasoning. We use this representation to estimate the BEV semantic map.
Additionally, we show that using the IPM features as a supervisory signal for
stereo features can give an improvement in performance. We demonstrate our
approach on two datasets:the KITTI dataset and a synthetically generated
dataset from the CARLA simulator. For both of these datasets, we establish
state-of-the-art performance compared to baseline techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1"&gt;Divam Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_W/0/1/0/all/0/1"&gt;Wei Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabor_T/0/1/0/all/0/1"&gt;Trenton Tabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1"&gt;Jeff Schneider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos. (arXiv:2105.11731v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11731</id>
        <link href="http://arxiv.org/abs/2105.11731"/>
        <updated>2021-05-26T01:22:10.081Z</updated>
        <summary type="html"><![CDATA[Detecting human-object interactions (HOI) is an important step toward a
comprehensive visual understanding of machines. While detecting non-temporal
HOIs (e.g., sitting on a chair) from static images is feasible, it is unlikely
even for humans to guess temporal-related HOIs (e.g., opening/closing a door)
from a single video frame, where the neighboring frames play an essential role.
However, conventional HOI methods operating on only static images have been
used to predict temporal-related interactions, which is essentially guessing
without temporal contexts and may lead to sub-optimal performance. In this
paper, we bridge this gap by detecting video-based HOIs with explicit temporal
information. We first show that a naive temporal-aware variant of a common
action detection baseline does not work on video-based HOIs due to a
feature-inconsistency issue. We then propose a simple yet effective
architecture named Spatial-Temporal HOI Detection (ST-HOI) utilizing temporal
information such as human and object trajectories, correctly-localized visual
features, and spatial-temporal masking pose features. We construct a new video
HOI benchmark dubbed VidHOI where our proposed approach serves as a solid
baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiou_M/0/1/0/all/0/1"&gt;Meng-Jiun Chiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1"&gt;Chun-Yu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li-Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roger Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Big data and big values: When companies need to rethink themselves. (arXiv:2105.12048v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.12048</id>
        <link href="http://arxiv.org/abs/2105.12048"/>
        <updated>2021-05-26T01:22:10.023Z</updated>
        <summary type="html"><![CDATA[In order to face the complexity of business environments and detect
priorities while triggering contingency strategies, we propose a new
methodological approach that combines text mining, social network and big data
analytics, with the assessment of stakeholders' attitudes towards company core
values. This approach was applied in a case study where we considered the
Twitter discourse about core values in Italy. We collected more than 94,000
tweets related to the core values of the firms listed in Fortune's ranking of
the World's Most Admired Companies (2013-2017). For the Italian scenario, we
found three predominant core values orientations (Customers, Employees and
Excellence) - which should be at the basis of any business strategy - and three
latent ones (Economic-Financial Growth, Citizenship and Social Responsibility),
which need periodic attention. Our contribution is mostly methodological and
extends the research on text mining and on online big data analytics applied in
complex business contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barchiesi_M/0/1/0/all/0/1"&gt;M. A. Barchiesi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Architecture Search with Random Labels. (arXiv:2101.11834v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11834</id>
        <link href="http://arxiv.org/abs/2101.11834"/>
        <updated>2021-05-26T01:22:10.014Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate a new variant of neural architecture search
(NAS) paradigm -- searching with random labels (RLNAS). The task sounds
counter-intuitive for most existing NAS algorithms since random label provides
few information on the performance of each candidate architecture. Instead, we
propose a novel NAS framework based on ease-of-convergence hypothesis, which
requires only random labels during searching. The algorithm involves two steps:
first, we train a SuperNet using random labels; second, from the SuperNet we
extract the sub-network whose weights change most significantly during the
training. Extensive experiments are evaluated on multiple datasets (e.g.
NAS-Bench-201 and ImageNet) and multiple search spaces (e.g. DARTS-like and
MobileNet-like). Very surprisingly, RLNAS achieves comparable or even better
results compared with state-of-the-art NAS methods such as PC-DARTS, Single
Path One-Shot, even though the counterparts utilize full ground truth labels
for searching. We hope our finding could inspire new understandings on the
essential of NAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuanyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1"&gt;Pengfei Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Visual Representation Learning by Online Constrained K-Means. (arXiv:2105.11527v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11527</id>
        <link href="http://arxiv.org/abs/2105.11527"/>
        <updated>2021-05-26T01:22:09.934Z</updated>
        <summary type="html"><![CDATA[Cluster discrimination is an effective pretext task for unsupervised
representation learning, which often consists of two phases: clustering and
discrimination. Clustering is to assign each instance a pseudo label that will
be used to learn representations in discrimination. The main challenge resides
in clustering since many prevalent clustering methods (e.g., k-means) have to
run in a batch mode that goes multiple iterations over the whole data.
Recently, a balanced online clustering method, i.e., SwAV, is proposed for
representation learning. However, the assignment is optimized within only a
small subset of data, which can be suboptimal. To address these challenges, we
first investigate the objective of clustering-based representation learning
from the perspective of distance metric learning. Based on this, we propose a
novel clustering-based pretext task with online \textbf{Co}nstrained
\textbf{K}-m\textbf{e}ans (\textbf{CoKe}) to learn representations and
relations between instances simultaneously. Compared with the balanced
clustering that each cluster has exactly the same size, we only constrain the
minimum size of clusters to flexibly capture the inherent data structure. More
importantly, our online assignment method has a theoretical guarantee to
approach the global optimum. Finally, two variance reduction strategies are
proposed to make the clustering robust for different augmentations. Without
keeping representations of instances, the data is accessed in an online mode in
CoKe while a single view of instances at each iteration is sufficient to
demonstrate a better performance than contrastive learning methods relying on
two views. Extensive experiments on ImageNet verify the efficacy of our
proposal. Code will be released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1"&gt;Qi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuanhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Juhua Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rong Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture of ELM based experts with trainable gating network. (arXiv:2105.11706v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11706</id>
        <link href="http://arxiv.org/abs/2105.11706"/>
        <updated>2021-05-26T01:22:09.886Z</updated>
        <summary type="html"><![CDATA[Mixture of experts method is a neural network based ensemble learning that
has great ability to improve the overall classification accuracy. This method
is based on the divide and conquer principle, in which the problem space is
divided between several experts by supervisition of gating network. In this
paper, we propose an ensemble learning method based on mixture of experts which
is named mixture of ELM based experts with trainable gating network (MEETG) to
improve the computing cost and to speed up the learning process of ME. The
structure of ME consists of multi layer perceptrons (MLPs) as base experts and
gating network, in which gradient-based learning algorithm is applied for
training the MLPs which is an iterative and time consuming process. In order to
overcome on these problems, we use the advantages of extreme learning machine
(ELM) for designing the structure of ME. ELM as a learning algorithm for single
hidden-layer feed forward neural networks provides much faster learning process
and better generalization ability in comparision with some other traditional
learning algorithms. Also, in the proposed method a trainable gating network is
applied to aggregate the outputs of the experts dynamically according to the
input sample. Our experimental results and statistical analysis on 11 benchmark
datasets confirm that MEETG has an acceptable performance in classification
problems. Furthermore, our experimental results show that the proposed approach
outperforms the original ELM on prediction stability and classification
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Armi_L/0/1/0/all/0/1"&gt;Laleh Armi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbasi_E/0/1/0/all/0/1"&gt;Elham Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zarepour_Ahmadabadi_J/0/1/0/all/0/1"&gt;Jamal Zarepour-Ahmadabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting malware threat intelligence using KGs. (arXiv:2102.05571v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05571</id>
        <link href="http://arxiv.org/abs/2102.05571"/>
        <updated>2021-05-26T01:22:09.842Z</updated>
        <summary type="html"><![CDATA[Large amounts of threat intelligence information about malware attacks are
available in disparate, typically unstructured, formats. Knowledge graphs can
capture this information and its context using RDF triples represented by
entities and relations. Sparse or inaccurate threat information, however, leads
to challenges such as incomplete or erroneous triples. Generic information
extraction (IE) models used to populate the knowledge graph cannot fully
guarantee domain-specific context. This paper proposes a system to generate a
Malware Knowledge Graph called MalKG, the first open-source automated knowledge
graph for malware threat intelligence. MalKG dataset (MT40K\footnote{ Anonymous
GitHub link: https://github.com/malkg-researcher/MalKG}) contains approximately
40,000 triples generated from 27,354 unique entities and 34 relations. For
ground truth, we manually curate a knowledge graph called MT3K, with 3,027
triples generated from 5,741 unique entities and 22 relations. We demonstrate
the intelligence prediction of MalKG using two use cases. Predicting malware
threat information using the benchmark model achieves 80.4 for the hits@10
metric (predicts the top 10 options for an information class), and 0.75 for the
MRR (mean reciprocal rank). We also propose an automated, contextual framework
for information extraction, both manually and automatically, at the sentence
level from 1,100 malware threat reports and from the common vulnerabilities and
exposures (CVE) database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_N/0/1/0/all/0/1"&gt;Nidhi Rastogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Sharmishtha Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christian_R/0/1/0/all/0/1"&gt;Ryan Christian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gridley_J/0/1/0/all/0/1"&gt;Jared Gridley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1"&gt;Mohammad Zaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1"&gt;Alex Gittens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1"&gt;Charu Aggarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN for Vision, KG for Relation: a Two-stage Deep Network for Zero-shot Action Recognition. (arXiv:2105.11789v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11789</id>
        <link href="http://arxiv.org/abs/2105.11789"/>
        <updated>2021-05-26T01:22:09.822Z</updated>
        <summary type="html"><![CDATA[Zero-shot action recognition can recognize samples of unseen classes that are
unavailable in training by exploring common latent semantic representation in
samples. However, most methods neglected the connotative relation and
extensional relation between the action classes, which leads to the poor
generalization ability of the zero-shot learning. Furthermore, the learned
classifier incline to predict the samples of seen class, which leads to poor
classification performance. To solve the above problems, we propose a two-stage
deep neural network for zero-shot action recognition, which consists of a
feature generation sub-network serving as the sampling stage and a graph
attention sub-network serving as the classification stage. In the sampling
stage, we utilize a generative adversarial networks (GAN) trained by action
features and word vectors of seen classes to synthesize the action features of
unseen classes, which can balance the training sample data of seen classes and
unseen classes. In the classification stage, we construct a knowledge graph
(KG) based on the relationship between word vectors of action classes and
related objects, and propose a graph convolution network (GCN) based on
attention mechanism, which dynamically updates the relationship between action
classes and objects, and enhances the generalization ability of zero-shot
learning. In both stages, we all use word vectors as bridges for feature
generation and classifier generalization from seen classes to unseen classes.
We compare our method with state-of-the-art methods on UCF101 and HMDB51
datasets. Experimental results show that our proposed method improves the
classification performance of the trained classifier and achieves higher
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Bin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1"&gt;Dehui Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shaofan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinghua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Baocai Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiaonan Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransLoc3D : Point Cloud based Large-scale Place Recognition using Adaptive Receptive Fields. (arXiv:2105.11605v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11605</id>
        <link href="http://arxiv.org/abs/2105.11605"/>
        <updated>2021-05-26T01:22:09.795Z</updated>
        <summary type="html"><![CDATA[Place recognition plays an essential role in the field of autonomous driving
and robot navigation. Although a number of point cloud based methods have been
proposed and achieved promising results, few of them take the size difference
of objects into consideration. For small objects like pedestrians and vehicles,
large receptive fields will capture unrelated information, while small
receptive fields would fail to encode complete geometric information for large
objects such as buildings. We argue that fixed receptive fields are not well
suited for place recognition, and propose a novel Adaptive Receptive Field
Module (ARFM), which can adaptively adjust the size of the receptive field
based on the input point cloud. We also present a novel network architecture,
named TransLoc3D, to obtain discriminative global descriptors of point clouds
for the place recognition task. TransLoc3D consists of a 3D sparse
convolutional module, an ARFM module, an external transformer network which
aims to capture long range dependency and a NetVLAD layer. Experiments show
that our method outperforms prior state-of-the-art results, with an improvement
of 1.1\% on average recall@1 on the Oxford RobotCar dataset, and 0.8\% on the
B.D. dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tian-Xing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yu-Kun Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Song-Hai Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep Spiking Neural Networks. (arXiv:2105.11654v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2105.11654</id>
        <link href="http://arxiv.org/abs/2105.11654"/>
        <updated>2021-05-26T01:22:09.789Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural
networks, have attracted great attentions from researchers and industry. The
most efficient way to train deep SNNs is through ANN-SNN conversion. However,
the conversion usually suffers from accuracy loss and long inference time,
which impede the practical application of SNN. In this paper, we theoretically
analyze ANN-SNN conversion and derive sufficient conditions of the optimal
conversion. To better correlate ANN-SNN and get greater accuracy, we propose
Rate Norm Layer to replace the ReLU activation function in source ANN training,
enabling direct conversion from a trained ANN to an SNN. Moreover, we propose
an optimal fit curve to quantify the fit between the activation value of source
ANN and the actual firing rate of target SNN. We show that the inference time
can be reduced by optimizing the upper bound of the fit curve in the revised
ANN to achieve fast inference. Our theory can explain the existing work on fast
reasoning and get better results. The experimental results show that the
proposed method achieves near loss less conversion with VGG-16,
PreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster
reasoning performance under 0.265x energy consumption of the typical method.
The code is available at
https://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jianhao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Criterion-based Heterogeneous Collaborative Filtering for Multi-behavior Implicit Recommendation. (arXiv:2105.11876v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.11876</id>
        <link href="http://arxiv.org/abs/2105.11876"/>
        <updated>2021-05-26T01:22:09.783Z</updated>
        <summary type="html"><![CDATA[With the increasing scale and diversification of interaction behaviors in
E-commerce, more and more researchers pay attention to multi-behavior
recommender systems that utilize interaction data of other auxiliary behaviors
such as view and cart. To address these challenges in heterogeneous scenarios,
non-sampling methods have shown superiority over negative sampling methods.
However, two observations are usually ignored in existing state-of-the-art
non-sampling methods based on binary regression: (1) users have different
preference strengths for different items, so they cannot be measured simply by
binary implicit data; (2) the dependency across multiple behaviors varies for
different users and items. To tackle the above issue, we propose a novel
non-sampling learning framework named \underline{C}riterion-guided
\underline{H}eterogeneous \underline{C}ollaborative \underline{F}iltering
(CHCF). CHCF introduces both upper and lower bounds to indicate selection
criteria, which will guide user preference learning. Besides, CHCF integrates
criterion learning and user preference learning into a unified framework, which
can be trained jointly for the interaction prediction on target behavior. We
further theoretically demonstrate that the optimization of Collaborative Metric
Learning can be approximately achieved by CHCF learning framework in a
non-sampling form effectively. Extensive experiments on two real-world datasets
show that CHCF outperforms the state-of-the-art methods in heterogeneous
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Daqing Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jinwen Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1"&gt;Minghua Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chen Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The advent and fall of a vocabulary learning bias from communicative efficiency. (arXiv:2105.11519v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11519</id>
        <link href="http://arxiv.org/abs/2105.11519"/>
        <updated>2021-05-26T01:22:09.776Z</updated>
        <summary type="html"><![CDATA[It is well-known that, when sufficiently young children encounter a new word,
they tend to attach it to a meaning that does not have a word yet in their
lexicon. In previous research, the strategy was shown to be optimal from an
information theoretic standpoint. However, the information theoretic model
employed neither explains the weakening of that vocabulary learning bias in
older children or polylinguals nor reproduces Zipf's meaning-frequency law,
namely the non-linear relationship between the number of meanings of a word and
its frequency. Here we consider a generalization of the model that is channeled
to reproduce that law. The analysis of the new model reveals regions of the
phase space where the bias disappears consistently with the weakening or loss
of the bias in older children or polylinguals. In the deep learning era, the
model is a transparent low-dimensional tool for future experimental research
and illustrates the predictive power of a theoretical framework originally
designed to shed light on the origins of Zipf's rank-frequency law.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carrera_Casado_D/0/1/0/all/0/1"&gt;David Carrera-Casado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1"&gt;Ramon Ferrer-i-Cancho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Accurate Scene Parsing via Bi-direction Alignment Networks. (arXiv:2105.11651v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11651</id>
        <link href="http://arxiv.org/abs/2105.11651"/>
        <updated>2021-05-26T01:22:09.771Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an effective method for fast and accurate scene
parsing called Bidirectional Alignment Network (BiAlignNet). Previously, one
representative work BiSeNet~\cite{bisenet} uses two different paths (Context
Path and Spatial Path) to achieve balanced learning of semantics and details,
respectively. However, the relationship between the two paths is not well
explored. We argue that both paths can benefit each other in a complementary
way. Motivated by this, we propose a novel network by aligning two-path
information into each other through a learned flow field. To avoid the noise
and semantic gaps, we introduce a Gated Flow Alignment Module to align both
features in a bidirectional way. Moreover, to make the Spatial Path learn more
detailed information, we present an edge-guided hard pixel mining loss to
supervise the aligned learning process. Our method achieves 80.1\% and 78.5\%
mIoU in validation and test set of Cityscapes while running at 30 FPS with full
resolution inputs. Code and models will be available at
\url{https://github.com/jojacola/BiAlignNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yanran Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangtai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yunhai Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1"&gt;Yang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1"&gt;Tao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1"&gt;Ruhui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1"&gt;Haibing Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoundarySqueeze: Image Segmentation as Boundary Squeezing. (arXiv:2105.11668v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11668</id>
        <link href="http://arxiv.org/abs/2105.11668"/>
        <updated>2021-05-26T01:22:09.755Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for fine-grained high-quality image segmentation of
both objects and scenes. Inspired by dilation and erosion from morphological
image processing techniques, we treat the pixel level segmentation problems as
squeezing object boundary. From this perspective, we propose \textbf{Boundary
Squeeze} module: a novel and efficient module that squeezes the object boundary
from both inner and outer directions which leads to precise mask
representation. To generate such squeezed representation, we propose a new
bidirectionally flow-based warping process and design specific loss signals to
supervise the learning process. Boundary Squeeze Module can be easily applied
to both instance and semantic segmentation tasks as a plug-and-play module by
building on top of existing models. We show that our simple yet effective
design can lead to high qualitative results on several different datasets and
we also provide several different metrics on boundary to prove the
effectiveness over previous work. Moreover, the proposed module is
light-weighted and thus has potential for practical usage. Our method yields
large gains on COCO, Cityscapes, for both instance and semantic segmentation
and outperforms previous state-of-the-art PointRend in both accuracy and speed
under the same setting. Code and model will be available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangtai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kuiyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jianping Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yunhai Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhengjun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_L/0/1/0/all/0/1"&gt;Lubin Weng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization. (arXiv:2105.12002v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12002</id>
        <link href="http://arxiv.org/abs/2105.12002"/>
        <updated>2021-05-26T01:22:09.750Z</updated>
        <summary type="html"><![CDATA[The Lottery Ticket Hypothesis suggests that an over-parametrized network
consists of "lottery tickets", and training a certain collection of them (i.e.,
a subnetwork) can match the performance of the full model. In this paper, we
study such a collection of tickets, which is referred to as "winning tickets",
in extremely over-parametrized models, e.g., pre-trained language models. We
observe that at certain compression ratios, generalization performance of the
winning tickets can not only match, but also exceed that of the full model. In
particular, we observe a phase transition phenomenon: As the compression ratio
increases, generalization performance of the winning tickets first improves
then deteriorates after a certain threshold. We refer to the tickets on the
threshold as "super tickets". We further show that the phase transition is task
and model dependent -- as model size becomes larger and training data set
becomes smaller, the transition becomes more pronounced. Our experiments on the
GLUE benchmark show that the super tickets improve single task fine-tuning by
$0.9$ points on BERT-base and $1.0$ points on BERT-large, in terms of
task-average score. We also demonstrate that adaptively sharing the super
tickets across tasks benefits multi-task learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1"&gt;Simiao Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minshuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TRACE: A Differentiable Approach to Line-level Stroke Recovery for Offline Handwritten Text. (arXiv:2105.11559v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11559</id>
        <link href="http://arxiv.org/abs/2105.11559"/>
        <updated>2021-05-26T01:22:09.743Z</updated>
        <summary type="html"><![CDATA[Stroke order and velocity are helpful features in the fields of signature
verification, handwriting recognition, and handwriting synthesis. Recovering
these features from offline handwritten text is a challenging and well-studied
problem. We propose a new model called TRACE (Trajectory Recovery by an
Adaptively-trained Convolutional Encoder). TRACE is a differentiable approach
that uses a convolutional recurrent neural network (CRNN) to infer temporal
stroke information from long lines of offline handwritten text with many
characters and dynamic time warping (DTW) to align predictions and ground truth
points. TRACE is perhaps the first system to be trained end-to-end on entire
lines of text of arbitrary width and does not require the use of dynamic
exemplars. Moreover, the system does not require images to undergo any
pre-processing, nor do the predictions require any post-processing.
Consequently, the recovered trajectory is differentiable and can be used as a
loss function for other tasks, including synthesizing offline handwritten text.

We demonstrate that temporal stroke information recovered by TRACE from
offline data can be used for handwriting synthesis and establish the first
benchmarks for a stroke trajectory recovery system trained on the IAM online
handwriting dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Archibald_T/0/1/0/all/0/1"&gt;Taylor Archibald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poggemann_M/0/1/0/all/0/1"&gt;Mason Poggemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Aaron Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_T/0/1/0/all/0/1"&gt;Tony Martinez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Transformer for Explainable Recommendation. (arXiv:2105.11601v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.11601</id>
        <link href="http://arxiv.org/abs/2105.11601"/>
        <updated>2021-05-26T01:22:09.736Z</updated>
        <summary type="html"><![CDATA[Personalization of natural language generation plays a vital role in a large
spectrum of tasks, such as explainable recommendation, review summarization and
dialog systems. In these tasks, user and item IDs are important identifiers for
personalization. Transformer, which is demonstrated with strong language
modeling capability, however, is not personalized and fails to make use of the
user and item IDs since the ID tokens are not even in the same semantic space
as the words. To address this problem, we present a PErsonalized Transformer
for Explainable Recommendation (PETER), on which we design a simple and
effective learning objective that utilizes the IDs to predict the words in the
target explanation, so as to endow the IDs with linguistic meanings and to
achieve personalized Transformer. Besides generating explanations, PETER can
also make recommendations, which makes it a unified model for the whole
recommendation-explanation pipeline. Extensive experiments show that our small
unpretrained model outperforms fine-tuned BERT on the generation task, in terms
of both effectiveness and efficiency, which highlights the importance and the
nice utility of our design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pan-sharpening via High-pass Modification Convolutional Neural Network. (arXiv:2105.11576v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11576</id>
        <link href="http://arxiv.org/abs/2105.11576"/>
        <updated>2021-05-26T01:22:09.730Z</updated>
        <summary type="html"><![CDATA[Most existing deep learning-based pan-sharpening methods have several widely
recognized issues, such as spectral distortion and insufficient spatial texture
enhancement, we propose a novel pan-sharpening convolutional neural network
based on a high-pass modification block. Different from existing methods, the
proposed block is designed to learn the high-pass information, leading to
enhance spatial information in each band of the multi-spectral-resolution
images. To facilitate the generation of visually appealing pan-sharpened
images, we propose a perceptual loss function and further optimize the model
based on high-level features in the near-infrared space. Experiments
demonstrate the superior performance of the proposed method compared to the
state-of-the-art pan-sharpening methods, both quantitatively and qualitatively.
The proposed model is open-sourced at https://github.com/jiaming-wang/HMB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhenfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruiqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiayi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taxonomy of academic plagiarism methods. (arXiv:2105.12068v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.12068</id>
        <link href="http://arxiv.org/abs/2105.12068"/>
        <updated>2021-05-26T01:22:09.723Z</updated>
        <summary type="html"><![CDATA[The article gives an overview of the plagiarism domain, with focus on
academic plagiarism. The article defines plagiarism, explains the origin of the
term, as well as plagiarism related terms. It identifies the extent of the
plagiarism domain and then focuses on the plagiarism subdomain of text
documents, for which it gives an overview of current classifications and
taxonomies and then proposes a more comprehensive classification according to
several criteria: their origin and purpose, technical implementation,
consequence, complexity of detection and according to the number of linguistic
sources. The article suggests the new classification of academic plagiarism,
describes sorts and methods of plagiarism, types and categories, approaches and
phases of plagiarism detection, the classification of methods and algorithms
for plagiarism detection. The title of the article explicitly targets the
academic community, but it is sufficiently general and interdisciplinary, so it
can be useful for many other professionals like software developers, linguists
and librarians.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vrbanec_T/0/1/0/all/0/1"&gt;Tedo Vrbanec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mestrovic_A/0/1/0/all/0/1"&gt;Ana Mestrovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards an Online Empathetic Chatbot with Emotion Causes. (arXiv:2105.11903v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11903</id>
        <link href="http://arxiv.org/abs/2105.11903"/>
        <updated>2021-05-26T01:22:09.707Z</updated>
        <summary type="html"><![CDATA[Existing emotion-aware conversational models usually focus on controlling the
response contents to align with a specific emotion class, whereas empathy is
the ability to understand and concern the feelings and experience of others.
Hence, it is critical to learn the causes that evoke the users' emotion for
empathetic responding, a.k.a. emotion causes. To gather emotion causes in
online environments, we leverage counseling strategies and develop an
empathetic chatbot to utilize the causal emotion information. On a real-world
online dataset, we verify the effectiveness of the proposed approach by
comparing our chatbot with several SOTA methods using automatic metrics,
expert-based human judgements as well as user-based online evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1"&gt;Hongke Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_x/0/1/0/all/0/1"&gt;xiaoqiang Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yalong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1"&gt;Jianwei Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaGCN:Adaptive Boosting Algorithm for Graph Convolutional Networks on Imbalanced Node Classification. (arXiv:2105.11625v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11625</id>
        <link href="http://arxiv.org/abs/2105.11625"/>
        <updated>2021-05-26T01:22:09.701Z</updated>
        <summary type="html"><![CDATA[The Graph Neural Network (GNN) has achieved remarkable success in graph data
representation. However, the previous work only considered the ideal balanced
dataset, and the practical imbalanced dataset was rarely considered, which, on
the contrary, is of more significance for the application of GNN. Traditional
methods such as resampling, reweighting and synthetic samples that deal with
imbalanced datasets are no longer applicable in GNN. Ensemble models can handle
imbalanced datasets better compared with single estimator. Besides, ensemble
learning can achieve higher estimation accuracy and has better reliability
compared with the single estimator. In this paper, we propose an ensemble model
called AdaGCN, which uses a Graph Convolutional Network (GCN) as the base
estimator during adaptive boosting. In AdaGCN, a higher weight will be set for
the training samples that are not properly classified by the previous
classifier, and transfer learning is used to reduce computational cost and
increase fitting capability. Experiments show that the AdaGCN model we proposed
achieves better performance than GCN, GraphSAGE, GAT, N-GCN and the most of
advanced reweighting and resampling methods on synthetic imbalanced datasets,
with an average improvement of 4.3%. Our model also improves state-of-the-art
baselines on all of the challenging node classification tasks we consider:
Cora, Citeseer, Pubmed, and NELL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;S. Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;L. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;J. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Sentiment Analysis by Transferring Multi-source Knowledge. (arXiv:2105.11902v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11902</id>
        <link href="http://arxiv.org/abs/2105.11902"/>
        <updated>2021-05-26T01:22:09.694Z</updated>
        <summary type="html"><![CDATA[Sentiment analysis (SA) is an important research area in cognitive
computation-thus in-depth studies of patterns of sentiment analysis are
necessary. At present, rich resource data-based SA has been well developed,
while the more challenging and practical multi-source unsupervised SA (i.e. a
target domain SA by transferring from multiple source domains) is seldom
studied. The challenges behind this problem mainly locate in the lack of
supervision information, the semantic gaps among domains (i.e., domain shifts),
and the loss of knowledge. However, existing methods either lack the
distinguishable capacity of the semantic gaps among domains or lose private
knowledge. To alleviate these problems, we propose a two-stage domain
adaptation framework. In the first stage, a multi-task methodology-based
shared-private architecture is employed to explicitly model the domain common
features and the domain-specific features for the labeled source domains. In
the second stage, two elaborate mechanisms are embedded in the shared private
architecture to transfer knowledge from multiple source domains. The first
mechanism is a selective domain adaptation (SDA) method, which transfers
knowledge from the closest source domain. And the second mechanism is a
target-oriented ensemble (TOE) method, in which knowledge is transferred
through a well-designed ensemble method. Extensive experiment evaluations
verify that the performance of the proposed framework outperforms unsupervised
state-of-the-art competitors. What can be concluded from the experiments is
that transferring from very different distributed source domains may degrade
the target-domain performance, and it is crucial to choose the proper source
domains to transfer from.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Hongguang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Discourse Resources with Dependency Framework. (arXiv:2101.00167v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00167</id>
        <link href="http://arxiv.org/abs/2101.00167"/>
        <updated>2021-05-26T01:22:09.688Z</updated>
        <summary type="html"><![CDATA[For text-level discourse analysis, there are various discourse schemes but
relatively few labeled data, because discourse research is still immature and
it is labor-intensive to annotate the inner logic of a text. In this paper, we
attempt to unify multiple Chinese discourse corpora under different annotation
schemes with discourse dependency framework by designing semi-automatic methods
to convert them into dependency structures. We also implement several benchmark
dependency parsers and research on how they can leverage the unified data to
improve performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sujian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yueyuan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoRT: Complementary Rankings from Transformers. (arXiv:2010.10252v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10252</id>
        <link href="http://arxiv.org/abs/2010.10252"/>
        <updated>2021-05-26T01:22:09.682Z</updated>
        <summary type="html"><![CDATA[Many recent approaches towards neural information retrieval mitigate their
computational costs by using a multi-stage ranking pipeline. In the first
stage, a number of potentially relevant candidates are retrieved using an
efficient retrieval model such as BM25. Although BM25 has proven decent
performance as a first-stage ranker, it tends to miss relevant passages. In
this context we propose CoRT, a simple neural first-stage ranking model that
leverages contextual representations from pretrained language models such as
BERT to complement term-based ranking functions while causing no significant
delay at query time. Using the MS MARCO dataset, we show that CoRT
significantly increases the candidate recall by complementing BM25 with missing
candidates. Consequently, we find subsequent re-rankers achieve superior
results with less candidates. We further demonstrate that passage retrieval
using CoRT can be realized with surprisingly low latencies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wrzalik_M/0/1/0/all/0/1"&gt;Marco Wrzalik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krechel_D/0/1/0/all/0/1"&gt;Dirk Krechel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation. (arXiv:2105.11494v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11494</id>
        <link href="http://arxiv.org/abs/2105.11494"/>
        <updated>2021-05-26T01:22:09.665Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a method for coarse camera pose computation which
is robust to viewing conditions and does not require a detailed model of the
scene. This method meets the growing need of easy deployment of robotics or
augmented reality applications in any environments, especially those for which
no accurate 3D model nor huge amount of ground truth data are available. It
exploits the ability of deep learning techniques to reliably detect objects
regardless of viewing conditions. Previous works have also shown that
abstracting the geometry of a scene of objects by an ellipsoid cloud allows to
compute the camera pose accurately enough for various application needs. Though
promising, these approaches use the ellipses fitted to the detection bounding
boxes as an approximation of the imaged objects. In this paper, we go one step
further and propose a learning-based method which detects improved elliptic
approximations of objects which are coherent with the 3D ellipsoid in terms of
perspective projection. Experiments prove that the accuracy of the computed
pose significantly increases thanks to our method and is more robust to the
variability of the boundaries of the detection boxes. This is achieved with
very little effort in terms of training data acquisition -- a few hundred
calibrated images of which only three need manual object annotation. Code and
models are released at
https://github.com/zinsmatt/3D-Aware-Ellipses-for-Visual-Localization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zins_M/0/1/0/all/0/1"&gt;Matthieu Zins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_G/0/1/0/all/0/1"&gt;Gilles Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1"&gt;Marie-Odile Berger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating Redundancy in Clinical Text. (arXiv:2105.11832v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11832</id>
        <link href="http://arxiv.org/abs/2105.11832"/>
        <updated>2021-05-26T01:22:09.659Z</updated>
        <summary type="html"><![CDATA[The current mode of use of Electronic Health Record (EHR) elicits text
redundancy. Clinicians often populate new documents by duplicating existing
notes, then updating accordingly. Data duplication can lead to a propagation of
errors, inconsistencies and misreporting of care. Therefore, quantifying
information redundancy can play an essential role in evaluating innovations
that operate on clinical narratives.

This work is a quantitative examination of information redundancy in EHR
notes. We present and evaluate two strategies to measure redundancy: an
information-theoretic approach and a lexicosyntactic and semantic model. We
evaluate the measures by training large Transformer-based language models using
clinical text from a large openly available US-based ICU dataset and a large
multi-site UK based Trust. By comparing the information-theoretic content of
the trained models with open-domain language models, the language models
trained using clinical text have shown ~1.5x to ~3x less efficient than
open-domain corpora. Manual evaluation shows a high correlation with
lexicosyntactic and semantic redundancy, with averages ~43 to ~65%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Searle_T/0/1/0/all/0/1"&gt;Thomas Searle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1"&gt;Zina Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1"&gt;James Teo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1"&gt;Richard JB Dobson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Learning of Generation and Classification for Emotion-Aware Dialogue Response Generation. (arXiv:2105.11696v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11696</id>
        <link href="http://arxiv.org/abs/2105.11696"/>
        <updated>2021-05-26T01:22:09.654Z</updated>
        <summary type="html"><![CDATA[For a computer to naturally interact with a human, it needs to be human-like.
In this paper, we propose a neural response generation model with multi-task
learning of generation and classification, focusing on emotion. Our model based
on BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model,
is trained to generate responses and recognize emotions simultaneously.
Furthermore, we weight the losses for the tasks to control the update of
parameters. Automatic evaluations and crowdsourced manual evaluations show that
the proposed model makes generated responses more emotionally aware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ide_T/0/1/0/all/0/1"&gt;Tatsuya Ide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawahara_D/0/1/0/all/0/1"&gt;Daisuke Kawahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference. (arXiv:2105.11618v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11618</id>
        <link href="http://arxiv.org/abs/2105.11618"/>
        <updated>2021-05-26T01:22:09.648Z</updated>
        <summary type="html"><![CDATA[Existing pre-trained language models (PLMs) are often computationally
expensive in inference, making them impractical in various resource-limited
real-world applications. To address this issue, we propose a dynamic token
reduction approach to accelerate PLMs' inference, named TR-BERT, which could
flexibly adapt the layer number of each token in inference to avoid redundant
calculation. Specially, TR-BERT formulates the token reduction process as a
multi-step token selection problem and automatically learns the selection
strategy via reinforcement learning. The experimental results on several
downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to
satisfy various performance demands. Moreover, TR-BERT can also achieve better
performance with less computation in a suite of long-text tasks since its
token-level layer number adaption greatly accelerates the self-attention
operation in PLMs. The source code and experiment details of this paper can be
obtained from https://github.com/thunlp/TR-BERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1"&gt;Deming Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yankai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yufei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending the Abstraction of Personality Types based on MBTI with Machine Learning and Natural Language Processing. (arXiv:2105.11798v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11798</id>
        <link href="http://arxiv.org/abs/2105.11798"/>
        <updated>2021-05-26T01:22:09.639Z</updated>
        <summary type="html"><![CDATA[A data-centric approach with Natural Language Processing (NLP) to predict
personality types based on the MBTI (an introspective self-assessment
questionnaire that indicates different psychological preferences about how
people perceive the world and make decisions) through systematic enrichment of
text representation, based on the domain of the area, under the generation of
features based on three types of analysis: sentimental, grammatical and
aspects. The experimentation had a robust baseline of stacked models, with
premature optimization of hyperparameters through grid search, with gradual
feedback, for each of the four classifiers (dichotomies) of MBTI. The results
showed that attention to the data iteration loop focused on quality,
explanatory power and representativeness for the abstraction of more
relevant/important resources for the studied phenomenon made it possible to
improve the evaluation metrics results more quickly and less costly than
complex models such as the LSTM or state of the art ones as BERT, as well as
the importance of these results by comparisons made from various perspectives.
In addition, the study demonstrated a broad spectrum for the evolution and
deepening of the task and possible approaches for a greater extension of the
abstraction of personality types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Basto_C/0/1/0/all/0/1"&gt;Carlos Basto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoRT: Complementary Rankings from Transformers. (arXiv:2010.10252v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10252</id>
        <link href="http://arxiv.org/abs/2010.10252"/>
        <updated>2021-05-26T01:22:09.612Z</updated>
        <summary type="html"><![CDATA[Many recent approaches towards neural information retrieval mitigate their
computational costs by using a multi-stage ranking pipeline. In the first
stage, a number of potentially relevant candidates are retrieved using an
efficient retrieval model such as BM25. Although BM25 has proven decent
performance as a first-stage ranker, it tends to miss relevant passages. In
this context we propose CoRT, a simple neural first-stage ranking model that
leverages contextual representations from pretrained language models such as
BERT to complement term-based ranking functions while causing no significant
delay at query time. Using the MS MARCO dataset, we show that CoRT
significantly increases the candidate recall by complementing BM25 with missing
candidates. Consequently, we find subsequent re-rankers achieve superior
results with less candidates. We further demonstrate that passage retrieval
using CoRT can be realized with surprisingly low latencies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wrzalik_M/0/1/0/all/0/1"&gt;Marco Wrzalik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krechel_D/0/1/0/all/0/1"&gt;Dirk Krechel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for Key Information Extraction from Documents. (arXiv:2105.11672v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11672</id>
        <link href="http://arxiv.org/abs/2105.11672"/>
        <updated>2021-05-26T01:22:09.605Z</updated>
        <summary type="html"><![CDATA[Recent grid-based document representations like BERTgrid allow the
simultaneous encoding of the textual and layout information of a document in a
2D feature map so that state-of-the-art image segmentation and/or object
detection models can be straightforwardly leveraged to extract key information
from documents. However, such methods have not achieved comparable performance
to state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK
yet. In this paper, we propose a new multi-modal backbone network by
concatenating a BERTgrid to an intermediate layer of a CNN model, where the
input of CNN is a document image and the BERTgrid is a grid of word embeddings,
to generate a more powerful grid-based document representation, named
ViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal
backbone network are trained jointly. Our experimental results demonstrate that
this joint training strategy improves significantly the representation ability
of ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction
approach has achieved state-of-the-art performance on real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weihong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qifang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhuoyao Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1"&gt;Kai Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1"&gt;Qin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1"&gt;Qiang Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BATS: A Spectral Biclustering Approach to Single Document Topic Modeling and Segmentation. (arXiv:2008.02218v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02218</id>
        <link href="http://arxiv.org/abs/2008.02218"/>
        <updated>2021-05-26T01:22:09.598Z</updated>
        <summary type="html"><![CDATA[Existing topic modeling and text segmentation methodologies generally require
large datasets for training, limiting their capabilities when only small
collections of text are available. In this work, we reexamine the inter-related
problems of "topic identification" and "text segmentation" for sparse document
learning, when there is a single new text of interest. In developing a
methodology to handle single documents, we face two major challenges. First is
sparse information: with access to only one document, we cannot train
traditional topic models or deep learning algorithms. Second is significant
noise: a considerable portion of words in any single document will produce only
noise and not help discern topics or segments. To tackle these issues, we
design an unsupervised, computationally efficient methodology called BATS:
Biclustering Approach to Topic modeling and Segmentation. BATS leverages three
key ideas to simultaneously identify topics and segment text: (i) a new
mechanism that uses word order information to reduce sample complexity, (ii) a
statistically sound graph-based biclustering technique that identifies latent
structures of words and sentences, and (iii) a collection of effective
heuristics that remove noise words and award important words to further improve
performance. Experiments on four datasets show that our approach outperforms
several state-of-the-art baselines when considering topic coherence, topic
diversity, segmentation, and runtime comparison metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qiong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_A/0/1/0/all/0/1"&gt;Adam Hare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sirui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1"&gt;Yuwei Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1"&gt;Christopher G. Brinton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanhua Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experimenting with Knowledge Distillation techniques for performing Brain Tumor Segmentation. (arXiv:2105.11486v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11486</id>
        <link href="http://arxiv.org/abs/2105.11486"/>
        <updated>2021-05-26T01:22:09.591Z</updated>
        <summary type="html"><![CDATA[Multi-modal magnetic resonance imaging (MRI) is a crucial method for
analyzing the human brain. It is usually used for diagnosing diseases and for
making valuable decisions regarding the treatments - for instance, checking for
gliomas in the human brain. With varying degrees of severity and detection,
properly diagnosing gliomas is one of the most daunting and significant
analysis tasks in modern-day medicine. Our primary focus is on working with
different approaches to perform the segmentation of brain tumors in multimodal
MRI scans. Now, the quantity, variability of the data used for training has
always been considered to be crucial for developing excellent models. Hence, we
also want to experiment with Knowledge Distillation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nalwade_A/0/1/0/all/0/1"&gt;Ashwin Nalwade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kisa_J/0/1/0/all/0/1"&gt;Jackie Kisa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Focus Attention: Promoting Faithfulness and Diversity in Summarization. (arXiv:2105.11921v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11921</id>
        <link href="http://arxiv.org/abs/2105.11921"/>
        <updated>2021-05-26T01:22:09.584Z</updated>
        <summary type="html"><![CDATA[Professional summaries are written with document-level information, such as
the theme of the document, in mind. This is in contrast with most seq2seq
decoders which simultaneously learn to focus on salient content, while deciding
what to generate, at each decoding step. With the motivation to narrow this
gap, we introduce Focus Attention Mechanism, a simple yet effective method to
encourage decoders to proactively generate tokens that are similar or topical
to the input document. Further, we propose a Focus Sampling method to enable
generation of diverse summaries, an area currently understudied in
summarization. When evaluated on the BBC extreme summarization task, two
state-of-the-art models augmented with Focus Attention generate summaries that
are closer to the target and more faithful to their input documents,
outperforming their vanilla counterparts on \rouge and multiple faithfulness
measures. We also empirically demonstrate that Focus Sampling is more effective
in generating diverse and faithful summaries than top-$k$ or nucleus
sampling-based decoding methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1"&gt;Rahul Aralikatte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1"&gt;Shashi Narayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1"&gt;Joshua Maynez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rothe_S/0/1/0/all/0/1"&gt;Sascha Rothe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1"&gt;Ryan McDonald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Argument Undermining: Counter-Argument Generation by Attacking Weak Premises. (arXiv:2105.11752v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11752</id>
        <link href="http://arxiv.org/abs/2105.11752"/>
        <updated>2021-05-26T01:22:09.565Z</updated>
        <summary type="html"><![CDATA[Text generation has received a lot of attention in computational
argumentation research as of recent. A particularly challenging task is the
generation of counter-arguments. So far, approaches primarily focus on
rebutting a given conclusion, yet other ways to counter an argument exist. In
this work, we go beyond previous research by exploring argument undermining,
that is, countering an argument by attacking one of its premises. We
hypothesize that identifying the argument's weak premises is key to effective
countering. Accordingly, we propose a pipeline approach that first assesses the
premises' strength and then generates a counter-argument targeting the weak
ones. On the one hand, both manual and automatic evaluation proves the
importance of identifying weak premises in counter-argument generation. On the
other hand, when considering correctness and content richness, human annotators
favored our approach over state-of-the-art counter-argument generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alshomary_M/0/1/0/all/0/1"&gt;Milad Alshomary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1"&gt;Shahbaz Syed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1"&gt;Martin Potthast&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1"&gt;Henning Wachsmuth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator. (arXiv:2105.11589v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11589</id>
        <link href="http://arxiv.org/abs/2105.11589"/>
        <updated>2021-05-26T01:22:09.560Z</updated>
        <summary type="html"><![CDATA[Interactive robots navigating photo-realistic environments face challenges
underlying vision-and-language navigation (VLN), but in addition, they need to
be trained to handle the dynamic nature of dialogue. However, research in
Cooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts
with a guide in natural language in order to reach a goal, treats the dialogue
history as a VLN-style static instruction. In this paper, we present VISITRON,
a navigator better suited to the interactive regime inherent to CVDN by being
trained to: i) identify and associate object-level concepts and semantics
between the environment and dialogue history, ii) identify when to interact vs.
navigate via imitation learning of a binary classification head. We perform
extensive ablations with VISITRON to gain empirical insights and improve
performance on CVDN. VISITRON is competitive with models on the static CVDN
leaderboard. We also propose a generalized interactive regime to fine-tune and
evaluate VISITRON and future such models with pre-trained guides for
adaptability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Ayush Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Karthik Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1"&gt;Robinson Piramuthu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1"&gt;Gokhan T&amp;#xfc;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1"&gt;Dilek Hakkani-T&amp;#xfc;r&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions. (arXiv:2105.11644v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11644</id>
        <link href="http://arxiv.org/abs/2105.11644"/>
        <updated>2021-05-26T01:22:09.553Z</updated>
        <summary type="html"><![CDATA[Knowledge base question answering (KBQA) aims to answer a question over a
knowledge base (KB). Recently, a large number of studies focus on semantically
or syntactically complicated questions. In this paper, we elaborately summarize
the typical challenges and solutions for complex KBQA. We begin with
introducing the background about the KBQA task. Next, we present the two
mainstream categories of methods for complex KBQA, namely semantic
parsing-based (SP-based) methods and information retrieval-based (IR-based)
methods. We then review the advanced methods comprehensively from the
perspective of the two categories. Specifically, we explicate their solutions
to the typical challenges. Finally, we conclude and discuss some promising
directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yunshi Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1"&gt;Gaole He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jinhao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Better Visual Dialog Agents with Pretrained Visual-Linguistic Representation. (arXiv:2105.11541v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11541</id>
        <link href="http://arxiv.org/abs/2105.11541"/>
        <updated>2021-05-26T01:22:09.546Z</updated>
        <summary type="html"><![CDATA[GuessWhat?! is a two-player visual dialog guessing game where player A asks a
sequence of yes/no questions (Questioner) and makes a final guess (Guesser)
about a target object in an image, based on answers from player B (Oracle).
Based on this dialog history between the Questioner and the Oracle, a Guesser
makes a final guess of the target object. Previous baseline Oracle model
encodes no visual information in the model, and it cannot fully understand
complex questions about color, shape, relationships and so on. Most existing
work for Guesser encode the dialog history as a whole and train the Guesser
models from scratch on the GuessWhat?! dataset. This is problematic since
language encoder tend to forget long-term history and the GuessWhat?! data is
sparse in terms of learning visual grounding of objects. Previous work for
Questioner introduces state tracking mechanism into the model, but it is
learned as a soft intermediates without any prior vision-linguistic insights.
To bridge these gaps, in this paper we propose Vilbert-based Oracle, Guesser
and Questioner, which are all built on top of pretrained vision-linguistic
model, Vilbert. We introduce two-way background/target fusion mechanism into
Vilbert-Oracle to account for both intra and inter-object questions. We propose
a unified framework for Vilbert-Guesser and Vilbert-Questioner, where
state-estimator is introduced to best utilize Vilbert's power on single-turn
referring expression comprehension. Experimental results show that our proposed
models outperform state-of-the-art models significantly by 7%, 10%, 12% for
Oracle, Guesser and End-to-End Questioner respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1"&gt;Tao Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_Q/0/1/0/all/0/1"&gt;Qing Ping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1"&gt;Govind Thattai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1"&gt;Gokhan Tur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1"&gt;Prem Natarajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positional Artefacts Propagate Through Masked Language Model Embeddings. (arXiv:2011.04393v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04393</id>
        <link href="http://arxiv.org/abs/2011.04393"/>
        <updated>2021-05-26T01:22:09.538Z</updated>
        <summary type="html"><![CDATA[In this work, we demonstrate that the contextualized word vectors derived
from pretrained masked language model-based encoders share a common, perhaps
undesirable pattern across layers. Namely, we find cases of persistent outlier
neurons within BERT and RoBERTa's hidden state vectors that consistently bear
the smallest or largest values in said vectors. In an attempt to investigate
the source of this information, we introduce a neuron-level analysis method,
which reveals that the outliers are closely related to information captured by
positional embeddings. We also pre-train the RoBERTa-base models from scratch
and find that the outliers disappear without using positional embeddings. These
outliers, we find, are the major cause of anisotropy of encoders' raw vector
spaces, and clipping them leads to increased similarity across vectors. We
demonstrate this in practice by showing that clipped vectors can more
accurately distinguish word senses, as well as lead to better sentence
embeddings when mean pooling. In three supervised tasks, we find that clipping
does not affect the performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Ziyang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulmizev_A/0/1/0/all/0/1"&gt;Artur Kulmizev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xiaoxi Mao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds. (arXiv:2010.00685v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00685</id>
        <link href="http://arxiv.org/abs/2010.00685"/>
        <updated>2021-05-26T01:22:09.520Z</updated>
        <summary type="html"><![CDATA[We seek to create agents that both act and communicate with other agents in
pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019) -- a
large-scale crowd-sourced fantasy text-game -- with a dataset of quests. These
contain natural language motivations paired with in-game goals and human
demonstrations; completing a quest might require dialogue or actions (or both).
We introduce a reinforcement learning system that (1) incorporates large-scale
language modeling-based and commonsense reasoning-based pre-training to imbue
the agent with relevant priors; and (2) leverages a factorized action space of
action commands and dialogue, balancing between the two. We conduct zero-shot
evaluations using held-out human expert demonstrations, showing that our agents
are able to act consistently and talk naturally with respect to their
motivations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1"&gt;Prithviraj Ammanabrolu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urbanek_J/0/1/0/all/0/1"&gt;Jack Urbanek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Margaret Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1"&gt;Arthur Szlam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1"&gt;Tim Rockt&amp;#xe4;schel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending rational models of communication from beliefs to actions. (arXiv:2105.11950v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11950</id>
        <link href="http://arxiv.org/abs/2105.11950"/>
        <updated>2021-05-26T01:22:09.513Z</updated>
        <summary type="html"><![CDATA[Speakers communicate to influence their partner's beliefs and shape their
actions. Belief- and action-based objectives have been explored independently
in recent computational models, but it has been challenging to explicitly
compare or integrate them. Indeed, we find that they are conflated in standard
referential communication tasks. To distinguish these accounts, we introduce a
new paradigm called signaling bandits, generalizing classic Lewis signaling
games to a multi-armed bandit setting where all targets in the context have
some relative value. We develop three speaker models: a belief-oriented speaker
with a purely informative objective; an action-oriented speaker with an
instrumental objective; and a combined speaker which integrates the two by
inducing listener beliefs that generally lead to desirable actions. We then
present a series of simulations demonstrating that grounding production choices
in future listener actions results in relevance effects and flexible uses of
nonliteral language. More broadly, our findings suggest that language games
based on richer decision problems are a promising avenue for insight into
rational communication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sumers_T/0/1/0/all/0/1"&gt;Theodore R. Sumers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1"&gt;Robert D. Hawkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1"&gt;Mark K. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1"&gt;Thomas L. Griffiths&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHD360: A Benchmark Dataset for Salient Human Detection in 360{\deg} Videos. (arXiv:2105.11578v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11578</id>
        <link href="http://arxiv.org/abs/2105.11578"/>
        <updated>2021-05-26T01:22:09.507Z</updated>
        <summary type="html"><![CDATA[Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of
great importance for various applications such as robotics, inter-human and
human-object interaction in augmented reality. However, 360{\deg} video SHD has
been seldom discussed in the computer vision community due to a lack of
datasets with large-scale omnidirectional videos and rich annotations. To this
end, we propose SHD360, the first 360{\deg} video SHD dataset collecting
various real-life daily scenes, providing six-level hierarchical annotations
for 6,268 key frames uniformly sampled from 37,403 omnidirectional video frames
at 4K resolution. Specifically, each collected key frame is labeled with a
super-class, a sub-class, associated attributes (e.g., geometrical distortion),
bounding boxes and per-pixel object-/instance-level masks. As a result, our
SHD360 contains totally 16,238 salient human instances with manually annotated
pixel-wise ground truth. Since so far there is no method proposed for 360{\deg}
SHD, we systematically benchmark 11 representative state-of-the-art salient
object detection (SOD) approaches on our SHD360, and explore key issues derived
from extensive experimenting results. We hope our proposed dataset and
benchmark could serve as a good starting point for advancing human-centric
researches towards 360{\deg} panoramic data. Our dataset and benchmark will be
publicly available at https://github.com/PanoAsh/SHD360.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1"&gt;Olivier Deforges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Adapters for Cross-lingual Low-resource Speech Recognition. (arXiv:2105.11905v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11905</id>
        <link href="http://arxiv.org/abs/2105.11905"/>
        <updated>2021-05-26T01:22:09.498Z</updated>
        <summary type="html"><![CDATA[Cross-lingual speech adaptation aims to solve the problem of leveraging
multiple rich-resource languages to build models for a low-resource target
language. Since the low-resource language has limited training data, speech
recognition models can easily overfit. In this paper, we propose to use
adapters to investigate the performance of multiple adapters for
parameter-efficient cross-lingual speech adaptation. Based on our previous
MetaAdapter that implicitly leverages adapters, we propose a novel algorithms
called SimAdapter for explicitly learning knowledge from adapters. Our
algorithm leverages adapters which can be easily integrated into the
Transformer structure.MetaAdapter leverages meta-learning to transfer the
general knowledge from training data to the test language. SimAdapter aims to
learn the similarities between the source and target languages during
fine-tuning using the adapters. We conduct extensive experiments on
five-low-resource languages in Common Voice dataset. Results demonstrate that
our MetaAdapter and SimAdapter methods can reduce WER by 2.98% and 2.55% with
only 2.5% and 15.5% of trainable parameters compared to the strong full-model
fine-tuning baseline. Moreover, we also show that these two novel algorithms
can be integrated for better performance with up to 3.55% relative WER
reduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1"&gt;Wenxin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Han Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yidong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renjun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1"&gt;Takahiro Shinozaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The incel lexicon: Deciphering the emergent cryptolect of a global misogynistic community. (arXiv:2105.12006v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.12006</id>
        <link href="http://arxiv.org/abs/2105.12006"/>
        <updated>2021-05-26T01:22:09.490Z</updated>
        <summary type="html"><![CDATA[Evolving out of a gender-neutral framing of an involuntary celibate identity,
the concept of `incels' has come to refer to an online community of men who
bear antipathy towards themselves, women, and society-at-large for their
perceived inability to find and maintain sexual relationships. By exploring
incel language use on Reddit, a global online message board, we contextualize
the incel community's online expressions of misogyny and real-world acts of
violence perpetrated against women. After assembling around three million
comments from incel-themed Reddit channels, we analyze the temporal dynamics of
a data driven rank ordering of the glossary of phrases belonging to an emergent
incel lexicon. Our study reveals the generation and normalization of an
extensive coded misogynist vocabulary in service of the group's identity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gothard_K/0/1/0/all/0/1"&gt;Kelly Gothard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dewhurst_D/0/1/0/all/0/1"&gt;David Rushing Dewhurst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minot_J/0/1/0/all/0/1"&gt;Joshua R. Minot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1"&gt;Jane Lydia Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1"&gt;Christopher M. Danforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1"&gt;Peter Sheridan Dodds&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator. (arXiv:2105.11589v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11589</id>
        <link href="http://arxiv.org/abs/2105.11589"/>
        <updated>2021-05-26T01:22:09.481Z</updated>
        <summary type="html"><![CDATA[Interactive robots navigating photo-realistic environments face challenges
underlying vision-and-language navigation (VLN), but in addition, they need to
be trained to handle the dynamic nature of dialogue. However, research in
Cooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts
with a guide in natural language in order to reach a goal, treats the dialogue
history as a VLN-style static instruction. In this paper, we present VISITRON,
a navigator better suited to the interactive regime inherent to CVDN by being
trained to: i) identify and associate object-level concepts and semantics
between the environment and dialogue history, ii) identify when to interact vs.
navigate via imitation learning of a binary classification head. We perform
extensive ablations with VISITRON to gain empirical insights and improve
performance on CVDN. VISITRON is competitive with models on the static CVDN
leaderboard. We also propose a generalized interactive regime to fine-tune and
evaluate VISITRON and future such models with pre-trained guides for
adaptability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Ayush Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Karthik Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1"&gt;Robinson Piramuthu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1"&gt;Gokhan T&amp;#xfc;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1"&gt;Dilek Hakkani-T&amp;#xfc;r&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting. (arXiv:2105.11698v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11698</id>
        <link href="http://arxiv.org/abs/2105.11698"/>
        <updated>2021-05-26T01:22:09.463Z</updated>
        <summary type="html"><![CDATA[This paper explores the task of Difficulty-Controllable Question Generation
(DCQG), which aims at generating questions with required difficulty levels.
Previous research on this task mainly defines the difficulty of a question as
whether it can be correctly answered by a Question Answering (QA) system,
lacking interpretability and controllability. In our work, we redefine question
difficulty as the number of inference steps required to answer it and argue
that Question Generation (QG) systems should have stronger control over the
logic of generated questions. To this end, we propose a novel framework that
progressively increases question difficulty through step-by-step rewriting
under the guidance of an extracted reasoning chain. A dataset is automatically
constructed to facilitate the research, on which extensive experiments are
conducted to test the performance of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Siyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1"&gt;Ruihui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sujian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chenghua Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Ethical Limits of Natural Language Processing on Legal Text. (arXiv:2105.02751v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02751</id>
        <link href="http://arxiv.org/abs/2105.02751"/>
        <updated>2021-05-26T01:22:09.456Z</updated>
        <summary type="html"><![CDATA[Natural language processing (NLP) methods for analyzing legal text offer
legal scholars and practitioners a range of tools allowing to empirically
analyze law on a large scale. However, researchers seem to struggle when it
comes to identifying ethical limits to using NLP systems for acquiring genuine
insights both about the law and the systems' predictive capacity. In this paper
we set out a number of ways in which to think systematically about such issues.
We place emphasis on three crucial normative parameters which have, to the best
of our knowledge, been underestimated by current debates: (a) the importance of
academic freedom, (b) the existence of a wide diversity of legal and ethical
norms domestically but even more so internationally and (c) the threat of
moralism in research related to computational law. For each of these three
parameters we provide specific recommendations for the legal NLP community. Our
discussion is structured around the study of a real-life scenario that has
prompted recent debate in the legal NLP research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsarapatsanis_D/0/1/0/all/0/1"&gt;Dimitrios Tsarapatsanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1"&gt;Nikolaos Aletras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elastic Shape Analysis of Brain Structures for Predictive Modeling of PTSD. (arXiv:2105.11547v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11547</id>
        <link href="http://arxiv.org/abs/2105.11547"/>
        <updated>2021-05-26T01:22:09.444Z</updated>
        <summary type="html"><![CDATA[There is increasing evidence on the importance of brain morphology in
predicting and classifying mental disorders. However, the vast majority of
current shape approaches rely heavily on vertex-wise analysis that may not
successfully capture complexities of subcortical structures. Additionally, the
past works do not include interactions between these structures and exposure
factors. Predictive modeling with such interactions is of paramount interest in
heterogeneous mental disorders such as PTSD, where trauma exposure interacts
with brain shape changes to influence behavior. We propose a comprehensive
framework that overcomes these limitations by representing brain substructures
as continuous parameterized surfaces and quantifying their shape differences
using elastic shape metrics. Using the elastic shape metric, we compute shape
summaries of subcortical data and represent individual shapes by their
principal scores. These representations allow visualization tools that help
localize changes when these PCs are varied. Subsequently, these PCs, the
auxiliary exposure variables, and their interactions are used for regression
modeling. We apply our method to data from the Grady Trauma Project, where the
goal is to predict clinical measures of PTSD using shapes of brain
substructures. Our analysis revealed considerably greater predictive power
under the elastic shape analysis than widely used approaches such as
vertex-wise shape analysis and even volumetric analysis. It helped identify
local deformations in brain shapes related to change in PTSD severity. To our
knowledge, this is one of the first brain shape analysis approaches that can
seamlessly integrate the pre-processing steps under one umbrella for improved
accuracy and are naturally able to account for interactions between brain shape
and additional covariates to yield superior predictive performance when
modeling clinical outcomes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuexuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1"&gt;Suprateek Kundu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevens_J/0/1/0/all/0/1"&gt;Jennifer S. Stevens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fani_N/0/1/0/all/0/1"&gt;Negar Fani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Anuj Srivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Example-Driven Intent Prediction with Observers. (arXiv:2010.08684v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08684</id>
        <link href="http://arxiv.org/abs/2010.08684"/>
        <updated>2021-05-26T01:22:09.432Z</updated>
        <summary type="html"><![CDATA[A key challenge of dialog systems research is to effectively and efficiently
adapt to new domains. A scalable paradigm for adaptation necessitates the
development of generalizable models that perform well in few-shot settings. In
this paper, we focus on the intent classification problem which aims to
identify user intents given utterances addressed to the dialog system. We
propose two approaches for improving the generalizability of utterance
classification models: (1) observers and (2) example-driven training. Prior
work has shown that BERT-like models tend to attribute a significant amount of
attention to the [CLS] token, which we hypothesize results in diluted
representations. Observers are tokens that are not attended to, and are an
alternative to the [CLS] token as a semantic representation of utterances.
Example-driven training learns to classify utterances by comparing to examples,
thereby using the underlying encoder as a sentence similarity model. These
methods are complementary; improving the representation through observers
allows the example-driven model to better measure sentence similarities. When
combined, the proposed methods attain state-of-the-art results on three intent
prediction datasets (\textsc{banking77}, \textsc{clinc150}, \textsc{hwu64}) in
both the full data and few-shot (10 examples per intent) settings. Furthermore,
we demonstrate that the proposed approach can transfer to new intents and
across datasets without any additional training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1"&gt;Shikib Mehri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eric_M/0/1/0/all/0/1"&gt;Mihail Eric&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Error Modeling Improves Robustness of Noisy Neural Sequence Labeling. (arXiv:2105.11872v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11872</id>
        <link href="http://arxiv.org/abs/2105.11872"/>
        <updated>2021-05-26T01:22:09.409Z</updated>
        <summary type="html"><![CDATA[Despite recent advances, standard sequence labeling systems often fail when
processing noisy user-generated text or consuming the output of an Optical
Character Recognition (OCR) process. In this paper, we improve the noise-aware
training method by proposing an empirical error generation approach that
employs a sequence-to-sequence model trained to perform translation from
error-free to erroneous text. Using an OCR engine, we generated a large
parallel text corpus for training and produced several real-world noisy
sequence labeling benchmarks for evaluation. Moreover, to overcome the data
sparsity problem that exacerbates in the case of imperfect textual input, we
learned noisy language model-based embeddings. Our approach outperformed the
baseline noise generation and error correction techniques on the erroneous
sequence labeling data sets. To facilitate future research on robustness, we
make our code, embeddings, and data conversion scripts publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Namysl_M/0/1/0/all/0/1"&gt;Marcin Namysl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1"&gt;Joachim K&amp;#xf6;hler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Look inside. Predicting stock prices by analysing an enterprise intranet social network and using word co-occurrence networks. (arXiv:2105.11780v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11780</id>
        <link href="http://arxiv.org/abs/2105.11780"/>
        <updated>2021-05-26T01:22:09.400Z</updated>
        <summary type="html"><![CDATA[This study looks into employees' communication, offering novel metrics which
can help to predict a company's stock price. We studied the intranet forum of a
large Italian company, exploring the interactions and the use of language of
about 8,000 employees. We built a network linking words included in the general
discourse. In this network, we focused on the position of the node representing
the company brand. We found that a lower sentiment, a higher betweenness
centrality of the company brand, a denser word co-occurrence network and more
equally distributed centrality scores of employees (lower group betweenness
centrality) are all significant predictors of higher stock prices. Our findings
offers new metrics that can be helpful for scholars, company managers and
professional investors and could be integrated into existing forecasting models
to improve their accuracy. Lastly, we contribute to the research on word
co-occurrence networks by extending their field of application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scettri_G/0/1/0/all/0/1"&gt;G. Scettri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BASS: Boosting Abstractive Summarization with Unified Semantic Graph. (arXiv:2105.12041v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12041</id>
        <link href="http://arxiv.org/abs/2105.12041"/>
        <updated>2021-05-26T01:22:09.391Z</updated>
        <summary type="html"><![CDATA[Abstractive summarization for long-document or multi-document remains
challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing
long-distance relations in text. In this paper, we present BASS, a novel
framework for Boosting Abstractive Summarization based on a unified Semantic
graph, which aggregates co-referent phrases distributing across a long range of
context and conveys rich relations between phrases. Further, a graph-based
encoder-decoder model is proposed to improve both the document representation
and summary generation process by leveraging the graph structure. Specifically,
several graph augmentation methods are designed to encode both the explicit and
implicit relations in the text while the graph-propagation attention mechanism
is developed in the decoder to select salient content into the summary.
Empirical results show that the proposed architecture brings substantial
improvements for both long-document and multi-document summarization tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xinyan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiachen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Ziqiang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sujian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer. (arXiv:2105.11741v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11741</id>
        <link href="http://arxiv.org/abs/2105.11741"/>
        <updated>2021-05-26T01:22:09.382Z</updated>
        <summary type="html"><![CDATA[Learning high-quality sentence representations benefits a wide range of
natural language processing tasks. Though BERT-based pre-trained language
models achieve high performance on many downstream tasks, the native derived
sentence representations are proved to be collapsed and thus produce a poor
performance on the semantic textual similarity (STS) tasks. In this paper, we
present ConSERT, a Contrastive Framework for Self-Supervised Sentence
Representation Transfer, that adopts contrastive learning to fine-tune BERT in
an unsupervised and effective way. By making use of unlabeled texts, ConSERT
solves the collapse issue of BERT-derived sentence representations and make
them more applicable for downstream tasks. Experiments on STS datasets
demonstrate that ConSERT achieves an 8\% relative improvement over the previous
state-of-the-art, even comparable to the supervised SBERT-NLI. And when further
incorporating NLI supervision, we achieve new state-of-the-art performance on
STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples
available, showing its robustness in data scarcity scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yuanmeng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rumei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sirui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fuzheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weiran Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NEUer at SemEval-2021 Task 4: Complete Summary Representation by Filling Answers into Question for Matching Reading Comprehension. (arXiv:2105.12051v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.12051</id>
        <link href="http://arxiv.org/abs/2105.12051"/>
        <updated>2021-05-26T01:22:09.373Z</updated>
        <summary type="html"><![CDATA[SemEval task 4 aims to find a proper option from multiple candidates to
resolve the task of machine reading comprehension. Most existing approaches
propose to concat question and option together to form a context-aware model.
However, we argue that straightforward concatenation can only provide a
coarse-grained context for the MRC task, ignoring the specific positions of the
option relative to the question. In this paper, we propose a novel MRC model by
filling options into the question to produce a fine-grained context (defined as
summary) which can better reveal the relationship between option and question.
We conduct a series of experiments on the given dataset, and the results show
that our approach outperforms other counterparts to a large extent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhixiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yikun Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guibing Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend Forecasting. (arXiv:2105.11826v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11826</id>
        <link href="http://arxiv.org/abs/2105.11826"/>
        <updated>2021-05-26T01:22:09.076Z</updated>
        <summary type="html"><![CDATA[This companion paper supports the replication of the fashion trend
forecasting experiments with the KERN (Knowledge Enhanced Recurrent Network)
method that we presented in the ICMR 2020. We provide an artifact that allows
the replication of the experiments using a Python implementation. The artifact
is easy to deploy with simple installation, training and evaluation. We
reproduce the experiments conducted in the original paper and obtain similar
performance as previously reported. The replication results of the experiments
support the main claims in the original paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yunshan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yujuan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1"&gt;Lizi Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1"&gt;Wai Keung Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1"&gt;Jinyoung Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1"&gt;Hong-Han Shuai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Links on Wikipedia with Anchor Text Information. (arXiv:2105.11734v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.11734</id>
        <link href="http://arxiv.org/abs/2105.11734"/>
        <updated>2021-05-26T01:22:09.060Z</updated>
        <summary type="html"><![CDATA[Wikipedia, the largest open-collaborative online encyclopedia, is a corpus of
documents bound together by internal hyperlinks. These links form the building
blocks of a large network whose structure contains important information on the
concepts covered in this encyclopedia. The presence of a link between two
articles, materialised by an anchor text in the source page pointing to the
target page, can increase readers' understanding of a topic. However, the
process of linking follows specific editorial rules to avoid both under-linking
and over-linking. In this paper, we study the transductive and the inductive
tasks of link prediction on several subsets of the English Wikipedia and
identify some key challenges behind automatic linking based on anchor text
information. We propose an appropriate evaluation sampling methodology and
compare several algorithms. Moreover, we propose baseline models that provide a
good estimation of the overall difficulty of the tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brochier_R/0/1/0/all/0/1"&gt;Robin Brochier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bechet_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;d&amp;#xe9;ric B&amp;#xe9;chet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Mobile GUI: from Pixel-Words to Screen-Sentences. (arXiv:2105.11941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11941</id>
        <link href="http://arxiv.org/abs/2105.11941"/>
        <updated>2021-05-26T01:22:09.041Z</updated>
        <summary type="html"><![CDATA[The ubiquity of mobile phones makes mobile GUI understanding an important
task. Most previous works in this domain require human-created metadata of
screens (e.g. View Hierarchy) during inference, which unfortunately is often
not available or reliable enough for GUI understanding. Inspired by the
impressive success of Transformers in NLP tasks, targeting for purely
vision-based GUI understanding, we extend the concepts of Words/Sentence to
Pixel-Words/Screen-Sentence, and propose a mobile GUI understanding
architecture: Pixel-Words to Screen-Sentence (PW2SS). In analogy to the
individual Words, we define the Pixel-Words as atomic visual components (text
and graphic components), which are visually consistent and semantically clear
across screenshots of a large variety of design styles. The Pixel-Words
extracted from a screenshot are aggregated into Screen-Sentence with a Screen
Transformer proposed to model their relations. Since the Pixel-Words are
defined as atomic visual components, the ambiguity between their visual
appearance and semantics is dramatically reduced. We are able to make use of
metadata available in training data to auto-generate high-quality annotations
for Pixel-Words. A dataset, RICO-PW, of screenshots with Pixel-Words
annotations is built based on the public RICO dataset, which will be released
to help to address the lack of high-quality training data in this area. We
train a detector to extract Pixel-Words from screenshots on this dataset and
achieve metadata-free GUI understanding during inference. We conduct
experiments and show that Pixel-Words can be well extracted on RICO-PW and well
generalized to a new dataset, P2S-UI, collected by ourselves. The effectiveness
of PW2SS is further verified in the GUI understanding tasks including relation
prediction, clickability prediction, screen retrieval, and app type
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jingwen Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuwang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sam Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliard_G/0/1/0/all/0/1"&gt;Grayson Hilliard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11866</id>
        <link href="http://arxiv.org/abs/2105.11866"/>
        <updated>2021-05-26T01:22:09.029Z</updated>
        <summary type="html"><![CDATA[Factorization machine (FM) is a prevalent approach to modeling pairwise
(second-order) feature interactions when dealing with high-dimensional sparse
data. However, on the one hand, FM fails to capture higher-order feature
interactions suffering from combinatorial expansion, on the other hand, taking
into account interaction between every pair of features may introduce noise and
degrade prediction accuracy. To solve the problems, we propose a novel approach
Graph Factorization Machine (GraphFM) by naturally representing features in the
graph structure. In particular, a novel mechanism is designed to select the
beneficial feature interactions and formulate them as edges between features.
Then our proposed model which integrates the interaction function of FM into
the feature aggregation strategy of Graph Neural Network (GNN), can model
arbitrary-order feature interactions on the graph-structured features by
stacking layers. Experimental results on several real-world datasets has
demonstrated the rationality and effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zeyu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reversible Adversarial Attack based on Reversible Image Transformation. (arXiv:1911.02360v7 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.02360</id>
        <link href="http://arxiv.org/abs/1911.02360"/>
        <updated>2021-05-26T01:22:09.016Z</updated>
        <summary type="html"><![CDATA[In order to prevent illegal or unauthorized access of image data such as
human faces and ensure legitimate users can use authorization-protected data,
reversible adversarial attack technique is rise. Reversible adversarial
examples (RAE) get both attack capability and reversibility at the same time.
However, the existing technique can not meet application requirements because
of serious distortion and failure of image recovery when adversarial
perturbations get strong. In this paper, we take advantage of Reversible Image
Transformation technique to generate RAE and achieve reversible adversarial
attack. Experimental results show that proposed RAE generation scheme can
ensure imperceptible image distortion and the original image can be
reconstructed error-free. What's more, both the attack ability and the image
quality are not limited by the perturbation amplitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zhaoxia Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Action Proposal Generation with Transformers. (arXiv:2105.12043v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12043</id>
        <link href="http://arxiv.org/abs/2105.12043"/>
        <updated>2021-05-26T01:22:09.005Z</updated>
        <summary type="html"><![CDATA[Transformer networks are effective at modeling long-range contextual
information and have recently demonstrated exemplary performance in the natural
language processing domain. Conventionally, the temporal action proposal
generation (TAPG) task is divided into two main sub-tasks: boundary prediction
and proposal confidence prediction, which rely on the frame-level dependencies
and proposal-level relationships separately. To capture the dependencies at
different levels of granularity, this paper intuitively presents a unified
temporal action proposal generation framework with original Transformers,
called TAPG Transformer, which consists of a Boundary Transformer and a
Proposal Transformer. Specifically, the Boundary Transformer captures long-term
temporal dependencies to predict precise boundary information and the Proposal
Transformer learns the rich inter-proposal relationships for reliable
confidence evaluation. Extensive experiments are conducted on two popular
benchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG
Transformer outperforms state-of-the-art methods. Equipped with the existing
action classifier, our method achieves remarkable performance on the temporal
action localization task. Codes and models will be available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lining Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haosen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Hongxun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hujie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism. (arXiv:2003.03955v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.03955</id>
        <link href="http://arxiv.org/abs/2003.03955"/>
        <updated>2021-05-26T01:22:08.996Z</updated>
        <summary type="html"><![CDATA[Food retrieval is an important task to perform analysis of food-related
information, where we are interested in retrieving relevant information about
the queried food item such as ingredients, cooking instructions, etc. In this
paper, we investigate cross-modal retrieval between food images and cooking
recipes. The goal is to learn an embedding of images and recipes in a common
feature space, such that the corresponding image-recipe embeddings lie close to
one another. Two major challenges in addressing this problem are 1) large
intra-variance and small inter-variance across cross-modal food data; and 2)
difficulties in obtaining discriminative recipe representations. To address
these two problems, we propose Semantic-Consistent and Attention-based Networks
(SCAN), which regularize the embeddings of the two modalities through aligning
output semantic probabilities. Besides, we exploit a self-attention mechanism
to improve the embedding of recipes. We evaluate the performance of the
proposed method on the large-scale Recipe1M dataset, and show that we can
outperform several state-of-the-art cross-modal retrieval strategies for food
images and cooking recipes by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahoo_D/0/1/0/all/0/1"&gt;Doyen Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chenghao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1"&gt;Ke Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achananuparp_P/0/1/0/all/0/1"&gt;Palakorn Achananuparp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1"&gt;Ee-peng Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1"&gt;Steven C. H. Hoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.12085</id>
        <link href="http://arxiv.org/abs/2105.12085"/>
        <updated>2021-05-26T01:22:08.984Z</updated>
        <summary type="html"><![CDATA[Long-range and short-range temporal modeling are two complementary and
crucial aspects of video recognition. Most of the state-of-the-arts focus on
short-range spatio-temporal modeling and then average multiple snippet-level
predictions to yield the final video-level prediction. Thus, their video-level
prediction does not consider spatio-temporal features of how video evolves
along the temporal dimension. In this paper, we introduce a novel Dynamic
Segment Aggregation (DSA) module to capture relationship among snippets. To be
more specific, we attempt to generate a dynamic kernel for a convolutional
operation to aggregate long-range temporal information among adjacent snippets
adaptively. The DSA module is an efficient plug-and-play module and can be
combined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform
powerful long-range modeling with minimal overhead. The final video
architecture, coined as DSANet. We conduct extensive experiments on several
video recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,
Something-Something V1 and ActivityNet) to show its superiority. Our proposed
DSA module is shown to benefit various video recognition models significantly.
For example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is
improved from 74.9% to 78.2% on Kinetics-400. Codes will be available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhikang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1"&gt;Mingde Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zichao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yifeng Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Movie Recommender System based on Resource Allocation. (arXiv:2105.11678v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.11678</id>
        <link href="http://arxiv.org/abs/2105.11678"/>
        <updated>2021-05-26T01:22:08.918Z</updated>
        <summary type="html"><![CDATA[Recommender Systems are inevitable to personalize user's experiences on the
Internet. They are using different approaches to recommend the Top-K items to
users according to their preferences. Nowadays recommender systems have become
one of the most important parts of largescale data mining techniques. In this
paper, we propose a Hybrid Movie Recommender System (HMRS) based on Resource
Allocation to improve the accuracy of recommendation and solve the cold start
problem for a new movie. HMRS-RA uses a self-organizing mapping neural network
to clustering the users into N clusters. The users' preferences are different
according to their age and gender, therefore HMRS-RA is a combination of a
Content-Based Method for solving the cold start problem for a new movie and a
Collaborative Filtering model besides the demographic information of users. The
experimental results based on the MovieLens dataset show that the HMRS-RA
increases the accuracy of recommendation compared to the state-of-art and
similar works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khalaji_M/0/1/0/all/0/1"&gt;Mostafa Khalaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dadkhah_C/0/1/0/all/0/1"&gt;Chitra Dadkhah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gharibshah_J/0/1/0/all/0/1"&gt;Joobin Gharibshah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VAD360: Viewport Aware Dynamic 360-Degree Video Frame Tiling. (arXiv:2105.11563v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2105.11563</id>
        <link href="http://arxiv.org/abs/2105.11563"/>
        <updated>2021-05-26T01:22:08.885Z</updated>
        <summary type="html"><![CDATA[360{\deg} videos a.k.a. spherical videos are getting popular among users
nevertheless, omnidirectional view of these videos demands high bandwidth and
processing power at the end devices. Recently proposed viewport aware streaming
mechanisms can reduce the amount of data transmitted by streaming a limited
portion of the frame covering the current user viewport (VP). However, they
still suffer from sending a high amount of redundant data, as the fixed tile
mechanisms can not provide finer granularity to the user VP. Though making the
tiles smaller can provide a finer granularity for user viewport, high encoding
overhead incurred. To overcome this trade-off, in this paper, we present a
computational geometric approach based adaptive tiling mechanism named VAD360,
which takes visual attention information on the 360{\deg} video frame as the
input and provide a suitable non-overlapping variable size tile cover on the
frame. Experimental results shows that VAD360 can save up to 31.1% of pixel
redundancy before compression and 35.4% of bandwidth saving compared to
recently proposed fixed tile configurations, providing tile schemes within
0.98($\pm$0.11)s time frame.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kattadige_C/0/1/0/all/0/1"&gt;Chamara Kattadige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thilakarathna_K/0/1/0/all/0/1"&gt;Kanchana Thilakarathna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend Forecasting. (arXiv:2105.11826v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11826</id>
        <link href="http://arxiv.org/abs/2105.11826"/>
        <updated>2021-05-26T01:22:08.847Z</updated>
        <summary type="html"><![CDATA[This companion paper supports the replication of the fashion trend
forecasting experiments with the KERN (Knowledge Enhanced Recurrent Network)
method that we presented in the ICMR 2020. We provide an artifact that allows
the replication of the experiments using a Python implementation. The artifact
is easy to deploy with simple installation, training and evaluation. We
reproduce the experiments conducted in the original paper and obtain similar
performance as previously reported. The replication results of the experiments
support the main claims in the original paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yunshan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yujuan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1"&gt;Lizi Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1"&gt;Wai Keung Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1"&gt;Jinyoung Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1"&gt;Hong-Han Shuai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Dynamic Convolution for Efficient Single-Image Segmentation. (arXiv:2104.08783v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08783</id>
        <link href="http://arxiv.org/abs/2104.08783"/>
        <updated>2021-05-25T01:56:12.706Z</updated>
        <summary type="html"><![CDATA[Interactive single-image segmentation is ubiquitous in the scientific and
commercial imaging software. In this work, we focus on the single-image
segmentation problem only with some seeds such as scribbles. Inspired by the
dynamic receptive field in the human being's visual system, we propose the
Gaussian dynamic convolution (GDC) to fast and efficiently aggregate the
contextual information for neural networks. The core idea is randomly selecting
the spatial sampling area according to the Gaussian distribution offsets. Our
GDC can be easily used as a module to build lightweight or complex segmentation
networks. We adopt the proposed GDC to address the typical single-image
segmentation tasks. Furthermore, we also build a Gaussian dynamic pyramid
Pooling to show its potential and generality in common semantic segmentation.
Experiments demonstrate that the GDC outperforms other existing convolutions on
three benchmark segmentation datasets including Pascal-Context, Pascal-VOC
2012, and Cityscapes. Additional experiments are also conducted to illustrate
that the GDC can produce richer and more vivid features compared with other
convolutions. In general, our GDC is conducive to the convolutional neural
networks to form an overall impression of the image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changrui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaorui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Junyu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sheng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSCAN: A Spatial-spectral Cross Attention Network for Hyperspectral Image Denoising. (arXiv:2105.10949v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10949</id>
        <link href="http://arxiv.org/abs/2105.10949"/>
        <updated>2021-05-25T01:56:12.688Z</updated>
        <summary type="html"><![CDATA[Hyperspectral images (HSIs) have been widely used in a variety of
applications thanks to the rich spectral information they are able to provide.
Among all HSI processing tasks, HSI denoising is a crucial step. Recently, deep
learning-based image denoising methods have made great progress and achieved
great performance. However, existing methods tend to ignore the correlations
between adjacent spectral bands, leading to problems such as spectral
distortion and blurred edges in denoised results. In this study, we propose a
novel HSI denoising network, termed SSCAN, that combines group convolutions and
attention modules. Specifically, we use a group convolution with a spatial
attention module to facilitate feature extraction by directing models'
attention to band-wise important features. We propose a spectral-spatial
attention block (SSAB) to exploit the spatial and spectral information in
hyperspectral images in an effective manner. In addition, we adopt residual
learning operations with skip connections to ensure training stability. The
experimental results indicate that the proposed SSCAN outperforms several
state-of-the-art HSI denoising algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhenfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sihang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novel ANN method for solving ordinary and fractional Black-Scholes equation. (arXiv:2105.11240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11240</id>
        <link href="http://arxiv.org/abs/2105.11240"/>
        <updated>2021-05-25T01:56:12.680Z</updated>
        <summary type="html"><![CDATA[The main aim of this study is to introduce a 2-layered Artificial Neural
Network (ANN) for solving the Black-Scholes partial differential equation (PDE)
of either fractional or ordinary orders. Firstly, a discretization method is
employed to change the model into a sequence of Ordinary Differential Equations
(ODE). Then each of these ODEs is solved with the aid of an ANN. Adam
optimization is employed as the learning paradigm since it can add the
foreknowledge of slowing down the process of optimization when getting close to
the actual optimum solution. The model also takes advantage of fine tuning for
speeding up the process and domain mapping to confront infinite domain issue.
Finally, the accuracy, speed, and convergence of the method for solving several
types of Black-Scholes model are reported.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bajalan_S/0/1/0/all/0/1"&gt;Saeed Bajalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajalan_N/0/1/0/all/0/1"&gt;Nastaran Bajalan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design to automate the detection and counting of Tuberculosis(TB) bacilli. (arXiv:2105.11432v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11432</id>
        <link href="http://arxiv.org/abs/2105.11432"/>
        <updated>2021-05-25T01:56:12.674Z</updated>
        <summary type="html"><![CDATA[Tuberculosis is a contagious disease which is one of the leading causes of
death, globally. The general diagnosis methods for tuberculosis include
microscopic examination, tuberculin skin test, culture method, enzyme linked
immunosorbent assay (ELISA) and electronic nose system. World Health
Organization (WHO) recommends standard microscopic examination for early
diagnosis of tuberculosis. In microscopy, the technician examines field of
views (FOVs) in sputum smear for presence of any TB bacilli and counts the
number of TB bacilli per FOV to report the level of severity. This process is
time consuming with an increased concentration for an experienced staff to
examine a single sputum smear. The examination demands for skilled technicians
in high-prevalence countries which may lead to overload, fatigue and diminishes
the quality of microscopy. Thus, a computer assisted system is proposed and
designed for the detection of tuberculosis bacilli to assist pathologists with
increased sensitivity and specificity. The manual efforts in detecting and
counting the number of TB bacilli is greatly minimized. The system obtains
Ziehl-Neelsen stained microscopic images from conventional microscope at 100x
magnification and passes the data to the detection system. Initially the
segmentation of TB bacilli was done using RGB thresholding and Sauvola's
adaptive thresholding algorithm. To eliminate the non-TB bacilli from coarse
level segmentation, shape descriptors like area, perimeter, convex hull, major
axis length and eccentricity are used to extract only the TB bacilli features.
Finally, the TB bacilli are counted using the generated bounding boxes to
report the level of severity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Samuel_D/0/1/0/all/0/1"&gt;Dinesh Jackson Samuel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Baskaran_R/0/1/0/all/0/1"&gt;Rajesh Kanna Baskaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stroke Lesion Segmentation with Visual Cortex Anatomy Alike Neural Nets. (arXiv:2105.06544v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06544</id>
        <link href="http://arxiv.org/abs/2105.06544"/>
        <updated>2021-05-25T01:56:12.668Z</updated>
        <summary type="html"><![CDATA[Cerebrovascular accident, or commonly known as stroke, is an acute disease
with extreme impact on patients and healthcare systems and is the second
largest cause of death worldwide. Fast and precise stroke lesion detection and
location is an extreme important process with regards to stroke diagnosis,
treatment, and prognosis. Except from the manual segmentation approach, machine
learning based segmentation methods are the most promising ones when
considering efficiency and accuracy, and convolutional neural network based
models are the first of its kind. However, most of these neural network models
do not really align with the brain anatomical structures. Intuitively, this
work presents a more brain alike model which mimics the anatomical structure of
the human visual cortex. Through the preliminary experiments on the stroke
lesion segmentation task, the proposed model is found to be able to perform
equally well or better to the de-facto standard U-Net. Part of the
implementation will be made available at https://github.com/DarkoBomer/VCA-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1"&gt;Chuanlong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking of Deep Learning Irradiance Forecasting Models from Sky Images -- an in-depth Analysis. (arXiv:2102.00721v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00721</id>
        <link href="http://arxiv.org/abs/2102.00721"/>
        <updated>2021-05-25T01:56:12.661Z</updated>
        <summary type="html"><![CDATA[A number of industrial applications, such as smart grids, power plant
operation, hybrid system management or energy trading, could benefit from
improved short-term solar forecasting, addressing the intermittent energy
production from solar panels. However, current approaches to modelling the
cloud cover dynamics from sky images still lack precision regarding the spatial
configuration of clouds, their temporal dynamics and physical interactions with
solar radiation. Benefiting from a growing number of large datasets, data
driven methods are being developed to address these limitations with promising
results. In this study, we compare four commonly used Deep Learning
architectures trained to forecast solar irradiance from sequences of
hemispherical sky images and exogenous variables. To assess the relative
performance of each model, we used the Forecast Skill metric based on the smart
persistence model, as well as ramp and time distortion metrics. The results
show that encoding spatiotemporal aspects of the sequence of sky images greatly
improved the predictions with 10 min ahead Forecast Skill reaching 20.4% on the
test year. However, based on the experimental data, we conclude that, with a
common setup, Deep Learning models tend to behave just as a 'very smart
persistence model', temporally aligned with the persistence model while
mitigating its most penalising errors. Thus, despite being captured by the sky
cameras, models often miss fundamental events causing large irradiance changes
such as clouds obscuring the sun. We hope that our work will contribute to a
shift of this approach to irradiance forecasting, from reactive to
anticipatory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1"&gt;Quentin Paletta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbod_G/0/1/0/all/0/1"&gt;Guillaume Arbod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DFUC2020: Analysis Towards Diabetic Foot Ulcer Detection. (arXiv:2004.11853v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11853</id>
        <link href="http://arxiv.org/abs/2004.11853"/>
        <updated>2021-05-25T01:56:12.655Z</updated>
        <summary type="html"><![CDATA[Every 20 seconds, a limb is amputated somewhere in the world due to diabetes.
This is a global health problem that requires a global solution. The MICCAI
challenge discussed in this paper, which concerns the automated detection of
diabetic foot ulcers using machine learning techniques, will accelerate the
development of innovative healthcare technology to address this unmet medical
need. In an effort to improve patient care and reduce the strain on healthcare
systems, recent research has focused on the creation of cloud-based detection
algorithms. These can be consumed as a service by a mobile app that patients
(or a carer, partner or family member) could use themselves at home to monitor
their condition and to detect the appearance of a diabetic foot ulcer (DFU).
Collaborative work between Manchester Metropolitan University, Lancashire
Teaching Hospital and the Manchester University NHS Foundation Trust has
created a repository of 4,000 DFU images for the purpose of supporting research
toward more advanced methods of DFU detection. Based on a joint effort
involving the lead scientists of the UK, US, India and New Zealand, this
challenge will solicit original work, and promote interactions between
researchers and interdisciplinary collaborations. This paper presents a dataset
description and analysis, assessment methods, benchmark algorithms and initial
evaluation results. It facilitates the challenge by providing useful insights
into state-of-the-art and ongoing research. This grand challenge takes on even
greater urgency in a peri and post-pandemic period, where stresses on resource
utilization will increase the need for technology that allows people to remain
active, healthy and intact in their home.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cassidy_B/0/1/0/all/0/1"&gt;Bill Cassidy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reeves_N/0/1/0/all/0/1"&gt;Neil D. Reeves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_P/0/1/0/all/0/1"&gt;Pappachan Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gillespie_D/0/1/0/all/0/1"&gt;David Gillespie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OShea_C/0/1/0/all/0/1"&gt;Claire O&amp;#x27;Shea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1"&gt;Satyan Rajbhandari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1"&gt;Arun G. Maiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1"&gt;Eibe Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boulton_A/0/1/0/all/0/1"&gt;Andrew Boulton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armstrong_D/0/1/0/all/0/1"&gt;David Armstrong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najafi_B/0/1/0/all/0/1"&gt;Bijan Najafi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Justina Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1"&gt;Moi Hoon Yap&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Regression based Single Event Transient Modeling Method for Circuit-Level Simulation. (arXiv:2105.10723v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10723</id>
        <link href="http://arxiv.org/abs/2105.10723"/>
        <updated>2021-05-25T01:56:12.648Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel machine learning regression based single event
transient (SET) modeling method is proposed. The proposed method can obtain a
reasonable and accurate model without considering the complex physical
mechanism. We got plenty of SET current data of SMIC 130nm bulk CMOS by TCAD
simulation under different conditions (e.g. different LET and different drain
bias voltage). A multilayer feedfordward neural network is used to build the
SET pulse current model by learning the data from TCAD simulation. The proposed
model is validated with the simulation results from TCAD simulation. The
trained SET pulse current model is implemented as a Verilog-A current source in
the Cadence Spectre circuit simulator and an inverter with five fan-outs is
used to show the practicability and reasonableness of the proposed SET pulse
current model for circuit-level single-event effect (SEE) simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;ChangQing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;XinFang Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;JiaLiang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;YinTang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embracing New Techniques in Deep Learning for Estimating Image Memorability. (arXiv:2105.10598v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10598</id>
        <link href="http://arxiv.org/abs/2105.10598"/>
        <updated>2021-05-25T01:56:12.640Z</updated>
        <summary type="html"><![CDATA[Various work has suggested that the memorability of an image is consistent
across people, and thus can be treated as an intrinsic property of an image.
Using computer vision models, we can make specific predictions about what
people will remember or forget. While older work has used now-outdated deep
learning architectures to predict image memorability, innovations in the field
have given us new techniques to apply to this problem. Here, we propose and
evaluate five alternative deep learning models which exploit developments in
the field from the last five years, largely the introduction of residual neural
networks, which are intended to allow the model to use semantic information in
the memorability estimation process. These new models were tested against the
prior state of the art with a combined dataset built to optimize both
within-category and across-category predictions. Our findings suggest that the
key prior memorability network had overstated its generalizability and was
overfit on its training set. Our new models outperform this prior model,
leading us to conclude that Residual Networks outperform simpler convolutional
neural networks in memorability regression. We make our new state-of-the-art
model readily available to the research community, allowing memory researchers
to make predictions about memorability on a wider range of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Needell_C/0/1/0/all/0/1"&gt;Coen D. Needell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bainbridge_W/0/1/0/all/0/1"&gt;Wilma A. Bainbridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-Training Sparsity-Aware Quantization. (arXiv:2105.11010v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11010</id>
        <link href="http://arxiv.org/abs/2105.11010"/>
        <updated>2021-05-25T01:56:12.620Z</updated>
        <summary type="html"><![CDATA[Quantization is a technique used in deep neural networks (DNNs) to increase
execution performance and hardware efficiency. Uniform post-training
quantization (PTQ) methods are common, since they can be implemented
efficiently in hardware and do not require extensive hardware resources or a
training set. Mapping FP32 models to INT8 using uniform PTQ yields models with
negligible accuracy degradation; however, reducing precision below 8 bits with
PTQ is challenging, as accuracy degradation becomes noticeable, due to the
increase in quantization noise. In this paper, we propose a sparsity-aware
quantization (SPARQ) method, in which the unstructured and dynamic activation
sparsity is leveraged in different representation granularities. 4-bit
quantization, for example, is employed by dynamically examining the bits of
8-bit values and choosing a window of 4 bits, while first skipping zero-value
bits. Moreover, instead of quantizing activation-by-activation to 4 bits, we
focus on pairs of 8-bit activations and examine whether one of the two is equal
to zero. If one is equal to zero, the second can opportunistically use the
other's 4-bit budget; if both do not equal zero, then each is dynamically
quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation,
2x speedup over widely used hardware architectures, and a practical hardware
implementation. The code is available at https://github.com/gilshm/sparq.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shomron_G/0/1/0/all/0/1"&gt;Gil Shomron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gabbay_F/0/1/0/all/0/1"&gt;Freddy Gabbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurzum_S/0/1/0/all/0/1"&gt;Samer Kurzum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiser_U/0/1/0/all/0/1"&gt;Uri Weiser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Humans for Action Recognition from Unseen Viewpoints. (arXiv:1912.04070v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.04070</id>
        <link href="http://arxiv.org/abs/1912.04070"/>
        <updated>2021-05-25T01:56:12.604Z</updated>
        <summary type="html"><![CDATA[Although synthetic training data has been shown to be beneficial for tasks
such as human pose estimation, its use for RGB human action recognition is
relatively unexplored. Our goal in this work is to answer the question whether
synthetic humans can improve the performance of human action recognition, with
a particular focus on generalization to unseen viewpoints. We make use of the
recent advances in monocular 3D human body reconstruction from real action
sequences to automatically render synthetic training videos for the action
labels. We make the following contributions: (i) we investigate the extent of
variations and augmentations that are beneficial to improving performance at
new viewpoints. We consider changes in body shape and clothing for individuals,
as well as more action relevant augmentations such as non-uniform frame
sampling, and interpolating between the motion of individuals performing the
same action; (ii) We introduce a new data generation methodology, SURREACT,
that allows training of spatio-temporal CNNs for action classification; (iii)
We substantially improve the state-of-the-art action recognition performance on
the NTU RGB+D and UESTC standard human action multi-view benchmarks; Finally,
(iv) we extend the augmentation approach to in-the-wild videos from a subset of
the Kinetics dataset to investigate the case when only one-shot training data
is available, and demonstrate improvements in this case as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1"&gt;G&amp;#xfc;l Varol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1"&gt;Ivan Laptev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1"&gt;Cordelia Schmid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstructing Small 3D Objects in front of a Textured Background. (arXiv:2105.11352v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11352</id>
        <link href="http://arxiv.org/abs/2105.11352"/>
        <updated>2021-05-25T01:56:12.586Z</updated>
        <summary type="html"><![CDATA[We present a technique for a complete 3D reconstruction of small objects
moving in front of a textured background. It is a particular variation of
multibody structure from motion, which specializes to two objects only. The
scene is captured in several static configurations between which the relative
pose of the two objects may change. We reconstruct every static configuration
individually and segment the points locally by finding multiple poses of
cameras that capture the scene's other configurations. Then, the local
segmentation results are combined, and the reconstructions are merged into the
resulting model of the scene. In experiments with real artifacts, we show that
our approach has practical advantages when reconstructing 3D objects from all
sides. In this setting, our method outperforms the state-of-the-art. We
integrate our method into the state of the art 3D reconstruction pipeline
COLMAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hruby_P/0/1/0/all/0/1"&gt;Petr Hruby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajdla_T/0/1/0/all/0/1"&gt;Tomas Pajdla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation of COVID-19 Chest CT Scan Images using Generative Adversarial Networks. (arXiv:2105.11241v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11241</id>
        <link href="http://arxiv.org/abs/2105.11241"/>
        <updated>2021-05-25T01:56:12.578Z</updated>
        <summary type="html"><![CDATA[SARS-CoV-2, also known as COVID-19 or Coronavirus, is a viral contagious
disease that is infected by a novel coronavirus, and has been rapidly spreading
across the globe. It is very important to test and isolate people to reduce
spread, and from here comes the need to do this quickly and efficiently.
According to some studies, Chest-CT outperforms RT-PCR lab testing, which is
the current standard, when diagnosing COVID-19 patients. Due to this, computer
vision researchers have developed various deep learning systems that can
predict COVID-19 using a Chest-CT scan correctly to a certain degree. The
accuracy of these systems is limited since deep learning neural networks such
as CNNs (Convolutional Neural Networks) need a significantly large quantity of
data for training in order to produce good quality results. Since the disease
is relatively recent and more focus has been on CXR (Chest XRay) images, the
available chest CT Scan image dataset is much less. We propose a method, by
utilizing GANs, to generate synthetic chest CT images of both positive and
negative COVID-19 patients. Using a pre-built predictive model, we concluded
that around 40% of the generated images are correctly predicted as COVID-19
positive. The dataset thus generated can be used to train a CNN-based
classifier which can help determine COVID-19 in a patient with greater
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mann_P/0/1/0/all/0/1"&gt;Prerak Mann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jain_S/0/1/0/all/0/1"&gt;Sahaj Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mittal_S/0/1/0/all/0/1"&gt;Saurabh Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhat_A/0/1/0/all/0/1"&gt;Aruna Bhat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Variational Semi-Supervised Novelty Detection. (arXiv:1911.04971v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.04971</id>
        <link href="http://arxiv.org/abs/1911.04971"/>
        <updated>2021-05-25T01:56:12.572Z</updated>
        <summary type="html"><![CDATA[In anomaly detection (AD), one seeks to identify whether a test sample is
abnormal, given a data set of normal samples. A recent and promising approach
to AD relies on deep generative models, such as variational autoencoders
(VAEs), for unsupervised learning of the normal data distribution. In
semi-supervised AD (SSAD), the data also includes a small sample of labeled
anomalies. In this work, we propose two variational methods for training VAEs
for SSAD. The intuitive idea in both methods is to train the encoder to
`separate' between latent vectors for normal and outlier data. We show that
this idea can be derived from principled probabilistic formulations of the
problem, and propose simple and effective algorithms. Our methods can be
applied to various data types, as we demonstrate on SSAD datasets ranging from
natural images to astronomy and medicine, can be combined with any VAE model
architecture, and are naturally compatible with ensembling. When comparing to
state-of-the-art SSAD methods that are not specific to particular data types,
we obtain marked improvement in outlier detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daniel_T/0/1/0/all/0/1"&gt;Tal Daniel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurutach_T/0/1/0/all/0/1"&gt;Thanard Kurutach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1"&gt;Aviv Tamar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion with Ensemble Monte Carlo Dropout for COVID-19 Detection. (arXiv:2105.08590v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08590</id>
        <link href="http://arxiv.org/abs/2105.08590"/>
        <updated>2021-05-25T01:56:12.552Z</updated>
        <summary type="html"><![CDATA[The COVID-19 (Coronavirus disease 2019) has infected more than 151 million
people and caused approximately 3.17 million deaths around the world up to the
present. The rapid spread of COVID-19 is continuing to threaten human's life
and health. Therefore, the development of computer-aided detection (CAD)
systems based on machine and deep learning methods which are able to accurately
differentiate COVID-19 from other diseases using chest computed tomography (CT)
and X-Ray datasets is essential and of immediate priority. Different from most
of the previous studies which used either one of CT or X-ray images, we
employed both data types with sufficient samples in implementation. On the
other hand, due to the extreme sensitivity of this pervasive virus, model
uncertainty should be considered, while most previous studies have overlooked
it. Therefore, we propose a novel powerful fusion model named
$UncertaintyFuseNet$ that consists of an uncertainty module: Ensemble Monte
Carlo (EMC) dropout. The obtained results prove the effectiveness of our
proposed fusion for COVID-19 detection using CT scan and X-Ray datasets. Also,
our proposed $UncertaintyFuseNet$ model is significantly robust to noise and
performs well with the previously unseen data. The source codes and models of
this study are available at:
https://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Abdar_M/0/1/0/all/0/1"&gt;Moloud Abdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salari_S/0/1/0/all/0/1"&gt;Soorena Salari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qahremani_S/0/1/0/all/0/1"&gt;Sina Qahremani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lam_H/0/1/0/all/0/1"&gt;Hak-Keung Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1"&gt;Sadiq Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1"&gt;U. Rajendra Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Visual Anomaly detection with Negative Learning. (arXiv:2105.11058v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11058</id>
        <link href="http://arxiv.org/abs/2105.11058"/>
        <updated>2021-05-25T01:56:12.545Z</updated>
        <summary type="html"><![CDATA[With the increase in the learning capability of deep convolution-based
architectures, various applications of such models have been proposed over
time. In the field of anomaly detection, improvements in deep learning opened
new prospects of exploration for the researchers whom tried to automate the
labor-intensive features of data collection. First, in terms of data
collection, it is impossible to anticipate all the anomalies that might exist
in a given environment. Second, assuming we limit the possibilities of
anomalies, it will still be hard to record all these scenarios for the sake of
training a model. Third, even if we manage to record a significant amount of
abnormal data, it's laborious to annotate this data on pixel or even frame
level. Various approaches address the problem by proposing one-class
classification using generative models trained on only normal data. In such
methods, only the normal data is used, which is abundantly available and
doesn't require significant human input. However, these are trained with only
normal data and at the test time, given abnormal data as input, may often
generate normal-looking output. This happens due to the hallucination
characteristic of generative models. Next, these systems are designed to not
use abnormal examples during the training. In this paper, we propose anomaly
detection with negative learning (ADNL), which employs the negative learning
concept for the enhancement of anomaly detection by utilizing a very small
number of labeled anomaly data as compared with the normal data during
training. The idea is to limit the reconstruction capability of a generative
model using the given a small amount of anomaly examples. This way, the network
not only learns to reconstruct normal data but also encloses the normal
distribution far from the possible distribution of anomalies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jin-Ha Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1"&gt;Marcella Astrid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Muhammad Zaigham Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seung-Ik Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting 2D Convolutional Neural Networks for Graph-based Applications. (arXiv:2105.11016v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11016</id>
        <link href="http://arxiv.org/abs/2105.11016"/>
        <updated>2021-05-25T01:56:12.537Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks (GCNs) are widely used in graph-based
applications such as graph classification and segmentation. However, current
GCNs have limitations on implementation such as network architectures due to
their irregular inputs. In contrast, convolutional neural networks (CNNs) are
capable of extracting rich features from large-scale input data, but they do
not support general graph inputs. To bridge the gap between GCNs and CNNs, in
this paper we study the problem of how to effectively and efficiently map
general graphs to 2D grids that CNNs can be directly applied to, while
preserving graph topology as much as possible. We therefore propose two novel
graph-to-grid mapping schemes, namely, {\em graph-preserving grid layout
(GPGL)} and its extension {\em Hierarchical GPGL (H-GPGL)} for computational
efficiency. We formulate the GPGL problem as integer programming and further
propose an approximate yet efficient solver based on a penalized Kamada-Kawai
method, a well-known optimization algorithm in 2D graph drawing. We propose a
novel vertex separation penalty that encourages graph vertices to lay on the
grid without any overlap. Along with this image representation, even extra 2D
maxpooling layers contribute to the PointNet, a widely applied point-based
neural network. We demonstrate the empirical success of GPGL on general graph
classification with small graphs and H-GPGL on 3D point cloud segmentation with
large graphs, based on 2D CNNs including VGG16, ResNet50 and multi-scale maxout
(MSM) CNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yecheng Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xinming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziming Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADNet: Attention-guided Deformable Convolutional Network for High Dynamic Range Imaging. (arXiv:2105.10697v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10697</id>
        <link href="http://arxiv.org/abs/2105.10697"/>
        <updated>2021-05-25T01:56:12.530Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an attention-guided deformable convolutional
network for hand-held multi-frame high dynamic range (HDR) imaging, namely
ADNet. This problem comprises two intractable challenges of how to handle
saturation and noise properly and how to tackle misalignments caused by object
motion or camera jittering. To address the former, we adopt a spatial attention
module to adaptively select the most appropriate regions of various exposure
low dynamic range (LDR) images for fusion. For the latter one, we propose to
align the gamma-corrected images in the feature-level with a Pyramid, Cascading
and Deformable (PCD) alignment module. The proposed ADNet shows
state-of-the-art performance compared with previous methods, achieving a
PSNR-$l$ of 39.4471 and a PSNR-$\mu$ of 37.6359 in NTIRE 2021 Multi-Frame HDR
Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wenjie Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinpeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Q/0/1/0/all/0/1"&gt;Qing Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Ting Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1"&gt;Mingyan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Haoqiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuaicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising Noisy Neural Networks: A Bayesian Approach with Compensation. (arXiv:2105.10699v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10699</id>
        <link href="http://arxiv.org/abs/2105.10699"/>
        <updated>2021-05-25T01:56:12.524Z</updated>
        <summary type="html"><![CDATA[Noisy neural networks (NoisyNNs) refer to the inference and training of NNs
in the presence of noise. Noise is inherent in most communication and storage
systems; hence, NoisyNNs emerge in many new applications, including federated
edge learning, where wireless devices collaboratively train a NN over a noisy
wireless channel, or when NNs are implemented/stored in an analog storage
medium. This paper studies a fundamental problem of NoisyNNs: how to estimate
the uncontaminated NN weights from their noisy observations or manifestations.
Whereas all prior works relied on the maximum likelihood (ML) estimation to
maximize the likelihood function of the estimated NN weights, this paper
demonstrates that the ML estimator is in general suboptimal. To overcome the
suboptimality of the conventional ML estimator, we put forth an
$\text{MMSE}_{pb}$ estimator to minimize a compensated mean squared error (MSE)
with a population compensator and a bias compensator. Our approach works well
for NoisyNNs arising in both 1) noisy inference, where noise is introduced only
in the inference phase on the already-trained NN weights; and 2) noisy
training, where noise is introduced over the course of training. Extensive
experiments on the CIFAR-10 and SST-2 datasets with different NN architectures
verify the significant performance gains of the $\text{MMSE}_{pb}$ estimator
over the ML estimator when used to denoise the NoisyNN. For noisy inference,
the average gains are up to $156\%$ for a noisy ResNet34 model and $14.7\%$ for
a noisy BERT model; for noisy training, the average gains are up to $18.1$ dB
for a noisy ResNet18 model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yulin Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1"&gt;Soung Chang Liew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAL: Intelligence Augmentation using Egocentric Visual Context Detection. (arXiv:2105.10735v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10735</id>
        <link href="http://arxiv.org/abs/2105.10735"/>
        <updated>2021-05-25T01:56:12.517Z</updated>
        <summary type="html"><![CDATA[Egocentric visual context detection can support intelligence augmentation
applications. We created a wearable system, called PAL, for wearable,
personalized, and privacy-preserving egocentric visual context detection. PAL
has a wearable device with a camera, heart-rate sensor, on-device deep
learning, and audio input/output. PAL also has a mobile/web application for
personalized context labeling. We used on-device deep learning models for
generic object and face detection, low-shot custom face and context recognition
(e.g., activities like brushing teeth), and custom context clustering (e.g.,
indoor locations). The models had over 80\% accuracy in in-the-wild contexts
(~1000 images) and we tested PAL for intelligence augmentation applications
like behavior change. We have made PAL is open-source to further support
intelligence augmentation using personalized and privacy-preserving egocentric
visual contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mina Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maes_P/0/1/0/all/0/1"&gt;Pattie Maes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoInt: Automatic Integration for Fast Neural Volume Rendering. (arXiv:2012.01714v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01714</id>
        <link href="http://arxiv.org/abs/2012.01714"/>
        <updated>2021-05-25T01:56:12.496Z</updated>
        <summary type="html"><![CDATA[Numerical integration is a foundational technique in scientific computing and
is at the core of many computer vision applications. Among these applications,
neural volume rendering has recently been proposed as a new paradigm for view
synthesis, achieving photorealistic image quality. However, a fundamental
obstacle to making these methods practical is the extreme computational and
memory requirements caused by the required volume integrations along the
rendered rays during training and inference. Millions of rays, each requiring
hundreds of forward passes through a neural network are needed to approximate
those integrations with Monte Carlo sampling. Here, we propose automatic
integration, a new framework for learning efficient, closed-form solutions to
integrals using coordinate-based neural networks. For training, we instantiate
the computational graph corresponding to the derivative of the network. The
graph is fitted to the signal to integrate. After optimization, we reassemble
the graph to obtain a network that represents the antiderivative. By the
fundamental theorem of calculus, this enables the calculation of any definite
integral in two evaluations of the network. Applying this approach to neural
rendering, we improve a tradeoff between rendering speed and image quality:
improving render times by greater than 10 times with a tradeoff of slightly
reduced image quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lindell_D/0/1/0/all/0/1"&gt;David B. Lindell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martel_J/0/1/0/all/0/1"&gt;Julien N. P. Martel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1"&gt;Gordon Wetzstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Advances and Trends in Multimodal Deep Learning: A Review. (arXiv:2105.11087v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11087</id>
        <link href="http://arxiv.org/abs/2105.11087"/>
        <updated>2021-05-25T01:56:12.490Z</updated>
        <summary type="html"><![CDATA[Deep Learning has implemented a wide range of applications and has become
increasingly popular in recent years. The goal of multimodal deep learning is
to create models that can process and link information using various
modalities. Despite the extensive development made for unimodal learning, it
still cannot cover all the aspects of human learning. Multimodal learning helps
to understand and analyze better when various senses are engaged in the
processing of information. This paper focuses on multiple types of modalities,
i.e., image, video, text, audio, body gestures, facial expressions, and
physiological signals. Detailed analysis of past and current baseline
approaches and an in-depth study of recent advancements in multimodal deep
learning applications has been provided. A fine-grained taxonomy of various
multimodal deep learning applications is proposed, elaborating on different
applications in more depth. Architectures and datasets used in these
applications are also discussed, along with their evaluation metrics. Last,
main issues are highlighted separately for each domain along with their
possible future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Summaira_J/0/1/0/all/0/1"&gt;Jabeen Summaira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoib_A/0/1/0/all/0/1"&gt;Amin Muhammad Shoib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Songyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdul_J/0/1/0/all/0/1"&gt;Jabbar Abdul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse to Fine Multi-Resolution Temporal Convolutional Network. (arXiv:2105.10859v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10859</id>
        <link href="http://arxiv.org/abs/2105.10859"/>
        <updated>2021-05-25T01:56:12.484Z</updated>
        <summary type="html"><![CDATA[Temporal convolutional networks (TCNs) are a commonly used architecture for
temporal video segmentation. TCNs however, tend to suffer from
over-segmentation errors and require additional refinement modules to ensure
smoothness and temporal coherency. In this work, we propose a novel temporal
encoder-decoder to tackle the problem of sequence fragmentation. In particular,
the decoder follows a coarse-to-fine structure with an implicit ensemble of
multiple temporal resolutions. The ensembling produces smoother segmentations
that are more accurate and better-calibrated, bypassing the need for additional
refinement modules. In addition, we enhance our training with a
multi-resolution feature-augmentation strategy to promote robustness to varying
temporal resolutions. Finally, to support our architecture and encourage
further sequence coherency, we propose an action loss that penalizes
misclassifications at the video level. Experiments show that our stand-alone
architecture, together with our novel feature-augmentation strategy and new
loss, outperforms the state-of-the-art on three temporal video segmentation
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhania_D/0/1/0/all/0/1"&gt;Dipika Singhania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahaman_R/0/1/0/all/0/1"&gt;Rahul Rahaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FCCDN: Feature Constraint Network for VHR Image Change Detection. (arXiv:2105.10860v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10860</id>
        <link href="http://arxiv.org/abs/2105.10860"/>
        <updated>2021-05-25T01:56:12.478Z</updated>
        <summary type="html"><![CDATA[Change detection is the process of identifying pixel-wise differences of
bi-temporal co-registered images. It is of great significance to Earth
observation. Recently, with the emerging of deep learning (DL), deep
convolutional neural networks (CNNs) based methods have shown their power and
feasibility in the field of change detection. However, there is still a lack of
effective supervision for change feature learning. In this work, a feature
constraint change detection network (FCCDN) is proposed. We constrain features
both on bi-temporal feature extraction and feature fusion. More specifically,
we propose a dual encoder-decoder network backbone for the change detection
task. At the center of the backbone, we design a non-local feature pyramid
network to extract and fuse multi-scale features. To fuse bi-temporal features
in a robust way, we build a dense connection-based feature fusion module.
Moreover, a self-supervised learning-based strategy is proposed to constrain
feature learning. Based on FCCDN, we achieve state-of-the-art performance on
two building change detection datasets (LEVIR-CD and WHU). On the LEVIR-CD
dataset, we achieve IoU of 0.8569 and F1 score of 0.9229. On the WHU dataset,
we achieve IoU of 0.8820 and F1 score of 0.9373. Moreover, we, for the first
time, achieve the acquire of accurate bi-temporal semantic segmentation results
without using semantic segmentation labels. It is vital for the application of
change detection because it saves the cost of labeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1"&gt;Danfeng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhengchao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baipeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HOME: Heatmap Output for future Motion Estimation. (arXiv:2105.10968v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10968</id>
        <link href="http://arxiv.org/abs/2105.10968"/>
        <updated>2021-05-25T01:56:12.470Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose HOME, a framework tackling the motion forecasting
problem with an image output representing the probability distribution of the
agent's future location. This method allows for a simple architecture with
classic convolution networks coupled with attention mechanism for agent
interactions, and outputs an unconstrained 2D top-view representation of the
agent's possible future. Based on this output, we design two methods to sample
a finite set of agent's future locations. These methods allow us to control the
optimization trade-off between miss rate and final displacement error for
multiple modalities without having to retrain any part of the model. We apply
our method to the Argoverse Motion Forecasting Benchmark and achieve 1st place
on the online leaderboard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1"&gt;Thomas Gilles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1"&gt;Stefano Sabatini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1"&gt;Dzmitry Tsishkou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1"&gt;Bogdan Stanciulescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1"&gt;Fabien Moutarde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wisdom for the Crowd: Discoursive Power in Annotation Instructions for Computer Vision. (arXiv:2105.10990v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10990</id>
        <link href="http://arxiv.org/abs/2105.10990"/>
        <updated>2021-05-25T01:56:12.445Z</updated>
        <summary type="html"><![CDATA[Developers of computer vision algorithms outsource some of the labor involved
in annotating training data through business process outsourcing companies and
crowdsourcing platforms. Many data annotators are situated in the Global South
and are considered independent contractors. This paper focuses on the
experiences of Argentinian and Venezuelan annotation workers. Through
qualitative methods, we explore the discourses encoded in the task instructions
that these workers follow to annotate computer vision datasets. Our preliminary
findings indicate that annotation instructions reflect worldviews imposed on
workers and, through their labor, on datasets. Moreover, we observe that
for-profit goals drive task instructions and that managers and algorithms make
sure annotations are done according to requesters' commands. This configuration
presents a form of commodified labor that perpetuates power asymmetries while
reinforcing social inequalities and is compelled to reproduce them into
datasets and, subsequently, in computer vision systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miceli_M/0/1/0/all/0/1"&gt;Milagros Miceli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Posada_J/0/1/0/all/0/1"&gt;Julian Posada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GOO: A Dataset for Gaze Object Prediction in Retail Environments. (arXiv:2105.10793v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10793</id>
        <link href="http://arxiv.org/abs/2105.10793"/>
        <updated>2021-05-25T01:56:12.437Z</updated>
        <summary type="html"><![CDATA[One of the most fundamental and information-laden actions humans do is to
look at objects. However, a survey of current works reveals that existing
gaze-related datasets annotate only the pixel being looked at, and not the
boundaries of a specific object of interest. This lack of object annotation
presents an opportunity for further advancing gaze estimation research. To this
end, we present a challenging new task called gaze object prediction, where the
goal is to predict a bounding box for a person's gazed-at object. To train and
evaluate gaze networks on this task, we present the Gaze On Objects (GOO)
dataset. GOO is composed of a large set of synthetic images (GOO Synth)
supplemented by a smaller subset of real images (GOO-Real) of people looking at
objects in a retail environment. Our work establishes extensive baselines on
GOO by re-implementing and evaluating selected state-of-the art models on the
task of gaze following and domain adaptation. Code is available on github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomas_H/0/1/0/all/0/1"&gt;Henri Tomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1"&gt;Marcus Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dionido_R/0/1/0/all/0/1"&gt;Raimarc Dionido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ty_M/0/1/0/all/0/1"&gt;Mark Ty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirando_J/0/1/0/all/0/1"&gt;Jonric Mirando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casimiro_J/0/1/0/all/0/1"&gt;Joel Casimiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1"&gt;Rowel Atienza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guinto_R/0/1/0/all/0/1"&gt;Richard Guinto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Mirror Descent for Regularized Reinforcement Learning: A Generalized Framework with Linear Convergence. (arXiv:2105.11066v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11066</id>
        <link href="http://arxiv.org/abs/2105.11066"/>
        <updated>2021-05-25T01:56:12.425Z</updated>
        <summary type="html"><![CDATA[Policy optimization, which learns the policy of interest by maximizing the
value function via large-scale optimization techniques, lies at the heart of
modern reinforcement learning (RL). In addition to value maximization, other
practical considerations arise commonly as well, including the need of
encouraging exploration, and that of ensuring certain structural properties of
the learned policy due to safety, resource and operational constraints. These
considerations can often be accounted for by resorting to regularized RL, which
augments the target value function with a structure-promoting regularization
term.

Focusing on an infinite-horizon discounted Markov decision process, this
paper proposes a generalized policy mirror descent (GPMD) algorithm for solving
regularized RL. As a generalization of policy mirror descent Lan (2021), the
proposed algorithm accommodates a general class of convex regularizers as well
as a broad family of Bregman divergence in cognizant of the regularizer in use.
We demonstrate that our algorithm converges linearly over an entire range of
learning rates, in a dimension-free fashion, to the global solution, even when
the regularizer lacks strong convexity and smoothness. In addition, this linear
convergence feature is provably stable in the face of inexact policy evaluation
and imperfect policy updates. Numerical experiments are provided to corroborate
the applicability and appealing performance of GPMD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1"&gt;Wenhao Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cen_S/0/1/0/all/0/1"&gt;Shicong Cen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Baihe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1"&gt;Yuejie Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cost-Accuracy Aware Adaptive Labeling for Active Learning. (arXiv:2105.11418v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11418</id>
        <link href="http://arxiv.org/abs/2105.11418"/>
        <updated>2021-05-25T01:56:12.417Z</updated>
        <summary type="html"><![CDATA[Conventional active learning algorithms assume a single labeler that produces
noiseless label at a given, fixed cost, and aim to achieve the best
generalization performance for given classifier under a budget constraint.
However, in many real settings, different labelers have different labeling
costs and can yield different labeling accuracies. Moreover, a given labeler
may exhibit different labeling accuracies for different instances. This setting
can be referred to as active learning with diverse labelers with varying costs
and accuracies, and it arises in many important real settings. It is therefore
beneficial to understand how to effectively trade-off between labeling accuracy
for different instances, labeling costs, as well as the informativeness of
training instances, so as to achieve the best generalization performance at the
lowest labeling cost. In this paper, we propose a new algorithm for selecting
instances, labelers (and their corresponding costs and labeling accuracies),
that employs generalization bound of learning with label noise to select
informative instances and labelers so as to achieve higher generalization
accuracy at a lower cost. Our proposed algorithm demonstrates state-of-the-art
performance on five UCI and a real crowdsourcing dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruijiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saar_tsechansky_M/0/1/0/all/0/1"&gt;Maytal Saar-tsechansky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal Ensemble Networks for Biomedical Image Segmentation. (arXiv:2105.10827v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10827</id>
        <link href="http://arxiv.org/abs/2105.10827"/>
        <updated>2021-05-25T01:56:12.410Z</updated>
        <summary type="html"><![CDATA[Despite the astonishing performance of deep-learning based approaches for
visual tasks such as semantic segmentation, they are known to produce
miscalibrated predictions, which could be harmful for critical decision-making
processes. Ensemble learning has shown to not only boost the performance of
individual models but also reduce their miscalibration by averaging independent
predictions. In this scenario, model diversity has become a key factor, which
facilitates individual models converging to different functional solutions. In
this work, we introduce Orthogonal Ensemble Networks (OEN), a novel framework
to explicitly enforce model diversity by means of orthogonal constraints. The
proposed method is based on the hypothesis that inducing orthogonality among
the constituents of the ensemble will increase the overall model diversity. We
resort to a new pairwise orthogonality constraint which can be used to
regularize a sequential ensemble training process, resulting on improved
predictive performance and better calibrated model outputs. We benchmark the
proposed framework in two challenging brain lesion segmentation tasks --brain
tumor and white matter hyper-intensity segmentation in MR images. The
experimental results show that our approach produces more robust and
well-calibrated ensemble models and can deal with challenging tasks in the
context of biomedical image segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Larrazabal_A/0/1/0/all/0/1"&gt;Agostina J. Larrazabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Martinez_C/0/1/0/all/0/1"&gt;C&amp;#xe9;sar Mart&amp;#xed;nez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1"&gt;Jose Dolz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1"&gt;Enzo Ferrante&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PLM: Partial Label Masking for Imbalanced Multi-label Classification. (arXiv:2105.10782v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10782</id>
        <link href="http://arxiv.org/abs/2105.10782"/>
        <updated>2021-05-25T01:56:12.390Z</updated>
        <summary type="html"><![CDATA[Neural networks trained on real-world datasets with long-tailed label
distributions are biased towards frequent classes and perform poorly on
infrequent classes. The imbalance in the ratio of positive and negative samples
for each class skews network output probabilities further from ground-truth
distributions. We propose a method, Partial Label Masking (PLM), which utilizes
this ratio during training. By stochastically masking labels during loss
computation, the method balances this ratio for each class, leading to improved
recall on minority classes and improved precision on frequent classes. The
ratio is estimated adaptively based on the network's performance by minimizing
the KL divergence between predicted and ground-truth distributions. Whereas
most existing approaches addressing data imbalance are mainly focused on
single-label classification and do not generalize well to the multi-label case,
this work proposes a general approach to solve the long-tail data imbalance
issue for multi-label classification. PLM is versatile: it can be applied to
most objective functions and it can be used alongside other strategies for
class imbalance. Our method achieves strong performance when compared to
existing methods on both multi-label (MultiMNIST and MSCOCO) and single-label
(imbalanced CIFAR-10 and CIFAR-100) image classification datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duarte_K/0/1/0/all/0/1"&gt;Kevin Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1"&gt;Yogesh S. Rawat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Few-Shot Classification with Deep Invertible Hybrid Models. (arXiv:2105.10644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10644</id>
        <link href="http://arxiv.org/abs/2105.10644"/>
        <updated>2021-05-25T01:56:12.384Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a deep invertible hybrid model which integrates
discriminative and generative learning at a latent space level for
semi-supervised few-shot classification. Various tasks for classifying new
species from image data can be modeled as a semi-supervised few-shot
classification, which assumes a labeled and unlabeled training examples and a
small support set of the target classes. Predicting target classes with a few
support examples per class makes the learning task difficult for existing
semi-supervised classification methods, including selftraining, which
iteratively estimates class labels of unlabeled training examples to learn a
classifier for the training classes. To exploit unlabeled training examples
effectively, we adopt as the objective function the composite likelihood, which
integrates discriminative and generative learning and suits better with deep
neural networks than the parameter coupling prior, the other popular integrated
learning approach. In our proposed model, the discriminative and generative
models are respectively Prototypical Networks, which have shown excellent
performance in various kinds of few-shot learning, and Normalizing Flow a deep
invertible model which returns the exact marginal likelihood unlike the other
three major methods, i.e., VAE, GAN, and autoregressive model. Our main
originality lies in our integration of these components at a latent space
level, which is effective in preventing overfitting. Experiments using
mini-ImageNet and VGG-Face datasets show that our method outperforms
selftraining based Prototypical Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ohtsubo_Y/0/1/0/all/0/1"&gt;Yusuke Ohtsubo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsukawa_T/0/1/0/all/0/1"&gt;Tetsu Matsukawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_E/0/1/0/all/0/1"&gt;Einoshin Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Texture synthesis via projection onto multiscale, multilayer statistics. (arXiv:2105.10825v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10825</id>
        <link href="http://arxiv.org/abs/2105.10825"/>
        <updated>2021-05-25T01:56:12.377Z</updated>
        <summary type="html"><![CDATA[We provide a new model for texture synthesis based on a multiscale,
multilayer feature extractor. Within the model, textures are represented by a
set of statistics computed from ReLU wavelet coefficients at different layers,
scales and orientations. A new image is synthesized by matching the target
statistics via an iterative projection algorithm. We explain the necessity of
the different types of pre-defined wavelet filters used in our model and the
advantages of multilayer structures for image synthesis. We demonstrate the
power of our model by generating samples of high quality textures and providing
insights into deep representations for texture images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jieqian He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirn_M/0/1/0/all/0/1"&gt;Matthew Hirn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HPNet: Deep Primitive Segmentation Using Hybrid Representations. (arXiv:2105.10620v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10620</id>
        <link href="http://arxiv.org/abs/2105.10620"/>
        <updated>2021-05-25T01:56:12.371Z</updated>
        <summary type="html"><![CDATA[This paper introduces HPNet, a novel deep-learning approach for segmenting a
3D shape represented as a point cloud into primitive patches. The key to deep
primitive segmentation is learning a feature representation that can separate
points of different primitives. Unlike utilizing a single feature
representation, HPNet leverages hybrid representations that combine one learned
semantic descriptor, two spectral descriptors derived from predicted geometric
parameters, as well as an adjacency matrix that encodes sharp edges. Moreover,
instead of merely concatenating the descriptors, HPNet optimally combines
hybrid representations by learning combination weights. This weighting module
builds on the entropy of input features. The output primitive segmentation is
obtained from a mean-shift clustering module. Experimental results on benchmark
datasets ANSI and ABCParts show that HPNet leads to significant performance
gains from baseline approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Siming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhenpei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chongyang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haibin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vouga_E/0/1/0/all/0/1"&gt;Etienne Vouga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qixing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-Radiotherapy PET Image Outcome Prediction by Deep Learning under Biological Model Guidance: A Feasibility Study of Oropharyngeal Cancer Application. (arXiv:2105.10650v1 [physics.med-ph])]]></title>
        <id>http://arxiv.org/abs/2105.10650</id>
        <link href="http://arxiv.org/abs/2105.10650"/>
        <updated>2021-05-25T01:56:12.365Z</updated>
        <summary type="html"><![CDATA[This paper develops a method of biologically guided deep learning for
post-radiation FDG-PET image outcome prediction based on pre-radiation images
and radiotherapy dose information. Based on the classic reaction-diffusion
mechanism, a novel biological model was proposed using a partial differential
equation that incorporates spatial radiation dose distribution as a
patient-specific treatment information variable. A 7-layer
encoder-decoder-based convolutional neural network (CNN) was designed and
trained to learn the proposed biological model. As such, the model could
generate post-radiation FDG-PET image outcome predictions with possible
time-series transition from pre-radiotherapy image states to post-radiotherapy
states. The proposed method was developed using 64 oropharyngeal patients with
paired FDG-PET studies before and after 20Gy delivery (2Gy/daily fraction) by
IMRT. In a two-branch deep learning execution, the proposed CNN learns specific
terms in the biological model from paired FDG-PET images and spatial dose
distribution as in one branch, and the biological model generates post-20Gy
FDG-PET image prediction in the other branch. The proposed method successfully
generated post-20Gy FDG-PET image outcome prediction with breakdown
illustrations of biological model components. Time-series FDG-PET image
predictions were generated to demonstrate the feasibility of disease response
rendering. The developed biologically guided deep learning method achieved
post-20Gy FDG-PET image outcome predictions in good agreement with ground-truth
results. With break-down biological modeling components, the outcome image
predictions could be used in adaptive radiotherapy decision-making to optimize
personalized plans for the best outcome in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Ji_H/0/1/0/all/0/1"&gt;Hangjie Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lafata_K/0/1/0/all/0/1"&gt;Kyle Lafata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mowery_Y/0/1/0/all/0/1"&gt;Yvonne Mowery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Brizel_D/0/1/0/all/0/1"&gt;David Brizel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bertozzi_A/0/1/0/all/0/1"&gt;Andrea L. Bertozzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yin_F/0/1/0/all/0/1"&gt;Fang-Fang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chunhao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[True Few-Shot Learning with Language Models. (arXiv:2105.11447v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11447</id>
        <link href="http://arxiv.org/abs/2105.11447"/>
        <updated>2021-05-25T01:56:12.347Z</updated>
        <summary type="html"><![CDATA[Pretrained language models (LMs) perform well on many tasks even when
learning from a few examples, but prior work uses many held-out examples to
tune various aspects of learning, such as hyperparameters, training objectives,
and natural language templates ("prompts"). Here, we evaluate the few-shot
ability of LMs when such held-out examples are unavailable, a setting we call
true few-shot learning. We test two model selection criteria, cross-validation
and minimum description length, for choosing LM prompts and hyperparameters in
the true few-shot setting. On average, both marginally outperform random
selection and greatly underperform selection based on held-out examples.
Moreover, selection criteria often prefer models that perform significantly
worse than randomly-selected ones. We find similar results even when taking
into account our uncertainty in a model's true performance during selection, as
well as when varying the amount of computation and number of examples used for
selection. Overall, our findings suggest that prior work significantly
overestimated the true few-shot ability of LMs given the difficulty of few-shot
model selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1"&gt;Ethan Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-model Back-translated Distillation for Unsupervised Machine Translation. (arXiv:2006.02163v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02163</id>
        <link href="http://arxiv.org/abs/2006.02163"/>
        <updated>2021-05-25T01:56:12.339Z</updated>
        <summary type="html"><![CDATA[Recent unsupervised machine translation (UMT) systems usually employ three
main principles: initialization, language modeling and iterative
back-translation, though they may apply them differently. Crucially, iterative
back-translation and denoising auto-encoding for language modeling provide data
diversity to train the UMT systems. However, the gains from these
diversification processes has seemed to plateau. We introduce a novel component
to the standard UMT framework called Cross-model Back-translated Distillation
(CBD), that is aimed to induce another level of data diversification that
existing principles lack. CBD is applicable to all previous UMT approaches. In
our experiments, CBD achieves the state of the art in the WMT'14
English-French, WMT'16 English-German and English-Romanian bilingual
unsupervised translation tasks, with 38.2, 30.1, and 36.3 BLEU respectively. It
also yields 1.5-3.3 BLEU improvements in IWSLT English-French and
English-German tasks. Through extensive experimental analyses, we show that CBD
is effective because it embraces data diversity while other similar variants do
not.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1"&gt;Xuan-Phi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thanh-Tung Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kui_W/0/1/0/all/0/1"&gt;Wu Kui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aw_A/0/1/0/all/0/1"&gt;Ai Ti Aw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Sokoban with forward-backward reinforcement learning. (arXiv:2105.01904v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01904</id>
        <link href="http://arxiv.org/abs/2105.01904"/>
        <updated>2021-05-25T01:56:12.333Z</updated>
        <summary type="html"><![CDATA[Despite seminal advances in reinforcement learning in recent years, many
domains where the rewards are sparse, e.g. given only at task completion,
remain quite challenging. In such cases, it can be beneficial to tackle the
task both from its beginning and end, and make the two ends meet. Existing
approaches that do so, however, are not effective in the common scenario where
the strategy needed near the end goal is very different from the one that is
effective earlier on.

In this work we propose a novel RL approach for such settings. In short, we
first train a backward-looking agent with a simple relaxed goal, and then
augment the state representation of the forward-looking agent with
straightforward hint features. This allows the learned forward agent to
leverage information from backward plans, without mimicking their policy.

We demonstrate the efficacy of our approach on the challenging game of
Sokoban, where we substantially surpass learned solvers that generalize across
levels, and are competitive with SOTA performance of the best highly-crafted
systems. Impressively, we achieve these results while learning from a small
number of practice levels and using simple RL techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1"&gt;Yaron Shoham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elidan_G/0/1/0/all/0/1"&gt;Gal Elidan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation of COVID-19 Chest CT Scan Images using Generative Adversarial Networks. (arXiv:2105.11241v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11241</id>
        <link href="http://arxiv.org/abs/2105.11241"/>
        <updated>2021-05-25T01:56:12.325Z</updated>
        <summary type="html"><![CDATA[SARS-CoV-2, also known as COVID-19 or Coronavirus, is a viral contagious
disease that is infected by a novel coronavirus, and has been rapidly spreading
across the globe. It is very important to test and isolate people to reduce
spread, and from here comes the need to do this quickly and efficiently.
According to some studies, Chest-CT outperforms RT-PCR lab testing, which is
the current standard, when diagnosing COVID-19 patients. Due to this, computer
vision researchers have developed various deep learning systems that can
predict COVID-19 using a Chest-CT scan correctly to a certain degree. The
accuracy of these systems is limited since deep learning neural networks such
as CNNs (Convolutional Neural Networks) need a significantly large quantity of
data for training in order to produce good quality results. Since the disease
is relatively recent and more focus has been on CXR (Chest XRay) images, the
available chest CT Scan image dataset is much less. We propose a method, by
utilizing GANs, to generate synthetic chest CT images of both positive and
negative COVID-19 patients. Using a pre-built predictive model, we concluded
that around 40% of the generated images are correctly predicted as COVID-19
positive. The dataset thus generated can be used to train a CNN-based
classifier which can help determine COVID-19 in a patient with greater
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mann_P/0/1/0/all/0/1"&gt;Prerak Mann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jain_S/0/1/0/all/0/1"&gt;Sahaj Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mittal_S/0/1/0/all/0/1"&gt;Saurabh Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhat_A/0/1/0/all/0/1"&gt;Aruna Bhat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data. (arXiv:2105.10837v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10837</id>
        <link href="http://arxiv.org/abs/2105.10837"/>
        <updated>2021-05-25T01:56:12.318Z</updated>
        <summary type="html"><![CDATA[The ultimate goal for an inference model is to be robust and functional in
real life applications. However, training vs. test data domain gaps often
negatively affect model performance. This issue is especially critical for the
monocular 3D human pose estimation problem, in which 3D human data is often
collected in a controlled lab setting. In this paper, we focus on alleviating
the negative effect of domain shift by presenting our adapted human pose (AHuP)
approach that addresses adaptation problems in both appearance and pose spaces.
AHuP is built around a practical assumption that in real applications, data
from target domain could be inaccessible or only limited information can be
acquired. We illustrate the 3D pose estimation performance of AHuP in two
scenarios. First, when source and target data differ significantly in both
appearance and pose spaces, in which we learn from synthetic 3D human data
(with zero real 3D human data) and show comparable performance with the
state-of-the-art 3D pose estimation models that have full access to the real 3D
human pose benchmarks for training. Second, when source and target datasets
differ mainly in the pose space, in which AHuP approach can be applied to
further improve the performance of the state-of-the-art models when tested on
the datasets different from their training dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuangjun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sehgal_N/0/1/0/all/0/1"&gt;Naveen Sehgal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1"&gt;Sarah Ostadabbas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mapping oil palm density at country scale: An active learning approach. (arXiv:2105.11207v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11207</id>
        <link href="http://arxiv.org/abs/2105.11207"/>
        <updated>2021-05-25T01:56:12.311Z</updated>
        <summary type="html"><![CDATA[Accurate mapping of oil palm is important for understanding its past and
future impact on the environment. We propose to map and count oil palms by
estimating tree densities per pixel for large-scale analysis. This allows for
fine-grained analysis, for example regarding different planting patterns. To
that end, we propose a new, active deep learning method to estimate oil palm
density at large scale from Sentinel-2 satellite images, and apply it to
generate complete maps for Malaysia and Indonesia. What makes the regression of
oil palm density challenging is the need for representative reference data that
covers all relevant geographical conditions across a large territory.
Specifically for density estimation, generating reference data involves
counting individual trees. To keep the associated labelling effort low we
propose an active learning (AL) approach that automatically chooses the most
relevant samples to be labelled. Our method relies on estimates of the
epistemic model uncertainty and of the diversity among samples, making it
possible to retrieve an entire batch of relevant samples in a single iteration.
Moreover, our algorithm has linear computational complexity and is easily
parallelisable to cover large areas. We use our method to compute the first oil
palm density map with $10\,$m Ground Sampling Distance (GSD) , for all of
Indonesia and Malaysia and for two different years, 2017 and 2019. The maps
have a mean absolute error of $\pm$7.3 trees/$ha$, estimated from an
independent validation set. We also analyse density variations between
different states within a country and compare them to official estimates.
According to our estimates there are, in total, $>1.2$ billion oil palms in
Indonesia covering $>$15 million $ha$, and $>0.5$ billion oil palms in Malaysia
covering $>6$ million $ha$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s C. Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1"&gt;Stefano D&amp;#x27;Aronco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;Jan D.Wegner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Distribution Detection in Dermatology using Input Perturbation and Subset Scanning. (arXiv:2105.11160v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11160</id>
        <link href="http://arxiv.org/abs/2105.11160"/>
        <updated>2021-05-25T01:56:12.287Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have led to breakthroughs in the development
of automated skin disease classification. As we observe an increasing interest
in these models in the dermatology space, it is crucial to address aspects such
as the robustness towards input data distribution shifts. Current skin disease
models could make incorrect inferences for test samples from different hardware
devices and clinical settings or unknown disease samples, which are
out-of-distribution (OOD) from the training samples.To this end, we propose a
simple yet effective approach that detect these OOD samples prior to making any
decision. The detection is performed via scanning in the latent space
representation (e.g., activations of the inner layers of any pre-trained skin
disease classifier). The input samples could also perturbed to maximise
divergence of OOD samples. We validate our ODD detection approach in two use
cases: 1) identify samples collected from different protocols, and 2) detect
samples from unknown disease classes. Additionally, we evaluate the performance
of the proposed approach and compare it with other state-of-the-art methods.
Furthermore, data-driven dermatology applications may deepen the disparity in
clinical care across racial and ethnic groups since most datasets are reported
to suffer from bias in skin tone distribution. Therefore, we also evaluate the
fairness of these OOD detection methods across different skin tones. Our
experiments resulted in competitive performance across multiple datasets in
detecting OOD samples, which could be used (in the future) to design more
effective transfer learning techniques prior to inferring on these samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hannah Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1"&gt;Girmaw Abebe Tadesse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1"&gt;Celia Cintas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1"&gt;Skyler Speakman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1"&gt;Kush Varshney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video-based Person Re-identification without Bells and Whistles. (arXiv:2105.10678v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10678</id>
        <link href="http://arxiv.org/abs/2105.10678"/>
        <updated>2021-05-25T01:56:12.281Z</updated>
        <summary type="html"><![CDATA[Video-based person re-identification (Re-ID) aims at matching the video
tracklets with cropped video frames for identifying the pedestrians under
different cameras. However, there exists severe spatial and temporal
misalignment for those cropped tracklets due to the imperfect detection and
tracking results generated with obsolete methods. To address this issue, we
present a simple re-Detect and Link (DL) module which can effectively reduce
those unexpected noise through applying the deep learning-based detection and
tracking on the cropped tracklets. Furthermore, we introduce an improved model
called Coarse-to-Fine Axial-Attention Network (CF-AAN). Based on the typical
Non-local Network, we replace the non-local module with three 1-D
position-sensitive axial attentions, in addition to our proposed coarse-to-fine
structure. With the developed CF-AAN, compared to the original non-local
operation, we can not only significantly reduce the computation cost but also
obtain the state-of-the-art performance (91.3% in rank-1 and 86.5% in mAP) on
the large-scale MARS dataset. Meanwhile, by simply adopting our DL module for
data alignment, to our surprise, several baseline models can achieve better or
comparable results with the current state-of-the-arts. Besides, we discover the
errors not only for the identity labels of tracklets but also for the
evaluation protocol for the test data of MARS. We hope that our work can help
the community for the further development of invariant representation without
the hassle of the spatial and temporal alignment and dataset noise. The code,
corrected labels, evaluation protocol, and the aligned data will be available
at https://github.com/jackie840129/CF-AAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chih-Ting Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jun-Cheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chu-Song Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1"&gt;Shao-Yi Chien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Testing Deep Learning Models for Image Analysis Using Object-Relevant Metamorphic Relations. (arXiv:1909.03824v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.03824</id>
        <link href="http://arxiv.org/abs/1909.03824"/>
        <updated>2021-05-25T01:56:12.274Z</updated>
        <summary type="html"><![CDATA[Deep learning models are widely used for image analysis. While they offer
high performance in terms of accuracy, people are concerned about if these
models inappropriately make inferences using irrelevant features that are not
encoded from the target object in a given image. To address the concern, we
propose a metamorphic testing approach that assesses if a given inference is
made based on irrelevant features. Specifically, we propose two novel
metamorphic relations to detect such inappropriate inferences. We applied our
approach to 10 image classification models and 10 object detection models, with
three large datasets, i.e., ImageNet, COCO, and Pascal VOC. Over 5.3% of the
top-5 correct predictions made by the image classification models are subject
to inappropriate inferences using irrelevant features. The corresponding rate
for the object detection models is over 8.5%. Based on the findings, we further
designed a new image generation strategy that can effectively attack existing
models. Comparing with a baseline approach, our strategy can double the success
rate of attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yongqiang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shiqing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1"&gt;Ming Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yepang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1"&gt;Shing-Chi Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taylor saves for later: disentanglement for video prediction using Taylor representation. (arXiv:2105.11062v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11062</id>
        <link href="http://arxiv.org/abs/2105.11062"/>
        <updated>2021-05-25T01:56:12.266Z</updated>
        <summary type="html"><![CDATA[Video prediction is a challenging task with wide application prospects in
meteorology and robot systems. Existing works fail to trade off short-term and
long-term prediction performances and extract robust latent dynamics laws in
video frames. We propose a two-branch seq-to-seq deep model to disentangle the
Taylor feature and the residual feature in video frames by a novel recurrent
prediction module (TaylorCell) and residual module. TaylorCell can expand the
video frames' high-dimensional features into the finite Taylor series to
describe the latent laws. In TaylorCell, we propose the Taylor prediction unit
(TPU) and the memory correction unit (MCU). TPU employs the first input frame's
derivative information to predict the future frames, avoiding error
accumulation. MCU distills all past frames' information to correct the
predicted Taylor feature from TPU. Correspondingly, the residual module
extracts the residual feature complementary to the Taylor feature. On three
generalist datasets (Moving MNIST, TaxiBJ, Human 3.6), our model outperforms or
reaches state-of-the-art models, and ablation experiments demonstrate the
effectiveness of our model in long-term prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1"&gt;Ting Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhuqing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jianan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1"&gt;Shiping Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_A/0/1/0/all/0/1"&gt;Aidong Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiying Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Crowd Counting with Transformers. (arXiv:2105.10926v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10926</id>
        <link href="http://arxiv.org/abs/2105.10926"/>
        <updated>2021-05-25T01:56:12.259Z</updated>
        <summary type="html"><![CDATA[Significant progress on the crowd counting problem has been achieved by
integrating larger context into convolutional neural networks (CNNs). This
indicates that global scene context is essential, despite the seemingly
bottom-up nature of the problem. This may be explained by the fact that context
knowledge can adapt and improve local feature extraction to a given scene. In
this paper, we therefore investigate the role of global context for crowd
counting. Specifically, a pure transformer is used to extract features with
global information from overlapping image patches. Inspired by classification,
we add a context token to the input sequence, to facilitate information
exchange with tokens corresponding to image patches throughout transformer
layers. Due to the fact that transformers do not explicitly model the
tried-and-true channel-wise interactions, we propose a token-attention module
(TAM) to recalibrate encoded features through channel-wise attention informed
by the context token. Beyond that, it is adopted to predict the total person
count of the image through regression-token module (RTM). Extensive experiments
demonstrate that our method achieves state-of-the-art performance on various
datasets, including ShanghaiTech, UCF-QNRF, JHU-CROWD++ and NWPU. On the
large-scale JHU-CROWD++ dataset, our method improves over the previous best
results by 26.9% and 29.9% in terms of MAE and MSE, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guolei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1"&gt;Thomas Probst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1"&gt;Danda Pani Paudel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1"&gt;Nikola Popovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised Cross-view 3D Human Pose Estimation. (arXiv:2105.10882v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10882</id>
        <link href="http://arxiv.org/abs/2105.10882"/>
        <updated>2021-05-25T01:56:12.239Z</updated>
        <summary type="html"><![CDATA[Although monocular 3D human pose estimation methods have made significant
progress, it's far from being solved due to the inherent depth ambiguity.
Instead, exploiting multi-view information is a practical way to achieve
absolute 3D human pose estimation. In this paper, we propose a simple yet
effective pipeline for weakly-supervised cross-view 3D human pose estimation.
By only using two camera views, our method can achieve state-of-the-art
performance in a weakly-supervised manner, requiring no 3D ground truth but
only 2D annotations. Specifically, our method contains two steps: triangulation
and refinement. First, given the 2D keypoints that can be obtained through any
classic 2D detection methods, triangulation is performed across two views to
lift the 2D keypoints into coarse 3D poses.Then, a novel cross-view U-shaped
graph convolutional network (CV-UGCN), which can explore the spatial
configurations and cross-view correlations, is designed to refine the coarse 3D
poses. In particular, the refinement progress is achieved through
weakly-supervised learning, in which geometric and structure-aware consistency
checks are performed. We evaluate our method on the standard benchmark dataset,
Human3.6M. The Mean Per Joint Position Error on the benchmark dataset is 27.4
mm, which outperforms the state-of-the-arts remarkably (27.4 mm vs 30.2 mm).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Guoliang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1"&gt;Runwei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-NILM: A Federated Learning-based Non-Intrusive Load Monitoring Method for Privacy-Protection. (arXiv:2105.11085v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11085</id>
        <link href="http://arxiv.org/abs/2105.11085"/>
        <updated>2021-05-25T01:56:12.232Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) decomposes the total load reading into
appliance-level load signals. Many deep learning-based methods have been
developed to accomplish NILM, and the training of deep neural networks (DNN)
requires massive load data containing different types of appliances. For local
data owners with inadequate load data but expect to accomplish a promising
model performance, the conduction of effective NILM co-modelling is
increasingly significant. While during the cooperation of local data owners,
data exchange and centralized data storage may increase the risk of power
consumer privacy breaches. To eliminate the potential risks, a novel NILM
method named Fed-NILM ap-plying Federated Learning (FL) is proposed in this
paper. In Fed-NILM, local parameters instead of load data are shared among
local data owners. The global model is obtained by weighted averaging the
parameters. In the experiments, Fed-NILM is validated on two real-world
datasets. Besides, a comparison of Fed-NILM with locally-trained NILMs and the
centrally-trained one is conducted in both residential and industrial
scenarios. The experimental results show that Fed-NILM outperforms
locally-trained NILMs and approximate the centrally-trained NILM which is
trained on the entire load dataset without privacy preservation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haijin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Caomingzhe Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Junhua Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments. (arXiv:2105.10937v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10937</id>
        <link href="http://arxiv.org/abs/2105.10937"/>
        <updated>2021-05-25T01:56:12.225Z</updated>
        <summary type="html"><![CDATA[Terrain traversability analysis plays a major role in ensuring safe robotic
navigation in unstructured environments. However, real-time constraints
frequently limit the accuracy of online tests, especially in scenarios where
realistic robot-terrain interactions are complex to model. In this context, we
propose a deep learning framework, trained in an end-to-end fashion from
elevation maps and trajectories, to estimate the occurrence of failure events.
The network is first trained and tested in simulation over synthetic maps
generated by the OpenSimplex algorithm. The prediction performance of the Deep
Learning framework is illustrated by being able to retain over 94% recall of
the original simulator at 30% of the computational time. Finally, the network
is transferred and tested on real elevation maps collected by the SEEKER
consortium during the Martian rover test trial in the Atacama desert in Chile.
We show that transferring and fine-tuning of an application-independent
pre-trained model retains better performance than training uniquely on scarcely
available real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Visca_M/0/1/0/all/0/1"&gt;Marco Visca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1"&gt;Sampo Kuutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1"&gt;Roger Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1"&gt;Saber Fallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Video Object Detection with Spatial-Temporal Transformers. (arXiv:2105.10920v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10920</id>
        <link href="http://arxiv.org/abs/2105.10920"/>
        <updated>2021-05-25T01:56:12.218Z</updated>
        <summary type="html"><![CDATA[Recently, DETR and Deformable DETR have been proposed to eliminate the need
for many hand-designed components in object detection while demonstrating good
performance as previous complex hand-crafted detectors. However, their
performance on Video Object Detection (VOD) has not been well explored. In this
paper, we present TransVOD, an end-to-end video object detection model based on
a spatial-temporal Transformer architecture. The goal of this paper is to
streamline the pipeline of VOD, effectively removing the need for many
hand-crafted components for feature aggregation, e.g., optical flow, recurrent
neural networks, relation networks. Besides, benefited from the object query
design in DETR, our method does not need complicated post-processing methods
such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and
clean. In particular, we present temporal Transformer to aggregate both the
spatial object queries and the feature memories of each frame. Our temporal
Transformer consists of three components: Temporal Deformable Transformer
Encoder (TDTE) to encode the multiple frame spatial details, Temporal Query
Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer
Decoder to obtain current frame detection results. These designs boost the
strong baseline deformable DETR by a significant margin (3%-4% mAP) on the
ImageNet VID dataset. TransVOD yields comparable results performance on the
benchmark of ImageNet VID. We hope our TransVOD can provide a new perspective
for video object detection. Code will be made publicly available at
https://github.com/SJTU-LuHe/TransVOD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qianyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangtai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1"&gt;Li Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenxuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yunhai Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liqing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skeleton-aware multi-scale heatmap regression for 2D hand pose estimation. (arXiv:2105.10904v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10904</id>
        <link href="http://arxiv.org/abs/2105.10904"/>
        <updated>2021-05-25T01:56:12.190Z</updated>
        <summary type="html"><![CDATA[Existing RGB-based 2D hand pose estimation methods learn the joint locations
from a single resolution, which is not suitable for different hand sizes. To
tackle this problem, we propose a new deep learning-based framework that
consists of two main modules. The former presents a segmentation-based approach
to detect the hand skeleton and localize the hand bounding box. The second
module regresses the 2D joint locations through a multi-scale heatmap
regression approach that exploits the predicted hand skeleton as a constraint
to guide the model. Furthermore, we construct a new dataset that is suitable
for both hand detection and pose estimation. We qualitatively and
quantitatively validate our method on two datasets. Results demonstrate that
the proposed method outperforms state-of-the-art and can recover the pose even
in cluttered images and complex poses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kourbane_I/0/1/0/all/0/1"&gt;Ikram Kourbane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genc_Y/0/1/0/all/0/1"&gt;Yakup Genc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Attribute-Object Compositions. (arXiv:2105.11373v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11373</id>
        <link href="http://arxiv.org/abs/2105.11373"/>
        <updated>2021-05-25T01:56:12.152Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning how to predict attribute-object compositions
from images, and its generalization to unseen compositions missing from the
training data. To the best of our knowledge, this is a first large-scale study
of this problem, involving hundreds of thousands of compositions. We train our
framework with images from Instagram using hashtags as noisy weak supervision.
We make careful design choices for data collection and modeling, in order to
handle noisy annotations and unseen compositions. Finally, extensive
evaluations show that learning to compose classifiers outperforms late fusion
of individual attribute and object predictions, especially in the case of
unseen attribute-object pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1"&gt;Filip Radenovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Animesh Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gordo_A/0/1/0/all/0/1"&gt;Albert Gordo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1"&gt;Dhruv Mahajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Remote Sensing Super-Resolution via Migration Image Prior. (arXiv:2105.03579v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03579</id>
        <link href="http://arxiv.org/abs/2105.03579"/>
        <updated>2021-05-25T01:56:12.150Z</updated>
        <summary type="html"><![CDATA[Recently, satellites with high temporal resolution have fostered wide
attention in various practical applications. Due to limitations of bandwidth
and hardware cost, however, the spatial resolution of such satellites is
considerably low, largely limiting their potentials in scenarios that require
spatially explicit information. To improve image resolution, numerous
approaches based on training low-high resolution pairs have been proposed to
address the super-resolution (SR) task. Despite their success, however,
low/high spatial resolution pairs are usually difficult to obtain in satellites
with a high temporal resolution, making such approaches in SR impractical to
use. In this paper, we proposed a new unsupervised learning framework, called
"MIP", which achieves SR tasks without low/high resolution image pairs. First,
random noise maps are fed into a designed generative adversarial network (GAN)
for reconstruction. Then, the proposed method converts the reference image to
latent space as the migration image prior. Finally, we update the input noise
via an implicit method, and further transfer the texture and structured
information from the reference image. Extensive experimental results on the
Draper dataset show that MIP achieves significant improvements over
state-of-the-art methods both quantitatively and qualitatively. The proposed
MIP is open-sourced at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhenfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruiqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Heterogeneity Hypothesis: Finding Layer-Wise Differentiated Network Architectures. (arXiv:2006.16242v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16242</id>
        <link href="http://arxiv.org/abs/2006.16242"/>
        <updated>2021-05-25T01:56:12.149Z</updated>
        <summary type="html"><![CDATA[In this paper, we tackle the problem of convolutional neural network design.
Instead of focusing on the design of the overall architecture, we investigate a
design space that is usually overlooked, i.e. adjusting the channel
configurations of predefined networks. We find that this adjustment can be
achieved by shrinking widened baseline networks and leads to superior
performance. Based on that, we articulate the heterogeneity hypothesis: with
the same training protocol, there exists a layer-wise differentiated network
architecture (LW-DNA) that can outperform the original network with regular
channel configurations but with a lower level of model complexity.

The LW-DNA models are identified without extra computational cost or training
time compared with the original network. This constraint leads to controlled
experiments which direct the focus to the importance of layer-wise specific
channel configurations. LW-DNA models come with advantages related to
overfitting, i.e. the relative relationship between model complexity and
dataset size. Experiments are conducted on various networks and datasets for
image classification, visual tracking and image restoration. The resultant
LW-DNA models consistently outperform the baseline models. Code is available at
https://github.com/ofsoundof/Heterogeneity_Hypothesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yawei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1"&gt;Martin Danelljan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shuhang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape Generation. (arXiv:2008.05440v3 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05440</id>
        <link href="http://arxiv.org/abs/2008.05440"/>
        <updated>2021-05-25T01:56:12.149Z</updated>
        <summary type="html"><![CDATA[D shape generation is a fundamental operation in computer graphics. While
significant progress has been made, especially with recent deep generative
models, it remains a challenge to synthesize high-quality shapes with rich
geometric details and complex structure, in a controllable manner. To tackle
this, we introduce DSG-Net, a deep neural network that learns a disentangled
structured and geometric mesh representation for 3D shapes, where two key
aspects of shapes, geometry, and structure, are encoded in a synergistic manner
to ensure plausibility of the generated shapes, while also being disentangled
as much as possible. This supports a range of novel shape generation
applications with disentangled control, such as interpolation of structure
(geometry) while keeping geometry (structure) unchanged. To achieve this, we
simultaneously learn structure and geometry through variational autoencoders
(VAEs) in a hierarchical manner for both, with bijective mappings at each
level. In this manner, we effectively encode geometry and structure in separate
latent spaces, while ensuring their compatibility: the structure is used to
guide the geometry and vice versa. At the leaf level, the part geometry is
represented using a conditional part VAE, to encode high-quality geometric
details, guided by the structure context as the condition. Our method not only
supports controllable generation applications but also produces high-quality
synthesized shapes, outperforming state-of-the-art methods. The code has been
released at https://github.com/IGLICT/DSG-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1"&gt;Kaichun Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yu-Kun Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas J. Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lin Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PBNS: Physically Based Neural Simulator for Unsupervised Garment Pose Space Deformation. (arXiv:2012.11310v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11310</id>
        <link href="http://arxiv.org/abs/2012.11310"/>
        <updated>2021-05-25T01:56:12.149Z</updated>
        <summary type="html"><![CDATA[We present a methodology to automatically obtain Pose Space Deformation (PSD)
basis for rigged garments through deep learning. Classical approaches rely on
Physically Based Simulations (PBS) to animate clothes. These are general
solutions that, given a sufficiently fine-grained discretization of space and
time, can achieve highly realistic results. However, they are computationally
expensive and any scene modification prompts the need of re-simulation. Linear
Blend Skinning (LBS) with PSD offers a lightweight alternative to PBS, though,
it needs huge volumes of data to learn proper PSD. We propose using deep
learning, formulated as an implicit PBS, to unsupervisedly learn realistic
cloth Pose Space Deformations in a constrained scenario: dressed humans.
Furthermore, we show it is possible to train these models in an amount of time
comparable to a PBS of a few sequences. To the best of our knowledge, we are
the first to propose a neural simulator for cloth. While deep-based approaches
in the domain are becoming a trend, these are data-hungry models. Moreover,
authors often propose complex formulations to better learn wrinkles from PBS
data. Supervised learning leads to physically inconsistent predictions that
require collision solving to be used. Also, dependency on PBS data limits the
scalability of these solutions, while their formulation hinders its
applicability and compatibility. By proposing an unsupervised methodology to
learn PSD for LBS models (3D animation standard), we overcome both of these
drawbacks. Results obtained show cloth-consistency in the animated garments and
meaningful pose-dependant folds and wrinkles. Our solution is extremely
efficient, handles multiple layers of cloth, allows unsupervised outfit
resizing and can be easily applied to any custom 3D avatar.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertiche_H/0/1/0/all/0/1"&gt;Hugo Bertiche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1"&gt;Meysam Madadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1"&gt;Sergio Escalera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Threshold for Online Object Recognition and Re-identification Tasks. (arXiv:2012.14305v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14305</id>
        <link href="http://arxiv.org/abs/2012.14305"/>
        <updated>2021-05-25T01:56:12.148Z</updated>
        <summary type="html"><![CDATA[Choosing a decision threshold is one of the challenging job in any
classification tasks. How much the model is accurate, if the deciding boundary
is not picked up carefully, its entire performance would go in vain. On the
other hand, for imbalance classification where one of the classes is dominant
over another, relying on the conventional method of choosing threshold would
result in poor performance. Even if the threshold or decision boundary is
properly chosen based on machine learning strategies like SVM and decision
tree, it will fail at some point for dynamically varying databases and in case
of identity-features that are more or less similar, like in face recognition
and person re-identification models. Hence, with the need for adaptability of
the decision threshold selection for imbalanced classification and incremental
database size, an online optimization-based statistical feature learning
adaptive technique is developed and tested on the LFW datasets and
self-prepared athletes datasets. This method of adopting adaptive threshold
resulted in 12-45% improvement in the model accuracy compared to the fixed
threshold {0.3,0.5,0.7} that are usually taken via the hit-and-trial method in
any classification and identification tasks. Source code for the complete
algorithm is available at: https://github.com/Varat7v2/adaptive-threshold]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohara_B/0/1/0/all/0/1"&gt;Bharat Bohara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighted Least Squares Twin Support Vector Machine with Fuzzy Rough Set Theory for Imbalanced Data Classification. (arXiv:2105.01198v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01198</id>
        <link href="http://arxiv.org/abs/2105.01198"/>
        <updated>2021-05-25T01:56:12.148Z</updated>
        <summary type="html"><![CDATA[Support vector machines (SVMs) are powerful supervised learning tools
developed to solve classification problems. However, SVMs are likely to perform
poorly in the classification of imbalanced data. The rough set theory presents
a mathematical tool for inference in nondeterministic cases that provides
methods for removing irrelevant information from data. In this work, we propose
an approach that efficiently used fuzzy rough set theory in weighted least
squares twin support vector machine called FRLSTSVM for classification of
imbalanced data. The first innovation is introducing a new fuzzy rough
set-based under-sampling strategy to make the classifier robust in terms of the
imbalanced data. For constructing the two proximal hyperplanes in FRLSTSVM,
data points from the minority class remain unchanged while a subset of data
points in the majority class are selected using a new method. In this model, we
embed the weight biases in the LSTSVM formulations to overcome the bias
phenomenon in the original twin SVM for the classification of imbalanced data.
In order to determine these weights in this formulation, we introduce a new
strategy that uses fuzzy rough set theory as the second innovation.
Experimental results on the famous imbalanced datasets, compared to the related
traditional SVM-based methods, demonstrate the superiority of the proposed
FRLSTSVM model in the imbalanced data classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Behmanesh_M/0/1/0/all/0/1"&gt;Maysam Behmanesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adibi_P/0/1/0/all/0/1"&gt;Peyman Adibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karshenas_H/0/1/0/all/0/1"&gt;Hossein Karshenas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Detection of Practical Universal Adversarial Perturbations. (arXiv:2105.07334v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07334</id>
        <link href="http://arxiv.org/abs/2105.07334"/>
        <updated>2021-05-25T01:56:12.138Z</updated>
        <summary type="html"><![CDATA[Universal Adversarial Perturbations (UAPs) are a prominent class of
adversarial examples that exploit the systemic vulnerabilities and enable
physically realizable and robust attacks against Deep Neural Networks (DNNs).
UAPs generalize across many different inputs; this leads to realistic and
effective attacks that can be applied at scale. In this paper we propose
HyperNeuron, an efficient and scalable algorithm that allows for the real-time
detection of UAPs by identifying suspicious neuron hyper-activations. Our
results show the effectiveness of HyperNeuron on multiple tasks (image
classification, object detection), against a wide variety of universal attacks,
and in realistic scenarios, like perceptual ad-blocking and adversarial
patches. HyperNeuron is able to simultaneously detect both adversarial mask and
patch UAPs with comparable or better performance than existing UAP defenses
whilst introducing a significantly reduced latency of only 0.86 milliseconds
per image. This suggests that many realistic and practical universal attacks
can be reliably mitigated in real-time, which shows promise for the robust
deployment of machine learning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Co_K/0/1/0/all/0/1"&gt;Kenneth T. Co&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munoz_Gonzalez_L/0/1/0/all/0/1"&gt;Luis Mu&amp;#xf1;oz-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanthan_L/0/1/0/all/0/1"&gt;Leslie Kanthan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1"&gt;Emil C. Lupu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Compact CNNs via Collaborative Compression. (arXiv:2105.11228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11228</id>
        <link href="http://arxiv.org/abs/2105.11228"/>
        <updated>2021-05-25T01:56:12.137Z</updated>
        <summary type="html"><![CDATA[Channel pruning and tensor decomposition have received extensive attention in
convolutional neural network compression. However, these two techniques are
traditionally deployed in an isolated manner, leading to significant accuracy
drop when pursuing high compression rates. In this paper, we propose a
Collaborative Compression (CC) scheme, which joints channel pruning and tensor
decomposition to compress CNN models by simultaneously learning the model
sparsity and low-rankness. Specifically, we first investigate the compression
sensitivity of each layer in the network, and then propose a Global Compression
Rate Optimization that transforms the decision problem of compression rate into
an optimization problem. After that, we propose multi-step heuristic
compression to remove redundant compression units step-by-step, which fully
considers the effect of the remaining compression space (i.e., unremoved
compression units). Our method demonstrates superior performance gains over
previous ones on various datasets and backbone architectures. For example, we
achieve 52.9% FLOPs reduction by removing 48.4% parameters on ResNet-50 with
only a Top-1 accuracy drop of 0.56% on ImageNet 2012.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shaohui Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jincheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOE-Net: A Self-Attention and Orientation Encoding Network for Point Cloud based Place Recognition. (arXiv:2011.12430v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12430</id>
        <link href="http://arxiv.org/abs/2011.12430"/>
        <updated>2021-05-25T01:56:12.137Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of place recognition from point cloud data and
introduce a self-attention and orientation encoding network (SOE-Net) that
fully explores the relationship between points and incorporates long-range
context into point-wise local descriptors. Local information of each point from
eight orientations is captured in a PointOE module, whereas long-range feature
dependencies among local descriptors are captured with a self-attention unit.
Moreover, we propose a novel loss function called Hard Positive Hard Negative
quadruplet loss (HPHN quadruplet), that achieves better performance than the
commonly used metric learning loss. Experiments on various benchmark datasets
demonstrate superior performance of the proposed network over the current
state-of-the-art approaches. Our code is released publicly at
https://github.com/Yan-Xia/SOE-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yusheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1"&gt;Juan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stilla_U/0/1/0/all/0/1"&gt;Uwe Stilla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Design Principles of Robust Vision Transformer. (arXiv:2105.07926v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07926</id>
        <link href="http://arxiv.org/abs/2105.07926"/>
        <updated>2021-05-25T01:56:12.137Z</updated>
        <summary type="html"><![CDATA[Recent advances on Vision Transformers (ViT) have shown that
self-attention-based networks, which take advantage of long-range dependencies
modeling ability, surpassed traditional convolution neural networks (CNNs) in
most vision tasks. To further expand the applicability for computer vision,
many improved variants are proposed to re-design the Transformer architecture
by considering the superiority of CNNs, i.e., locality, translation invariance,
for better performance. However, these methods only consider the standard
accuracy or computation cost of the model. In this paper, we rethink the design
principles of ViTs based on the robustness. We found some design components
greatly harm the robustness and generalization ability of ViTs while some
others are beneficial. By combining the robust design components, we propose
Robust Vision Transformer (RVT). RVT is a new vision transformer, which has
superior performance and strong robustness. We further propose two new
plug-and-play techniques called position-aware attention rescaling and
patch-wise augmentation to train our RVT. The experimental results on ImageNet
and six robustness benchmarks show the advanced robustness and generalization
ability of RVT compared with previous Transformers and state-of-the-art CNNs.
Our RVT-S* also achieves Top-1 rank on multiple robustness leaderboards
including ImageNet-C and ImageNet-Sketch. The code will be available at
https://github.com/vtddggg/Robust-Vision-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xiaofeng Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1"&gt;Gege Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaodan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1"&gt;Shaokai Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hui Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heuristic Weakly Supervised 3D Human Pose Estimation in Novel Contexts without Any 3D Pose Ground Truth. (arXiv:2105.10996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10996</id>
        <link href="http://arxiv.org/abs/2105.10996"/>
        <updated>2021-05-25T01:56:12.136Z</updated>
        <summary type="html"><![CDATA[Monocular 3D human pose estimation from a single RGB image has received a lot
attentions in the past few year. Pose inference models with competitive
performance however require supervision with 3D pose ground truth data or at
least known pose priors in their target domain. Yet, these data requirements in
many real-world applications with data collection constraints may not be
achievable. In this paper, we present a heuristic weakly supervised solution,
called HW-HuP to estimate 3D human pose in contexts that no ground truth 3D
data is accessible, even for fine-tuning. HW-HuP learns partial pose priors
from public 3D human pose datasets and uses easy-to-access observations from
the target domain to iteratively estimate 3D human pose and shape in an
optimization and regression hybrid cycle. In our design, depth data as an
auxiliary information is employed as weak supervision during training, yet it
is not needed for the inference. We evaluate HW-HuP performance qualitatively
on datasets of both in-bed human and infant poses, where no ground truth 3D
pose is provided neither any target prior. We also test HW-HuP performance
quantitatively on a publicly available motion capture dataset against the 3D
ground truth. HW-HuP is also able to be extended to other input modalities for
pose estimation tasks especially under adverse vision conditions, such as
occlusion or full darkness. On the Human3.6M benchmark, HW-HuP shows 104.1mm in
MPJPE and 50.4mm in PA MPJPE, comparable to the existing state-of-the-art
approaches that benefit from full 3D pose supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuangjun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaofei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_N/0/1/0/all/0/1"&gt;Nihang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1"&gt;Sarah Ostadabbas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection and Segmentation of Custom Objects using High Distraction Photorealistic Synthetic Data. (arXiv:2007.14354v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.14354</id>
        <link href="http://arxiv.org/abs/2007.14354"/>
        <updated>2021-05-25T01:56:12.136Z</updated>
        <summary type="html"><![CDATA[We show a straightforward and useful methodology for performing instance
segmentation using synthetic data. We apply this methodology on a basic case
and derived insights through quantitative analysis. We created a new public
dataset: The Expo Markers Dataset intended for detection and segmentation
tasks. This dataset contains 5,000 synthetic photorealistic images with their
corresponding pixel-perfect segmentation ground truth. The goal is to achieve
high performance on manually-gathered and annotated real-world data of custom
objects. We do that by creating 3D models of the target objects and other
possible distraction objects and place them within a simulated environment.
Expo Markers were chosen for this task, fitting our requirements of a custom
object due to the exact texture, size and 3D shape. An additional advantage is
the availability of this object in offices around the world for easy testing
and validation of our results. We generate the data using a domain
randomization technique that also simulates other photorealistic objects in the
scene, known as distraction objects. These objects provide visual complexity,
occlusions, and lighting challenges to help our model gain robustness in
training. We are also releasing our manually-gathered datasets used for
comparison and evaluation of our synthetic dataset. This white-paper provides
strong evidence that photorealistic simulated data can be used in practical
real world applications as a more scalable and flexible solution than
manually-captured data. Code is available at the following address:
https://github.com/DataGenResearchTeam/expo_markers]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ron_R/0/1/0/all/0/1"&gt;Roey Ron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elbaz_G/0/1/0/all/0/1"&gt;Gil Elbaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Testing Deep Learning Models for Image Analysis Using Object-Relevant Metamorphic Relations. (arXiv:1909.03824v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.03824</id>
        <link href="http://arxiv.org/abs/1909.03824"/>
        <updated>2021-05-25T01:56:12.116Z</updated>
        <summary type="html"><![CDATA[Deep learning models are widely used for image analysis. While they offer
high performance in terms of accuracy, people are concerned about if these
models inappropriately make inferences using irrelevant features that are not
encoded from the target object in a given image. To address the concern, we
propose a metamorphic testing approach that assesses if a given inference is
made based on irrelevant features. Specifically, we propose two novel
metamorphic relations to detect such inappropriate inferences. We applied our
approach to 10 image classification models and 10 object detection models, with
three large datasets, i.e., ImageNet, COCO, and Pascal VOC. Over 5.3% of the
top-5 correct predictions made by the image classification models are subject
to inappropriate inferences using irrelevant features. The corresponding rate
for the object detection models is over 8.5%. Based on the findings, we further
designed a new image generation strategy that can effectively attack existing
models. Comparing with a baseline approach, our strategy can double the success
rate of attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yongqiang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shiqing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1"&gt;Ming Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yepang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1"&gt;Shing-Chi Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic segmentation of vertebral features on ultrasound spine images using Stacked Hourglass Network. (arXiv:2105.03847v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03847</id>
        <link href="http://arxiv.org/abs/2105.03847"/>
        <updated>2021-05-25T01:56:12.116Z</updated>
        <summary type="html"><![CDATA[Objective: The spinous process angle (SPA) is one of the essential parameters
to denote three-dimensional (3-D) deformity of spine. We propose an automatic
segmentation method based on Stacked Hourglass Network (SHN) to detect the
spinous processes (SP) on ultrasound (US) spine images and to measure the SPAs
of clinical scoliotic subjects. Methods: The network was trained to detect
vertebral SP and laminae as five landmarks on 1200 ultrasound transverse images
and validated on 100 images. All the processed transverse images with
highlighted SP and laminae were reconstructed into a 3D image volume, and the
SPAs were measured on the projected coronal images. The trained network was
tested on 400 images by calculating the percentage of correct keypoints (PCK);
and the SPA measurements were evaluated on 50 scoliotic subjects by comparing
the results from US images and radiographs. Results: The trained network
achieved a high average PCK (86.8%) on the test datasets, particularly the PCK
of SP detection was 90.3%. The SPAs measured from US and radiographic methods
showed good correlation (r>0.85), and the mean absolute differences (MAD)
between two modalities were 3.3{\deg}, which was less than the clinical
acceptance error (5{\deg}). Conclusion: The vertebral features can be
accurately segmented on US spine images using SHN, and the measurement results
of SPA from US data was comparable to the gold standard from radiography.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Hong-Ye Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1"&gt;Song-Han Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yu-Chong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_D/0/1/0/all/0/1"&gt;De-Sen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1"&gt;Xu-Ming He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lou_E/0/1/0/all/0/1"&gt;Edmond Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Rui Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is the State of the Art of Computer Vision-Assisted Cytology? A Systematic Literature Review. (arXiv:2105.11277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11277</id>
        <link href="http://arxiv.org/abs/2105.11277"/>
        <updated>2021-05-25T01:56:12.115Z</updated>
        <summary type="html"><![CDATA[Cytology is a low-cost and non-invasive diagnostic procedure employed to
support the diagnosis of a broad range of pathologies. Computer Vision
technologies, by automatically generating quantitative and objective
descriptions of examinations' contents, can help minimize the chances of
misdiagnoses and shorten the time required for analysis. To identify the
state-of-art of computer vision techniques currently applied to cytology, we
conducted a Systematic Literature Review. We analyzed papers published in the
last 5 years. The initial search was executed in September 2020 and resulted in
431 articles. After applying the inclusion/exclusion criteria, 157 papers
remained, which we analyzed to build a picture of the tendencies and problems
present in this research area, highlighting the computer vision methods,
staining techniques, evaluation metrics, and the availability of the used
datasets and computer code. As a result, we identified that the most used
methods in the analyzed works are deep learning-based (70 papers), while fewer
works employ classic computer vision only (101 papers). The most recurrent
metric used for classification and object detection was the accuracy (33 papers
and 5 papers), while for segmentation it was the Dice Similarity Coefficient
(38 papers). Regarding staining techniques, Papanicolaou was the most employed
one (130 papers), followed by H&E (20 papers) and Feulgen (5 papers). Twelve of
the datasets used in the papers are publicly available, with the DTU/Herlev
dataset being the most used one. We conclude that there still is a lack of
high-quality datasets for many types of stains and most of the works are not
mature enough to be applied in a daily clinical diagnostic routine. We also
identified a growing tendency towards adopting deep learning-based approaches
as the methods of choice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matias_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Vict&amp;#xf3;ria Matias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amorim_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Gustavo Atkinson Amorim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macarini_L/0/1/0/all/0/1"&gt;Luiz Antonio Buschetto Macarini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerentini_A/0/1/0/all/0/1"&gt;Allan Cerentini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onofre_A/0/1/0/all/0/1"&gt;Alexandre Sherlley Casimiro Onofre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onofre_F/0/1/0/all/0/1"&gt;Fabiana Botelho de Miranda Onofre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daltoe_F/0/1/0/all/0/1"&gt;Felipe Perozzo Dalto&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stemmer_M/0/1/0/all/0/1"&gt;Marcelo Ricardo Stemmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wangenheim_A/0/1/0/all/0/1"&gt;Aldo von Wangenheim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06450</id>
        <link href="http://arxiv.org/abs/2103.06450"/>
        <updated>2021-05-25T01:56:12.115Z</updated>
        <summary type="html"><![CDATA[We present a Neural Network based Handwritten Text Recognition (HTR) model
architecture that can be trained to recognize full pages of handwritten or
printed text without image segmentation. Being based on Image to Sequence
architecture, it can extract text present in an image and then sequence it
correctly without imposing any constraints regarding orientation, layout and
size of text and non-text. Further, it can also be trained to generate
auxiliary markup related to formatting, layout and content. We use character
level vocabulary, thereby enabling language and terminology of any subject. The
model achieves a new state-of-art in paragraph level recognition on the IAM
dataset. When evaluated on scans of real world handwritten free form test
answers - beset with curved and slanted lines, drawings, tables, math,
chemistry and other symbols - it performs better than all commercially
available HTR cloud APIs. It is deployed in production as part of a commercial
web application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sumeet S. Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karayev_S/0/1/0/all/0/1"&gt;Sergey Karayev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Vision-and-Language Tasks via Text Generation. (arXiv:2102.02779v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02779</id>
        <link href="http://arxiv.org/abs/2102.02779"/>
        <updated>2021-05-25T01:56:12.114Z</updated>
        <summary type="html"><![CDATA[Existing methods for vision-and-language learning typically require designing
task-specific architectures and objectives for each task. For example, a
multi-label answer classifier for visual question answering, a region scorer
for referring expression comprehension, and a language decoder for image
captioning, etc. To alleviate these hassles, in this work, we propose a unified
framework that learns different tasks in a single architecture with the same
language modeling objective, i.e., multimodal conditional text generation,
where our models learn to generate labels in text based on the visual and
textual inputs. On 7 popular vision-and-language benchmarks, including visual
question answering, referring expression comprehension, visual commonsense
reasoning, most of which have been previously modeled as discriminative tasks,
our generative approach (with a single unified architecture) reaches comparable
performance to recent task-specific state-of-the-art vision-and-language
models. Moreover, our generative approach shows better generalization ability
on questions that have rare answers. Also, we show that our framework allows
multi-task learning in a single architecture with a single set of parameters,
achieving similar performance to separately optimized single-task models. Our
code is publicly available at: https://github.com/j-min/VL-T5]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1"&gt;Jaemin Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation of Gradient-Preserving Images allowing HOG Feature Extraction. (arXiv:2104.01350v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01350</id>
        <link href="http://arxiv.org/abs/2104.01350"/>
        <updated>2021-05-25T01:56:12.113Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a method for generating visually protected images,
referred to as gradient-preserving images. The protected images allow us to
directly extract Histogram-of-Oriented-Gradients (HOG) features for
privacy-preserving machine learning. In an experiment, HOG features extracted
from gradient-preserving images are applied to a face recognition algorithm to
demonstrate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kitayama_M/0/1/0/all/0/1"&gt;Masaki Kitayama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sill-Net: Feature Augmentation with Separated Illumination Representation. (arXiv:2102.03539v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03539</id>
        <link href="http://arxiv.org/abs/2102.03539"/>
        <updated>2021-05-25T01:56:12.111Z</updated>
        <summary type="html"><![CDATA[For visual object recognition tasks, the illumination variations can cause
distinct changes in object appearance and thus confuse the deep neural network
based recognition models. Especially for some rare illumination conditions,
collecting sufficient training samples could be time-consuming and expensive.
To solve this problem, in this paper we propose a novel neural network
architecture called Separating-Illumination Network (Sill-Net). Sill-Net learns
to separate illumination features from images, and then during training we
augment training samples with these separated illumination features in the
feature space. Experimental results demonstrate that our approach outperforms
current state-of-the-art methods in several object classification benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhong Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Ziang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions. (arXiv:2105.08276v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08276</id>
        <link href="http://arxiv.org/abs/2105.08276"/>
        <updated>2021-05-25T01:56:12.111Z</updated>
        <summary type="html"><![CDATA[We introduce NExT-QA, a rigorously designed video question answering
(VideoQA) benchmark to advance video understanding from describing to
explaining the temporal actions. Based on the dataset, we set up multi-choice
and open-ended QA tasks targeting causal action reasoning, temporal action
reasoning, and common scene comprehension. Through extensive analysis of
baselines and established VideoQA techniques, we find that top-performing
methods excel at shallow scene descriptions but are weak in causal and temporal
action reasoning. Furthermore, the models that are effective on multi-choice
QA, when adapted to open-ended QA, still struggle in generalizing the answers.
This raises doubt on the ability of these models to reason and highlights
possibilities for improvement. With detailed results for different question
types and heuristic observations for future works, we hope NExT-QA will guide
the next generation of VQA research to go beyond superficial scene description
towards a deeper understanding of videos. (The dataset and related resources
are available at https://github.com/doc-doc/NExT-QA.git)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Junbin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1"&gt;Xindi Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[luvHarris: A Practical Corner Detector for Event-cameras. (arXiv:2105.11443v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11443</id>
        <link href="http://arxiv.org/abs/2105.11443"/>
        <updated>2021-05-25T01:56:12.110Z</updated>
        <summary type="html"><![CDATA[There have been a number of corner detection methods proposed for event
cameras in the last years, since event-driven computer vision has become more
accessible. Current state-of-the-art have either unsatisfactory accuracy or
real-time performance when considered for practical use; random motion using a
live camera in an unconstrained environment. In this paper, we present yet
another method to perform corner detection, dubbed look-up event-Harris
(luvHarris), that employs the Harris algorithm for high accuracy but manages an
improved event throughput. Our method has two major contributions, 1. a novel
"threshold ordinal event-surface" that removes certain tuning parameters and is
well suited for Harris operations, and 2. an implementation of the Harris
algorithm such that the computational load per-event is minimised and
computational heavy convolutions are performed only 'as-fast-as-possible', i.e.
only as computational resources are available. The result is a practical,
real-time, and robust corner detector that runs more than $2.6\times$ the speed
of current state-of-the-art; a necessity when using high-resolution
event-camera in real-time. We explain the considerations taken for the
approach, compare the algorithm to current state-of-the-art in terms of
computational performance and detection accuracy, and discuss the validity of
the proposed approach for event cameras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Glover_A/0/1/0/all/0/1"&gt;Arren Glover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinale_A/0/1/0/all/0/1"&gt;Aiko Dinale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_L/0/1/0/all/0/1"&gt;Leandro De Souza Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamford_S/0/1/0/all/0/1"&gt;Simeon Bamford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartolozzi_C/0/1/0/all/0/1"&gt;Chiara Bartolozzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters. (arXiv:2105.08721v2 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08721</id>
        <link href="http://arxiv.org/abs/2105.08721"/>
        <updated>2021-05-25T01:56:12.110Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast
dominant wave periods in oceanic waters. First, we use the data collected from
CDIP buoys and apply various data filtering methods. The data filtering methods
allow us to obtain a high-quality dataset for training and validation purposes.
We then extract various wave-based features like wave heights, periods,
skewness, kurtosis, etc., and atmospheric features like humidity, pressure, and
air temperature for the buoys. Afterward, we train algorithms that use LightGBM
and Extra Trees through a hv-block cross-validation scheme to forecast dominant
wave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,
and 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,
Extra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,
15-day ahead, and 30 day ahead prediction. In case of the test dataset,
LightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and
30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day
ahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both
training and the test dataset suggests that the machine learning models
developed in this paper are robust. Since the LightGBM algorithm outperforms ET
for all the windows tested, it is taken as the final algorithm. Note that the
performance of both methods does not decrease significantly as the forecast
horizon increases. Likewise, the proposed method outperforms the numerical
approaches included in this paper in the test dataset. For 1 day ahead
prediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,
0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre
for Medium-range Weather Forecasts (ECMWF) model, which outperforms all the
other methods in the test dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1"&gt;Pujan Pokhrel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ioup_E/0/1/0/all/0/1"&gt;Elias Ioup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hoque_M/0/1/0/all/0/1"&gt;Md Tamjidul Hoque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Abdelguerfi_M/0/1/0/all/0/1"&gt;Mahdi Abdelguerfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Simeonov_J/0/1/0/all/0/1"&gt;Julian Simeonov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-centric Relation Segmentation: Dataset and Solution. (arXiv:2105.11168v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11168</id>
        <link href="http://arxiv.org/abs/2105.11168"/>
        <updated>2021-05-25T01:56:12.103Z</updated>
        <summary type="html"><![CDATA[Vision and language understanding techniques have achieved remarkable
progress, but currently it is still difficult to well handle problems involving
very fine-grained details. For example, when the robot is told to "bring me the
book in the girl's left hand", most existing methods would fail if the girl
holds one book respectively in her left and right hand. In this work, we
introduce a new task named human-centric relation segmentation (HRS), as a
fine-grained case of HOI-det. HRS aims to predict the relations between the
human and surrounding entities and identify the relation-correlated human
parts, which are represented as pixel-level masks. For the above exemplar case,
our HRS task produces results in the form of relation triplets <girl [left
hand], hold, book> and exacts segmentation masks of the book, with which the
robot can easily accomplish the grabbing task. Correspondingly, we collect a
new Person In Context (PIC) dataset for this new task, which contains 17,122
high-resolution images and densely annotated entity segmentation and relations,
including 141 object categories, 23 relation categories and 25 semantic human
parts. We also propose a Simultaneous Matching and Segmentation (SMS) framework
as a solution to the HRS task. I Outputs of the three branches are fused to
produce the final HRS results. Extensive experiments on PIC and V-COCO datasets
show that the proposed SMS method outperforms baselines with the 36 FPS
inference speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Si Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zitian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yulu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1"&gt;Lejian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yue Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1"&gt;Guanghui Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Shuicheng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressively Normalized Self-Attention Network for Video Polyp Segmentation. (arXiv:2105.08468v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08468</id>
        <link href="http://arxiv.org/abs/2105.08468"/>
        <updated>2021-05-25T01:56:12.102Z</updated>
        <summary type="html"><![CDATA[Existing video polyp segmentation (VPS) models typically employ convolutional
neural networks (CNNs) to extract features. However, due to their limited
receptive fields, CNNs can not fully exploit the global temporal and spatial
information in successive video frames, resulting in false-positive
segmentation results. In this paper, we propose the novel PNS-Net
(Progressively Normalized Self-attention Network), which can efficiently learn
representations from polyp videos with real-time speed (~140fps) on a single
RTX 2080 GPU and no post-processing. Our PNS-Net is based solely on a basic
normalized self-attention block, equipping with recurrence and CNNs entirely.
Experiments on challenging VPS datasets demonstrate that the proposed PNS-Net
achieves state-of-the-art performance. We also conduct extensive experiments to
study the effectiveness of the channel split, soft-attention, and progressive
learning strategy. We find that our PNS-Net works well under different
settings, making it a promising solution to the VPS task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yu-Cheng Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Geng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1"&gt;Debesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Triplet-Watershed for Hyperspectral Image Classification. (arXiv:2103.09384v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09384</id>
        <link href="http://arxiv.org/abs/2103.09384"/>
        <updated>2021-05-25T01:56:12.100Z</updated>
        <summary type="html"><![CDATA[Hyperspectral images (HSI) consist of rich spatial and spectral information,
which can potentially be used for several applications. However, noise, band
correlations and high dimensionality restrict the applicability of such data.
This is recently addressed using creative deep learning network architectures
such as ResNet, SSRN, and A2S2K. However, the last layer, i.e the
classification layer, remains unchanged and is taken to be the softmax
classifier. In this article, we propose to use a watershed classifier.
Watershed classifier extends the watershed operator from Mathematical
Morphology for classification. In its vanilla form, the watershed classifier
does not have any trainable parameters. In this article, we propose a novel
approach to train deep learning networks to obtain representations suitable for
the watershed classifier. The watershed classifier exploits the connectivity
patterns, a characteristic of HSI datasets, for better inference. We show that
exploiting such characteristics allows the Triplet-Watershed to achieve
state-of-art results in supervised and semi-supervised contexts. These results
are validated on Indianpines (IP), University of Pavia (UP), Kennedy Space
Center (KSC) and University of Houston (UH) datasets, relying on simple convnet
architecture using a quarter of parameters compared to previous
state-of-the-art networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Challa_A/0/1/0/all/0/1"&gt;Aditya Challa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danda_S/0/1/0/all/0/1"&gt;Sravan Danda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_B/0/1/0/all/0/1"&gt;B.S.Daya Sagar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1"&gt;Laurent Najman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces. (arXiv:2011.13495v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13495</id>
        <link href="http://arxiv.org/abs/2011.13495"/>
        <updated>2021-05-25T01:56:12.095Z</updated>
        <summary type="html"><![CDATA[Reconstructing continuous surfaces from 3D point clouds is a fundamental
operation in 3D geometry processing. Several recent state-of-the-art methods
address this problem using neural networks to learn signed distance functions
(SDFs). In this paper, we introduce \textit{Neural-Pull}, a new approach that
is simple and leads to high quality SDFs. Specifically, we train a neural
network to pull query 3D locations to their closest points on the surface using
the predicted signed distance values and the gradient at the query locations,
both of which are computed by the network itself. The pulling operation moves
each query location with a stride given by the distance predicted by the
network. Based on the sign of the distance, this may move the query location
along or against the direction of the gradient of the SDF. This is a
differentiable operation that allows us to update the signed distance value and
the gradient simultaneously during training. Our outperforming results under
widely used benchmarks demonstrate that we can learn SDFs more accurately and
flexibly for surface reconstruction and single image reconstruction than the
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1"&gt;Baorui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhizhong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu-Shen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1"&gt;Matthias Zwicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Entropy Subspace Clustering Network. (arXiv:2012.03176v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03176</id>
        <link href="http://arxiv.org/abs/2012.03176"/>
        <updated>2021-05-25T01:56:12.095Z</updated>
        <summary type="html"><![CDATA[Deep subspace clustering networks have attracted much attention in subspace
clustering, in which an auto-encoder non-linearly maps the input data into a
latent space, and a fully connected layer named self-expressiveness module is
introduced to learn the affinity matrix via a typical regularization term
(e.g., sparse or low-rank). However, the adopted regularization terms ignore
the connectivity within each subspace, limiting their clustering performance.
In addition, the adopted framework suffers from the coupling issue between the
auto-encoder module and the self-expressiveness module, making the network
training non-trivial. To tackle these two issues, we propose a novel deep
subspace clustering method named Maximum Entropy Subspace Clustering Network
(MESC-Net). Specifically, MESC-Net maximizes the entropy of the affinity matrix
to promote the connectivity within each subspace, in which its elements
corresponding to the same subspace are uniformly and densely distributed.
Furthermore, we design a novel framework to explicitly decouple the
auto-encoder module and the self-expressiveness module. We also theoretically
prove that the learned affinity matrix satisfies the block-diagonal property
under the independent subspaces. Extensive quantitative and qualitative results
on commonly used benchmark datasets validate MESC-Net significantly outperforms
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhihao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yuheng Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Junhui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qingfu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Class Queue for Large Scale Face Recognition In the Wild. (arXiv:2105.11113v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11113</id>
        <link href="http://arxiv.org/abs/2105.11113"/>
        <updated>2021-05-25T01:56:12.085Z</updated>
        <summary type="html"><![CDATA[Learning discriminative representation using large-scale face datasets in the
wild is crucial for real-world applications, yet it remains challenging. The
difficulties lie in many aspects and this work focus on computing resource
constraint and long-tailed class distribution. Recently, classification-based
representation learning with deep neural networks and well-designed losses have
demonstrated good recognition performance. However, the computing and memory
cost linearly scales up to the number of identities (classes) in the training
set, and the learning process suffers from unbalanced classes. In this work, we
propose a dynamic class queue (DCQ) to tackle these two problems. Specifically,
for each iteration during training, a subset of classes for recognition are
dynamically selected and their class weights are dynamically generated
on-the-fly which are stored in a queue. Since only a subset of classes is
selected for each iteration, the computing requirement is reduced. By using a
single server without model parallel, we empirically verify in large-scale
datasets that 10% of classes are sufficient to achieve similar performance as
using all classes. Moreover, the class weights are dynamically generated in a
few-shot manner and therefore suitable for tail classes with only a few
instances. We show clear improvement over a strong baseline in the largest
public dataset Megaface Challenge2 (MF2) which has 672K identities and over 88%
of them have less than 10 instances. Code is available at
https://github.com/bilylee/DCQ]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_T/0/1/0/all/0/1"&gt;Teng Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1"&gt;Haocheng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Junyu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingtuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation. (arXiv:2102.04525v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04525</id>
        <link href="http://arxiv.org/abs/2102.04525"/>
        <updated>2021-05-25T01:56:12.084Z</updated>
        <summary type="html"><![CDATA[Automatic segmentation methods are an important advancement in medical image
analysis. Machine learning techniques, and deep neural networks in particular,
are the state-of-the-art for most medical image segmentation tasks. Issues with
class imbalance pose a significant challenge in medical datasets, with lesions
often occupying a considerably smaller volume relative to the background. Loss
functions used in the training of deep learning algorithms differ in their
robustness to class imbalance, with direct consequences for model convergence.
The most commonly used loss functions for segmentation are based on either the
cross entropy loss, Dice loss or a combination of the two. We propose a Unified
Focal loss, a new framework that generalises Dice and cross entropy-based
losses for handling class imbalance. We evaluate our proposed loss function on
three highly class imbalanced, publicly available medical imaging datasets:
Breast Ultrasound 2017 (BUS2017), Brain Tumour Segmentation 2020 (BraTS20) and
Kidney Tumour Segmentation 2019 (KiTS19). We compare our loss function
performance against six Dice or cross entropy-based loss functions, and
demonstrate that our proposed loss function is robust to class imbalance,
outperforming the other loss functions across datasets. Finally, we use the
Unified Focal loss together with deep supervision to achieve state-of-the-art
results without modification of the original U-Net architecture, with a mean
Dice similarity coefficient (DSC)=0.948 on BUS2017, enhancing tumour region
DSC=0.800 on BraTS20 and kidney tumour DSC=0.758 on KiTS19. This highlights the
importance of carefully selecting a suitable loss function prior to the use of
more complex architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1"&gt;Michael Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1"&gt;Evis Sala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1"&gt;Leonardo Rundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Instance Attention for Multisource Fine-Grained Object Recognition. (arXiv:2105.10983v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10983</id>
        <link href="http://arxiv.org/abs/2105.10983"/>
        <updated>2021-05-25T01:56:12.083Z</updated>
        <summary type="html"><![CDATA[Multisource image analysis that leverages complementary spectral, spatial,
and structural information benefits fine-grained object recognition that aims
to classify an object into one of many similar subcategories. However, for
multisource tasks that involve relatively small objects, even the smallest
registration errors can introduce high uncertainty in the classification
process. We approach this problem from a weakly supervised learning perspective
in which the input images correspond to larger neighborhoods around the
expected object locations where an object with a given class label is present
in the neighborhood without any knowledge of its exact location. The proposed
method uses a single-source deep instance attention model with parallel
branches for joint localization and classification of objects, and extends this
model into a multisource setting where a reference source that is assumed to
have no location uncertainty is used to aid the fusion of multiple sources in
four different levels: probability level, logit level, feature level, and pixel
level. We show that all levels of fusion provide higher accuracies compared to
the state-of-the-art, with the best performing method of feature-level fusion
resulting in 53% accuracy for the recognition of 40 different types of trees,
corresponding to an improvement of 5.7% over the best performing baseline when
RGB, multispectral, and LiDAR data are used. We also provide an in-depth
comparison by evaluating each model at various parameter complexity settings,
where the increased model capacity results in a further improvement of 6.3%
over the default capacity setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aygunes_B/0/1/0/all/0/1"&gt;Bulut Aygunes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1"&gt;Ramazan Gokberk Cinbis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aksoy_S/0/1/0/all/0/1"&gt;Selim Aksoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSPC-Net: Semi-supervised Semantic 3D Point Cloud Segmentation Network. (arXiv:2104.07861v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07861</id>
        <link href="http://arxiv.org/abs/2104.07861"/>
        <updated>2021-05-25T01:56:12.083Z</updated>
        <summary type="html"><![CDATA[Point cloud semantic segmentation is a crucial task in 3D scene
understanding. Existing methods mainly focus on employing a large number of
annotated labels for supervised semantic segmentation. Nonetheless, manually
labeling such large point clouds for the supervised segmentation task is
time-consuming. In order to reduce the number of annotated labels, we propose a
semi-supervised semantic point cloud segmentation network, named SSPC-Net,
where we train the semantic segmentation network by inferring the labels of
unlabeled points from the few annotated 3D points. In our method, we first
partition the whole point cloud into superpoints and build superpoint graphs to
mine the long-range dependencies in point clouds. Based on the constructed
superpoint graph, we then develop a dynamic label propagation method to
generate the pseudo labels for the unsupervised superpoints. Particularly, we
adopt a superpoint dropout strategy to dynamically select the generated pseudo
labels. In order to fully exploit the generated pseudo labels of the
unsupervised superpoints, we furthermore propose a coupled attention mechanism
for superpoint feature embedding. Finally, we employ the cross-entropy loss to
train the semantic segmentation network with the labels of the supervised
superpoints and the pseudo labels of the unsupervised superpoints. Experiments
on various datasets demonstrate that our semi-supervised segmentation method
can achieve better performance than the current semi-supervised segmentation
method with fewer annotated 3D points. Our code is available at
https://github.com/MMCheng/SSPC-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Mingmei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1"&gt;Le Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COTR: Convolution in Transformer Network for End to End Polyp Detection. (arXiv:2105.10925v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10925</id>
        <link href="http://arxiv.org/abs/2105.10925"/>
        <updated>2021-05-25T01:56:12.081Z</updated>
        <summary type="html"><![CDATA[Purpose: Colorectal cancer (CRC) is the second most common cause of cancer
mortality worldwide. Colonoscopy is a widely used technique for colon screening
and polyp lesions diagnosis. Nevertheless, manual screening using colonoscopy
suffers from a substantial miss rate of polyps and is an overwhelming burden
for endoscopists. Computer-aided diagnosis (CAD) for polyp detection has the
potential to reduce human error and human burden. However, current polyp
detection methods based on object detection framework need many handcrafted
pre-processing and post-processing operations or user guidance that require
domain-specific knowledge.

Methods: In this paper, we propose a convolution in transformer (COTR)
network for end-to-end polyp detection. Motivated by the detection transformer
(DETR), COTR is constituted by a CNN for feature extraction, transformer
encoder layers interleaved with convolutional layers for feature encoding and
recalibration, transformer decoder layers for object querying, and a
feed-forward network for detection prediction. Considering the slow convergence
of DETR, COTR embeds convolution layers into transformer encoder for feature
reconstruction and convergence acceleration.

Results: Experimental results on two public polyp datasets show that COTR
achieved 91.49\% precision, 82.69% sensitivity, and 86.87% F1-score on the
ETIS-LARIB, and 91.67% precision, 93.54% sensitivity, and 92.60% F1-score on
the CVC-ColonDB.

Conclusion: This study proposed an end to end detection method based on
detection transformer for colorectal polyp detection. Experimental results on
ETIS-LARIB and CVC-ColonDB dataset demonstrated that the proposed model
achieved comparable performance against state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhiqiang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chaonan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shaohua Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instances as Queries. (arXiv:2105.01928v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01928</id>
        <link href="http://arxiv.org/abs/2105.01928"/>
        <updated>2021-05-25T01:56:12.072Z</updated>
        <summary type="html"><![CDATA[Recently, query based object detection frameworks achieve comparable
performance with previous state-of-the-art object detectors. However, how to
fully leverage such frameworks to perform instance segmentation remains an open
problem. In this paper, we present QueryInst (Instances as Queries), a query
based instance segmentation method driven by parallel supervision on dynamic
mask heads. The key insight of QueryInst is to leverage the intrinsic
one-to-one correspondence in object queries across different stages, as well as
one-to-one correspondence between mask RoI features and object queries in the
same stage. This approach eliminates the explicit multi-stage mask head
connection and the proposal distribution inconsistency issues inherent in
non-query based multi-stage instance segmentation methods. We conduct extensive
experiments on three challenging benchmarks, i.e., COCO, CityScapes, and
YouTube-VIS to evaluate the effectiveness of QueryInst in instance segmentation
and video instance segmentation (VIS) task. Specifically, using ResNet-101-FPN
backbone, QueryInst obtains 48.1 box AP and 42.8 mask AP on COCO test-dev,
which is 2 points higher than HTC in terms of both box AP and mask AP, while
runs 2.4 times faster. For video instance segmentation, QueryInst achieves the
best performance among all online VIS approaches and strikes a decent
speed-accuracy trade-off. Code is available at
\url{https://github.com/hustvl/QueryInst}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuxin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shusheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1"&gt;Chen Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Ying Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Bin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SmartPatch: Improving Handwritten Word Imitation with Patch Discriminators. (arXiv:2105.10528v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10528</id>
        <link href="http://arxiv.org/abs/2105.10528"/>
        <updated>2021-05-25T01:56:12.070Z</updated>
        <summary type="html"><![CDATA[As of recent generative adversarial networks have allowed for big leaps in
the realism of generated images in diverse domains, not the least of which
being handwritten text generation. The generation of realistic-looking
hand-written text is important because it can be used for data augmentation in
handwritten text recognition (HTR) systems or human-computer interaction. We
propose SmartPatch, a new technique increasing the performance of current
state-of-the-art methods by augmenting the training feedback with a tailored
solution to mitigate pen-level artifacts. We combine the well-known patch loss
with information gathered from the parallel trained handwritten text
recognition system and the separate characters of the word. This leads to a
more enhanced local discriminator and results in more realistic and
higher-quality generated handwritten words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mattick_A/0/1/0/all/0/1"&gt;Alexander Mattick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayr_M/0/1/0/all/0/1"&gt;Martin Mayr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seuret_M/0/1/0/all/0/1"&gt;Mathias Seuret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christlein_V/0/1/0/all/0/1"&gt;Vincent Christlein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIASSR: An Approach for Medical Image Arbitrary Scale Super-Resolution. (arXiv:2105.10738v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10738</id>
        <link href="http://arxiv.org/abs/2105.10738"/>
        <updated>2021-05-25T01:56:12.070Z</updated>
        <summary type="html"><![CDATA[Single image super-resolution (SISR) aims to obtain a high-resolution output
from one low-resolution image. Currently, deep learning-based SISR approaches
have been widely discussed in medical image processing, because of their
potential to achieve high-quality, high spatial resolution images without the
cost of additional scans. However, most existing methods are designed for
scale-specific SR tasks and are unable to generalise over magnification scales.
In this paper, we propose an approach for medical image arbitrary-scale
super-resolution (MIASSR), in which we couple meta-learning with generative
adversarial networks (GANs) to super-resolve medical images at any scale of
magnification in (1, 4]. Compared to state-of-the-art SISR algorithms on
single-modal magnetic resonance (MR) brain images (OASIS-brains) and
multi-modal MR brain images (BraTS), MIASSR achieves comparable fidelity
performance and the best perceptual quality with the smallest model size. We
also employ transfer learning to enable MIASSR to tackle SR tasks of new
medical modalities, such as cardiac MR images (ACDC) and chest computed
tomography images (COVID-CT). The source code of our work is also public. Thus,
MIASSR has the potential to become a new foundational pre-/post-processing step
in clinical image analysis tasks such as reconstruction, image quality
enhancement, and segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chuan Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Lio&amp;#x27;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VS-Net: Voting with Segmentation for Visual Localization. (arXiv:2105.10886v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10886</id>
        <link href="http://arxiv.org/abs/2105.10886"/>
        <updated>2021-05-25T01:56:12.068Z</updated>
        <summary type="html"><![CDATA[Visual localization is of great importance in robotics and computer vision.
Recently, scene coordinate regression based methods have shown good performance
in visual localization in small static scenes. However, it still estimates
camera poses from many inferior scene coordinates. To address this problem, we
propose a novel visual localization framework that establishes 2D-to-3D
correspondences between the query image and the 3D map with a series of
learnable scene-specific landmarks. In the landmark generation stage, the 3D
surfaces of the target scene are over-segmented into mosaic patches whose
centers are regarded as the scene-specific landmarks. To robustly and
accurately recover the scene-specific landmarks, we propose the Voting with
Segmentation Network (VS-Net) to segment the pixels into different landmark
patches with a segmentation branch and estimate the landmark locations within
each patch with a landmark location voting branch. Since the number of
landmarks in a scene may reach up to 5000, training a segmentation network with
such a large number of classes is both computation and memory costly for the
commonly used cross-entropy loss. We propose a novel prototype-based triplet
loss with hard negative mining, which is able to train semantic segmentation
networks with a large number of labels efficiently. Our proposed VS-Net is
extensively tested on multiple public benchmarks and can outperform
state-of-the-art visual localization methods. Code and models are available at
\href{https://github.com/zju3dv/VS-Net}{https://github.com/zju3dv/VS-Net}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhaoyang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Han Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yijin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bangbang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiaowei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hujun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guofeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comprehensible Convolutional Neural Networks via Guided Concept Learning. (arXiv:2101.03919v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03919</id>
        <link href="http://arxiv.org/abs/2101.03919"/>
        <updated>2021-05-25T01:56:12.067Z</updated>
        <summary type="html"><![CDATA[Learning concepts that are consistent with human perception is important for
Deep Neural Networks to win end-user trust. Post-hoc interpretation methods
lack transparency in the feature representations learned by the models. This
work proposes a guided learning approach with an additional concept layer in a
CNN- based architecture to learn the associations between visual features and
word phrases. We design an objective function that optimizes both prediction
accuracy and semantics of the learned feature representations. Experiment
results demonstrate that the proposed model can learn concepts that are
consistent with human perception and their corresponding contributions to the
model decision without compromising accuracy. Further, these learned concepts
are transferable to new classes of objects that have similar concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramanayake_S/0/1/0/all/0/1"&gt;Sandareka Wickramanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wynne Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mong Li Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoMuDANet: Unsupervised Adaptation for Visual Scene Understanding in Unstructured Driving Environments. (arXiv:2010.03523v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03523</id>
        <link href="http://arxiv.org/abs/2010.03523"/>
        <updated>2021-05-25T01:56:12.066Z</updated>
        <summary type="html"><![CDATA[We present an unsupervised adaptation approach for visual scene understanding
in unstructured traffic environments. Our method is designed for unstructured
real-world scenarios with dense and heterogeneous traffic consisting of cars,
trucks, two-and three-wheelers, and pedestrians. We describe a new semantic
segmentation technique based on unsupervised domain adaptation (DA), that can
identify the class or category of each region in RGB images or videos. We also
present a novel self-training algorithm (Alt-Inc) for multi-source DA that
improves the accuracy. Our overall approach is a deep learning-based technique
and consists of an unsupervised neural network that achieves 87.18% accuracy on
the challenging India Driving Dataset. Our method works well on roads that may
not be well-marked or may include dirt, unidentifiable debris, potholes, etc. A
key aspect of our approach is that it can also identify objects that are
encountered by the model for the fist time during the testing phase. We compare
our method against the state-of-the-art methods and show an improvement of
5.17% - 42.9%. Furthermore, we also conduct user studies that qualitatively
validate the improvements in visual scene understanding of unstructured driving
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1"&gt;Divya Kothandaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohan Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes. (arXiv:2105.10872v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10872</id>
        <link href="http://arxiv.org/abs/2105.10872"/>
        <updated>2021-05-25T01:56:12.061Z</updated>
        <summary type="html"><![CDATA[Malicious application of deepfakes (i.e., technologies can generate target
faces or face attributes) has posed a huge threat to our society. The fake
multimedia content generated by deepfake models can harm the reputation and
even threaten the property of the person who has been impersonated.
Fortunately, the adversarial watermark could be used for combating deepfake
models, leading them to generate distorted images. The existing methods require
an individual training process for every facial image, to generate the
adversarial watermark against a specific deepfake model, which are extremely
inefficient. To address this problem, we propose a universal adversarial attack
method on deepfake models, to generate a Cross-Model Universal Adversarial
Watermark (CMUA-Watermark) that can protect thousands of facial images from
multiple deepfake models. Specifically, we first propose a cross-model
universal attack pipeline by attacking multiple deepfake models and combining
gradients from these models iteratively. Then we introduce a batch-based method
to alleviate the conflict of adversarial watermarks generated by different
facial images. Finally, we design a more reasonable and comprehensive
evaluation method for evaluating the effectiveness of the adversarial
watermark. Experimental results demonstrate that the proposed CMUA-Watermark
can effectively distort the fake facial images generated by deepfake models and
successfully protect facial images from deepfakes in real scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wei Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weisi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai-Kuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Robustness of Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2105.10843v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10843</id>
        <link href="http://arxiv.org/abs/2105.10843"/>
        <updated>2021-05-25T01:56:12.054Z</updated>
        <summary type="html"><![CDATA[Recent studies imply that deep neural networks are vulnerable to adversarial
examples -- inputs with a slight but intentional perturbation are incorrectly
classified by the network. Such vulnerability makes it risky for some
security-related applications (e.g., semantic segmentation in autonomous cars)
and triggers tremendous concerns on the model reliability. For the first time,
we comprehensively evaluate the robustness of existing UDA methods and propose
a robust UDA approach. It is rooted in two observations: (i) the robustness of
UDA methods in semantic segmentation remains unexplored, which pose a security
concern in this field; and (ii) although commonly used self-supervision (e.g.,
rotation and jigsaw) benefits image tasks such as classification and
recognition, they fail to provide the critical supervision signals that could
learn discriminative representation for segmentation tasks. These observations
motivate us to propose adversarial self-supervision UDA (or ASSUDA) that
maximizes the agreement between clean images and their adversarial examples by
a contrastive loss in the output space. Extensive empirical studies on commonly
used benchmarks demonstrate that ASSUDA is resistant to adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1"&gt;Weizhi An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hehuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuzhi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peilin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyper-Convolution Networks for Biomedical Image Segmentation. (arXiv:2105.10559v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10559</id>
        <link href="http://arxiv.org/abs/2105.10559"/>
        <updated>2021-05-25T01:56:12.049Z</updated>
        <summary type="html"><![CDATA[The convolution operation is a central building block of neural network
architectures widely used in computer vision. The size of the convolution
kernels determines both the expressiveness of convolutional neural networks
(CNN), as well as the number of learnable parameters. Increasing the network
capacity to capture rich pixel relationships requires increasing the number of
learnable parameters, often leading to overfitting and/or lack of robustness.
In this paper, we propose a powerful novel building block, the
hyper-convolution, which implicitly represents the convolution kernel as a
function of kernel coordinates. Hyper-convolutions enable decoupling the kernel
size, and hence its receptive field, from the number of learnable parameters.
In our experiments, focused on challenging biomedical image segmentation tasks,
we demonstrate that replacing regular convolutions with hyper-convolutions
leads to more efficient architectures that achieve improved accuracy. Our
analysis also shows that learned hyper-convolutions are naturally regularized,
which can offer better generalization performance. We believe that
hyper-convolutions can be a powerful building block in future neural network
architectures solving computer vision tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tianyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1"&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sabuncu_M/0/1/0/all/0/1"&gt;Mert R. Sabuncu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FBI-Denoiser: Fast Blind Image Denoiser for Poisson-Gaussian Noise. (arXiv:2105.10967v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10967</id>
        <link href="http://arxiv.org/abs/2105.10967"/>
        <updated>2021-05-25T01:56:12.041Z</updated>
        <summary type="html"><![CDATA[We consider the challenging blind denoising problem for Poisson-Gaussian
noise, in which no additional information about clean images or noise level
parameters is available. Particularly, when only "single" noisy images are
available for training a denoiser, the denoising performance of existing
methods was not satisfactory. Recently, the blind pixelwise affine image
denoiser (BP-AIDE) was proposed and significantly improved the performance in
the above setting, to the extent that it is competitive with denoisers which
utilized additional information. However, BP-AIDE seriously suffered from slow
inference time due to the inefficiency of noise level estimation procedure and
that of the blind-spot network (BSN) architecture it used. To that end, we
propose Fast Blind Image Denoiser (FBI-Denoiser) for Poisson-Gaussian noise,
which consists of two neural network models; 1) PGE-Net that estimates
Poisson-Gaussian noise parameters 2000 times faster than the conventional
methods and 2) FBI-Net that realizes a much more efficient BSN for pixelwise
affine denoiser in terms of the number of parameters and inference speed.
Consequently, we show that our FBI-Denoiser blindly trained solely based on
single noisy images can achieve the state-of-the-art performance on several
real-world noisy image benchmark datasets with much faster inference time (x
10), compared to BP-AIDE. The official code of our method is available at
https://github.com/csm9493/FBI-Denoiser.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Byun_J/0/1/0/all/0/1"&gt;Jaeseok Byun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cha_S/0/1/0/all/0/1"&gt;Sungmin Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Application of Deep Self-Attention in Knowledge Tracing. (arXiv:2105.07909v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07909</id>
        <link href="http://arxiv.org/abs/2105.07909"/>
        <updated>2021-05-25T01:56:12.027Z</updated>
        <summary type="html"><![CDATA[The development of intelligent tutoring system has greatly influenced the way
students learn and practice, which increases their learning efficiency. The
intelligent tutoring system must model learners' mastery of the knowledge
before providing feedback and advices to learners, so one class of algorithm
called "knowledge tracing" is surely important. This paper proposed Deep
Self-Attentive Knowledge Tracing (DSAKT) based on the data of PTA, an online
assessment system used by students in many universities in China, to help these
students learn more efficiently. Experimentation on the data of PTA shows that
DSAKT outperforms the other models for knowledge tracing an improvement of AUC
by 2.1% on average, and this model also has a good performance on the ASSIST
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1"&gt;Junhao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qingchun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_N/0/1/0/all/0/1"&gt;Ning Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bochun Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal set of Observables for the Koopman Operator through Causal Embedding. (arXiv:2105.10759v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2105.10759</id>
        <link href="http://arxiv.org/abs/2105.10759"/>
        <updated>2021-05-25T01:56:12.011Z</updated>
        <summary type="html"><![CDATA[Obtaining repeated measurements from physical and natural systems for
building a more informative dynamical model of such systems is engraved in
modern science. Results in reconstructing equivalent chaotic dynamical systems
through delay coordinate mappings, Koopman operator based data-driven approach
and reservoir computing methods have shown the possibility of finding model
equations on a new phase space that is relatable to the dynamical system
generating the data. Recently, rigorous results that point to reducing the
functional complexity of the map that describes the dynamics in the new phase
have made the Koopman operator based approach very attractive for data-driven
modeling. However, choosing a set of nonlinear observable functions that can
work for different data sets is an open challenge. We use driven dynamical
systems comparable to that in reservoir computing with the \emph{causal
embedding property} to obtain the right set of observables through which the
dynamics in the new space is made equivalent or topologically conjugate to the
original system. Deep learning methods are used to learn a map that emerges as
a consequence of the topological conjugacy. Besides stability, amenability for
hardware implementations, causal embedding based models provide long-term
consistency even for maps that have failed under previously reported
data-driven or machine learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Manjunath_G/0/1/0/all/0/1"&gt;G Manjunath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Clercq_A/0/1/0/all/0/1"&gt;A de Clercq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Related Self-Supervised Learning for Remote Sensing Image Change Detection. (arXiv:2105.04951v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04951</id>
        <link href="http://arxiv.org/abs/2105.04951"/>
        <updated>2021-05-25T01:56:12.000Z</updated>
        <summary type="html"><![CDATA[Change detection for remote sensing images is widely applied for urban change
detection, disaster assessment and other fields. However, most of the existing
CNN-based change detection methods still suffer from the problem of inadequate
pseudo-changes suppression and insufficient feature representation. In this
work, an unsupervised change detection method based on Task-related
Self-supervised Learning Change Detection network with smooth mechanism(TSLCD)
is proposed to eliminate it. The main contributions include: (1) the
task-related self-supervised learning module is introduced to extract spatial
features more effectively. (2) a hard-sample-mining loss function is applied to
pay more attention to the hard-to-classify samples. (3) a smooth mechanism is
utilized to remove some of pseudo-changes and noise. Experiments on four remote
sensing change detection datasets reveal that the proposed TSLCD method
achieves the state-of-the-art for change detection task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhinan Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhiyu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yuan Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A hybrid classification-regression approach for 3D hand pose estimation using graph convolutional networks. (arXiv:2105.10902v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10902</id>
        <link href="http://arxiv.org/abs/2105.10902"/>
        <updated>2021-05-25T01:56:11.965Z</updated>
        <summary type="html"><![CDATA[Hand pose estimation is a crucial part of a wide range of augmented reality
and human-computer interaction applications. Predicting the 3D hand pose from a
single RGB image is challenging due to occlusion and depth ambiguities.
GCN-based (Graph Convolutional Networks) methods exploit the structural
relationship similarity between graphs and hand joints to model kinematic
dependencies between joints. These techniques use predefined or globally
learned joint relationships, which may fail to capture pose-dependent
constraints. To address this problem, we propose a two-stage GCN-based
framework that learns per-pose relationship constraints. Specifically, the
first phase quantizes the 2D/3D space to classify the joints into 2D/3D blocks
based on their locality. This spatial dependency information guides this phase
to estimate reliable 2D and 3D poses. The second stage further improves the 3D
estimation through a GCN-based module that uses an adaptative nearest neighbor
algorithm to determine joint relationships. Extensive experiments show that our
multi-stage GCN approach yields an efficient model that produces accurate 2D/3D
hand poses and outperforms the state-of-the-art on two public datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kourbane_I/0/1/0/all/0/1"&gt;Ikram Kourbane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genc_Y/0/1/0/all/0/1"&gt;Yakup Genc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soccer Player Tracking in Low Quality Video. (arXiv:2105.10700v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10700</id>
        <link href="http://arxiv.org/abs/2105.10700"/>
        <updated>2021-05-25T01:56:11.958Z</updated>
        <summary type="html"><![CDATA[In this paper we propose a system capable of tracking multiple soccer players
in different types of video quality. The main goal, in contrast to most
state-of-art soccer player tracking systems, is the ability of execute
effectively tracking in videos of low-quality. We adapted a state-of-art
Multiple Object Tracking to the task. In order to do that adaptation, we
created a Detection and a Tracking Dataset for 3 different qualities of video.
The results of our system are conclusive of its high performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martins_E/0/1/0/all/0/1"&gt;Eloi Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brito_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Henrique Brito&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drifting Features: Detection and evaluation in the context of automatic RRLs identification in VVV. (arXiv:2105.01714v3 [astro-ph.IM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01714</id>
        <link href="http://arxiv.org/abs/2105.01714"/>
        <updated>2021-05-25T01:56:11.951Z</updated>
        <summary type="html"><![CDATA[As most of the modern astronomical sky surveys produce data faster than
humans can analyze it, Machine Learning (ML) has become a central tool in
Astronomy. Modern ML methods can be characterized as highly resistant to some
experimental errors. However, small changes on the data over long distances or
long periods of time, which cannot be easily detected by statistical methods,
can be harmful to these methods. We develop a new strategy to cope with this
problem, also using ML methods in an innovative way, to identify these
potentially harmful features. We introduce and discuss the notion of Drifting
Features, related with small changes in the properties as measured in the data
features. We use the identification of RRLs in VVV based on an earlier work and
introduce a method for detecting Drifting Features. Our method forces a
classifier to learn the tile of origin of diverse sources (mostly stellar
'point sources'), and select the features more relevant to the task of finding
candidates to Drifting Features. We show that this method can efficiently
identify a reduced set of features that contains useful information about the
tile of origin of the sources. For our particular example of detecting RRLs in
VVV, we find that Drifting Features are mostly related to color indices. On the
other hand, we show that, even if we have a clear set of Drifting Features in
our problem, they are mostly insensitive to the identification of RRLs.
Drifting Features can be efficiently identified using ML methods. However, in
our example, removing Drifting Features does not improve the identification of
RRLs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Cabral_J/0/1/0/all/0/1"&gt;J. B. Cabral&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lares_M/0/1/0/all/0/1"&gt;M. Lares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gurovich_S/0/1/0/all/0/1"&gt;S. Gurovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Minniti_D/0/1/0/all/0/1"&gt;D. Minniti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Granitto_P/0/1/0/all/0/1"&gt;P. M. Granitto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Federated Learning Framework for Non-Intrusive Load Monitoring. (arXiv:2104.01618v1 [eess.SP] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2104.01618</id>
        <link href="http://arxiv.org/abs/2104.01618"/>
        <updated>2021-05-25T01:56:11.862Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) aims at decomposing the total reading of
the household power consumption into appliance-wise ones, which is beneficial
for consumer behavior analysis as well as energy conservation. NILM based on
deep learning has been a focus of research. To train a better neural network,
it is necessary for the network to be fed with massive data containing various
appliances and reflecting consumer behavior habits. Therefore, data cooperation
among utilities and DNOs (distributed network operators) who own the NILM data
has been increasingly significant. During the cooperation, however, risks of
consumer privacy leakage and losses of data control rights arise. To deal with
the problems above, a framework to improve the performance of NILM with
federated learning (FL) has been set up. In the framework, model weights
instead of the local data are shared among utilities. The global model is
generated by weighted averaging the locally-trained model weights to gather the
locally-trained model information. Optimal model selection help choose the
model which adapts to the data from different domains best. Experiments show
that this proposal improves the performance of local NILM runners. The
performance of this framework is close to that of the centrally-trained model
obtained by the convergent data without privacy protection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haijin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Si_C/0/1/0/all/0/1"&gt;Caomingzhe Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Junhua Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Entropy Guided Node Embedding Dimension Selection for Graph Neural Networks. (arXiv:2105.03178v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03178</id>
        <link href="http://arxiv.org/abs/2105.03178"/>
        <updated>2021-05-25T01:56:11.822Z</updated>
        <summary type="html"><![CDATA[Graph representation learning has achieved great success in many areas,
including e-commerce, chemistry, biology, etc. However, the fundamental problem
of choosing the appropriate dimension of node embedding for a given graph still
remains unsolved. The commonly used strategies for Node Embedding Dimension
Selection (NEDS) based on grid search or empirical knowledge suffer from heavy
computation and poor model performance. In this paper, we revisit NEDS from the
perspective of minimum entropy principle. Subsequently, we propose a novel
Minimum Graph Entropy (MinGE) algorithm for NEDS with graph data. To be
specific, MinGE considers both feature entropy and structure entropy on graphs,
which are carefully designed according to the characteristics of the rich
information in them. The feature entropy, which assumes the embeddings of
adjacent nodes to be more similar, connects node features and link topology on
graphs. The structure entropy takes the normalized degree as basic unit to
further measure the higher-order structure of graphs. Based on them, we design
MinGE to directly calculate the ideal node embedding dimension for any graph.
Finally, comprehensive experiments with popular Graph Neural Networks (GNNs) on
benchmark datasets demonstrate the effectiveness and generalizability of our
proposed MinGE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1"&gt;Gongxu Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BCNet: Searching for Network Width with Bilaterally Coupled Network. (arXiv:2105.10533v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10533</id>
        <link href="http://arxiv.org/abs/2105.10533"/>
        <updated>2021-05-25T01:56:11.773Z</updated>
        <summary type="html"><![CDATA[Searching for a more compact network width recently serves as an effective
way of channel pruning for the deployment of convolutional neural networks
(CNNs) under hardware constraints. To fulfill the searching, a one-shot
supernet is usually leveraged to efficiently evaluate the performance
\wrt~different network widths. However, current methods mainly follow a
\textit{unilaterally augmented} (UA) principle for the evaluation of each
width, which induces the training unfairness of channels in supernet. In this
paper, we introduce a new supernet called Bilaterally Coupled Network (BCNet)
to address this issue. In BCNet, each channel is fairly trained and responsible
for the same amount of network widths, thus each network width can be evaluated
more accurately. Besides, we leverage a stochastic complementary strategy for
training the BCNet, and propose a prior initial population sampling method to
boost the performance of the evolutionary search. Extensive experiments on
benchmark CIFAR-10 and ImageNet datasets indicate that our method can achieve
state-of-the-art or competing performance over other baseline methods.
Moreover, our method turns out to further boost the performance of NAS models
by refining their network widths. For example, with the same FLOPs budget, our
obtained EfficientNet-B0 achieves 77.36\% Top-1 accuracy on ImageNet dataset,
surpassing the performance of original setting by 0.48\%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Type-TD-TSR -- Extracting Tables from Document Images using a Multi-stage Pipeline for Table Detection and Table Structure Recognition: from OCR to Structured Table Representations. (arXiv:2105.11021v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11021</id>
        <link href="http://arxiv.org/abs/2105.11021"/>
        <updated>2021-05-25T01:56:11.695Z</updated>
        <summary type="html"><![CDATA[As global trends are shifting towards data-driven industries, the demand for
automated algorithms that can convert digital images of scanned documents into
machine readable information is rapidly growing. Besides the opportunity of
data digitization for the application of data analytic tools, there is also a
massive improvement towards automation of processes, which previously would
require manual inspection of the documents. Although the introduction of
optical character recognition technologies mostly solved the task of converting
human-readable characters from images into machine-readable characters, the
task of extracting table semantics has been less focused on over the years. The
recognition of tables consists of two main tasks, namely table detection and
table structure recognition. Most prior work on this problem focuses on either
task without offering an end-to-end solution or paying attention to real
application conditions like rotated images or noise artefacts inside the
document image. Recent work shows a clear trend towards deep learning
approaches coupled with the use of transfer learning for the task of table
structure recognition due to the lack of sufficiently large datasets. In this
paper we present a multistage pipeline named Multi-Type-TD-TSR, which offers an
end-to-end solution for the problem of table recognition. It utilizes
state-of-the-art deep learning models for table detection and differentiates
between 3 different types of tables based on the tables' borders. For the table
structure recognition we use a deterministic non-data driven algorithm, which
works on all table types. We additionally present two algorithms. One for
unbordered tables and one for bordered tables, which are the base of the used
table structure recognition algorithm. We evaluate Multi-Type-TD-TSR on the
ICDAR 2019 table structure recognition dataset and achieve a new
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_P/0/1/0/all/0/1"&gt;Pascal Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smajic_A/0/1/0/all/0/1"&gt;Alen Smajic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehler_A/0/1/0/all/0/1"&gt;Alexander Mehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abrami_G/0/1/0/all/0/1"&gt;Giuseppe Abrami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Effective and Efficient Method to Solve the High-Order and the Non-Linear Ordinary Differential Equations: the Ratio Net. (arXiv:2105.11309v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11309</id>
        <link href="http://arxiv.org/abs/2105.11309"/>
        <updated>2021-05-25T01:56:11.550Z</updated>
        <summary type="html"><![CDATA[An effective and efficient method that solves the high-order and the
non-linear ordinary differential equations is provided. The method is based on
the ratio net. By comparing the method with existing methods such as the
polynomial based method and the multilayer perceptron network based method, we
show that the ratio net gives good results and has higher efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1"&gt;Chen-Xin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ru-Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mao-Cai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chi-Chun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi-Liua/0/1/0/all/0/1"&gt;Yi-Liua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Book Cover Design via Layout Graphs. (arXiv:2105.11088v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11088</id>
        <link href="http://arxiv.org/abs/2105.11088"/>
        <updated>2021-05-25T01:56:11.532Z</updated>
        <summary type="html"><![CDATA[Book covers are intentionally designed and provide an introduction to a book.
However, they typically require professional skills to design and produce the
cover images. Thus, we propose a generative neural network that can produce
book covers based on an easy-to-use layout graph. The layout graph contains
objects such as text, natural scene objects, and solid color spaces. This
layout graph is embedded using a graph convolutional neural network and then
used with a mask proposal generator and a bounding-box generator and filled
using an object proposal generator. Next, the objects are compiled into a
single image and the entire network is trained using a combination of
adversarial training, perceptual training, and reconstruction. Finally, a Style
Retention Network (SRNet) is used to transfer the learned font style onto the
desired text. Using the proposed method allows for easily controlled and unique
book covers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wensheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyazono_T/0/1/0/all/0/1"&gt;Taiga Miyazono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1"&gt;Brian Kenji Iwana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Hiding Neural Networks Inside Neural Networks. (arXiv:2002.10078v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10078</id>
        <link href="http://arxiv.org/abs/2002.10078"/>
        <updated>2021-05-25T01:56:11.525Z</updated>
        <summary type="html"><![CDATA[Modern neural networks often contain significantly more parameters than the
size of their training data. We show that this excess capacity provides an
opportunity for embedding secret machine learning models within a trained
neural network. Our novel framework hides the existence of a secret neural
network with arbitrary desired functionality within a carrier network. We prove
theoretically that the secret network's detection is computationally infeasible
and demonstrate empirically that the carrier network does not compromise the
secret network's disguise. Our paper introduces a previously unknown
steganographic technique that can be exploited by adversaries if left
unchecked.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Ruihan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1"&gt;Kilian Q. Weinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning at the Network Edge: A Survey. (arXiv:1908.00080v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.00080</id>
        <link href="http://arxiv.org/abs/1908.00080"/>
        <updated>2021-05-25T01:56:11.519Z</updated>
        <summary type="html"><![CDATA[Resource-constrained IoT devices, such as sensors and actuators, have become
ubiquitous in recent years. This has led to the generation of large quantities
of data in real-time, which is an appealing target for AI systems. However,
deploying machine learning models on such end-devices is nearly impossible. A
typical solution involves offloading data to external computing systems (such
as cloud servers) for further processing but this worsens latency, leads to
increased communication costs, and adds to privacy concerns. To address this
issue, efforts have been made to place additional computing devices at the edge
of the network, i.e close to the IoT devices where the data is generated.
Deploying machine learning systems on such edge computing devices alleviates
the above issues by allowing computations to be performed close to the data
sources. This survey describes major research efforts where machine learning
systems have been deployed at the edge of computer networks, focusing on the
operational aspects including compression techniques, tools, frameworks, and
hardware used in successful applications of intelligent edge systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murshed_M/0/1/0/all/0/1"&gt;M.G. Sarwar Murshed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_C/0/1/0/all/0/1"&gt;Christopher Murphy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1"&gt;Daqing Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Nazar Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ananthanarayanan_G/0/1/0/all/0/1"&gt;Ganesh Ananthanarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1"&gt;Faraz Hussain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Rank Hankel Tensor Completion for Traffic Speed Estimation. (arXiv:2105.11335v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11335</id>
        <link href="http://arxiv.org/abs/2105.11335"/>
        <updated>2021-05-25T01:56:11.511Z</updated>
        <summary type="html"><![CDATA[This paper studies the traffic state estimation (TSE) problem using sparse
observations from mobile sensors. TSE can be considered a spatiotemporal
interpolation problem in which the evolution of traffic variables (e.g.,
speed/density) is governed by traffic flow dynamics (e.g., partial differential
equations). Most existing TSE methods either rely on well-defined physical
traffic flow models or require large amounts of simulation data as input to
train machine learning models. Different from previous studies, in this paper
we propose a purely data-driven and model-free solution. We consider TSE as a
spatiotemporal matrix completion/interpolation problem, and apply
spatiotemporal Hankel delay embedding to transforms the original incomplete
matrix to a fourth-order tensor. By imposing a low-rank assumption on this
tensor structure, we can approximate and characterize both global patterns and
the unknown and complex local spatiotemporal dynamics in a data-driven manner.
We use the truncated nuclear norm of the spatiotemporal unfolding (i.e., square
norm) to approximate the tensor rank and develop an efficient solution
algorithm based on the Alternating Direction Method of Multipliers (ADMM). The
proposed framework only involves two hyperparameters -- spatial and temporal
window lengths, which are easy to set given the degree of data sparsity. We
conduct numerical experiments on both synthetic simulation data and real-world
high-resolution trajectory data, and our results demonstrate the effectiveness
and superiority of the proposed model in some challenging scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xudong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuankai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1"&gt;Dingyi Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lijun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ESAD: End-to-end Deep Semi-supervised Anomaly Detection. (arXiv:2012.04905v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04905</id>
        <link href="http://arxiv.org/abs/2012.04905"/>
        <updated>2021-05-25T01:56:11.504Z</updated>
        <summary type="html"><![CDATA[This paper explores semi-supervised anomaly detection, a more practical
setting for anomaly detection where a small additional set of labeled samples
are provided. Based on the analysis of Deep SAD, the state-of-the-art for
semi-supervised anomaly detection, we propose a new KL-divergence based
objective function and show that two factors: the mutual information between
the data and latent representations, and the entropy of latent representations,
constitute an integral objective function for anomaly detection. To resolve the
contradiction in simultaneously optimizing the two factors, we propose a novel
encoder-decoder-encoder structure, with the first encoder focusing on
optimizing the mutual information and the second encoder focusing on optimizing
the entropy. The two encoders are enforced to share similar encoding with a
consistent constraint on their latent representations. Extensive experiments
have revealed that the proposed method significantly outperforms several
state-of-the-arts on multiple benchmark datasets, including medical diagnosis
and several classic anomaly detection benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chaoqin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan-Feng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Embedding Learning Framework for Numerical Features in CTR Prediction. (arXiv:2012.08986v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08986</id>
        <link href="http://arxiv.org/abs/2012.08986"/>
        <updated>2021-05-25T01:56:11.498Z</updated>
        <summary type="html"><![CDATA[Click-Through Rate (CTR) prediction is critical for industrial recommender
systems, where most deep CTR models follow an Embedding \& Feature Interaction
paradigm. However, the majority of methods focus on designing network
architectures to better capture feature interactions while the feature
embedding, especially for numerical features, has been overlooked. Existing
approaches for numerical features are difficult to capture informative
knowledge because of the low capacity or hard discretization based on the
offline expertise feature engineering. In this paper, we propose a novel
embedding learning framework for numerical features in CTR prediction (AutoDis)
with high model capacity, end-to-end training and unique representation
properties preserved. AutoDis consists of three core components:
meta-embeddings, automatic discretization and aggregation. Specifically, we
propose meta-embeddings for each numerical field to learn global knowledge from
the perspective of field with a manageable number of parameters. Then the
differentiable automatic discretization performs soft discretization and
captures the correlations between the numerical features and meta-embeddings.
Finally, distinctive and informative embeddings are learned via an aggregation
function. Comprehensive experiments on two public and one industrial datasets
are conducted to validate the effectiveness of AutoDis. Moreover, AutoDis has
been deployed onto a mainstream advertising platform, where online A/B test
demonstrates the improvement over the base model by 2.1% and 2.7% in terms of
CTR and eCPM, respectively. In addition, the code of our framework is publicly
available in
MindSpore(https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/recommend/autodis).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Huifeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weinan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning. (arXiv:1905.12127v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.12127</id>
        <link href="http://arxiv.org/abs/1905.12127"/>
        <updated>2021-05-25T01:56:11.488Z</updated>
        <summary type="html"><![CDATA[Solving tasks with sparse rewards is one of the most important challenges in
reinforcement learning. In the single-agent setting, this challenge is
addressed by introducing intrinsic rewards that motivate agents to explore
unseen regions of their state spaces; however, applying these techniques
naively to the multi-agent setting results in agents exploring independently,
without any coordination among themselves. Exploration in cooperative
multi-agent settings can be accelerated and improved if agents coordinate their
exploration. In this paper we introduce a framework for designing intrinsic
rewards which consider what other agents have explored such that the agents can
coordinate. Then, we develop an approach for learning how to dynamically select
between several exploration modalities to maximize extrinsic rewards.
Concretely, we formulate the approach as a hierarchical policy where a
high-level controller selects among sets of policies trained on diverse
intrinsic rewards and the low-level controllers learn the action policies of
all agents under these specific rewards. We demonstrate the effectiveness of
the proposed approach in cooperative domains with sparse rewards where
state-of-the-art methods fail and challenging multi-stage tasks that
necessitate changing modes of coordination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_S/0/1/0/all/0/1"&gt;Shariq Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1"&gt;Fei Sha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiXNet: Multiclass Multistage Multimodal Motion Prediction. (arXiv:2006.02000v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02000</id>
        <link href="http://arxiv.org/abs/2006.02000"/>
        <updated>2021-05-25T01:56:11.481Z</updated>
        <summary type="html"><![CDATA[One of the critical pieces of the self-driving puzzle is understanding the
surroundings of a self-driving vehicle (SDV) and predicting how these
surroundings will change in the near future. To address this task we propose
MultiXNet, an end-to-end approach for detection and motion prediction based
directly on lidar sensor data. This approach builds on prior work by handling
multiple classes of traffic actors, adding a jointly trained second-stage
trajectory refinement step, and producing a multimodal probability distribution
over future actor motion that includes both multiple discrete traffic behaviors
and calibrated continuous position uncertainties. The method was evaluated on
large-scale, real-world data collected by a fleet of SDVs in several cities,
with the results indicating that it outperforms existing state-of-the-art
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Djuric_N/0/1/0/all/0/1"&gt;Nemanja Djuric&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Henggang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1"&gt;Zhaoen Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shangxuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huahua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_F/0/1/0/all/0/1"&gt;Fang-Chieh Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1"&gt;Luisa San Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Song Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Rui Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dayan_A/0/1/0/all/0/1"&gt;Alyssa Dayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sidney Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becker_B/0/1/0/all/0/1"&gt;Brian C. Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_G/0/1/0/all/0/1"&gt;Gregory P. Meyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vallespi_Gonzalez_C/0/1/0/all/0/1"&gt;Carlos Vallespi-Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wellington_C/0/1/0/all/0/1"&gt;Carl K. Wellington&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantile Multi-Armed Bandits: Optimal Best-Arm Identification and a Differentially Private Scheme. (arXiv:2006.06792v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06792</id>
        <link href="http://arxiv.org/abs/2006.06792"/>
        <updated>2021-05-25T01:56:11.460Z</updated>
        <summary type="html"><![CDATA[We study the best-arm identification problem in multi-armed bandits with
stochastic, potentially private rewards, when the goal is to identify the arm
with the highest quantile at a fixed, prescribed level. First, we propose a
(non-private) successive elimination algorithm for strictly optimal best-arm
identification, we show that our algorithm is $\delta$-PAC and we characterize
its sample complexity. Further, we provide a lower bound on the expected number
of pulls, showing that the proposed algorithm is essentially optimal up to
logarithmic factors. Both upper and lower complexity bounds depend on a special
definition of the associated suboptimality gap, designed in particular for the
quantile bandit problem, as we show when the gap approaches zero, best-arm
identification is impossible. Second, motivated by applications where the
rewards are private, we provide a differentially private successive elimination
algorithm whose sample complexity is finite even for distributions with
infinite support-size, and we characterize its sample complexity. Our
algorithms do not require prior knowledge of either the suboptimality gap or
other statistical information related to the bandit problem at hand.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nikolakakis_K/0/1/0/all/0/1"&gt;Kontantinos E. Nikolakakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kalogerias_D/0/1/0/all/0/1"&gt;Dionysios S. Kalogerias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sheffet_O/0/1/0/all/0/1"&gt;Or Sheffet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sarwate_A/0/1/0/all/0/1"&gt;Anand D. Sarwate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Recognition of Pure & Mixed Stones using Intraoperative Endoscopic Digital Images. (arXiv:2105.10686v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10686</id>
        <link href="http://arxiv.org/abs/2105.10686"/>
        <updated>2021-05-25T01:56:11.442Z</updated>
        <summary type="html"><![CDATA[Objective: To assess automatic computer-aided in-situ recognition of
morphological features of pure and mixed urinary stones using intraoperative
digital endoscopic images acquired in a clinical setting. Materials and
methods: In this single-centre study, an experienced urologist intraoperatively
and prospectively examined the surface and section of all kidney stones
encountered. Calcium oxalate monohydrate (COM/Ia), dihydrate (COD/IIb) and uric
acid (UA/IIIb) morphological criteria were collected and classified to generate
annotated datasets. A deep convolutional neural network (CNN) was trained to
predict the composition of both pure and mixed stones. To explain the
predictions of the deep neural network model, coarse localisation heat-maps
were plotted to pinpoint key areas identified by the network. Results: This
study included 347 and 236 observations of stone surface and stone section,
respectively. A highest sensitivity of 98 % was obtained for the type "pure
IIIb/UA" using surface images. The most frequently encountered morphology was
that of the type "pure Ia/COM"; it was correctly predicted in 91 % and 94 % of
cases using surface and section images, respectively. Of the mixed type
"Ia/COM+IIb/COD", Ia/COM was predicted in 84 % of cases using surface images,
IIb/COD in 70 % of cases, and both in 65 % of cases. Concerning mixed
Ia/COM+IIIb/UA stones, Ia/COM was predicted in 91 % of cases using section
images, IIIb/UA in 69 % of cases, and both in 74 % of cases. Conclusions: This
preliminary study demonstrates that deep convolutional neural networks are
promising to identify kidney stone composition from endoscopic images acquired
intraoperatively. Both pure and mixed stone composition could be discriminated.
Collected in a clinical setting, surface and section images analysed by deep
CNN provide valuable information about stone morphology for computer-aided
diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Estrade_V/0/1/0/all/0/1"&gt;Vincent Estrade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daudon_M/0/1/0/all/0/1"&gt;Michel Daudon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_E/0/1/0/all/0/1"&gt;Emmanuel Richard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernhard_J/0/1/0/all/0/1"&gt;Jean-Christophe Bernhard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bladou_F/0/1/0/all/0/1"&gt;Franck Bladou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robert_G/0/1/0/all/0/1"&gt;Gregoire Robert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senneville_B/0/1/0/all/0/1"&gt;Baudouin Denis de Senneville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Redundancy-free Features for Generalized Zero-Shot Object Recognition. (arXiv:2006.08939v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08939</id>
        <link href="http://arxiv.org/abs/2006.08939"/>
        <updated>2021-05-25T01:56:11.347Z</updated>
        <summary type="html"><![CDATA[Zero-shot object recognition or zero-shot learning aims to transfer the
object recognition ability among the semantically related categories, such as
fine-grained animal or bird species. However, the images of different
fine-grained objects tend to merely exhibit subtle differences in appearance,
which will severely deteriorate zero-shot object recognition. To reduce the
superfluous information in the fine-grained objects, in this paper, we propose
to learn the redundancy-free features for generalized zero-shot learning. We
achieve our motivation by projecting the original visual features into a new
(redundancy-free) feature space and then restricting the statistical dependence
between these two feature spaces. Furthermore, we require the projected
features to keep and even strengthen the category relationship in the
redundancy-free feature space. In this way, we can remove the redundant
information from the visual features without losing the discriminative
information. We extensively evaluate the performance on four benchmark
datasets. The results show that our redundancy-free feature based generalized
zero-shot learning (RFF-GZSL) approach can achieve competitive results compared
with the state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zongyan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhenyong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Noise Injection in Generative Adversarial Networks. (arXiv:2006.05891v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05891</id>
        <link href="http://arxiv.org/abs/2006.05891"/>
        <updated>2021-05-25T01:56:11.334Z</updated>
        <summary type="html"><![CDATA[Noise injection has been proved to be one of the key technique advances in
generating high-fidelity images. Despite its successful usage in GANs, the
mechanism of its validity is still unclear. In this paper, we propose a
geometric framework to theoretically analyze the role of noise injection in
GANs. Based on Riemannian geometry, we successfully model the noise injection
framework as fuzzy equivalence on the geodesic normal coordinates. Guided by
our theories, we find that the existing method is incomplete and a new strategy
for noise injection is devised. Experiments on image generation and GAN
inversion demonstrate the superiority of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ruili Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Deli Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhengjun Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weight-Covariance Alignment for Adversarially Robust Neural Networks. (arXiv:2010.08852v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08852</id>
        <link href="http://arxiv.org/abs/2010.08852"/>
        <updated>2021-05-25T01:56:11.327Z</updated>
        <summary type="html"><![CDATA[Stochastic Neural Networks (SNNs) that inject noise into their hidden layers
have recently been shown to achieve strong robustness against adversarial
attacks. However, existing SNNs are usually heuristically motivated, and often
rely on adversarial training, which is computationally costly. We propose a new
SNN that achieves state-of-the-art performance without relying on adversarial
training, and enjoys solid theoretical justification. Specifically, while
existing SNNs inject learned or hand-tuned isotropic noise, our SNN learns an
anisotropic noise distribution to optimize a learning-theoretic bound on
adversarial robustness. We evaluate our method on a number of popular
benchmarks, show that it can be applied to different architectures, and that it
provides robustness to a variety of white-box and black-box attacks, while
being simple and fast to train compared to existing alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eustratiadis_P/0/1/0/all/0/1"&gt;Panagiotis Eustratiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gouk_H/0/1/0/all/0/1"&gt;Henry Gouk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Da Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voting with Random Classifiers (VORACE): Theoretical and Experimental Analysis. (arXiv:1909.08996v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.08996</id>
        <link href="http://arxiv.org/abs/1909.08996"/>
        <updated>2021-05-25T01:56:11.319Z</updated>
        <summary type="html"><![CDATA[In many machine learning scenarios, looking for the best classifier that fits
a particular dataset can be very costly in terms of time and resources.
Moreover, it can require deep knowledge of the specific domain. We propose a
new technique which does not require profound expertise in the domain and
avoids the commonly used strategy of hyper-parameter tuning and model
selection. Our method is an innovative ensemble technique that uses voting
rules over a set of randomly-generated classifiers. Given a new input sample,
we interpret the output of each classifier as a ranking over the set of
possible classes. We then aggregate these output rankings using a voting rule,
which treats them as preferences over the classes. We show that our approach
obtains good results compared to the state-of-the-art, both providing a
theoretical analysis and an empirical evaluation of the approach on several
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cornelio_C/0/1/0/all/0/1"&gt;Cristina Cornelio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donini_M/0/1/0/all/0/1"&gt;Michele Donini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loreggia_A/0/1/0/all/0/1"&gt;Andrea Loreggia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pini_M/0/1/0/all/0/1"&gt;Maria Silvia Pini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1"&gt;Francesca Rossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training. (arXiv:2105.11333v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11333</id>
        <link href="http://arxiv.org/abs/2105.11333"/>
        <updated>2021-05-25T01:56:11.311Z</updated>
        <summary type="html"><![CDATA[Recently a number of studies demonstrated impressive performance on diverse
vision-language multi-modal tasks such as image captioning and visual question
answering by extending the BERT architecture with multi-modal pre-training
objectives. In this work we explore a broad set of multi-modal representation
learning tasks in the medical domain, specifically using radiology images and
the unstructured report. We propose Medical Vision Language Learner (MedViLL)
which adopts a Transformer-based architecture combined with a novel multimodal
attention masking scheme to maximize generalization performance for both
vision-language understanding tasks (image-report retrieval, disease
classification, medical visual question answering) and vision-language
generation task (report generation). By rigorously evaluating the proposed
model on four downstream tasks with two chest X-ray image datasets (MIMIC-CXR
and Open-I), we empirically demonstrate the superior downstream task
performance of MedViLL against various baselines including task-specific
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1"&gt;Jong Hak Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hyungyung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1"&gt;Woncheol Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1"&gt;Edward Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for 3D Point Cloud Understanding: A Survey. (arXiv:2009.08920v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08920</id>
        <link href="http://arxiv.org/abs/2009.08920"/>
        <updated>2021-05-25T01:56:11.291Z</updated>
        <summary type="html"><![CDATA[The development of practical applications, such as autonomous driving and
robotics, has brought increasing attention to 3D point cloud understanding.
While deep learning has achieved remarkable success on image-based tasks, there
are many unique challenges faced by deep neural networks in processing massive,
unstructured and noisy 3D points. To demonstrate the latest progress of deep
learning for 3D point cloud understanding, this paper summarizes recent
remarkable research contributions in this area from several different
directions (classification, segmentation, detection, tracking, flow estimation,
registration, augmentation and completion), together with commonly used
datasets, metrics and state-of-the-art performances. More information regarding
this survey can be found at:
https://github.com/SHI-Labs/3D-Point-Cloud-Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haoming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Humphrey Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiasing Concept-based Explanations with Causal Analysis. (arXiv:2007.11500v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11500</id>
        <link href="http://arxiv.org/abs/2007.11500"/>
        <updated>2021-05-25T01:56:11.284Z</updated>
        <summary type="html"><![CDATA[Concept-based explanation approach is a popular model interpertability tool
because it expresses the reasons for a model's predictions in terms of concepts
that are meaningful for the domain experts. In this work, we study the problem
of the concepts being correlated with confounding information in the features.
We propose a new causal prior graph for modeling the impacts of unobserved
variables and a method to remove the impact of confounding information and
noise using a two-stage regression technique borrowed from the instrumental
variable literature. We also model the completeness of the concepts set and
show that our debiasing method works when the concepts are not complete. Our
synthetic and real-world experiments demonstrate the success of our method in
removing biases and improving the ranking of the concepts in terms of their
contribution to the explanation of the predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bahadori_M/0/1/0/all/0/1"&gt;Mohammad Taha Bahadori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1"&gt;David E. Heckerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor-variate Mixture of Experts for Proportional Myographic Control of a Robotic Hand. (arXiv:1902.11104v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.11104</id>
        <link href="http://arxiv.org/abs/1902.11104"/>
        <updated>2021-05-25T01:56:11.277Z</updated>
        <summary type="html"><![CDATA[When data are organized in matrices or arrays of higher dimensions (tensors),
classical regression methods first transform these data into vectors, therefore
ignoring the underlying structure of the data and increasing the dimensionality
of the problem. This flattening operation typically leads to overfitting when
only few training data is available. In this paper, we present a
mixture-of-experts model that exploits tensorial representations for regression
of tensor-valued data. The proposed formulation takes into account the
underlying structure of the data and remains efficient when few training data
are available. Evaluation on artificially generated data, as well as offline
and real-time experiments recognizing hand movements from tactile myography
prove the effectiveness of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaquier_N/0/1/0/all/0/1"&gt;No&amp;#xe9;mie Jaquier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haschke_R/0/1/0/all/0/1"&gt;Robert Haschke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calinon_S/0/1/0/all/0/1"&gt;Sylvain Calinon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Upsampling for Protest Size Detection. (arXiv:2105.11260v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11260</id>
        <link href="http://arxiv.org/abs/2105.11260"/>
        <updated>2021-05-25T01:56:11.270Z</updated>
        <summary type="html"><![CDATA[We propose a new task and dataset for a common problem in social science
research: "upsampling" coarse document labels to fine-grained labels or spans.
We pose the problem in a question answering format, with the answers providing
the fine-grained labels. We provide a benchmark dataset and baselines on a
socially impactful task: identifying the exact crowd size at protests and
demonstrations in the United States given only order-of-magnitude information
about protest attendance, a very small sample of fine-grained examples, and
English-language news text. We evaluate several baseline models, including
zero-shot results from rule-based and question-answering models, few-shot
models fine-tuned on a small set of documents, and weakly supervised models
using a larger set of coarsely-labeled documents. We find that our rule-based
model initially outperforms a zero-shot pre-trained transformer language model
but that further fine-tuning on a very small subset of 25 examples
substantially improves out-of-sample performance. We also demonstrate a method
for fine-tuning the transformer span on only the coarse labels that performs
similarly to our rule-based approach. This work will contribute to social
scientists' ability to generate data to understand the causes and successes of
collective action.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Halterman_A/0/1/0/all/0/1"&gt;Andrew Halterman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radford_B/0/1/0/all/0/1"&gt;Benjamin J. Radford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiXNet: Multiclass Multistage Multimodal Motion Prediction. (arXiv:2006.02000v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02000</id>
        <link href="http://arxiv.org/abs/2006.02000"/>
        <updated>2021-05-25T01:56:11.263Z</updated>
        <summary type="html"><![CDATA[One of the critical pieces of the self-driving puzzle is understanding the
surroundings of a self-driving vehicle (SDV) and predicting how these
surroundings will change in the near future. To address this task we propose
MultiXNet, an end-to-end approach for detection and motion prediction based
directly on lidar sensor data. This approach builds on prior work by handling
multiple classes of traffic actors, adding a jointly trained second-stage
trajectory refinement step, and producing a multimodal probability distribution
over future actor motion that includes both multiple discrete traffic behaviors
and calibrated continuous position uncertainties. The method was evaluated on
large-scale, real-world data collected by a fleet of SDVs in several cities,
with the results indicating that it outperforms existing state-of-the-art
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Djuric_N/0/1/0/all/0/1"&gt;Nemanja Djuric&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Henggang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1"&gt;Zhaoen Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shangxuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huahua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_F/0/1/0/all/0/1"&gt;Fang-Chieh Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1"&gt;Luisa San Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Song Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Rui Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dayan_A/0/1/0/all/0/1"&gt;Alyssa Dayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sidney Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becker_B/0/1/0/all/0/1"&gt;Brian C. Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_G/0/1/0/all/0/1"&gt;Gregory P. Meyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vallespi_Gonzalez_C/0/1/0/all/0/1"&gt;Carlos Vallespi-Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wellington_C/0/1/0/all/0/1"&gt;Carl K. Wellington&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Language Models for Nineteenth-Century English. (arXiv:2105.11321v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11321</id>
        <link href="http://arxiv.org/abs/2105.11321"/>
        <updated>2021-05-25T01:56:11.255Z</updated>
        <summary type="html"><![CDATA[We present four types of neural language models trained on a large historical
dataset of books in English, published between 1760-1900 and comprised of ~5.1
billion tokens. The language model architectures include static (word2vec and
fastText) and contextualized models (BERT and Flair). For each architecture, we
trained a model instance using the whole dataset. Additionally, we trained
separate instances on text published before 1850 for the two static models, and
four instances considering different time slices for BERT. Our models have
already been used in various downstream tasks where they consistently improved
performance. In this paper, we describe how the models have been created and
outline their reuse potential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_K/0/1/0/all/0/1"&gt;Kasra Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beelen_K/0/1/0/all/0/1"&gt;Kaspar Beelen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colavizza_G/0/1/0/all/0/1"&gt;Giovanni Colavizza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ardanuy_M/0/1/0/all/0/1"&gt;Mariona Coll Ardanuy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASIST: Annotation-free Synthetic Instance Segmentation and Tracking by Adversarial Simulations. (arXiv:2101.00567v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00567</id>
        <link href="http://arxiv.org/abs/2101.00567"/>
        <updated>2021-05-25T01:56:11.234Z</updated>
        <summary type="html"><![CDATA[Background: The quantitative analysis of microscope videos often requires
instance segmentation and tracking of cellular and subcellular objects. The
traditional method consists of two stages: (1) performing instance object
segmentation of each frame, and (2) associating objects frame-by-frame.
Recently, pixel-embedding-based deep learning approaches these two steps
simultaneously as a single stage holistic solution. In computer vision,
annotated training data with consistent segmentation and tracking is resource
intensive, the severity of which is multiplied in microscopy imaging due to (1)
dense objects (e.g., overlapping or touching), and (2) high dynamics (e.g.,
irregular motion and mitosis). Adversarial simulations have provided successful
solutions to alleviate the lack of such annotations in dynamics scenes in
computer vision, such as using simulated environments (e.g., computer games) to
train real-world self-driving systems. Methods: In this paper, we propose an
annotation-free synthetic instance segmentation and tracking (ASIST) method
with adversarial simulation and single-stage pixel-embedding based learning.
Contribution: The contribution of this paper is three-fold: (1) the proposed
method aggregates adversarial simulations and single-stage pixel-embedding
based deep learning; (2) the method is assessed with both the cellular (i.e.,
HeLa cells) and subcellular (i.e., microvilli) objects; and (3) to the best of
our knowledge, this is the first study to explore annotation-free instance
segmentation and tracking study for microscope videos. Results: The ASIST
method achieved an important step forward, when compared with fully supervised
approaches: ASIST shows 7% to 11% higher segmentation, detection and tracking
performance on microvilli relative to fully supervised methods, and comparable
performance on Hela cell videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gaeta_I/0/1/0/all/0/1"&gt;Isabella M. Gaeta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mengyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1"&gt;Ruining Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jha_A/0/1/0/all/0/1"&gt;Aadarsh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Millis_B/0/1/0/all/0/1"&gt;Bryan A. Millis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mahadevan_Jansen_A/0/1/0/all/0/1"&gt;Anita Mahadevan-Jansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tyska_M/0/1/0/all/0/1"&gt;Matthew J. Tyska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bin2vec: Learning Representations of Binary Executable Programs for Security Tasks. (arXiv:2002.03388v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03388</id>
        <link href="http://arxiv.org/abs/2002.03388"/>
        <updated>2021-05-25T01:56:11.227Z</updated>
        <summary type="html"><![CDATA[Tackling binary program analysis problems has traditionally implied manually
defining rules and heuristics, a tedious and time-consuming task for human
analysts. In order to improve automation and scalability, we propose an
alternative direction based on distributed representations of binary programs
with applicability to a number of downstream tasks. We introduce Bin2vec, a new
approach leveraging Graph Convolutional Networks (GCN) along with computational
program graphs in order to learn a high dimensional representation of binary
executable programs. We demonstrate the versatility of this approach by using
our representations to solve two semantically different binary analysis tasks -
functional algorithm classification and vulnerability discovery. We compare the
proposed approach to our own strong baseline as well as published results and
demonstrate improvement over state-of-the-art methods for both tasks. We
evaluated Bin2vec on 49191 binaries for the functional algorithm classification
task, and on 30 different CWE-IDs including at least 100 CVE entries each for
the vulnerability discovery task. We set a new state-of-the-art result by
reducing the classification error by 40% compared to the source-code-based
inst2vec approach, while working on binary code. For almost every vulnerability
class in our dataset, our prediction accuracy is over 80% (and over 90% in
multiple classes).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arakelyan_S/0/1/0/all/0/1"&gt;Shushan Arakelyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arasteh_S/0/1/0/all/0/1"&gt;Sima Arasteh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauser_C/0/1/0/all/0/1"&gt;Christophe Hauser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kline_E/0/1/0/all/0/1"&gt;Erik Kline&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1"&gt;Aram Galstyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Non-Local Priors via Self-Convolution For Highly-Efficient Image Restoration. (arXiv:2006.13714v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13714</id>
        <link href="http://arxiv.org/abs/2006.13714"/>
        <updated>2021-05-25T01:56:11.219Z</updated>
        <summary type="html"><![CDATA[Constructing effective image priors is critical to solving ill-posed inverse
problems in image processing and imaging. Recent works proposed to exploit
image non-local similarity for inverse problems by grouping similar patches and
demonstrated state-of-the-art results in many applications. However, compared
to classic methods based on filtering or sparsity, most of the non-local
algorithms are time-consuming, mainly due to the highly inefficient and
redundant block matching step, where the distance between each pair of
overlapping patches needs to be computed. In this work, we propose a novel
Self-Convolution operator to exploit image non-local similarity in a
self-supervised way. The proposed Self-Convolution can generalize the
commonly-used block matching step and produce equivalent results with much
cheaper computation. Furthermore, by applying Self-Convolution, we propose an
effective multi-modality image restoration scheme, which is much more efficient
than conventional block matching for non-local modeling. Experimental results
demonstrate that (1) Self-Convolution can significantly speed up most of the
popular non-local image restoration algorithms, with two-fold to nine-fold
faster block matching, and (2) the proposed multi-modality image restoration
scheme achieves superior denoising results in both efficiency and effectiveness
on RGB-NIR images. The code is publicly available at
\href{https://github.com/GuoLanqing/Self-Convolution}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Lanqing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhiyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravishankar_S/0/1/0/all/0/1"&gt;Saiprasad Ravishankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1"&gt;Bihan Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Green's Functions of Linear Reaction-Diffusion Equations with Application to Fast Numerical Solver. (arXiv:2105.11045v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11045</id>
        <link href="http://arxiv.org/abs/2105.11045"/>
        <updated>2021-05-25T01:56:11.211Z</updated>
        <summary type="html"><![CDATA[Partial differential equations are often used to model various physical
phenomena, such as heat diffusion, wave propagation, fluid dynamics,
elasticity, electrodynamics and image processing, and many analytic approaches
or traditional numerical methods have been developed and widely used for their
solutions. Inspired by rapidly growing impact of deep learning on scientific
and engineering research, in this paper we propose a novel neural network,
GF-Net, for learning the Green's functions of linear reaction-diffusion
equations in an unsupervised fashion. The proposed method overcomes the
challenges for finding the Green's functions of the equations on arbitrary
domains by utilizing physics-informed approach and the symmetry of the Green's
function. As a consequence, it particularly leads to an efficient way for
solving the target equations under different boundary conditions and sources.
We also demonstrate the effectiveness of the proposed approach by experiments
in square, annular and L-shape domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1"&gt;Yuankai Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1"&gt;Lili Ju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning at the Network Edge: A Survey. (arXiv:1908.00080v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.00080</id>
        <link href="http://arxiv.org/abs/1908.00080"/>
        <updated>2021-05-25T01:56:11.190Z</updated>
        <summary type="html"><![CDATA[Resource-constrained IoT devices, such as sensors and actuators, have become
ubiquitous in recent years. This has led to the generation of large quantities
of data in real-time, which is an appealing target for AI systems. However,
deploying machine learning models on such end-devices is nearly impossible. A
typical solution involves offloading data to external computing systems (such
as cloud servers) for further processing but this worsens latency, leads to
increased communication costs, and adds to privacy concerns. To address this
issue, efforts have been made to place additional computing devices at the edge
of the network, i.e close to the IoT devices where the data is generated.
Deploying machine learning systems on such edge computing devices alleviates
the above issues by allowing computations to be performed close to the data
sources. This survey describes major research efforts where machine learning
systems have been deployed at the edge of computer networks, focusing on the
operational aspects including compression techniques, tools, frameworks, and
hardware used in successful applications of intelligent edge systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murshed_M/0/1/0/all/0/1"&gt;M.G. Sarwar Murshed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_C/0/1/0/all/0/1"&gt;Christopher Murphy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1"&gt;Daqing Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Nazar Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ananthanarayanan_G/0/1/0/all/0/1"&gt;Ganesh Ananthanarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1"&gt;Faraz Hussain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic and Geometric Modeling with Neural Message Passing in 3D Scene Graphs for Hierarchical Mechanical Search. (arXiv:2012.04060v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04060</id>
        <link href="http://arxiv.org/abs/2012.04060"/>
        <updated>2021-05-25T01:56:11.189Z</updated>
        <summary type="html"><![CDATA[Searching for objects in indoor organized environments such as homes or
offices is part of our everyday activities. When looking for a target object,
we jointly reason about the rooms and containers the object is likely to be in;
the same type of container will have a different probability of having the
target depending on the room it is in. We also combine geometric and semantic
information to infer what container is best to search, or what other objects
are best to move, if the target object is hidden from view. We propose to use a
3D scene graph representation to capture the hierarchical, semantic, and
geometric aspects of this problem. To exploit this representation in a search
process, we introduce Hierarchical Mechanical Search (HMS), a method that
guides an agent's actions towards finding a target object specified with a
natural language description. HMS is based on a novel neural network
architecture that uses neural message passing of vectors with visual,
geometric, and linguistic information to allow HMS to reason across layers of
the graph while combining semantic and geometric cues. HMS is evaluated on a
novel dataset of 500 3D scene graphs with dense placements of semantically
related objects in storage locations, and is shown to be significantly better
than several baselines at finding objects and close to the oracle policy in
terms of the median number of actions required. Additional qualitative results
can be found at https://ai.stanford.edu/mech-search/hms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1"&gt;Andrey Kurenkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1"&gt;Jeff Ichnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Sampling for Accelerated MRI with Low-Rank Tensors. (arXiv:2012.12496v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12496</id>
        <link href="http://arxiv.org/abs/2012.12496"/>
        <updated>2021-05-25T01:56:11.189Z</updated>
        <summary type="html"><![CDATA[Magnetic resonance imaging (MRI) is a powerful imaging modality that
revolutionizes medicine and biology. The imaging speed of high-dimensional MRI
is often limited, which constrains its practical utility. Recently, low-rank
tensor models have been exploited to enable fast MR imaging with sparse
sampling. Most existing methods use some pre-defined sampling design, and
active sensing has not been explored for low-rank tensor imaging. In this
paper, we introduce an active low-rank tensor model for fast MR imaging. We
propose an active sampling method based on a Query-by-Committee model, making
use of the benefits of low-rank tensor structure. Numerical experiments on a
3-D MRI data set demonstrate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zichang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Bo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks. (arXiv:2012.12352v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12352</id>
        <link href="http://arxiv.org/abs/2012.12352"/>
        <updated>2021-05-25T01:56:11.188Z</updated>
        <summary type="html"><![CDATA[We investigate the reasoning ability of pretrained vision and language (V&L)
models in two tasks that require multimodal integration: (1) discriminating a
correct image-sentence pair from an incorrect one, and (2) counting entities in
an image. We evaluate three pretrained V&L models on these tasks: ViLBERT,
ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results
show that models solve task (1) very well, as expected, since all models are
pretrained on task (1). However, none of the pretrained V&L models is able to
adequately solve task (2), our counting probe, and they cannot generalise to
out-of-distribution quantities. We propose a number of explanations for these
findings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of
catastrophic forgetting on task (1). Concerning our results on the counting
probe, we find evidence that all models are impacted by dataset bias, and also
fail to individuate entities in the visual input. While a selling point of
pretrained V&L models is their ability to solve complex tasks, our findings
suggest that understanding their reasoning and grounding capabilities requires
more targeted investigations on specific phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1"&gt;Letitia Parcalabescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1"&gt;Albert Gatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1"&gt;Iacer Calixto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emerging Properties in Self-Supervised Vision Transformers. (arXiv:2104.14294v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14294</id>
        <link href="http://arxiv.org/abs/2104.14294"/>
        <updated>2021-05-25T01:56:11.188Z</updated>
        <summary type="html"><![CDATA[In this paper, we question if self-supervised learning provides new
properties to Vision Transformer (ViT) that stand out compared to convolutional
networks (convnets). Beyond the fact that adapting self-supervised methods to
this architecture works particularly well, we make the following observations:
first, self-supervised ViT features contain explicit information about the
semantic segmentation of an image, which does not emerge as clearly with
supervised ViTs, nor with convnets. Second, these features are also excellent
k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study
also underlines the importance of momentum encoder, multi-crop training, and
the use of small patches with ViTs. We implement our findings into a simple
self-supervised method, called DINO, which we interpret as a form of
self-distillation with no labels. We show the synergy between DINO and ViTs by
achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1"&gt;Mathilde Caron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1"&gt;Hugo Touvron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1"&gt;Ishan Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1"&gt;Herv&amp;#xe9; J&amp;#xe9;gou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1"&gt;Julien Mairal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1"&gt;Piotr Bojanowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1"&gt;Armand Joulin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust learning with anytime-guaranteed feedback. (arXiv:2105.11135v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.11135</id>
        <link href="http://arxiv.org/abs/2105.11135"/>
        <updated>2021-05-25T01:56:11.166Z</updated>
        <summary type="html"><![CDATA[Under data distributions which may be heavy-tailed, many stochastic
gradient-based learning algorithms are driven by feedback queried at points
with almost no performance guarantees on their own. Here we explore a modified
"anytime online-to-batch" mechanism which for smooth objectives admits
high-probability error bounds while requiring only lower-order moment bounds on
the stochastic gradients. Using this conversion, we can derive a wide variety
of "anytime robust" procedures, for which the task of performance analysis can
be effectively reduced to regret control, meaning that existing regret bounds
(for the bounded gradient case) can be robustified and leveraged in a
straightforward manner. As a direct takeaway, we obtain an easily implemented
stochastic gradient-based algorithm for which all queried points formally enjoy
sub-Gaussian error bounds, and in practice show noteworthy gains on real-world
data applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Holland_M/0/1/0/all/0/1"&gt;Matthew J. Holland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations. (arXiv:2009.02731v8 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.02731</id>
        <link href="http://arxiv.org/abs/2009.02731"/>
        <updated>2021-05-25T01:56:11.166Z</updated>
        <summary type="html"><![CDATA[We propose Corder, a self-supervised contrastive learning framework for
source code model. Corder is designed to alleviate the need of labeled data for
code retrieval and code summarization tasks. The pre-trained model of Corder
can be used in two ways: (1) it can produce vector representation of code which
can be applied to code retrieval tasks that do not have labeled data; (2) it
can be used in a fine-tuning process for tasks that might still require label
data such as code summarization. The key innovation is that we train the source
code model by asking it to recognize similar and dissimilar code snippets
through a contrastive learning objective. To do so, we use a set of
semantic-preserving transformation operators to generate code snippets that are
syntactically diverse but semantically equivalent. Through extensive
experiments, we have shown that the code models pretrained by Corder
substantially outperform the other baselines for code-to-code retrieval,
text-to-code retrieval, and code-to-text summarization tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1"&gt;Nghi D. Q. Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yijun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lingxiao Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can we imitate stock price behavior to reinforcement learn option price?. (arXiv:2105.11376v1 [q-fin.PR])]]></title>
        <id>http://arxiv.org/abs/2105.11376</id>
        <link href="http://arxiv.org/abs/2105.11376"/>
        <updated>2021-05-25T01:56:11.158Z</updated>
        <summary type="html"><![CDATA[This paper presents a framework of imitating the price behavior of the
underlying stock for reinforcement learning option price. We use accessible
features of the equities pricing data to construct a non-deterministic Markov
decision process for modeling stock price behavior driven by principal
investor's decision making. However, low signal-to-noise ratio and instability
that appear immanent in equity markets pose challenges to determine the state
transition (price change) after executing an action (principal investor's
decision) as well as decide an action based on current state (spot price). In
order to conquer these challenges, we resort to a Bayesian deep neural network
for computing the predictive distribution of the state transition led by an
action. Additionally, instead of exploring a state-action relationship to
formulate a policy, we seek for an episode based visible-hidden state-action
relationship to probabilistically imitate principal investor's successive
decision making. Our algorithm then maps imitative principal investor's
decisions to simulated stock price paths by a Bayesian deep neural network.
Eventually the optimal option price is reinforcement learned through maximizing
the cumulative risk-adjusted return of a dynamically hedged portfolio over
simulated price paths of the underlying.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face Anonymization by Manipulating Decoupled Identity Representation. (arXiv:2105.11137v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11137</id>
        <link href="http://arxiv.org/abs/2105.11137"/>
        <updated>2021-05-25T01:56:11.150Z</updated>
        <summary type="html"><![CDATA[Privacy protection on human biological information has drawn increasing
attention in recent years, among which face anonymization plays an importance
role. We propose a novel approach which protects identity information of facial
images from leakage with slightest modification. Specifically, we disentangle
identity representation from other facial attributes leveraging the power of
generative adversarial networks trained on a conditional multi-scale
reconstruction (CMR) loss and an identity loss. We evaulate the disentangle
ability of our model, and propose an effective method for identity
anonymization, namely Anonymous Identity Generation (AIG), to reach the goal of
face anonymization meanwhile maintaining similarity to the original image as
much as possible. Quantitative and qualitative results demonstrate our method's
superiority compared with the SOTAs on both visual quality and anonymization
success rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tianxiang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongze Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards High Performance Human Keypoint Detection. (arXiv:2002.00537v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00537</id>
        <link href="http://arxiv.org/abs/2002.00537"/>
        <updated>2021-05-25T01:56:11.150Z</updated>
        <summary type="html"><![CDATA[Human keypoint detection from a single image is very challenging due to
occlusion, blur, illumination and scale variance. In this paper, we address
this problem from three aspects by devising an efficient network structure,
proposing three effective training strategies, and exploiting four useful
postprocessing techniques. First, we find that context information plays an
important role in reasoning human body configuration and invisible keypoints.
Inspired by this, we propose a cascaded context mixer (CCM), which efficiently
integrates spatial and channel context information and progressively refines
them. Then, to maximize CCM's representation capability, we develop a
hard-negative person detection mining strategy and a joint-training strategy by
exploiting abundant unlabeled data. It enables CCM to learn discriminative
features from massive diverse poses. Third, we present several sub-pixel
refinement techniques for postprocessing keypoint predictions to improve
detection accuracy. Extensive experiments on the MS COCO keypoint detection
benchmark demonstrate the superiority of the proposed method over
representative state-of-the-art (SOTA) methods. Our single model achieves
comparable performance with the winner of the 2018 COCO Keypoint Detection
Challenge. The final ensemble model sets a new SOTA on this benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WSSOD: A New Pipeline for Weakly- and Semi-Supervised Object Detection. (arXiv:2105.11293v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11293</id>
        <link href="http://arxiv.org/abs/2105.11293"/>
        <updated>2021-05-25T01:56:11.149Z</updated>
        <summary type="html"><![CDATA[The performance of object detection, to a great extent, depends on the
availability of large annotated datasets. To alleviate the annotation cost, the
research community has explored a number of ways to exploit unlabeled or weakly
labeled data. However, such efforts have met with limited success so far. In
this work, we revisit the problem with a pragmatic standpoint, trying to
explore a new balance between detection performance and annotation cost by
jointly exploiting fully and weakly annotated data. Specifically, we propose a
weakly- and semi-supervised object detection framework (WSSOD), which involves
a two-stage learning procedure. An agent detector is first trained on a joint
dataset and then used to predict pseudo bounding boxes on weakly-annotated
images. The underlying assumptions in the current as well as common
semi-supervised pipelines are also carefully examined under a unified EM
formulation. On top of this framework, weakly-supervised loss (WSL), label
attention and random pseudo-label sampling (RPS) strategies are introduced to
relax these assumptions, bringing additional improvement on the efficacy of the
detection pipeline. The proposed framework demonstrates remarkable performance
on PASCAL-VOC and MSCOCO benchmark, achieving a high performance comparable to
those obtained in fully-supervised settings, with only one third of the
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Shijie Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuhang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wayne Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic region proposal networks for semantic segmentation in automated glaucoma screening. (arXiv:2105.11364v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11364</id>
        <link href="http://arxiv.org/abs/2105.11364"/>
        <updated>2021-05-25T01:56:11.148Z</updated>
        <summary type="html"><![CDATA[Screening for the diagnosis of glaucoma through a fundus image can be
determined by the optic cup to disc diameter ratio (CDR), which requires the
segmentation of the cup and disc regions. In this paper, we propose two novel
approaches, namely Parameter-Shared Branched Network (PSBN) andWeak Region of
Interest Model-based segmentation (WRoIM) to identify disc and cup boundaries.
Unlike the previous approaches, the proposed methods are trained end-to-end
through a single neural network architecture and use dynamic cropping instead
of manual or traditional computer vision-based cropping. We are able to achieve
similar performance as that of state-of-the-art approaches with less number of
network parameters. Our experiments include comparison with different best
known methods on publicly available Drishti-GS1 and RIM-ONE v3 datasets. With
$7.8 \times 10^6$ parameters our approach achieves a Dice score of 0.96/0.89
for disc/cup segmentation on Drishti-GS1 data whereas the existing
state-of-the-art approach uses $19.8\times 10^6$ parameters to achieve a dice
score of 0.97/0.89.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Shivam Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasukurthi_N/0/1/0/all/0/1"&gt;Nikhil Kasukurthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pande_H/0/1/0/all/0/1"&gt;Harshit Pande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[2nd-order Updates with 1st-order Complexity. (arXiv:2105.11439v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11439</id>
        <link href="http://arxiv.org/abs/2105.11439"/>
        <updated>2021-05-25T01:56:11.148Z</updated>
        <summary type="html"><![CDATA[It has long been a goal to efficiently compute and use second order
information on a function ($f$) to assist in numerical approximations. Here it
is shown how, using only basic physics and a numerical approximation, such
information can be accurately obtained at a cost of ${\cal O}(N)$ complexity,
where $N$ is the dimensionality of the parameter space of $f$. In this paper,
an algorithm ({\em VA-Flow}) is developed to exploit this second order
information, and pseudocode is presented. It is applied to two classes of
problems, that of inverse kinematics (IK) and gradient descent (GD). In the IK
application, the algorithm is fast and robust, and is shown to lead to smooth
behavior even near singularities. For GD the algorithm also works very well,
provided the cost function is locally well-described by a polynomial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1"&gt;Michael F. Zimmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Compression Auto-Encoder for Detecting Road Surface Abnormality via Vehicle Driving Noise. (arXiv:2103.12992v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12992</id>
        <link href="http://arxiv.org/abs/2103.12992"/>
        <updated>2021-05-25T01:56:11.147Z</updated>
        <summary type="html"><![CDATA[Road accident can be triggered by wet road because it decreases skid
resistance. To prevent the road accident, detecting road surface abnomality is
highly useful. In this paper, we propose the deep learning based cost-effective
real-time anomaly detection architecture, naming with non-compression
auto-encoder (NCAE). The proposed architecture can reflect forward and backward
causality of time series information via convolutional operation. Moreover, the
above architecture shows higher anomaly detection performance of published
anomaly detection model via experiments. We conclude that NCAE as a
cutting-edge model for road surface anomaly detection with 4.20\% higher AUROC
and 2.99 times faster decision than before.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1"&gt;YeongHyeon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1"&gt;JongHee Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Cost of a Reductions Approach to Private Fair Optimization. (arXiv:1906.09613v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.09613</id>
        <link href="http://arxiv.org/abs/1906.09613"/>
        <updated>2021-05-25T01:56:11.147Z</updated>
        <summary type="html"><![CDATA[Through the lens of information-theoretic reductions, we examine a reductions
approach to fair optimization and learning where a black-box optimizer is used
to learn a fair model for classification or regression. Quantifying the
complexity, both statistically and computationally, of making such models
satisfy the rigorous definition of differential privacy is our end goal. We
resolve a few open questions and show applicability to fair machine learning,
hypothesis testing, and to optimizing non-standard measures of classification
loss. Furthermore, our sample complexity bounds are tight amongst all
strategies that jointly minimize a composition of functions.

The reductions approach to fair optimization can be abstracted as the
constrained group-objective optimization problem where we aim to optimize an
objective that is a function of losses of individual groups, subject to some
constraints. We give the first polynomial-time algorithms to solve the problem
with $(\epsilon, 0)$ or $(\epsilon, \delta)$ differential privacy guarantees
when defined on a convex decision set (for example, the $\ell_P$ unit ball)
with convex constraints and losses. Accompanying information-theoretic lower
bounds for the problem are presented. In addition, compared to a previous
method for ensuring differential privacy subject to a relaxed form of the
equalized odds fairness constraint, the $(\epsilon, \delta)$-differentially
private algorithm we present provides asymptotically better sample complexity
guarantees, resulting in an exponential improvement in certain parameter
regimes. We introduce a class of bounded divergence linear optimizers, which
could be of independent interest, and specialize to pure and approximate
differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alabi_D/0/1/0/all/0/1"&gt;Daniel Alabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning for One-Class Classification with Few Examples using Order-Equivariant Network. (arXiv:2007.04459v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04459</id>
        <link href="http://arxiv.org/abs/2007.04459"/>
        <updated>2021-05-25T01:56:11.146Z</updated>
        <summary type="html"><![CDATA[This paper presents a meta-learning framework for few-shots One-Class
Classification (OCC) at test-time, a setting where labeled examples are only
available for the positive class, and no supervision is given for the negative
example. We consider that we have a set of `one-class classification'
objective-tasks with only a small set of positive examples available for each
task, and a set of training tasks with full supervision (i.e. highly imbalanced
classification). We propose an approach using order-equivariant networks to
learn a 'meta' binary-classifier. The model will take as input an example to
classify from a given task, as well as the corresponding supervised set of
positive examples for this OCC task. Thus, the output of the model will be
'conditioned' on the available positive example of a given task, allowing to
predict on new tasks and new examples without labeled negative examples. In
this paper, we are motivated by an astronomy application. Our goal is to
identify if stars belong to a specific stellar group (the 'one-class' for a
given task), called \textit{stellar streams}, where each stellar stream is a
different OCC-task. We show that our method transfers well on unseen (test)
synthetic streams, and outperforms the baselines even though it is not
retrained and accesses a much smaller part of the data per task to predict
(only positive supervision). We see however that it doesn't transfer as well on
the real stream GD-1. This could come from intrinsic differences from the
synthetic and real stream, highlighting the need for consistency in the
'nature' of the task for this method. However, light fine-tuning improve
performances and outperform our baselines. Our experiments show encouraging
results to further explore meta-learning methods for OCC tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oladosu_A/0/1/0/all/0/1"&gt;Ademola Oladosu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tony Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekfeldt_P/0/1/0/all/0/1"&gt;Philip Ekfeldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_B/0/1/0/all/0/1"&gt;Brian A. Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cranmer_M/0/1/0/all/0/1"&gt;Miles Cranmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1"&gt;Shirley Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_Whelan_A/0/1/0/all/0/1"&gt;Adrian M. Price-Whelan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Contardo_G/0/1/0/all/0/1"&gt;Gabriella Contardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Redundancy-free Features for Generalized Zero-Shot Object Recognition. (arXiv:2006.08939v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08939</id>
        <link href="http://arxiv.org/abs/2006.08939"/>
        <updated>2021-05-25T01:56:11.137Z</updated>
        <summary type="html"><![CDATA[Zero-shot object recognition or zero-shot learning aims to transfer the
object recognition ability among the semantically related categories, such as
fine-grained animal or bird species. However, the images of different
fine-grained objects tend to merely exhibit subtle differences in appearance,
which will severely deteriorate zero-shot object recognition. To reduce the
superfluous information in the fine-grained objects, in this paper, we propose
to learn the redundancy-free features for generalized zero-shot learning. We
achieve our motivation by projecting the original visual features into a new
(redundancy-free) feature space and then restricting the statistical dependence
between these two feature spaces. Furthermore, we require the projected
features to keep and even strengthen the category relationship in the
redundancy-free feature space. In this way, we can remove the redundant
information from the visual features without losing the discriminative
information. We extensively evaluate the performance on four benchmark
datasets. The results show that our redundancy-free feature based generalized
zero-shot learning (RFF-GZSL) approach can achieve competitive results compared
with the state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zongyan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhenyong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Security Classifiers with Verified Global Robustness Properties. (arXiv:2105.11363v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.11363</id>
        <link href="http://arxiv.org/abs/2105.11363"/>
        <updated>2021-05-25T01:56:11.124Z</updated>
        <summary type="html"><![CDATA[Recent works have proposed methods to train classifiers with local robustness
properties, which can provably eliminate classes of evasion attacks for most
inputs, but not all inputs. Since data distribution shift is very common in
security applications, e.g., often observed for malware detection, local
robustness cannot guarantee that the property holds for unseen inputs at the
time of deploying the classifier. Therefore, it is more desirable to enforce
global robustness properties that hold for all inputs, which is strictly
stronger than local robustness.

In this paper, we present a framework and tools for training classifiers that
satisfy global robustness properties. We define new notions of global
robustness that are more suitable for security classifiers. We design a novel
booster-fixer training framework to enforce global robustness properties. We
structure our classifier as an ensemble of logic rules and design a new
verifier to verify the properties. In our training algorithm, the booster
increases the classifier's capacity, and the fixer enforces verified global
robustness properties following counterexample guided inductive synthesis.

To the best of our knowledge, the only global robustness property that has
been previously achieved is monotonicity. Several previous works have defined
global robustness properties, but their training techniques failed to achieve
verified global robustness. In comparison, we show that we can train
classifiers to satisfy different global robustness properties for three
security datasets, and even multiple properties at the same time, with modest
impact on the classifier's performance. For example, we train a Twitter spam
account classifier to satisfy five global robustness properties, with 5.4%
decrease in true positive rate, and 0.1% increase in false positive rate,
compared to a baseline XGBoost model that doesn't satisfy any property.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yizheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shiqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yue Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xiaojing Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1"&gt;Suman Jana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1"&gt;David Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The gradient complexity of linear regression. (arXiv:1911.02212v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.02212</id>
        <link href="http://arxiv.org/abs/1911.02212"/>
        <updated>2021-05-25T01:56:11.103Z</updated>
        <summary type="html"><![CDATA[We investigate the computational complexity of several basic linear algebra
primitives, including largest eigenvector computation and linear regression, in
the computational model that allows access to the data via a matrix-vector
product oracle. We show that for polynomial accuracy, $\Theta(d)$ calls to the
oracle are necessary and sufficient even for a randomized algorithm.

Our lower bound is based on a reduction to estimating the least eigenvalue of
a random Wishart matrix. This simple distribution enables a concise proof,
leveraging a few key properties of the random Wishart ensemble.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1"&gt;Mark Braverman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1"&gt;Elad Hazan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1"&gt;Blake Woodworth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesizer: Rethinking Self-Attention in Transformer Models. (arXiv:2005.00743v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00743</id>
        <link href="http://arxiv.org/abs/2005.00743"/>
        <updated>2021-05-25T01:56:11.096Z</updated>
        <summary type="html"><![CDATA[The dot product self-attention is known to be central and indispensable to
state-of-the-art Transformer models. But is it really required? This paper
investigates the true importance and contribution of the dot product-based
self-attention mechanism on the performance of Transformer models. Via
extensive experiments, we find that (1) random alignment matrices surprisingly
perform quite competitively and (2) learning attention weights from token-token
(query-key) interactions is useful but not that important after all. To this
end, we propose \textsc{Synthesizer}, a model that learns synthetic attention
weights without token-token interactions. In our experiments, we first show
that simple Synthesizers achieve highly competitive performance when compared
against vanilla Transformer models across a range of tasks, including machine
translation, language modeling, text generation and GLUE/SuperGLUE benchmarks.
When composed with dot product attention, we find that Synthesizers
consistently outperform Transformers. Moreover, we conduct additional
comparisons of Synthesizers against Dynamic Convolutions, showing that simple
Random Synthesizer is not only $60\%$ faster but also improves perplexity by a
relative $3.5\%$. Finally, we show that simple factorized Synthesizers can
outperform Linformers on encoding only tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1"&gt;Da-Cheng Juan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Che Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dilated Residual Hierarchically Fashioned Segmentation Framework for Extracting Gleason Tissues and Grading Prostate Cancer from Whole Slide Images. (arXiv:2011.00527v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00527</id>
        <link href="http://arxiv.org/abs/2011.00527"/>
        <updated>2021-05-25T01:56:11.087Z</updated>
        <summary type="html"><![CDATA[Prostate cancer (PCa) is the second deadliest form of cancer in males, and it
can be clinically graded by examining the structural representations of Gleason
tissues. This paper proposes the first attempt, to the best of our knowledge,
for segmenting the Gleason tissues to grade PCa via the whole slide images
(WSI). Also, the proposed approach encompasses two main contributions: 1) A
synergy of hybrid dilation factors and hierarchical decomposition of latent
space representation for effective Gleason tissues extraction, and 2) A
three-tiered loss function which can penalize different semantic segmentation
models for accurately extracting the highly correlated patterns. In addition to
this, the proposed framework has been extensively evaluated on a large-scale
PCa dataset containing 10,516 whole slide scans (with around 71.7M patches),
where it outperforms state-of-the-art schemes by 3.22% (in terms of mean
intersection-over-union) for extracting the Gleason tissues and 6.91% (in terms
of F1 score) for grading the progression of PCa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1"&gt;Taimur Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1"&gt;Bilal Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Baz_A/0/1/0/all/0/1"&gt;Ayman El-Baz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1"&gt;Naoufel Werghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smart mobile microscopy: towards fully-automated digitization. (arXiv:2105.11179v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11179</id>
        <link href="http://arxiv.org/abs/2105.11179"/>
        <updated>2021-05-25T01:56:11.077Z</updated>
        <summary type="html"><![CDATA[Mobile microscopy is a newly formed field that emerged from a combination of
optical microscopy capabilities and spread, functionality, and ever-increasing
computing resources of mobile devices. Despite the idea of creating a system
that would successfully merge a microscope, numerous computer vision methods,
and a mobile device is regularly examined, the resulting implementations still
require the presence of a qualified operator to control specimen digitization.
In this paper, we address the task of surpassing this constraint and present a
``smart'' mobile microscope concept aimed at automatic digitization of the most
valuable visual information about the specimen. We perform this through
combining automated microscope setup control and classic techniques such as
auto-focusing, in-focus filtering, and focus-stacking -- adapted and optimized
as parts of a mobile cross-platform library.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kornilova_A/0/1/0/all/0/1"&gt;A. Kornilova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kirilenko_I/0/1/0/all/0/1"&gt;I. Kirilenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Iarosh_D/0/1/0/all/0/1"&gt;D. Iarosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kutuev_V/0/1/0/all/0/1"&gt;V. Kutuev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Strutovsky_M/0/1/0/all/0/1"&gt;M. Strutovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Video Summarization with a Convolutional Attentive Adversarial Network. (arXiv:2105.11131v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11131</id>
        <link href="http://arxiv.org/abs/2105.11131"/>
        <updated>2021-05-25T01:56:11.076Z</updated>
        <summary type="html"><![CDATA[With the explosive growth of video data, video summarization, which attempts
to seek the minimum subset of frames while still conveying the main story, has
become one of the hottest topics. Nowadays, substantial achievements have been
made by supervised learning techniques, especially after the emergence of deep
learning. However, it is extremely expensive and difficult to collect human
annotation for large-scale video datasets. To address this problem, we propose
a convolutional attentive adversarial network (CAAN), whose key idea is to
build a deep summarizer in an unsupervised way. Upon the generative adversarial
network, our overall framework consists of a generator and a discriminator. The
former predicts importance scores for all frames of a video while the latter
tries to distinguish the score-weighted frame features from original frame
features. Specifically, the generator employs a fully convolutional sequence
network to extract global representation of a video, and an attention-based
network to output normalized importance scores. To learn the parameters, our
objective function is composed of three loss functions, which can guide the
frame-level importance score prediction collaboratively. To validate this
proposed method, we have conducted extensive experiments on two public
benchmarks SumMe and TVSum. The results show the superiority of our proposed
method against other state-of-the-art unsupervised approaches. Our method even
outperforms some published supervised approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1"&gt;Guoqiang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1"&gt;Yanbing Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shucheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shizhou Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanning Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty quantification for distributed regression. (arXiv:2105.11425v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.11425</id>
        <link href="http://arxiv.org/abs/2105.11425"/>
        <updated>2021-05-25T01:56:11.076Z</updated>
        <summary type="html"><![CDATA[The ever-growing size of the datasets renders well-studied learning
techniques, such as Kernel Ridge Regression, inapplicable, posing a serious
computational challenge. Divide-and-conquer is a common remedy, suggesting to
split the dataset into disjoint partitions, obtain the local estimates and
average them, it allows to scale-up an otherwise ineffective base approach. In
the current study we suggest a fully data-driven approach to quantify
uncertainty of the averaged estimator. Namely, we construct simultaneous
element-wise confidence bands for the predictions yielded by the averaged
estimator on a given deterministic prediction set. The novel approach features
rigorous theoretical guaranties for a wide class of base learners with Kernel
Ridge regression being a special case. As a by-product of our analysis we also
obtain a sup-norm consistency result for the divide-and-conquer Kernel Ridge
Regression. The simulation study supports the theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Avanesov_V/0/1/0/all/0/1"&gt;Valeriy Avanesov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model. (arXiv:2105.11312v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11312</id>
        <link href="http://arxiv.org/abs/2105.11312"/>
        <updated>2021-05-25T01:56:11.075Z</updated>
        <summary type="html"><![CDATA[3D action recognition is referred to as the classification of action
sequences which consist of 3D skeleton joints. While many research work are
devoted to 3D action recognition, it mainly suffers from three problems: highly
complicated articulation, a great amount of noise, and a low implementation
efficiency. To tackle all these problems, we propose a real-time 3D action
recognition framework by integrating the locally aggregated kinematic-guided
skeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first
define the skeletonlet as a few combinations of joint offsets grouped in terms
of kinematic principle, and then represent an action sequence using LAKS, which
consists of a denoising phase and a locally aggregating phase. The denoising
phase detects the noisy action data and adjust it by replacing all the features
within it with the features of the corresponding previous frame, while the
locally aggregating phase sums the difference between an offset feature of the
skeletonlet and its cluster center together over all the offset features of the
sequence. Finally, the SHA model which combines sparse representation with a
hashing model, aiming at promoting the recognition accuracy while maintaining a
high efficiency. Experimental results on MSRAction3D, UTKinectAction3D and
Florence3DAction datasets demonstrate that the proposed method outperforms
state-of-the-art methods in both recognition accuracy and implementation
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Bin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1"&gt;Dehui Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shaofan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lichun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Baocai Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling. (arXiv:2012.03245v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03245</id>
        <link href="http://arxiv.org/abs/2012.03245"/>
        <updated>2021-05-25T01:56:11.075Z</updated>
        <summary type="html"><![CDATA[Conversion rate (CVR) prediction is one of the most critical tasks for
digital display advertising. Commercial systems often require to update models
in an online learning manner to catch up with the evolving data distribution.
However, conversions usually do not happen immediately after a user click. This
may result in inaccurate labeling, which is called delayed feedback problem. In
previous studies, delayed feedback problem is handled either by waiting
positive label for a long period of time, or by consuming the negative sample
on its arrival and then insert a positive duplicate when a conversion happens
later. Indeed, there is a trade-off between waiting for more accurate labels
and utilizing fresh data, which is not considered in existing works. To strike
a balance in this trade-off, we propose Elapsed-Time Sampling Delayed Feedback
Model (ES-DFM), which models the relationship between the observed conversion
distribution and the true conversion distribution. Then we optimize the
expectation of true conversion distribution via importance sampling under the
elapsed-time sampling distribution. We further estimate the importance weight
for each instance, which is used as the weight of loss function in CVR
prediction. To demonstrate the effectiveness of ES-DFM, we conduct extensive
experiments on a public data and a private industrial dataset. Experimental
results confirm that our method consistently outperforms the previous
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jia-Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shuguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_T/0/1/0/all/0/1"&gt;Tao Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiaoyi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_B/0/1/0/all/0/1"&gt;Bin Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across the Workspace. (arXiv:2105.11283v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.11283</id>
        <link href="http://arxiv.org/abs/2105.11283"/>
        <updated>2021-05-25T01:56:11.073Z</updated>
        <summary type="html"><![CDATA[When training control policies for robot manipulation via deep learning,
sim-to-real transfer can help satisfy the large data requirements. In this
paper, we study the problem of zero-shot sim-to-real when the task requires
both highly precise control, with sub-millimetre error tolerance, and full
workspace generalisation. Our framework involves a coarse-to-fine controller,
where trajectories initially begin with classical motion planning based on pose
estimation, and transition to an end-to-end controller which maps images to
actions and is trained in simulation with domain randomisation. In this way, we
achieve precise control whilst also generalising the controller across the
workspace and keeping the generality and robustness of vision-based, end-to-end
control. Real-world experiments on a range of different tasks show that, by
exploiting the best of both worlds, our framework significantly outperforms
purely motion planning methods, and purely learning-based methods. Furthermore,
we answer a range of questions on best practices for precise sim-to-real
transfer, such as how different image sensor modalities and image feature
representations perform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1"&gt;Eugene Valassakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1"&gt;Norman Di Palo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1"&gt;Edward Johns&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unpaired Image-to-Image Translation via Latent Energy Transport. (arXiv:2012.00649v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00649</id>
        <link href="http://arxiv.org/abs/2012.00649"/>
        <updated>2021-05-25T01:56:11.073Z</updated>
        <summary type="html"><![CDATA[Image-to-image translation aims to preserve source contents while translating
to discriminative target styles between two visual domains. Most works apply
adversarial learning in the ambient image space, which could be computationally
expensive and challenging to train. In this paper, we propose to deploy an
energy-based model (EBM) in the latent space of a pretrained autoencoder for
this task. The pretrained autoencoder serves as both a latent code extractor
and an image reconstruction worker. Our model, LETIT, is based on the
assumption that two domains share the same latent space, where latent
representation is implicitly decomposed as a content code and a domain-specific
style code. Instead of explicitly extracting the two codes and applying
adaptive instance normalization to combine them, our latent EBM can implicitly
learn to transport the source style code to the target style code while
preserving the content code, an advantage over existing image translation
methods. This simplified solution is also more efficient in the one-sided
unpaired image translation setting. Qualitative and quantitative comparisons
demonstrate superior translation quality and faithfulness for content
preservation. Our model is the first to be applicable to
1024$\times$1024-resolution unpaired image translation to the best of our
knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changyou Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification. (arXiv:2105.11043v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11043</id>
        <link href="http://arxiv.org/abs/2105.11043"/>
        <updated>2021-05-25T01:56:11.073Z</updated>
        <summary type="html"><![CDATA[Black-box skepticism is one of the main hindrances impeding
deep-learning-based automatic sleep scoring from being used in clinical
environments. Towards interpretability, this work proposes a
sequence-to-sequence sleep-staging model, namely SleepTransformer. It is based
on the transformer backbone whose self-attention scores offer interpretability
of the model's decisions at both the epoch and sequence level. At the epoch
level, the attention scores can be encoded as a heat map to highlight
sleep-relevant features captured from the input EEG signal. At the sequence
level, the attention scores are visualized as the influence of different
neighboring epochs in an input sequence (i.e. the context) to recognition of a
target epoch, mimicking the way manual scoring is done by human experts. We
further propose a simple yet efficient method to quantify uncertainty in the
model's decisions. The method, which is based on entropy, can serve as a metric
for deferring low-confidence epochs to a human expert for further inspection.
Additionally, we demonstrate that the proposed SleepTransformer outperforms
existing methods at a lower computational cost and achieves state-of-the-art
performance on two experimental databases of different sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1"&gt;Huy Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikkelsen_K/0/1/0/all/0/1"&gt;Kaare Mikkelsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_O/0/1/0/all/0/1"&gt;Oliver Y. Ch&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1"&gt;Philipp Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mertins_A/0/1/0/all/0/1"&gt;Alfred Mertins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1"&gt;Maarten De Vos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning in Diabetic Foot Ulcers Detection: A Comprehensive Evaluation. (arXiv:2010.03341v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03341</id>
        <link href="http://arxiv.org/abs/2010.03341"/>
        <updated>2021-05-25T01:56:11.065Z</updated>
        <summary type="html"><![CDATA[There has been a substantial amount of research involving computer methods
and technology for the detection and recognition of diabetic foot ulcers
(DFUs), but there is a lack of systematic comparisons of state-of-the-art deep
learning object detection frameworks applied to this problem. DFUC2020 provided
participants with a comprehensive dataset consisting of 2,000 images for
training and 2,000 images for testing. This paper summarises the results of
DFUC2020 by comparing the deep learning-based algorithms proposed by the
winning teams: Faster R-CNN, three variants of Faster R-CNN and an ensemble
method; YOLOv3; YOLOv5; EfficientDet; and a new Cascade Attention Network. For
each deep learning method, we provide a detailed description of model
architecture, parameter settings for training and additional stages including
pre-processing, data augmentation and post-processing. We provide a
comprehensive evaluation for each method. All the methods required a data
augmentation stage to increase the number of images available for training and
a post-processing stage to remove false positives. The best performance was
obtained from Deformable Convolution, a variant of Faster R-CNN, with a mean
average precision (mAP) of 0.6940 and an F1-Score of 0.7434. Finally, we
demonstrate that the ensemble method based on different deep learning methods
can enhanced the F1-Score but not the mAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1"&gt;Moi Hoon Yap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1"&gt;Ryo Hachiuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alavi_A/0/1/0/all/0/1"&gt;Azadeh Alavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brungel_R/0/1/0/all/0/1"&gt;Raphael Brungel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassidy_B/0/1/0/all/0/1"&gt;Bill Cassidy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_M/0/1/0/all/0/1"&gt;Manu Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongtao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruckert_J/0/1/0/all/0/1"&gt;Johannes Ruckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olshansky_M/0/1/0/all/0/1"&gt;Moshe Olshansky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1"&gt;Hideo Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1"&gt;Saeed Hassanpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedrich_C/0/1/0/all/0/1"&gt;Christoph M. Friedrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ascher_D/0/1/0/all/0/1"&gt;David Ascher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_A/0/1/0/all/0/1"&gt;Anping Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kajita_H/0/1/0/all/0/1"&gt;Hiroki Kajita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gillespie_D/0/1/0/all/0/1"&gt;David Gillespie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reeves_N/0/1/0/all/0/1"&gt;Neil D. Reeves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappachan_J/0/1/0/all/0/1"&gt;Joseph Pappachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OShea_C/0/1/0/all/0/1"&gt;Claire O&amp;#x27;Shea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1"&gt;Eibe Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GMAC: A Distributional Perspective on Actor-Critic Framework. (arXiv:2105.11366v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11366</id>
        <link href="http://arxiv.org/abs/2105.11366"/>
        <updated>2021-05-25T01:56:11.065Z</updated>
        <summary type="html"><![CDATA[In this paper, we devise a distributional framework on actor-critic as a
solution to distributional instability, action type restriction, and conflation
between samples and statistics. We propose a new method that minimizes the
Cram\'er distance with the multi-step Bellman target distribution generated
from a novel Sample-Replacement algorithm denoted SR($\lambda$), which learns
the correct value distribution under multiple Bellman operations.
Parameterizing a value distribution with Gaussian Mixture Model further
improves the efficiency and the performance of the method, which we name GMAC.
We empirically show that GMAC captures the correct representation of value
distributions and improves the performance of a conventional actor-critic
method with low computational cost, in both discrete and continuous action
spaces using Arcade Learning Environment (ALE) and PyBullet environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1"&gt;Daniel Wontae Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Younghoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chan Y. Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LineCounter: Learning Handwritten Text Line Segmentation by Counting. (arXiv:2105.11307v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11307</id>
        <link href="http://arxiv.org/abs/2105.11307"/>
        <updated>2021-05-25T01:56:11.058Z</updated>
        <summary type="html"><![CDATA[Handwritten Text Line Segmentation (HTLS) is a low-level but important task
for many higher-level document processing tasks like handwritten text
recognition. It is often formulated in terms of semantic segmentation or object
detection in deep learning. However, both formulations have serious
shortcomings. The former requires heavy post-processing of splitting/merging
adjacent segments, while the latter may fail on dense or curved texts. In this
paper, we propose a novel Line Counting formulation for HTLS -- that involves
counting the number of text lines from the top at every pixel location. This
formulation helps learn an end-to-end HTLS solution that directly predicts
per-pixel line number for a given document image. Furthermore, we propose a
deep neural network (DNN) model LineCounter to perform HTLS through the Line
Counting formulation. Our extensive experiments on the three public datasets
(ICDAR2013-HSC, HIT-MW, and VML-AHTE) demonstrate that LineCounter outperforms
state-of-the-art HTLS approaches. Source code is available at
https://github.com/Leedeng/Line-Counter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Deng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yicong Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward a Generalization Metric for Deep Generative Models. (arXiv:2011.00754v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00754</id>
        <link href="http://arxiv.org/abs/2011.00754"/>
        <updated>2021-05-25T01:56:11.057Z</updated>
        <summary type="html"><![CDATA[Measuring the generalization capacity of Deep Generative Models (DGMs) is
difficult because of the curse of dimensionality. Evaluation metrics for DGMs
such as Inception Score, Fr\'echet Inception Distance, Precision-Recall, and
Neural Net Divergence try to estimate the distance between the generated
distribution and the target distribution using a polynomial number of samples.
These metrics are the target of researchers when designing new models. Despite
the claims, it is still unclear how well can they measure the generalization
capacity of a generative model. In this paper, we investigate the capacity of
these metrics in measuring the generalization capacity. We introduce a
framework for comparing the robustness of evaluation metrics. We show that
better scores in these metrics do not imply better generalization. They can be
fooled easily by a generator that memorizes a small subset of the training set.
We propose a fix to the NND metric to make it more robust to noise in the
generated data. Toward building a robust metric for generalization, we propose
to apply the Minimum Description Length principle to the problem of evaluating
DGMs. We develop an efficient method for estimating the complexity of
Generative Latent Variable Models (GLVMs). Experimental results show that our
metric can effectively detect training set memorization and distinguish GLVMs
of different generalization capacities. Source code is available at
https://github.com/htt210/GeneralizationMetricGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thanh_Tung_H/0/1/0/all/0/1"&gt;Hoang Thanh-Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Penetration Testing with Reinforcement Learning Using Capture-the-Flag Challenges: Trade-offs between Model-free Learning and A Priori Knowledge. (arXiv:2005.12632v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12632</id>
        <link href="http://arxiv.org/abs/2005.12632"/>
        <updated>2021-05-25T01:56:11.056Z</updated>
        <summary type="html"><![CDATA[Penetration testing is a security exercise aimed at assessing the security of
a system by simulating attacks against it. So far, penetration testing has been
carried out mainly by trained human attackers and its success critically
depended on the available expertise. Automating this practice constitutes a
non-trivial problem, as the range of actions that a human expert may attempts
against a system and the range of knowledge she relies on to take her decisions
are hard to capture. In this paper, we focus our attention on simplified
penetration testing problems expressed in the form of capture the flag hacking
challenges, and we analyze how model-free reinforcement learning algorithms may
help to solve them. In modeling these capture the flag competitions as
reinforcement learning problems we highlight that a specific challenge that
characterize penetration testing is the problem of discovering the structure of
the problem at hand. We then show how this challenge may be eased by relying on
different forms of prior knowledge that may be provided to the agent. In this
way we demonstrate how the feasibility of tackling penetration testing using
reinforcement learning may rest on a careful trade-off between model-free and
model-based algorithms. By using techniques to inject a priori knowledge, we
show it is possible to better direct the agent and restrict the space of its
exploration problem, thus achieving solutions more efficiently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zennaro_F/0/1/0/all/0/1"&gt;Fabio Massimo Zennaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdodi_L/0/1/0/all/0/1"&gt;Laszlo Erdodi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimation of Orientation and Camera Parameters from Cryo-Electron Microscopy Images with Variational Autoencoders and Generative Adversarial Networks. (arXiv:1911.08121v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.08121</id>
        <link href="http://arxiv.org/abs/1911.08121"/>
        <updated>2021-05-25T01:56:11.055Z</updated>
        <summary type="html"><![CDATA[Cryo-electron microscopy (cryo-EM) is capable of producing reconstructed 3D
images of biomolecules at near-atomic resolution. As such, it represents one of
the most promising imaging techniques in structural biology. However, raw
cryo-EM images are only highly corrupted - noisy and band-pass filtered - 2D
projections of the target 3D biomolecules. Reconstructing the 3D molecular
shape starts with the removal of image outliers, the estimation of the
orientation of the biomolecule that has produced the given 2D image, and the
estimation of camera parameters to correct for intensity defects. Current
techniques performing these tasks are often computationally expensive, while
the dataset sizes keep growing. There is a need for next-generation algorithms
that preserve accuracy while improving speed and scalability. In this paper, we
combine variational autoencoders (VAEs) and generative adversarial networks
(GANs) to learn a low-dimensional latent representation of cryo-EM images. We
perform an exploratory analysis of the obtained latent space, that is shown to
have a structure of "orbits", in the sense of Lie group theory, consistent with
the acquisition procedure of cryo-EM images. This analysis leads us to design
an estimation method for orientation and camera parameters of single-particle
cryo-EM images, together with an outliers detection procedure. As such, it
opens the door to geometric approaches for unsupervised estimations of
orientations and camera parameters, making possible fast cryo-EM biomolecule
reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Miolane_N/0/1/0/all/0/1"&gt;Nina Miolane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Poitevin_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;d&amp;#xe9;ric Poitevin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yee-Ting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Holmes_S/0/1/0/all/0/1"&gt;Susan Holmes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Brain tumour segmentation using a triplanar ensemble of U-Nets. (arXiv:2105.11356v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11356</id>
        <link href="http://arxiv.org/abs/2105.11356"/>
        <updated>2021-05-25T01:56:11.033Z</updated>
        <summary type="html"><![CDATA[Gliomas appear with wide variation in their characteristics both in terms of
their appearance and location on brain MR images, which makes robust tumour
segmentation highly challenging, and leads to high inter-rater variability even
in manual segmentations. In this work, we propose a triplanar ensemble network,
with an independent tumour core prediction module, for accurate segmentation of
these tumours and their sub-regions. On evaluating our method on the MICCAI
Brain Tumor Segmentation (BraTS) challenge validation dataset, for tumour
sub-regions, we achieved a Dice similarity coefficient of 0.77 for both
enhancing tumour (ET) and tumour core (TC). In the case of the whole tumour
(WT) region, we achieved a Dice value of 0.89, which is on par with the
top-ranking methods from BraTS'17-19. Our method achieved an evaluation score
that was the equal 5th highest value (with our method ranking in 10th place) in
the BraTS'20 challenge, with mean Dice values of 0.81, 0.89 and 0.84 on ET, WT
and TC regions respectively on the BraTS'20 unseen test dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sundaresan_V/0/1/0/all/0/1"&gt;Vaanathi Sundaresan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Griffanti_L/0/1/0/all/0/1"&gt;Ludovica Griffanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1"&gt;Mark Jenkinson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypergraph Pre-training with Graph Neural Networks. (arXiv:2105.10862v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10862</id>
        <link href="http://arxiv.org/abs/2105.10862"/>
        <updated>2021-05-25T01:56:11.033Z</updated>
        <summary type="html"><![CDATA[Despite the prevalence of hypergraphs in a variety of high-impact
applications, there are relatively few works on hypergraph representation
learning, most of which primarily focus on hyperlink prediction, often
restricted to the transductive learning setting. Among others, a major hurdle
for effective hypergraph representation learning lies in the label scarcity of
nodes and/or hyperedges. To address this issue, this paper presents an
end-to-end, bi-level pre-training strategy with Graph Neural Networks for
hypergraphs. The proposed framework named HyperGene bears three distinctive
advantages. First, it is capable of ingesting the labeling information when
available, but more importantly, it is mainly designed in the self-supervised
fashion which significantly broadens its applicability. Second, at the heart of
the proposed HyperGene are two carefully designed pretexts, one on the node
level and the other on the hyperedge level, which enable us to encode both the
local and the global context in a mutually complementary way. Third, the
proposed framework can work in both transductive and inductive settings. When
applying the two proposed pretexts in tandem, it can accelerate the adaptation
of the knowledge from the pre-trained model to downstream applications in the
transductive setting, thanks to the bi-level nature of the proposed method. The
extensive experimental results demonstrate that: (1) HyperGene achieves up to
5.69% improvements in hyperedge classification, and (2) improves pre-training
efficiency by up to 42.80% on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Boxin Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1"&gt;Changhe Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barton_R/0/1/0/all/0/1"&gt;Robert Barton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiman_T/0/1/0/all/0/1"&gt;Tal Neiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1"&gt;Hanghang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Level Attentive Convoluntional Neural Network for Crowd Counting. (arXiv:2105.11422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11422</id>
        <link href="http://arxiv.org/abs/2105.11422"/>
        <updated>2021-05-25T01:56:11.032Z</updated>
        <summary type="html"><![CDATA[Recently the crowd counting has received more and more attention. Especially
the technology of high-density environment has become an important research
content, and the relevant methods for the existence of extremely dense crowd
are not optimal. In this paper, we propose a multi-level attentive
Convolutional Neural Network (MLAttnCNN) for crowd counting. We extract
high-level contextual information with multiple different scales applied in
pooling, and use multi-level attention modules to enrich the characteristics at
different layers to achieve more efficient multi-scale feature fusion, which is
able to be used to generate a more accurate density map with dilated
convolutions and a $1\times 1$ convolution. The extensive experiments on three
available public datasets show that our proposed network achieves
outperformance to the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1"&gt;Mengxiao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1"&gt;Chengjiang Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimized conformal classification using gradient descent approximation. (arXiv:2105.11255v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11255</id>
        <link href="http://arxiv.org/abs/2105.11255"/>
        <updated>2021-05-25T01:56:11.023Z</updated>
        <summary type="html"><![CDATA[Conformal predictors are an important class of algorithms that allow
predictions to be made with a user-defined confidence level. They are able to
do this by outputting prediction sets, rather than simple point predictions.
The conformal predictor is valid in the sense that the accuracy of its
predictions is guaranteed to meet the confidence level, only assuming
exchangeability in the data. Since accuracy is guaranteed, the performance of a
conformal predictor is measured through the efficiency of the prediction sets.
Typically, a conformal predictor is built on an underlying machine learning
algorithm and hence its predictive power is inherited from this algorithm.
However, since the underlying machine learning algorithm is not trained with
the objective of minimizing predictive efficiency it means that the resulting
conformal predictor may be sub-optimal and not aligned sufficiently to this
objective. Hence, in this study we consider an approach to train the conformal
predictor directly with maximum predictive efficiency as the optimization
objective, and we focus specifically on the inductive conformal predictor for
classification. To do this, the conformal predictor is approximated by a
differentiable objective function and gradient descent used to optimize it. The
resulting parameter estimates are then passed to a proper inductive conformal
predictor to give valid prediction sets. We test the method on several real
world data sets and find that the method is promising and in most cases gives
improved predictive efficiency against a baseline conformal predictor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bellotti_A/0/1/0/all/0/1"&gt;Anthony Bellotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Room Clearance with Feudal Hierarchical Reinforcement Learning. (arXiv:2105.11328v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11328</id>
        <link href="http://arxiv.org/abs/2105.11328"/>
        <updated>2021-05-25T01:56:11.016Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) is a general framework that allows systems to
learn autonomously through trial-and-error interaction with their environment.
In recent years combining RL with expressive, high-capacity neural network
models has led to impressive performance in a diverse range of domains.
However, dealing with the large state and action spaces often required for
problems in the real world still remains a significant challenge. In this paper
we introduce a new simulation environment, "Gambit", designed as a tool to
build scenarios that can drive RL research in a direction useful for military
analysis. Using this environment we focus on an abstracted and simplified room
clearance scenario, where a team of blue agents have to make their way through
a building and ensure that all rooms are cleared of (and remain clear) of enemy
red agents. We implement a multi-agent version of feudal hierarchical RL that
introduces a command hierarchy where a commander at the higher level sends
orders to multiple agents at the lower level who simply have to learn to follow
these orders. We find that breaking the task down in this way allows us to
solve a number of non-trivial floorplans that require the coordination of
multiple agents much more efficiently than the standard baseline RL algorithms
we compare with. We then go on to explore how qualitatively different behaviour
can emerge depending on what we prioritise in the agent's reward function (e.g.
clearing the building quickly vs. prioritising rescuing civilians).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Charlesworth_H/0/1/0/all/0/1"&gt;Henry Charlesworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Millea_A/0/1/0/all/0/1"&gt;Adrian Millea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pottrill_E/0/1/0/all/0/1"&gt;Eddie Pottrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riley_R/0/1/0/all/0/1"&gt;Rich Riley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weight-Covariance Alignment for Adversarially Robust Neural Networks. (arXiv:2010.08852v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08852</id>
        <link href="http://arxiv.org/abs/2010.08852"/>
        <updated>2021-05-25T01:56:11.009Z</updated>
        <summary type="html"><![CDATA[Stochastic Neural Networks (SNNs) that inject noise into their hidden layers
have recently been shown to achieve strong robustness against adversarial
attacks. However, existing SNNs are usually heuristically motivated, and often
rely on adversarial training, which is computationally costly. We propose a new
SNN that achieves state-of-the-art performance without relying on adversarial
training, and enjoys solid theoretical justification. Specifically, while
existing SNNs inject learned or hand-tuned isotropic noise, our SNN learns an
anisotropic noise distribution to optimize a learning-theoretic bound on
adversarial robustness. We evaluate our method on a number of popular
benchmarks, show that it can be applied to different architectures, and that it
provides robustness to a variety of white-box and black-box attacks, while
being simple and fast to train compared to existing alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eustratiadis_P/0/1/0/all/0/1"&gt;Panagiotis Eustratiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gouk_H/0/1/0/all/0/1"&gt;Henry Gouk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Da Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skew Orthogonal Convolutions. (arXiv:2105.11417v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11417</id>
        <link href="http://arxiv.org/abs/2105.11417"/>
        <updated>2021-05-25T01:56:10.999Z</updated>
        <summary type="html"><![CDATA[Training convolutional neural networks with a Lipschitz constraint under the
$l_{2}$ norm is useful for provable adversarial robustness, interpretable
gradients, stable training, etc. While 1-Lipschitz networks can be designed by
imposing a 1-Lipschitz constraint on each layer, training such networks
requires each layer to be gradient norm preserving (GNP) to prevent gradients
from vanishing. However, existing GNP convolutions suffer from slow training,
lead to significant reduction in accuracy and provide no guarantees on their
approximations. In this work, we propose a GNP convolution layer called
\methodnamebold\ (\methodabv) that uses the following mathematical property:
when a matrix is {\it Skew-Symmetric}, its exponential function is an {\it
orthogonal} matrix. To use this property, we first construct a convolution
filter whose Jacobian is Skew-Symmetric. Then, we use the Taylor series
expansion of the Jacobian exponential to construct the \methodabv\ layer that
is orthogonal. To efficiently implement \methodabv, we keep a finite number of
terms from the Taylor series and provide a provable guarantee on the
approximation error. Our experiments on CIFAR-10 and CIFAR-100 show that
\methodabv\ allows us to train provably Lipschitz, large convolutional neural
networks significantly faster than prior works while achieving significant
improvements for both standard and certified robust accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sahil Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1"&gt;Soheil Feizi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for 3D Point Cloud Understanding: A Survey. (arXiv:2009.08920v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08920</id>
        <link href="http://arxiv.org/abs/2009.08920"/>
        <updated>2021-05-25T01:56:10.998Z</updated>
        <summary type="html"><![CDATA[The development of practical applications, such as autonomous driving and
robotics, has brought increasing attention to 3D point cloud understanding.
While deep learning has achieved remarkable success on image-based tasks, there
are many unique challenges faced by deep neural networks in processing massive,
unstructured and noisy 3D points. To demonstrate the latest progress of deep
learning for 3D point cloud understanding, this paper summarizes recent
remarkable research contributions in this area from several different
directions (classification, segmentation, detection, tracking, flow estimation,
registration, augmentation and completion), together with commonly used
datasets, metrics and state-of-the-art performances. More information regarding
this survey can be found at:
https://github.com/SHI-Labs/3D-Point-Cloud-Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haoming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Humphrey Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards High Performance Human Keypoint Detection. (arXiv:2002.00537v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00537</id>
        <link href="http://arxiv.org/abs/2002.00537"/>
        <updated>2021-05-25T01:56:10.981Z</updated>
        <summary type="html"><![CDATA[Human keypoint detection from a single image is very challenging due to
occlusion, blur, illumination and scale variance. In this paper, we address
this problem from three aspects by devising an efficient network structure,
proposing three effective training strategies, and exploiting four useful
postprocessing techniques. First, we find that context information plays an
important role in reasoning human body configuration and invisible keypoints.
Inspired by this, we propose a cascaded context mixer (CCM), which efficiently
integrates spatial and channel context information and progressively refines
them. Then, to maximize CCM's representation capability, we develop a
hard-negative person detection mining strategy and a joint-training strategy by
exploiting abundant unlabeled data. It enables CCM to learn discriminative
features from massive diverse poses. Third, we present several sub-pixel
refinement techniques for postprocessing keypoint predictions to improve
detection accuracy. Extensive experiments on the MS COCO keypoint detection
benchmark demonstrate the superiority of the proposed method over
representative state-of-the-art (SOTA) methods. Our single model achieves
comparable performance with the winner of the 2018 COCO Keypoint Detection
Challenge. The final ensemble model sets a new SOTA on this benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Variational Semi-Supervised Novelty Detection. (arXiv:1911.04971v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.04971</id>
        <link href="http://arxiv.org/abs/1911.04971"/>
        <updated>2021-05-25T01:56:10.974Z</updated>
        <summary type="html"><![CDATA[In anomaly detection (AD), one seeks to identify whether a test sample is
abnormal, given a data set of normal samples. A recent and promising approach
to AD relies on deep generative models, such as variational autoencoders
(VAEs), for unsupervised learning of the normal data distribution. In
semi-supervised AD (SSAD), the data also includes a small sample of labeled
anomalies. In this work, we propose two variational methods for training VAEs
for SSAD. The intuitive idea in both methods is to train the encoder to
`separate' between latent vectors for normal and outlier data. We show that
this idea can be derived from principled probabilistic formulations of the
problem, and propose simple and effective algorithms. Our methods can be
applied to various data types, as we demonstrate on SSAD datasets ranging from
natural images to astronomy and medicine, can be combined with any VAE model
architecture, and are naturally compatible with ensembling. When comparing to
state-of-the-art SSAD methods that are not specific to particular data types,
we obtain marked improvement in outlier detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daniel_T/0/1/0/all/0/1"&gt;Tal Daniel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurutach_T/0/1/0/all/0/1"&gt;Thanard Kurutach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1"&gt;Aviv Tamar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence of Langevin Monte Carlo in Chi-Square Divergence. (arXiv:2007.11612v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11612</id>
        <link href="http://arxiv.org/abs/2007.11612"/>
        <updated>2021-05-25T01:56:10.967Z</updated>
        <summary type="html"><![CDATA[We study sampling from a target distribution $\nu_* = e^{-f}$ using the
unadjusted Langevin Monte Carlo (LMC) algorithm when the potential $f$
satisfies a strong dissipativity condition and it is first-order smooth with
Lipschitz gradient. We prove that, initialized with a Gaussian that has
sufficiently small variance, $\widetilde{\mathcal{O}}(\lambda d\epsilon^{-1})$
steps of the LMC algorithm are sufficient to reach $\epsilon$-neighborhood of
the target in Chi-square divergence, where $\lambda$ is the log-Sobolev
constant of $\nu_*$. Our results do not require warm-start to deal with
exponential dimension dependency in Chi-square divergence at initialization. In
particular, for strongly convex and first-order smooth potentials, we show that
the LMC algorithm achieves the rate estimate
$\widetilde{\mathcal{O}}(d\epsilon^{-1})$ which improves the previously known
rates in this metric, under the same assumptions. Translating to other metrics,
our result also recovers the best-known rate estimates in KL divergence, total
variation and $2$-Wasserstein distance in the same setup. Finally, as we rely
on the log-Sobolev inequality, our framework covers a wide range of non-convex
potentials that are first-order smooth and that exhibit strong convexity
outside of a compact region.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hosseinzadeh_R/0/1/0/all/0/1"&gt;Rasa Hosseinzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Matthew S. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2105.11166</id>
        <link href="http://arxiv.org/abs/2105.11166"/>
        <updated>2021-05-25T01:56:10.960Z</updated>
        <summary type="html"><![CDATA[State-of-the-art performance for many emerging edge applications is achieved
by deep neural networks (DNNs). Often, these DNNs are location and time
sensitive, and the parameters of a specific DNN must be delivered from an edge
server to the edge device rapidly and efficiently to carry out time-sensitive
inference tasks. We introduce AirNet, a novel training and analog transmission
method that allows efficient wireless delivery of DNNs. We first train the DNN
with noise injection to counter the wireless channel noise. We also employ
pruning to reduce the channel bandwidth necessary for transmission, and perform
knowledge distillation from a larger model to achieve satisfactory performance,
despite the channel perturbations. We show that AirNet achieves significantly
higher test accuracy compared to digital alternatives under the same bandwidth
and power constraints. It also exhibits graceful degradation with channel
quality, which reduces the requirement for accurate channel estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1"&gt;Mikolaj Jankowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1"&gt;Krystian Mikolajczyk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving DeepFake Detection Using Dynamic Face Augmentation. (arXiv:2102.09603v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09603</id>
        <link href="http://arxiv.org/abs/2102.09603"/>
        <updated>2021-05-25T01:56:10.959Z</updated>
        <summary type="html"><![CDATA[The creation of altered and manipulated faces has become more common due to
the improvement of DeepFake generation methods. Simultaneously, we have seen
detection models' development for differentiating between a manipulated and
original face from image or video content. We have observed that most publicly
available DeepFake detection datasets have limited variations, where a single
face is used in many videos, resulting in an oversampled training dataset. Due
to this, deep neural networks tend to overfit to the facial features instead of
learning to detect manipulation features of DeepFake content. As a result, most
detection architectures perform poorly when tested on unseen data. In this
paper, we provide a quantitative analysis to investigate this problem and
present a solution to prevent model overfitting due to the high volume of
samples generated from a small number of actors. We introduce Face-Cutout, a
data augmentation method for training Convolutional Neural Networks (CNN), to
improve DeepFake detection. In this method, training images with various
occlusions are dynamically generated using face landmark information
irrespective of orientation. Unlike other general-purpose augmentation methods,
it focuses on the facial information that is crucial for DeepFake detection.
Our method achieves a reduction in LogLoss of 15.2% to 35.3% on different
datasets, compared to other occlusion-based augmentation techniques. We show
that Face-Cutout can be easily integrated with any CNN-based recognition model
and improve detection performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sowmen Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1"&gt;Arup Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Md. Saiful Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1"&gt;Md. Ruhul Amin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Rainbow: Promoting more Insightful and Inclusive Deep Reinforcement Learning Research. (arXiv:2011.14826v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14826</id>
        <link href="http://arxiv.org/abs/2011.14826"/>
        <updated>2021-05-25T01:56:10.959Z</updated>
        <summary type="html"><![CDATA[Since the introduction of DQN, a vast majority of reinforcement learning
research has focused on reinforcement learning with deep neural networks as
function approximators. New methods are typically evaluated on a set of
environments that have now become standard, such as Atari 2600 games. While
these benchmarks help standardize evaluation, their computational cost has the
unfortunate side effect of widening the gap between those with ample access to
computational resources, and those without. In this work we argue that, despite
the community's emphasis on large-scale environments, the traditional
small-scale environments can still yield valuable scientific insights and can
help reduce the barriers to entry for underprivileged communities. To
substantiate our claims, we empirically revisit the paper which introduced the
Rainbow algorithm [Hessel et al., 2018] and present some new insights into the
algorithms used by Rainbow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Obando_Ceron_J/0/1/0/all/0/1"&gt;Johan S. Obando-Ceron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1"&gt;Pablo Samuel Castro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ESAD: End-to-end Deep Semi-supervised Anomaly Detection. (arXiv:2012.04905v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04905</id>
        <link href="http://arxiv.org/abs/2012.04905"/>
        <updated>2021-05-25T01:56:10.952Z</updated>
        <summary type="html"><![CDATA[This paper explores semi-supervised anomaly detection, a more practical
setting for anomaly detection where a small additional set of labeled samples
are provided. Based on the analysis of Deep SAD, the state-of-the-art for
semi-supervised anomaly detection, we propose a new KL-divergence based
objective function and show that two factors: the mutual information between
the data and latent representations, and the entropy of latent representations,
constitute an integral objective function for anomaly detection. To resolve the
contradiction in simultaneously optimizing the two factors, we propose a novel
encoder-decoder-encoder structure, with the first encoder focusing on
optimizing the mutual information and the second encoder focusing on optimizing
the entropy. The two encoders are enforced to share similar encoding with a
consistent constraint on their latent representations. Extensive experiments
have revealed that the proposed method significantly outperforms several
state-of-the-arts on multiple benchmark datasets, including medical diagnosis
and several classic anomaly detection benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chaoqin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan-Feng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic region proposal networks for semantic segmentation in automated glaucoma screening. (arXiv:2105.11364v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11364</id>
        <link href="http://arxiv.org/abs/2105.11364"/>
        <updated>2021-05-25T01:56:10.948Z</updated>
        <summary type="html"><![CDATA[Screening for the diagnosis of glaucoma through a fundus image can be
determined by the optic cup to disc diameter ratio (CDR), which requires the
segmentation of the cup and disc regions. In this paper, we propose two novel
approaches, namely Parameter-Shared Branched Network (PSBN) andWeak Region of
Interest Model-based segmentation (WRoIM) to identify disc and cup boundaries.
Unlike the previous approaches, the proposed methods are trained end-to-end
through a single neural network architecture and use dynamic cropping instead
of manual or traditional computer vision-based cropping. We are able to achieve
similar performance as that of state-of-the-art approaches with less number of
network parameters. Our experiments include comparison with different best
known methods on publicly available Drishti-GS1 and RIM-ONE v3 datasets. With
$7.8 \times 10^6$ parameters our approach achieves a Dice score of 0.96/0.89
for disc/cup segmentation on Drishti-GS1 data whereas the existing
state-of-the-art approach uses $19.8\times 10^6$ parameters to achieve a dice
score of 0.97/0.89.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Shivam Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasukurthi_N/0/1/0/all/0/1"&gt;Nikhil Kasukurthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pande_H/0/1/0/all/0/1"&gt;Harshit Pande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Directed Graph Embedding. (arXiv:2008.03667v3 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03667</id>
        <link href="http://arxiv.org/abs/2008.03667"/>
        <updated>2021-05-25T01:56:10.948Z</updated>
        <summary type="html"><![CDATA[Node representation learning for directed graphs is critically important to
facilitate many graph mining tasks. To capture the directed edges between
nodes, existing methods mostly learn two embedding vectors for each node,
source vector and target vector. However, these methods learn the source and
target vectors separately. For the node with very low indegree or outdegree,
the corresponding target vector or source vector cannot be effectively learned.
In this paper, we propose a novel Directed Graph embedding framework based on
Generative Adversarial Network, called DGGAN. The main idea is to use
adversarial mechanisms to deploy a discriminator and two generators that
jointly learn each node's source and target vectors. For a given node, the two
generators are trained to generate its fake target and source neighbor nodes
from the same underlying distribution, and the discriminator aims to
distinguish whether a neighbor node is real or fake. The two generators are
formulated into a unified framework and could mutually reinforce each other to
learn more robust source and target vectors. Extensive experiments show that
DGGAN consistently and significantly outperforms existing state-of-the-art
methods across multiple graph mining tasks on directed graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shijie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Senzhang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean-field Behaviour of Neural Tangent Kernel for Deep Neural Networks. (arXiv:1905.13654v10 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.13654</id>
        <link href="http://arxiv.org/abs/1905.13654"/>
        <updated>2021-05-25T01:56:10.947Z</updated>
        <summary type="html"><![CDATA[Recent work by Jacot et al. (2018) has shown that training a neural network
of any kind with gradient descent in parameter space is strongly related to
kernel gradient descent in function space with respect to the Neural Tangent
Kernel (NTK). Lee et al. (2019) built on this result by establishing that the
output of a neural network trained using gradient descent can be approximated
by a linear model for wide networks. In parallel, a recent line of studies
(Schoenholz et al. 2017; Hayou et al. 2019) has suggested that a special
initialization, known as the Edge of Chaos, improves training. In this paper,
we bridge the gap between these two concepts by quantifying the impact of the
initialization and the activation function on the NTK when the network depth
becomes large. In particular, we show that the performance of wide deep neural
networks cannot be explained by the NTK regime and we provide experiments
illustrating our theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1"&gt;Soufiane Hayou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rousseau_J/0/1/0/all/0/1"&gt;Judith Rousseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selection of Proper EEG Channels for Subject Intention Classification Using Deep Learning. (arXiv:2007.12764v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.12764</id>
        <link href="http://arxiv.org/abs/2007.12764"/>
        <updated>2021-05-25T01:56:10.916Z</updated>
        <summary type="html"><![CDATA[Brain signals could be used to control devices to assist individuals with
disabilities. Signals such as electroencephalograms are complicated and hard to
interpret. A set of signals are collected and should be classified to identify
the intention of the subject. Different approaches have tried to reduce the
number of channels before sending them to a classifier. We are proposing a deep
learning-based method for selecting an informative subset of channels that
produce high classification accuracy. The proposed network could be trained for
an individual subject for the selection of an appropriate set of channels.
Reduction of the number of channels could reduce the complexity of
brain-computer-interface devices. Our method could find a subset of channels.
The accuracy of our approach is comparable with a model trained on all
channels. Hence, our model's temporal and power costs are low, while its
accuracy is kept high.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ghorbanzade_G/0/1/0/all/0/1"&gt;Ghazale Ghorbanzade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nabizadeh_ShahreBabak_Z/0/1/0/all/0/1"&gt;Zahra Nabizadeh-ShahreBabak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Samavi_S/0/1/0/all/0/1"&gt;Shadrokh Samavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karimi_N/0/1/0/all/0/1"&gt;Nader Karimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Emami_A/0/1/0/all/0/1"&gt;Ali Emami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khadivi_P/0/1/0/all/0/1"&gt;Pejman Khadivi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Text Edition by Changing Answers of Specific Questions. (arXiv:2105.11018v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11018</id>
        <link href="http://arxiv.org/abs/2105.11018"/>
        <updated>2021-05-25T01:56:10.908Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce the new task of controllable text edition, in
which we take as input a long text, a question, and a target answer, and the
output is a minimally modified text, so that it fits the target answer. This
task is very important in many situations, such as changing some conditions,
consequences, or properties in a legal document, or changing some key
information of an event in a news text. This is very challenging, as it is hard
to obtain a parallel corpus for training, and we need to first find all text
positions that should be changed and then decide how to change them. We
constructed the new dataset WikiBioCTE for this task based on the existing
dataset WikiBio (originally created for table-to-text generation). We use
WikiBioCTE for training, and manually labeled a test set for testing. We also
propose novel evaluation metrics and a novel method for solving the new task.
Experimental results on the test set show that our proposed method is a good
fit for this novel NLP task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1"&gt;Lei Sha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hohenecker_P/0/1/0/all/0/1"&gt;Patrick Hohenecker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment. (arXiv:2005.05005v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05005</id>
        <link href="http://arxiv.org/abs/2005.05005"/>
        <updated>2021-05-25T01:56:10.900Z</updated>
        <summary type="html"><![CDATA[Existing face restoration researches typically relies on either the
degradation prior or explicit guidance labels for training, which often results
in limited generalization ability over real-world images with heterogeneous
degradations and rich background contents. In this paper, we investigate the
more challenging and practical "dual-blind" version of the problem by lifting
the requirements on both types of prior, termed as "Face Renovation"(FR).
Specifically, we formulated FR as a semantic-guided generation problem and
tackle it with a collaborative suppression and replenishment (CSR) approach.
This leads to HiFaceGAN, a multi-stage framework containing several nested CSR
units that progressively replenish facial details based on the hierarchical
semantic guidance extracted from the front-end content-adaptive suppression
modules. Extensive experiments on both synthetic and real face images have
verified the superior performance of HiFaceGAN over a wide range of challenging
restoration subtasks, demonstrating its versatility, robustness and
generalization ability towards real-world face processing applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lingbo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shanshe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Peiran Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Siwei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning with a Stage Incentive Mechanism of Dense Reward for Robotic Trajectory Planning. (arXiv:2009.12068v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12068</id>
        <link href="http://arxiv.org/abs/2009.12068"/>
        <updated>2021-05-25T01:56:10.839Z</updated>
        <summary type="html"><![CDATA[(This work has been submitted to the IEEE for possible publication. Copyright
may be transferred without notice, after which this version may no longer be
accessible.)

To improve the efficiency of deep reinforcement learning (DRL)-based methods
for robot manipulator trajectory planning in random working environments, we
present three dense reward functions. These rewards differ from the traditional
sparse reward. First, a posture reward function is proposed to speed up the
learning process with a more reasonable trajectory by modeling the distance and
direction constraints, which can reduce the blindness of exploration. Second, a
stride reward function is proposed to improve the stability of the learning
process by modeling the distance and movement distance of joint constraints.
Finally, in order to further improve learning efficiency, we are inspired by
the cognitive process of human behavior and propose a stage incentive
mechanism, including a hard stage incentive reward function and a soft stage
incentive reward function. Extensive experiments show that the soft stage
incentive reward function is able to improve the convergence rate by up to
46.9% with the state-of-the-art DRL methods. The percentage increase in the
convergence mean reward was 4.4-15.5% and the percentage decreases with respect
to standard deviation were 21.9-63.2%. In the evaluation experiments, the
success rate of trajectory planning for a robot manipulator reached 99.6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1"&gt;Gang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lia_X/0/1/0/all/0/1"&gt;Xinde Lia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khyam_M/0/1/0/all/0/1"&gt;Mohammad Omar Khyam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Convolutional Network for Removing DCT Artefacts From Images. (arXiv:1907.03798v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.03798</id>
        <link href="http://arxiv.org/abs/1907.03798"/>
        <updated>2021-05-25T01:56:10.832Z</updated>
        <summary type="html"><![CDATA[Image compression is one of the essential methods of image processing. Its
most prominent advantage is the significant reduction of image size allowing
for more efficient storage and transfer. However, lossy compression is
associated with the loss of some image details in favor of reducing its size.
In compressed images, the deficiencies are manifested by noticeable defects in
the form of artifacts; the most common are block artifacts, ringing effect, or
blur. In this article, we propose three models of fully convolutional networks
with different configurations and examine their abilities in reducing
compression artifacts. In the experiments, we research the extent to which the
results are improved for models that will process the image in a similar way to
the compression algorithm, and whether the initialization with predefined
filters would allow for better image reconstruction than developed solely
during learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Najgebauer_P/0/1/0/all/0/1"&gt;Patryk Najgebauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scherer_R/0/1/0/all/0/1"&gt;Rafal Scherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rutkowski_L/0/1/0/all/0/1"&gt;Leszek Rutkowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DDR-Net: Dividing and Downsampling Mixed Network for Diffeomorphic Image Registration. (arXiv:2105.11361v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11361</id>
        <link href="http://arxiv.org/abs/2105.11361"/>
        <updated>2021-05-25T01:56:10.812Z</updated>
        <summary type="html"><![CDATA[Deep diffeomorphic registration faces significant challenges for
high-dimensional images, especially in terms of memory limits. Existing
approaches either downsample original images, or approximate underlying
transformations, or reduce model size. The information loss during the
approximation or insufficient model capacity is a hindrance to the registration
accuracy for high-dimensional images, e.g., 3D medical volumes. In this paper,
we propose a Dividing and Downsampling mixed Registration network (DDR-Net), a
general architecture that preserves most of the image information at multiple
scales. DDR-Net leverages the global context via downsampling the input and
utilizes the local details from divided chunks of the input images. This design
reduces the network input size and its memory cost; meanwhile, by fusing global
and local information, DDR-Net obtains both coarse-level and fine-level
alignments in the final deformation fields. We evaluate DDR-Net on three public
datasets, i.e., OASIS, IBSR18, and 3DIRCADB-01, and the experimental results
demonstrate our approach outperforms existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1"&gt;Ankita Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEFT: Distilling Entangled Factors by Preventing Information Diffusion. (arXiv:2102.03986v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03986</id>
        <link href="http://arxiv.org/abs/2102.03986"/>
        <updated>2021-05-25T01:56:10.697Z</updated>
        <summary type="html"><![CDATA[Disentanglement is a highly desirable property of representation owing to its
similarity to human understanding and reasoning. Many works achieve
disentanglement upon information bottlenecks (IB). Despite their elegant
mathematical foundations, the IB branch usually exhibits lower performance. In
order to provide an insight into the problem, we develop an annealing test to
calculate the information freezing point (IFP), which is a transition state to
freeze information into the latent variables. We also explore these clues or
inductive biases for separating the entangled factors according to the
differences in the IFP distributions. We found the existing approaches suffer
from the information diffusion problem, according to which the increased
information diffuses in all latent variables.

Based on this insight, we propose a novel disentanglement framework, termed
the distilling entangled factor (DEFT), to address the information diffusion
problem by scaling backward information. DEFT applies a multistage training
strategy, including multigroup encoders with different learning rates and
piecewise disentanglement pressure, to disentangle the factors stage by stage.
We evaluate DEFT on three variants of dSprite and SmallNORB, which show
low-variance and high-level disentanglement scores. Furthermore, the experiment
under the correlative factors shows incapable of TC-based approaches. DEFT also
exhibits a competitive performance in the unsupervised setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiantao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fanqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chunxiuzi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A hybrid quantum-classical neural network with deep residual learning. (arXiv:2012.07772v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07772</id>
        <link href="http://arxiv.org/abs/2012.07772"/>
        <updated>2021-05-25T01:56:10.685Z</updated>
        <summary type="html"><![CDATA[Inspired by the success of classical neural networks, there has been
tremendous effort to develop classical effective neural networks into quantum
concept. In this paper, a novel hybrid quantum-classical neural network with
deep residual learning (Res-HQCNN) is proposed. We firstly analysis how to
connect residual block structure with a quantum neural network, and give the
corresponding training algorithm. At the same time, the advantages and
disadvantages of transforming deep residual learning into quantum concept are
provided. As a result, the model can be trained in an end-to-end fashion,
analogue to the backpropagation in classical neural networks.

To explore the effectiveness of Res-HQCNN , we perform extensive experiments
for quantum data with or without noisy on classical computer. The experimental
results show the Res-HQCNN performs better to learn an unknown unitary
transformation and has stronger robustness for noisy data, when compared to
state of the arts. Moreover, the possible methods of combining residual
learning with quantum neural networks are also discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yanying Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhu-Jun Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silven_O/0/1/0/all/0/1"&gt;Olli Silv&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guoying Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion with Ensemble Monte Carlo Dropout for COVID-19 Detection. (arXiv:2105.08590v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08590</id>
        <link href="http://arxiv.org/abs/2105.08590"/>
        <updated>2021-05-25T01:56:10.675Z</updated>
        <summary type="html"><![CDATA[The COVID-19 (Coronavirus disease 2019) has infected more than 151 million
people and caused approximately 3.17 million deaths around the world up to the
present. The rapid spread of COVID-19 is continuing to threaten human's life
and health. Therefore, the development of computer-aided detection (CAD)
systems based on machine and deep learning methods which are able to accurately
differentiate COVID-19 from other diseases using chest computed tomography (CT)
and X-Ray datasets is essential and of immediate priority. Different from most
of the previous studies which used either one of CT or X-ray images, we
employed both data types with sufficient samples in implementation. On the
other hand, due to the extreme sensitivity of this pervasive virus, model
uncertainty should be considered, while most previous studies have overlooked
it. Therefore, we propose a novel powerful fusion model named
$UncertaintyFuseNet$ that consists of an uncertainty module: Ensemble Monte
Carlo (EMC) dropout. The obtained results prove the effectiveness of our
proposed fusion for COVID-19 detection using CT scan and X-Ray datasets. Also,
our proposed $UncertaintyFuseNet$ model is significantly robust to noise and
performs well with the previously unseen data. The source codes and models of
this study are available at:
https://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Abdar_M/0/1/0/all/0/1"&gt;Moloud Abdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salari_S/0/1/0/all/0/1"&gt;Soorena Salari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qahremani_S/0/1/0/all/0/1"&gt;Sina Qahremani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lam_H/0/1/0/all/0/1"&gt;Hak-Keung Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1"&gt;Sadiq Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1"&gt;U. Rajendra Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Detection of Practical Universal Adversarial Perturbations. (arXiv:2105.07334v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07334</id>
        <link href="http://arxiv.org/abs/2105.07334"/>
        <updated>2021-05-25T01:56:10.668Z</updated>
        <summary type="html"><![CDATA[Universal Adversarial Perturbations (UAPs) are a prominent class of
adversarial examples that exploit the systemic vulnerabilities and enable
physically realizable and robust attacks against Deep Neural Networks (DNNs).
UAPs generalize across many different inputs; this leads to realistic and
effective attacks that can be applied at scale. In this paper we propose
HyperNeuron, an efficient and scalable algorithm that allows for the real-time
detection of UAPs by identifying suspicious neuron hyper-activations. Our
results show the effectiveness of HyperNeuron on multiple tasks (image
classification, object detection), against a wide variety of universal attacks,
and in realistic scenarios, like perceptual ad-blocking and adversarial
patches. HyperNeuron is able to simultaneously detect both adversarial mask and
patch UAPs with comparable or better performance than existing UAP defenses
whilst introducing a significantly reduced latency of only 0.86 milliseconds
per image. This suggests that many realistic and practical universal attacks
can be reliably mitigated in real-time, which shows promise for the robust
deployment of machine learning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Co_K/0/1/0/all/0/1"&gt;Kenneth T. Co&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munoz_Gonzalez_L/0/1/0/all/0/1"&gt;Luis Mu&amp;#xf1;oz-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanthan_L/0/1/0/all/0/1"&gt;Leslie Kanthan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1"&gt;Emil C. Lupu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PEMNET: A Transfer Learning-based Modeling Approach of High-Temperature Polymer Electrolyte Membrane Electrochemical Systems. (arXiv:2105.03057v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03057</id>
        <link href="http://arxiv.org/abs/2105.03057"/>
        <updated>2021-05-25T01:56:10.649Z</updated>
        <summary type="html"><![CDATA[Widespread adoption of high-temperature polymer electrolyte membrane fuel
cells (HT-PEMFCs) and HT-PEM electrochemical hydrogen pumps (HT-PEM ECHPs)
requires models and computational tools that provide accurate scale-up and
optimization. Knowledge-based modeling has limitations as it is time consuming
and requires information about the system that is not always available (e.g.,
material properties and interfacial behavior between different materials).
Data-driven modeling on the other hand, is easier to implement, but often
necessitates large datasets that could be difficult to obtain. In this
contribution, knowledge-based modeling and data-driven modeling are uniquely
combined by implementing a Few-Shot Learning (FSL) approach. A knowledge-based
model originally developed for a HT-PEMFC was used to generate simulated data
(887,735 points) and used to pretrain a neural network source model.
Furthermore, the source model developed for HT-PEMFCs was successfully applied
to HT-PEM ECHPs - a different electrochemical system that utilizes similar
materials to the fuel cell. Experimental datasets from both HT-PEMFCs and
HT-PEM ECHPs with different materials and operating conditions (~50 points
each) were used to train 8 target models via FSL. Models for the unseen data
reached high accuracies in all cases (rRMSE between 1.04 and 3.73% for HT-PEMCs
and between 6.38 and 8.46% for HT-PEM ECHPs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Briceno_Mena_L/0/1/0/all/0/1"&gt;Luis A. Briceno-Mena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arges_C/0/1/0/all/0/1"&gt;Christopher G. Arges&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romagnoli_J/0/1/0/all/0/1"&gt;Jose A. Romagnoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding. (arXiv:2105.07688v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07688</id>
        <link href="http://arxiv.org/abs/2105.07688"/>
        <updated>2021-05-25T01:56:10.642Z</updated>
        <summary type="html"><![CDATA[Semantic embedding has been widely investigated for aligning knowledge graph
(KG) entities. Current methods have explored and utilized the graph structure,
the entity names and attributes, but ignore the ontology (or ontological
schema) which contains critical meta information such as classes and their
membership relationships with entities. In this paper, we propose an
ontology-guided entity alignment method named OntoEA, where both KGs and their
ontologies are jointly embedded, and the class hierarchy and the class
disjointness are utilized to avoid false mappings. Extensive experiments on
seven public and industrial benchmarks have demonstrated the state-of-the-art
performance of OntoEA and the effectiveness of the ontologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yuejia Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhenxi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DFENet: A Novel Dimension Fusion Edge Guided Network for Brain MRI Segmentation. (arXiv:2105.07962v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07962</id>
        <link href="http://arxiv.org/abs/2105.07962"/>
        <updated>2021-05-25T01:56:10.631Z</updated>
        <summary type="html"><![CDATA[The rapid increment of morbidity of brain stroke in the last few years have
been a driving force towards fast and accurate segmentation of stroke lesions
from brain MRI images. With the recent development of deep-learning,
computer-aided and segmentation methods of ischemic stroke lesions have been
useful for clinicians in early diagnosis and treatment planning. However, most
of these methods suffer from inaccurate and unreliable segmentation results
because of their inability to capture sufficient contextual features from the
MRI volumes. To meet these requirements, 3D convolutional neural networks have
been proposed, which, however, suffer from huge computational requirements. To
mitigate these problems, we propose a novel Dimension Fusion Edge-guided
network (DFENet) that can meet both of these requirements by fusing the
features of 2D and 3D CNNs. Unlike other methods, our proposed network uses a
parallel partial decoder (PPD) module for aggregating and upsampling selected
features, rich in important contextual information. Additionally, we use an
edge-guidance and enhanced mixing loss for constantly supervising and
improvising the learning process of the network. The proposed method is
evaluated on publicly available Anatomical Tracings of Lesions After Stroke
(ATLAS) dataset, resulting in mean DSC, IoU, Precision and Recall values of
0.5457, 0.4015, 0.6371, and 0.4969 respectively. The results, when compared to
other state-of-the-art methods, outperforms them by a significant margin.
Therefore, the proposed model is robust, accurate, superior to the existing
methods, and can be relied upon for biomedical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Basak_H/0/1/0/all/0/1"&gt;Hritam Basak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hussain_R/0/1/0/all/0/1"&gt;Rukhshanda Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rana_A/0/1/0/all/0/1"&gt;Ajay Rana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Facial Expression Recognition with Convolutional Visual Transformers. (arXiv:2103.16854v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16854</id>
        <link href="http://arxiv.org/abs/2103.16854"/>
        <updated>2021-05-25T01:56:10.623Z</updated>
        <summary type="html"><![CDATA[Facial Expression Recognition (FER) in the wild is extremely challenging due
to occlusions, variant head poses, face deformation and motion blur under
unconstrained conditions. Although substantial progresses have been made in
automatic FER in the past few decades, previous studies are mainly designed for
lab-controlled FER. Real-world occlusions, variant head poses and other issues
definitely increase the difficulty of FER on account of these
information-deficient regions and complex backgrounds. Different from previous
pure CNNs based methods, we argue that it is feasible and practical to
translate facial images into sequences of visual words and perform expression
recognition from a global perspective. Therefore, we propose Convolutional
Visual Transformers to tackle FER in the wild by two main steps. First, we
propose an attentional selective fusion (ASF) for leveraging the feature maps
generated by two-branch CNNs. The ASF captures discriminative information by
fusing multiple features with global-local attention. The fused feature maps
are then flattened and projected into sequences of visual words. Second,
inspired by the success of Transformers in natural language processing, we
propose to model relationships between these visual words with global
self-attention. The proposed method are evaluated on three public in-the-wild
facial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same
settings, extensive experiments demonstrate that our method shows superior
performance over other methods, setting new state of the art on RAF-DB with
88.14%, FERPlus with 88.81% and AffectNet with 61.85%. We also conduct
cross-dataset evaluation on CK+ show the generalization capability of the
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fuyan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Bin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shutao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mapping oil palm density at country scale: An active learning approach. (arXiv:2105.11207v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11207</id>
        <link href="http://arxiv.org/abs/2105.11207"/>
        <updated>2021-05-25T01:56:10.571Z</updated>
        <summary type="html"><![CDATA[Accurate mapping of oil palm is important for understanding its past and
future impact on the environment. We propose to map and count oil palms by
estimating tree densities per pixel for large-scale analysis. This allows for
fine-grained analysis, for example regarding different planting patterns. To
that end, we propose a new, active deep learning method to estimate oil palm
density at large scale from Sentinel-2 satellite images, and apply it to
generate complete maps for Malaysia and Indonesia. What makes the regression of
oil palm density challenging is the need for representative reference data that
covers all relevant geographical conditions across a large territory.
Specifically for density estimation, generating reference data involves
counting individual trees. To keep the associated labelling effort low we
propose an active learning (AL) approach that automatically chooses the most
relevant samples to be labelled. Our method relies on estimates of the
epistemic model uncertainty and of the diversity among samples, making it
possible to retrieve an entire batch of relevant samples in a single iteration.
Moreover, our algorithm has linear computational complexity and is easily
parallelisable to cover large areas. We use our method to compute the first oil
palm density map with $10\,$m Ground Sampling Distance (GSD) , for all of
Indonesia and Malaysia and for two different years, 2017 and 2019. The maps
have a mean absolute error of $\pm$7.3 trees/$ha$, estimated from an
independent validation set. We also analyse density variations between
different states within a country and compare them to official estimates.
According to our estimates there are, in total, $>1.2$ billion oil palms in
Indonesia covering $>$15 million $ha$, and $>0.5$ billion oil palms in Malaysia
covering $>6$ million $ha$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s C. Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1"&gt;Stefano D&amp;#x27;Aronco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;Jan D.Wegner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic and Geometric Modeling with Neural Message Passing in 3D Scene Graphs for Hierarchical Mechanical Search. (arXiv:2012.04060v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04060</id>
        <link href="http://arxiv.org/abs/2012.04060"/>
        <updated>2021-05-25T01:56:10.550Z</updated>
        <summary type="html"><![CDATA[Searching for objects in indoor organized environments such as homes or
offices is part of our everyday activities. When looking for a target object,
we jointly reason about the rooms and containers the object is likely to be in;
the same type of container will have a different probability of having the
target depending on the room it is in. We also combine geometric and semantic
information to infer what container is best to search, or what other objects
are best to move, if the target object is hidden from view. We propose to use a
3D scene graph representation to capture the hierarchical, semantic, and
geometric aspects of this problem. To exploit this representation in a search
process, we introduce Hierarchical Mechanical Search (HMS), a method that
guides an agent's actions towards finding a target object specified with a
natural language description. HMS is based on a novel neural network
architecture that uses neural message passing of vectors with visual,
geometric, and linguistic information to allow HMS to reason across layers of
the graph while combining semantic and geometric cues. HMS is evaluated on a
novel dataset of 500 3D scene graphs with dense placements of semantically
related objects in storage locations, and is shown to be significantly better
than several baselines at finding objects and close to the oracle policy in
terms of the median number of actions required. Additional qualitative results
can be found at https://ai.stanford.edu/mech-search/hms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1"&gt;Andrey Kurenkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1"&gt;Jeff Ichnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Audio Representation Learning for Modeling Beehive Strengths. (arXiv:2105.10536v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.10536</id>
        <link href="http://arxiv.org/abs/2105.10536"/>
        <updated>2021-05-25T01:56:10.537Z</updated>
        <summary type="html"><![CDATA[Honey bees are critical to our ecosystem and food security as a pollinator,
contributing 35% of our global agriculture yield. In spite of their importance,
beekeeping is exclusively dependent on human labor and experience-derived
heuristics, while requiring frequent human checkups to ensure the colony is
healthy, which can disrupt the colony. Increasingly, pollinator populations are
declining due to threats from climate change, pests, environmental toxicity,
making their management even more critical than ever before in order to ensure
sustained global food security. To start addressing this pressing challenge, we
developed an integrated hardware sensing system for beehive monitoring through
audio and environment measurements, and a hierarchical semi-supervised deep
learning model, composed of an audio modeling module and a predictor, to model
the strength of beehives. The model is trained jointly on audio reconstruction
and prediction losses based on human inspections, in order to model both
low-level audio features and circadian temporal dynamics. We show that this
model performs well despite limited labels, and can learn an audio embedding
that is useful for characterizing different sound profiles of beehives. This is
the first instance to our knowledge of applying audio-based deep learning to
model beehives and population size in an observational setting across a large
number of hives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tony Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zmyslony_S/0/1/0/all/0/1"&gt;Szymon Zmyslony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nozdrenkov_S/0/1/0/all/0/1"&gt;Sergei Nozdrenkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1"&gt;Matthew Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hopkins_B/0/1/0/all/0/1"&gt;Brandon Hopkins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Kinetic Modeling of Biomass Pyrolysis using Chemical Reaction Neural Networks. (arXiv:2105.11397v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2105.11397</id>
        <link href="http://arxiv.org/abs/2105.11397"/>
        <updated>2021-05-25T01:56:10.529Z</updated>
        <summary type="html"><![CDATA[Modeling the burning processes of biomass such as wood, grass, and crops is
crucial for the modeling and prediction of wildland and urban fire behavior.
Despite its importance, the burning of solid fuels remains poorly understood,
which can be partly attributed to the unknown chemical kinetics of most solid
fuels. Most available kinetic models were built upon expert knowledge, which
requires chemical insights and years of experience. This work presents a
framework for autonomously discovering biomass pyrolysis kinetic models from
thermogravimetric analyzer (TGA) experimental data using the recently developed
chemical reaction neural networks (CRNN). The approach incorporated the CRNN
model into the framework of neural ordinary differential equations to predict
the residual mass in TGA data. In addition to the flexibility of
neural-network-based models, the learned CRNN model is fully interpretable, by
incorporating the fundamental physics laws, such as the law of mass action and
Arrhenius law, into the neural network structure. The learned CRNN model can
then be translated into the classical forms of biomass chemical kinetic models,
which facilitates the extraction of chemical insights and the integration of
the kinetic model into large-scale fire simulations. We demonstrated the
effectiveness of the framework in predicting the pyrolysis and oxidation of
cellulose. This successful demonstration opens the possibility of rapid and
autonomous chemical kinetic modeling of solid fuels, such as wildfire fuels and
industrial polymers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Ji_W/0/1/0/all/0/1"&gt;Weiqi Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Richter_F/0/1/0/all/0/1"&gt;Franz Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gollner_M/0/1/0/all/0/1"&gt;Michael J. Gollner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Deng_S/0/1/0/all/0/1"&gt;Sili Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-guided Temporal Coherent Video Object Matting. (arXiv:2105.11427v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11427</id>
        <link href="http://arxiv.org/abs/2105.11427"/>
        <updated>2021-05-25T01:56:10.521Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel deep learning-based video object matting method
that can achieve temporally coherent matting results. Its key component is an
attention-based temporal aggregation module that maximizes image matting
networks' strength for video matting networks. This module computes temporal
correlations for pixels adjacent to each other along the time axis in feature
space to be robust against motion noises. We also design a novel loss term to
train the attention weights, which drastically boosts the video matting
performance. Besides, we show how to effectively solve the trimap generation
problem by fine-tuning a state-of-the-art video object segmentation network
with a sparse set of user-annotated keyframes. To facilitate video matting and
trimap generation networks' training, we construct a large-scale video matting
dataset with 80 training and 28 validation foreground video clips with
ground-truth alpha mattes. Experimental results show that our method can
generate high-quality alpha mattes for various videos featuring appearance
change, occlusion, and fast motion. Our code and dataset can be found at
https://github.com/yunkezhang/TCVOM]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunke Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1"&gt;Miaomiao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Peiran Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xuansong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-sheng Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hujun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qixing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weiwei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Graph Representation Learning with Relation Awareness. (arXiv:2105.11122v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11122</id>
        <link href="http://arxiv.org/abs/2105.11122"/>
        <updated>2021-05-25T01:56:10.410Z</updated>
        <summary type="html"><![CDATA[Representation learning on heterogeneous graphs aims to obtain meaningful
node representations to facilitate various downstream tasks, such as node
classification and link prediction. Existing heterogeneous graph learning
methods are primarily developed by following the propagation mechanism of node
representations. There are few efforts on studying the role of relations for
improving the learning of more fine-grained node representations. Indeed, it is
important to collaboratively learn the semantic representations of relations
and discern node representations with respect to different relation types. To
this end, in this paper, we propose a novel Relation-aware Heterogeneous Graph
Neural Network, namely R-HGNN, to learn node representations on heterogeneous
graphs at a fine-grained level by considering relation-aware characteristics.
Specifically, a dedicated graph convolution component is first designed to
learn unique node representations from each relation-specific graph separately.
Then, a cross-relation message passing module is developed to improve the
interactions of node representations across different relations. Also, the
relation representations are learned in a layer-wise manner to capture relation
semantics, which are used to guide the node representation learning process.
Moreover, a semantic fusing module is presented to aggregate relation-aware
node representations into a compact representation with the learned relation
representations. Finally, we conduct extensive experiments on a variety of
graph learning tasks, and experimental results demonstrate that our approach
consistently outperforms existing methods among all the tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Le Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Leilei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bowen Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chuanren Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1"&gt;Weifeng Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation. (arXiv:2102.04525v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04525</id>
        <link href="http://arxiv.org/abs/2102.04525"/>
        <updated>2021-05-25T01:56:10.386Z</updated>
        <summary type="html"><![CDATA[Automatic segmentation methods are an important advancement in medical image
analysis. Machine learning techniques, and deep neural networks in particular,
are the state-of-the-art for most medical image segmentation tasks. Issues with
class imbalance pose a significant challenge in medical datasets, with lesions
often occupying a considerably smaller volume relative to the background. Loss
functions used in the training of deep learning algorithms differ in their
robustness to class imbalance, with direct consequences for model convergence.
The most commonly used loss functions for segmentation are based on either the
cross entropy loss, Dice loss or a combination of the two. We propose a Unified
Focal loss, a new framework that generalises Dice and cross entropy-based
losses for handling class imbalance. We evaluate our proposed loss function on
three highly class imbalanced, publicly available medical imaging datasets:
Breast Ultrasound 2017 (BUS2017), Brain Tumour Segmentation 2020 (BraTS20) and
Kidney Tumour Segmentation 2019 (KiTS19). We compare our loss function
performance against six Dice or cross entropy-based loss functions, and
demonstrate that our proposed loss function is robust to class imbalance,
outperforming the other loss functions across datasets. Finally, we use the
Unified Focal loss together with deep supervision to achieve state-of-the-art
results without modification of the original U-Net architecture, with a mean
Dice similarity coefficient (DSC)=0.948 on BUS2017, enhancing tumour region
DSC=0.800 on BraTS20 and kidney tumour DSC=0.758 on KiTS19. This highlights the
importance of carefully selecting a suitable loss function prior to the use of
more complex architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1"&gt;Michael Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1"&gt;Evis Sala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1"&gt;Leonardo Rundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prostate Gland Segmentation in Histology Images via Residual and Multi-Resolution U-Net. (arXiv:2105.10556v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10556</id>
        <link href="http://arxiv.org/abs/2105.10556"/>
        <updated>2021-05-25T01:56:10.378Z</updated>
        <summary type="html"><![CDATA[Prostate cancer is one of the most prevalent cancers worldwide. One of the
key factors in reducing its mortality is based on early detection. The
computer-aided diagnosis systems for this task are based on the glandular
structural analysis in histology images. Hence, accurate gland detection and
segmentation is crucial for a successful prediction. The methodological basis
of this work is a prostate gland segmentation based on U-Net convolutional
neural network architectures modified with residual and multi-resolution
blocks, trained using data augmentation techniques. The residual configuration
outperforms in the test subset the previous state-of-the-art approaches in an
image-level comparison, reaching an average Dice Index of 0.77.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1"&gt;Julio Silva-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Paya_Bosch_E/0/1/0/all/0/1"&gt;Elena Pay&amp;#xe1;-Bosch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1"&gt;Gabriel Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Distribution Detection in Dermatology using Input Perturbation and Subset Scanning. (arXiv:2105.11160v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11160</id>
        <link href="http://arxiv.org/abs/2105.11160"/>
        <updated>2021-05-25T01:56:10.353Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have led to breakthroughs in the development
of automated skin disease classification. As we observe an increasing interest
in these models in the dermatology space, it is crucial to address aspects such
as the robustness towards input data distribution shifts. Current skin disease
models could make incorrect inferences for test samples from different hardware
devices and clinical settings or unknown disease samples, which are
out-of-distribution (OOD) from the training samples.To this end, we propose a
simple yet effective approach that detect these OOD samples prior to making any
decision. The detection is performed via scanning in the latent space
representation (e.g., activations of the inner layers of any pre-trained skin
disease classifier). The input samples could also perturbed to maximise
divergence of OOD samples. We validate our ODD detection approach in two use
cases: 1) identify samples collected from different protocols, and 2) detect
samples from unknown disease classes. Additionally, we evaluate the performance
of the proposed approach and compare it with other state-of-the-art methods.
Furthermore, data-driven dermatology applications may deepen the disparity in
clinical care across racial and ethnic groups since most datasets are reported
to suffer from bias in skin tone distribution. Therefore, we also evaluate the
fairness of these OOD detection methods across different skin tones. Our
experiments resulted in competitive performance across multiple datasets in
detecting OOD samples, which could be used (in the future) to design more
effective transfer learning techniques prior to inferring on these samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hannah Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1"&gt;Girmaw Abebe Tadesse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1"&gt;Celia Cintas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1"&gt;Skyler Speakman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1"&gt;Kush Varshney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FineAction: A Fined Video Dataset for Temporal Action Localization. (arXiv:2105.11107v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11107</id>
        <link href="http://arxiv.org/abs/2105.11107"/>
        <updated>2021-05-25T01:56:10.346Z</updated>
        <summary type="html"><![CDATA[On the existing benchmark datasets, THUMOS14 and ActivityNet, temporal action
localization techniques have achieved great success. However, there are still
existing some problems, such as the source of the action is too single, there
are only sports categories in THUMOS14, coarse instances with uncertain
boundaries in ActivityNet and HACS Segments interfering with proposal
generation and behavior prediction. To take temporal action localization to a
new level, we develop FineAction, a new large-scale fined video dataset
collected from existing video datasets and web videos. Overall, this dataset
contains 139K fined action instances densely annotated in almost 17K untrimmed
videos spanning 106 action categories. FineAction has a more fined definition
of action categories and high-quality annotations to reduce the boundary
uncertainty compared to the existing action localization datasets. We
systematically investigate representative methods of temporal action
localization on our dataset and obtain some interesting findings with further
analysis. Experimental results reveal that our FineAction brings new challenges
for action localization on fined and multi-label instances with shorter
duration. This dataset will be public in the future and we hope our FineAction
could advance research towards temporal action localization. Our dataset
website is at https://deeperaction.github.io/fineaction/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yali Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-world Ride-hailing Vehicle Repositioning using Deep Reinforcement Learning. (arXiv:2103.04555v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04555</id>
        <link href="http://arxiv.org/abs/2103.04555"/>
        <updated>2021-05-25T01:56:10.340Z</updated>
        <summary type="html"><![CDATA[We present a new practical framework based on deep reinforcement learning and
decision-time planning for real-world vehicle repositioning on ride-hailing (a
type of mobility-on-demand, MoD) platforms. Our approach learns the
spatiotemporal state-value function using a batch training algorithm with deep
value networks. The optimal repositioning action is generated on-demand through
value-based policy search, which combines planning and bootstrapping with the
value networks. For the large-fleet problems, we develop several algorithmic
features that we incorporate into our framework and that we demonstrate to
induce coordination among the algorithmically-guided vehicles. We benchmark our
algorithm with baselines in a ride-hailing simulation environment to
demonstrate its superiority in improving income efficiency meausred by
income-per-hour. We have also designed and run a real-world experiment program
with regular drivers on a major ride-hailing platform. We have observed
significantly positive results on key metrics comparing our method with
experienced drivers who performed idle-time repositioning based on their own
expertise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yan Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuaiji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jieping Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners. (arXiv:2009.02476v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.02476</id>
        <link href="http://arxiv.org/abs/2009.02476"/>
        <updated>2021-05-25T01:56:10.332Z</updated>
        <summary type="html"><![CDATA[Successful teaching requires an assumption of how the learner learns - how
the learner uses experiences from the world to update their internal states. We
investigate what expectations people have about a learner when they teach them
in an online manner using rewards and punishment. We focus on a common
reinforcement learning method, Q-learning, and examine what assumptions people
have using a behavioral experiment. To do so, we first establish a normative
standard, by formulating the problem as a machine teaching optimization
problem. To solve the machine teaching optimization problem, we use a deep
learning approximation method which simulates learners in the environment and
learns to predict how feedback affects the learner's internal states. What do
people assume about a learner's learning and discount rates when they teach
them an idealized exploration-exploitation task? In a behavioral experiment, we
find that people can teach the task to Q-learners in a relatively efficient and
effective manner when the learner uses a small value for its discounting rate
and a large value for its learning rate. However, they still are suboptimal. We
also find that providing people with real-time updates of how possible feedback
would affect the Q-learner's internal states weakly helps them teach. Our
results reveal how people teach using evaluative feedback and provide guidance
for how engineers should design machine agents in a manner that is intuitive
for people.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1"&gt;Yun-Shiuan Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuezhou Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yuzhe Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1"&gt;Mark K. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Austerweil_J/0/1/0/all/0/1"&gt;Joseph L. Austerweil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaojin Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-level camera-LiDAR fusion for 3D object detection with machine learning. (arXiv:2105.11060v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11060</id>
        <link href="http://arxiv.org/abs/2105.11060"/>
        <updated>2021-05-25T01:56:10.325Z</updated>
        <summary type="html"><![CDATA[This paper tackles the 3D object detection problem, which is of vital
importance for applications such as autonomous driving. Our framework uses a
Machine Learning (ML) pipeline on a combination of monocular camera and LiDAR
data to detect vehicles in the surrounding 3D space of a moving platform. It
uses frustum region proposals generated by State-Of-The-Art (SOTA) 2D object
detectors to segment LiDAR point clouds into point clusters which represent
potentially individual objects. We evaluate the performance of classical ML
algorithms as part of an holistic pipeline for estimating the parameters of 3D
bounding boxes which surround the vehicles around the moving platform. Our
results demonstrate an efficient and accurate inference on a validation set,
achieving an overall accuracy of 87.1%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salazar_Gomez_G/0/1/0/all/0/1"&gt;Gustavo A. Salazar-Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saavedra_Ruiz_M/0/1/0/all/0/1"&gt;Miguel A. Saavedra-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_Cano_V/0/1/0/all/0/1"&gt;Victor A. Romero-Cano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Provably Convergent Information Bottleneck Solution via ADMM. (arXiv:2102.04729v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04729</id>
        <link href="http://arxiv.org/abs/2102.04729"/>
        <updated>2021-05-25T01:56:10.318Z</updated>
        <summary type="html"><![CDATA[The Information bottleneck (IB) method enables optimizing over the trade-off
between compression of data and prediction accuracy of learned representations,
and has successfully and robustly been applied to both supervised and
unsupervised representation learning problems. However, IB has several
limitations. First, the IB problem is hard to optimize. The IB Lagrangian
$\mathcal{L}_{IB}:=I(X;Z)-\beta I(Y;Z)$ is non-convex and existing solutions
guarantee only local convergence. As a result, the obtained solutions depend on
initialization. Second, the evaluation of a solution is also a challenging
task. Conventionally, it resorts to characterizing the information plane, that
is, plotting $I(Y;Z)$ versus $I(X;Z)$ for all solutions obtained from different
initial points. Furthermore, the IB Lagrangian has phase transitions while
varying the multiplier $\beta$. At phase transitions, both $I(X;Z)$ and
$I(Y;Z)$ increase abruptly and the rate of convergence becomes significantly
slow for existing solutions. Recent works with IB adopt variational surrogate
bounds to the IB Lagrangian. Although allowing efficient optimization, how
close are these surrogates to the IB Lagrangian is not clear. In this work, we
solve the IB Lagrangian using augmented Lagrangian methods. With augmented
variables, we show that the IB objective can be solved with the alternating
direction method of multipliers (ADMM). Different from prior works, we prove
that the proposed algorithm is consistently convergent, regardless of the value
of $\beta$. Empirically, our gradient-descent-based method results in
information plane points that are comparable to those obtained through the
conventional Blahut-Arimoto-based solvers and is convergent for a wider range
of the penalty coefficient than previous ADMM solvers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Teng-Hui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gamal_A/0/1/0/all/0/1"&gt;Aly El Gamal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise-Resilient Quantum Machine Learning for Stability Assessment of Power Systems. (arXiv:2104.04855v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04855</id>
        <link href="http://arxiv.org/abs/2104.04855"/>
        <updated>2021-05-25T01:56:10.295Z</updated>
        <summary type="html"><![CDATA[Transient stability assessment (TSA) is a cornerstone for resilient
operations of today's interconnected power grids. This paper is a confluence of
quantum computing, data science and machine learning to potentially address the
power system TSA challenge. We devise a quantum TSA (qTSA) method to enable
scalable and efficient data-driven transient stability prediction for bulk
power systems, which is the first attempt to tackle the TSA issue with quantum
computing. Our contributions are three-fold: 1) A low-depth, high
expressibility quantum neural network for accurate and noise-resilient TSA; 2)
A quantum natural gradient descent algorithm for efficient qTSA training; 3) A
systematical analysis on qTSA's performance under various quantum factors. qTSA
underpins a foundation of quantum-enabled and data-driven power grid stability
analytics. It renders the intractable TSA straightforward and effortless in the
Hilbert space, and therefore provides stability information for power system
operations. Extensive experiments on quantum simulators and real quantum
computers verify the accuracy, noise-resilience, scalability and universality
of qTSA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yifan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Encoding with AutoEncoders for Weakly-supervised Anomaly Detection. (arXiv:2105.10500v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10500</id>
        <link href="http://arxiv.org/abs/2105.10500"/>
        <updated>2021-05-25T01:56:10.261Z</updated>
        <summary type="html"><![CDATA[Weakly-supervised anomaly detection aims at learning an anomaly detector from
a limited amount of labeled data and abundant unlabeled data. Recent works
build deep neural networks for anomaly detection by discriminatively mapping
the normal samples and abnormal samples to different regions in the feature
space or fitting different distributions. However, due to the limited number of
annotated anomaly samples, directly training networks with the discriminative
loss may not be sufficient. To overcome this issue, this paper proposes a novel
strategy to transform the input data into a more meaningful representation that
could be used for anomaly detection. Specifically, we leverage an autoencoder
to encode the input data and utilize three factors, hidden representation,
reconstruction residual vector, and reconstruction error, as the new
representation for the input data. This representation amounts to encode a test
sample with its projection on the training data manifold, its direction to its
projection and its distance to its projection. In addition to this encoding, we
also propose a novel network architecture to seamlessly incorporate those three
factors. From our extensive experiments, the benefits of the proposed strategy
are clearly demonstrated by its superior performance over the competitive
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yingjie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xucheng Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanru Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fanxing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Ce Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingqiao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stereo Matching Based on Visual Sensitive Information. (arXiv:2105.10831v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10831</id>
        <link href="http://arxiv.org/abs/2105.10831"/>
        <updated>2021-05-25T01:56:10.227Z</updated>
        <summary type="html"><![CDATA[The area of computer vision is one of the most discussed topics amongst many
scholars, and stereo matching is its most important sub fields. After the
parallax map is transformed into a depth map, it can be applied to many
intelligent fields. In this paper, a stereo matching algorithm based on visual
sensitive information is proposed by using standard images from Middlebury
dataset. Aiming at the limitation of traditional stereo matching algorithms
regarding the cost window, a cost aggregation algorithm based on the dynamic
window is proposed, and the disparity image is optimized by using left and
right consistency detection to further reduce the error matching rate. The
experimental results show that the proposed algorithm can effectively enhance
the stereo matching effect of the image providing significant improvement in
accuracy as compared with the classical census algorithm. The proposed model
code, dataset, and experimental results are available at
https://github.com/WangHewei16/Stereo-Matching.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hewei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathan_M/0/1/0/all/0/1"&gt;Muhammad Salman Pathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1"&gt;Soumyabrata Dev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Fact-Checking for Assisting Human Fact-Checkers. (arXiv:2103.07769v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07769</id>
        <link href="http://arxiv.org/abs/2103.07769"/>
        <updated>2021-05-25T01:56:10.220Z</updated>
        <summary type="html"><![CDATA[The reporting and the analysis of current events around the globe has
expanded from professional, editor-lead journalism all the way to citizen
journalism. Nowadays, politicians and other key players enjoy direct access to
their audiences through social media, bypassing the filters of official cables
or traditional media. However, the multiple advantages of free speech and
direct communication are dimmed by the misuse of media to spread inaccurate or
misleading claims. These phenomena have led to the modern incarnation of the
fact-checker -- a professional whose main aim is to examine claims using
available evidence and to assess their veracity. As in other text forensics
tasks, the amount of information available makes the work of the fact-checker
more difficult. With this in mind, starting from the perspective of the
professional fact-checker, we survey the available intelligent technologies
that can support the human expert in the different steps of her fact-checking
endeavor. These include identifying claims worth fact-checking, detecting
relevant previously fact-checked claims, retrieving relevant evidence to
fact-check a claim, and actually verifying a claim. In each case, we pay
attention to the challenges in future work and the potential impact on
real-world fact-checking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1"&gt;Preslav Nakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corney_D/0/1/0/all/0/1"&gt;David Corney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1"&gt;Maram Hasanain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1"&gt;Tamer Elsayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barron_Cedeno_A/0/1/0/all/0/1"&gt;Alberto Barr&amp;#xf3;n-Cede&amp;#xf1;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papotti_P/0/1/0/all/0/1"&gt;Paolo Papotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1"&gt;Shaden Shaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1"&gt;Giovanni Da San Martino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hausdorff Dimension, Heavy Tails, and Generalization in Neural Networks. (arXiv:2006.09313v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09313</id>
        <link href="http://arxiv.org/abs/2006.09313"/>
        <updated>2021-05-25T01:56:10.208Z</updated>
        <summary type="html"><![CDATA[Despite its success in a wide range of applications, characterizing the
generalization properties of stochastic gradient descent (SGD) in non-convex
deep learning problems is still an important challenge. While modeling the
trajectories of SGD via stochastic differential equations (SDE) under
heavy-tailed gradient noise has recently shed light over several peculiar
characteristics of SGD, a rigorous treatment of the generalization properties
of such SDEs in a learning theoretical framework is still missing. Aiming to
bridge this gap, in this paper, we prove generalization bounds for SGD under
the assumption that its trajectories can be well-approximated by a \emph{Feller
process}, which defines a rich class of Markov processes that include several
recent SDE representations (both Brownian or heavy-tailed) as its special case.
We show that the generalization error can be controlled by the \emph{Hausdorff
dimension} of the trajectories, which is intimately linked to the tail behavior
of the driving process. Our results imply that heavier-tailed processes should
achieve better generalization; hence, the tail-index of the process can be used
as a notion of "capacity metric". We support our theory with experiments on
deep neural networks illustrating that the proposed capacity metric accurately
estimates the generalization error, and it does not necessarily grow with the
number of parameters unlike the existing capacity metrics in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1"&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sener_O/0/1/0/all/0/1"&gt;Ozan Sener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1"&gt;George Deligiannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Low-Pass Filters: Adaptive Feature Propagation on Graphs. (arXiv:2103.14187v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14187</id>
        <link href="http://arxiv.org/abs/2103.14187"/>
        <updated>2021-05-25T01:56:10.189Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have been extensively studied for prediction
tasks on graphs. As pointed out by recent studies, most GNNs assume local
homophily, i.e., strong similarities in local neighborhoods. This assumption
however limits the generalizability power of GNNs. To address this limitation,
we propose a flexible GNN model, which is capable of handling any graphs
without being restricted by their underlying homophily. At its core, this model
adopts a node attention mechanism based on multiple learnable spectral filters;
therefore, the aggregation scheme is learned adaptively for each graph in the
spectral domain. We evaluated the proposed model on node classification tasks
over eight benchmark datasets. The proposed model is shown to generalize well
to both homophilic and heterophilic graphs. Further, it outperforms all
state-of-the-art baselines on heterophilic graphs and performs comparably with
them on homophilic graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sean Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dongwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A self-supervised learning strategy for postoperative brain cavity segmentation simulating resections. (arXiv:2105.11239v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11239</id>
        <link href="http://arxiv.org/abs/2105.11239"/>
        <updated>2021-05-25T01:56:10.182Z</updated>
        <summary type="html"><![CDATA[Accurate segmentation of brain resection cavities (RCs) aids in postoperative
analysis and determining follow-up treatment. Convolutional neural networks
(CNNs) are the state-of-the-art image segmentation technique, but require large
annotated datasets for training. Annotation of 3D medical images is
time-consuming, requires highly-trained raters, and may suffer from high
inter-rater variability. Self-supervised learning strategies can leverage
unlabeled data for training.

We developed an algorithm to simulate resections from preoperative magnetic
resonance images (MRIs). We performed self-supervised training of a 3D CNN for
RC segmentation using our simulation method. We curated EPISURG, a dataset
comprising 430 postoperative and 268 preoperative MRIs from 430 refractory
epilepsy patients who underwent resective neurosurgery. We fine-tuned our model
on three small annotated datasets from different institutions and on the
annotated images in EPISURG, comprising 20, 33, 19 and 133 subjects.

The model trained on data with simulated resections obtained median
(interquartile range) Dice score coefficients (DSCs) of 81.7 (16.4), 82.4
(36.4), 74.9 (24.2) and 80.5 (18.7) for each of the four datasets. After
fine-tuning, DSCs were 89.2 (13.3), 84.1 (19.8), 80.2 (20.1) and 85.2 (10.8).
For comparison, inter-rater agreement between human annotators from our
previous study was 84.0 (9.9).

We present a self-supervised learning strategy for 3D CNNs using simulated
RCs to accurately segment real RCs on postoperative MRI. Our method generalizes
well to data from different institutions, pathologies and modalities. Source
code, segmentation models and the EPISURG dataset are available at
https://github.com/fepegar/ressegijcars .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dorent_R/0/1/0/all/0/1"&gt;Reuben Dorent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rizzi_M/0/1/0/all/0/1"&gt;Michele Rizzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cardinale_F/0/1/0/all/0/1"&gt;Francesco Cardinale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Frazzini_V/0/1/0/all/0/1"&gt;Valerio Frazzini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Navarro_V/0/1/0/all/0/1"&gt;Vincent Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Essert_C/0/1/0/all/0/1"&gt;Caroline Essert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ollivier_I/0/1/0/all/0/1"&gt;Ir&amp;#xe8;ne Ollivier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1"&gt;Tom Vercauteren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1"&gt;John S. Duncan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pulmonary embolism identification in computerized tomography pulmonary angiography scans with deep learning technologies in COVID-19 patients. (arXiv:2105.11187v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11187</id>
        <link href="http://arxiv.org/abs/2105.11187"/>
        <updated>2021-05-25T01:56:10.175Z</updated>
        <summary type="html"><![CDATA[The main objective of this work is to utilize state-of-the-art deep learning
approaches for the identification of pulmonary embolism in CTPA-Scans for
COVID-19 patients, provide an initial assessment of their performance and,
ultimately, provide a fast-track prototype solution (system). We adopted and
assessed some of the most popular convolutional neural network architectures
through transfer learning approaches, to strive to combine good model accuracy
with fast training. Additionally, we exploited one of the most popular
one-stage object detection models for the localization (through object
detection) of the pulmonary embolism regions-of-interests. The models of both
approaches are trained on an original CTPA-Scan dataset, where we annotated of
673 CTPA-Scan images with 1,465 bounding boxes in total, highlighting pulmonary
embolism regions-of-interests. We provide a brief assessment of some
state-of-the-art image classification models by achieving validation accuracies
of 91% in pulmonary embolism classification. Additionally, we achieved a
precision of about 68% on average in the object detection model for the
pulmonary embolism localization under 50% IoU threshold. For both approaches,
we provide the entire training pipelines for future studies (step by step
processes through source code). In this study, we present some of the most
accurate and fast deep learning models for pulmonary embolism identification in
CTPA-Scans images, through classification and localization (object detection)
approaches for patients infected by COVID-19. We provide a fast-track solution
(system) for the research community of the area, which combines both
classification and object detection models for improving the precision of
identifying pulmonary embolisms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kiourt_C/0/1/0/all/0/1"&gt;Chairi Kiourt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feretzakis_G/0/1/0/all/0/1"&gt;Georgios Feretzakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dalamarinis_K/0/1/0/all/0/1"&gt;Konstantinos Dalamarinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kalles_D/0/1/0/all/0/1"&gt;Dimitris Kalles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pantos_G/0/1/0/all/0/1"&gt;Georgios Pantos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Papadopoulos_I/0/1/0/all/0/1"&gt;Ioannis Papadopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kouris_S/0/1/0/all/0/1"&gt;Spyros Kouris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ioannakis_G/0/1/0/all/0/1"&gt;George Ioannakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Loupelis_E/0/1/0/all/0/1"&gt;Evangelos Loupelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sakagianni_A/0/1/0/all/0/1"&gt;Aikaterini Sakagianni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oriented RepPoints for Aerial Object Detection. (arXiv:2105.11111v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11111</id>
        <link href="http://arxiv.org/abs/2105.11111"/>
        <updated>2021-05-25T01:56:10.168Z</updated>
        <summary type="html"><![CDATA[In contrast to the oriented bounding boxes, point set representation has
great potential to capture the detailed structure of instances with the
arbitrary orientations, large aspect ratios and dense distribution in aerial
images. However, the conventional point set-based approaches are handcrafted
with the fixed locations using points-to-points supervision, which hurts their
flexibility on the fine-grained feature extraction. To address these
limitations, in this paper, we propose a novel approach to aerial object
detection, named Oriented RepPoints. Specifically, we suggest to employ a set
of adaptive points to capture the geometric and spatial information of the
arbitrary-oriented objects, which is able to automatically arrange themselves
over the object in a spatial and semantic scenario. To facilitate the
supervised learning, the oriented conversion function is proposed to explicitly
map the adaptive point set into an oriented bounding box. Moreover, we
introduce an effective quality assessment measure to select the point set
samples for training, which can choose the representative items with respect to
their potentials on orientated object detection. Furthermore, we suggest a
spatial constraint to penalize the outlier points outside the ground-truth
bounding box. In addition to the traditional evaluation metric mAP focusing on
overlap ratio, we propose a new metric mAOE to measure the orientation accuracy
that is usually neglected in the previous studies on oriented object detection.
Experiments on three widely used datasets including DOTA, HRSC2016 and UCAS-AOD
demonstrate that our proposed approach is effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wentong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianke Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fourier-based Framework for Domain Generalization. (arXiv:2105.11120v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11120</id>
        <link href="http://arxiv.org/abs/2105.11120"/>
        <updated>2021-05-25T01:56:10.161Z</updated>
        <summary type="html"><![CDATA[Modern deep neural networks suffer from performance degradation when
evaluated on testing data under different distributions from training data.
Domain generalization aims at tackling this problem by learning transferable
knowledge from multiple source domains in order to generalize to unseen target
domains. This paper introduces a novel Fourier-based perspective for domain
generalization. The main assumption is that the Fourier phase information
contains high-level semantics and is not easily affected by domain shifts. To
force the model to capture phase information, we develop a novel Fourier-based
data augmentation strategy called amplitude mix which linearly interpolates
between the amplitude spectrums of two images. A dual-formed consistency loss
called co-teacher regularization is further introduced between the predictions
induced from original and augmented images. Extensive experiments on three
benchmarks have demonstrated that the proposed method is able to achieve
state-of-the-arts performance for domain generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qinwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving DeepFake Detection Using Dynamic Face Augmentation. (arXiv:2102.09603v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09603</id>
        <link href="http://arxiv.org/abs/2102.09603"/>
        <updated>2021-05-25T01:56:10.143Z</updated>
        <summary type="html"><![CDATA[The creation of altered and manipulated faces has become more common due to
the improvement of DeepFake generation methods. Simultaneously, we have seen
detection models' development for differentiating between a manipulated and
original face from image or video content. We have observed that most publicly
available DeepFake detection datasets have limited variations, where a single
face is used in many videos, resulting in an oversampled training dataset. Due
to this, deep neural networks tend to overfit to the facial features instead of
learning to detect manipulation features of DeepFake content. As a result, most
detection architectures perform poorly when tested on unseen data. In this
paper, we provide a quantitative analysis to investigate this problem and
present a solution to prevent model overfitting due to the high volume of
samples generated from a small number of actors. We introduce Face-Cutout, a
data augmentation method for training Convolutional Neural Networks (CNN), to
improve DeepFake detection. In this method, training images with various
occlusions are dynamically generated using face landmark information
irrespective of orientation. Unlike other general-purpose augmentation methods,
it focuses on the facial information that is crucial for DeepFake detection.
Our method achieves a reduction in LogLoss of 15.2% to 35.3% on different
datasets, compared to other occlusion-based augmentation techniques. We show
that Face-Cutout can be easily integrated with any CNN-based recognition model
and improve detection performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sowmen Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1"&gt;Arup Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Md. Saiful Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1"&gt;Md. Ruhul Amin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking. (arXiv:2105.11237v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11237</id>
        <link href="http://arxiv.org/abs/2105.11237"/>
        <updated>2021-05-25T01:56:10.134Z</updated>
        <summary type="html"><![CDATA[Recently, most siamese network based trackers locate targets via object
classification and bounding-box regression. Generally, they select the
bounding-box with maximum classification confidence as the final prediction.
This strategy may miss the right result due to the accuracy misalignment
between classification and regression. In this paper, we propose a novel
siamese tracking algorithm called SiamRCR, addressing this problem with a
simple, light and effective solution. It builds reciprocal links between
classification and regression branches, which can dynamically re-weight their
losses for each positive sample. In addition, we add a localization branch to
predict the localization accuracy, so that it can work as the replacement of
the regression assistance link during inference. This branch makes the training
and inference more consistent. Extensive experimental results demonstrate the
effectiveness of SiamRCR and its superiority over the state-of-the-art
competitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.
Moreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jinlong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengkai Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yueyang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiyao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Higher Order Smoothness in Derivative-free Optimization and Continuous Bandits. (arXiv:2006.07862v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07862</id>
        <link href="http://arxiv.org/abs/2006.07862"/>
        <updated>2021-05-25T01:56:10.126Z</updated>
        <summary type="html"><![CDATA[We study the problem of zero-order optimization of a strongly convex
function. The goal is to find the minimizer of the function by a sequential
exploration of its values, under measurement noise. We study the impact of
higher order smoothness properties of the function on the optimization error
and on the cumulative regret. To solve this problem we consider a randomized
approximation of the projected gradient descent algorithm. The gradient is
estimated by a randomized procedure involving two function evaluations and a
smoothing kernel. We derive upper bounds for this algorithm both in the
constrained and unconstrained settings and prove minimax lower bounds for any
sequential search method. Our results imply that the zero-order algorithm is
nearly optimal in terms of sample complexity and the problem parameters. Based
on this algorithm, we also propose an estimator of the minimum value of the
function achieving almost sharp oracle behavior. We compare our results with
the state-of-the-art, highlighting a number of key improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhavan_A/0/1/0/all/0/1"&gt;Arya Akhavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1"&gt;Massimiliano Pontil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsybakov_A/0/1/0/all/0/1"&gt;Alexandre B. Tsybakov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Few-Shot Classification with Deep Invertible Hybrid Models. (arXiv:2105.10644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10644</id>
        <link href="http://arxiv.org/abs/2105.10644"/>
        <updated>2021-05-25T01:56:10.118Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a deep invertible hybrid model which integrates
discriminative and generative learning at a latent space level for
semi-supervised few-shot classification. Various tasks for classifying new
species from image data can be modeled as a semi-supervised few-shot
classification, which assumes a labeled and unlabeled training examples and a
small support set of the target classes. Predicting target classes with a few
support examples per class makes the learning task difficult for existing
semi-supervised classification methods, including selftraining, which
iteratively estimates class labels of unlabeled training examples to learn a
classifier for the training classes. To exploit unlabeled training examples
effectively, we adopt as the objective function the composite likelihood, which
integrates discriminative and generative learning and suits better with deep
neural networks than the parameter coupling prior, the other popular integrated
learning approach. In our proposed model, the discriminative and generative
models are respectively Prototypical Networks, which have shown excellent
performance in various kinds of few-shot learning, and Normalizing Flow a deep
invertible model which returns the exact marginal likelihood unlike the other
three major methods, i.e., VAE, GAN, and autoregressive model. Our main
originality lies in our integration of these components at a latent space
level, which is effective in preventing overfitting. Experiments using
mini-ImageNet and VGG-Face datasets show that our method outperforms
selftraining based Prototypical Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ohtsubo_Y/0/1/0/all/0/1"&gt;Yusuke Ohtsubo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsukawa_T/0/1/0/all/0/1"&gt;Tetsu Matsukawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_E/0/1/0/all/0/1"&gt;Einoshin Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stroke Lesion Segmentation with Visual Cortex Anatomy Alike Neural Nets. (arXiv:2105.06544v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06544</id>
        <link href="http://arxiv.org/abs/2105.06544"/>
        <updated>2021-05-25T01:56:10.110Z</updated>
        <summary type="html"><![CDATA[Cerebrovascular accident, or commonly known as stroke, is an acute disease
with extreme impact on patients and healthcare systems and is the second
largest cause of death worldwide. Fast and precise stroke lesion detection and
location is an extreme important process with regards to stroke diagnosis,
treatment, and prognosis. Except from the manual segmentation approach, machine
learning based segmentation methods are the most promising ones when
considering efficiency and accuracy, and convolutional neural network based
models are the first of its kind. However, most of these neural network models
do not really align with the brain anatomical structures. Intuitively, this
work presents a more brain alike model which mimics the anatomical structure of
the human visual cortex. Through the preliminary experiments on the stroke
lesion segmentation task, the proposed model is found to be able to perform
equally well or better to the de-facto standard U-Net. Part of the
implementation will be made available at https://github.com/DarkoBomer/VCA-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1"&gt;Chuanlong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Puck localization and multi-task event recognition in broadcast hockey videos. (arXiv:2105.10563v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10563</id>
        <link href="http://arxiv.org/abs/2105.10563"/>
        <updated>2021-05-25T01:56:10.084Z</updated>
        <summary type="html"><![CDATA[Puck localization is an important problem in ice hockey video analytics
useful for analyzing the game, determining play location, and assessing puck
possession. The problem is challenging due to the small size of the puck,
excessive motion blur due to high puck velocity and occlusions due to players
and boards. In this paper, we introduce and implement a network for puck
localization in broadcast hockey video. The network leverages expert NHL
play-by-play annotations and uses temporal context to locate the puck. Player
locations are incorporated into the network through an attention mechanism by
encoding player positions with a Gaussian-based spatial heatmap drawn at player
positions. Since event occurrence on the rink and puck location are related, we
also perform event recognition by augmenting the puck localization network with
an event recognition head and training the network through multi-task learning.
Experimental results demonstrate that the network is able to localize the puck
with an AUC of $73.1 \%$ on the test set. The puck location can be inferred in
720p broadcast videos at $5$ frames per second. It is also demonstrated that
multi-task learning with puck location improves event recognition accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1"&gt;Kanav Vats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fani_M/0/1/0/all/0/1"&gt;Mehrnaz Fani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clausi_D/0/1/0/all/0/1"&gt;David A. Clausi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1"&gt;John Zelek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Adversarial Inverse Reinforcement Learning. (arXiv:2102.02454v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02454</id>
        <link href="http://arxiv.org/abs/2102.02454"/>
        <updated>2021-05-25T01:56:10.077Z</updated>
        <summary type="html"><![CDATA[Extrapolating beyond-demonstrator (BD) through the inverse reinforcement
learning (IRL) algorithm aims to learn from and outperform the demonstrator. In
sharp contrast to the conventional reinforcement learning (RL) algorithms,
BD-IRL can overcome the dilemma incurred in the reward function design and
improve the exploration mechanism of RL, which opens new avenues to building
superior expert systems. Most existing BD-IRL algorithms are performed in two
stages by first inferring a reward function before learning a policy via RL.
However, such two-stage BD-IRL algorithms suffer from high computational
complexity, weak robustness, and large performance variations. In particular, a
poor reward function derived in the first stage will inevitably incur severe
performance loss in the second stage. In this work, we propose a hybrid
adversarial inverse reinforcement learning (HAIRL) algorithm that is one-stage,
model-free, generative-adversarial (GA) fashion and curiosity-driven. Thanks to
the one-stage design, the HAIRL can integrate both the reward function learning
and the policy optimization into one procedure, which leads to many advantages
such as low computational complexity, high robustness, and strong adaptability.
More specifically, HAIRL simultaneously imitates the demonstrator and explores
BD performance by utilizing hybrid rewards. In particular, the Wasserstein-1
distance (WD) is introduced into HAIRL to stabilize the imitation procedure
while a novel end-to-end curiosity module (ECM) is developed to improve the
exploration. Finally, extensive simulation results confirm that HAIRL can
achieve higher performance as compared to other similar BD-IRL algorithms. Our
code is available at our GitHub website
\footnote{\url{https://github.com/yuanmingqi/HAIRL}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Mingqi Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1"&gt;Man-On Pun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why did the distribution change?. (arXiv:2102.13384v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13384</id>
        <link href="http://arxiv.org/abs/2102.13384"/>
        <updated>2021-05-25T01:56:10.070Z</updated>
        <summary type="html"><![CDATA[We describe a formal approach based on graphical causal models to identify
the "root causes" of the change in the probability distribution of variables.
After factorizing the joint distribution into conditional distributions of each
variable, given its parents (the "causal mechanisms"), we attribute the change
to changes of these causal mechanisms. This attribution analysis accounts for
the fact that mechanisms often change independently and sometimes only some of
them change. Through simulations, we study the performance of our distribution
change attribution method. We then present a real-world case study identifying
the drivers of the difference in the income distribution between men and women.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Budhathoki_K/0/1/0/all/0/1"&gt;Kailash Budhathoki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Janzing_D/0/1/0/all/0/1"&gt;Dominik Janzing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bloebaum_P/0/1/0/all/0/1"&gt;Patrick Bloebaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ng_H/0/1/0/all/0/1"&gt;Hoiyi Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Query Language for Summarizing and Analyzing Business Process Data. (arXiv:2105.10911v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2105.10911</id>
        <link href="http://arxiv.org/abs/2105.10911"/>
        <updated>2021-05-25T01:56:10.063Z</updated>
        <summary type="html"><![CDATA[In modern enterprises, Business Processes (BPs) are realized over a mix of
workflows, IT systems, Web services and direct collaborations of people.
Accordingly, process data (i.e., BP execution data such as logs containing
events, interaction messages and other process artifacts) is scattered across
several systems and data sources, and increasingly show all typical properties
of the Big Data. Understanding the execution of process data is challenging as
key business insights remain hidden in the interactions among process entities:
most objects are interconnected, forming complex, heterogeneous but often
semi-structured networks. In the context of business processes, we consider the
Big Data problem as a massive number of interconnected data islands from
personal, shared and business data. We present a framework to model process
data as graphs, i.e., Process Graph, and present abstractions to summarize the
process graph and to discover concept hierarchies for entities based on both
data objects and their interactions in process graphs. We present a language,
namely BP-SPARQL, for the explorative querying and understanding of process
graphs from various user perspectives. We have implemented a scalable
architecture for querying, exploration and analysis of process graphs. We
report on experiments performed on both synthetic and real-world datasets that
show the viability and efficiency of the approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beheshti_A/0/1/0/all/0/1"&gt;Amin Beheshti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benatallah_B/0/1/0/all/0/1"&gt;Boualem Benatallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Motahari_Nezhad_H/0/1/0/all/0/1"&gt;Hamid Reza Motahari-Nezhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodratnama_S/0/1/0/all/0/1"&gt;Samira Ghodratnama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amouzgar_F/0/1/0/all/0/1"&gt;Farhad Amouzgar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulating SQL Injection Vulnerability Exploitation Using Q-Learning Reinforcement Learning Agents. (arXiv:2101.03118v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03118</id>
        <link href="http://arxiv.org/abs/2101.03118"/>
        <updated>2021-05-25T01:56:10.056Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a formalization of the process of exploitation of
SQL injection vulnerabilities. We consider a simplification of the dynamics of
SQL injection attacks by casting this problem as a security capture-the-flag
challenge. We model it as a Markov decision process, and we implement it as a
reinforcement learning problem. We then deploy reinforcement learning agents
tasked with learning an effective policy to perform SQL injection; we design
our training in such a way that the agent learns not just a specific strategy
to solve an individual challenge but a more generic policy that may be applied
to perform SQL injection attacks against any system instantiated randomly by
our problem generator. We analyze the results in terms of the quality of the
learned policy and in terms of convergence time as a function of the complexity
of the challenge and the learning agent's complexity. Our work fits in the
wider research on the development of intelligent agents for autonomous
penetration testing and white-hat hacking, and our results aim to contribute to
understanding the potential and the limits of reinforcement learning in a
security environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Erdodi_L/0/1/0/all/0/1"&gt;Laszlo Erdodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommervoll_%7B/0/1/0/all/0/1"&gt;&amp;#xc5;vald &amp;#xc5;slaugson Sommervoll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zennaro_F/0/1/0/all/0/1"&gt;Fabio Massimo Zennaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel-Based Smoothness Analysis of Residual Networks. (arXiv:2009.10008v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10008</id>
        <link href="http://arxiv.org/abs/2009.10008"/>
        <updated>2021-05-25T01:56:10.034Z</updated>
        <summary type="html"><![CDATA[A major factor in the success of deep neural networks is the use of
sophisticated architectures rather than the classical multilayer perceptron
(MLP). Residual networks (ResNets) stand out among these powerful modern
architectures. Previous works focused on the optimization advantages of deep
ResNets over deep MLPs. In this paper, we show another distinction between the
two models, namely, a tendency of ResNets to promote smoother interpolations
than MLPs. We analyze this phenomenon via the neural tangent kernel (NTK)
approach. First, we compute the NTK for a considered ResNet model and prove its
stability during gradient descent training. Then, we show by various evaluation
methodologies that for ReLU activations the NTK of ResNet, and its kernel
regression results, are smoother than the ones of MLP. The better smoothness
observed in our analysis may explain the better generalization ability of
ResNets and the practice of moderately attenuating the residual blocks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tirer_T/0/1/0/all/0/1"&gt;Tom Tirer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1"&gt;Raja Giryes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoInt: Automatic Integration for Fast Neural Volume Rendering. (arXiv:2012.01714v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01714</id>
        <link href="http://arxiv.org/abs/2012.01714"/>
        <updated>2021-05-25T01:56:10.022Z</updated>
        <summary type="html"><![CDATA[Numerical integration is a foundational technique in scientific computing and
is at the core of many computer vision applications. Among these applications,
neural volume rendering has recently been proposed as a new paradigm for view
synthesis, achieving photorealistic image quality. However, a fundamental
obstacle to making these methods practical is the extreme computational and
memory requirements caused by the required volume integrations along the
rendered rays during training and inference. Millions of rays, each requiring
hundreds of forward passes through a neural network are needed to approximate
those integrations with Monte Carlo sampling. Here, we propose automatic
integration, a new framework for learning efficient, closed-form solutions to
integrals using coordinate-based neural networks. For training, we instantiate
the computational graph corresponding to the derivative of the network. The
graph is fitted to the signal to integrate. After optimization, we reassemble
the graph to obtain a network that represents the antiderivative. By the
fundamental theorem of calculus, this enables the calculation of any definite
integral in two evaluations of the network. Applying this approach to neural
rendering, we improve a tradeoff between rendering speed and image quality:
improving render times by greater than 10 times with a tradeoff of slightly
reduced image quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lindell_D/0/1/0/all/0/1"&gt;David B. Lindell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martel_J/0/1/0/all/0/1"&gt;Julien N. P. Martel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1"&gt;Gordon Wetzstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Knee X-ray Report Generation. (arXiv:2105.10702v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10702</id>
        <link href="http://arxiv.org/abs/2105.10702"/>
        <updated>2021-05-25T01:56:10.013Z</updated>
        <summary type="html"><![CDATA[Gathering manually annotated images for the purpose of training a predictive
model is far more challenging in the medical domain than for natural images as
it requires the expertise of qualified radiologists. We therefore propose to
take advantage of past radiological exams (specifically, knee X-ray
examinations) and formulate a framework capable of learning the correspondence
between the images and reports, and hence be capable of generating diagnostic
reports for a given X-ray examination consisting of an arbitrary number of
image views. We demonstrate how aggregating the image features of individual
exams and using them as conditional inputs when training a language generation
model results in auto-generated exam reports that correlate well with
radiologist-generated reports.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gasimova_A/0/1/0/all/0/1"&gt;Aydan Gasimova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montana_G/0/1/0/all/0/1"&gt;Giovanni Montana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taylor saves for later: disentanglement for video prediction using Taylor representation. (arXiv:2105.11062v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11062</id>
        <link href="http://arxiv.org/abs/2105.11062"/>
        <updated>2021-05-25T01:56:09.991Z</updated>
        <summary type="html"><![CDATA[Video prediction is a challenging task with wide application prospects in
meteorology and robot systems. Existing works fail to trade off short-term and
long-term prediction performances and extract robust latent dynamics laws in
video frames. We propose a two-branch seq-to-seq deep model to disentangle the
Taylor feature and the residual feature in video frames by a novel recurrent
prediction module (TaylorCell) and residual module. TaylorCell can expand the
video frames' high-dimensional features into the finite Taylor series to
describe the latent laws. In TaylorCell, we propose the Taylor prediction unit
(TPU) and the memory correction unit (MCU). TPU employs the first input frame's
derivative information to predict the future frames, avoiding error
accumulation. MCU distills all past frames' information to correct the
predicted Taylor feature from TPU. Correspondingly, the residual module
extracts the residual feature complementary to the Taylor feature. On three
generalist datasets (Moving MNIST, TaxiBJ, Human 3.6), our model outperforms or
reaches state-of-the-art models, and ablation experiments demonstrate the
effectiveness of our model in long-term prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1"&gt;Ting Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhuqing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jianan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1"&gt;Shiping Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_A/0/1/0/all/0/1"&gt;Aidong Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiying Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic calibration of time of flight based non-line-of-sight reconstruction. (arXiv:2105.10603v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10603</id>
        <link href="http://arxiv.org/abs/2105.10603"/>
        <updated>2021-05-25T01:56:09.985Z</updated>
        <summary type="html"><![CDATA[Time of flight based Non-line-of-sight (NLOS) imaging approaches require
precise calibration of illumination and detector positions on the visible scene
to produce reasonable results. If this calibration error is sufficiently high,
reconstruction can fail entirely without any indication to the user. In this
work, we highlight the necessity of building autocalibration into NLOS
reconstruction in order to handle mis-calibration. We propose a forward model
of NLOS measurements that is differentiable with respect to both, the hidden
scene albedo, and virtual illumination and detector positions. With only a mean
squared error loss and no regularization, our model enables joint
reconstruction and recovery of calibration parameters by minimizing the
measurement residual using gradient descent. We demonstrate our method is able
to produce robust reconstructions using simulated and real data where the
calibration error applied causes other state of the art algorithms to fail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sadhu_S/0/1/0/all/0/1"&gt;Subhash Chandra Sadhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1"&gt;Abhishek Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maeda_T/0/1/0/all/0/1"&gt;Tomohiro Maeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Swedish_T/0/1/0/all/0/1"&gt;Tristan Swedish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_R/0/1/0/all/0/1"&gt;Ryan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sinha_L/0/1/0/all/0/1"&gt;Lagnojita Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raskar_R/0/1/0/all/0/1"&gt;Ramesh Raskar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Searching Collaborative Agents for Multi-plane Localization in 3D Ultrasound. (arXiv:2105.10626v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10626</id>
        <link href="http://arxiv.org/abs/2105.10626"/>
        <updated>2021-05-25T01:56:09.966Z</updated>
        <summary type="html"><![CDATA[3D ultrasound (US) has become prevalent due to its rich spatial and
diagnostic information not contained in 2D US. Moreover, 3D US can contain
multiple standard planes (SPs) in one shot. Thus, automatically localizing SPs
in 3D US has the potential to improve user-independence and
scanning-efficiency. However, manual SP localization in 3D US is challenging
because of the low image quality, huge search space and large anatomical
variability. In this work, we propose a novel multi-agent reinforcement
learning (MARL) framework to simultaneously localize multiple SPs in 3D US. Our
contribution is four-fold. First, our proposed method is general and it can
accurately localize multiple SPs in different challenging US datasets. Second,
we equip the MARL system with a recurrent neural network (RNN) based
collaborative module, which can strengthen the communication among agents and
learn the spatial relationship among planes effectively. Third, we explore to
adopt the neural architecture search (NAS) to automatically design the network
architecture of both the agents and the collaborative module. Last, we believe
we are the first to realize automatic SP localization in pelvic US volumes, and
note that our approach can handle both normal and abnormal uterus cases.
Extensively validated on two challenging datasets of the uterus and fetal
brain, our proposed method achieves the average localization accuracy of 7.03
degrees/1.59mm and 9.75 degrees/1.19mm. Experimental results show that our
light-weight MARL model has higher accuracy than state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuhao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1"&gt;Ruobing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1"&gt;Haoran Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jikuan Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaoqiong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Wenlong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuanji Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haixia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Study imbalance handling by various data sampling methods in binary classification. (arXiv:2105.10959v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10959</id>
        <link href="http://arxiv.org/abs/2105.10959"/>
        <updated>2021-05-25T01:56:09.956Z</updated>
        <summary type="html"><![CDATA[The purpose of this research report is to present the our learning curve and
the exposure to the Machine Learning life cycle, with the use of a Kaggle
binary classification data set and taking to explore various techniques from
pre-processing to the final optimization and model evaluation, also we
highlight on the data imbalance issue and we discuss the different methods of
handling that imbalance on the data level by over-sampling and under sampling
not only to reach a balanced class representation but to improve the overall
performance. This work also opens some gaps for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamama_M/0/1/0/all/0/1"&gt;Mohamed Hamama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighted Least Squares Twin Support Vector Machine with Fuzzy Rough Set Theory for Imbalanced Data Classification. (arXiv:2105.01198v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01198</id>
        <link href="http://arxiv.org/abs/2105.01198"/>
        <updated>2021-05-25T01:56:09.949Z</updated>
        <summary type="html"><![CDATA[Support vector machines (SVMs) are powerful supervised learning tools
developed to solve classification problems. However, SVMs are likely to perform
poorly in the classification of imbalanced data. The rough set theory presents
a mathematical tool for inference in nondeterministic cases that provides
methods for removing irrelevant information from data. In this work, we propose
an approach that efficiently used fuzzy rough set theory in weighted least
squares twin support vector machine called FRLSTSVM for classification of
imbalanced data. The first innovation is introducing a new fuzzy rough
set-based under-sampling strategy to make the classifier robust in terms of the
imbalanced data. For constructing the two proximal hyperplanes in FRLSTSVM,
data points from the minority class remain unchanged while a subset of data
points in the majority class are selected using a new method. In this model, we
embed the weight biases in the LSTSVM formulations to overcome the bias
phenomenon in the original twin SVM for the classification of imbalanced data.
In order to determine these weights in this formulation, we introduce a new
strategy that uses fuzzy rough set theory as the second innovation.
Experimental results on the famous imbalanced datasets, compared to the related
traditional SVM-based methods, demonstrate the superiority of the proposed
FRLSTSVM model in the imbalanced data classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Behmanesh_M/0/1/0/all/0/1"&gt;Maysam Behmanesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adibi_P/0/1/0/all/0/1"&gt;Peyman Adibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karshenas_H/0/1/0/all/0/1"&gt;Hossein Karshenas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Throughput Soybean Pod-Counting with In-Field Robotic Data Collection and Machine-Vision Based Data Analysis. (arXiv:2105.10568v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10568</id>
        <link href="http://arxiv.org/abs/2105.10568"/>
        <updated>2021-05-25T01:56:09.941Z</updated>
        <summary type="html"><![CDATA[We report promising results for high-throughput on-field soybean pod count
with small mobile robots and machine-vision algorithms. Our results show that
the machine-vision based soybean pod counts are strongly correlated with
soybean yield. While pod counts has a strong correlation with soybean yield,
pod counting is extremely labor intensive, and has been difficult to automate.
Our results establish that an autonomous robot equipped with vision sensors can
autonomously collect soybean data at maturity. Machine-vision algorithms can be
used to estimate pod-counts across a large diversity panel planted across
experimental units (EUs, or plots) in a high-throughput, automated manner. We
report a correlation of 0.67 between our automated pod counts and soybean
yield. The data was collected in an experiment consisting of 1463 single-row
plots maintained by the University of Illinois soybean breeding program during
the 2020 growing season. We also report a correlation of 0.88 between automated
pod counts and manual pod counts over a smaller data set of 16 plots.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McGuire_M/0/1/0/all/0/1"&gt;Michael McGuire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soman_C/0/1/0/all/0/1"&gt;Chinmay Soman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diers_B/0/1/0/all/0/1"&gt;Brian Diers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1"&gt;Girish Chowdhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06450</id>
        <link href="http://arxiv.org/abs/2103.06450"/>
        <updated>2021-05-25T01:56:09.934Z</updated>
        <summary type="html"><![CDATA[We present a Neural Network based Handwritten Text Recognition (HTR) model
architecture that can be trained to recognize full pages of handwritten or
printed text without image segmentation. Being based on Image to Sequence
architecture, it can extract text present in an image and then sequence it
correctly without imposing any constraints regarding orientation, layout and
size of text and non-text. Further, it can also be trained to generate
auxiliary markup related to formatting, layout and content. We use character
level vocabulary, thereby enabling language and terminology of any subject. The
model achieves a new state-of-art in paragraph level recognition on the IAM
dataset. When evaluated on scans of real world handwritten free form test
answers - beset with curved and slanted lines, drawings, tables, math,
chemistry and other symbols - it performs better than all commercially
available HTR cloud APIs. It is deployed in production as part of a commercial
web application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sumeet S. Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karayev_S/0/1/0/all/0/1"&gt;Sergey Karayev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised on Graphs: Contrastive, Generative,or Predictive. (arXiv:2105.07342v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07342</id>
        <link href="http://arxiv.org/abs/2105.07342"/>
        <updated>2021-05-25T01:56:09.913Z</updated>
        <summary type="html"><![CDATA[Deep learning on graphs has recently achieved remarkable success on a variety
of tasks while such success relies heavily on the massive and carefully labeled
data. However, precise annotations are generally very expensive and
time-consuming. To address this problem, self-supervised learning (SSL) is
emerging as a new paradigm for extracting informative knowledge through
well-designed pretext tasks without relying on manual labels. In this survey,
we extend the concept of SSL, which first emerged in the fields of computer
vision and natural language processing, to present a timely and comprehensive
review of the existing SSL techniques for graph data. Specifically, we divide
existing graph SSL methods into three categories: contrastive, generative, and
predictive. More importantly, unlike many other surveys that only provide a
high-level description of published research, we present an additional
mathematical summary of the existing works in a unified framework. Furthermore,
to facilitate methodological development and empirical comparisons, we also
summarize the commonly used datasets, evaluation metrics, downstream tasks, and
open-source implementations of various algorithms. Finally, we discuss the
technical challenges and potential future directions for improving graph
self-supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lirong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haitao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhangyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Cheng Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan.Z.Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Bus Bunching with Asynchronous Multi-Agent Reinforcement Learning. (arXiv:2105.00376v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00376</id>
        <link href="http://arxiv.org/abs/2105.00376"/>
        <updated>2021-05-25T01:56:09.905Z</updated>
        <summary type="html"><![CDATA[The bus system is a critical component of sustainable urban transportation.
However, due to the significant uncertainties in passenger demand and traffic
conditions, bus operation is unstable in nature and bus bunching has become a
common phenomenon that undermines the reliability and efficiency of bus
services. Despite recent advances in multi-agent reinforcement learning (MARL)
on traffic control, little research has focused on bus fleet control due to the
tricky asynchronous characteristic -- control actions only happen when a bus
arrives at a bus stop and thus agents do not act simultaneously. In this study,
we formulate route-level bus fleet control as an asynchronous multi-agent
reinforcement learning (ASMR) problem and extend the classical actor-critic
architecture to handle the asynchronous issue. Specifically, we design a novel
critic network to effectively approximate the marginal contribution for other
agents, in which graph attention neural network is used to conduct inductive
learning for policy evaluation. The critic structure also helps the ego agent
optimize its policy more efficiently. We evaluate the proposed framework on
real-world bus services and actual passenger demand derived from smart card
data. Our results show that the proposed model outperforms both traditional
headway-based control methods and existing MARL methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiawei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lijun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint learning of multiple Granger causal networks via non-convex regularizations: Inference of group-level brain connectivity. (arXiv:2105.07196v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07196</id>
        <link href="http://arxiv.org/abs/2105.07196"/>
        <updated>2021-05-25T01:56:09.897Z</updated>
        <summary type="html"><![CDATA[This paper considers joint learning of multiple sparse Granger graphical
models to discover underlying common and differential Granger causality (GC)
structures across multiple time series. This can be applied to drawing
group-level brain connectivity inferences from a homogeneous group of subjects
or discovering network differences among groups of signals collected under
heterogeneous conditions. By recognizing that the GC of a single multivariate
time series can be characterized by common zeros of vector autoregressive (VAR)
lag coefficients, a group sparse prior is included in joint regularized
least-squares estimations of multiple VAR models. Group-norm regularizations
based on group- and fused-lasso penalties encourage a decomposition of multiple
networks into a common GC structure, with other remaining parts defined in
individual-specific networks. Prior information about sparseness and sparsity
patterns of desired GC networks are incorporated as relative weights, while a
non-convex group norm in the penalty is proposed to enhance the accuracy of
network estimation in low-sample settings. Extensive numerical results on
simulations illustrated our method's improvements over existing sparse
estimation approaches on GC network sparsity recovery. Our methods were also
applied to available resting-state fMRI time series from the ADHD-200 data sets
to learn the differences of causality mechanisms, called effective brain
connectivity, between adolescents with ADHD and typically developing children.
Our analysis revealed that parts of the causality differences between the two
groups often resided in the orbitofrontal region and areas associated with the
limbic system, which agreed with clinical findings and data-driven results in
previous studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manomaisaowapak_P/0/1/0/all/0/1"&gt;Parinthorn Manomaisaowapak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Songsiri_J/0/1/0/all/0/1"&gt;Jitkomut Songsiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUGAR: Subgraph Neural Network with Reinforcement Pooling and Self-Supervised Mutual Information Mechanism. (arXiv:2101.08170v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08170</id>
        <link href="http://arxiv.org/abs/2101.08170"/>
        <updated>2021-05-25T01:56:09.889Z</updated>
        <summary type="html"><![CDATA[Graph representation learning has attracted increasing research attention.
However, most existing studies fuse all structural features and node attributes
to provide an overarching view of graphs, neglecting finer substructures'
semantics, and suffering from interpretation enigmas. This paper presents a
novel hierarchical subgraph-level selection and embedding based graph neural
network for graph classification, namely SUGAR, to learn more discriminative
subgraph representations and respond in an explanatory way. SUGAR reconstructs
a sketched graph by extracting striking subgraphs as the representative part of
the original graph to reveal subgraph-level patterns. To adaptively select
striking subgraphs without prior knowledge, we develop a reinforcement pooling
mechanism, which improves the generalization ability of the model. To
differentiate subgraph representations among graphs, we present a
self-supervised mutual information mechanism to encourage subgraph embedding to
be mindful of the global graph structural properties by maximizing their mutual
information. Extensive experiments on six typical bioinformatics datasets
demonstrate a significant and consistent improvement in model quality with
competitive performance and interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qingyun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1"&gt;Yuanxing Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Phillip S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Knowledge Distillation for Object Detection. (arXiv:2105.10633v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10633</id>
        <link href="http://arxiv.org/abs/2105.10633"/>
        <updated>2021-05-25T01:56:09.881Z</updated>
        <summary type="html"><![CDATA[The existing solutions for object detection distillation rely on the
availability of both a teacher model and ground-truth labels. We propose a new
perspective to relax this constraint. In our framework, a student is first
trained with pseudo labels generated by the teacher, and then fine-tuned using
labeled data, if any available. Extensive experiments demonstrate improvements
over existing object detection distillation algorithms. In addition, decoupling
the teacher and ground-truth distillation in this framework provides
interesting properties such: as 1) using unlabeled data to further improve the
student's performance, 2) combining multiple teacher models of different
architectures, even with different object categories, and 3) reducing the need
for labeled data (with only 20% of COCO labels, this method achieves the same
performance as the model trained on the entire set of labels). Furthermore, a
by-product of this approach is the potential usage for domain adaptation. We
verify these properties through extensive experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1"&gt;Amin Banitalebi-Dehkordi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Sampling Distributions Using Local 3D Workspace Decompositions for Motion Planning in High Dimensions. (arXiv:2010.15335v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15335</id>
        <link href="http://arxiv.org/abs/2010.15335"/>
        <updated>2021-05-25T01:56:09.874Z</updated>
        <summary type="html"><![CDATA[Earlier work has shown that reusing experience from prior motion planning
problems can improve the efficiency of similar, future motion planning queries.
However, for robots with many degrees-of-freedom, these methods exhibit poor
generalization across different environments and often require large datasets
that are impractical to gather. We present SPARK and FLAME , two
experience-based frameworks for sampling-based planning applicable to complex
manipulators in 3 D environments. Both combine samplers associated with
features from a workspace decomposition into a global biased sampling
distribution. SPARK decomposes the environment based on exact geometry while
FLAME is more general, and uses an octree-based decomposition obtained from
sensor data. We demonstrate the effectiveness of SPARK and FLAME on a Fetch
robot tasked with challenging pick-and-place manipulation problems. Our
approaches can be trained incrementally and significantly improve performance
with only a handful of examples, generalizing better over diverse tasks and
environments as compared to prior approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chamzas_C/0/1/0/all/0/1"&gt;Constantinos Chamzas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kingston_Z/0/1/0/all/0/1"&gt;Zachary Kingston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quintero_Pena_C/0/1/0/all/0/1"&gt;Carlos Quintero-Pe&amp;#xf1;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kavraki_L/0/1/0/all/0/1"&gt;Lydia E. Kavraki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Adaptive Control using Contraction Theory. (arXiv:2103.02987v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02987</id>
        <link href="http://arxiv.org/abs/2103.02987"/>
        <updated>2021-05-25T01:56:09.853Z</updated>
        <summary type="html"><![CDATA[We present a deep learning-based adaptive control framework for nonlinear
systems with multiplicatively separable parametrization, called aNCM - for
adaptive Neural Contraction Metric. The framework utilizes a deep neural
network to approximate a stabilizing adaptive control law parameterized by an
optimal contraction metric. The use of deep networks permits real-time
implementation of the control law and broad applicability to a variety of
systems, including systems modeled with basis function approximation methods.
We show using contraction theory that aNCM ensures exponential boundedness of
the distance between the target and controlled trajectories even under the
presence of the parametric uncertainty, robustly to the learning errors caused
by aNCM approximation as well as external additive disturbances. Its
superiority to the existing robust and adaptive control methods is demonstrated
in a simple cart-pole balancing task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsukamoto_H/0/1/0/all/0/1"&gt;Hiroyasu Tsukamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Soon-Jo Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slotine_J/0/1/0/all/0/1"&gt;Jean-Jacques Slotine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unpaired Image-to-Image Translation via Latent Energy Transport. (arXiv:2012.00649v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00649</id>
        <link href="http://arxiv.org/abs/2012.00649"/>
        <updated>2021-05-25T01:56:09.828Z</updated>
        <summary type="html"><![CDATA[Image-to-image translation aims to preserve source contents while translating
to discriminative target styles between two visual domains. Most works apply
adversarial learning in the ambient image space, which could be computationally
expensive and challenging to train. In this paper, we propose to deploy an
energy-based model (EBM) in the latent space of a pretrained autoencoder for
this task. The pretrained autoencoder serves as both a latent code extractor
and an image reconstruction worker. Our model, LETIT, is based on the
assumption that two domains share the same latent space, where latent
representation is implicitly decomposed as a content code and a domain-specific
style code. Instead of explicitly extracting the two codes and applying
adaptive instance normalization to combine them, our latent EBM can implicitly
learn to transport the source style code to the target style code while
preserving the content code, an advantage over existing image translation
methods. This simplified solution is also more efficient in the one-sided
unpaired image translation setting. Qualitative and quantitative comparisons
demonstrate superior translation quality and faithfulness for content
preservation. Our model is the first to be applicable to
1024$\times$1024-resolution unpaired image translation to the best of our
knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changyou Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Survey of Data Augmentation for Time Series Classification with Neural Networks. (arXiv:2007.15951v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15951</id>
        <link href="http://arxiv.org/abs/2007.15951"/>
        <updated>2021-05-25T01:56:09.820Z</updated>
        <summary type="html"><![CDATA[In recent times, deep artificial neural networks have achieved many successes
in pattern recognition. Part of this success can be attributed to the reliance
on big data to increase generalization. However, in the field of time series
recognition, many datasets are often very small. One method of addressing this
problem is through the use of data augmentation. In this paper, we survey data
augmentation techniques for time series and their application to time series
classification with neural networks. We outline four families of time series
data augmentation, including transformation-based methods, pattern mixing,
generative models, and decomposition methods, and detail their taxonomy.
Furthermore, we empirically evaluate 12 time series data augmentation methods
on 128 time series classification datasets with 6 different types of neural
networks. Through the results, we are able to analyze the characteristics,
advantages and disadvantages, and recommendations of each data augmentation
method. This survey aims to help in the selection of time series data
augmentation for neural network applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1"&gt;Brian Kenji Iwana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Lower Bounds for Standard and Robust Gaussian Process Bandit Optimization. (arXiv:2008.08757v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08757</id>
        <link href="http://arxiv.org/abs/2008.08757"/>
        <updated>2021-05-25T01:56:09.811Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider algorithm-independent lower bounds for the problem
of black-box optimization of functions having a bounded norm is some
Reproducing Kernel Hilbert Space (RKHS), which can be viewed as a non-Bayesian
Gaussian process bandit problem. In the standard noisy setting, we provide a
novel proof technique for deriving lower bounds on the regret, with benefits
including simplicity, versatility, and an improved dependence on the error
probability. In a robust setting in which every sampled point may be perturbed
by a suitably-constrained adversary, we provide a novel lower bound for
deterministic strategies, demonstrating an inevitable joint dependence of the
cumulative regret on the corruption level and the time horizon, in contrast
with existing lower bounds that only characterize the individual dependencies.
Furthermore, in a distinct robust setting in which the final point is perturbed
by an adversary, we strengthen an existing lower bound that only holds for
target success probabilities very close to one, by allowing for arbitrary
success probabilities above $\frac{2}{3}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cai_X/0/1/0/all/0/1"&gt;Xu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scarlett_J/0/1/0/all/0/1"&gt;Jonathan Scarlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrence of Optimum for Training Weight and Activation Quantized Networks. (arXiv:2012.05529v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05529</id>
        <link href="http://arxiv.org/abs/2012.05529"/>
        <updated>2021-05-25T01:56:09.793Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are quantized for efficient inference on
resource-constrained platforms. However, training deep learning models with
low-precision weights and activations involves a demanding optimization task,
which calls for minimizing a stage-wise loss function subject to a discrete
set-constraint. While numerous training methods have been proposed, existing
studies for full quantization of DNNs are mostly empirical. From a theoretical
point of view, we study practical techniques for overcoming the combinatorial
nature of network quantization. Specifically, we investigate a simple yet
powerful projected gradient-like algorithm for quantizing two-linear-layer
networks, which proceeds by repeatedly moving one step at float weights in the
negation of a heuristic \emph{fake} gradient of the loss function (so-called
coarse gradient) evaluated at quantized weights. For the first time, we prove
that under mild conditions, the sequence of quantized weights recurrently
visits the global optimum of the discrete minimization problem for training
fully quantized network. We also show numerical evidence of the recurrence
phenomenon of weight evolution in training quantized deep networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1"&gt;Ziang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Penghang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jack Xin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Vision-and-Language Tasks via Text Generation. (arXiv:2102.02779v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02779</id>
        <link href="http://arxiv.org/abs/2102.02779"/>
        <updated>2021-05-25T01:56:09.786Z</updated>
        <summary type="html"><![CDATA[Existing methods for vision-and-language learning typically require designing
task-specific architectures and objectives for each task. For example, a
multi-label answer classifier for visual question answering, a region scorer
for referring expression comprehension, and a language decoder for image
captioning, etc. To alleviate these hassles, in this work, we propose a unified
framework that learns different tasks in a single architecture with the same
language modeling objective, i.e., multimodal conditional text generation,
where our models learn to generate labels in text based on the visual and
textual inputs. On 7 popular vision-and-language benchmarks, including visual
question answering, referring expression comprehension, visual commonsense
reasoning, most of which have been previously modeled as discriminative tasks,
our generative approach (with a single unified architecture) reaches comparable
performance to recent task-specific state-of-the-art vision-and-language
models. Moreover, our generative approach shows better generalization ability
on questions that have rare answers. Also, we show that our framework allows
multi-task learning in a single architecture with a single set of parameters,
achieving similar performance to separately optimized single-task models. Our
code is publicly available at: https://github.com/j-min/VL-T5]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1"&gt;Jaemin Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarially Trained Models with Test-Time Covariate Shift Adaptation. (arXiv:2102.05096v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05096</id>
        <link href="http://arxiv.org/abs/2102.05096"/>
        <updated>2021-05-25T01:56:09.780Z</updated>
        <summary type="html"><![CDATA[We empirically demonstrate that test-time adaptive batch normalization, which
re-estimates the batch-normalization statistics during inference, can provide
$\ell_2$-certification as well as improve the commonly occurring corruption
robustness of adversarially trained models while maintaining their
state-of-the-art empirical robustness against adversarial attacks. Furthermore,
we obtain similar $\ell_2$-certification as the current state-of-the-art
certification models for CIFAR-10 by learning our adversarially trained model
using larger $\ell_2$-bounded adversaries. Therefore our work is a step towards
bridging the gap between the state-of-the-art certification and empirical
robustness. Our results also indicate that improving the empirical adversarial
robustness may be sufficient as we achieve certification and corruption
robustness as a by-product using test-time adaptive batch normalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nandy_J/0/1/0/all/0/1"&gt;Jay Nandy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1"&gt;Sudipan Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wynne Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mong Li Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning based mixed-dimensional GMM for characterizing variability in CryoEM. (arXiv:2101.10356v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10356</id>
        <link href="http://arxiv.org/abs/2101.10356"/>
        <updated>2021-05-25T01:56:09.773Z</updated>
        <summary type="html"><![CDATA[Structural flexibility and/or dynamic interactions with other molecules is a
critical aspect of protein function. CryoEM provides direct visualization of
individual macromolecules sampling different conformational and compositional
states. While numerous methods are available for computational classification
of discrete states, characterization of continuous conformational changes or
large numbers of discrete state without human supervision remains challenging.
Here we present e2gmm, a machine learning algorithm to determine a
conformational landscape for proteins or complexes using a 3-D Gaussian mixture
model mapped onto 2-D particle images in known orientations. Using a deep
neural network architecture, e2gmm can automatically resolve the structural
heterogeneity within the protein complex and map particles onto a small latent
space describing conformational and compositional changes. This system presents
a more intuitive and flexible representation than other manifold methods
currently in use. We demonstrate this method on both simulated data as well as
three biological systems, to explore compositional and conformational changes
at a range of scales. The software is distributed as part of EMAN2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ludtke_S/0/1/0/all/0/1"&gt;Steven Ludtke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Certifying L-infinity Robustness using Neural Networks with L-inf-dist Neurons. (arXiv:2102.05363v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05363</id>
        <link href="http://arxiv.org/abs/2102.05363"/>
        <updated>2021-05-25T01:56:09.766Z</updated>
        <summary type="html"><![CDATA[It is well-known that standard neural networks, even with a high
classification accuracy, are vulnerable to small $\ell_\infty$-norm bounded
adversarial perturbations. Although many attempts have been made, most previous
works either can only provide empirical verification of the defense to a
particular attack method, or can only develop a certified guarantee of the
model robustness in limited scenarios. In this paper, we seek for a new
approach to develop a theoretically principled neural network that inherently
resists $\ell_\infty$ perturbations. In particular, we design a novel neuron
that uses $\ell_\infty$-distance as its basic operation (which we call
$\ell_\infty$-dist neuron), and show that any neural network constructed with
$\ell_\infty$-dist neurons (called $\ell_{\infty}$-dist net) is naturally a
1-Lipschitz function with respect to $\ell_\infty$-norm. This directly provides
a rigorous guarantee of the certified robustness based on the margin of
prediction outputs. We also prove that such networks have enough expressive
power to approximate any 1-Lipschitz function with robust generalization
guarantee. Our experimental results show that the proposed network is
promising. Using $\ell_{\infty}$-dist nets as the basic building blocks, we
consistently achieve state-of-the-art performance on commonly used datasets:
93.09% certified accuracy on MNIST ($\epsilon=0.3$), 79.23% on Fashion MNIST
($\epsilon=0.1$) and 35.10% on CIFAR-10 ($\epsilon=8/255$).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bohang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhou Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Joint Source Channel Coding for WirelessImage Transmission with OFDM. (arXiv:2101.03909v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03909</id>
        <link href="http://arxiv.org/abs/2101.03909"/>
        <updated>2021-05-25T01:56:09.758Z</updated>
        <summary type="html"><![CDATA[We present a deep learning based joint source channel coding (JSCC) scheme
for wireless image transmission over multipath fading channels with non-linear
signal clipping. The proposed encoder and decoder use convolutional neural
networks (CNN) and directly map the source images to complex-valued baseband
samples for orthogonal frequency division multiplexing (OFDM) transmission. The
proposed model-driven machine learning approach eliminates the need for
separate source and channel coding while integrating an OFDM datapath to cope
with multipath fading channels. The end-to-end JSCC communication system
combines trainable CNN layers with non-trainable but differentiable layers
representing the multipath channel model and OFDM signal processing blocks. Our
results show that injecting domain expert knowledge by incorporating OFDM
baseband processing blocks into the machine learning framework significantly
enhances the overall performance compared to an unstructured CNN. Our method
outperforms conventional schemes that employ state-of-the-art but separate
source and channel coding such as BPG and LDPC with OFDM. Moreover, our method
is shown to be robust against non-linear signal clipping in OFDM for various
channel conditions that do not match the model parameter used during the
training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mingyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bian_C/0/1/0/all/0/1"&gt;Chenghong Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hun-Seok Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Sub-Patterns in Piecewise Continuous Functions. (arXiv:2010.15571v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15571</id>
        <link href="http://arxiv.org/abs/2010.15571"/>
        <updated>2021-05-25T01:56:09.734Z</updated>
        <summary type="html"><![CDATA[Most stochastic gradient descent algorithms can optimize neural networks that
are sub-differentiable in their parameters, which requires their activation
function to exhibit a degree of continuity. However, this continuity constraint
on the activation function prevents these neural models from uniformly
approximating discontinuous functions. This paper focuses on the case where the
discontinuities arise from distinct sub-patterns, each defined on different
parts of the input space. We propose a new discontinuous deep neural network
model trainable via a decoupled two-step procedure that avoids passing gradient
updates through the network's non-differentiable unit. We provide universal
approximation guarantees for our architecture in the space of bounded
continuous functions and in the space of piecewise continuous functions, which
we introduced herein. We present a novel semi-supervised two-step training
procedure for our discontinuous deep learning model, and we provide theoretical
support for its effectiveness. The performance of our architecture is evaluated
experimentally on two real-world datasets and one synthetic dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1"&gt;Anastasis Kratsios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamanlooy_B/0/1/0/all/0/1"&gt;Behnoosh Zamanlooy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConE: A Concurrent Edit Detection Tool for Large ScaleSoftware Development. (arXiv:2101.06542v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06542</id>
        <link href="http://arxiv.org/abs/2101.06542"/>
        <updated>2021-05-25T01:56:09.692Z</updated>
        <summary type="html"><![CDATA[Modern, complex software systems are being continuously extended and
adjusted. The developers responsible for this may come from different teams or
organizations, and may be distributed over the world. This may make it
difficult to keep track of what other developers are doing, which may result in
multiple developers concurrently editing the same code areas. This, in turn,
may lead to hard-to-merge changes or even merge conflicts, logical bugs that
are difficult to detect, duplication of work, and wasted developer
productivity. To address this, we explore the extent of this problem in the
pull request based software development model. We study half a year of changes
made to six large repositories in Microsoft in which at least 1,000 pull
requests are created each month. We find that files concurrently edited in
different pull requests are more likely to introduce bugs. Motivated by these
findings, we design, implement, and deploy a service named ConE (Concurrent
Edit Detector) that proactively detects pull requests containing concurrent
edits, to help mitigate the problems caused by them. ConE has been designed to
scale, and to minimize false alarms while still flagging relevant concurrently
edited files. Key concepts of ConE include the detection of the Extent of
Overlap between pull requests, and the identification of Rarely Concurrently
Edited Files. To evaluate ConE, we report on its operational deployment on 234
repositories inside Microsoft. ConE assessed 26,000 pull requests and made 775
recommendations about conflicting changes, which were rated as useful in over
70% (554) of the cases. From interviews with 48 users we learned that they
believed ConE would save time in conflict resolution and avoiding duplicate
work, and that over 90% intend to keep using the service on a daily basis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maddila_C/0/1/0/all/0/1"&gt;Chandra Maddila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagappan_N/0/1/0/all/0/1"&gt;Nachiappan Nagappan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bird_C/0/1/0/all/0/1"&gt;Christian Bird&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gousios_G/0/1/0/all/0/1"&gt;Georgios Gousios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1"&gt;Arie van Deursen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedScale: Benchmarking Model and System Performance of Federated Learning. (arXiv:2105.11367v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11367</id>
        <link href="http://arxiv.org/abs/2105.11367"/>
        <updated>2021-05-25T01:56:09.535Z</updated>
        <summary type="html"><![CDATA[We present FedScale, a diverse set of challenging and realistic benchmark
datasets to facilitate scalable, comprehensive, and reproducible federated
learning (FL) research. FedScale datasets are large-scale, encompassing a
diverse range of important FL tasks, such as image classification, object
detection, language modeling, speech recognition, and reinforcement learning.
For each dataset, we provide a unified evaluation protocol using realistic data
splits and evaluation metrics. To meet the pressing need for reproducing
realistic FL at scale, we have also built an efficient evaluation platform to
simplify and standardize the process of FL experimental setup and model
evaluation. Our evaluation platform provides flexible APIs to implement new FL
algorithms and include new execution backends with minimal developer efforts.
Finally, we perform indepth benchmark experiments on these datasets. Our
experiments suggest that FedScale presents significant challenges of
heterogeneity-aware co-optimizations of the system and statistical efficiency
under realistic FL characteristics, indicating fruitful opportunities for
future research. FedScale is open-source with permissive licenses and actively
maintained, and we welcome feedback and contributions from the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1"&gt;Fan Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yinwei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiangfeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Mosharaf Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across the Workspace. (arXiv:2105.11283v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.11283</id>
        <link href="http://arxiv.org/abs/2105.11283"/>
        <updated>2021-05-25T01:56:09.525Z</updated>
        <summary type="html"><![CDATA[When training control policies for robot manipulation via deep learning,
sim-to-real transfer can help satisfy the large data requirements. In this
paper, we study the problem of zero-shot sim-to-real when the task requires
both highly precise control, with sub-millimetre error tolerance, and full
workspace generalisation. Our framework involves a coarse-to-fine controller,
where trajectories initially begin with classical motion planning based on pose
estimation, and transition to an end-to-end controller which maps images to
actions and is trained in simulation with domain randomisation. In this way, we
achieve precise control whilst also generalising the controller across the
workspace and keeping the generality and robustness of vision-based, end-to-end
control. Real-world experiments on a range of different tasks show that, by
exploiting the best of both worlds, our framework significantly outperforms
purely motion planning methods, and purely learning-based methods. Furthermore,
we answer a range of questions on best practices for precise sim-to-real
transfer, such as how different image sensor modalities and image feature
representations perform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1"&gt;Eugene Valassakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1"&gt;Norman Di Palo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1"&gt;Edward Johns&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-stage Training for Learning from Label Proportions. (arXiv:2105.10635v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10635</id>
        <link href="http://arxiv.org/abs/2105.10635"/>
        <updated>2021-05-25T01:56:09.506Z</updated>
        <summary type="html"><![CDATA[Learning from label proportions (LLP) aims at learning an instance-level
classifier with label proportions in grouped training data. Existing deep
learning based LLP methods utilize end-to-end pipelines to obtain the
proportional loss with Kullback-Leibler divergence between the bag-level prior
and posterior class distributions. However, the unconstrained optimization on
this objective can hardly reach a solution in accordance with the given
proportions. Besides, concerning the probabilistic classifier, this strategy
unavoidably results in high-entropy conditional class distributions at the
instance level. These issues further degrade the performance of the
instance-level classification. In this paper, we regard these problems as noisy
pseudo labeling, and instead impose the strict proportion consistency on the
classifier with a constrained optimization as a continuous training stage for
existing LLP classifiers. In addition, we introduce the mixup strategy and
symmetric crossentropy to further reduce the label noise. Our framework is
model-agnostic, and demonstrates compelling performance improvement in
extensive experiments, when incorporated into other deep LLP models as a
post-hoc phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiabin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhiquan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yingjie Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[View Distillation with Unlabeled Data for Extracting Adverse Drug Effects from User-Generated Data. (arXiv:2105.11354v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11354</id>
        <link href="http://arxiv.org/abs/2105.11354"/>
        <updated>2021-05-25T01:56:09.499Z</updated>
        <summary type="html"><![CDATA[We present an algorithm based on multi-layer transformers for identifying
Adverse Drug Reactions (ADR) in social media data. Our model relies on the
properties of the problem and the characteristics of contextual word embeddings
to extract two views from documents. Then a classifier is trained on each view
to label a set of unlabeled documents to be used as an initializer for a new
classifier in the other view. Finally, the initialized classifier in each view
is further trained using the initial training examples. We evaluated our model
in the largest publicly available ADR dataset. The experiments testify that our
model significantly outperforms the transformer-based models pretrained on
domain-specific data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1"&gt;Payam Karisani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jinho D. Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Position-Sensing Graph Neural Networks: Proactively Learning Nodes Relative Positions. (arXiv:2105.11346v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11346</id>
        <link href="http://arxiv.org/abs/2105.11346"/>
        <updated>2021-05-25T01:56:09.490Z</updated>
        <summary type="html"><![CDATA[Most existing graph neural networks (GNNs) learn node embeddings using the
framework of message passing and aggregation. Such GNNs are incapable of
learning relative positions between graph nodes within a graph. To empower GNNs
with the awareness of node positions, some nodes are set as anchors. Then,
using the distances from a node to the anchors, GNNs can infer relative
positions between nodes. However, P-GNNs arbitrarily select anchors, leading to
compromising position-awareness and feature extraction. To eliminate this
compromise, we demonstrate that selecting evenly distributed and asymmetric
anchors is essential. On the other hand, we show that choosing anchors that can
aggregate embeddings of all the nodes within a graph is NP-hard. Therefore,
devising efficient optimal algorithms in a deterministic approach is
practically not feasible. To ensure position-awareness and bypass
NP-completeness, we propose Position-Sensing Graph Neural Networks (PSGNNs),
learning how to choose anchors in a back-propagatable fashion. Experiments
verify the effectiveness of PSGNNs against state-of-the-art GNNs, substantially
improving performance on various synthetic and real-world graph datasets while
enjoying stable scalability. Specifically, PSGNNs on average boost AUC more
than 14% for pairwise node classification and 18% for link prediction over the
existing state-of-the-art position-aware methods. Our source code is publicly
available at: https://github.com/ZhenyueQin/PSGNN]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhenyue Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1"&gt;Saeed Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dongwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1"&gt;Pan Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1"&gt;Tom Gedeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Watermarking using Diffusion of Logo into Autoencoder Feature Maps. (arXiv:2105.11095v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2105.11095</id>
        <link href="http://arxiv.org/abs/2105.11095"/>
        <updated>2021-05-25T01:56:09.406Z</updated>
        <summary type="html"><![CDATA[Digital contents have grown dramatically in recent years, leading to
increased attention to copyright. Image watermarking has been considered one of
the most popular methods for copyright protection. With the recent advancements
in applying deep neural networks in image processing, these networks have also
been used in image watermarking. Robustness and imperceptibility are two
challenging features of watermarking methods that the trade-off between them
should be satisfied. In this paper, we propose to use an end-to-end network for
watermarking. We use a convolutional neural network (CNN) to control the
embedding strength based on the image content. Dynamic embedding helps the
network to have the lowest effect on the visual quality of the watermarked
image. Different image processing attacks are simulated as a network layer to
improve the robustness of the model. Our method is a blind watermarking
approach that replicates the watermark string to create a matrix of the same
size as the input image. Instead of diffusing the watermark data into the input
image, we inject the data into the feature space and force the network to do
this in regions that increase the robustness against various attacks.
Experimental results show the superiority of the proposed method in terms of
imperceptibility and robustness compared to the state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jamali_M/0/1/0/all/0/1"&gt;Maedeh Jamali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1"&gt;Nader Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khadivi_P/0/1/0/all/0/1"&gt;Pejman Khadivi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirani_S/0/1/0/all/0/1"&gt;Shahram Shirani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1"&gt;Shadrokh Samavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Descent in Materio. (arXiv:2105.11233v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2105.11233</id>
        <link href="http://arxiv.org/abs/2105.11233"/>
        <updated>2021-05-25T01:56:09.386Z</updated>
        <summary type="html"><![CDATA[Deep learning, a multi-layered neural network approach inspired by the brain,
has revolutionized machine learning. One of its key enablers has been
backpropagation, an algorithm that computes the gradient of a loss function
with respect to the weights in the neural network model, in combination with
its use in gradient descent. However, the implementation of deep learning in
digital computers is intrinsically wasteful, with energy consumption becoming
prohibitively high for many applications. This has stimulated the development
of specialized hardware, ranging from neuromorphic CMOS integrated circuits and
integrated photonic tensor cores to unconventional, material-based computing
systems. The learning process in these material systems, taking place, e.g., by
artificial evolution or surrogate neural network modelling, is still a
complicated and time-consuming process. Here, we demonstrate an efficient and
accurate homodyne gradient extraction method for performing gradient descent on
the loss function directly in the material system. We demonstrate the method in
our recently developed dopant network processing units, where we readily
realize all Boolean gates. This shows that gradient descent can in principle be
fully implemented in materio using simple electronics, opening up the way to
autonomously learning material systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boon_M/0/1/0/all/0/1"&gt;Marcus N. Boon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Euler_H/0/1/0/all/0/1"&gt;Hans-Christian Ruiz Euler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ven_B/0/1/0/all/0/1"&gt;Bram van de Ven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibarra_U/0/1/0/all/0/1"&gt;Unai Alegre Ibarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bobbert_P/0/1/0/all/0/1"&gt;Peter A. Bobbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiel_W/0/1/0/all/0/1"&gt;Wilfred G. van der Wiel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized, Hybrid MAC Design with Reduced State Information Exchange for Low-Delay IoT Applications. (arXiv:2105.11213v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2105.11213</id>
        <link href="http://arxiv.org/abs/2105.11213"/>
        <updated>2021-05-25T01:56:09.373Z</updated>
        <summary type="html"><![CDATA[We consider a system of several collocated nodes sharing a time slotted
wireless channel, and seek a MAC that (i) provides low mean delay, (ii) has
distributed control (i.e., there is no central scheduler), and (iii) does not
require explicit exchange of state information or control signals. The design
of such MAC protocols must keep in mind the need for contention access at light
traffic, and scheduled access in heavy traffic, leading to the long-standing
interest in hybrid, adaptive MACs.

We first propose EZMAC, a simple extension of an existing decentralized,
hybrid MAC called ZMAC. Next, motivated by our results on delay and throughput
optimality in partially observed, constrained queuing networks, we develop
another decentralized MAC protocol that we term QZMAC. A method to improve the
short-term fairness of QZMAC is proposed and analysed, and the resulting
modified algorithm is shown to possess better fairness properties than QZMAC.
The theory developed to reduce delay is also shown to work %with different
traffic types (batch arrivals, for example) and even in the presence of
transmission errors and fast fading.

Extensions to handle time critical traffic (alarms, for example) and hidden
nodes are also discussed. Practical implementation issues, such as handling
Clear Channel Assessment (CCA) errors, are outlined. We implement and
demonstrate the performance of QZMAC on a test bed consisting of CC2420 based
Crossbow telosB motes, running the 6TiSCH communication stack on the Contiki
operating system over the 2.4GHz ISM band.

Finally, using simulations, we show that both protocols achieve mean delays
much lower than those achieved by ZMAC, and QZMAC provides mean delays very
close to the minimum achievable in this setting, i.e., that of the centralized
complete knowledge scheduler.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1"&gt;Avinash Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_A/0/1/0/all/0/1"&gt;Arpan Chattopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vatsa_S/0/1/0/all/0/1"&gt;Shivam Vinayak Vatsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Anurag Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Federated Learning by Balancing Communication Trade-Offs. (arXiv:2105.11028v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11028</id>
        <link href="http://arxiv.org/abs/2105.11028"/>
        <updated>2021-05-25T01:56:09.357Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has recently received a lot of attention for
large-scale privacy-preserving machine learning. However, high communication
overheads due to frequent gradient transmissions decelerate FL. To mitigate the
communication overheads, two main techniques have been studied: (i) local
update of weights characterizing the trade-off between communication and
computation and (ii) gradient compression characterizing the trade-off between
communication and precision. To the best of our knowledge, studying and
balancing those two trade-offs jointly and dynamically while considering their
impacts on convergence has remained unresolved even though it promises
significantly faster FL. In this paper, we first formulate our problem to
minimize learning error with respect to two variables: local update
coefficients and sparsity budgets of gradient compression who characterize
trade-offs between communication and computation/precision, respectively. We
then derive an upper bound of the learning error in a given wall-clock time
considering the interdependency between the two variables. Based on this
theoretical analysis, we propose an enhanced FL scheme, namely Fast FL (FFL),
that jointly and dynamically adjusts the two variables to minimize the learning
error. We demonstrate that FFL consistently achieves higher accuracies faster
than similar schemes existing in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nori_M/0/1/0/all/0/1"&gt;Milad Khademi Nori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Sangseok Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1"&gt;Il-Min Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Early Bird Catches the Worm: Better Early Life Cycle Defect Predictors. (arXiv:2105.11082v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2105.11082</id>
        <link href="http://arxiv.org/abs/2105.11082"/>
        <updated>2021-05-25T01:56:09.349Z</updated>
        <summary type="html"><![CDATA[Before researchers rush to reason across all available data, they should
first check if the information is densest within some small region. We say this
since, in 240 GitHub projects, we find that the information in that data
``clumps'' towards the earliest parts of the project. In fact, a defect
prediction model learned from just the first 150 commits works as well, or
better than state-of-the-art alternatives. Using just this early life cycle
data, we can build models very quickly (using weeks, not months, of CPU time).
Also, we can find simple models (with just two features) that generalize to
hundreds of software projects. Based on this experience, we warn that prior
work on generalizing software engineering defect prediction models may have
needlessly complicated an inherently simple process. Further, prior work that
focused on later-life cycle data now needs to be revisited since their
conclusions were drawn from relatively uninformative regions. Replication note:
all our data and scripts are online at
https://github.com/snaraya7/early-defect-prediction-tse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shrikanth_N/0/1/0/all/0/1"&gt;N.C. Shrikanth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1"&gt;Tim Menzies&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved OOD Generalization via Adversarial Training and Pre-training. (arXiv:2105.11144v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11144</id>
        <link href="http://arxiv.org/abs/2105.11144"/>
        <updated>2021-05-25T01:56:09.332Z</updated>
        <summary type="html"><![CDATA[Recently, learning a model that generalizes well on out-of-distribution (OOD)
data has attracted great attention in the machine learning community. In this
paper, after defining OOD generalization via Wasserstein distance, we
theoretically show that a model robust to input perturbation generalizes well
on OOD data. Inspired by previous findings that adversarial training helps
improve input-robustness, we theoretically show that adversarially trained
models have converged excess risk on OOD data, and empirically verify it on
both image classification and natural language understanding tasks. Besides, in
the paradigm of first pre-training and then fine-tuning, we theoretically show
that a pre-trained model that is more robust to input perturbation provides a
better initialization for generalization on downstream OOD data. Empirically,
after fine-tuning, this better-initialized model from adversarial pre-training
also has better OOD generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1"&gt;Mingyang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Lu Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiacheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1"&gt;Lifeng Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhi-Ming Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiFair: Multi-Group Fairness in Machine Learning. (arXiv:2105.11069v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11069</id>
        <link href="http://arxiv.org/abs/2105.11069"/>
        <updated>2021-05-25T01:56:09.325Z</updated>
        <summary type="html"><![CDATA[Algorithmic fairness is becoming increasingly important in data mining and
machine learning, and one of the most fundamental notions is group fairness.
The vast majority of the existing works on group fairness, with a few
exceptions, primarily focus on debiasing with respect to a single sensitive
attribute, despite the fact that the co-existence of multiple sensitive
attributes (e.g., gender, race, marital status, etc.) in the real-world is
commonplace. As such, methods that can ensure a fair learning outcome with
respect to all sensitive attributes of concern simultaneously need to be
developed. In this paper, we study multi-group fairness in machine learning
(MultiFair), where statistical parity, a representative group fairness measure,
is guaranteed among demographic groups formed by multiple sensitive attributes
of interest. We formulate it as a mutual information minimization problem and
propose a generic end-to-end algorithmic framework to solve it. The key idea is
to leverage a variational representation of mutual information, which considers
the variational distribution between learning outcomes and sensitive
attributes, as well as the density ratio between the variational and the
original distributions. Our proposed framework is generalizable to many
different settings, including other statistical notions of fairness, and could
handle any type of learning task equipped with a gradient-based optimizer.
Empirical evaluations in the fair classification task on three real-world
datasets demonstrate that our proposed framework can effectively debias the
classification results with minimal impact to the classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1"&gt;Jian Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tiankai Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xintao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maciejewski_R/0/1/0/all/0/1"&gt;Ross Maciejewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1"&gt;Hanghang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Artificial Intelligence Enabled Financial Crime Detection. (arXiv:2105.10866v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10866</id>
        <link href="http://arxiv.org/abs/2105.10866"/>
        <updated>2021-05-25T01:56:09.318Z</updated>
        <summary type="html"><![CDATA[Recently, financial institutes have been dealing with an increase in
financial crimes. In this context, financial services firms started to improve
their vigilance and use new technologies and approaches to identify and predict
financial fraud and crime possibilities. This task is challenging as
institutions need to upgrade their data and analytics capabilities to enable
new technologies such as Artificial Intelligence (AI) to predict and detect
financial crimes. In this paper, we put a step towards AI-enabled financial
crime detection in general and money laundering detection in particular to
address this challenge. We study and analyse the recent works done in financial
crime detection and present a novel model to detect money laundering cases with
minimum human intervention needs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rouhollahi_Z/0/1/0/all/0/1"&gt;Zeinab Rouhollahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dorylus: Affordable, Scalable, and Accurate GNN Training over Billion-Edge Graphs. (arXiv:2105.11118v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2105.11118</id>
        <link href="http://arxiv.org/abs/2105.11118"/>
        <updated>2021-05-25T01:56:09.311Z</updated>
        <summary type="html"><![CDATA[A graph neural network (GNN) enables deep learning on structured graph data.
There are two major GNN training obstacles: 1) it relies on high-end servers
with many GPUs which are expensive to purchase and maintain, and 2) limited
memory on GPUs cannot scale to today's billion-edge graphs. This paper presents
Dorylus: a distributed system for training GNNs. Uniquely, Dorylus can take
advantage of serverless computing to increase scalability at a low cost.

The key insight guiding our design is computation separation. Computation
separation makes it possible to construct a deep, bounded-asynchronous pipeline
where graph and tensor parallel tasks can fully overlap, effectively hiding the
network latency incurred by Lambdas. With the help of thousands of Lambda
threads, Dorylus scales GNN training to billion-edge graphs. Currently, for
large graphs, CPU servers offer the best performance-per-dollar over GPU
servers. Just using Lambdas on top of CPU servers offers up to 2.75x more
performance-per-dollar than training only with CPU servers. Concretely, Dorylus
is 1.22x faster and 4.83x cheaper than GPU servers for massive sparse graphs.
Dorylus is up to 3.8x faster and 10.7x cheaper compared to existing
sampling-based systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thorpe_J/0/1/0/all/0/1"&gt;John Thorpe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yifan Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eyolfson_J/0/1/0/all/0/1"&gt;Jonathan Eyolfson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1"&gt;Shen Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guanzhou Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zhihao Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jinliang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vora_K/0/1/0/all/0/1"&gt;Keval Vora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1"&gt;Ravi Netravali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Miryung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guoqing Harry Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Hawkes Processes for Discovering Time-evolving Communities' States behind Diffusion Processes. (arXiv:2105.11152v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.11152</id>
        <link href="http://arxiv.org/abs/2105.11152"/>
        <updated>2021-05-25T01:56:09.304Z</updated>
        <summary type="html"><![CDATA[Sequences of events including infectious disease outbreaks, social network
activities, and crimes are ubiquitous and the data on such events carry
essential information about the underlying diffusion processes between
communities (e.g., regions, online user groups). Modeling diffusion processes
and predicting future events are crucial in many applications including
epidemic control, viral marketing, and predictive policing. Hawkes processes
offer a central tool for modeling the diffusion processes, in which the
influence from the past events is described by the triggering kernel. However,
the triggering kernel parameters, which govern how each community is influenced
by the past events, are assumed to be static over time. In the real world, the
diffusion processes depend not only on the influences from the past, but also
the current (time-evolving) states of the communities, e.g., people's awareness
of the disease and people's current interests. In this paper, we propose a
novel Hawkes process model that is able to capture the underlying dynamics of
community states behind the diffusion processes and predict the occurrences of
events based on the dynamics. Specifically, we model the latent dynamic
function that encodes these hidden dynamics by a mixture of neural networks.
Then we design the triggering kernel using the latent dynamic function and its
integral. The proposed method, termed DHP (Dynamic Hawkes Processes), offers a
flexible way to learn complex representations of the time-evolving communities'
states, while at the same time it allows to computing the exact likelihood,
which makes parameter learning tractable. Extensive experiments on four
real-world event datasets show that DHP outperforms five widely adopted methods
for event prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okawa_M/0/1/0/all/0/1"&gt;Maya Okawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1"&gt;Tomoharu Iwata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_Y/0/1/0/all/0/1"&gt;Yusuke Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_H/0/1/0/all/0/1"&gt;Hiroyuki Toda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurashima_T/0/1/0/all/0/1"&gt;Takeshi Kurashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fuzzy inference system application for oil-water flow patterns identification. (arXiv:2105.11181v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2105.11181</id>
        <link href="http://arxiv.org/abs/2105.11181"/>
        <updated>2021-05-25T01:56:09.297Z</updated>
        <summary type="html"><![CDATA[With the continuous development of the petroleum industry, long-distance
transportation of oil and gas has been the norm. Due to gravity differentiation
in horizontal wells and highly deviated wells (non-vertical wells), the water
phase at the bottom of the pipeline will cause scaling and corrosion in the
pipeline. Scaling and corrosion will make the transportation process difficult,
and transportation costs will be considerably increased. Therefore, the study
of the oil-water two-phase flow pattern is of great importance to oil
production. In this paper, a fuzzy inference system is used to predict the flow
pattern of the fluid, get the prediction result, and compares it with the
prediction result of the BP neural network. From the comparison of the results,
we found that the prediction results of the fuzzy inference system are more
accurate and reliable than the prediction results of the BP neural network. At
the same time, it can realize real-time monitoring and has less error control.
Experimental results demonstrate that in the entire production logging process
of non-vertical wells, the use of a fuzzy inference system to predict fluid
flow patterns can greatly save production costs while ensuring the safe
operation of production equipment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuyan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Haimin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hongwei Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1"&gt;Rui Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2105.11166</id>
        <link href="http://arxiv.org/abs/2105.11166"/>
        <updated>2021-05-25T01:56:09.273Z</updated>
        <summary type="html"><![CDATA[State-of-the-art performance for many emerging edge applications is achieved
by deep neural networks (DNNs). Often, these DNNs are location and time
sensitive, and the parameters of a specific DNN must be delivered from an edge
server to the edge device rapidly and efficiently to carry out time-sensitive
inference tasks. We introduce AirNet, a novel training and analog transmission
method that allows efficient wireless delivery of DNNs. We first train the DNN
with noise injection to counter the wireless channel noise. We also employ
pruning to reduce the channel bandwidth necessary for transmission, and perform
knowledge distillation from a larger model to achieve satisfactory performance,
despite the channel perturbations. We show that AirNet achieves significantly
higher test accuracy compared to digital alternatives under the same bandwidth
and power constraints. It also exhibits graceful degradation with channel
quality, which reduces the requirement for accurate channel estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1"&gt;Mikolaj Jankowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1"&gt;Krystian Mikolajczyk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual World: A Robotic Benchmark For Continual Reinforcement Learning. (arXiv:2105.10919v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10919</id>
        <link href="http://arxiv.org/abs/2105.10919"/>
        <updated>2021-05-25T01:56:09.264Z</updated>
        <summary type="html"><![CDATA[Continual learning (CL) -- the ability to continuously learn, building on
previously acquired knowledge -- is a natural requirement for long-lived
autonomous reinforcement learning (RL) agents. While building such agents, one
needs to balance opposing desiderata, such as constraints on capacity and
compute, the ability to not catastrophically forget, and to exhibit positive
transfer on new tasks. Understanding the right trade-off is conceptually and
computationally challenging, which we argue has led the community to overly
focus on catastrophic forgetting. In response to these issues, we advocate for
the need to prioritize forward transfer and propose Continual World, a
benchmark consisting of realistic and meaningfully diverse robotic tasks built
on top of Meta-World as a testbed. Following an in-depth empirical evaluation
of existing CL methods, we pinpoint their limitations and highlight unique
algorithmic challenges in the RL setting. Our benchmark aims to provide a
meaningful and computationally inexpensive challenge for the community and thus
help better understand the performance of existing and future solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolczyk_M/0/1/0/all/0/1"&gt;Maciej Wo&amp;#x142;czyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zajac_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Zaj&amp;#x105;c&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kucinski_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Kuci&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1"&gt;Piotr Mi&amp;#x142;o&amp;#x15b;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed CNN Inference on Resource-Constrained UAVs for Surveillance Systems: Design and Optimization. (arXiv:2105.11013v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2105.11013</id>
        <link href="http://arxiv.org/abs/2105.11013"/>
        <updated>2021-05-25T01:56:09.257Z</updated>
        <summary type="html"><![CDATA[Unmanned Aerial Vehicles (UAVs) have attracted great interest in the last few
years owing to their ability to cover large areas and access difficult and
hazardous target zones, which is not the case of traditional systems relying on
direct observations obtained from fixed cameras and sensors. Furthermore,
thanks to the advancements in computer vision and machine learning, UAVs are
being adopted for a broad range of solutions and applications. However, Deep
Neural Networks (DNNs) are progressing toward deeper and complex models that
prevent them from being executed on-board. In this paper, we propose a DNN
distribution methodology within UAVs to enable data classification in
resource-constrained devices and avoid extra delays introduced by the
server-based solutions due to data communication over air-to-ground links. The
proposed method is formulated as an optimization problem that aims to minimize
the latency between data collection and decision-making while considering the
mobility model and the resource constraints of the UAVs as part of the
air-to-air communication. We also introduce the mobility prediction to adapt
our system to the dynamics of UAVs and the network variation. The simulation
conducted to evaluate the performance and benchmark the proposed methods,
namely Optimal UAV-based Layer Distribution (OULD) and OULD with Mobility
Prediction (OULD-MP), were run in an HPC cluster. The obtained results show
that our optimization solution outperforms the existing and heuristic-based
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jouhari_M/0/1/0/all/0/1"&gt;Mohammed Jouhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Ali_A/0/1/0/all/0/1"&gt;Abdulla Al-Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1"&gt;Emna Baccour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1"&gt;Mohsen Guizani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1"&gt;Mounir Hamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EXoN: EXplainable encoder Network. (arXiv:2105.10867v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.10867</id>
        <link href="http://arxiv.org/abs/2105.10867"/>
        <updated>2021-05-25T01:56:09.247Z</updated>
        <summary type="html"><![CDATA[We propose a new semi-supervised learning method of Variational AutoEncoder
(VAE) which yields explainable latent space by EXplainable encoder Network
(EXoN). The EXoN provides two useful tools for implementing VAE. First, we can
freely assign a conceptual center of latent distribution for a specific label.
We separate the latent space of VAE with multi-modal property of the Gaussian
mixture distribution according to labels of observations. Next, we can easily
investigate the latent subspace by a simple statistics, known as
$F$-statistics, obtained from the EXoN. We found that both negative
cross-entropy and Kullback-Leibler divergence play a crucial role in
constructing explainable latent space and the variability of the generated
samples from our proposed model depends on a specific subspace, called
`activated latent subspace'. With MNIST and CIFAR-10 dataset, we show that the
EXoN can produce explainable latent space which effectively represents labels
and characteristics of the images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+An_S/0/1/0/all/0/1"&gt;SeungHwan An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jeon_J/0/1/0/all/0/1"&gt;Jong-June Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Choi_H/0/1/0/all/0/1"&gt;Hosik Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding. (arXiv:2105.10912v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10912</id>
        <link href="http://arxiv.org/abs/2105.10912"/>
        <updated>2021-05-25T01:56:09.230Z</updated>
        <summary type="html"><![CDATA[Scientific document understanding is challenging as the data is highly domain
specific and diverse. However, datasets for tasks with scientific text require
expensive manual annotation and tend to be small and limited to only one or a
few fields. At the same time, scientific documents contain many potential
training signals, such as citations, which can be used to build large labelled
datasets. Given this, we present an in-depth study of cite-worthiness detection
in English, where a sentence is labelled for whether or not it cites an
external source. To accomplish this, we introduce CiteWorth, a large,
contextualized, rigorously cleaned labelled dataset for cite-worthiness
detection built from a massive corpus of extracted plain-text scientific
documents. We show that CiteWorth is high-quality, challenging, and suitable
for studying problems such as domain adaptation. Our best performing
cite-worthiness detection model is a paragraph-level contextualized sentence
labelling model based on Longformer, exhibiting a 5 F1 point improvement over
SciBERT which considers only individual sentences. Finally, we demonstrate that
language model fine-tuning with cite-worthiness as a secondary task leads to
improved performance on downstream scientific document understanding tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1"&gt;Dustin Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOK: Fake News Outbreak 2021: Can We Stop the Viral Spread?. (arXiv:2105.10671v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.10671</id>
        <link href="http://arxiv.org/abs/2105.10671"/>
        <updated>2021-05-25T01:56:09.222Z</updated>
        <summary type="html"><![CDATA[Social Networks' omnipresence and ease of use has revolutionized the
generation and distribution of information in today's world. However, easy
access to information does not equal an increased level of public knowledge.
Unlike traditional media channels, social networks also facilitate faster and
wider spread of disinformation and misinformation. Viral spread of false
information has serious implications on the behaviors, attitudes and beliefs of
the public, and ultimately can seriously endanger the democratic processes.
Limiting false information's negative impact through early detection and
control of extensive spread presents the main challenge facing researchers
today. In this survey paper, we extensively analyze a wide range of different
solutions for the early detection of fake news in the existing literature. More
precisely, we examine Machine Learning (ML) models for the identification and
classification of fake news, online fake news detection competitions,
statistical outputs as well as the advantages and disadvantages of some of the
available data sets. Finally, we evaluate the online web browsing tools
available for detecting and mitigating fake news and present some open research
challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1"&gt;Tanveer Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michalas_A/0/1/0/all/0/1"&gt;Antonis Michalas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhunzada_A/0/1/0/all/0/1"&gt;Adnan Akhunzada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Type-TD-TSR -- Extracting Tables from Document Images using a Multi-stage Pipeline for Table Detection and Table Structure Recognition: from OCR to Structured Table Representations. (arXiv:2105.11021v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11021</id>
        <link href="http://arxiv.org/abs/2105.11021"/>
        <updated>2021-05-25T01:56:09.213Z</updated>
        <summary type="html"><![CDATA[As global trends are shifting towards data-driven industries, the demand for
automated algorithms that can convert digital images of scanned documents into
machine readable information is rapidly growing. Besides the opportunity of
data digitization for the application of data analytic tools, there is also a
massive improvement towards automation of processes, which previously would
require manual inspection of the documents. Although the introduction of
optical character recognition technologies mostly solved the task of converting
human-readable characters from images into machine-readable characters, the
task of extracting table semantics has been less focused on over the years. The
recognition of tables consists of two main tasks, namely table detection and
table structure recognition. Most prior work on this problem focuses on either
task without offering an end-to-end solution or paying attention to real
application conditions like rotated images or noise artefacts inside the
document image. Recent work shows a clear trend towards deep learning
approaches coupled with the use of transfer learning for the task of table
structure recognition due to the lack of sufficiently large datasets. In this
paper we present a multistage pipeline named Multi-Type-TD-TSR, which offers an
end-to-end solution for the problem of table recognition. It utilizes
state-of-the-art deep learning models for table detection and differentiates
between 3 different types of tables based on the tables' borders. For the table
structure recognition we use a deterministic non-data driven algorithm, which
works on all table types. We additionally present two algorithms. One for
unbordered tables and one for bordered tables, which are the base of the used
table structure recognition algorithm. We evaluate Multi-Type-TD-TSR on the
ICDAR 2019 table structure recognition dataset and achieve a new
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_P/0/1/0/all/0/1"&gt;Pascal Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smajic_A/0/1/0/all/0/1"&gt;Alen Smajic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehler_A/0/1/0/all/0/1"&gt;Alexander Mehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abrami_G/0/1/0/all/0/1"&gt;Giuseppe Abrami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Baseline Values for Shapley Values. (arXiv:2105.10719v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10719</id>
        <link href="http://arxiv.org/abs/2105.10719"/>
        <updated>2021-05-25T01:56:09.205Z</updated>
        <summary type="html"><![CDATA[This paper aims to formulate the problem of estimating the optimal baseline
values for the Shapley value in game theory. The Shapley value measures the
attribution of each input variable of a complex model, which is computed as the
marginal benefit from the presence of this variable w.r.t.its absence under
different contexts. To this end, people usually set the input variable to its
baseline value to represent the absence of this variable (i.e.the no-signal
state of this variable). Previous studies usually determine the baseline values
in an empirical manner, which hurts the trustworthiness of the Shapley value.
In this paper, we revisit the feature representation of a deep model from the
perspective of game theory, and define the multi-variate interaction patterns
of input variables to define the no-signal state of an input variable. Based on
the multi-variate interaction, we learn the optimal baseline value of each
input variable. Experimental results have demonstrated the effectiveness of our
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhanpeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qirui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Graph Learning -- A Position Paper. (arXiv:2105.11099v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11099</id>
        <link href="http://arxiv.org/abs/2105.11099"/>
        <updated>2021-05-25T01:56:09.198Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNN) have been successful in many fields, and derived
various researches and applications in real industries. However, in some
privacy sensitive scenarios (like finance, healthcare), training a GNN model
centrally faces challenges due to the distributed data silos. Federated
learning (FL) is a an emerging technique that can collaboratively train a
shared model while keeping the data decentralized, which is a rational solution
for distributed GNN training. We term it as federated graph learning (FGL).
Although FGL has received increasing attention recently, the definition and
challenges of FGL is still up in the air. In this position paper, we present a
categorization to clarify it. Considering how graph data are distributed among
clients, we propose four types of FGL: inter-graph FL, intra-graph FL and
graph-structured FL, where intra-graph is further divided into horizontal and
vertical FGL. For each type of FGL, we make a detailed discussion about the
formulation and applications, and propose some potential challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanding Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1"&gt;Tao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Mingyang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chao Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyFed: A Hybrid Federated Framework for Privacy-preserving Machine Learning. (arXiv:2105.10545v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10545</id>
        <link href="http://arxiv.org/abs/2105.10545"/>
        <updated>2021-05-25T01:56:09.192Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) enables multiple clients to jointly train a global
model under the coordination of a central server. Although FL is a
privacy-aware paradigm, where raw data sharing is not required, recent studies
have shown that FL might leak the private data of a client through the model
parameters shared with the server or the other clients. In this paper, we
present the HyFed framework, which enhances the privacy of FL while preserving
the utility of the global model. HyFed provides developers with a generic API
to develop federated, privacy-preserving algorithms. HyFed supports both
simulation and federated operation modes and its source code is publicly
available at https://github.com/tum-aimed/hyfed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasirigerdeh_R/0/1/0/all/0/1"&gt;Reza Nasirigerdeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torkzadehmahani_R/0/1/0/all/0/1"&gt;Reihaneh Torkzadehmahani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matschinske_J/0/1/0/all/0/1"&gt;Julian Matschinske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumbach_J/0/1/0/all/0/1"&gt;Jan Baumbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments. (arXiv:2105.10937v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10937</id>
        <link href="http://arxiv.org/abs/2105.10937"/>
        <updated>2021-05-25T01:56:09.185Z</updated>
        <summary type="html"><![CDATA[Terrain traversability analysis plays a major role in ensuring safe robotic
navigation in unstructured environments. However, real-time constraints
frequently limit the accuracy of online tests, especially in scenarios where
realistic robot-terrain interactions are complex to model. In this context, we
propose a deep learning framework, trained in an end-to-end fashion from
elevation maps and trajectories, to estimate the occurrence of failure events.
The network is first trained and tested in simulation over synthetic maps
generated by the OpenSimplex algorithm. The prediction performance of the Deep
Learning framework is illustrated by being able to retain over 94% recall of
the original simulator at 30% of the computational time. Finally, the network
is transferred and tested on real elevation maps collected by the SEEKER
consortium during the Martian rover test trial in the Atacama desert in Chile.
We show that transferring and fine-tuning of an application-independent
pre-trained model retains better performance than training uniquely on scarcely
available real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Visca_M/0/1/0/all/0/1"&gt;Marco Visca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1"&gt;Sampo Kuutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1"&gt;Roger Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1"&gt;Saber Fallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-level camera-LiDAR fusion for 3D object detection with machine learning. (arXiv:2105.11060v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11060</id>
        <link href="http://arxiv.org/abs/2105.11060"/>
        <updated>2021-05-25T01:56:09.178Z</updated>
        <summary type="html"><![CDATA[This paper tackles the 3D object detection problem, which is of vital
importance for applications such as autonomous driving. Our framework uses a
Machine Learning (ML) pipeline on a combination of monocular camera and LiDAR
data to detect vehicles in the surrounding 3D space of a moving platform. It
uses frustum region proposals generated by State-Of-The-Art (SOTA) 2D object
detectors to segment LiDAR point clouds into point clusters which represent
potentially individual objects. We evaluate the performance of classical ML
algorithms as part of an holistic pipeline for estimating the parameters of 3D
bounding boxes which surround the vehicles around the moving platform. Our
results demonstrate an efficient and accurate inference on a validation set,
achieving an overall accuracy of 87.1%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salazar_Gomez_G/0/1/0/all/0/1"&gt;Gustavo A. Salazar-Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saavedra_Ruiz_M/0/1/0/all/0/1"&gt;Miguel A. Saavedra-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_Cano_V/0/1/0/all/0/1"&gt;Victor A. Romero-Cano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularization Can Help Mitigate Poisoning Attacks... with the Right Hyperparameters. (arXiv:2105.10948v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10948</id>
        <link href="http://arxiv.org/abs/2105.10948"/>
        <updated>2021-05-25T01:56:09.171Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms are vulnerable to poisoning attacks, where a
fraction of the training data is manipulated to degrade the algorithms'
performance. We show that current approaches, which typically assume that
regularization hyperparameters remain constant, lead to an overly pessimistic
view of the algorithms' robustness and of the impact of regularization. We
propose a novel optimal attack formulation that considers the effect of the
attack on the hyperparameters, modelling the attack as a \emph{minimax bilevel
optimization problem}. This allows to formulate optimal attacks, select
hyperparameters and evaluate robustness under worst case conditions. We apply
this formulation to logistic regression using $L_2$ regularization, empirically
show the limitations of previous strategies and evidence the benefits of using
$L_2$ regularization to dampen the effect of poisoning attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carnerero_Cano_J/0/1/0/all/0/1"&gt;Javier Carnerero-Cano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munoz_Gonzalez_L/0/1/0/all/0/1"&gt;Luis Mu&amp;#xf1;oz-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spencer_P/0/1/0/all/0/1"&gt;Phillippa Spencer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1"&gt;Emil C. Lupu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based Reinforcement Learning for Real-Time UAV Semantic Communication. (arXiv:2105.10716v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2105.10716</id>
        <link href="http://arxiv.org/abs/2105.10716"/>
        <updated>2021-05-25T01:56:09.138Z</updated>
        <summary type="html"><![CDATA[In this article, we study the problem of air-to-ground ultra-reliable and
low-latency communication (URLLC) for a moving ground user. This is done by
controlling multiple unmanned aerial vehicles (UAVs) in real time while
avoiding inter-UAV collisions. To this end, we propose a novel multi-agent deep
reinforcement learning (MADRL) framework, coined a graph attention exchange
network (GAXNet). In GAXNet, each UAV constructs an attention graph locally
measuring the level of attention to its neighboring UAVs, while exchanging the
attention weights with other UAVs so as to reduce the attention mismatch
between them. Simulation results corroborates that GAXNet achieves up to 4.5x
higher rewards during training. At execution, without incurring inter-UAV
collisions, GAXNet achieves 6.5x lower latency with the target 0.0000001 error
rate, compared to a state-of-the-art baseline framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1"&gt;Won Joon Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1"&gt;Byungju Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1"&gt;Soyi Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1"&gt;Young-Chai Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jihong Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Joongheon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Pruning for Recurrent Neural Networks. (arXiv:2105.10832v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.10832</id>
        <link href="http://arxiv.org/abs/2105.10832"/>
        <updated>2021-05-25T01:56:09.114Z</updated>
        <summary type="html"><![CDATA[Pruning techniques for neural networks with a recurrent architecture, such as
the recurrent neural network (RNN), are strongly desired for their application
to edge-computing devices. However, the recurrent architecture is generally not
robust to pruning because even small pruning causes accumulation error and the
total error increases significantly over time. In this paper, we propose an
appropriate pruning algorithm for RNNs inspired by "spectral pruning", and
provide the generalization error bounds for compressed RNNs. We also provide
numerical experiments to demonstrate our theoretical results and show the
effectiveness of our pruning method compared with existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Furuya_T/0/1/0/all/0/1"&gt;Takashi Furuya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suetake_K/0/1/0/all/0/1"&gt;Kazuma Suetake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Taniguchi_K/0/1/0/all/0/1"&gt;Koichi Taniguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kusumoto_H/0/1/0/all/0/1"&gt;Hiroyuki Kusumoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Saiin_R/0/1/0/all/0/1"&gt;Ryuji Saiin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Daimon_T/0/1/0/all/0/1"&gt;Tomohiro Daimon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressing Heavy-Tailed Weight Matrices for Non-Vacuous Generalization Bounds. (arXiv:2105.11025v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11025</id>
        <link href="http://arxiv.org/abs/2105.11025"/>
        <updated>2021-05-25T01:56:09.089Z</updated>
        <summary type="html"><![CDATA[Heavy-tailed distributions have been studied in statistics, random matrix
theory, physics, and econometrics as models of correlated systems, among other
domains. Further, heavy-tail distributed eigenvalues of the covariance matrix
of the weight matrices in neural networks have been shown to empirically
correlate with test set accuracy in several works (e.g. arXiv:1901.08276), but
a formal relationship between heavy-tail distributed parameters and
generalization bounds was yet to be demonstrated. In this work, the compression
framework of arXiv:1802.05296 is utilized to show that matrices with heavy-tail
distributed matrix elements can be compressed, resulting in networks with
sparse weight matrices. Since the parameter count has been reduced to a sum of
the non-zero elements of sparse matrices, the compression framework allows us
to bound the generalization gap of the resulting compressed network with a
non-vacuous generalization bound. Further, the action of these matrices on a
vector is discussed, and how they may relate to compression and resilient
classification is analyzed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;John Y. Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embedding Information onto a Dynamical System. (arXiv:2105.10766v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2105.10766</id>
        <link href="http://arxiv.org/abs/2105.10766"/>
        <updated>2021-05-25T01:56:09.081Z</updated>
        <summary type="html"><![CDATA[The celebrated Takens' embedding theorem concerns embedding an attractor of a
dynamical system in a Euclidean space of appropriate dimension through a
generic delay-observation map. The embedding also establishes a topological
conjugacy. In this paper, we show how an arbitrary sequence can be mapped into
another space as an attractive solution of a nonautonomous dynamical system.
Such mapping also entails a topological conjugacy and an embedding between the
sequence and the attractive solution spaces. This result is not a
generalization of Takens embedding theorem but helps us understand what exactly
is required by discrete-time state space models widely used in applications to
embed an external stimulus onto its solution space. Our results settle another
basic problem concerning the perturbation of an autonomous dynamical system. We
describe what exactly happens to the dynamics when exogenous noise perturbs
continuously a local irreducible attracting set (such as a stable fixed point)
of a discrete-time autonomous dynamical system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Manjunath_G/0/1/0/all/0/1"&gt;G Manjunath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly. (arXiv:2105.10762v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10762</id>
        <link href="http://arxiv.org/abs/2105.10762"/>
        <updated>2021-05-25T01:56:09.075Z</updated>
        <summary type="html"><![CDATA[The learning rate (LR) schedule is one of the most important hyper-parameters
needing careful tuning in training DNNs. However, it is also one of the least
automated parts of machine learning systems and usually costs significant
manual effort and computing. Though there are pre-defined LR schedules and
optimizers with adaptive LR, they introduce new hyperparameters that need to be
tuned separately for different tasks/datasets. In this paper, we consider the
question: Can we automatically tune the LR over the course of training without
human involvement? We propose an efficient method, AutoLRS, which automatically
optimizes the LR for each training stage by modeling training dynamics. AutoLRS
aims to find an LR applied to every $\tau$ steps that minimizes the resulted
validation loss. We solve this black-box optimization on the fly by Bayesian
optimization (BO). However, collecting training instances for BO requires a
system to evaluate each LR queried by BO's acquisition function for $\tau$
steps, which is prohibitively expensive in practice. Instead, we apply each
candidate LR for only $\tau'\ll\tau$ steps and train an exponential model to
predict the validation loss after $\tau$ steps. This mutual-training process
between BO and the loss-prediction model allows us to limit the training steps
invested in the BO search. We demonstrate the advantages and the generality of
AutoLRS through extensive experiments of training DNNs for tasks from diverse
domains using different optimizers. The LR schedules auto-generated by AutoLRS
lead to a speedup of $1.22\times$, $1.43\times$, and $1.5\times$ when training
ResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in
their original papers, and an average speedup of $1.31\times$ over
state-of-the-art heavily-tuned LR schedules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yuchen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liangyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yibo Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chuanxiong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canini_M/0/1/0/all/0/1"&gt;Marco Canini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1"&gt;Arvind Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inclusion of Domain-Knowledge into GNNs using Mode-Directed Inverse Entailment. (arXiv:2105.10709v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10709</id>
        <link href="http://arxiv.org/abs/2105.10709"/>
        <updated>2021-05-25T01:56:09.068Z</updated>
        <summary type="html"><![CDATA[We present a general technique for constructing Graph Neural Networks (GNNs)
capable of using multi-relational domain knowledge. The technique is based on
mode-directed inverse entailment (MDIE) developed in Inductive Logic
Programming (ILP). Given a data instance $e$ and background knowledge $B$, MDIE
identifies a most-specific logical formula $\bot_B(e)$ that contains all the
relational information in $B$ that is related to $e$. We transform $\bot_B(e)$
into a corresponding "bottom-graph" that can be processed for use by standard
GNN implementations. This transformation allows a principled way of
incorporating generic background knowledge into GNNs: we use the term `BotGNN'
for this form of graph neural networks. For several GNN variants, using
real-world datasets with substantial background knowledge, we show that BotGNNs
perform significantly better than both GNNs without background knowledge and a
recently proposed simplified technique for including domain knowledge into
GNNs. We also provide experimental evidence comparing BotGNNs favourably to
multi-layer perceptrons (MLPs) that use features representing a
"propositionalised" form of the background knowledge; and BotGNNs to a standard
ILP based on the use of most-specific clauses. Taken together, these results
point to BotGNNs as capable of combining the computational efficacy of GNNs
with the representational versatility of ILP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1"&gt;Tirtharaj Dash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1"&gt;Ashwin Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baskar_A/0/1/0/all/0/1"&gt;A Baskar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hater-O-Genius Aggression Classification using Capsule Networks. (arXiv:2105.11219v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11219</id>
        <link href="http://arxiv.org/abs/2105.11219"/>
        <updated>2021-05-25T01:56:09.061Z</updated>
        <summary type="html"><![CDATA[Contending hate speech in social media is one of the most challenging social
problems of our time. There are various types of anti-social behavior in social
media. Foremost of them is aggressive behavior, which is causing many social
issues such as affecting the social lives and mental health of social media
users. In this paper, we propose an end-to-end ensemble-based architecture to
automatically identify and classify aggressive tweets. Tweets are classified
into three categories - Covertly Aggressive, Overtly Aggressive, and
Non-Aggressive. The proposed architecture is an ensemble of smaller subnetworks
that are able to characterize the feature embeddings effectively. We
demonstrate qualitatively that each of the smaller subnetworks is able to learn
unique features. Our best model is an ensemble of Capsule Networks and results
in a 65.2% F1 score on the Facebook test set, which results in a performance
gain of 0.95% over the TRAC-2018 winners. The code and the model weights are
publicly available at
https://github.com/parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1"&gt;Parth Patwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+PYKL_S/0/1/0/all/0/1"&gt;Srinivas PYKL&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Amitava Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_P/0/1/0/all/0/1"&gt;Prerana Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulabaigari_V/0/1/0/all/0/1"&gt;Viswanath Pulabaigari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pulmonary embolism identification in computerized tomography pulmonary angiography scans with deep learning technologies in COVID-19 patients. (arXiv:2105.11187v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.11187</id>
        <link href="http://arxiv.org/abs/2105.11187"/>
        <updated>2021-05-25T01:56:09.038Z</updated>
        <summary type="html"><![CDATA[The main objective of this work is to utilize state-of-the-art deep learning
approaches for the identification of pulmonary embolism in CTPA-Scans for
COVID-19 patients, provide an initial assessment of their performance and,
ultimately, provide a fast-track prototype solution (system). We adopted and
assessed some of the most popular convolutional neural network architectures
through transfer learning approaches, to strive to combine good model accuracy
with fast training. Additionally, we exploited one of the most popular
one-stage object detection models for the localization (through object
detection) of the pulmonary embolism regions-of-interests. The models of both
approaches are trained on an original CTPA-Scan dataset, where we annotated of
673 CTPA-Scan images with 1,465 bounding boxes in total, highlighting pulmonary
embolism regions-of-interests. We provide a brief assessment of some
state-of-the-art image classification models by achieving validation accuracies
of 91% in pulmonary embolism classification. Additionally, we achieved a
precision of about 68% on average in the object detection model for the
pulmonary embolism localization under 50% IoU threshold. For both approaches,
we provide the entire training pipelines for future studies (step by step
processes through source code). In this study, we present some of the most
accurate and fast deep learning models for pulmonary embolism identification in
CTPA-Scans images, through classification and localization (object detection)
approaches for patients infected by COVID-19. We provide a fast-track solution
(system) for the research community of the area, which combines both
classification and object detection models for improving the precision of
identifying pulmonary embolisms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kiourt_C/0/1/0/all/0/1"&gt;Chairi Kiourt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feretzakis_G/0/1/0/all/0/1"&gt;Georgios Feretzakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dalamarinis_K/0/1/0/all/0/1"&gt;Konstantinos Dalamarinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kalles_D/0/1/0/all/0/1"&gt;Dimitris Kalles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pantos_G/0/1/0/all/0/1"&gt;Georgios Pantos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Papadopoulos_I/0/1/0/all/0/1"&gt;Ioannis Papadopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kouris_S/0/1/0/all/0/1"&gt;Spyros Kouris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ioannakis_G/0/1/0/all/0/1"&gt;George Ioannakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Loupelis_E/0/1/0/all/0/1"&gt;Evangelos Loupelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sakagianni_A/0/1/0/all/0/1"&gt;Aikaterini Sakagianni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific Caching. (arXiv:2105.10554v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2105.10554</id>
        <link href="http://arxiv.org/abs/2105.10554"/>
        <updated>2021-05-25T01:56:09.030Z</updated>
        <summary type="html"><![CDATA[Analysis engines based on Graph Neural Networks (GNNs) are vital for many
real-world problems that model relationships using large graphs. Challenges for
a GNN hardware platform include the ability to (a) host a variety of GNNs, (b)
handle high sparsity in input node feature vectors and the graph adjacency
matrix and the accompanying random memory access patterns, and (c) maintain
load-balanced computation in the face of uneven workloads induced by high
sparsity and power-law vertex degree distributions in real datasets. The
proposes GNNIE, an accelerator designed to run a broad range of GNNs. It
tackles workload imbalance by (i) splitting node feature operands into blocks,
(ii) reordering and redistributing computations, and (iii) using a flexible MAC
architecture with low communication overheads among the processing elements. In
addition, it adopts a graph partitioning scheme and a graph-specific caching
policy that efficiently uses off-chip memory bandwidth that is well suited to
the characteristics of real-world graphs. Random memory access effects are
mitigated by partitioning and degree-aware caching to enable the reuse of
high-degree vertices. GNNIE achieves average speedups of over 8890x over a CPU
and 295x over a GPU over multiple datasets on graph attention networks (GATs),
graph convolutional networks (GCNs), GraphSAGE, GINConv, and DiffPool, Compared
to prior approaches, GNNIE achieves an average speedup of 9.74x over HyGCN for
GCN, GraphSAGE, and GINConv; HyGCN cannot implement GATs. GNNIE achieves an
average speedup of 2.28x over AWB-GCN (which runs only GCNs), despite using
3.4x fewer processing units.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1"&gt;Sudipta Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manasi_S/0/1/0/all/0/1"&gt;Susmita Dey Manasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunal_K/0/1/0/all/0/1"&gt;Kishor Kunal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapatnekar_S/0/1/0/all/0/1"&gt;Sachin S. Sapatnekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Amplification Via Bernoulli Sampling. (arXiv:2105.10594v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10594</id>
        <link href="http://arxiv.org/abs/2105.10594"/>
        <updated>2021-05-25T01:56:09.023Z</updated>
        <summary type="html"><![CDATA[Balancing privacy and accuracy is a major challenge in designing
differentially private machine learning algorithms. To improve this tradeoff,
prior work has looked at privacy amplification methods which analyze how common
training operations such as iteration and subsampling the data can lead to
higher privacy. In this paper, we analyze privacy amplification properties of a
new operation, sampling from the posterior, that is used in Bayesian inference.
In particular, we look at Bernoulli sampling from a posterior that is described
by a differentially private parameter. We provide an algorithm to compute the
amplification factor in this setting, and establish upper and lower bounds on
this factor. Finally, we look at what happens when we draw k posterior samples
instead of one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imola_J/0/1/0/all/0/1"&gt;Jacob Imola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1"&gt;Kamalika Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Bandit under Differential Privacy. (arXiv:2105.11126v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11126</id>
        <link href="http://arxiv.org/abs/2105.11126"/>
        <updated>2021-05-25T01:56:09.016Z</updated>
        <summary type="html"><![CDATA[This paper studies \emph{differential privacy (DP)} and \emph{local
differential privacy (LDP)} in cascading bandits. Under DP, we propose an
algorithm which guarantees $\epsilon$-indistinguishability and a regret of
$\mathcal{O}((\frac{\log T}{\epsilon})^{1+\xi})$ for an arbitrarily small
$\xi$. This is a significant improvement from the previous work of
$\mathcal{O}(\frac{\log^3 T}{\epsilon})$ regret. Under
($\epsilon$,$\delta$)-LDP, we relax the $K^2$ dependence through the tradeoff
between privacy budget $\epsilon$ and error probability $\delta$, and obtain a
regret of $\mathcal{O}(\frac{K\log (1/\delta) \log T}{\epsilon^2})$, where $K$
is the size of the arm subset. This result holds for both Gaussian mechanism
and Laplace mechanism by analyses on the composition. Our results extend to
combinatorial semi-bandit. We show respective lower bounds for DP and LDP
cascading bandits. Extensive experiments corroborate our theoretic findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Baoxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1"&gt;Shuo Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feasible Actor-Critic: Constrained Reinforcement Learning for Ensuring Statewise Safety. (arXiv:2105.10682v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10682</id>
        <link href="http://arxiv.org/abs/2105.10682"/>
        <updated>2021-05-25T01:56:09.003Z</updated>
        <summary type="html"><![CDATA[The safety constraints commonly used by existing safe reinforcement learning
(RL) methods are defined only on expectation of initial states, but allow each
certain state to be unsafe, which is unsatisfying for real-world
safety-critical tasks. In this paper, we introduce the feasible actor-critic
(FAC) algorithm, which is the first model-free constrained RL method that
considers statewise safety, e.g, safety for each initial state. We claim that
some states are inherently unsafe no matter what policy we choose, while for
other states there exist policies ensuring safety, where we say such states and
policies are feasible. By constructing a statewise Lagrange function available
on RL sampling and adopting an additional neural network to approximate the
statewise Lagrange multiplier, we manage to obtain the optimal feasible policy
which ensures safety for each feasible state and the safest possible policy for
infeasible states. Furthermore, the trained multiplier net can indicate whether
a given state is feasible or not through the statewise complementary slackness
condition. We provide theoretical guarantees that FAC outperforms previous
expectation-based constrained RL methods in terms of both constraint
satisfaction and reward optimization. Experimental results on both robot
locomotive tasks and safe exploration tasks verify the safety enhancement and
feasibility interpretation of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Haitong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yang Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shegnbo Eben Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangteng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Sifa Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianyu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GOALS: Gradient-Only Approximations for Line Searches Towards Robust and Consistent Training of Deep Neural Networks. (arXiv:2105.10915v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.10915</id>
        <link href="http://arxiv.org/abs/2105.10915"/>
        <updated>2021-05-25T01:56:08.997Z</updated>
        <summary type="html"><![CDATA[Mini-batch sub-sampling (MBSS) is favored in deep neural network training to
reduce the computational cost. Still, it introduces an inherent sampling error,
making the selection of appropriate learning rates challenging. The sampling
errors can manifest either as a bias or variances in a line search. Dynamic
MBSS re-samples a mini-batch at every function evaluation. Hence, dynamic MBSS
results in point-wise discontinuous loss functions with smaller bias but larger
variance than static sampled loss functions. However, dynamic MBSS has the
advantage of having larger data throughput during training but requires the
complexity regarding discontinuities to be resolved. This study extends the
gradient-only surrogate (GOS), a line search method using quadratic
approximation models built with only directional derivative information, for
dynamic MBSS loss functions. We propose a gradient-only approximation line
search (GOALS) with strong convergence characteristics with defined optimality
criterion. We investigate GOALS's performance by applying it on various
optimizers that include SGD, RMSprop and Adam on ResNet-18 and EfficientNetB0.
We also compare GOALS's against the other existing learning rate methods. We
quantify both the best performing and most robust algorithms. For the latter,
we introduce a relative robust criterion that allows us to quantify the
difference between an algorithm and the best performing algorithm for a given
problem. The results show that training a model with the recommended learning
rate for a class of search directions helps to reduce the model errors in
multimodal cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chae_Y/0/1/0/all/0/1"&gt;Younghwan Chae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wilke_D/0/1/0/all/0/1"&gt;Daniel N. Wilke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kafka_D/0/1/0/all/0/1"&gt;Dominic Kafka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embracing New Techniques in Deep Learning for Estimating Image Memorability. (arXiv:2105.10598v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10598</id>
        <link href="http://arxiv.org/abs/2105.10598"/>
        <updated>2021-05-25T01:56:08.978Z</updated>
        <summary type="html"><![CDATA[Various work has suggested that the memorability of an image is consistent
across people, and thus can be treated as an intrinsic property of an image.
Using computer vision models, we can make specific predictions about what
people will remember or forget. While older work has used now-outdated deep
learning architectures to predict image memorability, innovations in the field
have given us new techniques to apply to this problem. Here, we propose and
evaluate five alternative deep learning models which exploit developments in
the field from the last five years, largely the introduction of residual neural
networks, which are intended to allow the model to use semantic information in
the memorability estimation process. These new models were tested against the
prior state of the art with a combined dataset built to optimize both
within-category and across-category predictions. Our findings suggest that the
key prior memorability network had overstated its generalizability and was
overfit on its training set. Our new models outperform this prior model,
leading us to conclude that Residual Networks outperform simpler convolutional
neural networks in memorability regression. We make our new state-of-the-art
model readily available to the research community, allowing memory researchers
to make predictions about memorability on a wider range of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Needell_C/0/1/0/all/0/1"&gt;Coen D. Needell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bainbridge_W/0/1/0/all/0/1"&gt;Wilma A. Bainbridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-Training Sparsity-Aware Quantization. (arXiv:2105.11010v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.11010</id>
        <link href="http://arxiv.org/abs/2105.11010"/>
        <updated>2021-05-25T01:56:08.971Z</updated>
        <summary type="html"><![CDATA[Quantization is a technique used in deep neural networks (DNNs) to increase
execution performance and hardware efficiency. Uniform post-training
quantization (PTQ) methods are common, since they can be implemented
efficiently in hardware and do not require extensive hardware resources or a
training set. Mapping FP32 models to INT8 using uniform PTQ yields models with
negligible accuracy degradation; however, reducing precision below 8 bits with
PTQ is challenging, as accuracy degradation becomes noticeable, due to the
increase in quantization noise. In this paper, we propose a sparsity-aware
quantization (SPARQ) method, in which the unstructured and dynamic activation
sparsity is leveraged in different representation granularities. 4-bit
quantization, for example, is employed by dynamically examining the bits of
8-bit values and choosing a window of 4 bits, while first skipping zero-value
bits. Moreover, instead of quantizing activation-by-activation to 4 bits, we
focus on pairs of 8-bit activations and examine whether one of the two is equal
to zero. If one is equal to zero, the second can opportunistically use the
other's 4-bit budget; if both do not equal zero, then each is dynamically
quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation,
2x speedup over widely used hardware architectures, and a practical hardware
implementation. The code is available at https://github.com/gilshm/sparq.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shomron_G/0/1/0/all/0/1"&gt;Gil Shomron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gabbay_F/0/1/0/all/0/1"&gt;Freddy Gabbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurzum_S/0/1/0/all/0/1"&gt;Samer Kurzum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiser_U/0/1/0/all/0/1"&gt;Uri Weiser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combinatorial Blocking Bandits with Stochastic Delays. (arXiv:2105.10625v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10625</id>
        <link href="http://arxiv.org/abs/2105.10625"/>
        <updated>2021-05-25T01:56:08.964Z</updated>
        <summary type="html"><![CDATA[Recent work has considered natural variations of the multi-armed bandit
problem, where the reward distribution of each arm is a special function of the
time passed since its last pulling. In this direction, a simple (yet widely
applicable) model is that of blocking bandits, where an arm becomes unavailable
for a deterministic number of rounds after each play. In this work, we extend
the above model in two directions: (i) We consider the general combinatorial
setting where more than one arms can be played at each round, subject to
feasibility constraints. (ii) We allow the blocking time of each arm to be
stochastic. We first study the computational/unconditional hardness of the
above setting and identify the necessary conditions for the problem to become
tractable (even in an approximate sense). Based on these conditions, we provide
a tight analysis of the approximation guarantee of a natural greedy heuristic
that always plays the maximum expected reward feasible subset among the
available (non-blocked) arms. When the arms' expected rewards are unknown, we
adapt the above heuristic into a bandit algorithm, based on UCB, for which we
provide sublinear (approximate) regret guarantees, matching the theoretical
lower bounds in the limiting case of absence of delays.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atsidakou_A/0/1/0/all/0/1"&gt;Alexia Atsidakou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadigenopoulos_O/0/1/0/all/0/1"&gt;Orestis Papadigenopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1"&gt;Soumya Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caramanis_C/0/1/0/all/0/1"&gt;Constantine Caramanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novel Deep Learning Architecture for Heart Disease Prediction using Convolutional Neural Network. (arXiv:2105.10816v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10816</id>
        <link href="http://arxiv.org/abs/2105.10816"/>
        <updated>2021-05-25T01:56:08.957Z</updated>
        <summary type="html"><![CDATA[Healthcare is one of the most important aspects of human life. Heart disease
is known to be one of the deadliest diseases which is hampering the lives of
many people around the world. Heart disease must be detected early so the loss
of lives can be prevented. The availability of large-scale data for medical
diagnosis has helped developed complex machine learning and deep learning-based
models for automated early diagnosis of heart diseases. The classical
approaches have been limited in terms of not generalizing well to new data
which have not been seen in the training set. This is indicated by a large gap
in training and test accuracies. This paper proposes a novel deep learning
architecture using a 1D convolutional neural network for classification between
healthy and non-healthy persons to overcome the limitations of classical
approaches. Various clinical parameters are used for assessing the risk profile
in the patients which helps in early diagnosis. Various techniques are used to
avoid overfitting in the proposed network. The proposed network achieves over
97% training accuracy and 96% test accuracy on the dataset. The accuracy of the
model is compared in detail with other classification algorithms using various
performance parameters which proves the effectiveness of the proposed
architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1"&gt;Shadab Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barigidad_S/0/1/0/all/0/1"&gt;Susmith Barigidad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_S/0/1/0/all/0/1"&gt;Shadab Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suaib_M/0/1/0/all/0/1"&gt;Md Suaib&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Knee X-ray Report Generation. (arXiv:2105.10702v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10702</id>
        <link href="http://arxiv.org/abs/2105.10702"/>
        <updated>2021-05-25T01:56:08.951Z</updated>
        <summary type="html"><![CDATA[Gathering manually annotated images for the purpose of training a predictive
model is far more challenging in the medical domain than for natural images as
it requires the expertise of qualified radiologists. We therefore propose to
take advantage of past radiological exams (specifically, knee X-ray
examinations) and formulate a framework capable of learning the correspondence
between the images and reports, and hence be capable of generating diagnostic
reports for a given X-ray examination consisting of an arbitrary number of
image views. We demonstrate how aggregating the image features of individual
exams and using them as conditional inputs when training a language generation
model results in auto-generated exam reports that correlate well with
radiologist-generated reports.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gasimova_A/0/1/0/all/0/1"&gt;Aydan Gasimova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montana_G/0/1/0/all/0/1"&gt;Giovanni Montana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation and Analysis of Feature-Dependent Pseudo Noise for Training Deep Neural Networks. (arXiv:2105.10796v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10796</id>
        <link href="http://arxiv.org/abs/2105.10796"/>
        <updated>2021-05-25T01:56:08.929Z</updated>
        <summary type="html"><![CDATA[Training Deep neural networks (DNNs) on noisy labeled datasets is a
challenging problem, because learning on mislabeled examples deteriorates the
performance of the network. As the ground truth availability is limited with
real-world noisy datasets, previous papers created synthetic noisy datasets by
randomly modifying the labels of training examples of clean datasets. However,
no final conclusions can be derived by just using this random noise, since it
excludes feature-dependent noise. Thus, it is imperative to generate
feature-dependent noisy datasets that additionally provide ground truth.
Therefore, we propose an intuitive approach to creating feature-dependent noisy
datasets by utilizing the training predictions of DNNs on clean datasets that
also retain true label information. We refer to these datasets as "Pseudo Noisy
datasets". We conduct several experiments to establish that Pseudo noisy
datasets resemble feature-dependent noisy datasets across different conditions.
We further randomly generate synthetic noisy datasets with the same noise
distribution as that of Pseudo noise (referred as "Randomized Noise") to
empirically show that i) learning is easier with feature-dependent label noise
compared to random noise, ii) irrespective of noise distribution, Pseudo noisy
datasets mimic feature-dependent label noise and iii) current training methods
are not generalizable to feature-dependent label noise. Therefore, we believe
that Pseudo noisy datasets will be quite helpful to study and develop robust
training methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamabattula_S/0/1/0/all/0/1"&gt;Sree Ram Kamabattula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musini_K/0/1/0/all/0/1"&gt;Kumudha Musini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namazi_B/0/1/0/all/0/1"&gt;Babak Namazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_G/0/1/0/all/0/1"&gt;Ganesh Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devarajan_V/0/1/0/all/0/1"&gt;Venkat Devarajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DepressionNet: A Novel Summarization Boosted Deep Framework for Depression Detection on Social Media. (arXiv:2105.10878v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10878</id>
        <link href="http://arxiv.org/abs/2105.10878"/>
        <updated>2021-05-25T01:56:08.921Z</updated>
        <summary type="html"><![CDATA[Twitter is currently a popular online social media platform which allows
users to share their user-generated content. This publicly-generated user data
is also crucial to healthcare technologies because the discovered patterns
would hugely benefit them in several ways. One of the applications is in
automatically discovering mental health problems, e.g., depression. Previous
studies to automatically detect a depressed user on online social media have
largely relied upon the user behaviour and their linguistic patterns including
user's social interactions. The downside is that these models are trained on
several irrelevant content which might not be crucial towards detecting a
depressed user. Besides, these content have a negative impact on the overall
efficiency and effectiveness of the model. To overcome the shortcomings in the
existing automatic depression detection methods, we propose a novel
computational framework for automatic depression detection that initially
selects relevant content through a hybrid extractive and abstractive
summarization strategy on the sequence of all user tweets leading to a more
fine-grained and relevant content. The content then goes to our novel deep
learning framework comprising of a unified learning machinery comprising of
Convolutional Neural Network (CNN) coupled with attention-enhanced Gated
Recurrent Units (GRU) models leading to better empirical performance than
existing strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zogan_H/0/1/0/all/0/1"&gt;Hamad Zogan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razzak_I/0/1/0/all/0/1"&gt;Imran Razzak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jameel_S/0/1/0/all/0/1"&gt;Shoaib Jameel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guandong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Killing Two Birds with One Stone: Stealing Model and Inferring Attribute from BERT-based APIs. (arXiv:2105.10909v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.10909</id>
        <link href="http://arxiv.org/abs/2105.10909"/>
        <updated>2021-05-25T01:56:08.913Z</updated>
        <summary type="html"><![CDATA[The advances in pre-trained models (e.g., BERT, XLNET and etc) have largely
revolutionized the predictive performance of various modern natural language
processing tasks. This allows corporations to provide machine learning as a
service (MLaaS) by encapsulating fine-tuned BERT-based models as commercial
APIs. However, previous works have discovered a series of vulnerabilities in
BERT- based APIs. For example, BERT-based APIs are vulnerable to both model
extraction attack and adversarial example transferrability attack. However, due
to the high capacity of BERT-based APIs, the fine-tuned model is easy to be
overlearned, what kind of information can be leaked from the extracted model
remains unknown and is lacking. To bridge this gap, in this work, we first
present an effective model extraction attack, where the adversary can
practically steal a BERT-based API (the target/victim model) by only querying a
limited number of queries. We further develop an effective attribute inference
attack to expose the sensitive attribute of the training data used by the
BERT-based APIs. Our extensive experiments on benchmark datasets under various
realistic settings demonstrate the potential vulnerabilities of BERT-based
APIs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lingjuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuanli He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attacks and Mitigation for Anomaly Detectors of Cyber-Physical Systems. (arXiv:2105.10707v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.10707</id>
        <link href="http://arxiv.org/abs/2105.10707"/>
        <updated>2021-05-25T01:56:08.906Z</updated>
        <summary type="html"><![CDATA[The threats faced by cyber-physical systems (CPSs) in critical infrastructure
have motivated research into a multitude of attack detection mechanisms,
including anomaly detectors based on neural network models. The effectiveness
of anomaly detectors can be assessed by subjecting them to test suites of
attacks, but less consideration has been given to adversarial attackers that
craft noise specifically designed to deceive them. While successfully applied
in domains such as images and audio, adversarial attacks are much harder to
implement in CPSs due to the presence of other built-in defence mechanisms such
as rule checkers(or invariant checkers). In this work, we present an
adversarial attack that simultaneously evades the anomaly detectors and rule
checkers of a CPS. Inspired by existing gradient-based approaches, our
adversarial attack crafts noise over the sensor and actuator values, then uses
a genetic algorithm to optimise the latter, ensuring that the neural network
and the rule checking system are both deceived.We implemented our approach for
two real-world critical infrastructure testbeds, successfully reducing the
classification accuracy of their detectors by over 50% on average, while
simultaneously avoiding detection by rule checkers. Finally, we explore whether
these attacks can be mitigated by training the detectors on adversarial
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yifan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poskitt_C/0/1/0/all/0/1"&gt;Christopher M. Poskitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1"&gt;Sudipta Chattopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuqi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[V2V Spatiotemporal Interactive Pattern Recognition and Risk Analysis in Lane Changes. (arXiv:2105.10688v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2105.10688</id>
        <link href="http://arxiv.org/abs/2105.10688"/>
        <updated>2021-05-25T01:56:08.899Z</updated>
        <summary type="html"><![CDATA[In complex lane change (LC) scenarios, semantic interpretation and safety
analysis of dynamic interactive pattern are necessary for autonomous vehicles
to make appropriate decisions. This study proposes an unsupervised learning
framework that combines primitive-based interactive pattern recognition methods
and risk analysis methods. The Hidden Markov Model with the Gaussian mixture
model (GMM-HMM) approach is developed to decompose the LC scenarios into
primitives. Then the Dynamic Time Warping (DTW) distance based K-means
clustering is applied to gather the primitives to 13 types of interactive
patterns. Finally, this study considers two types of time-to-collision (TTC)
involved in the LC process as indicators to analyze the risk of the interactive
patterns and extract high-risk LC interactive patterns. The results obtained
from The Highway Drone Dataset (highD) demonstrate that the identified LC
interactive patterns contain interpretable semantic information. This study
explores the spatiotemporal evolution law and risk formation mechanism of the
LC interactive patterns and the findings are useful for comprehensively
understanding the latent interactive patterns, improving the rationality and
safety of autonomous vehicle's decision-making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yajie Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingtao Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10880</id>
        <link href="http://arxiv.org/abs/2105.10880"/>
        <updated>2021-05-25T01:56:08.875Z</updated>
        <summary type="html"><![CDATA[Climate change has largely impacted our daily lives. As one of its
consequences, we are experiencing more wildfires. In the year 2020, wildfires
burned a record number of 8,888,297 acres in the US. To awaken people's
attention to climate change, and to visualize the current risk of wildfires, We
developed RtFPS, "Real-Time Fire Prediction System". It provides a real-time
prediction visualization of wildfire risk at specific locations base on a
Machine Learning model. It also provides interactive map features that show the
historical wildfire events with environmental info.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1"&gt;Hermawan Mulyono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiyin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1"&gt;Desmond Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallelizing Contextual Linear Bandits. (arXiv:2105.10590v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.10590</id>
        <link href="http://arxiv.org/abs/2105.10590"/>
        <updated>2021-05-25T01:56:08.867Z</updated>
        <summary type="html"><![CDATA[Standard approaches to decision-making under uncertainty focus on sequential
exploration of the space of decisions. However, \textit{simultaneously}
proposing a batch of decisions, which leverages available resources for
parallel experimentation, has the potential to rapidly accelerate exploration.
We present a family of (parallel) contextual linear bandit algorithms, whose
regret is nearly identical to their perfectly sequential counterparts -- given
access to the same total number of oracle queries -- up to a lower-order
"burn-in" term that is dependent on the context-set geometry. We provide
matching information-theoretic lower bounds on parallel regret performance to
establish our algorithms are asymptotically optimal in the time horizon.
Finally, we also present an empirical evaluation of these parallel algorithms
in several domains, including materials discovery and biological sequence
design problems, to demonstrate the utility of parallelized bandits in
practical settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chan_J/0/1/0/all/0/1"&gt;Jeffrey Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tripuraneni_N/0/1/0/all/0/1"&gt;Nilesh Tripuraneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yun S. Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter Bartlett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Finite to Countable-Armed Bandits. (arXiv:2105.10721v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10721</id>
        <link href="http://arxiv.org/abs/2105.10721"/>
        <updated>2021-05-25T01:56:08.859Z</updated>
        <summary type="html"><![CDATA[We consider a stochastic bandit problem with countably many arms that belong
to a finite set of types, each characterized by a unique mean reward. In
addition, there is a fixed distribution over types which sets the proportion of
each type in the population of arms. The decision maker is oblivious to the
type of any arm and to the aforementioned distribution over types, but
perfectly knows the total number of types occurring in the population of arms.
We propose a fully adaptive online learning algorithm that achieves O(log n)
distribution-dependent expected cumulative regret after any number of plays n,
and show that this order of regret is best possible. The analysis of our
algorithm relies on newly discovered concentration and convergence properties
of optimism-based policies like UCB in finite-armed bandit problems with "zero
gap," which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalvit_A/0/1/0/all/0/1"&gt;Anand Kalvit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeevi_A/0/1/0/all/0/1"&gt;Assaf Zeevi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Techniques Toward Optimizing Viewability in RTB Ad Campaigns Using Reinforcement Learning. (arXiv:2105.10587v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10587</id>
        <link href="http://arxiv.org/abs/2105.10587"/>
        <updated>2021-05-25T01:56:08.850Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) is an effective technique for training
decision-making agents through interactions with their environment. The advent
of deep learning has been associated with highly notable successes with
sequential decision making problems - such as defeating some of the
highest-ranked human players at Go. In digital advertising, real-time bidding
(RTB) is a common method of allocating advertising inventory through real-time
auctions. Bidding strategies need to incorporate logic for dynamically
adjusting parameters in order to deliver pre-assigned campaign goals. Here we
discuss techniques toward using RL to train bidding agents. As a campaign
metric we particularly focused on viewability: the percentage of inventory
which goes on to be viewed by an end user.

This paper is presented as a survey of techniques and experiments which we
developed through the course of this research. We discuss expanding our
training data to include edge cases by training on simulated interactions. We
discuss the experimental results comparing the performance of several promising
RL algorithms, and an approach to hyperparameter optimization of an
actor/critic training pipeline through Bayesian optimization. Finally, we
present live-traffic tests of some of our RL agents against a rule-based
feedback-control approach, demonstrating the potential for this method as well
as areas for further improvement. This paper therefore presents an arrangement
of our findings in this quickly developing field, and ways that it can be
applied to an RTB use case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tashman_M/0/1/0/all/0/1"&gt;Michael Tashman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1"&gt;John Hoffman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiayi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fengdan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morsali_A/0/1/0/all/0/1"&gt;Atefeh Morsali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winikor_L/0/1/0/all/0/1"&gt;Lee Winikor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerami_R/0/1/0/all/0/1"&gt;Rouzbeh Gerami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding. (arXiv:2105.07688v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07688</id>
        <link href="http://arxiv.org/abs/2105.07688"/>
        <updated>2021-05-25T01:56:08.843Z</updated>
        <summary type="html"><![CDATA[Semantic embedding has been widely investigated for aligning knowledge graph
(KG) entities. Current methods have explored and utilized the graph structure,
the entity names and attributes, but ignore the ontology (or ontological
schema) which contains critical meta information such as classes and their
membership relationships with entities. In this paper, we propose an
ontology-guided entity alignment method named OntoEA, where both KGs and their
ontologies are jointly embedded, and the class hierarchy and the class
disjointness are utilized to avoid false mappings. Extensive experiments on
seven public and industrial benchmarks have demonstrated the state-of-the-art
performance of OntoEA and the effectiveness of the ontologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yuejia Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhenxi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[THP: Topological Hawkes Processes for Learning Granger Causality on Event Sequences. (arXiv:2105.10884v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10884</id>
        <link href="http://arxiv.org/abs/2105.10884"/>
        <updated>2021-05-25T01:56:08.836Z</updated>
        <summary type="html"><![CDATA[Learning Granger causality among event types on multi-type event sequences is
an important but challenging task. Existing methods, such as the Multivariate
Hawkes processes, mostly assumed that each sequence is independent and
identically distributed. However, in many real-world applications, it is
commonplace to encounter a topological network behind the event sequences such
that an event is excited or inhibited not only by its history but also by its
topological neighbors. Consequently, the failure in describing the topological
dependency among the event sequences leads to the error detection of the causal
structure. By considering the Hawkes processes from the view of temporal
convolution, we propose a Topological Hawkes processes (THP) to draw a
connection between the graph convolution in topology domain and the temporal
convolution in time domains. We further propose a Granger causality learning
method on THP in a likelihood framework. The proposed method is featured with
the graph convolution-based likelihood function of THP and a sparse
optimization scheme with an Expectation-Maximization of the likelihood
function. Theoretical analysis and experiments on both synthetic and real-world
data demonstrate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1"&gt;Ruichu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Siyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1"&gt;Jie Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1"&gt;Zhifeng Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Keli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Robust and Generalized Framework for Adversarial Graph Embedding. (arXiv:2105.10651v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10651</id>
        <link href="http://arxiv.org/abs/2105.10651"/>
        <updated>2021-05-25T01:56:08.815Z</updated>
        <summary type="html"><![CDATA[Graph embedding is essential for graph mining tasks. With the prevalence of
graph data in real-world applications, many methods have been proposed in
recent years to learn high-quality graph embedding vectors various types of
graphs. However, most existing methods usually randomly select the negative
samples from the original graph to enhance the training data without
considering the noise. In addition, most of these methods only focus on the
explicit graph structures and cannot fully capture complex semantics of edges
such as various relationships or asymmetry. In order to address these issues,
we propose a robust and generalized framework for adversarial graph embedding
based on generative adversarial networks. Inspired by generative adversarial
network, we propose a robust and generalized framework for adversarial graph
embedding, named AGE. AGE generates the fake neighbor nodes as the enhanced
negative samples from the implicit distribution, and enables the discriminator
and generator to jointly learn each node's robust and generalized
representation. Based on this framework, we propose three models to handle
three types of graph data and derive the corresponding optimization algorithms,
i.e., UG-AGE and DG-AGE for undirected and directed homogeneous graphs,
respectively, and HIN-AGE for heterogeneous information networks. Extensive
experiments show that our methods consistently and significantly outperform
existing state-of-the-art methods across multiple graph mining tasks, including
link prediction, node classification, and graph reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xingcheng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Senzhang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shijie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qingyun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising Noisy Neural Networks: A Bayesian Approach with Compensation. (arXiv:2105.10699v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10699</id>
        <link href="http://arxiv.org/abs/2105.10699"/>
        <updated>2021-05-25T01:56:08.808Z</updated>
        <summary type="html"><![CDATA[Noisy neural networks (NoisyNNs) refer to the inference and training of NNs
in the presence of noise. Noise is inherent in most communication and storage
systems; hence, NoisyNNs emerge in many new applications, including federated
edge learning, where wireless devices collaboratively train a NN over a noisy
wireless channel, or when NNs are implemented/stored in an analog storage
medium. This paper studies a fundamental problem of NoisyNNs: how to estimate
the uncontaminated NN weights from their noisy observations or manifestations.
Whereas all prior works relied on the maximum likelihood (ML) estimation to
maximize the likelihood function of the estimated NN weights, this paper
demonstrates that the ML estimator is in general suboptimal. To overcome the
suboptimality of the conventional ML estimator, we put forth an
$\text{MMSE}_{pb}$ estimator to minimize a compensated mean squared error (MSE)
with a population compensator and a bias compensator. Our approach works well
for NoisyNNs arising in both 1) noisy inference, where noise is introduced only
in the inference phase on the already-trained NN weights; and 2) noisy
training, where noise is introduced over the course of training. Extensive
experiments on the CIFAR-10 and SST-2 datasets with different NN architectures
verify the significant performance gains of the $\text{MMSE}_{pb}$ estimator
over the ML estimator when used to denoise the NoisyNN. For noisy inference,
the average gains are up to $156\%$ for a noisy ResNet34 model and $14.7\%$ for
a noisy BERT model; for noisy training, the average gains are up to $18.1$ dB
for a noisy ResNet18 model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yulin Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1"&gt;Soung Chang Liew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating leverage scores via rank revealing methods and randomization. (arXiv:2105.11004v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.11004</id>
        <link href="http://arxiv.org/abs/2105.11004"/>
        <updated>2021-05-25T01:56:08.798Z</updated>
        <summary type="html"><![CDATA[We study algorithms for estimating the statistical leverage scores of
rectangular dense or sparse matrices of arbitrary rank. Our approach is based
on combining rank revealing methods with compositions of dense and sparse
randomized dimensionality reduction transforms. We first develop a set of fast
novel algorithms for rank estimation, column subset selection and least squares
preconditioning. We then describe the design and implementation of leverage
score estimators based on these primitives. These estimators are also effective
for rank deficient input, which is frequently the case in data analytics
applications. We provide detailed complexity analyses for all algorithms as
well as meaningful approximation bounds and comparisons with the
state-of-the-art. We conduct extensive numerical experiments to evaluate our
algorithms and to illustrate their properties and performance using synthetic
and real world data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sobczyk_A/0/1/0/all/0/1"&gt;Aleksandros Sobczyk&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Gallopoulos_E/0/1/0/all/0/1"&gt;Efstratios Gallopoulos&lt;/a&gt; (2) ((1) IBM Research Europe, Zurich, Switzerland (2) Computer Engineering and Informatics Department, University of Patras, Greece)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Commodities News Corpus: A Resource forUnderstanding Commodity News Better. (arXiv:2105.08214v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08214</id>
        <link href="http://arxiv.org/abs/2105.08214"/>
        <updated>2021-05-25T01:56:08.792Z</updated>
        <summary type="html"><![CDATA[Commodity News contains a wealth of information such as sum-mary of the
recent commodity price movement and notable events that led tothe movement.
Through event extraction, useful information extracted fromcommodity news is
extremely useful in mining for causal relation betweenevents and commodity
price movement, which can be used for commodity priceprediction. To facilitate
the future research, we introduce a new dataset withthe following information
identified and annotated: (i) entities (both nomi-nal and named), (ii) events
(trigger words and argument roles), (iii) eventmetadata: modality, polarity and
intensity and (iv) event-event relations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Meisin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soon_L/0/1/0/all/0/1"&gt;Lay-Ki Soon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siew_E/0/1/0/all/0/1"&gt;Eu-Gene Siew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugianto_L/0/1/0/all/0/1"&gt;Ly Fie Sugianto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Potential Drug Targets Using Tensor Factorisation and Knowledge Graph Embeddings. (arXiv:2105.10578v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2105.10578</id>
        <link href="http://arxiv.org/abs/2105.10578"/>
        <updated>2021-05-25T01:56:08.786Z</updated>
        <summary type="html"><![CDATA[The drug discovery and development process is a long and expensive one,
costing over 1 billion USD on average per drug and taking 10-15 years. To
reduce the high levels of attrition throughout the process, there has been a
growing interest in applying machine learning methodologies to various stages
of drug discovery process in the recent decade, including at the earliest stage
- identification of druggable disease genes. In this paper, we have developed a
new tensor factorisation model to predict potential drug targets (i.e.,genes or
proteins) for diseases. We created a three dimensional tensor which consists of
1,048 targets, 860 diseases and 230,011 evidence attributes and clinical
outcomes connecting them, using data extracted from the Open Targets and
PharmaProjects databases. We enriched the data with gene representations
learned from a drug discovery-oriented knowledge graph and applied our proposed
method to predict the clinical outcomes for unseen target and dis-ease pairs.
We designed three evaluation strategies to measure the prediction performance
and benchmarked several commonly used machine learning classifiers together
with matrix and tensor factorisation methods. The result shows that
incorporating knowledge graph embeddings significantly improves the prediction
accuracy and that training tensor factorisation alongside a dense neural
network outperforms other methods. In summary, our framework combines two
actively studied machine learning approaches to disease target identification,
tensor factorisation and knowledge graph representation learning, which could
be a promising avenue for further exploration in data-driven drug discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1"&gt;Cheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1"&gt;Rowan Swiers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1"&gt;Stephen Bonner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1"&gt;Ian Barrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Properties of the After Kernel. (arXiv:2105.10585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10585</id>
        <link href="http://arxiv.org/abs/2105.10585"/>
        <updated>2021-05-25T01:56:08.767Z</updated>
        <summary type="html"><![CDATA[The Neural Tangent Kernel (NTK) is the wide-network limit of a kernel defined
using neural networks at initialization, whose embedding is the gradient of the
output of the network with respect to its parameters. We study the "after
kernel", which is defined using the same embedding, except after training, for
neural networks with standard architectures, on binary classification problems
extracted from MNIST and CIFAR-10, trained using SGD in a standard way. Lyu and
Li described a sense in which neural networks, under certain conditions, are
equivalent to SVM with the after kernel. Our experiments are consistent with
this proposition under natural conditions. For networks with an architecure
similar to VGG, the after kernel is more "global", in the sense that it is less
invariant to transformations of input images that disrupt the global structure
of the image while leaving the local statistics largely intact. For fully
connected networks, the after kernel is less global in this sense. The after
kernel tends to be more invariant to small shifts, rotations and zooms; data
augmentation does not improve these invariances. The (finite approximation to
the) conjugate kernel, obtained using the last layer of hidden nodes,
sometimes, but not always, provides a good approximation to the NTK and the
after kernel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_P/0/1/0/all/0/1"&gt;Philip M. Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Monolingual Models: Data can be Scarce when Language Similarity is High. (arXiv:2105.02855v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02855</id>
        <link href="http://arxiv.org/abs/2105.02855"/>
        <updated>2021-05-25T01:56:08.760Z</updated>
        <summary type="html"><![CDATA[For many (minority) languages, the resources needed to train large models are
not available. We investigate the performance of zero-shot transfer learning
with as little data as possible, and the influence of language similarity in
this process. We retrain the lexical layers of four BERT-based models using
data from two low-resource target language varieties, while the Transformer
layers are independently fine-tuned on a POS-tagging task in the model's source
language. By combining the new lexical layers and fine-tuned Transformer
layers, we achieve high task performance for both target languages. With high
language similarity, 10MB of data appears sufficient to achieve substantial
monolingual transfer performance. Monolingual BERT-based models generally
achieve higher downstream task performance after retraining the lexical layer
than multilingual BERT, even when the target language is included in the
multilingual model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vries_W/0/1/0/all/0/1"&gt;Wietse de Vries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1"&gt;Martijn Bartelds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1"&gt;Malvina Nissim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1"&gt;Martijn Wieling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A CCG-Based Version of the DisCoCat Framework. (arXiv:2105.07720v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07720</id>
        <link href="http://arxiv.org/abs/2105.07720"/>
        <updated>2021-05-25T01:56:08.752Z</updated>
        <summary type="html"><![CDATA[While the DisCoCat model (Coecke et al., 2010) has been proved a valuable
tool for studying compositional aspects of language at the level of semantics,
its strong dependency on pregroup grammars poses important restrictions: first,
it prevents large-scale experimentation due to the absence of a pregroup
parser; and second, it limits the expressibility of the model to context-free
grammars. In this paper we solve these problems by reformulating DisCoCat as a
passage from Combinatory Categorial Grammar (CCG) to a category of semantics.
We start by showing that standard categorial grammars can be expressed as a
biclosed category, where all rules emerge as currying/uncurrying the identity;
we then proceed to model permutation-inducing rules by exploiting the symmetry
of the compact closed category encoding the word meaning. We provide a proof of
concept for our method, converting "Alice in Wonderland" into DisCoCat form, a
corpus that we make available to the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_R/0/1/0/all/0/1"&gt;Richie Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kartsaklis_D/0/1/0/all/0/1"&gt;Dimitri Kartsaklis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plot and Rework: Modeling Storylines for Visual Storytelling. (arXiv:2105.06950v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06950</id>
        <link href="http://arxiv.org/abs/2105.06950"/>
        <updated>2021-05-25T01:56:08.513Z</updated>
        <summary type="html"><![CDATA[Writing a coherent and engaging story is not easy. Creative writers use their
knowledge and worldview to put disjointed elements together to form a coherent
storyline, and work and rework iteratively toward perfection. Automated visual
storytelling (VIST) models, however, make poor use of external knowledge and
iterative generation when attempting to create stories. This paper introduces
PR-VIST, a framework that represents the input image sequence as a story graph
in which it finds the best path to form a storyline. PR-VIST then takes this
path and learns to generate the final story via an iterative training process.
This framework produces stories that are superior in terms of diversity,
coherence, and humanness, per both automatic and human evaluations. An ablation
study shows that both plotting and reworking contribute to the model's
superiority.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Chi-Yang Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1"&gt;Yun-Wei Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Ting-Hao &amp;#x27;Kenneth&amp;#x27; Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1"&gt;Lun-Wei Ku&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06450</id>
        <link href="http://arxiv.org/abs/2103.06450"/>
        <updated>2021-05-25T01:56:08.489Z</updated>
        <summary type="html"><![CDATA[We present a Neural Network based Handwritten Text Recognition (HTR) model
architecture that can be trained to recognize full pages of handwritten or
printed text without image segmentation. Being based on Image to Sequence
architecture, it can extract text present in an image and then sequence it
correctly without imposing any constraints regarding orientation, layout and
size of text and non-text. Further, it can also be trained to generate
auxiliary markup related to formatting, layout and content. We use character
level vocabulary, thereby enabling language and terminology of any subject. The
model achieves a new state-of-art in paragraph level recognition on the IAM
dataset. When evaluated on scans of real world handwritten free form test
answers - beset with curved and slanted lines, drawings, tables, math,
chemistry and other symbols - it performs better than all commercially
available HTR cloud APIs. It is deployed in production as part of a commercial
web application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sumeet S. Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karayev_S/0/1/0/all/0/1"&gt;Sergey Karayev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-shift Conditioning using Adaptable Filtering via Hierarchical Embeddings for Robust Chinese Spell Check. (arXiv:2008.12281v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12281</id>
        <link href="http://arxiv.org/abs/2008.12281"/>
        <updated>2021-05-25T01:56:08.470Z</updated>
        <summary type="html"><![CDATA[Spell check is a useful application which processes noisy human-generated
text. Spell check for Chinese poses unresolved problems due to the large number
of characters, the sparse distribution of errors, and the dearth of resources
with sufficient coverage of heterogeneous and shifting error domains. For
Chinese spell check, filtering using confusion sets narrows the search space
and makes finding corrections easier. However, most, if not all, confusion sets
used to date are fixed and thus do not include new, shifting error domains. We
propose a scalable adaptable filter that exploits hierarchical character
embeddings to (1) obviate the need to handcraft confusion sets, and (2) resolve
sparsity problems related to infrequent errors. Our approach compares favorably
with competitive baselines and obtains SOTA results on the 2014 and 2015
Chinese Spelling Check Bake-off datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_G/0/1/0/all/0/1"&gt;Gia H. Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nancy F. Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCR-Net: A Multi-Step Co-Interactive Relation Network for Unanswerable Questions on Machine Reading Comprehension. (arXiv:2103.04567v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04567</id>
        <link href="http://arxiv.org/abs/2103.04567"/>
        <updated>2021-05-25T01:56:08.464Z</updated>
        <summary type="html"><![CDATA[Question answering systems usually use keyword searches to retrieve potential
passages related to a question, and then extract the answer from passages with
the machine reading comprehension methods. However, many questions tend to be
unanswerable in the real world. In this case, it is significant and challenging
how the model determines when no answer is supported by the passage and
abstains from answering. Most of the existing systems design a simple
classifier to determine answerability implicitly without explicitly modeling
mutual interaction and relation between the question and passage, leading to
the poor performance for determining the unanswerable questions. To tackle this
problem, we propose a Multi-Step Co-Interactive Relation Network (MCR-Net) to
explicitly model the mutual interaction and locate key clues from coarse to
fine by introducing a co-interactive relation module. The co-interactive
relation module contains a stack of interaction and fusion blocks to
continuously integrate and fuse history-guided and current-query-guided clues
in an explicit way. Experiments on the SQuAD 2.0 and DuReader datasets show
that our model achieves a remarkable improvement, outperforming the BERT-style
baselines in literature. Visualization analysis also verifies the importance of
the mutual interaction between the question and passage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yue Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1"&gt;Luxi Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuqiang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zihao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yajing Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZJUKLAB at SemEval-2021 Task 4: Negative Augmentation with Language Model for Reading Comprehension of Abstract Meaning. (arXiv:2102.12828v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12828</id>
        <link href="http://arxiv.org/abs/2102.12828"/>
        <updated>2021-05-25T01:56:08.456Z</updated>
        <summary type="html"><![CDATA[This paper presents our systems for the three Subtasks of SemEval Task4:
Reading Comprehension of Abstract Meaning (ReCAM). We explain the algorithms
used to learn our models and the process of tuning the algorithms and selecting
the best model. Inspired by the similarity of the ReCAM task and the language
pre-training, we propose a simple yet effective technology, namely, negative
augmentation with language model. Evaluation results demonstrate the
effectiveness of our proposed approach. Our models achieve the 4th rank on both
official test sets of Subtask 1 and Subtask 2 with an accuracy of 87.9% and an
accuracy of 92.8%, respectively. We further conduct comprehensive model
analysis and observe interesting error cases, which may promote future
researches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangnan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialogue Graph Modeling for Conversational Machine Reading. (arXiv:2012.14827v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14827</id>
        <link href="http://arxiv.org/abs/2012.14827"/>
        <updated>2021-05-25T01:56:08.449Z</updated>
        <summary type="html"><![CDATA[Conversational Machine Reading (CMR) aims at answering questions in a
complicated manner. Machine needs to answer questions through interactions with
users based on given rule document, user scenario and dialogue history, and ask
questions to clarify if necessary. In this paper, we propose a dialogue graph
modeling framework to improve the understanding and reasoning ability of
machine on CMR task. There are three types of graph in total. Specifically,
Discourse Graph is designed to learn explicitly and extract the discourse
relation among rule texts as well as the extra knowledge of scenario;
Decoupling Graph is used for understanding local and contextualized connection
within rule texts. And finally a global graph for fusing the information
together and reply to the user with our final decision being either
"Yes/No/Irrelevant" or to ask a follow-up question to clarify.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1"&gt;Siru Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness Testing of Language Understanding in Task-Oriented Dialog. (arXiv:2012.15262v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15262</id>
        <link href="http://arxiv.org/abs/2012.15262"/>
        <updated>2021-05-25T01:56:08.441Z</updated>
        <summary type="html"><![CDATA[Most language understanding models in task-oriented dialog systems are
trained on a small amount of annotated training data, and evaluated in a small
set from the same distribution. However, these models can lead to system
failure or undesirable output when being exposed to natural language
perturbation or variation in practice. In this paper, we conduct comprehensive
evaluation and analysis with respect to the robustness of natural language
understanding models, and introduce three important aspects related to language
understanding in real-world dialog systems, namely, language variety, speech
characteristics, and noise perturbation. We propose a model-agnostic toolkit
LAUG to approximate natural language perturbations for testing the robustness
issues in task-oriented dialog. Four data augmentation approaches covering the
three aspects are assembled in LAUG, which reveals critical robustness issues
in state-of-the-art models. The augmented dataset through LAUG can be used to
facilitate future research on the robustness testing of language understanding
in task-oriented dialog.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiexi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1"&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Jiaxin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1"&gt;Dazhen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongguang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1"&gt;Weiran Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Event Detection with Prototypical Amortized Conditional Random Field. (arXiv:2012.02353v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02353</id>
        <link href="http://arxiv.org/abs/2012.02353"/>
        <updated>2021-05-25T01:56:08.434Z</updated>
        <summary type="html"><![CDATA[Event detection tends to struggle when it needs to recognize novel event
types with a few samples. The previous work attempts to solve this problem in
the identify-then-classify manner but ignores the trigger discrepancy between
event types, thus suffering from the error propagation. In this paper, we
present a novel unified model which converts the task to a few-shot tagging
problem with a double-part tagging scheme. To this end, we first propose the
Prototypical Amortized Conditional Random Field (PA-CRF) to model the label
dependency in the few-shot scenario, which approximates the transition scores
between labels based on the label prototypes. Then Gaussian distribution is
introduced for modeling of the transition scores to alleviate the uncertain
estimation resulting from insufficient data. Experimental results show that the
unified models work better than existing identify-then-classify models and our
PA-CRF further achieves the best results on the benchmark dataset FewEvent. Our
code and data are available at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1"&gt;Xin Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shiyao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bowen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tingwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yubin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Computational Framework for Slang Generation. (arXiv:2102.01826v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01826</id>
        <link href="http://arxiv.org/abs/2102.01826"/>
        <updated>2021-05-25T01:56:08.414Z</updated>
        <summary type="html"><![CDATA[Slang is a common type of informal language, but its flexible nature and
paucity of data resources present challenges for existing natural language
systems. We take an initial step toward machine generation of slang by
developing a framework that models the speaker's word choice in slang context.
Our framework encodes novel slang meaning by relating the conventional and
slang senses of a word while incorporating syntactic and contextual knowledge
in slang usage. We construct the framework using a combination of probabilistic
inference and neural contrastive learning. We perform rigorous evaluations on
three slang dictionaries and show that our approach not only outperforms
state-of-the-art language models, but also better predicts the historical
emergence of slang word usages from 1960s to 2000s. We interpret the proposed
models and find that the contrastively learned semantic space is sensitive to
the similarities between slang and conventional senses of words. Our work
creates opportunities for the automated generation and interpretation of
informal language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhewei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard Zemel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Vision-and-Language Tasks via Text Generation. (arXiv:2102.02779v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02779</id>
        <link href="http://arxiv.org/abs/2102.02779"/>
        <updated>2021-05-25T01:56:08.407Z</updated>
        <summary type="html"><![CDATA[Existing methods for vision-and-language learning typically require designing
task-specific architectures and objectives for each task. For example, a
multi-label answer classifier for visual question answering, a region scorer
for referring expression comprehension, and a language decoder for image
captioning, etc. To alleviate these hassles, in this work, we propose a unified
framework that learns different tasks in a single architecture with the same
language modeling objective, i.e., multimodal conditional text generation,
where our models learn to generate labels in text based on the visual and
textual inputs. On 7 popular vision-and-language benchmarks, including visual
question answering, referring expression comprehension, visual commonsense
reasoning, most of which have been previously modeled as discriminative tasks,
our generative approach (with a single unified architecture) reaches comparable
performance to recent task-specific state-of-the-art vision-and-language
models. Moreover, our generative approach shows better generalization ability
on questions that have rare answers. Also, we show that our framework allows
multi-task learning in a single architecture with a single set of parameters,
achieving similar performance to separately optimized single-task models. Our
code is publicly available at: https://github.com/j-min/VL-T5]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1"&gt;Jaemin Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ERNIE-Doc: A Retrospective Long-Document Modeling Transformer. (arXiv:2012.15688v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15688</id>
        <link href="http://arxiv.org/abs/2012.15688"/>
        <updated>2021-05-25T01:56:08.400Z</updated>
        <summary type="html"><![CDATA[Transformers are not suited for processing long documents, due to their
quadratically increasing memory and time consumption. Simply truncating a long
document or applying the sparse attention mechanism will incur the context
fragmentation problem or lead to an inferior modeling capability against
comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level
language pretraining model based on Recurrence Transformers. Two well-designed
techniques, namely the retrospective feed mechanism and the enhanced recurrence
mechanism, enable ERNIE-Doc, which has a much longer effective context length,
to capture the contextual information of a complete document. We pretrain
ERNIE-Doc to explicitly learn the relationships among segments with an
additional document-aware segment-reordering objective. Various experiments
were conducted on both English and Chinese document-level tasks. ERNIE-Doc
improved the state-of-the-art language modeling result of perplexity to 16.8 on
WikiText-103. Moreover, it outperformed competitive pretraining models by a
large margin on most language understanding tasks, such as text classification
and question answering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Siyu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1"&gt;Junyuan Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuohuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1"&gt;Hao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks. (arXiv:2012.12352v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12352</id>
        <link href="http://arxiv.org/abs/2012.12352"/>
        <updated>2021-05-25T01:56:08.392Z</updated>
        <summary type="html"><![CDATA[We investigate the reasoning ability of pretrained vision and language (V&L)
models in two tasks that require multimodal integration: (1) discriminating a
correct image-sentence pair from an incorrect one, and (2) counting entities in
an image. We evaluate three pretrained V&L models on these tasks: ViLBERT,
ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results
show that models solve task (1) very well, as expected, since all models are
pretrained on task (1). However, none of the pretrained V&L models is able to
adequately solve task (2), our counting probe, and they cannot generalise to
out-of-distribution quantities. We propose a number of explanations for these
findings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of
catastrophic forgetting on task (1). Concerning our results on the counting
probe, we find evidence that all models are impacted by dataset bias, and also
fail to individuate entities in the visual input. While a selling point of
pretrained V&L models is their ability to solve complex tasks, our findings
suggest that understanding their reasoning and grounding capabilities requires
more targeted investigations on specific phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1"&gt;Letitia Parcalabescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1"&gt;Albert Gatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1"&gt;Iacer Calixto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model. (arXiv:2105.11314v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11314</id>
        <link href="http://arxiv.org/abs/2105.11314"/>
        <updated>2021-05-25T01:56:08.382Z</updated>
        <summary type="html"><![CDATA[We present RobeCzech, a monolingual RoBERTa language representation model
trained on Czech data. RoBERTa is a robustly optimized Transformer-based
pretraining approach. We show that RobeCzech considerably outperforms
equally-sized multilingual and Czech-trained contextualized language
representation models, surpasses current state of the art in all five evaluated
NLP tasks and reaches state-of-theart results in four of them. The RobeCzech
model is released publicly at https://hdl.handle.net/11234/1-3691 and
https://huggingface.co/ufal/robeczech-base.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1"&gt;Milan Straka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naplava_J/0/1/0/all/0/1"&gt;Jakub N&amp;#xe1;plava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1"&gt;Jana Strakov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1"&gt;David Samuel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages. (arXiv:2012.05628v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05628</id>
        <link href="http://arxiv.org/abs/2012.05628"/>
        <updated>2021-05-25T01:56:08.362Z</updated>
        <summary type="html"><![CDATA[Large generative language models have been very successful for English, but
other languages lag behind, in part due to data and computational limitations.
We propose a method that may overcome these problems by adapting existing
pre-trained models to new languages. Specifically, we describe the adaptation
of English GPT-2 to Italian and Dutch by retraining lexical embeddings without
tuning the Transformer layers. As a result, we obtain lexical embeddings for
Italian and Dutch that are aligned with the original English lexical
embeddings. Additionally, we scale up complexity by transforming relearned
lexical embeddings of GPT-2 small to the GPT-2 medium embedding space. This
method minimises the amount of training and prevents losing information during
adaptation that was learned by GPT-2. English GPT-2 models with relearned
lexical embeddings can generate realistic sentences in Italian and Dutch.
Though on average these sentences are still identifiable as artificial by
humans, they are assessed on par with sentences generated by a GPT-2 model
fully trained from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vries_W/0/1/0/all/0/1"&gt;Wietse de Vries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1"&gt;Malvina Nissim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bird's Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach. (arXiv:2105.02629v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02629</id>
        <link href="http://arxiv.org/abs/2105.02629"/>
        <updated>2021-05-25T01:56:08.354Z</updated>
        <summary type="html"><![CDATA[NLP has a rich history of representing our prior understanding of language in
the form of graphs. Recent work on analyzing contextualized text
representations has focused on hand-designed probe models to understand how and
to what extent do these representations encode a particular linguistic
phenomenon. However, due to the inter-dependence of various phenomena and
randomness of training probe models, detecting how these representations encode
the rich information in these linguistic graphs remains a challenging problem.
In this paper, we propose a new information-theoretic probe, Bird's Eye, which
is a fairly simple probe method for detecting if and how these representations
encode the information in these linguistic graphs. Instead of using classifier
performance, our probe takes an information-theoretic view of probing and
estimates the mutual information between the linguistic graph embedded in a
continuous space and the contextualized word representations. Furthermore, we
also propose an approach to use our probe to investigate localized linguistic
information in the linguistic graphs using perturbation analysis. We call this
probing setup Worm's Eye. Using these probes, we analyze BERT models on their
ability to encode a syntactic and a semantic graph structure, and find that
these models encode to some degree both syntactic as well as semantic
information; albeit syntactic information to a greater extent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yifan Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation. (arXiv:2104.05848v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05848</id>
        <link href="http://arxiv.org/abs/2104.05848"/>
        <updated>2021-05-25T01:56:08.347Z</updated>
        <summary type="html"><![CDATA[We translate a closed text that is known in advance into a severely low
resource language by leveraging massive source parallelism. In other words,
given a text in 124 source languages, we translate it into a severely low
resource language using only ~1,000 lines of low resource data without any
external help. Firstly, we propose a systematic method to rank and choose
source languages that are close to the low resource language. We call the
linguistic definition of language family Family of Origin (FAMO), and we call
the empirical definition of higher-ranked languages using our metrics Family of
Choice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual
Order-preserving Lexiconized Transformer (IPML) to train on ~1,000 lines
(~3.5%) of low resource data. To translate named entities correctly, we build a
massive lexicon table for 2,939 Bible named entities in 124 source languages,
and include many that occur once and covers more than 66 severely low resource
languages. Moreover, we also build a novel method of combining translations
from different source languages into one. Using English as a hypothetical low
resource language, we get a +23.9 BLEU increase over a multilingual baseline,
and a +10.3 BLEU increase over our asymmetric baseline in the Bible dataset. We
get a 42.8 BLEU score for Portuguese-English translation on the medical EMEA
dataset. We also have good results for a real severely low resource Mayan
language, Eastern Pokomchi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1"&gt;Alex Waibel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering. (arXiv:2104.07242v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07242</id>
        <link href="http://arxiv.org/abs/2104.07242"/>
        <updated>2021-05-25T01:56:08.340Z</updated>
        <summary type="html"><![CDATA[In open-domain question answering (QA), retrieve-and-read mechanism has the
inherent benefit of interpretability and the easiness of adding, removing, or
editing knowledge compared to the parametric approaches of closed-book QA
models. However, it is also known to suffer from its large storage footprint
due to its document corpus and index. Here, we discuss several orthogonal
strategies to drastically reduce the footprint of a retrieve-and-read
open-domain QA system by up to 160x. Our results indicate that
retrieve-and-read can be a viable option even in a highly constrained serving
environment such as edge devices, as we show that it can achieve better
accuracy than a purely parametric model with comparable docker-level system
size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sohee Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1"&gt;Minjoon Seo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Fact-Checking for Assisting Human Fact-Checkers. (arXiv:2103.07769v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07769</id>
        <link href="http://arxiv.org/abs/2103.07769"/>
        <updated>2021-05-25T01:56:08.333Z</updated>
        <summary type="html"><![CDATA[The reporting and the analysis of current events around the globe has
expanded from professional, editor-lead journalism all the way to citizen
journalism. Nowadays, politicians and other key players enjoy direct access to
their audiences through social media, bypassing the filters of official cables
or traditional media. However, the multiple advantages of free speech and
direct communication are dimmed by the misuse of media to spread inaccurate or
misleading claims. These phenomena have led to the modern incarnation of the
fact-checker -- a professional whose main aim is to examine claims using
available evidence and to assess their veracity. As in other text forensics
tasks, the amount of information available makes the work of the fact-checker
more difficult. With this in mind, starting from the perspective of the
professional fact-checker, we survey the available intelligent technologies
that can support the human expert in the different steps of her fact-checking
endeavor. These include identifying claims worth fact-checking, detecting
relevant previously fact-checked claims, retrieving relevant evidence to
fact-check a claim, and actually verifying a claim. In each case, we pay
attention to the challenges in future work and the potential impact on
real-world fact-checking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1"&gt;Preslav Nakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corney_D/0/1/0/all/0/1"&gt;David Corney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1"&gt;Maram Hasanain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1"&gt;Tamer Elsayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barron_Cedeno_A/0/1/0/all/0/1"&gt;Alberto Barr&amp;#xf3;n-Cede&amp;#xf1;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papotti_P/0/1/0/all/0/1"&gt;Paolo Papotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1"&gt;Shaden Shaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1"&gt;Giovanni Da San Martino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialogue Response Selection with Hierarchical Curriculum Learning. (arXiv:2012.14756v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14756</id>
        <link href="http://arxiv.org/abs/2012.14756"/>
        <updated>2021-05-25T01:56:08.292Z</updated>
        <summary type="html"><![CDATA[We study the learning of a matching model for dialogue response selection.
Motivated by the recent finding that models trained with random negative
samples are not ideal in real-world scenarios, we propose a hierarchical
curriculum learning framework that trains the matching model in an
"easy-to-difficult" scheme. Our learning framework consists of two
complementary curricula: (1) corpus-level curriculum (CC); and (2)
instance-level curriculum (IC). In CC, the model gradually increases its
ability in finding the matching clues between the dialogue context and a
response candidate. As for IC, it progressively strengthens the model's ability
in identifying the mismatching information between the dialogue context and a
response candidate. Empirical studies on three benchmark datasets with three
state-of-the-art matching models demonstrate that the proposed learning
framework significantly improves the model performance across various
evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yixuan Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qingyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zibo Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baker_S/0/1/0/all/0/1"&gt;Simon Baker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yunbo Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuming Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1"&gt;Nigel Collier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Answering Any-hop Open-domain Questions with Iterative Document Reranking. (arXiv:2009.07465v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07465</id>
        <link href="http://arxiv.org/abs/2009.07465"/>
        <updated>2021-05-25T01:56:08.285Z</updated>
        <summary type="html"><![CDATA[Existing approaches for open-domain question answering (QA) are typically
designed for questions that require either single-hop or multi-hop reasoning,
which make strong assumptions of the complexity of questions to be answered.
Also, multi-step document retrieval often incurs higher number of relevant but
non-supporting documents, which dampens the downstream noise-sensitive reader
module for answer extraction. To address these challenges, we propose a unified
QA framework to answer any-hop open-domain questions, which iteratively
retrieves, reranks and filters documents, and adaptively determines when to
stop the retrieval process. To improve the retrieval accuracy, we propose a
graph-based reranking model that perform multi-document interaction as the core
of our iterative reranking framework. Our method consistently achieves
performance comparable to or better than the state-of-the-art on both
single-hop and multi-hop open-domain QA datasets, including Natural Questions
Open, SQuAD Open, and HotpotQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nie_P/0/1/0/all/0/1"&gt;Ping Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamurthy_A/0/1/0/all/0/1"&gt;Arun Ramamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Le Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VANiLLa : Verbalized Answers in Natural Language at Large Scale. (arXiv:2105.11407v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11407</id>
        <link href="http://arxiv.org/abs/2105.11407"/>
        <updated>2021-05-25T01:56:08.278Z</updated>
        <summary type="html"><![CDATA[In the last years, there have been significant developments in the area of
Question Answering over Knowledge Graphs (KGQA). Despite all the notable
advancements, current KGQA datasets only provide the answers as the direct
output result of the formal query, rather than full sentences incorporating
question context. For achieving coherent answers sentence with the question's
vocabulary, template-based verbalization so are usually employed for a better
representation of answers, which in turn require extensive expert intervention.
Thus, making way for machine learning approaches; however, there is a scarcity
of datasets that empower machine learning models in this area. Hence, we
provide the VANiLLa dataset which aims at reducing this gap by offering answers
in natural language sentences. The answer sentences in this dataset are
syntactically and semantically closer to the question than to the triple fact.
Our dataset consists of over 100k simple questions adapted from the CSQA and
SimpleQuestionsWikidata datasets and generated using a semi-automatic
framework. We also present results of training our dataset on multiple baseline
models adapted from current state-of-the-art Natural Language Generation (NLG)
architectures. We believe that this dataset will allow researchers to focus on
finding suitable methodologies and architectures for answer verbalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_D/0/1/0/all/0/1"&gt;Debanjali Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_M/0/1/0/all/0/1"&gt;Mohnish Dubey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rony_M/0/1/0/all/0/1"&gt;Md Rashad Al Hasan Rony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[True Few-Shot Learning with Language Models. (arXiv:2105.11447v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11447</id>
        <link href="http://arxiv.org/abs/2105.11447"/>
        <updated>2021-05-25T01:56:08.272Z</updated>
        <summary type="html"><![CDATA[Pretrained language models (LMs) perform well on many tasks even when
learning from a few examples, but prior work uses many held-out examples to
tune various aspects of learning, such as hyperparameters, training objectives,
and natural language templates ("prompts"). Here, we evaluate the few-shot
ability of LMs when such held-out examples are unavailable, a setting we call
true few-shot learning. We test two model selection criteria, cross-validation
and minimum description length, for choosing LM prompts and hyperparameters in
the true few-shot setting. On average, both marginally outperform random
selection and greatly underperform selection based on held-out examples.
Moreover, selection criteria often prefer models that perform significantly
worse than randomly-selected ones. We find similar results even when taking
into account our uncertainty in a model's true performance during selection, as
well as when varying the amount of computation and number of examples used for
selection. Overall, our findings suggest that prior work significantly
overestimated the true few-shot ability of LMs given the difficulty of few-shot
model selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1"&gt;Ethan Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence Modeling. (arXiv:2009.02725v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.02725</id>
        <link href="http://arxiv.org/abs/2009.02725"/>
        <updated>2021-05-25T01:56:08.265Z</updated>
        <summary type="html"><![CDATA[This paper proposes an any-to-many location-relative, sequence-to-sequence
(seq2seq), non-parallel voice conversion approach, which utilizes text
supervision during training. In this approach, we combine a bottle-neck feature
extractor (BNE) with a seq2seq synthesis module. During the training stage, an
encoder-decoder-based hybrid connectionist-temporal-classification-attention
(CTC-attention) phoneme recognizer is trained, whose encoder has a bottle-neck
layer. A BNE is obtained from the phoneme recognizer and is utilized to extract
speaker-independent, dense and rich spoken content representations from
spectral features. Then a multi-speaker location-relative attention based
seq2seq synthesis model is trained to reconstruct spectral features from the
bottle-neck features, conditioning on speaker representations for speaker
identity control in the generated speech. To mitigate the difficulties of using
seq2seq models to align long sequences, we down-sample the input spectral
feature along the temporal dimension and equip the synthesis model with a
discretized mixture of logistic (MoL) attention mechanism. Since the phoneme
recognizer is trained with large speech recognition data corpus, the proposed
approach can conduct any-to-many voice conversion. Objective and subjective
evaluations show that the proposed any-to-many approach has superior voice
conversion performance in terms of both naturalness and speaker similarity.
Ablation studies are conducted to confirm the effectiveness of feature
selection and model design strategies in the proposed approach. The proposed VC
approach can readily be extended to support any-to-any VC (also known as
one/few-shot VC), and achieve high performance according to objective and
subjective evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1"&gt;Songxiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuewen Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1"&gt;Disong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xixin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xunying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better Robustness by More Coverage: Adversarial Training with Mixup Augmentation for Robust Fine-tuning. (arXiv:2012.15699v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15699</id>
        <link href="http://arxiv.org/abs/2012.15699"/>
        <updated>2021-05-25T01:56:08.246Z</updated>
        <summary type="html"><![CDATA[Pretrained language models (PLMs) perform poorly under adversarial attacks.
To improve the adversarial robustness, adversarial data augmentation (ADA) has
been widely adopted to cover more search space of adversarial attacks by adding
textual adversarial examples during training. However, the number of
adversarial examples for text augmentation is still extremely insufficient due
to the exponentially large attack search space. In this work, we propose a
simple and effective method to cover a much larger proportion of the attack
search space, called Adversarial and Mixup Data Augmentation (AMDA).
Specifically, AMDA linearly interpolates the representations of pairs of
training samples to form new virtual samples, which are more abundant and
diverse than the discrete text adversarial examples in conventional ADA.
Moreover, to fairly evaluate the robustness of different models, we adopt a
challenging evaluation setup, which generates a new set of adversarial examples
targeting each model. In text classification experiments of BERT and RoBERTa,
AMDA achieves significant robustness gains under two strong adversarial attacks
and alleviates the performance degradation of ADA on the clean data. Our code
is released at: https://github.com/thunlp/MixADA .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Chenglei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yasheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Math KCs via Task-Adaptive Pre-Trained BERT. (arXiv:2105.11343v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11343</id>
        <link href="http://arxiv.org/abs/2105.11343"/>
        <updated>2021-05-25T01:56:08.240Z</updated>
        <summary type="html"><![CDATA[Educational content labeled with proper knowledge components (KCs) are
particularly useful to teachers or content organizers. However, manually
labeling educational content is labor intensive and error-prone. To address
this challenge, prior research proposed machine learning based solutions to
auto-label educational content with limited success. In this work, we
significantly improve prior research by (1) expanding the input types to
include KC descriptions, instructional video titles, and problem descriptions
(i.e., three types of prediction task), (2) doubling the granularity of the
prediction from 198 to 385 KC labels (i.e., more practical setting but much
harder multinomial classification problem), (3) improving the prediction
accuracies by 0.5-2.3% using Task-adaptive Pre-trained BERT, outperforming six
baselines, and (4) proposing a simple evaluation measure by which we can
recover 56-73% of mispredicted KC labels. All codes and data sets in the
experiments are available at:https://github.com/tbs17/TAPT-BERT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jia Tracy Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_M/0/1/0/all/0/1"&gt;Michiharu Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prihar_E/0/1/0/all/0/1"&gt;Ethan Prihar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1"&gt;Neil Heffernan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xintao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGrew_S/0/1/0/all/0/1"&gt;Sean McGrew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dongwon Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios. (arXiv:2004.04507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.04507</id>
        <link href="http://arxiv.org/abs/2004.04507"/>
        <updated>2021-05-25T01:56:08.232Z</updated>
        <summary type="html"><![CDATA[Unsupervised neural machine translation (UNMT) that relies solely on massive
monolingual corpora has achieved remarkable results in several translation
tasks. However, in real-world scenarios, massive monolingual corpora do not
exist for some extremely low-resource languages such as Estonian, and UNMT
systems usually perform poorly when there is not adequate training corpus for
one language. In this paper, we first define and analyze the unbalanced
training data scenario for UNMT. Based on this scenario, we propose UNMT
self-training mechanisms to train a robust UNMT system and improve its
performance in this case. Experimental results on several language pairs show
that the proposed methods substantially outperform conventional UNMT systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haipeng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kehai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1"&gt;Masao Utiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1"&gt;Eiichiro Sumita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiejun Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesizer: Rethinking Self-Attention in Transformer Models. (arXiv:2005.00743v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00743</id>
        <link href="http://arxiv.org/abs/2005.00743"/>
        <updated>2021-05-25T01:56:08.225Z</updated>
        <summary type="html"><![CDATA[The dot product self-attention is known to be central and indispensable to
state-of-the-art Transformer models. But is it really required? This paper
investigates the true importance and contribution of the dot product-based
self-attention mechanism on the performance of Transformer models. Via
extensive experiments, we find that (1) random alignment matrices surprisingly
perform quite competitively and (2) learning attention weights from token-token
(query-key) interactions is useful but not that important after all. To this
end, we propose \textsc{Synthesizer}, a model that learns synthetic attention
weights without token-token interactions. In our experiments, we first show
that simple Synthesizers achieve highly competitive performance when compared
against vanilla Transformer models across a range of tasks, including machine
translation, language modeling, text generation and GLUE/SuperGLUE benchmarks.
When composed with dot product attention, we find that Synthesizers
consistently outperform Transformers. Moreover, we conduct additional
comparisons of Synthesizers against Dynamic Convolutions, showing that simple
Random Synthesizer is not only $60\%$ faster but also improves perplexity by a
relative $3.5\%$. Finally, we show that simple factorized Synthesizers can
outperform Linformers on encoding only tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1"&gt;Da-Cheng Juan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Che Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducibility Report: Contextualizing Hate Speech Classifiers with Post-hoc Explanation. (arXiv:2105.11412v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11412</id>
        <link href="http://arxiv.org/abs/2105.11412"/>
        <updated>2021-05-25T01:56:08.215Z</updated>
        <summary type="html"><![CDATA[The presented report evaluates Contextualizing Hate Speech Classifiers with
Post-hoc Explanation paper within the scope of ML Reproducibility Challenge
2020. Our work focuses on both aspects constituting the paper: the method
itself and the validity of the stated results. In the following sections, we
have described the paper, related works, algorithmic frameworks, our
experiments and evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1"&gt;Kiran Purohit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_O/0/1/0/all/0/1"&gt;Owais Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1"&gt;Ankan Mullick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diacritics Restoration using BERT with Analysis on Czech language. (arXiv:2105.11408v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11408</id>
        <link href="http://arxiv.org/abs/2105.11408"/>
        <updated>2021-05-25T01:56:08.194Z</updated>
        <summary type="html"><![CDATA[We propose a new architecture for diacritics restoration based on
contextualized embeddings, namely BERT, and we evaluate it on 12 languages with
diacritics. Furthermore, we conduct a detailed error analysis on Czech, a
morphologically rich language with a high level of diacritization. Notably, we
manually annotate all mispredictions, showing that roughly 44% of them are
actually not errors, but either plausible variants (19%), or the system
corrections of erroneous data (25%). Finally, we categorize the real errors in
detail. We release the code at
https://github.com/ufal/bert-diacritics-restoration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naplava_J/0/1/0/all/0/1"&gt;Jakub N&amp;#xe1;plava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1"&gt;Milan Straka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1"&gt;Jana Strakov&amp;#xe1;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Lightweight Neural Model for Biomedical Entity Linking. (arXiv:2012.08844v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08844</id>
        <link href="http://arxiv.org/abs/2012.08844"/>
        <updated>2021-05-25T01:56:08.186Z</updated>
        <summary type="html"><![CDATA[Biomedical entity linking aims to map biomedical mentions, such as diseases
and drugs, to standard entities in a given knowledge base. The specific
challenge in this context is that the same biomedical entity can have a wide
range of names, including synonyms, morphological variations, and names with
different word orderings. Recently, BERT-based methods have advanced the
state-of-the-art by allowing for rich representations of word sequences.
However, they often have hundreds of millions of parameters and require heavy
computing resources, which limits their applications in resource-limited
scenarios. Here, we propose a lightweight neural method for biomedical entity
linking, which needs just a fraction of the parameters of a BERT model and much
less computing resources. Our method uses a simple alignment layer with
attention mechanisms to capture the variations between mention and entity
names. Yet, we show that our model is competitive with previous work on
standard evaluation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lihu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varoquaux_G/0/1/0/all/0/1"&gt;Ga&amp;#xeb;l Varoquaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1"&gt;Fabian M. Suchanek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11347</id>
        <link href="http://arxiv.org/abs/2105.11347"/>
        <updated>2021-05-25T01:56:08.178Z</updated>
        <summary type="html"><![CDATA[In this article, we present a description of our systems as a part of our
participation in the shared task namely Artificial Intelligence for Legal
Assistance (AILA 2019). This is an integral event of Forum for Information
Retrieval Evaluation-2019. The outcomes of this track would be helpful for the
automation of the working process of the Indian Judiciary System. The manual
working procedures and documentation at any level (from lower to higher court)
of the judiciary system are very complex in nature. The systems produced as a
part of this track would assist the law practitioners. It would be helpful for
common men too. This kind of track also opens the path of research of Natural
Language Processing (NLP) in the judicial domain. This track defined two
problems such as Task 1: Identifying relevant prior cases for a given situation
and Task 2: Identifying the most relevant statutes for a given situation. We
tackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As
per the results declared by the task organizers, we are in 3rd and a modest
position in Task 1 and Task 2 respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1"&gt;Baban Gain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1"&gt;Dibyanayan Bandyopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1"&gt;Arkadipta De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1"&gt;Tanik Saikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fighting an Infodemic: COVID-19 Fake News Dataset. (arXiv:2011.03327v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03327</id>
        <link href="http://arxiv.org/abs/2011.03327"/>
        <updated>2021-05-25T01:56:08.170Z</updated>
        <summary type="html"><![CDATA[Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news
and rumors are rampant on social media. Believing in rumors can cause
significant harm. This is further exacerbated at the time of a pandemic. To
tackle this, we curate and release a manually annotated dataset of 10,700
social media posts and articles of real and fake news on COVID-19. We benchmark
the annotated dataset with four machine learning baselines - Decision Tree,
Logistic Regression, Gradient Boost, and Support Vector Machine (SVM). We
obtain the best performance of 93.46% F1-score with SVM. The data and code is
available at: https://github.com/parthpatwa/covid19-fake-news-dectection]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1"&gt;Parth Patwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Shivam Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pykl_S/0/1/0/all/0/1"&gt;Srinivas Pykl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guptha_V/0/1/0/all/0/1"&gt;Vineeth Guptha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumari_G/0/1/0/all/0/1"&gt;Gitanjali Kumari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1"&gt;Md Shad Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Amitava Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-microphone Complex Spectral Mapping for Utterance-wise and Continuous Speech Separation. (arXiv:2010.01703v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01703</id>
        <link href="http://arxiv.org/abs/2010.01703"/>
        <updated>2021-05-25T01:56:08.162Z</updated>
        <summary type="html"><![CDATA[We propose multi-microphone complex spectral mapping, a simple way of
applying deep learning for time-varying non-linear beamforming, for speaker
separation in reverberant conditions. We aim at both speaker separation and
dereverberation. Our study first investigates offline utterance-wise speaker
separation and then extends to block-online continuous speech separation (CSS).
Assuming a fixed array geometry between training and testing, we train deep
neural networks (DNN) to predict the real and imaginary (RI) components of
target speech at a reference microphone from the RI components of multiple
microphones. We then integrate multi-microphone complex spectral mapping with
minimum variance distortionless response (MVDR) beamforming and post-filtering
to further improve separation, and combine it with frame-level speaker counting
for block-online CSS. Although our system is trained on simulated room impulse
responses (RIR) based on a fixed number of microphones arranged in a given
geometry, it generalizes well to a real array with the same geometry.
State-of-the-art separation performance is obtained on the simulated two-talker
SMS-WSJ corpus and the real-recorded LibriCSS dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhong-Qiu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peidong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;DeLiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-model Back-translated Distillation for Unsupervised Machine Translation. (arXiv:2006.02163v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02163</id>
        <link href="http://arxiv.org/abs/2006.02163"/>
        <updated>2021-05-25T01:56:08.143Z</updated>
        <summary type="html"><![CDATA[Recent unsupervised machine translation (UMT) systems usually employ three
main principles: initialization, language modeling and iterative
back-translation, though they may apply them differently. Crucially, iterative
back-translation and denoising auto-encoding for language modeling provide data
diversity to train the UMT systems. However, the gains from these
diversification processes has seemed to plateau. We introduce a novel component
to the standard UMT framework called Cross-model Back-translated Distillation
(CBD), that is aimed to induce another level of data diversification that
existing principles lack. CBD is applicable to all previous UMT approaches. In
our experiments, CBD achieves the state of the art in the WMT'14
English-French, WMT'16 English-German and English-Romanian bilingual
unsupervised translation tasks, with 38.2, 30.1, and 36.3 BLEU respectively. It
also yields 1.5-3.3 BLEU improvements in IWSLT English-French and
English-German tasks. Through extensive experimental analyses, we show that CBD
is effective because it embraces data diversity while other similar variants do
not.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1"&gt;Xuan-Phi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thanh-Tung Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kui_W/0/1/0/all/0/1"&gt;Wu Kui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aw_A/0/1/0/all/0/1"&gt;Ai Ti Aw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Language Models for Nineteenth-Century English. (arXiv:2105.11321v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11321</id>
        <link href="http://arxiv.org/abs/2105.11321"/>
        <updated>2021-05-25T01:56:08.134Z</updated>
        <summary type="html"><![CDATA[We present four types of neural language models trained on a large historical
dataset of books in English, published between 1760-1900 and comprised of ~5.1
billion tokens. The language model architectures include static (word2vec and
fastText) and contextualized models (BERT and Flair). For each architecture, we
trained a model instance using the whole dataset. Additionally, we trained
separate instances on text published before 1850 for the two static models, and
four instances considering different time slices for BERT. Our models have
already been used in various downstream tasks where they consistently improved
performance. In this paper, we describe how the models have been created and
outline their reuse potential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_K/0/1/0/all/0/1"&gt;Kasra Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beelen_K/0/1/0/all/0/1"&gt;Kaspar Beelen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colavizza_G/0/1/0/all/0/1"&gt;Giovanni Colavizza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ardanuy_M/0/1/0/all/0/1"&gt;Mariona Coll Ardanuy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[De-identification of Privacy-related Entities in Job Postings. (arXiv:2105.11223v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11223</id>
        <link href="http://arxiv.org/abs/2105.11223"/>
        <updated>2021-05-25T01:56:08.122Z</updated>
        <summary type="html"><![CDATA[De-identification is the task of detecting privacy-related entities in text,
such as person names, emails and contact data. It has been well-studied within
the medical domain. The need for de-identification technology is increasing, as
privacy-preserving data handling is in high demand in many domains. In this
paper, we focus on job postings. We present JobStack, a new corpus for
de-identification of personal data in job vacancies on Stackoverflow. We
introduce baselines, comparing Long-Short Term Memory (LSTM) and Transformer
models. To improve upon these baselines, we experiment with contextualized
embeddings and distantly related auxiliary data via multi-task learning. Our
results show that auxiliary data improves de-identification performance.
Surprisingly, vanilla BERT turned out to be more effective than a BERT model
trained on other portions of Stackoverflow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jensen_K/0/1/0/all/0/1"&gt;Kristian N&amp;#xf8;rgaard Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mike Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1"&gt;Barbara Plank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PTR: Prompt Tuning with Rules for Text Classification. (arXiv:2105.11259v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11259</id>
        <link href="http://arxiv.org/abs/2105.11259"/>
        <updated>2021-05-25T01:56:08.114Z</updated>
        <summary type="html"><![CDATA[Fine-tuned pre-trained language models (PLMs) have achieved awesome
performance on almost all NLP tasks. By using additional prompts to fine-tune
PLMs, we can further stimulate the rich knowledge distributed in PLMs to better
serve downstream task. Prompt tuning has achieved promising results on some
few-class classification tasks such as sentiment classification and natural
language inference. However, manually designing lots of language prompts is
cumbersome and fallible. For those auto-generated prompts, it is also expensive
and time-consuming to verify their effectiveness in non-few-shot scenarios.
Hence, it is challenging for prompt tuning to address many-class classification
tasks. To this end, we propose prompt tuning with rules (PTR) for many-class
text classification, and apply logic rules to construct prompts with several
sub-prompts. In this way, PTR is able to encode prior knowledge of each class
into prompt tuning. We conduct experiments on relation classification, a
typical many-class classification task, and the results on benchmarks show that
PTR can significantly and consistently outperform existing state-of-the-art
baselines. This indicates that PTR is a promising approach to take advantage of
PLMs for those complicated classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Weilin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Ning Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing the Talk Markup Language (TalkML):Adding a little social intelligence to industrial speech interfaces. (arXiv:2105.11294v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11294</id>
        <link href="http://arxiv.org/abs/2105.11294"/>
        <updated>2021-05-25T01:56:08.103Z</updated>
        <summary type="html"><![CDATA[Virtual Personal Assistants like Siri have great potential but such
developments hit the fundamental problem of how to make computational devices
that understand human speech. Natural language understanding is one of the more
disappointing failures of AI research and it seems there is something we
computer scientists don't get about the nature of language. Of course
philosophers and linguists think quite differently about language and this
paper describes how we have taken ideas from other disciplines and implemented
them. The background to the work is to take seriously the notion of language as
action and look at what people actually do with language using the techniques
of Conversation Analysis. The observation has been that human communication is
(behind the scenes) about the management of social relations as well as the
(foregrounded) passing of information. To claim this is one thing but to
implement it requires a mechanism. The mechanism described here is based on the
notion of language being intentional - we think intentionally, talk about them
and recognise them in others - and cooperative in that we are compelled to help
out. The way we are compelled points to a solution to the ever present problem
of keeping the human on topic. The approach has led to a recent success in
which we significantly improve user satisfaction independent of task
completion. Talk Markup Language (TalkML) is a draft alternative to VoiceXML
that, we propose, greatly simplifies the scripting of interaction by providing
default behaviours for no input and not recognised speech events.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1"&gt;Peter Wallis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[View Distillation with Unlabeled Data for Extracting Adverse Drug Effects from User-Generated Data. (arXiv:2105.11354v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11354</id>
        <link href="http://arxiv.org/abs/2105.11354"/>
        <updated>2021-05-25T01:56:08.089Z</updated>
        <summary type="html"><![CDATA[We present an algorithm based on multi-layer transformers for identifying
Adverse Drug Reactions (ADR) in social media data. Our model relies on the
properties of the problem and the characteristics of contextual word embeddings
to extract two views from documents. Then a classifier is trained on each view
to label a set of unlabeled documents to be used as an initializer for a new
classifier in the other view. Finally, the initialized classifier in each view
is further trained using the initial training examples. We evaluated our model
in the largest publicly available ADR dataset. The experiments testify that our
model significantly outperforms the transformer-based models pretrained on
domain-specific data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1"&gt;Payam Karisani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jinho D. Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DaN+: Danish Nested Named Entities and Lexical Normalization. (arXiv:2105.11301v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11301</id>
        <link href="http://arxiv.org/abs/2105.11301"/>
        <updated>2021-05-25T01:56:08.068Z</updated>
        <summary type="html"><![CDATA[This paper introduces DaN+, a new multi-domain corpus and annotation
guidelines for Danish nested named entities (NEs) and lexical normalization to
support research on cross-lingual cross-domain learning for a less-resourced
language. We empirically assess three strategies to model the two-layer Named
Entity Recognition (NER) task. We compare transfer capabilities from German
versus in-language annotation from scratch. We examine language-specific versus
multilingual BERT, and study the effect of lexical normalization on NER. Our
results show that 1) the most robust strategy is multi-task learning which is
rivaled by multi-label decoding, 2) BERT-based NER models are sensitive to
domain shifts, and 3) in-language BERT and lexical normalization are the most
beneficial on the least canonical data. Our results also show that an
out-of-domain setup remains challenging, while performance on news plateaus
quickly. This highlights the importance of cross-domain evaluation of
cross-lingual transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1"&gt;Barbara Plank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jensen_K/0/1/0/all/0/1"&gt;Kristian N&amp;#xf8;rgaard Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1"&gt;Rob van der Goot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving BERT with Syntax-aware Local Attention. (arXiv:2012.15150v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15150</id>
        <link href="http://arxiv.org/abs/2012.15150"/>
        <updated>2021-05-25T01:56:08.061Z</updated>
        <summary type="html"><![CDATA[Pre-trained Transformer-based neural language models, such as BERT, have
achieved remarkable results on varieties of NLP tasks. Recent works have shown
that attention-based models can benefit from more focused attention over local
regions. Most of them restrict the attention scope within a linear span, or
confine to certain tasks such as machine translation and question answering. In
this paper, we propose a syntax-aware local attention, where the attention
scopes are restrained based on the distances in the syntactic structure. The
proposed syntax-aware local attention can be integrated with pretrained
language models, such as BERT, to render the model to focus on syntactically
relevant words. We conduct experiments on various single-sentence benchmarks,
including sentence classification and sequence labeling tasks. Experimental
results show consistent gains over BERT on all benchmark datasets. The
extensive studies verify that our model achieves better performance owing to
more focused attention over syntactically relevant words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhongli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qingyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yunbo Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Editorial introduction: The power of words and networks. (arXiv:2105.11263v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11263</id>
        <link href="http://arxiv.org/abs/2105.11263"/>
        <updated>2021-05-25T01:56:08.051Z</updated>
        <summary type="html"><![CDATA[According to Freud "words were originally magic and to this day words have
retained much of their ancient magical power". By words, behaviors are
transformed and problems are solved. The way we use words reveals our
intentions, goals and values. Novel tools for text analysis help understand the
magical power of words. This power is multiplied, if it is combined with the
study of social networks, i.e. with the analysis of relationships among social
units. This special issue of the International Journal of Information
Management, entitled "Combining Social Network Analysis and Text Mining: from
Theory to Practice", includes heterogeneous and innovative research at the
nexus of text mining and social network analysis. It aims to enrich work at the
intersection of these fields, which still lags behind in theoretical,
empirical, and methodological foundations. The nine articles accepted for
inclusion in this special issue all present methods and tools that have
business applications. They are summarized in this editorial introduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1"&gt;P. Gloor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iezzi_D/0/1/0/all/0/1"&gt;D. F. Iezzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing perceived organizational leadership styles through twitter text mining. (arXiv:2105.11276v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11276</id>
        <link href="http://arxiv.org/abs/2105.11276"/>
        <updated>2021-05-25T01:56:08.043Z</updated>
        <summary type="html"><![CDATA[We propose a text classification tool based on support vector machines for
the assessment of organizational leadership styles, as appearing to Twitter
users. We collected Twitter data over 51 days, related to the first 30 Italian
organizations in the 2015 ranking of Forbes Global 2000-out of which we
selected the five with the most relevant volumes of tweets. We analyzed the
communication of the company leaders, together with the dialogue among the
stakeholders of each company, to understand the association with perceived
leadership styles and dimensions. To assess leadership profiles, we referred to
the 10-factor model developed by Barchiesi and La Bella in 2007. We maintain
the distinctiveness of the approach we propose, as it allows a rapid assessment
of the perceived leadership capabilities of an enterprise, as they emerge from
its social media interactions. It can also be used to show how companies
respond and manage their communication when specific events take place, and to
assess their stakeholder's reactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bella_A/0/1/0/all/0/1"&gt;A. La Bella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battistoni_E/0/1/0/all/0/1"&gt;E. Battistoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castellan_S/0/1/0/all/0/1"&gt;S. Castellan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francucci_M/0/1/0/all/0/1"&gt;M. Francucci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs. (arXiv:2105.11225v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11225</id>
        <link href="http://arxiv.org/abs/2105.11225"/>
        <updated>2021-05-25T01:56:08.035Z</updated>
        <summary type="html"><![CDATA[Label noise and long-tailed distributions are two major challenges in
distantly supervised relation extraction. Recent studies have shown great
progress on denoising, but pay little attention to the problem of long-tailed
relations. In this paper, we introduce constraint graphs to model the
dependencies between relation labels. On top of that, we further propose a
novel constraint graph-based relation extraction framework(CGRE) to handle the
two challenges simultaneously. CGRE employs graph convolution networks (GCNs)
to propagate information from data-rich relation nodes to data-poor relation
nodes, and thus boosts the representation learning of long-tailed relations. To
further improve the noise immunity, a constraint-aware attention module is
designed in CGRE to integrate the constraint information. Experimental results
on a widely-used benchmark dataset indicate that our approach achieves
significant improvements over the previous methods for both denoising and
long-tailed relation extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tianming Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1"&gt;Gaurav Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Maozu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Machine Translation with Monolingual Translation Memory. (arXiv:2105.11269v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11269</id>
        <link href="http://arxiv.org/abs/2105.11269"/>
        <updated>2021-05-25T01:56:08.009Z</updated>
        <summary type="html"><![CDATA[Prior work has proved that Translation memory (TM) can boost the performance
of Neural Machine Translation (NMT). In contrast to existing work that uses
bilingual corpus as TM and employs source-side similarity search for memory
retrieval, we propose a new framework that uses monolingual memory and performs
learnable memory retrieval in a cross-lingual manner. Our framework has unique
advantages. First, the cross-lingual memory retriever allows abundant
monolingual data to be TM. Second, the memory retriever and NMT model can be
jointly optimized for the ultimate translation goal. Experiments show that the
proposed method obtains substantial improvements. Remarkably, it even
outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the
ability to leverage monolingual data, our model also demonstrates effectiveness
in low-resource and domain adaptation scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huayang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lemao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StructuralLM: Structural Pre-training for Form Understanding. (arXiv:2105.11210v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11210</id>
        <link href="http://arxiv.org/abs/2105.11210"/>
        <updated>2021-05-25T01:56:08.000Z</updated>
        <summary type="html"><![CDATA[Large pre-trained language models achieve state-of-the-art results when
fine-tuned on downstream NLP tasks. However, they almost exclusively focus on
text-only representation, while neglecting cell-level layout information that
is important for form image understanding. In this paper, we propose a new
pre-training approach, StructuralLM, to jointly leverage cell and layout
information from scanned documents. Specifically, we pre-train StructuralLM
with two new designs to make the most of the interactions of cell and layout
information: 1) each cell as a semantic unit; 2) classification of cell
positions. The pre-trained StructuralLM achieves new state-of-the-art results
in different types of downstream tasks, including form understanding (from
78.95 to 85.14), document visual question answering (from 72.59 to 83.94) and
document image classification (from 94.43 to 96.08).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1"&gt;Bin Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Songfang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Upsampling for Protest Size Detection. (arXiv:2105.11260v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11260</id>
        <link href="http://arxiv.org/abs/2105.11260"/>
        <updated>2021-05-25T01:56:07.984Z</updated>
        <summary type="html"><![CDATA[We propose a new task and dataset for a common problem in social science
research: "upsampling" coarse document labels to fine-grained labels or spans.
We pose the problem in a question answering format, with the answers providing
the fine-grained labels. We provide a benchmark dataset and baselines on a
socially impactful task: identifying the exact crowd size at protests and
demonstrations in the United States given only order-of-magnitude information
about protest attendance, a very small sample of fine-grained examples, and
English-language news text. We evaluate several baseline models, including
zero-shot results from rule-based and question-answering models, few-shot
models fine-tuned on a small set of documents, and weakly supervised models
using a larger set of coarsely-labeled documents. We find that our rule-based
model initially outperforms a zero-shot pre-trained transformer language model
but that further fine-tuning on a very small subset of 25 examples
substantially improves out-of-sample performance. We also demonstrate a method
for fine-tuning the transformer span on only the coarse labels that performs
similarly to our rule-based approach. This work will contribute to social
scientists' ability to generate data to understand the causes and successes of
collective action.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Halterman_A/0/1/0/all/0/1"&gt;Andrew Halterman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radford_B/0/1/0/all/0/1"&gt;Benjamin J. Radford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hater-O-Genius Aggression Classification using Capsule Networks. (arXiv:2105.11219v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11219</id>
        <link href="http://arxiv.org/abs/2105.11219"/>
        <updated>2021-05-25T01:56:07.977Z</updated>
        <summary type="html"><![CDATA[Contending hate speech in social media is one of the most challenging social
problems of our time. There are various types of anti-social behavior in social
media. Foremost of them is aggressive behavior, which is causing many social
issues such as affecting the social lives and mental health of social media
users. In this paper, we propose an end-to-end ensemble-based architecture to
automatically identify and classify aggressive tweets. Tweets are classified
into three categories - Covertly Aggressive, Overtly Aggressive, and
Non-Aggressive. The proposed architecture is an ensemble of smaller subnetworks
that are able to characterize the feature embeddings effectively. We
demonstrate qualitatively that each of the smaller subnetworks is able to learn
unique features. Our best model is an ensemble of Capsule Networks and results
in a 65.2% F1 score on the Facebook test set, which results in a performance
gain of 0.95% over the TRAC-2018 winners. The code and the model weights are
publicly available at
https://github.com/parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1"&gt;Parth Patwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+PYKL_S/0/1/0/all/0/1"&gt;Srinivas PYKL&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Amitava Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_P/0/1/0/all/0/1"&gt;Prerana Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulabaigari_V/0/1/0/all/0/1"&gt;Viswanath Pulabaigari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Standard Criteria for human evaluation of Chatbots: A Survey. (arXiv:2105.11197v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11197</id>
        <link href="http://arxiv.org/abs/2105.11197"/>
        <updated>2021-05-25T01:56:07.966Z</updated>
        <summary type="html"><![CDATA[Human evaluation is becoming a necessity to test the performance of Chatbots.
However, off-the-shelf settings suffer the severe reliability and replication
issues partly because of the extremely high diversity of criteria. It is high
time to come up with standard criteria and exact definitions. To this end, we
conduct a through investigation of 105 papers involving human evaluation for
Chatbots. Deriving from this, we propose five standard criteria along with
precise definitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hongru Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huaqing Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-lingual Text Classification with Heterogeneous Graph Neural Network. (arXiv:2105.11246v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11246</id>
        <link href="http://arxiv.org/abs/2105.11246"/>
        <updated>2021-05-25T01:56:07.957Z</updated>
        <summary type="html"><![CDATA[Cross-lingual text classification aims at training a classifier on the source
language and transferring the knowledge to target languages, which is very
useful for low-resource languages. Recent multilingual pretrained language
models (mPLM) achieve impressive results in cross-lingual classification tasks,
but rarely consider factors beyond semantic similarity, causing performance
degradation between some language pairs. In this paper we propose a simple yet
effective method to incorporate heterogeneous information within and across
languages for cross-lingual text classification using graph convolutional
networks (GCN). In particular, we construct a heterogeneous graph by treating
documents and words as nodes, and linking nodes with different relations, which
include part-of-speech roles, semantic similarity, and document translations.
Extensive experiments show that our graph-based method significantly
outperforms state-of-the-art models on all tasks, and also achieves consistent
performance gain over baselines in low-resource settings where external tools
like translators are unavailable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Peiji Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shixing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhisheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models. (arXiv:2105.11136v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11136</id>
        <link href="http://arxiv.org/abs/2105.11136"/>
        <updated>2021-05-25T01:56:07.937Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models have achieved human-level performance on many
Machine Reading Comprehension (MRC) tasks, but it remains unclear whether these
models truly understand language or answer questions by exploiting statistical
biases in datasets. Here, we demonstrate a simple yet effective method to
attack MRC models and reveal the statistical biases in these models. We apply
the method to the RACE dataset, for which the answer to each MRC question is
selected from 4 options. It is found that several pre-trained language models,
including BERT, ALBERT, and RoBERTa, show consistent preference to some
options, even when these options are irrelevant to the question. When
interfered by these irrelevant options, the performance of MRC models can be
reduced from human-level performance to the chance-level performance. Human
readers, however, are not clearly affected by these irrelevant options.
Finally, we propose an augmented training method that can greatly reduce
models' statistical biases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jieyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Jiajie Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Nai Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Preserving Text Simplification. (arXiv:2105.11178v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11178</id>
        <link href="http://arxiv.org/abs/2105.11178"/>
        <updated>2021-05-25T01:56:07.832Z</updated>
        <summary type="html"><![CDATA[We present a context-preserving text simplification (TS) approach that
recursively splits and rephrases complex English sentences into a semantic
hierarchy of simplified sentences. Using a set of linguistically principled
transformation patterns, input sentences are converted into a hierarchical
representation in the form of core sentences and accompanying contexts that are
linked via rhetorical relations. Hence, as opposed to previously proposed
sentence splitting approaches, which commonly do not take into account
discourse-level aspects, our TS approach preserves the semantic relationship of
the decomposed constituents in the output. A comparative analysis with the
annotations contained in the RST-DT shows that we are able to capture the
contextual hierarchy between the split sentences with a precision of 89% and
reach an average precision of 69% for the classification of the rhetorical
relations that hold between them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niklaus_C/0/1/0/all/0/1"&gt;Christina Niklaus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cetto_M/0/1/0/all/0/1"&gt;Matthias Cetto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Freitas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Handschuh_S/0/1/0/all/0/1"&gt;Siegfried Handschuh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prevent the Language Model from being Overconfident in Neural Machine Translation. (arXiv:2105.11098v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11098</id>
        <link href="http://arxiv.org/abs/2105.11098"/>
        <updated>2021-05-25T01:56:07.756Z</updated>
        <summary type="html"><![CDATA[The Neural Machine Translation (NMT) model is essentially a joint language
model conditioned on both the source sentence and partial translation.
Therefore, the NMT model naturally involves the mechanism of the Language Model
(LM) that predicts the next token only based on partial translation. Despite
its success, NMT still suffers from the hallucination problem, generating
fluent but inadequate translations. The main reason is that NMT pays excessive
attention to the partial translation while neglecting the source sentence to
some extent, namely overconfidence of the LM. Accordingly, we define the Margin
between the NMT and the LM, calculated by subtracting the predicted probability
of the LM from that of the NMT model for each token. The Margin is negatively
correlated to the overconfidence degree of the LM. Based on the property, we
propose a Margin-based Token-level Objective (MTO) and a Margin-based
Sentencelevel Objective (MSO) to maximize the Margin for preventing the LM from
being overconfident. Experiments on WMT14 English-to-German, WMT19
Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate
the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements,
respectively, compared to the Transformer baseline. The human evaluation
further verifies that our approaches improve translation adequacy as well as
fluency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_M/0/1/0/all/0/1"&gt;Mengqi Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yijin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiao-Hua Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Text Edition by Changing Answers of Specific Questions. (arXiv:2105.11018v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11018</id>
        <link href="http://arxiv.org/abs/2105.11018"/>
        <updated>2021-05-25T01:56:07.749Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce the new task of controllable text edition, in
which we take as input a long text, a question, and a target answer, and the
output is a minimally modified text, so that it fits the target answer. This
task is very important in many situations, such as changing some conditions,
consequences, or properties in a legal document, or changing some key
information of an event in a news text. This is very challenging, as it is hard
to obtain a parallel corpus for training, and we need to first find all text
positions that should be changed and then decide how to change them. We
constructed the new dataset WikiBioCTE for this task based on the existing
dataset WikiBio (originally created for table-to-text generation). We use
WikiBioCTE for training, and manually labeled a test set for testing. We also
propose novel evaluation metrics and a novel method for solving the new task.
Experimental results on the test set show that our proposed method is a good
fit for this novel NLP task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1"&gt;Lei Sha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hohenecker_P/0/1/0/all/0/1"&gt;Patrick Hohenecker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structural Pre-training for Dialogue Comprehension. (arXiv:2105.10956v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10956</id>
        <link href="http://arxiv.org/abs/2105.10956"/>
        <updated>2021-05-25T01:56:07.740Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (PrLMs) have demonstrated superior performance
due to their strong ability to learn universal language representations from
self-supervised pre-training. However, even with the help of the powerful
PrLMs, it is still challenging to effectively capture task-related knowledge
from dialogue texts which are enriched by correlations among speaker-aware
utterances. In this work, we present SPIDER, Structural Pre-traIned DialoguE
Reader, to capture dialogue exclusive features. To simulate the dialogue-like
features, we propose two training objectives in addition to the original LM
objectives: 1) utterance order restoration, which predicts the order of the
permuted utterances in dialogue context; 2) sentence backbone regularization,
which regularizes the model to improve the factual correctness of summarized
subject-verb-object triplets. Experimental results on widely used dialogue
benchmarks verify the effectiveness of the newly introduced self-supervised
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One2Set: Generating Diverse Keyphrases as a Set. (arXiv:2105.11134v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11134</id>
        <link href="http://arxiv.org/abs/2105.11134"/>
        <updated>2021-05-25T01:56:07.733Z</updated>
        <summary type="html"><![CDATA[Recently, the sequence-to-sequence models have made remarkable progress on
the task of keyphrase generation (KG) by concatenating multiple keyphrases in a
predefined order as a target sequence during training. However, the keyphrases
are inherently an unordered set rather than an ordered sequence. Imposing a
predefined order will introduce wrong bias during training, which can highly
penalize shifts in the order between keyphrases. In this work, we propose a new
training paradigm One2Set without predefining an order to concatenate the
keyphrases. To fit this paradigm, we propose a novel model that utilizes a
fixed set of learned control codes as conditions to generate a set of
keyphrases in parallel. To solve the problem that there is no correspondence
between each prediction and target during training, we propose a $K$-step
target assignment mechanism via bipartite matching, which greatly increases the
diversity and reduces the duplication ratio of generated keyphrases. The
experimental results on multiple benchmarks demonstrate that our approach
significantly outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jiacheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1"&gt;Tao Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yichao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yige Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Base Data To Knowledge Discovery -- A Life Cycle Approach -- Using Multilayer Networks. (arXiv:2105.11410v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.11410</id>
        <link href="http://arxiv.org/abs/2105.11410"/>
        <updated>2021-05-25T01:56:07.724Z</updated>
        <summary type="html"><![CDATA[Any large complex data analysis to infer or discover meaningful
information/knowledge involves the following steps (in addition to data
collection, cleaning, preparing the data for analysis such as attribute
elimination): i) Modeling the data -- an approach for modeling and deriving a
data representation for analysis using that approach, ii) translating analysis
objectives into computations on the model generated; this can be as simple as a
single computation (e.g., community detection) or may involve a sequence of
operations (e.g., pair-wise community detection over multiple networks) using
expressions based on the model, iii) computation of the expressions generated
-- efficiency and scalability come into picture here, and iv) drill-down of
results to interpret or understand them clearly. Beyond this, it is also
meaningful to visualize results for easier understanding. Covid-19
visualization dashboard presented in this paper is an example of this.

This paper covers all of the above steps of data analysis life cycle using a
data representation that is gaining importance for multi-entity, multi-feature
data sets - Multilayer Networks. We use several data sets to establish the
effectiveness of modeling using MLNs and analyze them using the proposed
decoupling approach. For coverage, we use different types of MLNs for modeling,
and community and centrality computations for analysis. The data sets used - US
commercial airlines, IMDb, DBLP, and Covid-19 data set. Our experimental
analyses using the identified steps validate modeling, breadth of objectives
that can be computed, and overall versatility of the life cycle approach.
Correctness of results is verified, where possible, using independently
available ground truth. We demonstrate drill-down that is afforded by this
approach (due to structure and semantics preservation) for a better
understanding and visualization of results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santra_A/0/1/0/all/0/1"&gt;Abhishek Santra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komar_K/0/1/0/all/0/1"&gt;Kanthi Komar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhowmick_S/0/1/0/all/0/1"&gt;Sanjukta Bhowmick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthy_S/0/1/0/all/0/1"&gt;Sharma Chakravarthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retrieval Enhanced Model for Commonsense Generation. (arXiv:2105.11174v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11174</id>
        <link href="http://arxiv.org/abs/2105.11174"/>
        <updated>2021-05-25T01:56:07.702Z</updated>
        <summary type="html"><![CDATA[Commonsense generation is a challenging task of generating a plausible
sentence describing an everyday scenario using provided concepts. Its
requirement of reasoning over commonsense knowledge and compositional
generalization ability even puzzles strong pre-trained language generation
models. We propose a novel framework using retrieval methods to enhance both
the pre-training and fine-tuning for commonsense generation. We retrieve
prototype sentence candidates by concept matching and use them as auxiliary
input. For fine-tuning, we further boost its performance with a trainable
sentence retriever. We demonstrate experimentally on the large-scale CommonGen
benchmark that our approach achieves new state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Han Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chenguang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1"&gt;Linjun Shou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Ming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yichong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1"&gt;Michael Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fighting an Infodemic: COVID-19 Fake News Dataset. (arXiv:2011.03327v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03327</id>
        <link href="http://arxiv.org/abs/2011.03327"/>
        <updated>2021-05-25T01:56:07.682Z</updated>
        <summary type="html"><![CDATA[Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news
and rumors are rampant on social media. Believing in rumors can cause
significant harm. This is further exacerbated at the time of a pandemic. To
tackle this, we curate and release a manually annotated dataset of 10,700
social media posts and articles of real and fake news on COVID-19. We benchmark
the annotated dataset with four machine learning baselines - Decision Tree,
Logistic Regression, Gradient Boost, and Support Vector Machine (SVM). We
obtain the best performance of 93.46% F1-score with SVM. The data and code is
available at: https://github.com/parthpatwa/covid19-fake-news-dectection]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1"&gt;Parth Patwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Shivam Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pykl_S/0/1/0/all/0/1"&gt;Srinivas Pykl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guptha_V/0/1/0/all/0/1"&gt;Vineeth Guptha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumari_G/0/1/0/all/0/1"&gt;Gitanjali Kumari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1"&gt;Md Shad Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Amitava Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Techniques Toward Optimizing Viewability in RTB Ad Campaigns Using Reinforcement Learning. (arXiv:2105.10587v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10587</id>
        <link href="http://arxiv.org/abs/2105.10587"/>
        <updated>2021-05-25T01:56:07.674Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) is an effective technique for training
decision-making agents through interactions with their environment. The advent
of deep learning has been associated with highly notable successes with
sequential decision making problems - such as defeating some of the
highest-ranked human players at Go. In digital advertising, real-time bidding
(RTB) is a common method of allocating advertising inventory through real-time
auctions. Bidding strategies need to incorporate logic for dynamically
adjusting parameters in order to deliver pre-assigned campaign goals. Here we
discuss techniques toward using RL to train bidding agents. As a campaign
metric we particularly focused on viewability: the percentage of inventory
which goes on to be viewed by an end user.

This paper is presented as a survey of techniques and experiments which we
developed through the course of this research. We discuss expanding our
training data to include edge cases by training on simulated interactions. We
discuss the experimental results comparing the performance of several promising
RL algorithms, and an approach to hyperparameter optimization of an
actor/critic training pipeline through Bayesian optimization. Finally, we
present live-traffic tests of some of our RL agents against a rule-based
feedback-control approach, demonstrating the potential for this method as well
as areas for further improvement. This paper therefore presents an arrangement
of our findings in this quickly developing field, and ways that it can be
applied to an RTB use case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tashman_M/0/1/0/all/0/1"&gt;Michael Tashman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1"&gt;John Hoffman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiayi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fengdan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morsali_A/0/1/0/all/0/1"&gt;Atefeh Morsali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winikor_L/0/1/0/all/0/1"&gt;Lee Winikor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerami_R/0/1/0/all/0/1"&gt;Rouzbeh Gerami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deconfounded Recommendation for Alleviating Bias Amplification. (arXiv:2105.10648v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10648</id>
        <link href="http://arxiv.org/abs/2105.10648"/>
        <updated>2021-05-25T01:56:07.633Z</updated>
        <summary type="html"><![CDATA[Recommender systems usually amplify the biases in the data. The model learned
from historical interactions with imbalanced item distribution will amplify the
imbalance by over-recommending items from the major groups. Addressing this
issue is essential for a healthy ecosystem of recommendation in the long run.
Existing works apply bias control to the ranking targets (e.g., calibration,
fairness, and diversity), but ignore the true reason for bias amplification and
trade-off the recommendation accuracy.

In this work, we scrutinize the cause-effect factors for bias amplification,
identifying the main reason lies in the confounder effect of imbalanced item
distribution on user representation and prediction score. The existence of such
confounder pushes us to go beyond merely modeling the conditional probability
and embrace the causal modeling for recommendation. Towards this end, we
propose a Deconfounded Recommender System (DecRS), which models the causal
effect of user representation on the prediction score. The key to eliminating
the impact of the confounder lies in backdoor adjustment, which is however
difficult to do due to the infinite sample space of the confounder. For this
challenge, we contribute an approximation operator for backdoor adjustment
which can be easily plugged into most recommender models. Lastly, we devise an
inference strategy to dynamically regulate backdoor adjustment according to
user status. We instantiate DecRS on two representative models FM and NFM, and
conduct extensive experiments over two benchmarks to validate the superiority
of our proposed DecRS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Abusive Language Detection in Heterogeneous Contexts: Dataset Collection and the Role of Supervised Attention. (arXiv:2105.11119v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11119</id>
        <link href="http://arxiv.org/abs/2105.11119"/>
        <updated>2021-05-25T01:56:07.607Z</updated>
        <summary type="html"><![CDATA[Abusive language is a massive problem in online social platforms. Existing
abusive language detection techniques are particularly ill-suited to comments
containing heterogeneous abusive language patterns, i.e., both abusive and
non-abusive parts. This is due in part to the lack of datasets that explicitly
annotate heterogeneity in abusive language. We tackle this challenge by
providing an annotated dataset of abusive language in over 11,000 comments from
YouTube. We account for heterogeneity in this dataset by separately annotating
both the comment as a whole and the individual sentences that comprise each
comment. We then propose an algorithm that uses a supervised attention
mechanism to detect and categorize abusive content using multi-task learning.
We empirically demonstrate the challenges of using traditional techniques on
heterogeneous content and the comparative gains in performance of the proposed
approach over state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1"&gt;Hongyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valido_A/0/1/0/all/0/1"&gt;Alberto Valido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ingram_K/0/1/0/all/0/1"&gt;Katherine M. Ingram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fanti_G/0/1/0/all/0/1"&gt;Giulia Fanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Suma Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espelage_D/0/1/0/all/0/1"&gt;Dorothy L. Espelage&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Fact-Checking for Assisting Human Fact-Checkers. (arXiv:2103.07769v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07769</id>
        <link href="http://arxiv.org/abs/2103.07769"/>
        <updated>2021-05-25T01:56:07.584Z</updated>
        <summary type="html"><![CDATA[The reporting and the analysis of current events around the globe has
expanded from professional, editor-lead journalism all the way to citizen
journalism. Nowadays, politicians and other key players enjoy direct access to
their audiences through social media, bypassing the filters of official cables
or traditional media. However, the multiple advantages of free speech and
direct communication are dimmed by the misuse of media to spread inaccurate or
misleading claims. These phenomena have led to the modern incarnation of the
fact-checker -- a professional whose main aim is to examine claims using
available evidence and to assess their veracity. As in other text forensics
tasks, the amount of information available makes the work of the fact-checker
more difficult. With this in mind, starting from the perspective of the
professional fact-checker, we survey the available intelligent technologies
that can support the human expert in the different steps of her fact-checking
endeavor. These include identifying claims worth fact-checking, detecting
relevant previously fact-checked claims, retrieving relevant evidence to
fact-check a claim, and actually verifying a claim. In each case, we pay
attention to the challenges in future work and the potential impact on
real-world fact-checking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1"&gt;Preslav Nakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corney_D/0/1/0/all/0/1"&gt;David Corney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1"&gt;Maram Hasanain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1"&gt;Tamer Elsayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barron_Cedeno_A/0/1/0/all/0/1"&gt;Alberto Barr&amp;#xf3;n-Cede&amp;#xf1;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papotti_P/0/1/0/all/0/1"&gt;Paolo Papotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1"&gt;Shaden Shaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1"&gt;Giovanni Da San Martino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Video Summarization with a Convolutional Attentive Adversarial Network. (arXiv:2105.11131v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.11131</id>
        <link href="http://arxiv.org/abs/2105.11131"/>
        <updated>2021-05-25T01:56:07.554Z</updated>
        <summary type="html"><![CDATA[With the explosive growth of video data, video summarization, which attempts
to seek the minimum subset of frames while still conveying the main story, has
become one of the hottest topics. Nowadays, substantial achievements have been
made by supervised learning techniques, especially after the emergence of deep
learning. However, it is extremely expensive and difficult to collect human
annotation for large-scale video datasets. To address this problem, we propose
a convolutional attentive adversarial network (CAAN), whose key idea is to
build a deep summarizer in an unsupervised way. Upon the generative adversarial
network, our overall framework consists of a generator and a discriminator. The
former predicts importance scores for all frames of a video while the latter
tries to distinguish the score-weighted frame features from original frame
features. Specifically, the generator employs a fully convolutional sequence
network to extract global representation of a video, and an attention-based
network to output normalized importance scores. To learn the parameters, our
objective function is composed of three loss functions, which can guide the
frame-level importance score prediction collaboratively. To validate this
proposed method, we have conducted extensive experiments on two public
benchmarks SumMe and TVSum. The results show the superiority of our proposed
method against other state-of-the-art unsupervised approaches. Our method even
outperforms some published supervised approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1"&gt;Guoqiang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1"&gt;Yanbing Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shucheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shizhou Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanning Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesizer: Rethinking Self-Attention in Transformer Models. (arXiv:2005.00743v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00743</id>
        <link href="http://arxiv.org/abs/2005.00743"/>
        <updated>2021-05-25T01:56:07.532Z</updated>
        <summary type="html"><![CDATA[The dot product self-attention is known to be central and indispensable to
state-of-the-art Transformer models. But is it really required? This paper
investigates the true importance and contribution of the dot product-based
self-attention mechanism on the performance of Transformer models. Via
extensive experiments, we find that (1) random alignment matrices surprisingly
perform quite competitively and (2) learning attention weights from token-token
(query-key) interactions is useful but not that important after all. To this
end, we propose \textsc{Synthesizer}, a model that learns synthetic attention
weights without token-token interactions. In our experiments, we first show
that simple Synthesizers achieve highly competitive performance when compared
against vanilla Transformer models across a range of tasks, including machine
translation, language modeling, text generation and GLUE/SuperGLUE benchmarks.
When composed with dot product attention, we find that Synthesizers
consistently outperform Transformers. Moreover, we conduct additional
comparisons of Synthesizers against Dynamic Convolutions, showing that simple
Random Synthesizer is not only $60\%$ faster but also improves perplexity by a
relative $3.5\%$. Finally, we show that simple factorized Synthesizers can
outperform Linformers on encoding only tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1"&gt;Da-Cheng Juan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Che Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CEREC: A Corpus for Entity Resolution in Email Conversations. (arXiv:2105.10606v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10606</id>
        <link href="http://arxiv.org/abs/2105.10606"/>
        <updated>2021-05-25T01:56:07.501Z</updated>
        <summary type="html"><![CDATA[We present the first large scale corpus for entity resolution in email
conversations (CEREC). The corpus consists of 6001 email threads from the Enron
Email Corpus containing 36,448 email messages and 60,383 entity coreference
chains. The annotation is carried out as a two-step process with minimal manual
effort. Experiments are carried out for evaluating different features and
performance of four baselines on the created corpus. For the task of mention
identification and coreference resolution, a best performance of 60.08 F1 is
reported, highlighting the room for improvement. An in-depth qualitative and
quantitative error analysis is presented to understand the limitations of the
baselines considered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dakle_P/0/1/0/all/0/1"&gt;Parag Pravin Dakle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moldovan_D/0/1/0/all/0/1"&gt;Dan I. Moldovan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLOW : Global Weighted Self-Attention Network for Web Search. (arXiv:2007.05186v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05186</id>
        <link href="http://arxiv.org/abs/2007.05186"/>
        <updated>2021-05-25T01:56:07.491Z</updated>
        <summary type="html"><![CDATA[Deep matching models aim to facilitate search engines retrieving more
relevant documents by mapping queries and documents into semantic vectors in
the first-stage retrieval. When leveraging BERT as the deep matching model, the
attention score across two words are solely built upon local contextualized
word embeddings. It lacks prior global knowledge to distinguish the importance
of different words, which has been proved to play a critical role in
information retrieval tasks. In addition to this, BERT only performs attention
across sub-words tokens which weakens whole word attention representation. We
propose a novel Global Weighted Self-Attention (GLOW) network for web document
search. GLOW fuses global corpus statistics into the deep matching model. By
adding prior weights into attention generation from global information, like
BM25, GLOW successfully learns weighted attention scores jointly with query
matrix $Q$ and key matrix $K$. We also present an efficient whole word weight
sharing solution to bring prior whole word knowledge into sub-words level
attention. It aids Transformer to learn whole word level attention. To make our
models applicable to complicated web search scenarios, we introduce combined
fields representation to accommodate documents with multiple fields even with
variable number of instances. We demonstrate GLOW is more efficient to capture
the topical and semantic representation both in queries and documents.
Intrinsic evaluation and experiments conducted on public data sets reveal GLOW
to be a general framework for document retrieve task. It significantly
outperforms BERT and other competitive baselines by a large margin while
retaining the same model complexity with BERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shan_X/0/1/0/all/0/1"&gt;Xuan Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chuanjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yiqian Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yusi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kaize Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yaobo Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1"&gt;Angen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuxiang Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Attention Networks Can Process Bounded Hierarchical Languages. (arXiv:2105.11115v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11115</id>
        <link href="http://arxiv.org/abs/2105.11115"/>
        <updated>2021-05-25T01:56:07.481Z</updated>
        <summary type="html"><![CDATA[Despite their impressive performance in NLP, self-attention networks were
recently proved to be limited for processing formal languages with hierarchical
structure, such as $\mathsf{Dyck}_k$, the language consisting of well-nested
parentheses of $k$ types. This suggested that natural language can be
approximated well with models that are too weak for formal languages, or that
the role of hierarchy and recursion in natural language might be limited. We
qualify this implication by proving that self-attention networks can process
$\mathsf{Dyck}_{k, D}$, the subset of $\mathsf{Dyck}_{k}$ with depth bounded by
$D$, which arguably better captures the bounded hierarchical structure of
natural language. Specifically, we construct a hard-attention network with
$D+1$ layers and $O(\log k)$ memory size (per token per layer) that recognizes
$\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and
$O(\log k)$ memory size that generates $\mathsf{Dyck}_{k, D}$. Experiments show
that self-attention networks trained on $\mathsf{Dyck}_{k, D}$ generalize to
longer inputs with near-perfect accuracy, and also verify the theoretical
memory advantage of self-attention networks over recurrent networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1"&gt;Shunyu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Binghui Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1"&gt;Christos Papadimitriou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue. (arXiv:2009.09945v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09945</id>
        <link href="http://arxiv.org/abs/2009.09945"/>
        <updated>2021-05-25T01:56:07.473Z</updated>
        <summary type="html"><![CDATA[Recommendation is a prevalent and critical service in information systems. To
provide personalized suggestions to users, industry players embrace machine
learning, more specifically, building predictive models based on the click
behavior data. This is known as the Click-Through Rate (CTR) prediction, which
has become the gold standard for building personalized recommendation service.
However, we argue that there is a significant gap between clicks and user
satisfaction -- it is common that a user is "cheated" to click an item by the
attractive title/cover of the item. This will severely hurt user's trust on the
system if the user finds the actual content of the clicked item disappointing.
What's even worse, optimizing CTR models on such flawed data will result in the
Matthew Effect, making the seemingly attractive but actually low-quality items
be more frequently recommended.

In this paper, we formulate the recommendation models as a causal graph that
reflects the cause-effect factors in recommendation, and address the clickbait
issue by performing counterfactual inference on the causal graph. We imagine a
counterfactual world where each item has only exposure features (i.e., the
features that the user can see before making a click decision). By estimating
the click likelihood of a user in the counterfactual world, we are able to
reduce the direct effect of exposure features and eliminate the clickbait
issue. Experiments on real-world datasets demonstrate that our method
significantly improves the post-click satisfaction of CTR models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanwang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effects of VR Gaming and Game Genre on Player Experience. (arXiv:2105.10754v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2105.10754</id>
        <link href="http://arxiv.org/abs/2105.10754"/>
        <updated>2021-05-25T01:56:07.464Z</updated>
        <summary type="html"><![CDATA[With the increasing availability of modern virtual reality (VR) headsets, the
use and applications of VR technology for gaming purposes have become more
pervasive than ever. Despite the growing popularity of VR gaming, user studies
into how it might affect the player experience (PX) during the gameplay are
scarce. Accordingly, the current study investigated the effects of VR gaming
and game genre on PX. We compared PX metrics for two game genres, strategy
(more interactive) and racing (less interactive), across two gaming platforms,
VR and traditional desktop gaming. Participants were randomly assigned to one
of the gaming platforms, played both a strategy and racing game on their
corresponding platform, and provided PX ratings. Results revealed that,
regardless of the game genre, participants in the VR gaming condition
experienced a greater level of sense of presence than did those in the desktop
gaming condition. That said, results showed that the two gaming platforms did
not significantly differ from one another in PX ratings. As for the effect of
game genre, participants provided greater PX ratings for the strategy game than
for the racing game, regardless of whether the game was played on a VR headset
or desktop computer. Collectively, these results indicate that although VR
gaming affords a greater sense of presence in the game environment, this
increase in presence does not seem to translate into a more satisfactory PX
when playing either a strategy or racing game.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carroll_M/0/1/0/all/0/1"&gt;Michael Carroll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osborne_E/0/1/0/all/0/1"&gt;Ethan Osborne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yildirim_C/0/1/0/all/0/1"&gt;Caglar Yildirim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising. (arXiv:2105.08489v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08489</id>
        <link href="http://arxiv.org/abs/2105.08489"/>
        <updated>2021-05-25T01:56:07.438Z</updated>
        <summary type="html"><![CDATA[In most real-world large-scale online applications (e.g., e-commerce or
finance), customer acquisition is usually a multi-step conversion process of
audiences. For example, an impression->click->purchase process is usually
performed of audiences for e-commerce platforms. However, it is more difficult
to acquire customers in financial advertising (e.g., credit card advertising)
than in traditional advertising. On the one hand, the audience multi-step
conversion path is longer. On the other hand, the positive feedback is sparser
(class imbalance) step by step, and it is difficult to obtain the final
positive feedback due to the delayed feedback of activation. Multi-task
learning is a typical solution in this direction. While considerable multi-task
efforts have been made in this direction, a long-standing challenge is how to
explicitly model the long-path sequential dependence among audience multi-step
conversions for improving the end-to-end conversion. In this paper, we propose
an Adaptive Information Transfer Multi-task (AITM) framework, which models the
sequential dependence among audience multi-step conversions via the Adaptive
Information Transfer (AIT) module. The AIT module can adaptively learn what and
how much information to transfer for different conversion stages. Besides, by
combining the Behavioral Expectation Calibrator in the loss function, the AITM
framework can yield more accurate end-to-end conversion identification. The
proposed framework is deployed in Meituan app, which utilizes it to real-timely
show a banner to the audience with a high end-to-end conversion rate for
Meituan Co-Branded Credit Cards. Offline experimental results on both
industrial and public real-world datasets clearly demonstrate that the proposed
framework achieves significantly better performance compared with
state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1"&gt;Dongbo Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1"&gt;Peng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yinger Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yongchun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1"&gt;Fuzhen Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Aware Learning to Rank with Self-Attention. (arXiv:2005.10084v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10084</id>
        <link href="http://arxiv.org/abs/2005.10084"/>
        <updated>2021-05-25T01:56:07.424Z</updated>
        <summary type="html"><![CDATA[Learning to rank is a key component of many e-commerce search engines. In
learning to rank, one is interested in optimising the global ordering of a list
of items according to their utility for users.Popular approaches learn a
scoring function that scores items individually (i.e. without the context of
other items in the list) by optimising a pointwise, pairwise or listwise loss.
The list is then sorted in the descending order of the scores. Possible
interactions between items present in the same list are taken into account in
the training phase at the loss level. However, during inference, items are
scored individually, and possible interactions between them are not considered.
In this paper, we propose a context-aware neural network model that learns item
scores by applying a self-attention mechanism. The relevance of a given item is
thus determined in the context of all other items present in the list, both in
training and in inference. We empirically demonstrate significant performance
gains of self-attention based neural architecture over Multi-LayerPerceptron
baselines, in particular on a dataset coming from search logs of a large scale
e-commerce marketplace, Allegro.pl. This effect is consistent across popular
pointwise, pairwise and listwise losses.Finally, we report new state-of-the-art
results on MSLR-WEB30K, the learning to rank benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pobrotyn_P/0/1/0/all/0/1"&gt;Przemys&amp;#x142;aw Pobrotyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartczak_T/0/1/0/all/0/1"&gt;Tomasz Bartczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synowiec_M/0/1/0/all/0/1"&gt;Miko&amp;#x142;aj Synowiec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bialobrzeski_R/0/1/0/all/0/1"&gt;Rados&amp;#x142;aw Bia&amp;#x142;obrzeski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojar_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Bojar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11347</id>
        <link href="http://arxiv.org/abs/2105.11347"/>
        <updated>2021-05-25T01:56:07.414Z</updated>
        <summary type="html"><![CDATA[In this article, we present a description of our systems as a part of our
participation in the shared task namely Artificial Intelligence for Legal
Assistance (AILA 2019). This is an integral event of Forum for Information
Retrieval Evaluation-2019. The outcomes of this track would be helpful for the
automation of the working process of the Indian Judiciary System. The manual
working procedures and documentation at any level (from lower to higher court)
of the judiciary system are very complex in nature. The systems produced as a
part of this track would assist the law practitioners. It would be helpful for
common men too. This kind of track also opens the path of research of Natural
Language Processing (NLP) in the judicial domain. This track defined two
problems such as Task 1: Identifying relevant prior cases for a given situation
and Task 2: Identifying the most relevant statutes for a given situation. We
tackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As
per the results declared by the task organizers, we are in 3rd and a modest
position in Task 1 and Task 2 respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1"&gt;Baban Gain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1"&gt;Dibyanayan Bandyopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1"&gt;Arkadipta De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1"&gt;Tanik Saikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Embedding Learning Framework for Numerical Features in CTR Prediction. (arXiv:2012.08986v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08986</id>
        <link href="http://arxiv.org/abs/2012.08986"/>
        <updated>2021-05-25T01:56:07.396Z</updated>
        <summary type="html"><![CDATA[Click-Through Rate (CTR) prediction is critical for industrial recommender
systems, where most deep CTR models follow an Embedding \& Feature Interaction
paradigm. However, the majority of methods focus on designing network
architectures to better capture feature interactions while the feature
embedding, especially for numerical features, has been overlooked. Existing
approaches for numerical features are difficult to capture informative
knowledge because of the low capacity or hard discretization based on the
offline expertise feature engineering. In this paper, we propose a novel
embedding learning framework for numerical features in CTR prediction (AutoDis)
with high model capacity, end-to-end training and unique representation
properties preserved. AutoDis consists of three core components:
meta-embeddings, automatic discretization and aggregation. Specifically, we
propose meta-embeddings for each numerical field to learn global knowledge from
the perspective of field with a manageable number of parameters. Then the
differentiable automatic discretization performs soft discretization and
captures the correlations between the numerical features and meta-embeddings.
Finally, distinctive and informative embeddings are learned via an aggregation
function. Comprehensive experiments on two public and one industrial datasets
are conducted to validate the effectiveness of AutoDis. Moreover, AutoDis has
been deployed onto a mainstream advertising platform, where online A/B test
demonstrates the improvement over the base model by 2.1% and 2.7% in terms of
CTR and eCPM, respectively. In addition, the code of our framework is publicly
available in
MindSpore(https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/recommend/autodis).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Huifeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weinan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CITIES: Contextual Inference of Tail-Item Embeddings for Sequential Recommendation. (arXiv:2105.10868v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10868</id>
        <link href="http://arxiv.org/abs/2105.10868"/>
        <updated>2021-05-25T01:56:07.386Z</updated>
        <summary type="html"><![CDATA[Sequential recommendation techniques provide users with product
recommendations fitting their current preferences by handling dynamic user
preferences over time. Previous studies have focused on modeling sequential
dynamics without much regard to which of the best-selling products (i.e., head
items) or niche products (i.e., tail items) should be recommended. We
scrutinize the structural reason for why tail items are barely served in the
current sequential recommendation model, which consists of an item-embedding
layer, a sequence-modeling layer, and a recommendation layer. Well-designed
sequence-modeling and recommendation layers are expected to naturally learn
suitable item embeddings. However, tail items are likely to fall short of this
expectation because the current model structure is not suitable for learning
high-quality embeddings with insufficient data. Thus, tail items are rarely
recommended. To eliminate this issue, we propose a framework called CITIES,
which aims to enhance the quality of the tail-item embeddings by training an
embedding-inference function using multiple contextual head items so that the
recommendation performance improves for not only the tail items but also for
the head items. Moreover, our framework can infer new-item embeddings without
an additional learning process. Extensive experiments on two real-world
datasets show that applying CITIES to the state-of-the-art methods improves
recommendation performance for both tail and head items. We conduct an
additional experiment to verify that CITIES can infer suitable new-item
embeddings as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1"&gt;Seongwon Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hoyeop Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyunsouk Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Sehee Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable Relaxation of Sorting. (arXiv:2102.07831v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07831</id>
        <link href="http://arxiv.org/abs/2102.07831"/>
        <updated>2021-05-25T01:56:07.362Z</updated>
        <summary type="html"><![CDATA[Learning to Rank (LTR) algorithms are usually evaluated using Information
Retrieval metrics like Normalised Discounted Cumulative Gain (NDCG) or Mean
Average Precision. As these metrics rely on sorting predicted items' scores
(and thus, on items' ranks), their derivatives are either undefined or zero
everywhere. This makes them unsuitable for gradient-based optimisation, which
is the usual method of learning appropriate scoring functions. Commonly used
LTR loss functions are only loosely related to the evaluation metrics, causing
a mismatch between the optimisation objective and the evaluation criterion. In
this paper, we address this mismatch by proposing NeuralNDCG, a novel
differentiable approximation to NDCG. Since NDCG relies on the
non-differentiable sorting operator, we obtain NeuralNDCG by relaxing that
operator using NeuralSort, a differentiable approximation of sorting. As a
result, we obtain a new ranking loss function which is an arbitrarily accurate
approximation to the evaluation metric, thus closing the gap between the
training and the evaluation of LTR models. We introduce two variants of the
proposed loss function. Finally, the empirical evaluation shows that our
proposed method outperforms previous work aimed at direct optimisation of NDCG
and is competitive with the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pobrotyn_P/0/1/0/all/0/1"&gt;Przemys&amp;#x142;aw Pobrotyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bialobrzeski_R/0/1/0/all/0/1"&gt;Rados&amp;#x142;aw Bia&amp;#x142;obrzeski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregating E-commerce Search Results from Heterogeneous Sources via Hierarchical Reinforcement Learning. (arXiv:1902.08882v1 [cs.IR] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/1902.08882</id>
        <link href="http://arxiv.org/abs/1902.08882"/>
        <updated>2021-05-25T01:56:07.346Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the task of aggregating search results from
heterogeneous sources in an E-commerce environment. First, unlike traditional
aggregated web search that merely presents multi-sourced results in the first
page, this new task may present aggregated results in all pages and has to
dynamically decide which source should be presented in the current page.
Second, as pointed out by many existing studies, it is not trivial to rank
items from heterogeneous sources because the relevance scores from different
source systems are not directly comparable. To address these two issues, we
decompose the task into two subtasks in a hierarchical structure: a high-level
task for source selection where we model the sequential patterns of user
behaviors onto aggregated results in different pages so as to understand user
intents and select the relevant sources properly; and a low-level task for item
presentation where we formulate a slot filling process to sequentially present
the items instead of giving each item a relevance score when deciding the
presentation order of heterogeneous items. Since both subtasks can be naturally
formulated as sequential decision problems and learn from the future user
feedback on search results, we build our model with hierarchical reinforcement
learning. Extensive experiments demonstrate that our model obtains remarkable
improvements in search performance metrics, and achieves a higher user
satisfaction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1"&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_T/0/1/0/all/0/1"&gt;Tao Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haihong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bo Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OntoED: Low-resource Event Detection with Ontology Embedding. (arXiv:2105.10922v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10922</id>
        <link href="http://arxiv.org/abs/2105.10922"/>
        <updated>2021-05-25T01:56:07.333Z</updated>
        <summary type="html"><![CDATA[Event Detection (ED) aims to identify event trigger words from a given text
and classify it into an event type. Most of current methods to ED rely heavily
on training instances, and almost ignore the correlation of event types. Hence,
they tend to suffer from data scarcity and fail to handle new unseen event
types. To address these problems, we formulate ED as a process of event
ontology population: linking event instances to pre-defined event types in
event ontology, and propose a novel ED framework entitled OntoED with ontology
embedding. We enrich event ontology with linkages among event types, and
further induce more event-event correlations. Based on the event ontology,
OntoED can leverage and propagate correlation knowledge, particularly from
data-rich to data-poor event types. Furthermore, OntoED can be applied to new
unseen event types, by establishing linkages to existing ones. Experiments
indicate that OntoED is more predominant and robust than previous approaches to
ED, especially in data-scarce scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Luoqiu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1"&gt;Huaixiao Tou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mosha Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Speech Recognition. (arXiv:2105.11084v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.11084</id>
        <link href="http://arxiv.org/abs/2105.11084"/>
        <updated>2021-05-25T01:56:07.323Z</updated>
        <summary type="html"><![CDATA[Despite rapid progress in the recent past, current speech recognition systems
still require labeled training data which limits this technology to a small
fraction of the languages spoken around the globe. This paper describes
wav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition
models without any labeled data. We leverage self-supervised speech
representations to segment unlabeled audio and learn a mapping from these
representations to phonemes via adversarial training. The right representations
are key to the success of our method. Compared to the best previous
unsupervised work, wav2vec-U reduces the phoneme error rate on the TIMIT
benchmark from 26.1 to 11.3. On the larger English Librispeech benchmark,
wav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the
best published systems trained on 960 hours of labeled data from only two years
ago. We also experiment on nine other languages, including low-resource
languages such as Kyrgyz, Swahili and Tatar.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1"&gt;Alexei Baevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wei-Ning Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1"&gt;Alexis Conneau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1"&gt;Michael Auli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Artificial Intelligence Enabled Financial Crime Detection. (arXiv:2105.10866v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10866</id>
        <link href="http://arxiv.org/abs/2105.10866"/>
        <updated>2021-05-25T01:56:07.304Z</updated>
        <summary type="html"><![CDATA[Recently, financial institutes have been dealing with an increase in
financial crimes. In this context, financial services firms started to improve
their vigilance and use new technologies and approaches to identify and predict
financial fraud and crime possibilities. This task is challenging as
institutions need to upgrade their data and analytics capabilities to enable
new technologies such as Artificial Intelligence (AI) to predict and detect
financial crimes. In this paper, we put a step towards AI-enabled financial
crime detection in general and money laundering detection in particular to
address this challenge. We study and analyse the recent works done in financial
crime detection and present a novel model to detect money laundering cases with
minimum human intervention needs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rouhollahi_Z/0/1/0/all/0/1"&gt;Zeinab Rouhollahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Killing Two Birds with One Stone: Stealing Model and Inferring Attribute from BERT-based APIs. (arXiv:2105.10909v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.10909</id>
        <link href="http://arxiv.org/abs/2105.10909"/>
        <updated>2021-05-25T01:56:07.260Z</updated>
        <summary type="html"><![CDATA[The advances in pre-trained models (e.g., BERT, XLNET and etc) have largely
revolutionized the predictive performance of various modern natural language
processing tasks. This allows corporations to provide machine learning as a
service (MLaaS) by encapsulating fine-tuned BERT-based models as commercial
APIs. However, previous works have discovered a series of vulnerabilities in
BERT- based APIs. For example, BERT-based APIs are vulnerable to both model
extraction attack and adversarial example transferrability attack. However, due
to the high capacity of BERT-based APIs, the fine-tuned model is easy to be
overlearned, what kind of information can be leaked from the extracted model
remains unknown and is lacking. To bridge this gap, in this work, we first
present an effective model extraction attack, where the adversary can
practically steal a BERT-based API (the target/victim model) by only querying a
limited number of queries. We further develop an effective attribute inference
attack to expose the sensitive attribute of the training data used by the
BERT-based APIs. Our extensive experiments on benchmark datasets under various
realistic settings demonstrate the potential vulnerabilities of BERT-based
APIs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lingjuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuanli He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DepressionNet: A Novel Summarization Boosted Deep Framework for Depression Detection on Social Media. (arXiv:2105.10878v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10878</id>
        <link href="http://arxiv.org/abs/2105.10878"/>
        <updated>2021-05-25T01:56:07.245Z</updated>
        <summary type="html"><![CDATA[Twitter is currently a popular online social media platform which allows
users to share their user-generated content. This publicly-generated user data
is also crucial to healthcare technologies because the discovered patterns
would hugely benefit them in several ways. One of the applications is in
automatically discovering mental health problems, e.g., depression. Previous
studies to automatically detect a depressed user on online social media have
largely relied upon the user behaviour and their linguistic patterns including
user's social interactions. The downside is that these models are trained on
several irrelevant content which might not be crucial towards detecting a
depressed user. Besides, these content have a negative impact on the overall
efficiency and effectiveness of the model. To overcome the shortcomings in the
existing automatic depression detection methods, we propose a novel
computational framework for automatic depression detection that initially
selects relevant content through a hybrid extractive and abstractive
summarization strategy on the sequence of all user tweets leading to a more
fine-grained and relevant content. The content then goes to our novel deep
learning framework comprising of a unified learning machinery comprising of
Convolutional Neural Network (CNN) coupled with attention-enhanced Gated
Recurrent Units (GRU) models leading to better empirical performance than
existing strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zogan_H/0/1/0/all/0/1"&gt;Hamad Zogan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razzak_I/0/1/0/all/0/1"&gt;Imran Razzak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jameel_S/0/1/0/all/0/1"&gt;Shoaib Jameel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guandong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10880</id>
        <link href="http://arxiv.org/abs/2105.10880"/>
        <updated>2021-05-25T01:56:07.229Z</updated>
        <summary type="html"><![CDATA[Climate change has largely impacted our daily lives. As one of its
consequences, we are experiencing more wildfires. In the year 2020, wildfires
burned a record number of 8,888,297 acres in the US. To awaken people's
attention to climate change, and to visualize the current risk of wildfires, We
developed RtFPS, "Real-Time Fire Prediction System". It provides a real-time
prediction visualization of wildfire risk at specific locations base on a
Machine Learning model. It also provides interactive map features that show the
historical wildfire events with environmental info.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1"&gt;Hermawan Mulyono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiyin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1"&gt;Desmond Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding. (arXiv:2105.10912v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10912</id>
        <link href="http://arxiv.org/abs/2105.10912"/>
        <updated>2021-05-25T01:56:07.215Z</updated>
        <summary type="html"><![CDATA[Scientific document understanding is challenging as the data is highly domain
specific and diverse. However, datasets for tasks with scientific text require
expensive manual annotation and tend to be small and limited to only one or a
few fields. At the same time, scientific documents contain many potential
training signals, such as citations, which can be used to build large labelled
datasets. Given this, we present an in-depth study of cite-worthiness detection
in English, where a sentence is labelled for whether or not it cites an
external source. To accomplish this, we introduce CiteWorth, a large,
contextualized, rigorously cleaned labelled dataset for cite-worthiness
detection built from a massive corpus of extracted plain-text scientific
documents. We show that CiteWorth is high-quality, challenging, and suitable
for studying problems such as domain adaptation. Our best performing
cite-worthiness detection model is a paragraph-level contextualized sentence
labelling model based on Longformer, exhibiting a 5 F1 point improvement over
SciBERT which considers only individual sentences. Finally, we demonstrate that
language model fine-tuning with cite-worthiness as a secondary task leads to
improved performance on downstream scientific document understanding tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1"&gt;Dustin Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Language Model based Ranking in Baidu Search. (arXiv:2105.11108v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.11108</id>
        <link href="http://arxiv.org/abs/2105.11108"/>
        <updated>2021-05-25T01:56:07.201Z</updated>
        <summary type="html"><![CDATA[As the heart of a search engine, the ranking system plays a crucial role in
satisfying users' information demands. More recently, neural rankers fine-tuned
from pre-trained language models (PLMs) establish state-of-the-art ranking
effectiveness. However, it is nontrivial to directly apply these PLM-based
rankers to the large-scale web search system due to the following challenging
issues:(1) the prohibitively expensive computations of massive neural PLMs,
especially for long texts in the web-document, prohibit their deployments in an
online ranking system that demands extremely low latency;(2) the discrepancy
between existing ranking-agnostic pre-training objectives and the ad-hoc
retrieval scenarios that demand comprehensive relevance modeling is another
main barrier for improving the online ranking system;(3) a real-world search
engine typically involves a committee of ranking components, and thus the
compatibility of the individually fine-tuned ranking model is critical for a
cooperative ranking system. In this work, we contribute a series of
successfully applied techniques in tackling these exposed issues when deploying
the state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the
online search engine system. We first articulate a novel practice to
cost-efficiently summarize the web document and contextualize the resultant
summary content with the query using a cheap yet powerful Pyramid-ERNIE
architecture. Then we endow an innovative paradigm to finely exploit the
large-scale noisy and biased post-click behavioral data for relevance-oriented
pre-training. We also propose a human-anchored fine-tuning strategy tailored
for the online ranking system, aiming to stabilize the ranking signals across
various online components. Extensive offline and online experimental results
show that the proposed techniques significantly boost the search engine's
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1"&gt;Lixin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengqiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hengyi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1"&gt;Dehong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Suqi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Daiting Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhicong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"&gt;Dawei Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Product Ontology Extraction from Textual Reviews. (arXiv:2105.10966v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10966</id>
        <link href="http://arxiv.org/abs/2105.10966"/>
        <updated>2021-05-25T01:56:07.153Z</updated>
        <summary type="html"><![CDATA[Ontologies have proven beneficial in different settings that make use of
textual reviews. However, manually constructing ontologies is a laborious and
time-consuming process in need of automation. We propose a novel methodology
for automatically extracting ontologies, in the form of meronomies, from
product reviews, using a very limited amount of hand-annotated training data.
We show that the ontologies generated by our method outperform hand-crafted
ontologies (WordNet) and ontologies extracted by existing methods (Text2Onto
and COMET) in several, diverse settings. Specifically, our generated ontologies
outperform the others when evaluated by human annotators as well as on an
existing Q&A dataset from Amazon. Moreover, our method is better able to
generalise, in capturing knowledge about unseen products. Finally, we consider
a real-world setting, showing that our method is better able to determine
recommended products based on their reviews, in alternative to using Amazon's
standard score aggregations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oksanen_J/0/1/0/all/0/1"&gt;Joel Oksanen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1"&gt;Oana Cocarascu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1"&gt;Francesca Toni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RST Parsing from Scratch. (arXiv:2105.10861v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10861</id>
        <link href="http://arxiv.org/abs/2105.10861"/>
        <updated>2021-05-25T01:56:07.132Z</updated>
        <summary type="html"><![CDATA[We introduce a novel top-down end-to-end formulation of document-level
discourse parsing in the Rhetorical Structure Theory (RST) framework. In this
formulation, we consider discourse parsing as a sequence of splitting decisions
at token boundaries and use a seq2seq network to model the splitting decisions.
Our framework facilitates discourse parsing from scratch without requiring
discourse segmentation as a prerequisite; rather, it yields segmentation as
part of the parsing process. Our unified parsing model adopts a beam search to
decode the best tree structure by searching through a space of high-scoring
trees. With extensive experiments on the standard English RST discourse
treebank, we demonstrate that our parser outperforms existing methods by a good
margin in both end-to-end parsing and parsing with gold segmentation. More
importantly, it does so without using any handcrafted features, making it
faster and easily adaptable to new languages and domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thanh-Tung Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1"&gt;Xuan-Phi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Watermarking using Diffusion of Logo into Autoencoder Feature Maps. (arXiv:2105.11095v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2105.11095</id>
        <link href="http://arxiv.org/abs/2105.11095"/>
        <updated>2021-05-25T01:56:06.430Z</updated>
        <summary type="html"><![CDATA[Digital contents have grown dramatically in recent years, leading to
increased attention to copyright. Image watermarking has been considered one of
the most popular methods for copyright protection. With the recent advancements
in applying deep neural networks in image processing, these networks have also
been used in image watermarking. Robustness and imperceptibility are two
challenging features of watermarking methods that the trade-off between them
should be satisfied. In this paper, we propose to use an end-to-end network for
watermarking. We use a convolutional neural network (CNN) to control the
embedding strength based on the image content. Dynamic embedding helps the
network to have the lowest effect on the visual quality of the watermarked
image. Different image processing attacks are simulated as a network layer to
improve the robustness of the model. Our method is a blind watermarking
approach that replicates the watermark string to create a matrix of the same
size as the input image. Instead of diffusing the watermark data into the input
image, we inject the data into the feature space and force the network to do
this in regions that increase the robustness against various attacks.
Experimental results show the superiority of the proposed method in terms of
imperceptibility and robustness compared to the state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jamali_M/0/1/0/all/0/1"&gt;Maedeh Jamali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1"&gt;Nader Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khadivi_P/0/1/0/all/0/1"&gt;Pejman Khadivi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirani_S/0/1/0/all/0/1"&gt;Shahram Shirani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1"&gt;Shadrokh Samavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers. (arXiv:2105.08059v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08059</id>
        <link href="http://arxiv.org/abs/2105.08059"/>
        <updated>2021-05-24T05:08:43.206Z</updated>
        <summary type="html"><![CDATA[Supervised deep learning has swiftly become a workhorse for accelerated MRI
in recent years, offering state-of-the-art performance in image reconstruction
from undersampled acquisitions. Training deep supervised models requires large
datasets of undersampled and fully-sampled acquisitions typically from a
matching set of subjects. Given scarce access to large medical datasets, this
limitation has sparked interest in unsupervised methods that reduce reliance on
fully-sampled ground-truth data. A common framework is based on the deep image
prior, where network-driven regularization is enforced directly during
inference on undersampled acquisitions. Yet, canonical convolutional
architectures are suboptimal in capturing long-range relationships, and
randomly initialized networks may hamper convergence. To address these
limitations, here we introduce a novel unsupervised MRI reconstruction method
based on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a
deep adversarial network with cross-attention transformer blocks to map noise
and latent variables onto MR images. This unconditional network learns a
high-quality MRI prior in a self-supervised encoding task. A zero-shot
reconstruction is performed on undersampled test data, where inference is
performed by optimizing network parameters, latent and noise variables to
ensure maximal consistency to multi-coil MRI data. Comprehensive experiments on
brain MRI datasets clearly demonstrate the superior performance of SLATER
against several state-of-the-art unsupervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1"&gt;Yilmaz Korkmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1"&gt;Salman UH Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1"&gt;Mahmut Yurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1"&gt;Muzaffer &amp;#xd6;zbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1"&gt;Tolga &amp;#xc7;ukur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-guided Chained Context Aggregation for Semantic Segmentation. (arXiv:2002.12041v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.12041</id>
        <link href="http://arxiv.org/abs/2002.12041"/>
        <updated>2021-05-24T05:08:43.184Z</updated>
        <summary type="html"><![CDATA[The way features propagate in Fully Convolutional Networks is of momentous
importance to capture multi-scale contexts for obtaining precise segmentation
masks. This paper proposes a novel series-parallel hybrid paradigm called the
Chained Context Aggregation Module (CAM) to diversify feature propagation. CAM
gains features of various spatial scales through chain-connected ladder-style
information flows and fuses them in a two-stage process, namely pre-fusion and
re-fusion. The serial flow continuously increases receptive fields of output
neurons and those in parallel encode different region-based contexts. Each
information flow is a shallow encoder-decoder with appropriate down-sampling
scales to sufficiently capture contextual information. We further adopt an
attention model in CAM to guide feature re-fusion. Based on these developments,
we construct the Chained Context Aggregation Network (CANet), which employs an
asymmetric decoder to recover precise spatial details of prediction maps. We
conduct extensive experiments on six challenging datasets, including Pascal VOC
2012, Pascal Context, Cityscapes, CamVid, SUN-RGBD and GATECH. Results evidence
that CANet achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1"&gt;Quan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fagui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations. (arXiv:2005.01456v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01456</id>
        <link href="http://arxiv.org/abs/2005.01456"/>
        <updated>2021-05-24T05:08:43.164Z</updated>
        <summary type="html"><![CDATA[High quality perception is essential for autonomous driving (AD) systems. To
reach the accuracy and robustness that are required by such systems, several
types of sensors must be combined. Currently, mostly cameras and laser scanners
(lidar) are deployed to build a representation of the world around the vehicle.
While radar sensors have been used for a long time in the automotive industry,
they are still under-used for AD despite their appealing characteristics
(notably, their ability to measure the relative speed of obstacles and to
operate even in adverse weather conditions). To a large extent, this situation
is due to the relative lack of automotive datasets with real radar signals that
are both raw and annotated. In this work, we introduce CARRADA, a dataset of
synchronized camera and radar recordings with range-angle-Doppler annotations.
We also present a semi-automatic annotation approach, which was used to
annotate the dataset, and a radar semantic segmentation baseline, which we
evaluate on several metrics. Both our code and dataset are available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouaknine_A/0/1/0/all/0/1"&gt;A. Ouaknine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1"&gt;A. Newson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rebut_J/0/1/0/all/0/1"&gt;J. Rebut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tupin_F/0/1/0/all/0/1"&gt;F. Tupin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1"&gt;P. P&amp;#xe9;rez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identity-Free Facial Expression Recognition using conditional Generative Adversarial Network. (arXiv:1903.08051v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.08051</id>
        <link href="http://arxiv.org/abs/1903.08051"/>
        <updated>2021-05-24T05:08:43.156Z</updated>
        <summary type="html"><![CDATA[A novel Identity-Free conditional Generative Adversarial Network (IF-GAN) was
proposed for Facial Expression Recognition (FER) to explicitly reduce high
inter-subject variations caused by identity-related facial attributes, e.g.,
age, race, and gender. As part of an end-to-end system, a cGAN was designed to
transform a given input facial expression image to an "average" identity face
with the same expression as the input. Then, identity-free FER is possible
since the generated images have the same synthetic "average" identity and
differ only in their displayed expressions. Experiments on four facial
expression datasets, one with spontaneous expressions, show that IF-GAN
outperforms the baseline CNN and achieves state-of-the-art performance for FER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jie Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zibo Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Ahmed Shehab Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OReilly_J/0/1/0/all/0/1"&gt;James O&amp;#x27;Reilly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shizhong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yan Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The SpaceNet Multi-Temporal Urban Development Challenge. (arXiv:2102.11958v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11958</id>
        <link href="http://arxiv.org/abs/2102.11958"/>
        <updated>2021-05-24T05:08:43.135Z</updated>
        <summary type="html"><![CDATA[Building footprints provide a useful proxy for a great many humanitarian
applications. For example, building footprints are useful for high fidelity
population estimates, and quantifying population statistics is fundamental to
~1/4 of the United Nations Sustainable Development Goals Indicators. In this
paper we (the SpaceNet Partners) discuss efforts to develop techniques for
precise building footprint localization, tracking, and change detection via the
SpaceNet Multi-Temporal Urban Development Challenge (also known as SpaceNet 7).
In this NeurIPS 2020 competition, participants were asked identify and track
buildings in satellite imagery time series collected over rapidly urbanizing
areas. The competition centered around a brand new open source dataset of
Planet Labs satellite imagery mosaics at 4m resolution, which includes 24
images (one per month) covering ~100 unique geographies. Tracking individual
buildings at this resolution is quite challenging, yet the winning participants
demonstrated impressive performance with the newly developed SpaceNet Change
and Object Tracking (SCOT) metric. This paper details the top-5 winning
approaches, as well as analysis of results that yielded a handful of
interesting anecdotes such as decreasing performance with latitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Etten_A/0/1/0/all/0/1"&gt;Adam Van Etten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogan_D/0/1/0/all/0/1"&gt;Daniel Hogan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words. (arXiv:2101.00265v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00265</id>
        <link href="http://arxiv.org/abs/2101.00265"/>
        <updated>2021-05-24T05:08:43.123Z</updated>
        <summary type="html"><![CDATA[Text-to-image retrieval is an essential task in cross-modal information
retrieval, i.e., retrieving relevant images from a large and unlabelled dataset
given textual queries. In this paper, we propose VisualSparta, a novel
(Visual-text Sparse Transformer Matching) model that shows significant
improvement in terms of both accuracy and efficiency. VisualSparta is capable
of outperforming previous state-of-the-art scalable methods in MSCOCO and
Flickr30K. We also show that it achieves substantial retrieving speed
advantages, i.e., for a 1 million image index, VisualSparta using CPU gets
~391X speedup compared to CPU vector search and ~5.4X speedup compared to
vector search with GPU acceleration. Experiments show that this speed advantage
even gets bigger for larger datasets because VisualSparta can be efficiently
implemented as an inverted index. To the best of our knowledge, VisualSparta is
the first transformer-based text-to-image retrieval model that can achieve
real-time searching for large-scale datasets, with significant accuracy
improvement compared to previous state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyusong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer. (arXiv:2011.07112v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07112</id>
        <link href="http://arxiv.org/abs/2011.07112"/>
        <updated>2021-05-24T05:08:43.117Z</updated>
        <summary type="html"><![CDATA[Domain randomisation is a very popular method for visual sim-to-real transfer
in robotics, due to its simplicity and ability to achieve transfer without any
real-world images at all. Nonetheless, a number of design choices must be made
to achieve optimal transfer. In this paper, we perform a comprehensive
benchmarking study on these different choices, with two key experiments
evaluated on a real-world object pose estimation task. First, we study the
rendering quality, and find that a small number of high-quality images is
superior to a large number of low-quality images. Second, we study the type of
randomisation, and find that both distractors and textures are important for
generalisation to novel environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alghonaim_R/0/1/0/all/0/1"&gt;Raghad Alghonaim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1"&gt;Edward Johns&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Domain Loss Function for Deep Exposure Correction of Dark Images. (arXiv:2104.10856v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10856</id>
        <link href="http://arxiv.org/abs/2104.10856"/>
        <updated>2021-05-24T05:08:42.990Z</updated>
        <summary type="html"><![CDATA[We address the problem of exposure correction of dark, blurry and noisy
images captured in low-light conditions in the wild. Classical image-denoising
filters work well in the frequency space but are constrained by several factors
such as the correct choice of thresholds, frequency estimates etc. On the other
hand, traditional deep networks are trained end-to-end in the RGB space by
formulating this task as an image-translation problem. However, that is done
without any explicit constraints on the inherent noise of the dark images and
thus produce noisy and blurry outputs. To this end we propose a DCT/FFT based
multi-scale loss function, which when combined with traditional losses, trains
a network to translate the important features for visually pleasing output. Our
loss function is end-to-end differentiable, scale-agnostic, and generic; i.e.,
it can be applied to both RAW and JPEG images in most existing frameworks
without additional overhead. Using this loss function, we report significant
improvements over the state-of-the-art using quantitative metrics and
subjective tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yadav_O/0/1/0/all/0/1"&gt;Ojasvi Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghosal_K/0/1/0/all/0/1"&gt;Koustav Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lutz_S/0/1/0/all/0/1"&gt;Sebastian Lutz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smolic_A/0/1/0/all/0/1"&gt;Aljosa Smolic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Deeper through the Gleason Scoring Scale: An Automatic end-to-end System for Histology Prostate Grading and Cribriform Pattern Detection. (arXiv:2105.10490v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10490</id>
        <link href="http://arxiv.org/abs/2105.10490"/>
        <updated>2021-05-24T05:08:42.984Z</updated>
        <summary type="html"><![CDATA[The Gleason scoring system is the primary diagnostic and prognostic tool for
prostate cancer. In recent years, with the development of digitisation devices,
the use of computer vision techniques for the analysis of biopsies has
increased. However, to the best of the authors' knowledge, the development of
algorithms to automatically detect individual cribriform patterns belonging to
Gleason grade 4 has not yet been studied in the literature. The objective of
the work presented in this paper is to develop a deep-learning-based system
able to support pathologists in the daily analysis of prostate biopsies. The
methodological core of this work is a patch-wise predictive model based on
convolutional neural networks able to determine the presence of cancerous
patterns. In particular, we train from scratch a simple self-design
architecture. The cribriform pattern is detected by retraining the set of
filters of the last convolutional layer in the network. From the reconstructed
prediction map, we compute the percentage of each Gleason grade in the tissue
to feed a multi-layer perceptron which provides a biopsy-level score.mIn our
SICAPv2 database, composed of 182 annotated whole slide images, we obtained a
Cohen's quadratic kappa of 0.77 in the test set for the patch-level Gleason
grading with the proposed architecture trained from scratch. Our results
outperform previous ones reported in the literature. Furthermore, this model
reaches the level of fine-tuned state-of-the-art architectures in a
patient-based four groups cross validation. In the cribriform pattern detection
task, we obtained an area under ROC curve of 0.82. Regarding the biopsy Gleason
scoring, we achieved a quadratic Cohen's Kappa of 0.81 in the test subset.
Shallow CNN architectures trained from scratch outperform current
state-of-the-art methods for Gleason grades classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1"&gt;Julio Silva-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sales_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a A. Sales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Molina_R/0/1/0/all/0/1"&gt;Rafael Molina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distinguishing artefacts: evaluating the saturation point of convolutional neural networks. (arXiv:2105.10448v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10448</id>
        <link href="http://arxiv.org/abs/2105.10448"/>
        <updated>2021-05-24T05:08:42.966Z</updated>
        <summary type="html"><![CDATA[Prior work has shown Convolutional Neural Networks (CNNs) trained on
surrogate Computer Aided Design (CAD) models are able to detect and classify
real-world artefacts from photographs. The applications of which support
twinning of digital and physical assets in design, including rapid extraction
of part geometry from model repositories, information search \& retrieval and
identifying components in the field for maintenance, repair, and recording. The
performance of CNNs in classification tasks have been shown dependent on
training data set size and number of classes. Where prior works have used
relatively small surrogate model data sets ($<100$ models), the question
remains as to the ability of a CNN to differentiate between models in
increasingly large model repositories. This paper presents a method for
generating synthetic image data sets from online CAD model repositories, and
further investigates the capacity of an off-the-shelf CNN architecture trained
on synthetic data to classify models as class size increases. 1,000 CAD models
were curated and processed to generate large scale surrogate data sets,
featuring model coverage at steps of 10$^{\circ}$, 30$^{\circ}$, 60$^{\circ}$,
and 120$^{\circ}$ degrees. The findings demonstrate the capability of computer
vision algorithms to classify artefacts in model repositories of up to 200,
beyond this point the CNN's performance is observed to deteriorate
significantly, limiting its present ability for automated twinning of physical
to digital artefacts. Although, a match is more often found in the top-5
results showing potential for information search and retrieval on large
repositories of surrogate models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Real_R/0/1/0/all/0/1"&gt;Ric Real&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopsill_J/0/1/0/all/0/1"&gt;James Gopsill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1"&gt;David Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snider_C/0/1/0/all/0/1"&gt;Chris Snider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hicks_B/0/1/0/all/0/1"&gt;Ben Hicks&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-learning for weakly supervised Gleason grading of local patterns. (arXiv:2105.10420v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10420</id>
        <link href="http://arxiv.org/abs/2105.10420"/>
        <updated>2021-05-24T05:08:42.960Z</updated>
        <summary type="html"><![CDATA[Prostate cancer is one of the main diseases affecting men worldwide. The gold
standard for diagnosis and prognosis is the Gleason grading system. In this
process, pathologists manually analyze prostate histology slides under
microscope, in a high time-consuming and subjective task. In the last years,
computer-aided-diagnosis (CAD) systems have emerged as a promising tool that
could support pathologists in the daily clinical practice. Nevertheless, these
systems are usually trained using tedious and prone-to-error pixel-level
annotations of Gleason grades in the tissue. To alleviate the need of manual
pixel-wise labeling, just a handful of works have been presented in the
literature. Motivated by this, we propose a novel weakly-supervised
deep-learning model, based on self-learning CNNs, that leverages only the
global Gleason score of gigapixel whole slide images during training to
accurately perform both, grading of patch-level patterns and biopsy-level
scoring. To evaluate the performance of the proposed method, we perform
extensive experiments on three different external datasets for the patch-level
Gleason grading, and on two different test sets for global Grade Group
prediction. We empirically demonstrate that our approach outperforms its
supervised counterpart on patch-level Gleason grading by a large margin, as
well as state-of-the-art methods on global biopsy-level scoring. Particularly,
the proposed model brings an average improvement on the Cohen's quadratic kappa
(k) score of nearly 18% compared to full-supervision for the patch-level
Gleason grading task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1"&gt;Julio Silva-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1"&gt;Jose Dolz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICON: Learning Regular Maps Through Inverse Consistency. (arXiv:2105.04459v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04459</id>
        <link href="http://arxiv.org/abs/2105.04459"/>
        <updated>2021-05-24T05:08:42.954Z</updated>
        <summary type="html"><![CDATA[Learning maps between data samples is fundamental. Applications range from
representation learning, image translation and generative modeling, to the
estimation of spatial deformations. Such maps relate feature vectors, or map
between feature spaces. Well-behaved maps should be regular, which can be
imposed explicitly or may emanate from the data itself. We explore what induces
regularity for spatial transformations, e.g., when computing image
registrations. Classical optimization-based models compute maps between pairs
of samples and rely on an appropriate regularizer for well-posedness. Recent
deep learning approaches have attempted to avoid using such regularizers
altogether by relying on the sample population instead. We explore if it is
possible to obtain spatial regularity using an inverse consistency loss only
and elucidate what explains map regularity in such a context. We find that deep
networks combined with an inverse consistency loss and randomized off-grid
interpolation yield well behaved, approximately diffeomorphic, spatial
transformations. Despite the simplicity of this approach, our experiments
present compelling evidence, on both synthetic and real data, that regular maps
can be obtained without carefully tuned explicit regularizers, while achieving
competitive registration performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greer_H/0/1/0/all/0/1"&gt;Hastings Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwitt_R/0/1/0/all/0/1"&gt;Roland Kwitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vialard_F/0/1/0/all/0/1"&gt;Francois-Xavier Vialard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1"&gt;Marc Niethammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Framework for Efficient Deep Learning Using Metasurfaces Optics. (arXiv:2011.11728v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11728</id>
        <link href="http://arxiv.org/abs/2011.11728"/>
        <updated>2021-05-24T05:08:42.948Z</updated>
        <summary type="html"><![CDATA[Deep learning using Convolutional Neural Networks (CNNs) has been shown to
significantly out-performed many conventional vision algorithms. Despite
efforts to increase the CNN efficiency both algorithmically and with
specialized hardware, deep learning remains difficult to deploy in
resource-constrained environments. In this paper, we propose an end-to-end
framework to explore optically compute the CNNs in free-space, much like a
computational camera. Compared to existing free-space optics-based approaches
which are limited to processing single-channel (i.e., grayscale) inputs, we
propose the first general approach, based on nanoscale meta-surface optics,
that can process RGB data directly from the natural scenes. Our system achieves
up to an order of magnitude energy saving, simplifies the sensor design, all
the while sacrificing little network accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burgos_C/0/1/0/all/0/1"&gt;Carlos Mauricio Villegas Burgos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vamivakas_N/0/1/0/all/0/1"&gt;Nick Vamivakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuhao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Uncertainty from Different Sources in Deep Neural Networks for Image Classification. (arXiv:2011.08712v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08712</id>
        <link href="http://arxiv.org/abs/2011.08712"/>
        <updated>2021-05-24T05:08:42.941Z</updated>
        <summary type="html"><![CDATA[Quantifying uncertainty in a model's predictions is important as it enables
the safety of an AI system to be increased by acting on the model's output in
an informed manner. This is crucial for applications where the cost of an error
is high, such as in autonomous vehicle control, medical image analysis,
financial estimations or legal fields. Deep Neural Networks are powerful
predictors that have recently achieved state-of-the-art performance on a wide
spectrum of tasks. Quantifying predictive uncertainty in DNNs is a challenging
and yet on-going problem. In this paper we propose a complete framework to
capture and quantify all of these three types of uncertainties in DNNs for
image classification. This framework includes an ensemble of CNNs for model
uncertainty, a supervised reconstruction auto-encoder to capture distributional
uncertainty and using the output of activation functions in the last layer of
the network, to capture data uncertainty. Finally we demonstrate the efficiency
of our method on popular image datasets for classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khoshsirat_A/0/1/0/all/0/1"&gt;Aria Khoshsirat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Answer Validation for Knowledge-Based VQA. (arXiv:2103.12248v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12248</id>
        <link href="http://arxiv.org/abs/2103.12248"/>
        <updated>2021-05-24T05:08:42.923Z</updated>
        <summary type="html"><![CDATA[The problem of knowledge-based visual question answering involves answering
questions that require external knowledge in addition to the content of the
image. Such knowledge typically comes in a variety of forms, including visual,
textual, and commonsense knowledge. The use of more knowledge sources, however,
also increases the chance of retrieving more irrelevant or noisy facts, making
it difficult to comprehend the facts and find the answer. To address this
challenge, we propose Multi-modal Answer Validation using External knowledge
(MAVEx), where the idea is to validate a set of promising answer candidates
based on answer-specific knowledge retrieval. This is in contrast to existing
approaches that search for the answer in a vast collection of often irrelevant
facts. Our approach aims to learn which knowledge source should be trusted for
each answer candidate and how to validate the candidate using that source. We
consider a multi-modal setting, relying on both textual and visual knowledge
resources, including images searched using Google, sentences from Wikipedia
articles, and concepts from ConceptNet. Our experiments with OK-VQA, a
challenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jialin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiasen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Halluci-Net: Scene Completion by Exploiting Object Co-occurrence Relationships. (arXiv:2004.08614v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08614</id>
        <link href="http://arxiv.org/abs/2004.08614"/>
        <updated>2021-05-24T05:08:42.917Z</updated>
        <summary type="html"><![CDATA[Recently, there has been substantial progress in image synthesis from
semantic labelmaps. However, methods used for this task assume the availability
of complete and unambiguous labelmaps, with instance boundaries of objects, and
class labels for each pixel. This reliance on heavily annotated inputs
restricts the application of image synthesis techniques to real-world
applications, especially under uncertainty due to weather, occlusion, or noise.
On the other hand, algorithms that can synthesize images from sparse labelmaps
or sketches are highly desirable as tools that can guide content creators and
artists to quickly generate scenes by simply specifying locations of a few
objects. In this paper, we address the problem of complex scene completion from
sparse labelmaps. Under this setting, very few details about the scene (30\% of
object instances) are available as input for image synthesis. We propose a
two-stage deep network based method, called `Halluci-Net', that learns
co-occurence relationships between objects in scenes, and then exploits these
relationships to produce a dense and complete labelmap. The generated dense
labelmap can then be used as input by state-of-the-art image synthesis
techniques like pix2pixHD to obtain the final image. The proposed method is
evaluated on the Cityscapes dataset and it outperforms two baselines methods on
performance metrics like Fr\'echet Inception Distance (FID), semantic
segmentation accuracy, and similarity in object co-occurrences. We also show
qualitative results on a subset of ADE20K dataset that contains bedroom images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1"&gt;Kuldeep Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1"&gt;Tejas Gokhale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rajhans Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1"&gt;Pavan Turaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aswin Sankaranarayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impressions2Font: Generating Fonts by Specifying Impressions. (arXiv:2103.10036v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10036</id>
        <link href="http://arxiv.org/abs/2103.10036"/>
        <updated>2021-05-24T05:08:42.910Z</updated>
        <summary type="html"><![CDATA[Various fonts give us various impressions, which are often represented by
words. This paper proposes Impressions2Font (Imp2Font) that generates font
images with specific impressions. Imp2Font is an extended version of
conditional generative adversarial networks (GANs). More precisely, Imp2Font
accepts an arbitrary number of impression words as the condition to generate
the font images. These impression words are converted into a soft-constraint
vector by an impression embedding module built on a word embedding technique.
Qualitative and quantitative evaluations prove that Imp2Font generates font
images with higher quality than comparative methods by providing multiple
impression words or even unlearned words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matsuda_S/0/1/0/all/0/1"&gt;Seiya Matsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1"&gt;Akisato Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Psychophysical Oriented Saliency Map Prediction Model. (arXiv:2011.04076v8 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04076</id>
        <link href="http://arxiv.org/abs/2011.04076"/>
        <updated>2021-05-24T05:08:42.904Z</updated>
        <summary type="html"><![CDATA[Visual attention is one of the most significant characteristics for selecting
and understanding the outside redundancy world. The nature of complex scenes
includes enormous redundancy. The human vision system can not process all
information simultaneously because of visual information bottleneck. The human
visual system mainly focuses on dominant parts of the scenes to reduce the
input visual redundancy information. It is commonly known as visual attention
prediction or visual saliency map. This paper proposes a new psychophysical
saliency prediction architecture, WECSF, inspired by human low-level visual
cortex function. The model consists of opponent color channels, wavelet
transform, wavelet energy map, and contrast sensitivity function for extracting
low-level image features and maximum approximation to the human visual system.
The proposed model is evaluated several datasets, including MIT1003, MIT300,
TORONTO, SID4VAM and UCF Sports dataset to explain its efficiency. We also
quantitatively and qualitatively compared the performance of saliency
prediction with other state-of-the-art models. Our model achieved very stable
and good performance. Second, we also confirmed that Fourier and
spectral-inspired saliency prediction models achieved outperformance compared
to other start-of-the-art non-neural networks and even deep neural network
models on psychophysical synthesis images. Finally, the proposed model also can
be applied to spatial-temporal saliency prediction and got better performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressing Deep CNNs using Basis Representation and Spectral Fine-tuning. (arXiv:2105.10436v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10436</id>
        <link href="http://arxiv.org/abs/2105.10436"/>
        <updated>2021-05-24T05:08:42.897Z</updated>
        <summary type="html"><![CDATA[We propose an efficient and straightforward method for compressing deep
convolutional neural networks (CNNs) that uses basis filters to represent the
convolutional layers, and optimizes the performance of the compressed network
directly in the basis space. Specifically, any spatial convolution layer of the
CNN can be replaced by two successive convolution layers: the first is a set of
three-dimensional orthonormal basis filters, followed by a layer of
one-dimensional filters that represents the original spatial filters in the
basis space. We jointly fine-tune both the basis and the filter representation
to directly mitigate any performance loss due to the truncation. Generality of
the proposed approach is demonstrated by applying it to several well known deep
CNN architectures and data sets for image classification and object detection.
We also present the execution time and power usage at different compression
levels on the Xavier Jetson AGX processor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tayyab_M/0/1/0/all/0/1"&gt;Muhammad Tayyab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Ahmad Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahalanobis_A/0/1/0/all/0/1"&gt;Abhijit Mahalanobis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Realization of Augmented Intelligence in Dermatology: Advances and Future Directions. (arXiv:2105.10477v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10477</id>
        <link href="http://arxiv.org/abs/2105.10477"/>
        <updated>2021-05-24T05:08:42.891Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) algorithms using deep learning have advanced the
classification of skin disease images; however these algorithms have been
mostly applied "in silico" and not validated clinically. Most dermatology AI
algorithms perform binary classification tasks (e.g. malignancy versus benign
lesions), but this task is not representative of dermatologists' diagnostic
range. The American Academy of Dermatology Task Force on Augmented Intelligence
published a position statement emphasizing the importance of clinical
validation to create human-computer synergy, termed augmented intelligence
(AuI). Liu et al's recent paper, "A deep learning system for differential
diagnosis of skin diseases" represents a significant advancement of AI in
dermatology, bringing it closer to clinical impact. However, significant issues
must be addressed before this algorithm can be integrated into clinical
workflow. These issues include accurate and equitable model development,
defining and assessing appropriate clinical outcomes, and real-world
integration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daneshjou_R/0/1/0/all/0/1"&gt;Roxana Daneshjou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovarik_C/0/1/0/all/0/1"&gt;Carrie Kovarik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1"&gt;Justin M Ko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Visual Learning by Variable Playback Speeds Prediction of a Video. (arXiv:2003.02692v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.02692</id>
        <link href="http://arxiv.org/abs/2003.02692"/>
        <updated>2021-05-24T05:08:42.873Z</updated>
        <summary type="html"><![CDATA[We propose a self-supervised visual learning method by predicting the
variable playback speeds of a video. Without semantic labels, we learn the
spatio-temporal visual representation of the video by leveraging the variations
in the visual appearance according to different playback speeds under the
assumption of temporal coherence. To learn the spatio-temporal visual
variations in the entire video, we have not only predicted a single playback
speed but also generated clips of various playback speeds and directions with
randomized starting points. Hence the visual representation can be successfully
learned from the meta information (playback speeds and directions) of the
video. We also propose a new layer dependable temporal group normalization
method that can be applied to 3D convolutional networks to improve the
representation learning performance where we divide the temporal features into
several groups and normalize each one using the different corresponding
parameters. We validate the effectiveness of our method by fine-tuning it to
the action recognition and video retrieval tasks on UCF-101 and HMDB-51.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyeon Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hyung Jin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1"&gt;Wonjun Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalisable and distinctive 3D local deep descriptors for point cloud registration. (arXiv:2105.10382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10382</id>
        <link href="http://arxiv.org/abs/2105.10382"/>
        <updated>2021-05-24T05:08:42.867Z</updated>
        <summary type="html"><![CDATA[An effective 3D descriptor should be invariant to different geometric
transformations, such as scale and rotation, repeatable in the case of
occlusions and clutter, and generalisable in different contexts when data is
captured with different sensors. We present a simple but yet effective method
to learn generalisable and distinctive 3D local descriptors that can be used to
register point clouds captured in different contexts with different sensors.
Point cloud patches are extracted, canonicalised with respect to their local
reference frame, and encoded into scale and rotation-invariant compact
descriptors by a point permutation-invariant deep neural network. Our
descriptors can effectively generalise across sensor modalities from locally
and randomly sampled points. We evaluate and compare our descriptors with
alternative handcrafted and deep learning-based descriptors on several indoor
and outdoor datasets reconstructed using both RGBD sensors and laser scanners.
Our descriptors outperform most recent descriptors by a large margin in terms
of generalisation, and become the state of the art also in benchmarks where
training and testing are performed in the same scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1"&gt;Fabio Poiesi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boscaini_D/0/1/0/all/0/1"&gt;Davide Boscaini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Consistency Regularized Mean Teacher for Semi-supervised 3D Left Atrium Segmentation. (arXiv:2105.10369v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10369</id>
        <link href="http://arxiv.org/abs/2105.10369"/>
        <updated>2021-05-24T05:08:42.861Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved promising segmentation performance on 3D left
atrium MR images. However, annotations for segmentation tasks are expensive,
costly and difficult to obtain. In this paper, we introduce a novel
hierarchical consistency regularized mean teacher framework for 3D left atrium
segmentation. In each iteration, the student model is optimized by multi-scale
deep supervision and hierarchical consistency regularization, concurrently.
Extensive experiments have shown that our method achieves competitive
performance as compared with full annotation, outperforming other
stateof-the-art semi-supervised segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shumeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Ziyuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaixin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1"&gt;Zeng Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Weight Initialization with Sylvester Solvers. (arXiv:2105.10335v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2105.10335</id>
        <link href="http://arxiv.org/abs/2105.10335"/>
        <updated>2021-05-24T05:08:42.855Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a data-driven scheme to initialize the parameters of
a deep neural network. This is in contrast to traditional approaches which
randomly initialize parameters by sampling from transformed standard
distributions. Such methods do not use the training data to produce a more
informed initialization. Our method uses a sequential layer-wise approach where
each layer is initialized using its input activations. The initialization is
cast as an optimization problem where we minimize a combination of encoding and
decoding losses of the input activations, which is further constrained by a
user-defined latent code. The optimization problem is then restructured into
the well-known Sylvester equation, which has fast and efficient gradient-free
solutions. Our data-driven method achieves a boost in performance compared to
random initialization methods, both before start of training and after training
is over. We show that our proposed method is especially effective in few-shot
and fine-tuning settings. We conclude this paper with analyses on time
complexity and the effect of different latent codes on the recognition
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Debasmit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1"&gt;Yash Bhalgat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1"&gt;Fatih Porikli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. (arXiv:2010.14925v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14925</id>
        <link href="http://arxiv.org/abs/2010.14925"/>
        <updated>2021-05-24T05:08:42.839Z</updated>
        <summary type="html"><![CDATA[We present MedMNIST, a collection of 10 pre-processed medical open datasets.
MedMNIST is standardized to perform classification tasks on lightweight 28x28
images, which requires no background knowledge. Covering the primary data
modalities in medical image analysis, it is diverse on data scale (from 100 to
100,000) and tasks (binary/multi-class, ordinal regression and multi-label).
MedMNIST could be used for educational purpose, rapid prototyping, multi-modal
machine learning or AutoML in medical image analysis. Moreover, MedMNIST
Classification Decathlon is designed to benchmark AutoML algorithms on all 10
datasets; We have compared several baseline methods, including open-source or
commercial AutoML tools. The datasets, evaluation code and baseline methods for
MedMNIST are publicly available at https://medmnist.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiancheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1"&gt;Rui Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Fairness of Generative Adversarial Networks (GANs). (arXiv:2103.00950v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00950</id>
        <link href="http://arxiv.org/abs/2103.00950"/>
        <updated>2021-05-24T05:08:42.833Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are one of the greatest advances in AI
in recent years. With their ability to directly learn the probability
distribution of data, and then sample synthetic realistic data. Many
applications have emerged, using GANs to solve classical problems in machine
learning, such as data augmentation, class unbalance problems, and fair
representation learning. In this paper, we analyze and highlight fairness
concerns of GANs model. In this regard, we show empirically that GANs models
may inherently prefer certain groups during the training process and therefore
they're not able to homogeneously generate data from different groups during
the testing phase. Furthermore, we propose solutions to solve this issue by
conditioning the GAN model towards samples' group or using ensemble method
(boosting) to allow the GAN model to leverage distributed structure of data
during the training phase and generate groups at equal rate during the testing
phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1"&gt;Patrik Joslin Kenfack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arapov_D/0/1/0/all/0/1"&gt;Daniil Dmitrievich Arapov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1"&gt;Rasheed Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1"&gt;S.M. Ahsan Kazmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Adil Mehmood Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeLF: Practical Novel View Synthesis with Neural Light Field. (arXiv:2105.07112v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07112</id>
        <link href="http://arxiv.org/abs/2105.07112"/>
        <updated>2021-05-24T05:08:42.820Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an efficient and robust deep learning solution for
novel view synthesis of complex scenes. In our approach, a 3D scene is
represented as a light field, i.e., a set of rays, each of which has a
corresponding color when reaching the image plane. For efficient novel view
rendering, we adopt a 4D parameterization of the light field, where each ray is
characterized by a 4D parameter. We then formulate the light field as a 4D
function that maps 4D coordinates to corresponding color values. We train a
deep fully connected network to optimize this implicit function and memorize
the 3D scene. Then, the scene-specific model is used to synthesize novel views.
Different from previous light field approaches which require dense view
sampling to reliably render novel views, our method can render novel views by
sampling rays and querying the color for each ray from the network directly,
thus enabling high-quality light field rendering with a sparser set of training
images. Our method achieves state-of-the-art novel view synthesis results while
maintaining an interactive frame rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Celong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Junsong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of voxel-based 3D object detection methods efficiency for real-time embedded systems. (arXiv:2105.10316v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10316</id>
        <link href="http://arxiv.org/abs/2105.10316"/>
        <updated>2021-05-24T05:08:42.764Z</updated>
        <summary type="html"><![CDATA[Real-time detection of objects in the 3D scene is one of the tasks an
autonomous agent needs to perform for understanding its surroundings. While
recent Deep Learning-based solutions achieve satisfactory performance, their
high computational cost renders their application in real-life settings in
which computations need to be performed on embedded platforms intractable. In
this paper, we analyze the efficiency of two popular voxel-based 3D object
detection methods providing a good compromise between high performance and
speed based on two aspects, their ability to detect objects located at large
distances from the agent and their ability to operate in real time on embedded
platforms equipped with high-performance GPUs. Our experiments show that these
methods mostly fail to detect distant small objects due to the sparsity of the
input point clouds at large distances. Moreover, models trained on near objects
achieve similar or better performance compared to those trained on all objects
in the scene. This means that the models learn object appearance
representations mostly from near objects. Our findings suggest that a
considerable part of the computations of existing methods is focused on
locations of the scene that do not contribute with successful detection. This
means that the methods can achieve a speed-up of $40$-$60\%$ by restricting
operation to near objects while not sacrificing much in performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oleksiienko_I/0/1/0/all/0/1"&gt;Illia Oleksiienko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotation invariant CNN using scattering transform for image classification. (arXiv:2105.10175v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10175</id>
        <link href="http://arxiv.org/abs/2105.10175"/>
        <updated>2021-05-24T05:08:42.723Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks accuracy is heavily impacted by rotations
of the input data. In this paper, we propose a convolutional predictor that is
invariant to rotations in the input. This architecture is capable of predicting
the angular orientation without angle-annotated data. Furthermore, the
predictor maps continuously the random rotation of the input to a circular
space of the prediction. For this purpose, we use the roto-translation
properties existing in the Scattering Transform Networks with a series of 3D
Convolutions. We validate the results by training with upright and randomly
rotated samples. This allows further applications of this work on fields like
automatic re-orientation of randomly oriented datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salas_R/0/1/0/all/0/1"&gt;Rosemberg Rodriguez Salas&lt;/a&gt; (LIGM), &lt;a href="http://arxiv.org/find/cs/1/au:+Dokladalova_E/0/1/0/all/0/1"&gt;Eva Dokladalova&lt;/a&gt; (LIGM), &lt;a href="http://arxiv.org/find/cs/1/au:+Dokladal_P/0/1/0/all/0/1"&gt;Petr Dokl&amp;#xe1;dal&lt;/a&gt; (CMM)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection By Autoencoder Based On Weighted Frequency Domain Loss. (arXiv:2105.10214v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10214</id>
        <link href="http://arxiv.org/abs/2105.10214"/>
        <updated>2021-05-24T05:08:42.711Z</updated>
        <summary type="html"><![CDATA[In image anomaly detection, Autoencoders are the popular methods that
reconstruct the input image that might contain anomalies and output a clean
image with no abnormalities. These Autoencoder-based methods usually calculate
the anomaly score from the reconstruction error, the difference between the
input image and the reconstructed image. On the other hand, the accuracy of the
reconstruction is insufficient in many of these methods, so it leads to
degraded accuracy of anomaly detection. To improve the accuracy of the
reconstruction, we consider defining loss function in the frequency domain. In
general, we know that natural images contain many low-frequency components and
few high-frequency components. Hence, to improve the accuracy of the
reconstruction of high-frequency components, we introduce a new loss function
named weighted frequency domain loss(WFDL). WFDL provides a sharper
reconstructed image, which contributes to improving the accuracy of anomaly
detection. In this paper, we show our method's superiority over the
conventional Autoencoder methods by comparing it with AUROC on the MVTec AD
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nakanishi_M/0/1/0/all/0/1"&gt;Masaki Nakanishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sato_K/0/1/0/all/0/1"&gt;Kazuki Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Terada_H/0/1/0/all/0/1"&gt;Hideo Terada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search. (arXiv:2105.10154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10154</id>
        <link href="http://arxiv.org/abs/2105.10154"/>
        <updated>2021-05-24T05:08:42.704Z</updated>
        <summary type="html"><![CDATA[Human pose estimation has achieved significant progress in recent years.
However, most of the recent methods focus on improving accuracy using
complicated models and ignoring real-time efficiency. To achieve a better
trade-off between accuracy and efficiency, we propose a novel neural
architecture search (NAS) method, termed ViPNAS, to search networks in both
spatial and temporal levels for fast online video pose estimation. In the
spatial level, we carefully design the search space with five different
dimensions including network depth, width, kernel size, group number, and
attentions. In the temporal level, we search from a series of temporal feature
fusions to optimize the total accuracy and speed across multiple video frames.
To the best of our knowledge, we are the first to search for the temporal
feature fusion and automatic computation allocation in videos. Extensive
experiments demonstrate the effectiveness of our approach on the challenging
COCO2017 and PoseTrack2018 datasets. Our discovered model family, S-ViPNAS and
T-ViPNAS, achieve significantly higher inference speed (CPU real-time) without
sacrificing the accuracy compared to the previous state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lumin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yingda Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Sheng Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Properties of Vision Transformers. (arXiv:2105.10497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10497</id>
        <link href="http://arxiv.org/abs/2105.10497"/>
        <updated>2021-05-24T05:08:42.674Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViT) have demonstrated impressive performance across
various machine vision problems. These models are based on multi-head
self-attention mechanisms that can flexibly attend to a sequence of image
patches to encode contextual cues. An important question is how such
flexibility in attending image-wide context conditioned on a given patch can
facilitate handling nuisances in natural images e.g., severe occlusions, domain
shifts, spatial permutations, adversarial and natural perturbations. We
systematically study this question via an extensive set of experiments
encompassing three ViT families and comparisons with a high-performing
convolutional neural network (CNN). We show and analyze the following
intriguing properties of ViT: (a) Transformers are highly robust to severe
occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1
accuracy on ImageNet even after randomly occluding 80% of the image content.
(b) The robust performance to occlusions is not due to a bias towards local
textures, and ViTs are significantly less biased towards textures compared to
CNNs. When properly trained to encode shape-based features, ViTs demonstrate
shape recognition capability comparable to that of human visual system,
previously unmatched in the literature. (c) Using ViTs to encode shape
representation leads to an interesting consequence of accurate semantic
segmentation without pixel-level supervision. (d) Off-the-shelf features from a
single ViT model can be combined to create a feature ensemble, leading to high
accuracy rates across a range of classification datasets in both traditional
and few-shot learning paradigms. We show effective features of ViTs are due to
flexible and dynamic receptive fields possible via the self-attention
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1"&gt;Munawar Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DAVOS: Semi-Supervised Video Object Segmentation via Adversarial Domain Adaptation. (arXiv:2105.10201v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10201</id>
        <link href="http://arxiv.org/abs/2105.10201"/>
        <updated>2021-05-24T05:08:42.646Z</updated>
        <summary type="html"><![CDATA[Domain shift has always been one of the primary issues in video object
segmentation (VOS), for which models suffer from degeneration when tested on
unfamiliar datasets. Recently, many online methods have emerged to narrow the
performance gap between training data (source domain) and test data (target
domain) by fine-tuning on annotations of test data which are usually in
shortage. In this paper, we propose a novel method to tackle domain shift by
first introducing adversarial domain adaptation to the VOS task, with
supervised training on the source domain and unsupervised training on the
target domain. By fusing appearance and motion features with a convolution
layer, and by adding supervision onto the motion branch, our model achieves
state-of-the-art performance on DAVIS2016 with 82.6% mean IoU score after
supervised training. Meanwhile, our adversarial domain adaptation strategy
significantly raises the performance of the trained model when applied on
FBMS59 and Youtube-Object, without exploiting extra annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinshuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Songyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1"&gt;Gang Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeGleNet: A Weakly-Supervised Convolutional Neural Network for the Semantic Segmentation of Gleason Grades in Prostate Histology Images. (arXiv:2105.10445v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10445</id>
        <link href="http://arxiv.org/abs/2105.10445"/>
        <updated>2021-05-24T05:08:42.636Z</updated>
        <summary type="html"><![CDATA[Prostate cancer is one of the main diseases affecting men worldwide. The
Gleason scoring system is the primary diagnostic tool for prostate cancer. This
is obtained via the visual analysis of cancerous patterns in prostate biopsies
performed by expert pathologists, and the aggregation of the main Gleason
grades in a combined score. Computer-aided diagnosis systems allow to reduce
the workload of pathologists and increase the objectivity. Recently, efforts
have been made in the literature to develop algorithms aiming the direct
estimation of the global Gleason score at biopsy/core level with global labels.
However, these algorithms do not cover the accurate localization of the Gleason
patterns into the tissue. In this work, we propose a deep-learning-based system
able to detect local cancerous patterns in the prostate tissue using only the
global-level Gleason score during training. The methodological core of this
work is the proposed weakly-supervised-trained convolutional neural network,
WeGleNet, based on a multi-class segmentation layer after the feature
extraction module, a global-aggregation, and the slicing of the background
class for the model loss estimation during training. We obtained a Cohen's
quadratic kappa (k) of 0.67 for the pixel-level prediction of cancerous
patterns in the validation cohort. We compared the model performance for
semantic segmentation of Gleason grades with supervised state-of-the-art
architectures in the test cohort. We obtained a pixel-level k of 0.61 and a
macro-averaged f1-score of 0.58, at the same level as fully-supervised methods.
Regarding the estimation of the core-level Gleason score, we obtained a k of
0.76 and 0.67 between the model and two different pathologists. WeGleNet is
capable of performing the semantic segmentation of Gleason grades similarly to
fully-supervised methods without requiring pixel-level annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1"&gt;Julio Silva-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct Simultaneous Multi-Image Registration. (arXiv:2105.10087v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10087</id>
        <link href="http://arxiv.org/abs/2105.10087"/>
        <updated>2021-05-24T05:08:42.626Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel algorithm that registers a collection of
mono-modal 3D images in a simultaneous fashion, named as Direct Simultaneous
Registration (DSR). The algorithm optimizes global poses of local frames
directly based on the intensities of images (without extracting features from
the images). To obtain the optimal result, we start with formulating a Direct
Bundle Adjustment (DBA) problem which jointly optimizes pose parameters of
local frames and intensities of panoramic image. By proving the independence of
the pose from panoramic image in the iterative process, DSR is proposed and
proved to be able to generate the same optimal poses as DBA, but without
optimizing the intensities of the panoramic image. The proposed DSR method is
particularly suitable in mono-modal registration and in the scenarios where
distinct features are not available, such as Transesophageal Echocardiography
(TEE) images. The proposed method is validated via simulated and in-vivo 3D TEE
images. It is shown that the proposed method outperforms conventional
sequential registration method in terms of accuracy and the obtained results
can produce good alignment in in-vivo images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1"&gt;Zhehua Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shoudong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yiting Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1"&gt;Alex Pui-Wai Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AC-CovidNet: Attention Guided Contrastive CNN for Recognition of Covid-19 in Chest X-Ray Images. (arXiv:2105.10239v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10239</id>
        <link href="http://arxiv.org/abs/2105.10239"/>
        <updated>2021-05-24T05:08:42.619Z</updated>
        <summary type="html"><![CDATA[Covid-19 global pandemic continues to devastate health care systems across
the world. In many countries, the 2nd wave is very severe. Economical and rapid
testing, as well as diagnosis, is urgently needed to control the pandemic. At
present, the Covid-19 testing is costly and time-consuming. Chest X-Ray (CXR)
testing can be the fastest, scalable, and non-invasive method. The existing
methods suffer due to the limited CXR samples available from Covid-19. Thus,
inspired by the limitations of the open-source work in this field, we propose
attention guided contrastive CNN architecture (AC-CovidNet) for Covid-19
detection in CXR images. The proposed method learns the robust and
discriminative features with the help of contrastive loss. Moreover, the
proposed method gives more importance to the infected regions as guided by the
attention mechanism. We compute the sensitivity of the proposed method over the
publicly available Covid-19 dataset. It is observed that the proposed
AC-CovidNet exhibits very promising performance as compared to the existing
methods even with limited training data. It can tackle the bottleneck of CXR
Covid-19 datasets being faced by the researchers. The code used in this paper
is released publicly at \url{https://github.com/shivram1987/AC-CovidNet/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ambati_A/0/1/0/all/0/1"&gt;Anirudh Ambati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dubey_S/0/1/0/all/0/1"&gt;Shiv Ram Dubey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guidance and Teaching Network for Video Salient Object Detection. (arXiv:2105.10110v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10110</id>
        <link href="http://arxiv.org/abs/2105.10110"/>
        <updated>2021-05-24T05:08:42.612Z</updated>
        <summary type="html"><![CDATA[Owing to the difficulties of mining spatial-temporal cues, the existing
approaches for video salient object detection (VSOD) are limited in
understanding complex and noisy scenarios, and often fail in inferring
prominent objects. To alleviate such shortcomings, we propose a simple yet
efficient architecture, termed Guidance and Teaching Network (GTNet), to
independently distil effective spatial and temporal cues with implicit guidance
and explicit teaching at feature- and decision-level, respectively. To be
specific, we (a) introduce a temporal modulator to implicitly bridge features
from motion into the appearance branch, which is capable of fusing cross-modal
features collaboratively, and (b) utilise motion-guided mask to propagate the
explicit cues during the feature aggregation. This novel learning strategy
achieves satisfactory results via decoupling the complex spatial-temporal cues
and mapping informative cues across different modalities. Extensive experiments
on three challenging benchmarks show that the proposed method can run at ~28
fps on a single TITAN Xp GPU and perform competitively against 14 cutting-edge
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yu-Cheng Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuming Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shouyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1"&gt;Ge Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Resilient Collaborative Intelligence via Low-Rank Tensor Completion. (arXiv:2105.10341v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10341</id>
        <link href="http://arxiv.org/abs/2105.10341"/>
        <updated>2021-05-24T05:08:42.595Z</updated>
        <summary type="html"><![CDATA[In the race to bring Artificial Intelligence (AI) to the edge, collaborative
intelligence has emerged as a promising way to lighten the computation load on
edge devices that run applications based on Deep Neural Networks (DNNs).
Typically, a deep model is split at a certain layer into edge and cloud
sub-models. The deep feature tensor produced by the edge sub-model is
transmitted to the cloud, where the remaining computationally intensive
workload is performed by the cloud sub-model. The communication channel between
the edge and cloud is imperfect, which will result in missing data in the deep
feature tensor received at the cloud side. In this study, we examine the
effectiveness of four low-rank tensor completion methods in recovering missing
data in the deep feature tensor. We consider both sparse tensors, such as those
produced by the VGG16 model, as well as non-sparse tensors, such as those
produced by ResNet34 model. We study tensor completion effectiveness in both
conplexity-constrained and unconstrained scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bragilevsky_L/0/1/0/all/0/1"&gt;Lior Bragilevsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1"&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction. (arXiv:2105.10446v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10446</id>
        <link href="http://arxiv.org/abs/2105.10446"/>
        <updated>2021-05-24T05:08:42.589Z</updated>
        <summary type="html"><![CDATA[This work attempts to provide a plausible theoretical framework that aims to
interpret modern deep (convolutional) networks from the principles of data
compression and discriminative representation. We show that for
high-dimensional multi-class data, the optimal linear discriminative
representation maximizes the coding rate difference between the whole dataset
and the average of all the subsets. We show that the basic iterative gradient
ascent scheme for optimizing the rate reduction objective naturally leads to a
multi-layer deep network, named ReduNet, that shares common characteristics of
modern deep networks. The deep layered architectures, linear and nonlinear
operators, and even parameters of the network are all explicitly constructed
layer-by-layer via forward propagation, instead of learned via back
propagation. All components of so-obtained "white-box" network have precise
optimization, statistical, and geometric interpretation. Moreover, all linear
operators of the so-derived network naturally become multi-channel convolutions
when we enforce classification to be rigorously shift-invariant. The derivation
also indicates that such a deep convolution network is significantly more
efficient to construct and learn in the spectral domain. Our preliminary
simulations and experiments clearly verify the effectiveness of both the rate
reduction objective and the associated ReduNet. All code and data are available
at https://github.com/Ma-Lab-Berkeley.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Kwan Ho Ryan Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haozhi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1"&gt;John Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Endmember-Guided Unmixing Network (EGU-Net): A General Deep Learning Framework for Self-Supervised Hyperspectral Unmixing. (arXiv:2105.10194v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10194</id>
        <link href="http://arxiv.org/abs/2105.10194"/>
        <updated>2021-05-24T05:08:42.582Z</updated>
        <summary type="html"><![CDATA[Over the past decades, enormous efforts have been made to improve the
performance of linear or nonlinear mixing models for hyperspectral unmixing,
yet their ability to simultaneously generalize various spectral variabilities
and extract physically meaningful endmembers still remains limited due to the
poor ability in data fitting and reconstruction and the sensitivity to various
spectral variabilities. Inspired by the powerful learning ability of deep
learning, we attempt to develop a general deep learning approach for
hyperspectral unmixing, by fully considering the properties of endmembers
extracted from the hyperspectral imagery, called endmember-guided unmixing
network (EGU-Net). Beyond the alone autoencoder-like architecture, EGU-Net is a
two-stream Siamese deep network, which learns an additional network from the
pure or nearly-pure endmembers to correct the weights of another unmixing
network by sharing network parameters and adding spectrally meaningful
constraints (e.g., non-negativity and sum-to-one) towards a more accurate and
interpretable unmixing solution. Furthermore, the resulting general framework
is not only limited to pixel-wise spectral unmixing but also applicable to
spatial information modeling with convolutional operators for spatial-spectral
unmixing. Experimental results conducted on three different datasets with the
ground-truth of abundance maps corresponding to each material demonstrate the
effectiveness and superiority of the EGU-Net over state-of-the-art unmixing
algorithms. The codes will be available from the website:
https://github.com/danfenghong/IEEE_TNNLS_EGU-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hong_D/0/1/0/all/0/1"&gt;Danfeng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianru Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jing Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yokoya_N/0/1/0/all/0/1"&gt;Naoto Yokoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heiden_U/0/1/0/all/0/1"&gt;Uta Heiden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning Visual Prototypes with BERT Embeddings for Few-Shot Learning. (arXiv:2105.10195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10195</id>
        <link href="http://arxiv.org/abs/2105.10195"/>
        <updated>2021-05-24T05:08:42.562Z</updated>
        <summary type="html"><![CDATA[Few-shot learning (FSL) is the task of learning to recognize previously
unseen categories of images from a small number of training examples. This is a
challenging task, as the available examples may not be enough to unambiguously
determine which visual features are most characteristic of the considered
categories. To alleviate this issue, we propose a method that additionally
takes into account the names of the image classes. While the use of class names
has already been explored in previous work, our approach differs in two key
aspects. First, while previous work has aimed to directly predict visual
prototypes from word embeddings, we found that better results can be obtained
by treating visual and text-based prototypes separately. Second, we propose a
simple strategy for learning class name embeddings using the BERT language
model, which we found to substantially outperform the GloVe vectors that were
used in previous work. We furthermore propose a strategy for dealing with the
high dimensionality of these vectors, inspired by models for aligning
cross-lingual word embeddings. We provide experiments on miniImageNet, CUB and
tieredImageNet, showing that our approach consistently improves the
state-of-the-art in metric-based FSL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1"&gt;Kun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouraoui_Z/0/1/0/all/0/1"&gt;Zied Bouraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Ping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jameel_S/0/1/0/all/0/1"&gt;Shoaib Jameel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1"&gt;Steven Schockaert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-color balance for color constancy. (arXiv:2105.10228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10228</id>
        <link href="http://arxiv.org/abs/2105.10228"/>
        <updated>2021-05-24T05:08:42.552Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel multi-color balance adjustment for color
constancy. The proposed method, called "n-color balancing," allows us not only
to perfectly correct n target colors on the basis of corresponding ground truth
colors but also to correct colors other than the n colors. In contrast,
although white-balancing can perfectly adjust white, colors other than white
are not considered in the framework of white-balancing in general. In an
experiment, the proposed multi-color balancing is demonstrated to outperform
both conventional white and multi-color balance adjustments including
Bradford's model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akazawa_T/0/1/0/all/0/1"&gt;Teruaki Akazawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1"&gt;Yuma Kinoshita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Fine-Grained Low-Shot Learning. (arXiv:2105.10438v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10438</id>
        <link href="http://arxiv.org/abs/2105.10438"/>
        <updated>2021-05-24T05:08:42.522Z</updated>
        <summary type="html"><![CDATA[We develop a novel compositional generative model for zero- and few-shot
learning to recognize fine-grained classes with a few or no training samples.
Our key observation is that generating holistic features for fine-grained
classes fails to capture small attribute differences between classes.
Therefore, we propose a feature composition framework that learns to extract
attribute features from training samples and combines them to construct
fine-grained features for rare and unseen classes. Feature composition allows
us to not only selectively compose features of every class from only relevant
training samples, but also obtain diversity among composed features via
changing samples used for the composition. In addition, instead of building
holistic features for classes, we use our attribute features to form dense
representations capable of capturing fine-grained attribute details of classes.
We propose a training scheme that uses a discriminative model to construct
features that are subsequently used to train the model itself. Therefore, we
directly train the discriminative model on the composed features without
learning a separate generative model. We conduct experiments on four popular
datasets of DeepFashion, AWA2, CUB, and SUN, showing the effectiveness of our
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1"&gt;Dat Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elhamifar_E/0/1/0/all/0/1"&gt;Ehsan Elhamifar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharing Pain: Using Domain Transfer Between Pain Types for Recognition of Sparse Pain Expressions in Horses. (arXiv:2105.10313v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10313</id>
        <link href="http://arxiv.org/abs/2105.10313"/>
        <updated>2021-05-24T05:08:42.516Z</updated>
        <summary type="html"><![CDATA[Orthopedic disorders are a common cause for euthanasia among horses, which
often could have been avoided with earlier detection. These conditions often
create varying degrees of subtle but long-term pain. It is challenging to train
a visual pain recognition method with video data depicting such pain, since the
resulting pain behavior also is subtle, sparsely appearing, and varying, making
it challenging for even an expert human labeler to provide accurate
ground-truth for the data. We show that transferring features from a dataset of
horses with acute nociceptive pain (where labeling is less ambiguous) can aid
the learning to recognize more complex orthopedic pain. Moreover, we present a
human expert baseline for the problem, as well as an extensive empirical study
of various domain transfer methods and of what is detected by the pain
recognition method trained on acute pain in the orthopedic dataset. Finally,
this is accompanied with a discussion around the challenges posed by real-world
animal behavior datasets and how best practices can be established for similar
fine-grained action recognition tasks. Our code is available at
https://github.com/sofiabroome/painface-recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1"&gt;Sofia Broom&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ask_K/0/1/0/all/0/1"&gt;Katrina Ask&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1"&gt;Maheen Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1"&gt;Pia Haubro Andersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1"&gt;Hedvig Kjellstr&amp;#xf6;m&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma. (arXiv:2105.10238v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10238</id>
        <link href="http://arxiv.org/abs/2105.10238"/>
        <updated>2021-05-24T05:08:42.510Z</updated>
        <summary type="html"><![CDATA[Pelvic ring disruptions result from blunt injury mechanisms and are often
found in patients with multi-system trauma. To grade pelvic fracture severity
in trauma victims based on whole-body CT, the Tile AO/OTA classification is
frequently used. Due to the high volume of whole-body trauma CTs generated in
busy trauma centers, an automated approach to Tile classification would provide
substantial value, e.,g., to prioritize the reading queue of the attending
trauma radiologist. In such scenario, an automated method should perform
grading based on a transparent process and based on interpretable features to
enable interaction with human readers and lower their workload by offering
insights from a first automated read of the scan. This paper introduces an
automated yet interpretable pelvic trauma decision support system to assist
radiologists in fracture detection and Tile grade classification. The method
operates similarly to human interpretation of CT scans and first detects
distinct pelvic fractures on CT with high specificity using a Faster-RCNN model
that are then interpreted using a structural causal model based on clinical
best practices to infer an initial Tile grade. The Bayesian causal model and
finally, the object detector are then queried for likely co-occurring fractures
that may have been rejected initially due to the highly specific operating
point of the detector, resulting in an updated list of detected fractures and
corresponding final Tile grade. Our method is transparent in that it provides
finding location and type using the object detector, as well as information on
important counterfactuals that would invalidate the system's recommendation and
achieves an AUC of 83.3%/85.1% for translational/rotational instability.
Despite being designed for human-machine teaming, our approach does not
compromise on performance compared to previous black-box approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zapaishchykova_A/0/1/0/all/0/1"&gt;Anna Zapaishchykova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dreizin_D/0/1/0/all/0/1"&gt;David Dreizin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhaoshuo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jie Ying Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roohi_S/0/1/0/all/0/1"&gt;Shahrooz Faghih Roohi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Unberath_M/0/1/0/all/0/1"&gt;Mathias Unberath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Driving-Signal Aware Full-Body Avatars. (arXiv:2105.10441v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10441</id>
        <link href="http://arxiv.org/abs/2105.10441"/>
        <updated>2021-05-24T05:08:42.501Z</updated>
        <summary type="html"><![CDATA[We present a learning-based method for building driving-signal aware
full-body avatars. Our model is a conditional variational autoencoder that can
be animated with incomplete driving signals, such as human pose and facial
keypoints, and produces a high-quality representation of human geometry and
view-dependent appearance. The core intuition behind our method is that better
drivability and generalization can be achieved by disentangling the driving
signals and remaining generative factors, which are not available during
animation. To this end, we explicitly account for information deficiency in the
driving signal by introducing a latent space that exclusively captures the
remaining information, thus enabling the imputation of the missing factors
required during full-body animation, while remaining faithful to the driving
signal. We also propose a learnable localized compression for the driving
signal which promotes better generalization, and helps minimize the influence
of global chance-correlations often found in real datasets. For a given driving
signal, the resulting variational model produces a compact space of uncertainty
for missing factors that allows for an imputation strategy best suited to a
particular application. We demonstrate the efficacy of our approach on the
challenging problem of full-body animation for virtual telepresence with
driving signals acquired from minimal sensors placed in the environment and
mounted on a VR-headset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1"&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenglei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1"&gt;Tomas Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1"&gt;Fabian Prada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1"&gt;Takaaki Shiratori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1"&gt;Shih-En Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weipeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1"&gt;Yaser Sheikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1"&gt;Jason Saragih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IDEAL: Independent Domain Embedding Augmentation Learning. (arXiv:2105.10112v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10112</id>
        <link href="http://arxiv.org/abs/2105.10112"/>
        <updated>2021-05-24T05:08:42.495Z</updated>
        <summary type="html"><![CDATA[Many efforts have been devoted to designing sampling, mining, and weighting
strategies in high-level deep metric learning (DML) loss objectives. However,
little attention has been paid to low-level but essential data transformation.
In this paper, we develop a novel mechanism, the independent domain embedding
augmentation learning ({IDEAL}) method. It can simultaneously learn multiple
independent embedding spaces for multiple domains generated by predefined data
transformations. Our IDEAL is orthogonal to existing DML techniques and can be
seamlessly combined with prior DML approaches for enhanced performance.
Empirical results on visual retrieval tasks demonstrate the superiority of the
proposed method. For example, the IDEAL improves the performance of MS loss by
a large margin, 84.5\% $\rightarrow$ 87.1\% on Cars-196, and 65.8\%
$\rightarrow$ 69.5\% on CUB-200 at Recall$@1$. Our IDEAL with MS loss also
achieves the new state-of-the-art performance on three image retrieval
benchmarks, \ie, \emph{Cars-196}, \emph{CUB-200}, and \emph{SOP}. It
outperforms the most recent DML approaches, such as Circle loss and XBM,
significantly. The source code and pre-trained models of our method will be
available at\emph{\url{https://github.com/emdata-ailab/IDEAL}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_G/0/1/0/all/0/1"&gt;Guang Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wennan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sheaves as a Framework for Understanding and Interpreting Model Fit. (arXiv:2105.10414v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10414</id>
        <link href="http://arxiv.org/abs/2105.10414"/>
        <updated>2021-05-24T05:08:42.478Z</updated>
        <summary type="html"><![CDATA[As data grows in size and complexity, finding frameworks which aid in
interpretation and analysis has become critical. This is particularly true when
data comes from complex systems where extensive structure is available, but
must be drawn from peripheral sources. In this paper we argue that in such
situations, sheaves can provide a natural framework to analyze how well a
statistical model fits at the local level (that is, on subsets of related
datapoints) vs the global level (on all the data). The sheaf-based approach
that we propose is suitably general enough to be useful in a range of
applications, from analyzing sensor networks to understanding the feature space
of a deep learning model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jefferson_B/0/1/0/all/0/1"&gt;Brett Jefferson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joslyn_C/0/1/0/all/0/1"&gt;Cliff Joslyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purvine_E/0/1/0/all/0/1"&gt;Emilie Purvine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Act Like a Radiologist: Towards Reliable Multi-view Correspondence Reasoning for Mammogram Mass Detection. (arXiv:2105.10160v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10160</id>
        <link href="http://arxiv.org/abs/2105.10160"/>
        <updated>2021-05-24T05:08:42.472Z</updated>
        <summary type="html"><![CDATA[Mammogram mass detection is crucial for diagnosing and preventing the breast
cancers in clinical practice. The complementary effect of multi-view mammogram
images provides valuable information about the breast anatomical prior
structure and is of great significance in digital mammography interpretation.
However, unlike radiologists who can utilize the natural reasoning ability to
identify masses based on multiple mammographic views, how to endow the existing
object detection models with the capability of multi-view reasoning is vital
for decision-making in clinical diagnosis but remains the boundary to explore.
In this paper, we propose an Anatomy-aware Graph convolutional Network (AGN),
which is tailored for mammogram mass detection and endows existing detection
methods with multi-view reasoning ability. The proposed AGN consists of three
steps. Firstly, we introduce a Bipartite Graph convolutional Network (BGN) to
model the intrinsic geometric and semantic relations of ipsilateral views.
Secondly, considering that the visual asymmetry of bilateral views is widely
adopted in clinical practice to assist the diagnosis of breast lesions, we
propose an Inception Graph convolutional Network (IGN) to model the structural
similarities of bilateral views. Finally, based on the constructed graphs, the
multi-view information is propagated through nodes methodically, which equips
the features learned from the examined view with multi-view reasoning ability.
Experiments on two standard benchmarks reveal that AGN significantly exceeds
the state-of-the-art performance. Visualization results show that AGN provides
interpretable visual cues for clinical diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuhang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fandong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaoqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Siwen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Human Pose Regression using Graph Convolutional Network. (arXiv:2105.10379v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10379</id>
        <link href="http://arxiv.org/abs/2105.10379"/>
        <updated>2021-05-24T05:08:42.465Z</updated>
        <summary type="html"><![CDATA[3D human pose estimation is a difficult task, due to challenges such as
occluded body parts and ambiguous poses. Graph convolutional networks encode
the structural information of the human skeleton in the form of an adjacency
matrix, which is beneficial for better pose prediction. We propose one such
graph convolutional network named PoseGraphNet for 3D human pose regression
from 2D poses. Our network uses an adaptive adjacency matrix and kernels
specific to neighbor groups. We evaluate our model on the Human3.6M dataset
which is a standard dataset for 3D pose estimation. Our model's performance is
close to the state-of-the-art, but with much fewer parameters. The model learns
interesting adjacency relations between joints that have no physical
connections, but are behaviorally similar.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banik_S/0/1/0/all/0/1"&gt;Soubarna Banik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gracia_A/0/1/0/all/0/1"&gt;Alejandro Mendoza Gracia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning. (arXiv:2105.10203v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10203</id>
        <link href="http://arxiv.org/abs/2105.10203"/>
        <updated>2021-05-24T05:08:42.459Z</updated>
        <summary type="html"><![CDATA[Hidden features in neural network usually fail to learn informative
representation for 3D segmentation as supervisions are only given on output
prediction, while this can be solved by omni-scale supervision on intermediate
layers. In this paper, we bring the first omni-scale supervision method to
point cloud segmentation via the proposed gradual Receptive Field Component
Reasoning (RFCR), where target Receptive Field Component Codes (RFCCs) are
designed to record categories within receptive fields for hidden units in the
encoder. Then, target RFCCs will supervise the decoder to gradually infer the
RFCCs in a coarse-to-fine categories reasoning manner, and finally obtain the
semantic labels. Because many hidden features are inactive with tiny magnitude
and make minor contributions to RFCC prediction, we propose a Feature
Densification with a centrifugal potential to obtain more unambiguous features,
and it is in effect equivalent to entropy regularization over features. More
active features can further unleash the potential of our omni-supervision
method. We embed our method into four prevailing backbones and test on three
challenging benchmarks. Our method can significantly improve the backbones in
all three datasets. Specifically, our method brings new state-of-the-art
performances for S3DIS as well as Semantic3D and ranks the 1st in the ScanNet
benchmark among all the point-based methods. Code will be publicly available at
https://github.com/azuki-miho/RFCR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1"&gt;Jingyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiachen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xin Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Haichuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1"&gt;Yanyun Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Transformer Generators with Convolutional Discriminators. (arXiv:2105.10189v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10189</id>
        <link href="http://arxiv.org/abs/2105.10189"/>
        <updated>2021-05-24T05:08:42.452Z</updated>
        <summary type="html"><![CDATA[Transformer models have recently attracted much interest from computer vision
researchers and have since been successfully employed for several problems
traditionally addressed with convolutional neural networks. At the same time,
image synthesis using generative adversarial networks (GANs) has drastically
improved over the last few years. The recently proposed TransGAN is the first
GAN using only transformer-based architectures and achieves competitive results
when compared to convolutional GANs. However, since transformers are
data-hungry architectures, TransGAN requires data augmentation, an auxiliary
super-resolution task during training, and a masking prior to guide the
self-attention mechanism. In this paper, we study the combination of a
transformer-based generator and convolutional discriminator and successfully
remove the need of the aforementioned required design choices. We evaluate our
approach by conducting a benchmark of well-known CNN discriminators, ablate the
size of the transformer-based generator, and show that combining both
architectural elements into a hybrid model leads to better results.
Furthermore, we investigate the frequency spectrum properties of generated
images and observe that our model retains the benefits of an attention based
generator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1"&gt;Ricard Durall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1"&gt;Stanislav Frolov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1"&gt;Janis Keuper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlated Input-Dependent Label Noise in Large-Scale Image Classification. (arXiv:2105.10305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10305</id>
        <link href="http://arxiv.org/abs/2105.10305"/>
        <updated>2021-05-24T05:08:42.430Z</updated>
        <summary type="html"><![CDATA[Large scale image classification datasets often contain noisy labels. We take
a principled probabilistic approach to modelling input-dependent, also known as
heteroscedastic, label noise in these datasets. We place a multivariate Normal
distributed latent variable on the final hidden layer of a neural network
classifier. The covariance matrix of this latent variable, models the aleatoric
uncertainty due to label noise. We demonstrate that the learned covariance
structure captures known sources of label noise between semantically similar
and co-occurring classes. Compared to standard neural network training and
other baselines, we show significantly improved accuracy on Imagenet ILSVRC
2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a
new state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These
datasets range from over 1M to over 300M training examples and from 1k classes
to more than 21k classes. Our method is simple to use, and we provide an
implementation that is a drop-in replacement for the final fully-connected
layer in a deep classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Collier_M/0/1/0/all/0/1"&gt;Mark Collier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1"&gt;Basil Mustafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokiopoulou_E/0/1/0/all/0/1"&gt;Efi Kokiopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berent_J/0/1/0/all/0/1"&gt;Jesse Berent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Helsinki Deblur Challenge 2021: description of photographic data. (arXiv:2105.10233v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10233</id>
        <link href="http://arxiv.org/abs/2105.10233"/>
        <updated>2021-05-24T05:08:42.415Z</updated>
        <summary type="html"><![CDATA[The photographic dataset collected for the Helsinki Deblur Challenge 2021
(HDC2021) contains pairs of images taken by two identical cameras of the same
target but with different conditions. One camera is always in focus and
produces sharp and low-noise images the other camera produces blurred and noisy
images as it is gradually more and more out of focus and has a higher ISO
setting. Even though the dataset was designed and captured with the HDC2021 in
mind it can be used for any testing and benchmarking of image deblurring
algorithms. The data is available here: https://doi.org/10.5281/zenodo.477228]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Juvonen_M/0/1/0/all/0/1"&gt;Markus Juvonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Siltanen_S/0/1/0/all/0/1"&gt;Samuli Siltanen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moura_F/0/1/0/all/0/1"&gt;Fernando Silva de Moura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Training Approach for Very Large Scale Face Recognition. (arXiv:2105.10375v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10375</id>
        <link href="http://arxiv.org/abs/2105.10375"/>
        <updated>2021-05-24T05:08:42.409Z</updated>
        <summary type="html"><![CDATA[Face recognition has achieved significant progress in deep-learning era due
to the ultra-large-scale and well-labeled datasets. However, training on
ultra-large-scale datasets is time-consuming and takes up a lot of hardware
resource. Therefore, how to design an appropriate training approach is very
crucial and indispensable. The computational and hardware cost of training
ultra-large-scale datasets mainly focuses on the Fully-Connected (FC) layer
rather than convolutional layers. To this end, we propose a novel training
approach for ultra-large-scale face datasets, termed Faster Face Classification
(F$^2$C). In F$^2$C, we first define a Gallery Net and a Probe Net that are
used to generate identities' centers and extract faces' features for face
recognition, respectively. Gallery Net has the same structure as Probe Net and
inherits the parameters from Probe Net with a moving average paradigm. After
that, to reduce the training time and hardware resource occupancy of the FC
layer, we propose the Dynamic Class Pool that stores the features from Gallery
Net and calculates the inner product (logits) with positive samples (its
identities appear in Dynamic Class Pool) in each mini-batch. Dynamic Class Pool
can be regarded as a substitute for the FC layer and its size is much smaller
than FC, which is the reason why Dynamic Class Pool can largely reduce the time
and resource cost. For negative samples (its identities are not appear in the
Dynamic Class Pool), we minimize the cosine similarities between negative
samples and Dynamic Class Pool. Then, to improve the update efficiency and
speed of Dynamic Class Pool's parameters, we design the Dual Loaders including
Identity-based and Instance-based Loaders. Dual Loaders load images from given
dataset by instances and identities to generate batches for training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhipeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaobo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xiaojiang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Baigui Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices. (arXiv:2105.10288v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10288</id>
        <link href="http://arxiv.org/abs/2105.10288"/>
        <updated>2021-05-24T05:08:42.383Z</updated>
        <summary type="html"><![CDATA[Single-Image Super Resolution (SISR) is a classical computer vision problem
and it has been studied for over decades. With the recent success of deep
learning methods, recent work on SISR focuses solutions with deep learning
methodologies and achieves state-of-the-art results. However most of the
state-of-the-art SISR methods contain millions of parameters and layers, which
limits their practical applications. In this paper, we propose a hardware
(Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization
robust real-time super resolution network (XLSR). The proposed model's building
block is inspired from root modules for Image classification. We successfully
applied root modules to SISR problem, further more to make the model uint8
quantization robust we used Clipped ReLU at the last layer of the network and
achieved great balance between reconstruction quality and runtime. Furthermore,
although the proposed network contains 30x fewer parameters than VDSR its
performance surpasses it on Div2K validation set. The network proved itself by
winning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayazoglu_M/0/1/0/all/0/1"&gt;Mustafa Ayazoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task, Multi-Domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets. (arXiv:2105.10310v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10310</id>
        <link href="http://arxiv.org/abs/2105.10310"/>
        <updated>2021-05-24T05:08:42.370Z</updated>
        <summary type="html"><![CDATA[Automatic segmentation of magnetic resonance (MR) images is crucial for
morphological evaluation of the pediatric musculoskeletal system in clinical
practice. However, the accuracy and generalization performance of individual
segmentation models are limited due to the restricted amount of annotated
pediatric data. Hence, we propose to train a segmentation model on multiple
datasets, arising from different parts of the anatomy, in a multi-task and
multi-domain learning framework. This approach allows to overcome the inherent
scarcity of pediatric data while benefiting from a more robust shared
representation. The proposed segmentation network comprises shared
convolutional filters, domain-specific batch normalization parameters that
compute the respective dataset statistics and a domain-specific segmentation
layer. Furthermore, a supervised contrastive regularization is integrated to
further improve generalization capabilities, by promoting intra-domain
similarity and impose inter-domain margins in embedded space. We evaluate our
contributions on two pediatric imaging datasets of the ankle and shoulder
joints for bone segmentation. Results demonstrate that the proposed model
outperforms state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boutillon_A/0/1/0/all/0/1"&gt;Arnaud Boutillon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conze_P/0/1/0/all/0/1"&gt;Pierre-Henri Conze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pons_C/0/1/0/all/0/1"&gt;Christelle Pons&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burdin_V/0/1/0/all/0/1"&gt;Val&amp;#xe9;rie Burdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borotikar_B/0/1/0/all/0/1"&gt;Bhushan Borotikar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Visible Connectivity Dynamics for Cloth Smoothing. (arXiv:2105.10389v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10389</id>
        <link href="http://arxiv.org/abs/2105.10389"/>
        <updated>2021-05-24T05:08:42.352Z</updated>
        <summary type="html"><![CDATA[Robotic manipulation of cloth remains challenging for robotics due to the
complex dynamics of the cloth, lack of a low-dimensional state representation,
and self-occlusions. In contrast to previous model-based approaches that learn
a pixel-based dynamics model or a compressed latent vector dynamics, we propose
to learn a particle-based dynamics model from a partial point cloud
observation. To overcome the challenges of partial observability, we infer
which visible points are connected on the underlying cloth mesh. We then learn
a dynamics model over this visible connectivity graph. Compared to previous
learning-based approaches, our model poses strong inductive bias with its
particle based representation for learning the underlying cloth physics; it is
invariant to visual features; and the predictions can be more easily
visualized. We show that our method greatly outperforms previous
state-of-the-art model-based and model-free reinforcement learning methods in
simulation. Furthermore, we demonstrate zero-shot sim-to-real transfer where we
deploy the model trained in simulation on a Franka arm and show that the model
can successfully smooth different types of cloth from crumpled configurations.
Videos can be found on our project website.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xingyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yufei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1"&gt;David Held&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection of Test-Time Evasion Attacks using Class-conditional Generative Adversarial Networks. (arXiv:2105.10101v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10101</id>
        <link href="http://arxiv.org/abs/2105.10101"/>
        <updated>2021-05-24T05:08:42.346Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have been shown vulnerable to adversarial
(Test-Time Evasion (TTE)) attacks which, by making small changes to the input,
alter the DNN's decision. We propose an attack detector based on
class-conditional Generative Adversarial Networks (GANs). We model the
distribution of clean data conditioned on the predicted class label by an
Auxiliary Classifier GAN (ACGAN). Given a test sample and its predicted class,
three detection statistics are calculated using the ACGAN Generator and
Discriminator. Experiments on image classification datasets under different TTE
attack methods show that our method outperforms state-of-the-art detection
methods. We also investigate the effectiveness of anomaly detection using
different DNN layers (input features or internal-layer features) and
demonstrate that anomalies are harder to detect using features closer to the
DNN's output layer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1"&gt;David J. Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1"&gt;George Kesidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Memory Implementations of Ridge Solutions for Broad Learning System with Incremental Learning. (arXiv:2105.10424v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10424</id>
        <link href="http://arxiv.org/abs/2105.10424"/>
        <updated>2021-05-24T05:08:42.339Z</updated>
        <summary type="html"><![CDATA[The existing low-memory BLS implementation proposed recently avoids the need
for storing and inverting large matrices, to achieve efficient usage of
memories. However, the existing low-memory BLS implementation sacrifices the
testing accuracy as a price for efficient usage of memories, since it can no
longer obtain the generalized inverse or ridge solution for the output weights
during incremental learning, and it cannot work under the very small ridge
parameter that is utilized in the original BLS. Accordingly, it is required to
develop the low-memory BLS implementations, which can work under very small
ridge parameters and compute the generalized inverse or ridge solution for the
output weights in the process of incremental learning. In this paper, firstly
we propose the low-memory implementations for the recently proposed recursive
and square-root BLS algorithms on added inputs and the recently proposed
squareroot BLS algorithm on added nodes, by simply processing a batch of inputs
or nodes in each recursion. Since the recursive BLS implementation includes the
recursive updates of the inverse matrix that may introduce numerical
instabilities after a large number of iterations, and needs the extra
computational load to decompose the inverse matrix into the Cholesky factor
when cooperating with the proposed low-memory implementation of the square-root
BLS algorithm on added nodes, we only improve the low-memory implementations of
the square-root BLS algorithms on added inputs and nodes, to propose the full
lowmemory implementation of the square-root BLS algorithm. All the proposed
low-memory BLS implementations compute the ridge solution for the output
weights in the process of incremental learning, and most of them can work under
very small ridge parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hufei Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Gaussian Model Boosting. (arXiv:2105.08966v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08966</id>
        <link href="http://arxiv.org/abs/2105.08966"/>
        <updated>2021-05-24T05:08:42.333Z</updated>
        <summary type="html"><![CDATA[Latent Gaussian models and boosting are widely used techniques in statistics
and machine learning. Tree-boosting shows excellent predictive accuracy on many
data sets, but potential drawbacks are that it assumes conditional independence
of samples, produces discontinuous predictions for, e.g., spatial data, and it
can have difficulty with high-cardinality categorical variables. Latent
Gaussian models, such as Gaussian process and grouped random effects models,
are flexible prior models that allow for making probabilistic predictions.
However, existing latent Gaussian models usually assume either a zero or a
linear prior mean function which can be an unrealistic assumption. This article
introduces a novel approach that combines boosting and latent Gaussian models
in order to remedy the above-mentioned drawbacks and to leverage the advantages
of both techniques. We obtain increased predictive accuracy compared to
existing approaches in both simulated and real-world data experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sigrist_F/0/1/0/all/0/1"&gt;Fabio Sigrist&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond permutation equivariance in graph networks. (arXiv:2103.14066v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14066</id>
        <link href="http://arxiv.org/abs/2103.14066"/>
        <updated>2021-05-24T05:08:42.327Z</updated>
        <summary type="html"><![CDATA[In this draft paper, we introduce a novel architecture for graph networks
which is equivariant to the Euclidean group in $n$-dimensions. The model is
designed to work with graph networks in their general form and can be shown to
include particular variants as special cases. Thanks to its equivariance
properties, we expect the proposed model to be more data efficient with respect
to classical graph architectures and also intrinsically equipped with a better
inductive bias. We defer investigating this matter to future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1"&gt;Emma Slade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1"&gt;Francesco Farina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words. (arXiv:2101.00265v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00265</id>
        <link href="http://arxiv.org/abs/2101.00265"/>
        <updated>2021-05-24T05:08:42.310Z</updated>
        <summary type="html"><![CDATA[Text-to-image retrieval is an essential task in cross-modal information
retrieval, i.e., retrieving relevant images from a large and unlabelled dataset
given textual queries. In this paper, we propose VisualSparta, a novel
(Visual-text Sparse Transformer Matching) model that shows significant
improvement in terms of both accuracy and efficiency. VisualSparta is capable
of outperforming previous state-of-the-art scalable methods in MSCOCO and
Flickr30K. We also show that it achieves substantial retrieving speed
advantages, i.e., for a 1 million image index, VisualSparta using CPU gets
~391X speedup compared to CPU vector search and ~5.4X speedup compared to
vector search with GPU acceleration. Experiments show that this speed advantage
even gets bigger for larger datasets because VisualSparta can be efficiently
implemented as an inverted index. To the best of our knowledge, VisualSparta is
the first transformer-based text-to-image retrieval model that can achieve
real-time searching for large-scale datasets, with significant accuracy
improvement compared to previous state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyusong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BOTD: Bold Outline Text Detector. (arXiv:2011.14714v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14714</id>
        <link href="http://arxiv.org/abs/2011.14714"/>
        <updated>2021-05-24T05:08:42.278Z</updated>
        <summary type="html"><![CDATA[Recently, text detection has attracted sufficient attention in the field of
computer vision and artificial intelligence. Among the existing approaches,
regression-based models are limited to handle the texts with arbitrary shapes,
while segmentation-based algorithms have high computational costs and suffer
from the text adhesion problem. In this paper, we propose a new one-stage text
detector, termed as Bold Outline Text Detector (BOTD), which is able to process
the arbitrary-shaped text with low model complexity. Different from previous
works, BOTD utilizes the Polar Minimum Distance (PMD) to encode the shortest
distance between the center point and the contour of the text instance, and
generates a Center Mask (CM) for each text instance. After learning the PMD
heat map and CM map, the final results can be obtained with a simple Text
Reconstruction Module (TRM). Since the CM resides within the text box exactly,
the text adhesion problem is avoided naturally. Meanwhile, all the points on
the text contour share the same PMD, so the complexity of BOTD is much lower
than existing segmentation-based methods. Experimental results on three
real-world benchmarks show the state-of-the-art performance of BOTD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chuang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1"&gt;Zhitong Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mulin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[e-ACJ: Accurate Junction Extraction For Event Cameras. (arXiv:2101.11251v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11251</id>
        <link href="http://arxiv.org/abs/2101.11251"/>
        <updated>2021-05-24T05:08:42.266Z</updated>
        <summary type="html"><![CDATA[Junctions reflect the important geometrical structure information of the
image, and are of primary significance to applications such as image matching
and motion analysis. Previous event-based feature extraction methods are mainly
focused on corners, which mainly find their locations, however, ignoring the
geometrical structure information like orientations and scales of edges. This
paper adapts the frame-based a-contrario junction detector(ACJ) to event data,
proposing the event-based a-contrario junction detector(e-ACJ), which yields
junctions' locations while giving the scales and orientations of their
branches. The proposed method relies on an a-contrario model and can operate on
asynchronous events directly without generating synthesized event frames. We
evaluate the performance on public event datasets. The result shows our method
successfully finds the orientations and scales of branches, while maintaining
high accuracy in junction's location.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yuqian Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Fidelity Fingerprint Generation: Quality, Uniqueness, and Privacy. (arXiv:2105.10403v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10403</id>
        <link href="http://arxiv.org/abs/2105.10403"/>
        <updated>2021-05-24T05:08:42.259Z</updated>
        <summary type="html"><![CDATA[In this work, we utilize progressive growth-based Generative Adversarial
Networks (GANs) to develop the Clarkson Fingerprint Generator (CFG). We
demonstrate that the CFG is capable of generating realistic, high fidelity,
$512\times512$ pixels, full, plain impression fingerprints. Our results suggest
that the fingerprints generated by the CFG are unique, diverse, and resemble
the training dataset in terms of minutiae configuration and quality, while not
revealing the underlying identities of the training data. We make the
pre-trained CFG model and the synthetically generated dataset publicly
available at https://github.com/keivanB/Clarkson_Finger_Gen]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bahmani_K/0/1/0/all/0/1"&gt;Keivan Bahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plesh_R/0/1/0/all/0/1"&gt;Richard Plesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_P/0/1/0/all/0/1"&gt;Peter Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuckers_S/0/1/0/all/0/1"&gt;Stephanie Schuckers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swyka_T/0/1/0/all/0/1"&gt;Timothy Swyka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Remote Sensing Benchmark Datasets for Land Cover Classification with A Shared and Specific Feature Learning Model. (arXiv:2105.10196v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10196</id>
        <link href="http://arxiv.org/abs/2105.10196"/>
        <updated>2021-05-24T05:08:42.253Z</updated>
        <summary type="html"><![CDATA[As remote sensing (RS) data obtained from different sensors become available
largely and openly, multimodal data processing and analysis techniques have
been garnering increasing interest in the RS and geoscience community. However,
due to the gap between different modalities in terms of imaging sensors,
resolutions, and contents, embedding their complementary information into a
consistent, compact, accurate, and discriminative representation, to a great
extent, remains challenging. To this end, we propose a shared and specific
feature learning (S2FL) model. S2FL is capable of decomposing multimodal RS
data into modality-shared and modality-specific components, enabling the
information blending of multi-modalities more effectively, particularly for
heterogeneous data sources. Moreover, to better assess multimodal baselines and
the newly-proposed S2FL model, three multimodal RS benchmark datasets, i.e.,
Houston2013 -- hyperspectral and multispectral data, Berlin -- hyperspectral
and synthetic aperture radar (SAR) data, Augsburg -- hyperspectral, SAR, and
digital surface model (DSM) data, are released and used for land cover
classification. Extensive experiments conducted on the three datasets
demonstrate the superiority and advancement of our S2FL model in the task of
land cover classification in comparison with previously-proposed
state-of-the-art baselines. Furthermore, the baseline codes and datasets used
in this paper will be made available freely at
https://github.com/danfenghong/ISPRS_S2FL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1"&gt;Danfeng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jingliang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jing Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swimmer Stroke Rate Estimation From Overhead Race Video. (arXiv:2104.12056v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12056</id>
        <link href="http://arxiv.org/abs/2104.12056"/>
        <updated>2021-05-24T05:08:42.130Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a swimming analytics system for automatically
determining swimmer stroke rates from overhead race video (ORV). General ORV is
defined as any footage of swimmers in competition, taken for the purposes of
viewing or analysis. Examples of this are footage from live streams,
broadcasts, or specialized camera equipment, with or without camera motion.
These are the most typical forms of swimming competition footage. We detail how
to create a system that will automatically collect swimmer stroke rates in any
competition, given the video of the competition of interest. With this
information, better systems can be created and additions to our analytics
system can be proposed to automatically extract other swimming metrics of
interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Woinoski_T/0/1/0/all/0/1"&gt;Timothy Woinoski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1"&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pyramid Fusion Dark Channel Prior for Single Image Dehazing. (arXiv:2105.10192v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10192</id>
        <link href="http://arxiv.org/abs/2105.10192"/>
        <updated>2021-05-24T05:08:42.112Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose the pyramid fusion dark channel prior (PF-DCP) for
single image dehazing. Based on the well-known Dark Channel Prior (DCP), we
introduce an easy yet effective approach PF-DCP by employing the DCP algorithm
at a pyramid of multi-scale images to alleviate the problem of patch size
selection. In this case, we obtain the final transmission map by fusing
transmission maps at each level to recover a high-quality haze-free image.
Experiments on RESIDE SOTS show that PF-DCP not only outperforms the
traditional prior-based methods with a large margin but also achieves
comparable or even better results of state-of-art deep learning approaches.
Furthermore, the visual quality is also greatly improved with much fewer color
distortions and halo artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1"&gt;Qiyuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1"&gt;Bin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1"&gt;Chong-Wah Ngo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Convolutional Networks in Feature Space for Image Deblurring and Super-resolution. (arXiv:2105.10465v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10465</id>
        <link href="http://arxiv.org/abs/2105.10465"/>
        <updated>2021-05-24T05:08:42.089Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks (GCNs) have achieved great success in dealing
with data of non-Euclidean structures. Their success directly attributes to
fitting graph structures effectively to data such as in social media and
knowledge databases. For image processing applications, the use of graph
structures and GCNs have not been fully explored. In this paper, we propose a
novel encoder-decoder network with added graph convolutions by converting
feature maps to vertexes of a pre-generated graph to synthetically construct
graph-structured data. By doing this, we inexplicitly apply graph Laplacian
regularization to the feature maps, making them more structured. The
experiments show that it significantly boosts performance for image restoration
tasks, including deblurring and super-resolution. We believe it opens up
opportunities for GCN-based approaches in more applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Boyan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hujun Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Data Augmentation Approaches for NLP. (arXiv:2105.03075v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03075</id>
        <link href="http://arxiv.org/abs/2105.03075"/>
        <updated>2021-05-24T05:08:42.075Z</updated>
        <summary type="html"><![CDATA[Data augmentation has recently seen increased interest in NLP due to more
work in low-resource domains, new tasks, and the popularity of large-scale
neural networks that require large amounts of training data. Despite this
recent upsurge, this area is still relatively underexplored, perhaps due to the
challenges posed by the discrete nature of language data. In this paper, we
present a comprehensive and unifying survey of data augmentation for NLP by
summarizing the literature in a structured manner. We first introduce and
motivate data augmentation for NLP, and then discuss major methodologically
representative approaches. Next, we highlight techniques that are used for
popular NLP applications and tasks. We conclude by outlining current challenges
and directions for future research. Overall, our paper aims to clarify the
landscape of existing literature in data augmentation for NLP and motivate
additional work in this area. We also present a GitHub repository with a paper
list that will be continuously updated at
https://github.com/styfeng/DataAug4NLP]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Steven Y. Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1"&gt;Varun Gangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1"&gt;Teruko Mitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elliptical Ordinal Embedding. (arXiv:2105.10457v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10457</id>
        <link href="http://arxiv.org/abs/2105.10457"/>
        <updated>2021-05-24T05:08:42.060Z</updated>
        <summary type="html"><![CDATA[Ordinal embedding aims at finding a low dimensional representation of objects
from a set of constraints of the form "item $j$ is closer to item $i$ than item
$k$". Typically, each object is mapped onto a point vector in a low dimensional
metric space. We argue that mapping to a density instead of a point vector
provides some interesting advantages, including an inherent reflection of the
uncertainty about the representation itself and its relative location in the
space. Indeed, in this paper, we propose to embed each object as a Gaussian
distribution. We investigate the ability of these embeddings to capture the
underlying structure of the data while satisfying the constraints, and explore
properties of the representation. Experiments on synthetic and real-world
datasets showcase the advantages of our approach. In addition, we illustrate
the merit of modelling uncertainty, which enriches the visual perception of the
mapped objects in the space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diallo_A/0/1/0/all/0/1"&gt;A&amp;#xef;ssatou Diallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Analysis Of Protected Health Information Leakage In Deep-Learning Based De-Identification Algorithms. (arXiv:2101.12099v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12099</id>
        <link href="http://arxiv.org/abs/2101.12099"/>
        <updated>2021-05-24T05:08:42.050Z</updated>
        <summary type="html"><![CDATA[The increasing complexity of algorithms for analyzing medical data, including
de-identification tasks, raises the possibility that complex algorithms are
learning not just the general representation of the problem, but specifics of
given individuals within the data. Modern legal frameworks specifically
prohibit the intentional or accidental distribution of patient data, but have
not addressed this potential avenue for leakage of such protected health
information. Modern deep learning algorithms have the highest potential of such
leakage due to complexity of the models. Recent research in the field has
highlighted such issues in non-medical data, but all analysis is likely to be
data and algorithm specific. We, therefore, chose to analyze a state-of-the-art
free-text de-identification algorithm based on LSTM (Long Short-Term Memory)
and its potential in encoding any individual in the training set. Using the
i2b2 Challenge Data, we trained, then analyzed the model to assess whether the
output of the LSTM, before the compression layer of the classifier, could be
used to estimate the membership of the training data. Furthermore, we used
different attacks including membership inference attack method to attack the
model. Results indicate that the attacks could not identify whether members of
the training data were distinguishable from non-members based on the model
output. This indicates that the model does not provide any strong evidence into
the identification of the individuals in the training data set and there is not
yet empirical evidence it is unsafe to distribute the model for general use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seyedi_S/0/1/0/all/0/1"&gt;Salman Seyedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nemati_S/0/1/0/all/0/1"&gt;Shamim Nemati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1"&gt;Gari D. Clifford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Model Distillation with Noise-Free Differential Privacy. (arXiv:2009.05537v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05537</id>
        <link href="http://arxiv.org/abs/2009.05537"/>
        <updated>2021-05-24T05:08:42.031Z</updated>
        <summary type="html"><![CDATA[Conventional federated learning directly averages model weights, which is
only possible for collaboration between models with homogeneous architectures.
Sharing prediction instead of weight removes this obstacle and eliminates the
risk of white-box inference attacks in conventional federated learning.
However, the predictions from local models are sensitive and would leak
training data privacy to the public. To address this issue, one naive approach
is adding the differentially private random noise to the predictions, which
however brings a substantial trade-off between privacy budget and model
performance. In this paper, we propose a novel framework called FEDMD-NFDP,
which applies a Noise-Free Differential Privacy (NFDP) mechanism into a
federated model distillation framework. Our extensive experimental results on
various datasets validate that FEDMD-NFDP can deliver not only comparable
utility and communication efficiency but also provide a noise-free differential
privacy guarantee. We also demonstrate the feasibility of our FEDMD-NFDP by
considering both IID and non-IID setting, heterogeneous model architectures,
and unlabelled public datasets from a different distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lingjuan Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Backdoor Attacks on Self-Supervised Learning. (arXiv:2105.10123v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10123</id>
        <link href="http://arxiv.org/abs/2105.10123"/>
        <updated>2021-05-24T05:08:42.024Z</updated>
        <summary type="html"><![CDATA[Large-scale unlabeled data has allowed recent progress in self-supervised
learning methods that learn rich visual representations. State-of-the-art
self-supervised methods for learning representations from images (MoCo and
BYOL) use an inductive bias that different augmentations (e.g. random crops) of
an image should produce similar embeddings. We show that such methods are
vulnerable to backdoor attacks where an attacker poisons a part of the
unlabeled data by adding a small trigger (known to the attacker) to the images.
The model performance is good on clean test images but the attacker can
manipulate the decision of the model by showing the trigger at test time.
Backdoor attacks have been studied extensively in supervised learning and to
the best of our knowledge, we are the first to study them for self-supervised
learning. Backdoor attacks are more practical in self-supervised learning since
the unlabeled data is large and as a result, an inspection of the data to avoid
the presence of poisoned data is prohibitive. We show that in our targeted
attack, the attacker can produce many false positives for the target category
by using the trigger at test time. We also propose a knowledge distillation
based defense algorithm that succeeds in neutralizing the attack. Our code is
available here: https://github.com/UMBCvision/SSL-Backdoor .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1"&gt;Aniruddha Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1"&gt;Ajinkya Tejankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1"&gt;Soroush Abbasi Koohpayegani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1"&gt;Hamed Pirsiavash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A parallel-network continuous quantitative trading model with GARCH and PPO. (arXiv:2105.03625v2 [q-fin.TR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03625</id>
        <link href="http://arxiv.org/abs/2105.03625"/>
        <updated>2021-05-24T05:08:42.019Z</updated>
        <summary type="html"><![CDATA[It is a difficult task for both professional investors and individual traders
continuously making profit in stock market. With the development of computer
science and deep reinforcement learning, Buy\&Hold (B\&H) has been oversteped
by many artificial intelligence trading algorithms. However, the information
and process are not enough, which limit the performance of reinforcement
learning algorithms. Thus, we propose a parallel-network continuous
quantitative trading model with GARCH and PPO to enrich the basical deep
reinforcement learning model, where the deep learning parallel network layers
deal with 3 different frequencies data (including GARCH information) and
proximal policy optimization (PPO) algorithm interacts actions and rewards with
stock trading environment. Experiments in 5 stocks from Chinese stock market
show our method achieves more extra profit comparing with basical reinforcement
learning methods and bench models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhishun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaixin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zixi Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo Pixel-level Labeling for Images with Evolving Content. (arXiv:2105.09975v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09975</id>
        <link href="http://arxiv.org/abs/2105.09975"/>
        <updated>2021-05-24T05:08:42.013Z</updated>
        <summary type="html"><![CDATA[Annotating images for semantic segmentation requires intense manual labor and
is a time-consuming and expensive task especially for domains with a scarcity
of experts, such as Forensic Anthropology. We leverage the evolving nature of
images depicting the decay process in human decomposition data to design a
simple yet effective pseudo-pixel-level label generation technique to reduce
the amount of effort for manual annotation of such images. We first identify
sequences of images with a minimum variation that are most suitable to share
the same or similar annotation using an unsupervised approach. Given one
user-annotated image in each sequence, we propagate the annotation to the
remaining images in the sequence by merging it with annotations produced by a
state-of-the-art CAM-based pseudo label generation technique. To evaluate the
quality of our pseudo-pixel-level labels, we train two semantic segmentation
models with VGG and ResNet backbones on images labeled using our pseudo
labeling method and those of a state-of-the-art method. The results indicate
that using our pseudo-labels instead of those generated using the
state-of-the-art method in the training process improves the mean-IoU and the
frequency-weighted-IoU of the VGG and ResNet-based semantic segmentation models
by 3.36%, 2.58%, 10.39%, and 12.91% respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1"&gt;Sara Mousavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhenning Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cross_K/0/1/0/all/0/1"&gt;Kelley Cross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steadman_D/0/1/0/all/0/1"&gt;Dawnie Steadman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mockus_A/0/1/0/all/0/1"&gt;Audris Mockus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Filters in Graph Convolutional Neural Networks. (arXiv:2105.10377v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10377</id>
        <link href="http://arxiv.org/abs/2105.10377"/>
        <updated>2021-05-24T05:08:42.006Z</updated>
        <summary type="html"><![CDATA[Over the last few years, we have seen increasing data generated from
non-Euclidean domains, which are usually represented as graphs with complex
relationships, and Graph Neural Networks (GNN) have gained a high interest
because of their potential in processing graph-structured data. In particular,
there is a strong interest in exploring the possibilities in performing
convolution on graphs using an extension of the GNN architecture, generally
referred to as Graph Convolutional Neural Networks (GCNN). Convolution on
graphs has been achieved mainly in two forms: spectral and spatial
convolutions. Due to the higher flexibility in exploring and exploiting the
graph structure of data, recently, there is an increasing interest in
investigating the possibilities that the spatial approach can offer. The idea
of finding a way to adapt the network behaviour to the inputs they process to
maximize the total performances has aroused much interest in the neural
networks literature over the years. This paper presents a novel method to adapt
the behaviour of a GCNN to the input proposing two ways to perform spatial
convolution on graphs using input-based filters which are dynamically
generated. Our model also investigates the problem of discovering and refining
relations among nodes. The experimental assessment confirms the capabilities of
the proposed approach, which achieves satisfying results using simple
architectures with a low number of filters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1"&gt;Andrea Apicella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1"&gt;Francesco Isgr&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollastro_A/0/1/0/all/0/1"&gt;Andrea Pollastro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1"&gt;Roberto Prevete&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Optical physics inspired CNN approach for intrinsic image decomposition. (arXiv:2105.10076v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10076</id>
        <link href="http://arxiv.org/abs/2105.10076"/>
        <updated>2021-05-24T05:08:42.000Z</updated>
        <summary type="html"><![CDATA[Intrinsic Image Decomposition is an open problem of generating the
constituents of an image. Generating reflectance and shading from a single
image is a challenging task specifically when there is no ground truth. There
is a lack of unsupervised learning approaches for decomposing an image into
reflectance and shading using a single image. We propose a neural network
architecture capable of this decomposition using physics-based parameters
derived from the image. Through experimental results, we show that (a) the
proposed methodology outperforms the existing deep learning-based IID
techniques and (b) the derived parameters improve the efficacy significantly.
We conclude with a closer analysis of the results (numerical and example
images) showing several avenues for improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weligampola_H/0/1/0/all/0/1"&gt;Harshana Weligampola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayatilaka_G/0/1/0/all/0/1"&gt;Gihan Jayatilaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sritharan_S/0/1/0/all/0/1"&gt;Suren Sritharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekanayake_P/0/1/0/all/0/1"&gt;Parakrama Ekanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragel_R/0/1/0/all/0/1"&gt;Roshan Ragel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herath_V/0/1/0/all/0/1"&gt;Vijitha Herath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Godaliyadda_R/0/1/0/all/0/1"&gt;Roshan Godaliyadda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09996</id>
        <link href="http://arxiv.org/abs/2105.09996"/>
        <updated>2021-05-24T05:08:41.986Z</updated>
        <summary type="html"><![CDATA[We present a simplified, task-agnostic multi-modal pre-training approach that
can accept either video or text input, or both for a variety of end tasks.
Existing pre-training are task-specific by adopting either a single cross-modal
encoder that requires both modalities, limiting their use for retrieval-style
end tasks or more complex multitask learning with two unimodal encoders,
limiting early cross-modal fusion. We instead introduce new pretraining masking
schemes that better mix across modalities (e.g. by forcing masks for text to
predict the closest video embeddings) while also maintaining separability (e.g.
unimodal predictions are sometimes required, without using all the input).
Experimental results show strong performance across a wider range of tasks than
any previous methods, often outperforming task-specific pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1"&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Po-Yao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1"&gt;Prahal Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1"&gt;Masoumeh Aminzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1"&gt;Christoph Feichtenhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1"&gt;Florian Metze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers. (arXiv:2105.08059v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08059</id>
        <link href="http://arxiv.org/abs/2105.08059"/>
        <updated>2021-05-24T05:08:41.964Z</updated>
        <summary type="html"><![CDATA[Supervised deep learning has swiftly become a workhorse for accelerated MRI
in recent years, offering state-of-the-art performance in image reconstruction
from undersampled acquisitions. Training deep supervised models requires large
datasets of undersampled and fully-sampled acquisitions typically from a
matching set of subjects. Given scarce access to large medical datasets, this
limitation has sparked interest in unsupervised methods that reduce reliance on
fully-sampled ground-truth data. A common framework is based on the deep image
prior, where network-driven regularization is enforced directly during
inference on undersampled acquisitions. Yet, canonical convolutional
architectures are suboptimal in capturing long-range relationships, and
randomly initialized networks may hamper convergence. To address these
limitations, here we introduce a novel unsupervised MRI reconstruction method
based on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a
deep adversarial network with cross-attention transformer blocks to map noise
and latent variables onto MR images. This unconditional network learns a
high-quality MRI prior in a self-supervised encoding task. A zero-shot
reconstruction is performed on undersampled test data, where inference is
performed by optimizing network parameters, latent and noise variables to
ensure maximal consistency to multi-coil MRI data. Comprehensive experiments on
brain MRI datasets clearly demonstrate the superior performance of SLATER
against several state-of-the-art unsupervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1"&gt;Yilmaz Korkmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1"&gt;Salman UH Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1"&gt;Mahmut Yurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1"&gt;Muzaffer &amp;#xd6;zbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1"&gt;Tolga &amp;#xc7;ukur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Implicit CSI Feedback in Massive MIMO. (arXiv:2105.10100v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2105.10100</id>
        <link href="http://arxiv.org/abs/2105.10100"/>
        <updated>2021-05-24T05:08:41.945Z</updated>
        <summary type="html"><![CDATA[Massive multiple-input multiple-output can obtain more performance gain by
exploiting the downlink channel state information (CSI) at the base station
(BS). Therefore, studying CSI feedback with limited communication resources in
frequency-division duplexing systems is of great importance. Recently, deep
learning (DL)-based CSI feedback has shown considerable potential. However, the
existing DL-based explicit feedback schemes are difficult to deploy because
current fifth-generation mobile communication protocols and systems are
designed based on an implicit feedback mechanism. In this paper, we propose a
DL-based implicit feedback architecture to inherit the low-overhead
characteristic, which uses neural networks (NNs) to replace the precoding
matrix indicator (PMI) encoding and decoding modules. By using environment
information, the NNs can achieve a more refined mapping between the precoding
matrix and the PMI compared with codebooks. The correlation between subbands is
also used to further improve the feedback performance. Simulation results show
that, for a single resource block (RB), the proposed architecture can save
25.0% and 40.0% of overhead compared with Type I codebook under two antenna
configurations, respectively. For a wideband system with 52 RBs, overhead can
be saved by 30.7% and 48.0% compared with Type II codebook when ignoring and
considering extracting subband correlation, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muhan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiajia Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_C/0/1/0/all/0/1"&gt;Chao-Kai Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jin_S/0/1/0/all/0/1"&gt;Shi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1"&gt;Geoffrey Ye Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_A/0/1/0/all/0/1"&gt;Ang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Uniform OPE and Model-based Offline Reinforcement Learning in Time-Homogeneous, Reward-Free and Task-Agnostic Settings. (arXiv:2105.06029v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06029</id>
        <link href="http://arxiv.org/abs/2105.06029"/>
        <updated>2021-05-24T05:08:41.938Z</updated>
        <summary type="html"><![CDATA[This work studies the statistical limits of uniform convergence for offline
policy evaluation (OPE) problems with model-based methods (for finite horizon
MDP) and provides a unified view towards optimal learning for several
well-motivated offline tasks. Uniform OPE
$\sup_\Pi|Q^\pi-\hat{Q}^\pi|<\epsilon$ (initiated by \citet{yin2021near}) is a
stronger measure than the point-wise (fixed policy) OPE and ensures offline
policy learning when $\Pi$ contains all policies (global policy class). In this
paper, we establish an $\Omega(H^2 S/d_m\epsilon^2)$ lower bound (over
model-based family) for the global uniform OPE, where $d_m$ is the minimal
state-action probability induced by the behavior policy. Next, our main result
establishes an episode complexity of $\tilde{O}(H^2/d_m\epsilon^2)$ for
\emph{local} uniform convergence that applies to all \emph{near-empirically
optimal} policies for the MDPs with \emph{stationary} transition. This result
implies the optimal sample complexity for offline learning and separates the
local uniform OPE from the global case due to the extra $S$ factor.
Paramountly, the model-based method combining with our new analysis technique
(singleton absorbing MDP) can be adapted to the new settings: offline
task-agnostic and the offline reward-free with optimal complexity
$\tilde{O}(H^2\log(K)/d_m\epsilon^2)$ ($K$ is the number of tasks) and
$\tilde{O}(H^2S/d_m\epsilon^2)$ respectively, which provides a unified
framework for simultaneously solving different offline RL problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Ming Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. (arXiv:2010.14925v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14925</id>
        <link href="http://arxiv.org/abs/2010.14925"/>
        <updated>2021-05-24T05:08:41.932Z</updated>
        <summary type="html"><![CDATA[We present MedMNIST, a collection of 10 pre-processed medical open datasets.
MedMNIST is standardized to perform classification tasks on lightweight 28x28
images, which requires no background knowledge. Covering the primary data
modalities in medical image analysis, it is diverse on data scale (from 100 to
100,000) and tasks (binary/multi-class, ordinal regression and multi-label).
MedMNIST could be used for educational purpose, rapid prototyping, multi-modal
machine learning or AutoML in medical image analysis. Moreover, MedMNIST
Classification Decathlon is designed to benchmark AutoML algorithms on all 10
datasets; We have compared several baseline methods, including open-source or
commercial AutoML tools. The datasets, evaluation code and baseline methods for
MedMNIST are publicly available at https://medmnist.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiancheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1"&gt;Rui Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments. (arXiv:2012.10315v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10315</id>
        <link href="http://arxiv.org/abs/2012.10315"/>
        <updated>2021-05-24T05:08:41.925Z</updated>
        <summary type="html"><![CDATA[Negative control is a strategy for learning the causal relationship between
treatment and outcome in the presence of unmeasured confounding. The treatment
effect can nonetheless be identified if two auxiliary variables are available:
a negative control treatment (which has no effect on the actual outcome), and a
negative control outcome (which is not affected by the actual treatment). These
auxiliary variables can also be viewed as proxies for a traditional set of
control variables, and they bear resemblance to instrumental variables. I
propose a family of algorithms based on kernel ridge regression for learning
nonparametric treatment effects with negative controls. Examples include dose
response curves, dose response curves with distribution shift, and
heterogeneous treatment effects. Data may be discrete or continuous, and low,
high, or infinite dimensional. I prove uniform consistency and provide finite
sample rates of convergence. I estimate the dose response curve of cigarette
smoking on infant birth weight adjusting for unobserved confounding due to
household income, using a data set of singleton births in the state of
Pennsylvania between 1989 and 1991.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rahul Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning in EEG: Advance of the Last Ten-Year Critical Period. (arXiv:2011.11128v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11128</id>
        <link href="http://arxiv.org/abs/2011.11128"/>
        <updated>2021-05-24T05:08:41.918Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved excellent performance in a wide range of domains,
especially in speech recognition and computer vision. Relatively less work has
been done for EEG, but there is still significant progress attained in the last
decade. Due to the lack of a comprehensive and topic widely covered survey for
deep learning in EEG, we attempt to summarize recent progress to provide an
overview, as well as perspectives for future developments. We first briefly
mention the artifacts removal for EEG signal and then introduce deep learning
models that have been utilized in EEG processing and classification.
Subsequently, the applications of deep learning in EEG are reviewed by
categorizing them into groups such as brain-computer interface, disease
detection, and emotion recognition. They are followed by the discussion, in
which the pros and cons of deep learning are presented and future directions
and challenges for deep learning in EEG are proposed. We hope that this paper
could serve as a summary of past work for deep learning in EEG and the
beginning of further developments and achievements of EEG studies based on deep
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gong_S/0/1/0/all/0/1"&gt;Shu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_K/0/1/0/all/0/1"&gt;Kaibo Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cichocki_A/0/1/0/all/0/1"&gt;Andrzej Cichocki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Junhua Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Universal NLG for Dialogue Systems and Simulators with Future Bridging. (arXiv:2105.10267v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10267</id>
        <link href="http://arxiv.org/abs/2105.10267"/>
        <updated>2021-05-24T05:08:41.900Z</updated>
        <summary type="html"><![CDATA[In a dialogue system pipeline, a natural language generation (NLG) unit
converts the dialogue direction and content to a corresponding natural language
realization. A recent trend for dialogue systems is to first pre-train on large
datasets and then fine-tune in a supervised manner using datasets annotated
with application-specific features. Though novel behaviours can be learned from
custom annotation, the required effort severely bounds the quantity of the
training set, and the application-specific nature limits the reuse. In light of
the recent success of data-driven approaches, we propose the novel future
bridging NLG (FBNLG) concept for dialogue systems and simulators. The critical
step is for an FBNLG to accept a future user or system utterance to bridge the
present context towards. Future bridging enables self supervised training over
annotation-free datasets, decoupled the training of NLG from the rest of the
system. An FBNLG, pre-trained with massive datasets, is expected to apply in
classical or new dialogue scenarios with minimal adaptation effort. We evaluate
a prototype FBNLG to show that future bridging can be a viable approach to a
universal few-shot NLG for task-oriented and chit-chat dialogues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ennen_P/0/1/0/all/0/1"&gt;Philipp Ennen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yen-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozbay_A/0/1/0/all/0/1"&gt;Ali Girayhan Ozbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Insalata_F/0/1/0/all/0/1"&gt;Ferdinando Insalata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Ye Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalali_S/0/1/0/all/0/1"&gt;Sepehr Jalali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Algorithms for k-means Clustering. (arXiv:2008.00358v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00358</id>
        <link href="http://arxiv.org/abs/2008.00358"/>
        <updated>2021-05-24T05:08:41.893Z</updated>
        <summary type="html"><![CDATA[This paper gives a k-means approximation algorithm that is efficient in the
relational algorithms model. This is an algorithm that operates directly on a
relational database without performing a join to convert it to a matrix whose
rows represent the data points. The running time is potentially exponentially
smaller than $N$, the number of data points to be clustered that the relational
database represents.

Few relational algorithms are known and this paper offers techniques for
designing relational algorithms as well as characterizing their limitations. We
show that given two data points as cluster centers, if we cluster points
according to their closest centers, it is NP-Hard to approximate the number of
points in the clusters on a general relational input. This is trivial for
conventional data inputs and this result exemplifies that standard algorithmic
techniques may not be directly applied when designing an efficient relational
algorithm. This paper then introduces a new method that leverages rejection
sampling and the $k$-means++ algorithm to construct an O(1)-approximate k-means
solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1"&gt;Benjamin Moseley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruhs_K/0/1/0/all/0/1"&gt;Kirk Pruhs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samadian_A/0/1/0/all/0/1"&gt;Alireza Samadian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuyan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Reconstruction of Transparent Objects by Altering Incident Light Paths Through Refraction. (arXiv:2105.09993v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09993</id>
        <link href="http://arxiv.org/abs/2105.09993"/>
        <updated>2021-05-24T05:08:41.887Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of reconstructing the surface shape of
transparent objects. The difficulty of this problem originates from the
viewpoint dependent appearance of a transparent object, which quickly makes
reconstruction methods tailored for diffuse surfaces fail disgracefully. In
this paper, we introduce a fixed viewpoint approach to dense surface
reconstruction of transparent objects based on refraction of light. We present
a simple setup that allows us to alter the incident light paths before light
rays enter the object by immersing the object partially in a liquid, and
develop a method for recovering the object surface through reconstructing and
triangulating such incident light paths. Our proposed approach does not need to
model the complex interactions of light as it travels through the object,
neither does it assume any parametric form for the object shape nor the exact
number of refractions and reflections taken place along the light paths. It can
therefore handle transparent objects with a relatively complex shape and
structure, with unknown and inhomogeneous refractive index. We also show that
for thin transparent objects, our proposed acquisition setup can be further
simplified by adopting a single refraction approximation. Experimental results
on both synthetic and real data demonstrate the feasibility and accuracy of our
proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wong_K/0/1/0/all/0/1"&gt;Kwan-Yee K. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1"&gt;Miaomiao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-05-24T05:08:41.880Z</updated>
        <summary type="html"><![CDATA[Camera movement and unpredictable environmental conditions like dust and wind
induce noise into video feeds. We observe that popular unsupervised MOT methods
are dependent on noise-free conditions. We show that the addition of a small
amount of artificial random noise causes a sharp degradation in model
performance on benchmark metrics. We resolve this problem by introducing a
robust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed
single-head attention model helps limit the negative impact of noise by
learning visual representations at different segment scales. AttU-Net shows
better unsupervised MOT tracking performance over variational inference-based
state-of-the-art baselines. We evaluate our method in the MNIST and the Atari
game video benchmark. We also provide two extended video datasets consisting of
complex visual patterns that include Kuzushiji characters and fashion images to
validate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakam_T/0/1/0/all/0/1"&gt;Tomokazu Murakam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10013</id>
        <link href="http://arxiv.org/abs/2105.10013"/>
        <updated>2021-05-24T05:08:41.872Z</updated>
        <summary type="html"><![CDATA[Image classification methods are usually trained to perform predictions
taking into account a predefined group of known classes. Real-world problems,
however, may not allow for a full knowledge of the input and label spaces,
making failures in recognition a hazard to deep visual learning. Open set
recognition methods are characterized by the ability to correctly identifying
inputs of known and unknown classes. In this context, we propose GeMOS: simple
and plug-and-play open set recognition modules that can be attached to
pretrained Deep Neural Networks for visual recognition. The GeMOS framework
pairs pre-trained Convolutional Neural Networks with generative models for open
set recognition to extract open set scores for each sample, allowing for
failure recognition in object recognition tasks. We conduct a thorough
evaluation of the proposed method in comparison with state-of-the-art open set
algorithms, finding that GeMOS either outperforms or is statistically
indistinguishable from more complex and costly models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1"&gt;Marcos Vendramini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1"&gt;Alexei Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficiency-boosting Client Selection Scheme for Federated Learning with Fairness Guarantee. (arXiv:2011.01783v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01783</id>
        <link href="http://arxiv.org/abs/2011.01783"/>
        <updated>2021-05-24T05:08:41.866Z</updated>
        <summary type="html"><![CDATA[The issue of potential privacy leakage during centralized AI's model training
has drawn intensive concern from the public. A Parallel and Distributed
Computing (or PDC) scheme, termed Federated Learning (FL), has emerged as a new
paradigm to cope with the privacy issue by allowing clients to perform model
training locally, without the necessity to upload their personal sensitive
data. In FL, the number of clients could be sufficiently large, but the
bandwidth available for model distribution and re-upload is quite limited,
making it sensible to only involve part of the volunteers to participate in the
training process. The client selection policy is critical to an FL process in
terms of training efficiency, the final model's quality as well as fairness. In
this paper, we will model the fairness guaranteed client selection as a
Lyapunov optimization problem and then a C2MAB-based method is proposed for
estimation of the model exchange time between each client and the server, based
on which we design a fairness guaranteed algorithm termed RBCS-F for
problem-solving. The regret of RBCS-F is strictly bounded by a finite constant,
justifying its theoretical feasibility. Barring the theoretical results, more
empirical data can be derived from our real training experiments on public
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiansheng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiwei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wentai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Ligang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Keqin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zomaya_A/0/1/0/all/0/1"&gt;Albert Y.Zomaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Disentangled Representations for Time Series. (arXiv:2105.08179v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08179</id>
        <link href="http://arxiv.org/abs/2105.08179"/>
        <updated>2021-05-24T05:08:41.847Z</updated>
        <summary type="html"><![CDATA[Time-series representation learning is a fundamental task for time-series
analysis. While significant progress has been made to achieve accurate
representations for downstream applications, the learned representations often
lack interpretability and do not expose semantic meanings. Different from
previous efforts on the entangled feature space, we aim to extract the
semantic-rich temporal correlations in the latent interpretable factorized
representation of the data. Motivated by the success of disentangled
representation learning in computer vision, we study the possibility of
learning semantic-rich time-series representations, which remains unexplored
due to three main challenges: 1) sequential data structure introduces complex
temporal correlations and makes the latent representations hard to interpret,
2) sequential models suffer from KL vanishing problem, and 3) interpretable
semantic concepts for time-series often rely on multiple factors instead of
individuals. To bridge the gap, we propose Disentangle Time Series (DTS), a
novel disentanglement enhancement framework for sequential data. Specifically,
to generate hierarchical semantic concepts as the interpretable and
disentangled representation of time-series, DTS introduces multi-level
disentanglement strategies by covering both individual latent factors and group
semantic segments. We further theoretically show how to alleviate the KL
vanishing problem: DTS introduces a mutual information maximization term, while
preserving a heavier penalty on the total correlation and the dimension-wise KL
to keep the disentanglement property. Experimental results on various
real-world benchmark datasets demonstrate that the representations learned by
DTS achieve superior performance in downstream applications, with high
interpretability of semantic concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuening Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhengzhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1"&gt;Daochen Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1"&gt;Mengnan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Denghui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LAPAR: Linearly-Assembled Pixel-Adaptive Regression Network for Single Image Super-Resolution and Beyond. (arXiv:2105.10422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10422</id>
        <link href="http://arxiv.org/abs/2105.10422"/>
        <updated>2021-05-24T05:08:41.840Z</updated>
        <summary type="html"><![CDATA[Single image super-resolution (SISR) deals with a fundamental problem of
upsampling a low-resolution (LR) image to its high-resolution (HR) version.
Last few years have witnessed impressive progress propelled by deep learning
methods. However, one critical challenge faced by existing methods is to strike
a sweet spot of deep model complexity and resulting SISR quality. This paper
addresses this pain point by proposing a linearly-assembled pixel-adaptive
regression network (LAPAR), which casts the direct LR to HR mapping learning
into a linear coefficient regression task over a dictionary of multiple
predefined filter bases. Such a parametric representation renders our model
highly lightweight and easy to optimize while achieving state-of-the-art
results on SISR benchmarks. Moreover, based on the same idea, LAPAR is extended
to tackle other restoration tasks, e.g., image denoising and JPEG image
deblocking, and again, yields strong performance. The code is available at
https://github.com/dvlab-research/Simple-SR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenbo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lu Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nianjuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiangbo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Properties of Vision Transformers. (arXiv:2105.10497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10497</id>
        <link href="http://arxiv.org/abs/2105.10497"/>
        <updated>2021-05-24T05:08:41.834Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViT) have demonstrated impressive performance across
various machine vision problems. These models are based on multi-head
self-attention mechanisms that can flexibly attend to a sequence of image
patches to encode contextual cues. An important question is how such
flexibility in attending image-wide context conditioned on a given patch can
facilitate handling nuisances in natural images e.g., severe occlusions, domain
shifts, spatial permutations, adversarial and natural perturbations. We
systematically study this question via an extensive set of experiments
encompassing three ViT families and comparisons with a high-performing
convolutional neural network (CNN). We show and analyze the following
intriguing properties of ViT: (a) Transformers are highly robust to severe
occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1
accuracy on ImageNet even after randomly occluding 80% of the image content.
(b) The robust performance to occlusions is not due to a bias towards local
textures, and ViTs are significantly less biased towards textures compared to
CNNs. When properly trained to encode shape-based features, ViTs demonstrate
shape recognition capability comparable to that of human visual system,
previously unmatched in the literature. (c) Using ViTs to encode shape
representation leads to an interesting consequence of accurate semantic
segmentation without pixel-level supervision. (d) Off-the-shelf features from a
single ViT model can be combined to create a feature ensemble, leading to high
accuracy rates across a range of classification datasets in both traditional
and few-shot learning paradigms. We show effective features of ViTs are due to
flexible and dynamic receptive fields possible via the self-attention
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1"&gt;Munawar Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Recursive Markov Boundary-Based Approach to Causal Structure Learning. (arXiv:2010.04992v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04992</id>
        <link href="http://arxiv.org/abs/2010.04992"/>
        <updated>2021-05-24T05:08:41.827Z</updated>
        <summary type="html"><![CDATA[Constraint-based methods are one of the main approaches for causal structure
learning that are particularly valued as they are asymptotically guaranteed to
find a structure that is Markov equivalent to the causal graph of the system.
On the other hand, they may require an exponentially large number of
conditional independence (CI) tests in the number of variables of the system.
In this paper, we propose a novel recursive constraint-based method for causal
structure learning that significantly reduces the required number of CI tests
compared to the existing literature. The idea of the proposed approach is to
use Markov boundary information to identify a specific variable that can be
removed from the set of variables without affecting the statistical
dependencies among the other variables. Having identified such a variable, we
discover its neighborhood, remove that variable from the set of variables, and
recursively learn the causal structure over the remaining variables. We further
provide a lower bound on the number of CI tests required by any
constraint-based method. Comparing this lower bound to our achievable bound
demonstrates the efficiency of the proposed approach. Our experimental results
show that the proposed algorithm outperforms state-of-the-art both on synthetic
and real-world structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mokhtarian_E/0/1/0/all/0/1"&gt;Ehsan Mokhtarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbari_S/0/1/0/all/0/1"&gt;Sina Akbari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1"&gt;AmirEmad Ghassami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1"&gt;Negar Kiyavash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. (arXiv:2006.03659v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03659</id>
        <link href="http://arxiv.org/abs/2006.03659"/>
        <updated>2021-05-24T05:08:41.820Z</updated>
        <summary type="html"><![CDATA[Sentence embeddings are an important component of many natural language
processing (NLP) systems. Like word embeddings, sentence embeddings are
typically learned on large text corpora and then transferred to various
downstream tasks, such as clustering and retrieval. Unlike word embeddings, the
highest performing solutions for learning sentence embeddings require labelled
data, limiting their usefulness to languages and domains where labelled data is
abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for
Unsupervised Textual Representations. Inspired by recent advances in deep
metric learning (DML), we carefully design a self-supervised objective for
learning universal sentence embeddings that does not require labelled training
data. When used to extend the pretraining of transformer-based language models,
our approach closes the performance gap between unsupervised and supervised
pretraining for universal sentence encoders. Importantly, our experiments
suggest that the quality of the learned embeddings scale with both the number
of trainable parameters and the amount of unlabelled training data, making
further improvements straightforward. Our code and pretrained models are
publicly available and can be easily adapted to new domains or used to embed
unseen text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1"&gt;John Giorgi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nitski_O/0/1/0/all/0/1"&gt;Osvald Nitski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bader_G/0/1/0/all/0/1"&gt;Gary Bader&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Robustness over High Level Driving Instruction for Autonomous Driving. (arXiv:2105.10014v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10014</id>
        <link href="http://arxiv.org/abs/2105.10014"/>
        <updated>2021-05-24T05:08:41.803Z</updated>
        <summary type="html"><![CDATA[In recent years, we have witnessed increasingly high performance in the field
of autonomous end-to-end driving. In particular, more and more research is
being done on driving in urban environments, where the car has to follow high
level commands to navigate. However, few evaluations are made on the ability of
these agents to react in an unexpected situation. Specifically, no evaluations
are conducted on the robustness of driving agents in the event of a bad
high-level command. We propose here an evaluation method, namely a benchmark
that allows to assess the robustness of an agent, and to appreciate its
understanding of the environment through its ability to keep a safe behavior,
regardless of the instruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carton_F/0/1/0/all/0/1"&gt;Florence Carton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1"&gt;David Filliat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabarisoa_J/0/1/0/all/0/1"&gt;Jaonary Rabarisoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1"&gt;Quoc Cuong Pham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel 3D-UNet Deep Learning Framework Based on High-Dimensional Bilateral Grid for Edge Consistent Single Image Depth Estimation. (arXiv:2105.10129v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10129</id>
        <link href="http://arxiv.org/abs/2105.10129"/>
        <updated>2021-05-24T05:08:41.797Z</updated>
        <summary type="html"><![CDATA[The task of predicting smooth and edge-consistent depth maps is notoriously
difficult for single image depth estimation. This paper proposes a novel
Bilateral Grid based 3D convolutional neural network, dubbed as 3DBG-UNet, that
parameterizes high dimensional feature space by encoding compact 3D bilateral
grids with UNets and infers sharp geometric layout of the scene. Further,
another novel 3DBGES-UNet model is introduced that integrate 3DBG-UNet for
inferring an accurate depth map given a single color view. The 3DBGES-UNet
concatenates 3DBG-UNet geometry map with the inception network edge
accentuation map and a spatial object's boundary map obtained by leveraging
semantic segmentation and train the UNet model with ResNet backbone. Both
models are designed with a particular attention to explicitly account for edges
or minute details. Preserving sharp discontinuities at depth edges is critical
for many applications such as realistic integration of virtual objects in AR
video or occlusion-aware view synthesis for 3D display applications.The
proposed depth prediction network achieves state-of-the-art performance in both
qualitative and quantitative evaluations on the challenging NYUv2-Depth data.
The code and corresponding pre-trained weights will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1"&gt;Mansi Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_K/0/1/0/all/0/1"&gt;Kadvekar Rohit Tushar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panneer_A/0/1/0/all/0/1"&gt;Avinash Panneer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Generation and Evaluation of Visual Stories via Semantic Consistency. (arXiv:2105.10026v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10026</id>
        <link href="http://arxiv.org/abs/2105.10026"/>
        <updated>2021-05-24T05:08:41.790Z</updated>
        <summary type="html"><![CDATA[Story visualization is an under-explored task that falls at the intersection
of many important research directions in both computer vision and natural
language processing. In this task, given a series of natural language captions
which compose a story, an agent must generate a sequence of images that
correspond to the captions. Prior work has introduced recurrent generative
models which outperform text-to-image synthesis models on this task. However,
there is room for improvement of generated images in terms of visual quality,
coherence and relevance. We present a number of improvements to prior modeling
approaches, including (1) the addition of a dual learning framework that
utilizes video captioning to reinforce the semantic alignment between the story
and generated images, (2) a copy-transform mechanism for
sequentially-consistent story visualization, and (3) MART-based transformers to
model complex interactions between frames. We present ablation studies to
demonstrate the effect of each of these techniques on the generative power of
the model for both individual images as well as the entire narrative.
Furthermore, due to the complexity and generative nature of the task, standard
evaluation metrics do not accurately reflect performance. Therefore, we also
provide an exploration of evaluation metrics for the model, focused on aspects
of the generated frames such as the presence/quality of generated characters,
the relevance to captions, and the diversity of the generated images. We also
present correlation experiments of our proposed automated metrics with human
evaluations. Code and data available at:
https://github.com/adymaharana/StoryViz]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1"&gt;Adyasha Maharana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hannan_D/0/1/0/all/0/1"&gt;Darryl Hannan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An interpretable object detection based model for the diagnosis of neonatal lung diseases using Ultrasound images. (arXiv:2105.10081v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10081</id>
        <link href="http://arxiv.org/abs/2105.10081"/>
        <updated>2021-05-24T05:08:41.783Z</updated>
        <summary type="html"><![CDATA[Over the last few decades, Lung Ultrasound (LUS) has been increasingly used
to diagnose and monitor different lung diseases in neonates. It is a non
invasive tool that allows a fast bedside examination while minimally handling
the neonate. Acquiring a LUS scan is easy, but understanding the artifacts
concerned with each respiratory disease is challenging. Mixed artifact patterns
found in different respiratory diseases may limit LUS readability by the
operator. While machine learning (ML), especially deep learning can assist in
automated analysis, simply feeding the ultrasound images to an ML model for
diagnosis is not enough to earn the trust of medical professionals. The
algorithm should output LUS features that are familiar to the operator instead.
Therefore, in this paper we present a unique approach for extracting seven
meaningful LUS features that can be easily associated with a specific
pathological lung condition: Normal pleura, irregular pleura, thick pleura,
Alines, Coalescent B-lines, Separate B-lines and Consolidations. These
artifacts can lead to early prediction of infants developing later respiratory
distress symptoms. A single multi-class region proposal-based object detection
model faster-RCNN (fRCNN) was trained on lower posterior lung ultrasound videos
to detect these LUS features which are further linked to four common neonatal
diseases. Our results show that fRCNN surpasses single stage models such as
RetinaNet and can successfully detect the aforementioned LUS features with a
mean average precision of 86.4%. Instead of a fully automatic diagnosis from
images without any interpretability, detection of such LUS features leave the
ultimate control of diagnosis to the clinician, which can result in a more
trustworthy intelligent system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bassiouny_R/0/1/0/all/0/1"&gt;Rodina Bassiouny&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Adel Mohamed&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Umapathy_K/0/1/0/all/0/1"&gt;Karthi Umapathy&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naimul Khan&lt;/a&gt; (1) ((1) Ryerson University, Toronto, Canada, (2) Mount Sinai Hospital, University of Toronto, Toronto, Canada)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sheaves as a Framework for Understanding and Interpreting Model Fit. (arXiv:2105.10414v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10414</id>
        <link href="http://arxiv.org/abs/2105.10414"/>
        <updated>2021-05-24T05:08:41.687Z</updated>
        <summary type="html"><![CDATA[As data grows in size and complexity, finding frameworks which aid in
interpretation and analysis has become critical. This is particularly true when
data comes from complex systems where extensive structure is available, but
must be drawn from peripheral sources. In this paper we argue that in such
situations, sheaves can provide a natural framework to analyze how well a
statistical model fits at the local level (that is, on subsets of related
datapoints) vs the global level (on all the data). The sheaf-based approach
that we propose is suitably general enough to be useful in a range of
applications, from analyzing sensor networks to understanding the feature space
of a deep learning model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jefferson_B/0/1/0/all/0/1"&gt;Brett Jefferson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joslyn_C/0/1/0/all/0/1"&gt;Cliff Joslyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purvine_E/0/1/0/all/0/1"&gt;Emilie Purvine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Performance of Knowledge Graph Embeddings in Drug Discovery. (arXiv:2105.10488v1 [q-bio.BM])]]></title>
        <id>http://arxiv.org/abs/2105.10488</id>
        <link href="http://arxiv.org/abs/2105.10488"/>
        <updated>2021-05-24T05:08:41.669Z</updated>
        <summary type="html"><![CDATA[Knowledge Graphs (KG) and associated Knowledge Graph Embedding (KGE) models
have recently begun to be explored in the context of drug discovery and have
the potential to assist in key challenges such as target identification. In the
drug discovery domain, KGs can be employed as part of a process which can
result in lab-based experiments being performed, or impact on other decisions,
incurring significant time and financial costs and most importantly, ultimately
influencing patient healthcare. For KGE models to have impact in this domain, a
better understanding of not only of performance, but also the various factors
which determine it, is required.

In this study we investigate, over the course of many thousands of
experiments, the predictive performance of five KGE models on two public drug
discovery-oriented KGs. Our goal is not to focus on the best overall model or
configuration, instead we take a deeper look at how performance can be affected
by changes in the training setup, choice of hyperparameters, model parameter
initialisation seed and different splits of the datasets. Our results highlight
that these factors have significant impact on performance and can even affect
the ranking of models. Indeed these factors should be reported along with model
architectures to ensure complete reproducibility and fair comparisons of future
work, and we argue this is critical for the acceptance of use, and impact of
KGEs in a biomedical setting. To aid reproducibility of our own work, we
release all experimentation code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1"&gt;Stephen Bonner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1"&gt;Ian P Barrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1"&gt;Cheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1"&gt;Rowan Swiers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Engkvist_O/0/1/0/all/0/1"&gt;Ola Engkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Knowledge Expansion. (arXiv:2103.14431v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14431</id>
        <link href="http://arxiv.org/abs/2103.14431"/>
        <updated>2021-05-24T05:08:41.629Z</updated>
        <summary type="html"><![CDATA[The popularity of multimodal sensors and the accessibility of the Internet
have brought us a massive amount of unlabeled multimodal data. Since existing
datasets and well-trained models are primarily unimodal, the modality gap
between a unimodal network and unlabeled multimodal data poses an interesting
problem: how to transfer a pre-trained unimodal network to perform the same
task on unlabeled multimodal data? In this work, we propose multimodal
knowledge expansion (MKE), a knowledge distillation-based framework to
effectively utilize multimodal data without requiring labels. Opposite to
traditional knowledge distillation, where the student is designed to be
lightweight and inferior to the teacher, we observe that a multimodal student
model consistently denoises pseudo labels and generalizes better than its
teacher. Extensive experiments on four tasks and different modalities verify
this finding. Furthermore, we connect the mechanism of MKE to semi-supervised
learning and offer both empirical and theoretical explanations to understand
the denoising capability of a multimodal student.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temp-Frustum Net: 3D Object Detection with Temporal Fusion. (arXiv:2104.12106v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12106</id>
        <link href="http://arxiv.org/abs/2104.12106"/>
        <updated>2021-05-24T05:08:41.622Z</updated>
        <summary type="html"><![CDATA[3D object detection is a core component of automated driving systems.
State-of-the-art methods fuse RGB imagery and LiDAR point cloud data
frame-by-frame for 3D bounding box regression. However, frame-by-frame 3D
object detection suffers from noise, field-of-view obstruction, and sparsity.
We propose a novel Temporal Fusion Module (TFM) to use information from
previous time-steps to mitigate these problems. First, a state-of-the-art
frustum network extracts point cloud features from raw RGB and LiDAR point
cloud data frame-by-frame. Then, our TFM module fuses these features with a
recurrent neural network. As a result, 3D object detection becomes robust
against single frame failures and transient occlusions. Experiments on the
KITTI object tracking dataset show the efficiency of the proposed TFM, where we
obtain ~6%, ~4%, and ~6% improvements on Car, Pedestrian, and Cyclist classes,
respectively, compared to frame-by-frame baselines. Furthermore, ablation
studies reinforce that the subject of improvement is temporal fusion and show
the effects of different placements of TFM in the object detection pipeline.
Our code is open-source and available at
https://github.com/emecercelik/Temp-Frustum-Net.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ercelik_E/0/1/0/all/0/1"&gt;Eme&amp;#xe7; Er&amp;#xe7;elik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yurtsever_E/0/1/0/all/0/1"&gt;Ekim Yurtsever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Triplet Autoencoder for Histopathological Colon Cancer Nuclei Retrieval. (arXiv:2105.10262v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10262</id>
        <link href="http://arxiv.org/abs/2105.10262"/>
        <updated>2021-05-24T05:08:41.616Z</updated>
        <summary type="html"><![CDATA[Deep learning has shown a great improvement in the performance of visual
tasks. Image retrieval is the task of extracting the visually similar images
from a database for a query image. The feature matching is performed to rank
the images. Various hand-designed features have been derived in past to
represent the images. Nowadays, the power of deep learning is being utilized
for automatic feature learning from data in the field of biomedical image
analysis. Autoencoder and Siamese networks are two deep learning models to
learn the latent space (i.e., features or embedding). Autoencoder works based
on the reconstruction of the image from latent space. Siamese network utilizes
the triplets to learn the intra-class similarity and inter-class dissimilarity.
Moreover, Autoencoder is unsupervised, whereas Siamese network is supervised.
We propose a Joint Triplet Autoencoder Network (JTANet) by facilitating the
triplet learning in autoencoder framework. A joint supervised learning for
Siamese network and unsupervised learning for Autoencoder is performed.
Moreover, the Encoder network of Autoencoder is shared with Siamese network and
referred as the Siamcoder network. The features are extracted by using the
trained Siamcoder network for retrieval purpose. The experiments are performed
over Histopathological Routine Colon Cancer dataset. We have observed the
promising performance using the proposed JTANet model against the Autoencoder
and Siamese models for colon cancer nuclei retrieval in histopathological
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satya Rajendra Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1"&gt;Shiv Ram Dubey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MS_S/0/1/0/all/0/1"&gt;Shruthi MS&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventrapragada_S/0/1/0/all/0/1"&gt;Sairathan Ventrapragada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasharatha_S/0/1/0/all/0/1"&gt;Saivamshi Salla Dasharatha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning. (arXiv:2105.05883v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05883</id>
        <link href="http://arxiv.org/abs/2105.05883"/>
        <updated>2021-05-24T05:08:41.595Z</updated>
        <summary type="html"><![CDATA[This work addresses the problem of optimizing communications between server
and clients in federated learning (FL). Current sampling approaches in FL are
either biased, or non optimal in terms of server-clients communications and
training stability. To overcome this issue, we introduce \textit{clustered
sampling} for clients selection. We prove that clustered sampling leads to
better clients representatitivity and to reduced variance of the clients
stochastic aggregation weights in FL. Compatibly with our theory, we provide
two different clustering approaches enabling clients aggregation based on 1)
sample size, and 2) models similarity. Through a series of experiments in
non-iid and unbalanced scenarios, we demonstrate that model aggregation through
clustered sampling consistently leads to better training convergence and
variability when compared to standard sampling approaches. Our approach does
not require any additional operation on the clients side, and can be seamlessly
integrated in standard FL implementations. Finally, clustered sampling is
compatible with existing methods and technologies for privacy enhancement, and
for communication reduction through model compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fraboni_Y/0/1/0/all/0/1"&gt;Yann Fraboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1"&gt;Richard Vidal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kameni_L/0/1/0/all/0/1"&gt;Laetitia Kameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1"&gt;Marco Lorenzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uma implementa\c{c}\~ao do jogo Pedra, Papel e Tesoura utilizando Visao Computacional. (arXiv:2105.10063v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10063</id>
        <link href="http://arxiv.org/abs/2105.10063"/>
        <updated>2021-05-24T05:08:41.575Z</updated>
        <summary type="html"><![CDATA[This paper presents a game, controlled by computer vision, in identification
of hand gestures (hand-tracking). The proposed work is based on image
segmentation and construction of a convex hull with Jarvis Algorithm , and
determination of the pattern based on the extraction of area characteristics in
the convex hull.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santos_E/0/1/0/all/0/1"&gt;Ezequiel Fran&amp;#xe7;a dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fontenelle_G/0/1/0/all/0/1"&gt;Gabriel Fontenelle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Context for improving recognition of Online Handwritten Mathematical Expressions. (arXiv:2105.10156v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10156</id>
        <link href="http://arxiv.org/abs/2105.10156"/>
        <updated>2021-05-24T05:08:41.549Z</updated>
        <summary type="html"><![CDATA[This paper presents a temporal classification method for all three subtasks
of symbol segmentation, symbol recognition and relation classification in
online handwritten mathematical expressions (HMEs). The classification model is
trained by multiple paths of symbols and spatial relations derived from the
Symbol Relation Tree (SRT) representation of HMEs. The method benefits from
global context of a deep bidirectional Long Short-term Memory network, which
learns the temporal classification directly from online handwriting by the
Connectionist Temporal Classification loss. To recognize an online HME, a
symbol-level parse tree with Context-Free Grammar is constructed, where symbols
and spatial relations are obtained from the temporal classification results. We
show the effectiveness of the proposed method on the two latest CROHME
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Cuong Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Thanh-Nghia Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hung Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1"&gt;Masaki Nakagawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GSSF: A Generative Sequence Similarity Function based on a Seq2Seq model for clustering online handwritten mathematical answers. (arXiv:2105.10159v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10159</id>
        <link href="http://arxiv.org/abs/2105.10159"/>
        <updated>2021-05-24T05:08:41.541Z</updated>
        <summary type="html"><![CDATA[Toward a computer-assisted marking for descriptive math questions,this paper
presents clustering of online handwritten mathematical expressions (OnHMEs) to
help human markers to mark them efficiently and reliably. We propose a
generative sequence similarity function for computing a similarity score of two
OnHMEs based on a sequence-to-sequence OnHME recognizer. Each OnHME is
represented by a similarity-based representation (SbR) vector. The SbR matrix
is inputted to the k-means algorithm for clustering OnHMEs. Experiments are
conducted on an answer dataset (Dset_Mix) of 200 OnHMEs mixed of real patterns
and synthesized patterns for each of 10 questions and a real online handwritten
mathematical answer dataset of 122 student answers at most for each of 15
questions (NIER_CBT). The best clustering results achieved around 0.916 and
0.915 for purity, and around 0.556 and 0.702 for the marking cost on Dset_Mix
and NIER_CBT, respectively. Our method currently outperforms the previous
methods for clustering HMEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1"&gt;Huy Quang Ung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Cuong Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hung Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1"&gt;Masaki Nakagawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Network and Embedding Usage: New Tricks of Node Classification with Graph Convolutional Networks. (arXiv:2105.08330v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08330</id>
        <link href="http://arxiv.org/abs/2105.08330"/>
        <updated>2021-05-24T05:08:41.530Z</updated>
        <summary type="html"><![CDATA[Graph Convolutional Networks (GCNs) and subsequent variants have been
proposed to solve tasks on graphs, especially node classification tasks. In the
literature, however, most tricks or techniques are either briefly mentioned as
implementation details or only visible in source code. In this paper, we first
summarize some existing effective tricks used in GCNs mini-batch training.
Based on this, two novel tricks named GCN_res Framework and Embedding Usage are
proposed by leveraging residual network and pre-trained embedding to improve
baseline's test accuracy in different datasets. Experiments on Open Graph
Benchmark (OGB) show that, by combining these techniques, the test accuracy of
various GCNs increases by 1.21%~2.84%. We open source our implementation at
https://github.com/ytchx1999/PyG-OGB-Tricks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1"&gt;Huixuan Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1"&gt;Qinfen Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1"&gt;Hong Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behind the leaves -- Estimation of occluded grapevine berries with conditional generative adversarial networks. (arXiv:2105.10325v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10325</id>
        <link href="http://arxiv.org/abs/2105.10325"/>
        <updated>2021-05-24T05:08:41.520Z</updated>
        <summary type="html"><![CDATA[The need for accurate yield estimates for viticulture is becoming more
important due to increasing competition in the wine market worldwide. One of
the most promising methods to estimate the harvest is berry counting, as it can
be approached non-destructively, and its process can be automated. In this
article, we present a method that addresses the challenge of occluded berries
with leaves to obtain a more accurate estimate of the number of berries that
will enable a better estimate of the harvest. We use generative adversarial
networks, a deep learning-based approach that generates a likely scenario
behind the leaves exploiting learned patterns from images with non-occluded
berries. Our experiments show that the estimate of the number of berries after
applying our method is closer to the manually counted reference. In contrast to
applying a factor to the berry count, our approach better adapts to local
conditions by directly involving the appearance of the visible berries.
Furthermore, we show that our approach can identify which areas in the image
should be changed by adding new berries without explicitly requiring
information about hidden areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1"&gt;Jana Kierdorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1"&gt;Immanuel Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kicherer_A/0/1/0/all/0/1"&gt;Anna Kicherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zabawa_L/0/1/0/all/0/1"&gt;Laura Zabawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drees_L/0/1/0/all/0/1"&gt;Lukas Drees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1"&gt;Ribana Roscher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Hash Code Generation for Cancelable Fingerprint Templates using Vector Permutation and Shift-order Process. (arXiv:2105.10227v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.10227</id>
        <link href="http://arxiv.org/abs/2105.10227"/>
        <updated>2021-05-24T05:08:41.502Z</updated>
        <summary type="html"><![CDATA[Cancelable biometric techniques have been used to prevent the compromise of
biometric data by generating and using their corresponding cancelable templates
for user authentication. However, the non-invertible distance preserving
transformation methods employed in various schemes are often vulnerable to
information leakage since matching is performed in the transformed domain. In
this paper, we propose a non-invertible distance preserving scheme based on
vector permutation and shift-order process. First, the dimension of feature
vectors is reduced using kernelized principle component analysis (KPCA) prior
to randomly permuting the extracted vector features. A shift-order process is
then applied to the generated features in order to achieve non-invertibility
and combat similarity-based attacks. The generated hash codes are resilient to
different security and privacy attacks whilst fulfilling the major revocability
and unlinkability requirements. Experimental evaluation conducted on 6 datasets
of FVC2002 and FVC2004 reveals a high-performance accuracy of the proposed
scheme better than other existing state-of-the-art schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdullahi_S/0/1/0/all/0/1"&gt;Sani M. Abdullahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuifa_S/0/1/0/all/0/1"&gt;Sun Shuifa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EMface: Detecting Hard Faces by Exploring Receptive Field Pyraminds. (arXiv:2105.10104v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10104</id>
        <link href="http://arxiv.org/abs/2105.10104"/>
        <updated>2021-05-24T05:08:41.496Z</updated>
        <summary type="html"><![CDATA[Scale variation is one of the most challenging problems in face detection.
Modern face detectors employ feature pyramids to deal with scale variation.
However, it might break the feature consistency across different scales of
faces. In this paper, we propose a simple yet effective method named the
receptive field pyramids (RFP) method to enhance the representation ability of
feature pyramids. It can learn different receptive fields in each feature map
adaptively based on the varying scales of detected faces. Empirical results on
two face detection benchmark datasets, i.e., WIDER FACE and UFDD, demonstrate
that our proposed method can accelerate the inference rate significantly while
achieving state-of-the-art performance. The source code of our method is
available at \url{https://github.com/emdata-ailab/EMface}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Leilei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual representation of negation: Real world data analysis on comic image design. (arXiv:2105.10131v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10131</id>
        <link href="http://arxiv.org/abs/2105.10131"/>
        <updated>2021-05-24T05:08:41.489Z</updated>
        <summary type="html"><![CDATA[There has been a widely held view that visual representations (e.g.,
photographs and illustrations) do not depict negation, for example, one that
can be expressed by a sentence "the train is not coming". This view is
empirically challenged by analyzing the real-world visual representations of
comic (manga) illustrations. In the experiment using image captioning tasks, we
gave people comic illustrations and asked them to explain what they could read
from them. The collected data showed that some comic illustrations could depict
negation without any aid of sequences (multiple panels) or conventional devices
(special symbols). This type of comic illustrations was subjected to further
experiments, classifying images into those containing negation and those not
containing negation. While this image classification was easy for humans, it
was difficult for data-driven machines, i.e., deep learning models (CNN), to
achieve the same high performance. Given the findings, we argue that some comic
illustrations evoke background knowledge and thus can depict negation with
purely visual elements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1"&gt;Yuri Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mineshima_K/0/1/0/all/0/1"&gt;Koji Mineshima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_K/0/1/0/all/0/1"&gt;Kazuhiro Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From parcel to continental scale -- A first European crop type map based on Sentinel-1 and LUCAS Copernicus in-situ observations. (arXiv:2105.09261v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09261</id>
        <link href="http://arxiv.org/abs/2105.09261"/>
        <updated>2021-05-24T05:08:41.483Z</updated>
        <summary type="html"><![CDATA[Detailed parcel-level crop type mapping for the whole European Union (EU) is
necessary for the evaluation of agricultural policies. The Copernicus program,
and Sentinel-1 (S1) in particular, offers the opportunity to monitor
agricultural land at a continental scale and in a timely manner. However, so
far the potential of S1 has not been explored at such a scale. Capitalizing on
the unique LUCAS 2018 Copernicus in-situ survey, we present the first
continental crop type map at 10-m spatial resolution for the EU based on S1A
and S1B Synthetic Aperture Radar observations for the year 2018. Random forest
classification algorithms are tuned to detect 19 different crop types. We
assess the accuracy of this EU crop map with three approaches. First, the
accuracy is assessed with independent LUCAS core in-situ observations over the
continent. Second, an accuracy assessment is done specifically for main crop
types from farmers declarations from 6 EU member countries or regions totaling
>3M parcels and 8.21 Mha. Finally, the crop areas derived by classification are
compared to the subnational (NUTS 2) area statistics reported by Eurostat. The
overall accuracy for the map is reported as 80.3% when grouping main crop
classes and 76% when considering all 19 crop type classes separately. Highest
accuracies are obtained for rape and turnip rape with user and produced
accuracies higher than 96%. The correlation between the remotely sensed
estimated and Eurostat reported crop area ranges from 0.93 (potatoes) to 0.99
(rape and turnip rape). Finally, we discuss how the framework presented here
can underpin the operational delivery of in-season high-resolution based crop
mapping.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+dAndrimont_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l d&amp;#x27;Andrimont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Verhegghen_A/0/1/0/all/0/1"&gt;Astrid Verhegghen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lemoine_G/0/1/0/all/0/1"&gt;Guido Lemoine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kempeneers_P/0/1/0/all/0/1"&gt;Pieter Kempeneers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meroni_M/0/1/0/all/0/1"&gt;Michele Meroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Velde_M/0/1/0/all/0/1"&gt;Marijn van der Velde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safety Metrics for Semantic Segmentation in Autonomous Driving. (arXiv:2105.10142v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10142</id>
        <link href="http://arxiv.org/abs/2105.10142"/>
        <updated>2021-05-24T05:08:41.476Z</updated>
        <summary type="html"><![CDATA[Within the context of autonomous driving, safety-related metrics for deep
neural networks have been widely studied for image classification and object
detection. In this paper, we further consider safety-aware correctness and
robustness metrics specialized for semantic segmentation. The novelty of our
proposal is to move beyond pixel-level metrics: Given two images with each
having N pixels being class-flipped, the designed metrics should, depending on
the clustering of pixels being class-flipped or the location of occurrence,
reflect a different level of safety criticality. The result evaluated on an
autonomous driving dataset demonstrates the validity and practicality of our
proposed methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chih-Hong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1"&gt;Hsuan-Cheng Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06977</id>
        <link href="http://arxiv.org/abs/2105.06977"/>
        <updated>2021-05-24T05:08:41.459Z</updated>
        <summary type="html"><![CDATA[Context-aware machine translation models are designed to leverage contextual
information, but often fail to do so. As a result, they inaccurately
disambiguate pronouns and polysemous words that require context for resolution.
In this paper, we ask several questions: What contexts do human translators use
to resolve ambiguous words? Are models paying large amounts of attention to the
same context? What if we explicitly train them to do so? To answer these
questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a
new English-French dataset comprising supporting context words for 14K
translations that professional translators found useful for pronoun
disambiguation. Using SCAT, we perform an in-depth analysis of the context used
to disambiguate, examining positional and lexical characteristics of the
supporting words. Furthermore, we measure the degree of alignment between the
model's attention scores and the supporting context from SCAT, and apply a
guided attention strategy to encourage agreement between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kayo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1"&gt;Patrick Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1"&gt;Danish Pruthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1"&gt;Aditi Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter. (arXiv:2105.09967v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09967</id>
        <link href="http://arxiv.org/abs/2105.09967"/>
        <updated>2021-05-24T05:08:41.422Z</updated>
        <summary type="html"><![CDATA[Datasets with induced emotion labels are scarce but of utmost importance for
many NLP tasks. We present a new, automated method for collecting texts along
with their induced reaction labels. The method exploits the online use of
reaction GIFs, which capture complex affective states. We show how to augment
the data with induced emotion and induced sentiment labels. We use our method
to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K
tweets. We provide baselines for three new tasks, including induced sentiment
prediction and multilabel classification of induced emotions. Our method and
dataset open new research opportunities in emotion detection and affective
computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmueli_B/0/1/0/all/0/1"&gt;Boaz Shmueli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1"&gt;Soumya Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1"&gt;Lun-Wei Ku&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Fairness of Generative Adversarial Networks (GANs). (arXiv:2103.00950v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00950</id>
        <link href="http://arxiv.org/abs/2103.00950"/>
        <updated>2021-05-24T05:08:41.413Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are one of the greatest advances in AI
in recent years. With their ability to directly learn the probability
distribution of data, and then sample synthetic realistic data. Many
applications have emerged, using GANs to solve classical problems in machine
learning, such as data augmentation, class unbalance problems, and fair
representation learning. In this paper, we analyze and highlight fairness
concerns of GANs model. In this regard, we show empirically that GANs models
may inherently prefer certain groups during the training process and therefore
they're not able to homogeneously generate data from different groups during
the testing phase. Furthermore, we propose solutions to solve this issue by
conditioning the GAN model towards samples' group or using ensemble method
(boosting) to allow the GAN model to leverage distributed structure of data
during the training phase and generate groups at equal rate during the testing
phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1"&gt;Patrik Joslin Kenfack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arapov_D/0/1/0/all/0/1"&gt;Daniil Dmitrievich Arapov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1"&gt;Rasheed Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1"&gt;S.M. Ahsan Kazmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Adil Mehmood Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Stroke Rehabilitation Assessment using Wearable Accelerometers in Free-Living Environments. (arXiv:2009.08798v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08798</id>
        <link href="http://arxiv.org/abs/2009.08798"/>
        <updated>2021-05-24T05:08:41.403Z</updated>
        <summary type="html"><![CDATA[Stroke is known as a major global health problem, and for stroke survivors it
is key to monitor the recovery levels. However, traditional stroke
rehabilitation assessment methods (such as the popular clinical assessment) can
be subjective and expensive, and it is also less convenient for patients to
visit clinics in a high frequency. To address this issue, in this work based on
wearable sensing and machine learning techniques, we developed an automated
system that can predict the assessment score in an objective manner. With
wrist-worn sensors, accelerometer data was collected from 59 stroke survivors
in free-living environments for a duration of 8 weeks, and we aim to map the
week-wise accelerometer data (3 days per week) to the assessment score by
developing signal processing and predictive model pipeline. To achieve this, we
proposed two types of new features, which can encode the rehabilitation
information from both paralysed/non-paralysed sides while suppressing the
high-level noises such as irrelevant daily activities. Based on the proposed
features, we further developed the longitudinal mixed-effects model with
Gaussian process prior (LMGP), which can model the random effects caused by
different subjects and time slots (during the 8 weeks). Comprehensive
experiments were conducted to evaluate our system on both acute and chronic
patients, and the results suggested its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yu Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jian-Qing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Du_X/0/1/0/all/0/1"&gt;Xiu-Li Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eyre_J/0/1/0/all/0/1"&gt;Janet Eyre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedProf: Efficient Federated Learning with Data Representation Profiling. (arXiv:2102.01733v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01733</id>
        <link href="http://arxiv.org/abs/2102.01733"/>
        <updated>2021-05-24T05:08:41.397Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has shown great potential as a privacy-preserving
solution to learning from decentralized data which are only accessible locally
on end devices (i.e., clients). In many scenarios, however, a large proportion
of the clients are probably in possession of low-quality data that are biased,
noisy or even irrelevant. As a result, they could significantly slow down the
convergence of the global model we aim to build and also compromise its
quality. In light of this, we propose FedProf, a novel protocol for optimizing
FL under such circumstances without breaching data privacy. The key of our
approach is using the global model to dynamically profile the latent
representations of data (termed representation footprints) on the clients. By
matching local footprints on clients against a baseline footprint on the
server, we adaptively score each client and adjust its probability of being
selected each round so as to mitigate the impact of the clients with
low-quality data on the training process. We have conducted extensive
experiments on public data sets using various FL settings. The results show
that FedProf effectively reduces the number of communication rounds and overall
time (providing up to 4.5x speedup) for the global model to converge while
improving the accuracy of the final global model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wentai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Ligang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiwei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1"&gt;Rui Mao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LoopNet: Musical Loop Synthesis Conditioned On Intuitive Musical Parameters. (arXiv:2105.10371v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.10371</id>
        <link href="http://arxiv.org/abs/2105.10371"/>
        <updated>2021-05-24T05:08:41.378Z</updated>
        <summary type="html"><![CDATA[Loops, seamlessly repeatable musical segments, are a cornerstone of modern
music production. Contemporary artists often mix and match various sampled or
pre-recorded loops based on musical criteria such as rhythm, harmony and
timbral texture to create compositions. Taking such criteria into account, we
present LoopNet, a feed-forward generative model for creating loops conditioned
on intuitive parameters. We leverage Music Information Retrieval (MIR) models
as well as a large collection of public loop samples in our study and use the
Wave-U-Net architecture to map control parameters to audio. We also evaluate
the quality of the generated audio and propose intuitive controls for composers
to map the ideas in their minds to an audio loop.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandna_P/0/1/0/all/0/1"&gt;Pritish Chandna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramires_A/0/1/0/all/0/1"&gt;Ant&amp;#xf3;nio Ramires&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1"&gt;Xavier Serra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_E/0/1/0/all/0/1"&gt;Emilia G&amp;#xf3;mez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Uncertainty from Different Sources in Deep Neural Networks for Image Classification. (arXiv:2011.08712v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08712</id>
        <link href="http://arxiv.org/abs/2011.08712"/>
        <updated>2021-05-24T05:08:41.367Z</updated>
        <summary type="html"><![CDATA[Quantifying uncertainty in a model's predictions is important as it enables
the safety of an AI system to be increased by acting on the model's output in
an informed manner. This is crucial for applications where the cost of an error
is high, such as in autonomous vehicle control, medical image analysis,
financial estimations or legal fields. Deep Neural Networks are powerful
predictors that have recently achieved state-of-the-art performance on a wide
spectrum of tasks. Quantifying predictive uncertainty in DNNs is a challenging
and yet on-going problem. In this paper we propose a complete framework to
capture and quantify all of these three types of uncertainties in DNNs for
image classification. This framework includes an ensemble of CNNs for model
uncertainty, a supervised reconstruction auto-encoder to capture distributional
uncertainty and using the output of activation functions in the last layer of
the network, to capture data uncertainty. Finally we demonstrate the efficiency
of our method on popular image datasets for classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khoshsirat_A/0/1/0/all/0/1"&gt;Aria Khoshsirat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elliptical Ordinal Embedding. (arXiv:2105.10457v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10457</id>
        <link href="http://arxiv.org/abs/2105.10457"/>
        <updated>2021-05-24T05:08:41.329Z</updated>
        <summary type="html"><![CDATA[Ordinal embedding aims at finding a low dimensional representation of objects
from a set of constraints of the form "item $j$ is closer to item $i$ than item
$k$". Typically, each object is mapped onto a point vector in a low dimensional
metric space. We argue that mapping to a density instead of a point vector
provides some interesting advantages, including an inherent reflection of the
uncertainty about the representation itself and its relative location in the
space. Indeed, in this paper, we propose to embed each object as a Gaussian
distribution. We investigate the ability of these embeddings to capture the
underlying structure of the data while satisfying the constraints, and explore
properties of the representation. Experiments on synthetic and real-world
datasets showcase the advantages of our approach. In addition, we illustrate
the merit of modelling uncertainty, which enriches the visual perception of the
mapped objects in the space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diallo_A/0/1/0/all/0/1"&gt;A&amp;#xef;ssatou Diallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Explainable Classification Model for Chronic Kidney Disease Patients. (arXiv:2105.10368v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10368</id>
        <link href="http://arxiv.org/abs/2105.10368"/>
        <updated>2021-05-24T05:08:41.320Z</updated>
        <summary type="html"><![CDATA[Currently, Chronic Kidney Disease (CKD) is experiencing a globally increasing
incidence and high cost to health systems. A delayed recognition leads to
premature mortality due to progressive loss of kidney function. The employment
of data mining to discover subtle patterns in CKD indicators would contribute
to an early diagnosis. This work develops a classifier model that would support
healthcare professionals in the early diagnosis of CKD patients. Through a data
pipeline, an exhaustive search is performed to find the best data mining
classifier with different parameters of the data preparation's sub-stages like
data missing or feature selection. Therefore, Extra Trees is selected as the
best classifier with a 100% and 99% of accuracy with, respectively,
cross-validation technique and with new unseen data. Moreover, the 8 features
selected are employed to assess the explainability of the model's results
denoting which features are more relevant in the model's output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_Sanchez_P/0/1/0/all/0/1"&gt;Pedro A. Moreno-Sanchez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Explaining Random Forests with SAT. (arXiv:2105.10278v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10278</id>
        <link href="http://arxiv.org/abs/2105.10278"/>
        <updated>2021-05-24T05:08:41.312Z</updated>
        <summary type="html"><![CDATA[Random Forest (RFs) are among the most widely used Machine Learning (ML)
classifiers. Even though RFs are not interpretable, there are no dedicated
non-heuristic approaches for computing explanations of RFs. Moreover, there is
recent work on polynomial algorithms for explaining ML models, including naive
Bayes classifiers. Hence, one question is whether finding explanations of RFs
can be solved in polynomial time. This paper answers this question negatively,
by proving that computing one PI-explanation of an RF is D^P-complete.
Furthermore, the paper proposes a propositional encoding for computing
explanations of RFs, thus enabling finding PI-explanations with a SAT solver.
This contrasts with earlier work on explaining boosted trees (BTs) and neural
networks (NNs), which requires encodings based on SMT/MILP. Experimental
results, obtained on a wide range of publicly available datasets, demontrate
that the proposed SAT-based approach scales to RFs of sizes common in practical
applications. Perhaps more importantly, the experimental results demonstrate
that, for the vast majority of examples considered, the SAT-based approach
proposed in this paper significantly outperforms existing heuristic approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Izza_Y/0/1/0/all/0/1"&gt;Yacine Izza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marques_Silva_J/0/1/0/all/0/1"&gt;Joao Marques-Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Resilient Collaborative Intelligence via Low-Rank Tensor Completion. (arXiv:2105.10341v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10341</id>
        <link href="http://arxiv.org/abs/2105.10341"/>
        <updated>2021-05-24T05:08:41.305Z</updated>
        <summary type="html"><![CDATA[In the race to bring Artificial Intelligence (AI) to the edge, collaborative
intelligence has emerged as a promising way to lighten the computation load on
edge devices that run applications based on Deep Neural Networks (DNNs).
Typically, a deep model is split at a certain layer into edge and cloud
sub-models. The deep feature tensor produced by the edge sub-model is
transmitted to the cloud, where the remaining computationally intensive
workload is performed by the cloud sub-model. The communication channel between
the edge and cloud is imperfect, which will result in missing data in the deep
feature tensor received at the cloud side. In this study, we examine the
effectiveness of four low-rank tensor completion methods in recovering missing
data in the deep feature tensor. We consider both sparse tensors, such as those
produced by the VGG16 model, as well as non-sparse tensors, such as those
produced by ResNet34 model. We study tensor completion effectiveness in both
conplexity-constrained and unconstrained scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bragilevsky_L/0/1/0/all/0/1"&gt;Lior Bragilevsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1"&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective. (arXiv:2105.09985v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09985</id>
        <link href="http://arxiv.org/abs/2105.09985"/>
        <updated>2021-05-24T05:08:41.287Z</updated>
        <summary type="html"><![CDATA[In this work we study the problem of measuring the fairness of a machine
learning model under noisy information. Focusing on group fairness metrics, we
investigate the particular but common situation when the evaluation requires
controlling for the confounding effect of covariate variables. In a practical
setting, we might not be able to jointly observe the covariate and group
information, and a standard workaround is to then use proxies for one or more
of these variables. Prior works have demonstrated the challenges with using a
proxy for sensitive attributes, and strong independence assumptions are needed
to provide guarantees on the accuracy of the noisy estimates. In contrast, in
this work we study using a proxy for the covariate variable and present a
theoretical analysis that aims to characterize weaker conditions under which
accurate fairness evaluation is possible.

Furthermore, our theory identifies potential sources of errors and decouples
them into two interpretable parts $\gamma$ and $\epsilon$. The first part
$\gamma$ depends solely on the performance of the proxy such as precision and
recall, whereas the second part $\epsilon$ captures correlations between all
the variables of interest. We show that in many scenarios the error in the
estimates is dominated by $\gamma$ via a linear dependence, whereas the
dependence on the correlations $\epsilon$ only constitutes a lower order term.
As a result we expand the understanding of scenarios where measuring model
fairness via proxies can be an effective approach. Finally, we compare, via
simulations, the theoretical upper-bounds to the distribution of simulated
estimation errors and show that assuming some structure on the data, even weak,
is key to significantly improve both theoretical guarantees and empirical
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prost_F/0/1/0/all/0/1"&gt;Flavien Prost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1"&gt;Pranjal Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blumm_N/0/1/0/all/0/1"&gt;Nick Blumm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumthekar_A/0/1/0/all/0/1"&gt;Aditee Kumthekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potter_T/0/1/0/all/0/1"&gt;Trevor Potter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Li Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1"&gt;Alex Beutel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices. (arXiv:2105.10288v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10288</id>
        <link href="http://arxiv.org/abs/2105.10288"/>
        <updated>2021-05-24T05:08:41.280Z</updated>
        <summary type="html"><![CDATA[Single-Image Super Resolution (SISR) is a classical computer vision problem
and it has been studied for over decades. With the recent success of deep
learning methods, recent work on SISR focuses solutions with deep learning
methodologies and achieves state-of-the-art results. However most of the
state-of-the-art SISR methods contain millions of parameters and layers, which
limits their practical applications. In this paper, we propose a hardware
(Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization
robust real-time super resolution network (XLSR). The proposed model's building
block is inspired from root modules for Image classification. We successfully
applied root modules to SISR problem, further more to make the model uint8
quantization robust we used Clipped ReLU at the last layer of the network and
achieved great balance between reconstruction quality and runtime. Furthermore,
although the proposed network contains 30x fewer parameters than VDSR its
performance surpasses it on Div2K validation set. The network proved itself by
winning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayazoglu_M/0/1/0/all/0/1"&gt;Mustafa Ayazoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep-Learned Event Variables for Collider Phenomenology. (arXiv:2105.10126v1 [hep-ph])]]></title>
        <id>http://arxiv.org/abs/2105.10126</id>
        <link href="http://arxiv.org/abs/2105.10126"/>
        <updated>2021-05-24T05:08:41.274Z</updated>
        <summary type="html"><![CDATA[The choice of optimal event variables is crucial for achieving the maximal
sensitivity of experimental analyses. Over time, physicists have derived
suitable kinematic variables for many typical event topologies in collider
physics. Here we introduce a deep learning technique to design good event
variables, which are sensitive over a wide range of values for the unknown
model parameters. We demonstrate that the neural networks trained with our
technique on some simple event topologies are able to reproduce standard event
variables like invariant mass, transverse mass, and stransverse mass. The
method is automatable, completely general, and can be used to derive sensitive,
previously unknown, event variables for other, more complex event topologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Kim_D/0/1/0/all/0/1"&gt;Doojin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kyoungchul Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Matchev_K/0/1/0/all/0/1"&gt;Konstantin T. Matchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Park_M/0/1/0/all/0/1"&gt;Myeonghun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Shyamsundar_P/0/1/0/all/0/1"&gt;Prasanth Shyamsundar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Sizing for PPE with a Point Cloud Based Variational Autoencoder. (arXiv:2105.10067v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10067</id>
        <link href="http://arxiv.org/abs/2105.10067"/>
        <updated>2021-05-24T05:08:41.258Z</updated>
        <summary type="html"><![CDATA[Sizing and fitting of Personal Protective Equipment (PPE) is a critical part
of the product creation process; however, traditional methods to do this type
of work can be labor intensive and based on limited or non-representative
anthropomorphic data. In the case of PPE, a poor fit can jeopardize an
individual's health and safety. In this paper we present an unsupervised
machine learning algorithm that can identify a representative set of exemplars,
individuals that can be utilized by designers as idealized sizing models. The
algorithm is based around a Variational Autoencoder (VAE) with a Point-Net
inspired encoder and decoder architecture trained on Human point-cloud data
obtained from the CEASAR dataset. The learned latent space is then clustered to
identify a specified number of sizing groups. We demonstrate this technique on
scans of human faces to provide designers of masks and facial coverings a
reference set of individuals to test existing mask styles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Searcy_J/0/1/0/all/0/1"&gt;Jacob A. Searcy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sokolowski_S/0/1/0/all/0/1"&gt;Susan L. Sokolowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Compression. (arXiv:2105.10059v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10059</id>
        <link href="http://arxiv.org/abs/2105.10059"/>
        <updated>2021-05-24T05:08:41.222Z</updated>
        <summary type="html"><![CDATA[With time, machine learning models have increased in their scope,
functionality and size. Consequently, the increased functionality and size of
such models requires high-end hardware to both train and provide inference
after the fact. This paper aims to explore the possibilities within the domain
of model compression and discuss the efficiency of each of the possible
approaches while comparing model size and performance with respect to pre- and
post-compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishtiaq_A/0/1/0/all/0/1"&gt;Arhum Ishtiaq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_S/0/1/0/all/0/1"&gt;Sara Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anees_M/0/1/0/all/0/1"&gt;Maheen Anees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mumtaz_N/0/1/0/all/0/1"&gt;Neha Mumtaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter. (arXiv:2105.09967v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09967</id>
        <link href="http://arxiv.org/abs/2105.09967"/>
        <updated>2021-05-24T05:08:41.216Z</updated>
        <summary type="html"><![CDATA[Datasets with induced emotion labels are scarce but of utmost importance for
many NLP tasks. We present a new, automated method for collecting texts along
with their induced reaction labels. The method exploits the online use of
reaction GIFs, which capture complex affective states. We show how to augment
the data with induced emotion and induced sentiment labels. We use our method
to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K
tweets. We provide baselines for three new tasks, including induced sentiment
prediction and multilabel classification of induced emotions. Our method and
dataset open new research opportunities in emotion detection and affective
computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmueli_B/0/1/0/all/0/1"&gt;Boaz Shmueli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1"&gt;Soumya Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1"&gt;Lun-Wei Ku&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CapillaryNet: An Automated System to Analyze Microcirculation Videos from Handheld Vital Microscopy. (arXiv:2104.11574v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11574</id>
        <link href="http://arxiv.org/abs/2104.11574"/>
        <updated>2021-05-24T05:08:41.208Z</updated>
        <summary type="html"><![CDATA[Capillaries are the smallest vessels in the body responsible for the delivery
of oxygen and nutrients to the surrounding cells. Various diseases have been
shown to alter the density of nutritive capillaries and the flow velocity of
erythrocytes. In previous studies, capillary density and flow velocity have
been assessed manually by trained specialists. Manual analysis of a 20-second
long microvascular video takes on average 20 minutes and requires extensive
training. Several studies have reported that manual analysis hinders the
application of microvascular microscopy in a clinical setting. In this paper,
we present a fully automated system, called CapillaryNet, that can automate
microvascular microscopy analysis so it can be used as a clinical application.
Moreover, CapillaryNet measures several microvascular parameters that
researchers were previously unable to quantify, i.e. capillary hematocrit and
intra-capillary flow velocity heterogeneity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1"&gt;Maged Helmy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1"&gt;Anastasiya Dykyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Tuyen Trung Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1"&gt;Paulo Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1"&gt;Eric Jul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Sufficient Explanations. (arXiv:2105.10118v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10118</id>
        <link href="http://arxiv.org/abs/2105.10118"/>
        <updated>2021-05-24T05:08:41.199Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior of learned classifiers is an important task, and
various black-box explanations, logical reasoning approaches, and
model-specific methods have been proposed. In this paper, we introduce
probabilistic sufficient explanations, which formulate explaining an instance
of classification as choosing the "simplest" subset of features such that only
observing those features is "sufficient" to explain the classification. That
is, sufficient to give us strong probabilistic guarantees that the model will
behave similarly when all features are observed under the data distribution. In
addition, we leverage tractable probabilistic reasoning tools such as
probabilistic circuits and expected predictions to design a scalable algorithm
for finding the desired explanations while keeping the guarantees intact. Our
experiments demonstrate the effectiveness of our algorithm in finding
sufficient explanations, and showcase its advantages compared to Anchors and
logical explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Eric Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosravi_P/0/1/0/all/0/1"&gt;Pasha Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1"&gt;Guy Van den Broeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Clustering and Representation Learning with Geometric Structure Preservation. (arXiv:2009.09590v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09590</id>
        <link href="http://arxiv.org/abs/2009.09590"/>
        <updated>2021-05-24T05:08:41.162Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel framework for Deep Clustering and
multi-manifold Representation Learning (DCRL) that preserves the geometric
structure of data. In the proposed framework, manifold clustering is done in
the latent space guided by a clustering loss. To overcome the problem that
clustering-oriented losses may deteriorate the geometric structure of
embeddings in the latent space, an isometric loss is proposed for preserving
intra-manifold structure locally and a ranking loss for inter-manifold
structure globally. Experimental results on various datasets show that DCRL
leads to performances comparable to current state-of-the-art deep clustering
algorithms, yet exhibits superior performance for manifold representation. Our
results also demonstrate the importance and effectiveness of the proposed
losses in preserving geometric structure in terms of visualization and
performance metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lirong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1"&gt;Zelin Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1"&gt;Jun Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Siyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan. Z Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL-IoT: Reinforcement Learning to Interact with IoT Devices. (arXiv:2105.00884v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00884</id>
        <link href="http://arxiv.org/abs/2105.00884"/>
        <updated>2021-05-24T05:08:41.150Z</updated>
        <summary type="html"><![CDATA[Our life is getting filled by Internet of Things (IoT) devices. These devices
often rely on closed or poorly documented protocols, with unknown formats and
semantics. Learning how to interact with such devices in an autonomous manner
is the key for interoperability and automatic verification of their
capabilities. In this paper, we propose RL-IoT, a system that explores how to
automatically interact with possibly unknown IoT devices. We leverage
reinforcement learning (RL) to recover the semantics of protocol messages and
to take control of the device to reach a given goal, while minimizing the
number of interactions. We assume to know only a database of possible IoT
protocol messages, whose semantics are however unknown. RL-IoT exchanges
messages with the target IoT device, learning those commands that are useful to
reach the given goal. Our results show that RL-IoT is able to solve both simple
and complex tasks. With properly tuned parameters, RL-IoT learns how to perform
actions with the target device, a Yeelight smart bulb in our case study,
completing non-trivial patterns with as few as 400 interactions. RL-IoT paves
the road for automatic interactions with poorly documented IoT protocols, thus
enabling interoperable systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milan_G/0/1/0/all/0/1"&gt;Giulia Milan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vassio_L/0/1/0/all/0/1"&gt;Luca Vassio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drago_I/0/1/0/all/0/1"&gt;Idilio Drago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mellia_M/0/1/0/all/0/1"&gt;Marco Mellia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian hierarchical stacking: Some models are (somewhere) useful. (arXiv:2101.08954v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08954</id>
        <link href="http://arxiv.org/abs/2101.08954"/>
        <updated>2021-05-24T05:08:41.136Z</updated>
        <summary type="html"><![CDATA[Stacking is a widely used model averaging technique that asymptotically
yields optimal predictions among linear averages. We show that stacking is most
effective when model predictive performance is heterogeneous in inputs, and we
can further improve the stacked mixture with a hierarchical model. We
generalize stacking to Bayesian hierarchical stacking. The model weights are
varying as a function of data, partially-pooled, and inferred using Bayesian
inference. We further incorporate discrete and continuous inputs, other
structured priors, and time series and longitudinal data. To verify the
performance gain of the proposed method, we derive theory bounds, and
demonstrate on several applied problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuling Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pirs_G/0/1/0/all/0/1"&gt;Gregor Pir&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1"&gt;Aki Vehtari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gelman_A/0/1/0/all/0/1"&gt;Andrew Gelman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Removing the mini-batching error in Bayesian inference using Adaptive Langevin dynamics. (arXiv:2105.10347v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.10347</id>
        <link href="http://arxiv.org/abs/2105.10347"/>
        <updated>2021-05-24T05:08:41.096Z</updated>
        <summary type="html"><![CDATA[The computational cost of usual Monte Carlo methods for sampling a posteriori
laws in Bayesian inference scales linearly with the number of data points. One
option to reduce it to a fraction of this cost is to resort to mini-batching in
conjunction with unadjusted discretizations of Langevin dynamics, in which case
only a random fraction of the data is used to estimate the gradient. However,
this leads to an additional noise in the dynamics and hence a bias on the
invariant measure which is sampled by the Markov chain. We advocate using the
so-called Adaptive Langevin dynamics, which is a modification of standard
inertial Langevin dynamics with a dynamical friction which automatically
corrects for the increased noise arising from mini-batching. We investigate the
practical relevance of the assumptions underpinning Adaptive Langevin (constant
covariance for the estimation of the gradient), which are not satisfied in
typical models of Bayesian inference; and show how to extend the approach to
more general situations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sekkat_I/0/1/0/all/0/1"&gt;Inass Sekkat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Stoltz_G/0/1/0/all/0/1"&gt;Gabriel Stoltz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Constrained Reinforcement Learning. (arXiv:2011.09999v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09999</id>
        <link href="http://arxiv.org/abs/2011.09999"/>
        <updated>2021-05-24T05:08:41.089Z</updated>
        <summary type="html"><![CDATA[In real world settings, numerous constraints are present which are hard to
specify mathematically. However, for the real world deployment of reinforcement
learning (RL), it is critical that RL agents are aware of these constraints, so
that they can act safely. In this work, we consider the problem of learning
constraints from demonstrations of a constraint-abiding agent's behavior. We
experimentally validate our approach and show that our framework can
successfully learn the most likely constraints that the agent respects. We
further show that these learned constraints are \textit{transferable} to new
agents that may have different morphologies and/or reward functions. Previous
works in this regard have either mainly been restricted to tabular (discrete)
settings, specific types of constraints or assume the environment's transition
dynamics. In contrast, our framework is able to learn arbitrary
\textit{Markovian} constraints in high-dimensions in a completely model-free
setting. The code can be found it:
\url{https://github.com/shehryar-malik/icrl}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_U/0/1/0/all/0/1"&gt;Usman Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_S/0/1/0/all/0/1"&gt;Shehryar Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghasi_A/0/1/0/all/0/1"&gt;Alireza Aghasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Ali Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Knowledge Expansion. (arXiv:2103.14431v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14431</id>
        <link href="http://arxiv.org/abs/2103.14431"/>
        <updated>2021-05-24T05:08:41.068Z</updated>
        <summary type="html"><![CDATA[The popularity of multimodal sensors and the accessibility of the Internet
have brought us a massive amount of unlabeled multimodal data. Since existing
datasets and well-trained models are primarily unimodal, the modality gap
between a unimodal network and unlabeled multimodal data poses an interesting
problem: how to transfer a pre-trained unimodal network to perform the same
task on unlabeled multimodal data? In this work, we propose multimodal
knowledge expansion (MKE), a knowledge distillation-based framework to
effectively utilize multimodal data without requiring labels. Opposite to
traditional knowledge distillation, where the student is designed to be
lightweight and inferior to the teacher, we observe that a multimodal student
model consistently denoises pseudo labels and generalizes better than its
teacher. Extensive experiments on four tasks and different modalities verify
this finding. Furthermore, we connect the mechanism of MKE to semi-supervised
learning and offer both empirical and theoretical explanations to understand
the denoising capability of a multimodal student.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Particle gradient descent model for point process generation. (arXiv:2010.14928v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14928</id>
        <link href="http://arxiv.org/abs/2010.14928"/>
        <updated>2021-05-24T05:08:41.039Z</updated>
        <summary type="html"><![CDATA[This paper introduces a generative model for planar point processes in a
square window, built upon a single realization of a stationary, ergodic point
process observed in this window. Inspired by recent advances in gradient
descent methods for maximum entropy models, we propose a method to generate
similar point patterns by jointly moving particles of an initial Poisson
configuration towards a target counting measure. The target measure is
generated via a deterministic gradient descent algorithm, so as to match a set
of statistics of the given, observed realization. Our statistics are estimators
of the multi-scale wavelet phase harmonic covariance, recently proposed in
image modeling. They allow one to capture geometric structures through
multi-scale interactions between wavelet coefficients. Both our statistics and
the gradient descent algorithm scale better with the number of observed points
than the classical k-nearest neighbour distances previously used in generative
models for point processes, based on the rejection sampling or
simulated-annealing. The overall quality of our model is evaluated on point
processes with various geometric structures through spectral and topological
data analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Brochard_A/0/1/0/all/0/1"&gt;Antoine Brochard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blaszczyszyn_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej B&amp;#x142;aszczyszyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mallat_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Mallat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sixin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CapillaryNet: An Automated System to Analyze Microcirculation Videos from Handheld Vital Microscopy. (arXiv:2104.11574v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11574</id>
        <link href="http://arxiv.org/abs/2104.11574"/>
        <updated>2021-05-24T05:08:41.032Z</updated>
        <summary type="html"><![CDATA[Capillaries are the smallest vessels in the body responsible for the delivery
of oxygen and nutrients to the surrounding cells. Various diseases have been
shown to alter the density of nutritive capillaries and the flow velocity of
erythrocytes. In previous studies, capillary density and flow velocity have
been assessed manually by trained specialists. Manual analysis of a 20-second
long microvascular video takes on average 20 minutes and requires extensive
training. Several studies have reported that manual analysis hinders the
application of microvascular microscopy in a clinical setting. In this paper,
we present a fully automated system, called CapillaryNet, that can automate
microvascular microscopy analysis so it can be used as a clinical application.
Moreover, CapillaryNet measures several microvascular parameters that
researchers were previously unable to quantify, i.e. capillary hematocrit and
intra-capillary flow velocity heterogeneity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1"&gt;Maged Helmy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1"&gt;Anastasiya Dykyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Tuyen Trung Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1"&gt;Paulo Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1"&gt;Eric Jul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Scalable Modeling of Biology in Event-B. (arXiv:2105.10344v1 [q-bio.MN])]]></title>
        <id>http://arxiv.org/abs/2105.10344</id>
        <link href="http://arxiv.org/abs/2105.10344"/>
        <updated>2021-05-24T05:08:41.025Z</updated>
        <summary type="html"><![CDATA[Biology offers many examples of large-scale, complex, concurrent systems:
many processes take place in parallel, compete on resources and influence each
other's behavior. The scalable modeling of biological systems continues to be a
very active field of research. In this paper we introduce a new approach based
on Event-B, a state-based formal method with refinement as its central
ingredient, allowing us to check for model consistency step-by-step in an
automated way. Our approach based on functions leads to an elegant and concise
modeling method. We demonstrate this approach by constructing what is, to our
knowledge, the largest ever built Event-B model, describing the ErbB signaling
pathway, a key evolutionary pathway with a significant role in development and
in many types of cancer. The Event-B model for the ErbB pathway describes 1320
molecular reactions through 242 events.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Sanwal_U/0/1/0/all/0/1"&gt;Usman Sanwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hoang_T/0/1/0/all/0/1"&gt;Thai Son Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Petre_L/0/1/0/all/0/1"&gt;Luigia Petre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Petre_I/0/1/0/all/0/1"&gt;Ion Petre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient PAC Reinforcement Learning in Regular Decision Processes. (arXiv:2105.06784v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06784</id>
        <link href="http://arxiv.org/abs/2105.06784"/>
        <updated>2021-05-24T05:08:41.018Z</updated>
        <summary type="html"><![CDATA[Recently regular decision processes have been proposed as a well-behaved form
of non-Markov decision process. Regular decision processes are characterised by
a transition function and a reward function that depend on the whole history,
though regularly (as in regular languages). In practice both the transition and
the reward functions can be seen as finite transducers. We study reinforcement
learning in regular decision processes. Our main contribution is to show that a
near-optimal policy can be PAC-learned in polynomial time in a set of
parameters that describe the underlying decision process. We argue that the
identified set of parameters is minimal and it reasonably captures the
difficulty of a regular decision process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ronca_A/0/1/0/all/0/1"&gt;Alessandro Ronca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1"&gt;Giuseppe De Giacomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance-Free Sparse Bayesian Learning. (arXiv:2105.10439v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2105.10439</id>
        <link href="http://arxiv.org/abs/2105.10439"/>
        <updated>2021-05-24T05:08:41.001Z</updated>
        <summary type="html"><![CDATA[Sparse Bayesian learning (SBL) is a powerful framework for tackling the
sparse coding problem while also providing uncertainty quantification. However,
the most popular inference algorithms for SBL become too expensive for
high-dimensional problems due to the need to maintain a large covariance
matrix. To resolve this issue, we introduce a new SBL inference algorithm that
avoids explicit computation of the covariance matrix, thereby saving
significant time and space. Instead of performing costly matrix inversions, our
covariance-free method solves multiple linear systems to obtain provably
unbiased estimates of the posterior statistics needed by SBL. These systems can
be solved in parallel, enabling further acceleration of the algorithm via
graphics processing units. In practice, our method can be up to thousands of
times faster than existing baselines, reducing hours of computation time to
seconds. We showcase how our new algorithm enables SBL to tractably tackle
high-dimensional signal recovery problems, such as deconvolution of calcium
imaging data and multi-contrast reconstruction of magnetic resonance images.
Finally, we open-source a toolbox containing all of our implementations to
drive future research in SBL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1"&gt;Alexander Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_A/0/1/0/all/0/1"&gt;Andrew H. Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1"&gt;Berkin Bilgic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ba_D/0/1/0/all/0/1"&gt;Demba Ba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Sparse-Reward Object-Interaction Tasks in a First-person Simulated 3D Environment. (arXiv:2010.15195v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15195</id>
        <link href="http://arxiv.org/abs/2010.15195"/>
        <updated>2021-05-24T05:08:40.993Z</updated>
        <summary type="html"><![CDATA[First-person object-interaction tasks in high-fidelity, 3D, simulated
environments such as the AI2Thor virtual home-environment pose significant
sample-efficiency challenges for reinforcement learning (RL) agents learning
from sparse task rewards. To alleviate these challenges, prior work has
provided extensive supervision via a combination of reward-shaping,
ground-truth object-information, and expert demonstrations. In this work, we
show that one can learn object-interaction tasks from scratch without
supervision by learning an attentive object-model as an auxiliary task during
task learning with an object-centric relational RL agent. Our key insight is
that learning an object-model that incorporates object-attention into forward
prediction provides a dense learning signal for unsupervised representation
learning of both objects and their relationships. This, in turn, enables faster
policy learning for an object-centric relational RL agent. We demonstrate our
agent by introducing a set of challenging object-interaction tasks in the
AI2Thor environment where learning with our attentive object-model is key to
strong performance. Specifically, we compare our agent and relational RL agents
with alternative auxiliary tasks to a relational RL agent equipped with
ground-truth object-information, and show that learning with our object-model
best closes the performance gap in terms of both learning speed and maximum
success rate. Additionally, we find that incorporating object-attention into an
object-model's forward predictions is key to learning representations which
capture object-category and object-state.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1"&gt;Wilka Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_A/0/1/0/all/0/1"&gt;Anthony Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kimin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1"&gt;Sungryull Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_R/0/1/0/all/0/1"&gt;Richard L. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satinder Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Bounds of the Invariant Statistics in Machine Learning of Ergodic It\^o Diffusions. (arXiv:2105.10102v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10102</id>
        <link href="http://arxiv.org/abs/2105.10102"/>
        <updated>2021-05-24T05:08:40.984Z</updated>
        <summary type="html"><![CDATA[This paper studies the theoretical underpinnings of machine learning of
ergodic It\^o diffusions. The objective is to understand the convergence
properties of the invariant statistics when the underlying system of stochastic
differential equations (SDEs) is empirically estimated with a supervised
regression framework. Using the perturbation theory of ergodic Markov chains
and the linear response theory, we deduce a linear dependence of the errors of
one-point and two-point invariant statistics on the error in the learning of
the drift and diffusion coefficients. More importantly, our study shows that
the usual $L^2$-norm characterization of the learning generalization error is
insufficient for achieving this linear dependence result. We find that
sufficient conditions for such a linear dependence result are through learning
algorithms that produce a uniformly Lipschitz and consistent estimator in the
hypothesis space that retains certain characteristics of the drift
coefficients, such as the usual linear growth condition that guarantees the
existence of solutions of the underlying SDEs. We examine these conditions on
two well-understood learning algorithms: the kernel-based spectral regression
method and the shallow random neural networks with the ReLU activation
function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;He Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harlim_J/0/1/0/all/0/1"&gt;John Harlim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiantao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Halluci-Net: Scene Completion by Exploiting Object Co-occurrence Relationships. (arXiv:2004.08614v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08614</id>
        <link href="http://arxiv.org/abs/2004.08614"/>
        <updated>2021-05-24T05:08:40.959Z</updated>
        <summary type="html"><![CDATA[Recently, there has been substantial progress in image synthesis from
semantic labelmaps. However, methods used for this task assume the availability
of complete and unambiguous labelmaps, with instance boundaries of objects, and
class labels for each pixel. This reliance on heavily annotated inputs
restricts the application of image synthesis techniques to real-world
applications, especially under uncertainty due to weather, occlusion, or noise.
On the other hand, algorithms that can synthesize images from sparse labelmaps
or sketches are highly desirable as tools that can guide content creators and
artists to quickly generate scenes by simply specifying locations of a few
objects. In this paper, we address the problem of complex scene completion from
sparse labelmaps. Under this setting, very few details about the scene (30\% of
object instances) are available as input for image synthesis. We propose a
two-stage deep network based method, called `Halluci-Net', that learns
co-occurence relationships between objects in scenes, and then exploits these
relationships to produce a dense and complete labelmap. The generated dense
labelmap can then be used as input by state-of-the-art image synthesis
techniques like pix2pixHD to obtain the final image. The proposed method is
evaluated on the Cityscapes dataset and it outperforms two baselines methods on
performance metrics like Fr\'echet Inception Distance (FID), semantic
segmentation accuracy, and similarity in object co-occurrences. We also show
qualitative results on a subset of ADE20K dataset that contains bedroom images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1"&gt;Kuldeep Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1"&gt;Tejas Gokhale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rajhans Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1"&gt;Pavan Turaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aswin Sankaranarayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Recommender Systems at Scale: Communication-Efficient Model and Data Parallelism. (arXiv:2010.08899v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08899</id>
        <link href="http://arxiv.org/abs/2010.08899"/>
        <updated>2021-05-24T05:08:40.952Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider hybrid parallelism -- a paradigm that employs both
Data Parallelism (DP) and Model Parallelism (MP) -- to scale distributed
training of large recommendation models. We propose a compression framework
called Dynamic Communication Thresholding (DCT) for communication-efficient
hybrid training. DCT filters the entities to be communicated across the network
through a simple hard-thresholding function, allowing only the most relevant
information to pass through. For communication efficient DP, DCT compresses the
parameter gradients sent to the parameter server during model synchronization.
The threshold is updated only once every few thousand iterations to reduce the
computational overhead of compression. For communication efficient MP, DCT
incorporates a novel technique to compress the activations and gradients sent
across the network during the forward and backward propagation, respectively.
This is done by identifying and updating only the most relevant neurons of the
neural network for each training sample in the data. We evaluate DCT on
publicly available natural language processing and recommender models and
datasets, as well as recommendation systems used in production at Facebook. DCT
reduces communication by at least $100\times$ and $20\times$ during DP and MP,
respectively. The algorithm has been deployed in production, and it improves
end-to-end training time for a state-of-the-art industrial recommender model by
37\%, without any loss in performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vipul Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_D/0/1/0/all/0/1"&gt;Dhruv Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1"&gt;Ping Tak Peter Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiaohan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuzhen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kejariwal_A/0/1/0/all/0/1"&gt;Arun Kejariwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoolMomentum: A Method for Stochastic Optimization by Langevin Dynamics with Simulated Annealing. (arXiv:2005.14605v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.14605</id>
        <link href="http://arxiv.org/abs/2005.14605"/>
        <updated>2021-05-24T05:08:40.934Z</updated>
        <summary type="html"><![CDATA[Deep learning applications require global optimization of non-convex
objective functions, which have multiple local minima. The same problem is
often found in physical simulations and may be resolved by the methods of
Langevin dynamics with Simulated Annealing, which is a well-established
approach for minimization of many-particle potentials. This analogy provides
useful insights for non-convex stochastic optimization in machine learning.
Here we find that integration of the discretized Langevin equation gives a
coordinate updating rule equivalent to the famous Momentum optimization
algorithm. As a main result, we show that a gradual decrease of the momentum
coefficient from the initial value close to unity until zero is equivalent to
application of Simulated Annealing or slow cooling, in physical terms. Making
use of this novel approach, we propose CoolMomentum -- a new stochastic
optimization method. Applying Coolmomentum to optimization of Resnet-20 on
Cifar-10 dataset and Efficientnet-B0 on Imagenet, we demonstrate that it is
able to achieve high accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Borysenko_O/0/1/0/all/0/1"&gt;Oleksandr Borysenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Byshkin_M/0/1/0/all/0/1"&gt;Maksym Byshkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Horizon Forecasting for Limit Order Books: Novel Deep Learning Approaches and Hardware Acceleration using Intelligent Processing Units. (arXiv:2105.10430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10430</id>
        <link href="http://arxiv.org/abs/2105.10430"/>
        <updated>2021-05-24T05:08:40.927Z</updated>
        <summary type="html"><![CDATA[We design multi-horizon forecasting models for limit order book (LOB) data by
using deep learning techniques. Unlike standard structures where a single
prediction is made, we adopt encoder-decoder models with sequence-to-sequence
and Attention mechanisms, to generate a forecasting path. Our methods achieve
comparable performance to state-of-art algorithms at short prediction horizons.
Importantly, they outperform when generating predictions over long horizons by
leveraging the multi-horizon setup. Given that encoder-decoder models rely on
recurrent neural layers, they generally suffer from a slow training process. To
remedy this, we experiment with utilising novel hardware, so-called Intelligent
Processing Units (IPUs) produced by Graphcore. IPUs are specifically designed
for machine intelligence workload with the aim to speed up the computation
process. We show that in our setup this leads to significantly faster training
times when compared to training models with GPUs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online DR-Submodular Maximization with Stochastic Cumulative Constraints. (arXiv:2005.14708v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.14708</id>
        <link href="http://arxiv.org/abs/2005.14708"/>
        <updated>2021-05-24T05:08:40.916Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider online continuous DR-submodular maximization with
linear stochastic long-term constraints. Compared to the prior work on online
submodular maximization, our setting introduces the extra complication of
stochastic linear constraint functions that are i.i.d. generated at each round.
To be precise, at step $t\in\{1,\dots,T\}$, a DR-submodular utility function
$f_t(\cdot)$ and a constraint vector $p_t$, i.i.d. generated from an unknown
distribution with mean $p$, are revealed after committing to an action $x_t$
and we aim to maximize the overall utility while the expected cumulative
resource consumption $\sum_{t=1}^T \langle p,x_t\rangle$ is below a fixed
budget $B_T$. Stochastic long-term constraints arise naturally in applications
where there is a limited budget or resource available and resource consumption
at each step is governed by stochastically time-varying environments. We
propose the Online Lagrangian Frank-Wolfe (OLFW) algorithm to solve this class
of online problems. We analyze the performance of the OLFW algorithm and we
obtain sub-linear regret bounds as well as sub-linear cumulative constraint
violation bounds, both in expectation and with high probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Raut_P/0/1/0/all/0/1"&gt;Prasanna Sanjay Raut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sadeghi_O/0/1/0/all/0/1"&gt;Omid Sadeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fazel_M/0/1/0/all/0/1"&gt;Maryam Fazel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Effects of Linguistic Properties. (arXiv:2010.12919v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12919</id>
        <link href="http://arxiv.org/abs/2010.12919"/>
        <updated>2021-05-24T05:08:40.908Z</updated>
        <summary type="html"><![CDATA[We consider the problem of using observational data to estimate the causal
effects of linguistic properties. For example, does writing a complaint
politely lead to a faster response time? How much will a positive product
review increase sales? This paper addresses two technical challenges related to
the problem before developing a practical method. First, we formalize the
causal quantity of interest as the effect of a writer's intent, and establish
the assumptions necessary to identify this from observational data. Second, in
practice, we only have access to noisy proxies for the linguistic properties of
interest -- e.g., predictions from classifiers and lexicons. We propose an
estimator for this setting and prove that its bias is bounded when we perform
an adjustment for the text. Based on these results, we introduce TextCause, an
algorithm for estimating causal effects of linguistic properties. The method
leverages (1) distant supervision to improve the quality of noisy proxies, and
(2) a pre-trained language model (BERT) to adjust for the text. We show that
the proposed method outperforms related approaches when estimating the effect
of Amazon review sentiment on semi-simulated sales figures. Finally, we present
an applied case study investigating the effects of complaint politeness on
bureaucratic response times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1"&gt;Reid Pryzant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1"&gt;Dallas Card&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1"&gt;Dan Jurafsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1"&gt;Victor Veitch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridhar_D/0/1/0/all/0/1"&gt;Dhanya Sridhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Weight Initialization with Sylvester Solvers. (arXiv:2105.10335v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2105.10335</id>
        <link href="http://arxiv.org/abs/2105.10335"/>
        <updated>2021-05-24T05:08:40.901Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a data-driven scheme to initialize the parameters of
a deep neural network. This is in contrast to traditional approaches which
randomly initialize parameters by sampling from transformed standard
distributions. Such methods do not use the training data to produce a more
informed initialization. Our method uses a sequential layer-wise approach where
each layer is initialized using its input activations. The initialization is
cast as an optimization problem where we minimize a combination of encoding and
decoding losses of the input activations, which is further constrained by a
user-defined latent code. The optimization problem is then restructured into
the well-known Sylvester equation, which has fast and efficient gradient-free
solutions. Our data-driven method achieves a boost in performance compared to
random initialization methods, both before start of training and after training
is over. We show that our proposed method is especially effective in few-shot
and fine-tuning settings. We conclude this paper with analyses on time
complexity and the effect of different latent codes on the recognition
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Debasmit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1"&gt;Yash Bhalgat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1"&gt;Fatih Porikli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beware the Black-Box: on the Robustness of Recent Defenses to Adversarial Examples. (arXiv:2006.10876v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10876</id>
        <link href="http://arxiv.org/abs/2006.10876"/>
        <updated>2021-05-24T05:08:40.883Z</updated>
        <summary type="html"><![CDATA[Many defenses have recently been proposed at venues like NIPS, ICML, ICLR and
CVPR. These defenses are mainly focused on mitigating white-box attacks. They
do not properly examine black-box attacks. In this paper, we expand upon the
analysis of these defenses to include adaptive black-box adversaries. Our
evaluation is done on nine defenses including Barrage of Random Transforms,
ComDefend, Ensemble Diversity, Feature Distillation, The Odds are Odd, Error
Correcting Codes, Distribution Classifier Defense, K-Winner Take All and Buffer
Zones. Our investigation is done using two black-box adversarial models and six
widely studied adversarial attacks for CIFAR-10 and Fashion-MNIST datasets. Our
analyses show most recent defenses (7 out of 9) provide only marginal
improvements in security ($<25\%$), as compared to undefended networks. For
every defense, we also show the relationship between the amount of data the
adversary has at their disposal, and the effectiveness of adaptive black-box
attacks. Overall, our results paint a clear picture: defenses need both
thorough white-box and black-box analyses to be considered secure. We provide
this large scale study and analyses to motivate the field to move towards the
development of more robust black-box defenses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_K/0/1/0/all/0/1"&gt;Kaleel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevin_D/0/1/0/all/0/1"&gt;Deniz Gurevin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1"&gt;Marten van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Ha Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlated Input-Dependent Label Noise in Large-Scale Image Classification. (arXiv:2105.10305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10305</id>
        <link href="http://arxiv.org/abs/2105.10305"/>
        <updated>2021-05-24T05:08:40.876Z</updated>
        <summary type="html"><![CDATA[Large scale image classification datasets often contain noisy labels. We take
a principled probabilistic approach to modelling input-dependent, also known as
heteroscedastic, label noise in these datasets. We place a multivariate Normal
distributed latent variable on the final hidden layer of a neural network
classifier. The covariance matrix of this latent variable, models the aleatoric
uncertainty due to label noise. We demonstrate that the learned covariance
structure captures known sources of label noise between semantically similar
and co-occurring classes. Compared to standard neural network training and
other baselines, we show significantly improved accuracy on Imagenet ILSVRC
2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a
new state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These
datasets range from over 1M to over 300M training examples and from 1k classes
to more than 21k classes. Our method is simple to use, and we provide an
implementation that is a drop-in replacement for the final fully-connected
layer in a deep classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Collier_M/0/1/0/all/0/1"&gt;Mark Collier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1"&gt;Basil Mustafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokiopoulou_E/0/1/0/all/0/1"&gt;Efi Kokiopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berent_J/0/1/0/all/0/1"&gt;Jesse Berent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evening the Score: Targeting SARS-CoV-2 Protease Inhibition in Graph Generative Models for Therapeutic Candidates. (arXiv:2105.10489v1 [q-bio.BM])]]></title>
        <id>http://arxiv.org/abs/2105.10489</id>
        <link href="http://arxiv.org/abs/2105.10489"/>
        <updated>2021-05-24T05:08:40.870Z</updated>
        <summary type="html"><![CDATA[We examine a pair of graph generative models for the therapeutic design of
novel drug candidates targeting SARS-CoV-2 viral proteins. Due to a sense of
urgency, we chose well-validated models with unique strengths: an autoencoder
that generates molecules with similar structures to a dataset of drugs with
anti-SARS activity and a reinforcement learning algorithm that generates highly
novel molecules. During generation, we explore optimization toward several
design targets to balance druglikeness, synthetic accessability, and anti-SARS
activity based on \icfifty. This generative
framework\footnote{https://github.com/exalearn/covid-drug-design} will
accelerate drug discovery in future pandemics through the high-throughput
generation of targeted therapeutic candidates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bilbrey_J/0/1/0/all/0/1"&gt;Jenna Bilbrey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ward_L/0/1/0/all/0/1"&gt;Logan Ward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Choudhury_S/0/1/0/all/0/1"&gt;Sutanay Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sivaraman_G/0/1/0/all/0/1"&gt;Ganesh Sivaraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Detection of Abnormal EEGs in Epilepsy With a Compact and Efficient CNN Model. (arXiv:2105.10358v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2105.10358</id>
        <link href="http://arxiv.org/abs/2105.10358"/>
        <updated>2021-05-24T05:08:40.863Z</updated>
        <summary type="html"><![CDATA[Electroencephalography (EEG) is essential for the diagnosis of epilepsy, but
it requires expertise and experience to identify abnormalities. It is thus
crucial to develop automated models for the detection of abnormal EEGs related
to epilepsy. This paper describes the development of a novel class of compact
and efficient convolutional neural networks (CNNs) for detecting abnormal time
intervals and electrodes in EEGs for epilepsy. The designed model is inspired
by a CNN developed for brain-computer interfacing called multichannel EEGNet
(mEEGNet). Unlike the EEGNet, the proposed model, mEEGNet, has the same number
of electrode inputs and outputs to detect abnormalities. The mEEGNet was
evaluated with a clinical dataset consisting of 29 cases of juvenile and
childhood absence epilepsy labeled by a clinical expert. The labels were given
to paroxysmal discharges visually observed in both ictal (seizure) and
interictal (nonseizure) intervals. Results showed that the mEEGNet detected
abnormal EEGs with the area under the curve, F1-values, and sensitivity
equivalent to or higher than those of existing CNNs. Moreover, the number of
parameters is much smaller than other CNN models. To our knowledge, the dataset
of absence epilepsy validated with machine learning through this research is
the largest in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shoji_T/0/1/0/all/0/1"&gt;Taku Shoji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoshida_N/0/1/0/all/0/1"&gt;Noboru Yoshida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tanaka_T/0/1/0/all/0/1"&gt;Toshihisa Tanaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Understanding for Field and Service Robots in a Priori Unknown Environments. (arXiv:2105.10396v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10396</id>
        <link href="http://arxiv.org/abs/2105.10396"/>
        <updated>2021-05-24T05:08:40.831Z</updated>
        <summary type="html"><![CDATA[Contemporary approaches to perception, planning, estimation, and control have
allowed robots to operate robustly as our remote surrogates in uncertain,
unstructured environments. There is now an opportunity for robots to operate
not only in isolation, but also with and alongside humans in our complex
environments. Natural language provides an efficient and flexible medium
through which humans can communicate with collaborative robots. Through
significant progress in statistical methods for natural language understanding,
robots are now able to interpret a diverse array of free-form navigation,
manipulation, and mobile manipulation commands. However, most contemporary
approaches require a detailed prior spatial-semantic map of the robot's
environment that models the space of possible referents of the utterance.
Consequently, these methods fail when robots are deployed in new, previously
unknown, or partially observed environments, particularly when mental models of
the environment differ between the human operator and the robot. This paper
provides a comprehensive description of a novel learning framework that allows
field and service robots to interpret and correctly execute natural language
instructions in a priori unknown, unstructured environments. Integral to our
approach is its use of language as a "sensor" -- inferring spatial,
topological, and semantic information implicit in natural language utterances
and then exploiting this information to learn a distribution over a latent
environment model. We incorporate this distribution in a probabilistic language
grounding model and infer a distribution over a symbolic representation of the
robot's action space. We use imitation learning to identify a belief space
policy that reasons over the environment and behavior distributions. We
evaluate our framework through a variety of different navigation and mobile
manipulation experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1"&gt;Matthew R. Walter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patki_S/0/1/0/all/0/1"&gt;Siddharth Patki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniele_A/0/1/0/all/0/1"&gt;Andrea F. Daniele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fahnestock_E/0/1/0/all/0/1"&gt;Ethan Fahnestock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duvallet_F/0/1/0/all/0/1"&gt;Felix Duvallet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hemachandra_S/0/1/0/all/0/1"&gt;Sachithra Hemachandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stentz_A/0/1/0/all/0/1"&gt;Anthony Stentz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1"&gt;Nicholas Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howard_T/0/1/0/all/0/1"&gt;Thomas M. Howard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LDP-FL: Practical Private Aggregation in Federated Learning with Local Differential Privacy. (arXiv:2007.15789v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15789</id>
        <link href="http://arxiv.org/abs/2007.15789"/>
        <updated>2021-05-24T05:08:40.812Z</updated>
        <summary type="html"><![CDATA[Train machine learning models on sensitive user data has raised increasing
privacy concerns in many areas. Federated learning is a popular approach for
privacy protection that collects the local gradient information instead of real
data. One way to achieve a strict privacy guarantee is to apply local
differential privacy into federated learning. However, previous works do not
give a practical solution due to three issues. First, the noisy data is close
to its original value with high probability, increasing the risk of information
exposure. Second, a large variance is introduced to the estimated average,
causing poor accuracy. Last, the privacy budget explodes due to the high
dimensionality of weights in deep learning models. In this paper, we proposed a
novel design of local differential privacy mechanism for federated learning to
address the abovementioned issues. It is capable of making the data more
distinct from its original value and introducing lower variance. Moreover, the
proposed mechanism bypasses the curse of dimensionality by splitting and
shuffling model updates. A series of empirical evaluations on three commonly
used datasets, MNIST, Fashion-MNIST and CIFAR-10, demonstrate that our solution
can not only achieve superior deep learning performance but also provide a
strong privacy guarantee at the same time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jianwei Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum and Leaky Maximum Propagation. (arXiv:2105.10277v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10277</id>
        <link href="http://arxiv.org/abs/2105.10277"/>
        <updated>2021-05-24T05:08:40.800Z</updated>
        <summary type="html"><![CDATA[In this work, we present an alternative to conventional residual connections,
which is inspired by maxout nets. This means that instead of the addition in
residual connections, our approach only propagates the maximum value or, in the
leaky formulation, propagates a percentage of both. In our evaluation, we show
on different public data sets that the presented approaches are comparable to
the residual connections and have other interesting properties, such as better
generalization with a constant batch normalization, faster learning, and also
the possibility to generalize without additional activation functions. In
addition, the proposed approaches work very well if ensembles together with
residual networks are formed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1"&gt;Wolfgang Fuhl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Federated Learning in Phishing Email Detection. (arXiv:2007.13300v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13300</id>
        <link href="http://arxiv.org/abs/2007.13300"/>
        <updated>2021-05-24T05:08:40.795Z</updated>
        <summary type="html"><![CDATA[The use of Artificial Intelligence (AI) to detect phishing emails is
primarily dependent on large-scale centralized datasets, which opens it up to a
myriad of privacy, trust, and legal issues. Moreover, organizations are loathed
to share emails, given the risk of leakage of commercially sensitive
information. So, it is uncommon to obtain sufficient emails to train a global
AI model efficiently. Accordingly, privacy-preserving distributed and
collaborative machine learning, particularly Federated Learning (FL), is a
desideratum. Already prevalent in the healthcare sector, questions remain
regarding the effectiveness and efficacy of FL-based phishing detection within
the context of multi-organization collaborations. To the best of our knowledge,
the work herein is the first to investigate the use of FL in email
anti-phishing. This paper builds upon a deep neural network model, particularly
RNN and BERT for phishing email detection. It analyzes the FL-entangled
learning performance under various settings, including balanced and
asymmetrical data distribution. Our results corroborate comparable performance
statistics of FL in phishing email detection to centralized learning for
balanced datasets, and low organization counts. Moreover, we observe a
variation in performance when increasing organizational counts. For a fixed
total email dataset, the global RNN based model suffers by a 1.8% accuracy drop
when increasing organizational counts from 2 to 10. In contrast, BERT accuracy
rises by 0.6% when going from 2 to 5 organizations. However, if we allow
increasing the overall email dataset with the introduction of new organizations
in the FL framework, the organizational level performance is improved by
achieving a faster convergence speed. Besides, FL suffers in its overall global
model performance due to highly unstable outputs if the email dataset
distribution is highly asymmetric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thapa_C/0/1/0/all/0/1"&gt;Chandra Thapa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jun Wen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1"&gt;Alsharif Abuadbba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yansong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camtepe_S/0/1/0/all/0/1"&gt;Seyit Camtepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1"&gt;Surya Nepal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almashor_M/0/1/0/all/0/1"&gt;Mahathir Almashor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yifeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Generative Adversarial Networks via stochastic Nash games. (arXiv:2010.10013v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10013</id>
        <link href="http://arxiv.org/abs/2010.10013"/>
        <updated>2021-05-24T05:08:40.782Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are a class of generative models with
two antagonistic neural networks: a generator and a discriminator. These two
neural networks compete against each other through an adversarial process that
can be modeled as a stochastic Nash equilibrium problem. Since the associated
training process is challenging, it is fundamental to design reliable
algorithms to compute an equilibrium. In this paper, we propose a stochastic
relaxed forward-backward (SRFB) algorithm for GANs and we show convergence to
an exact solution when an increasing number of data is available. We also show
convergence of an averaged variant of the SRFB algorithm to a neighborhood of
the solution when only few samples are available. In both cases, convergence is
guaranteed when the pseudogradient mapping of the game is monotone. This
assumption is among the weakest known in the literature. Moreover, we apply our
algorithm to the image generation problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Franci_B/0/1/0/all/0/1"&gt;Barbara Franci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grammatico_S/0/1/0/all/0/1"&gt;Sergio Grammatico&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining. (arXiv:2105.10419v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10419</id>
        <link href="http://arxiv.org/abs/2105.10419"/>
        <updated>2021-05-24T05:08:40.774Z</updated>
        <summary type="html"><![CDATA[Existing models of multilingual sentence embeddings require large parallel
data resources which are not available for low-resource languages. We propose a
novel unsupervised method to derive multilingual sentence embeddings relying
only on monolingual data. We first produce a synthetic parallel corpus using
unsupervised machine translation, and use it to fine-tune a pretrained
cross-lingual masked language model (XLM) to derive the multilingual sentence
representations. The quality of the representations is evaluated on two
parallel corpus mining tasks with improvements of up to 22 F1 points over
vanilla XLM. In addition, we observe that a single synthetic bilingual corpus
is able to improve results for other language pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvapilikova_I/0/1/0/all/0/1"&gt;Ivana Kvapil&amp;#x131;kova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1"&gt;Gorka Labaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1"&gt;Eneko Agirre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Bojar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Knowledge Expansion. (arXiv:2103.14431v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14431</id>
        <link href="http://arxiv.org/abs/2103.14431"/>
        <updated>2021-05-24T05:08:40.756Z</updated>
        <summary type="html"><![CDATA[The popularity of multimodal sensors and the accessibility of the Internet
have brought us a massive amount of unlabeled multimodal data. Since existing
datasets and well-trained models are primarily unimodal, the modality gap
between a unimodal network and unlabeled multimodal data poses an interesting
problem: how to transfer a pre-trained unimodal network to perform the same
task on unlabeled multimodal data? In this work, we propose multimodal
knowledge expansion (MKE), a knowledge distillation-based framework to
effectively utilize multimodal data without requiring labels. Opposite to
traditional knowledge distillation, where the student is designed to be
lightweight and inferior to the teacher, we observe that a multimodal student
model consistently denoises pseudo labels and generalizes better than its
teacher. Extensive experiments on four tasks and different modalities verify
this finding. Furthermore, we connect the mechanism of MKE to semi-supervised
learning and offer both empirical and theoretical explanations to understand
the denoising capability of a multimodal student.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Do What Doesn't Matter: Intrinsic Motivation with Action Usefulness. (arXiv:2105.09992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09992</id>
        <link href="http://arxiv.org/abs/2105.09992"/>
        <updated>2021-05-24T05:08:40.748Z</updated>
        <summary type="html"><![CDATA[Sparse rewards are double-edged training signals in reinforcement learning:
easy to design but hard to optimize. Intrinsic motivation guidances have thus
been developed toward alleviating the resulting exploration problem. They
usually incentivize agents to look for new states through novelty signals. Yet,
such methods encourage exhaustive exploration of the state space rather than
focusing on the environment's salient interaction opportunities. We propose a
new exploration method, called Don't Do What Doesn't Matter (DoWhaM), shifting
the emphasis from state novelty to state with relevant actions. While most
actions consistently change the state when used, \textit{e.g.} moving the
agent, some actions are only effective in specific states, \textit{e.g.},
\emph{opening} a door, \emph{grabbing} an object. DoWhaM detects and rewards
actions that seldom affect the environment. We evaluate DoWhaM on the
procedurally-generated environment MiniGrid, against state-of-the-art methods
and show that DoWhaM greatly reduces sample complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seurin_M/0/1/0/all/0/1"&gt;Mathieu Seurin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1"&gt;Florian Strub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1"&gt;Philippe Preux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1"&gt;Olivier Pietquin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-05-24T05:08:40.726Z</updated>
        <summary type="html"><![CDATA[Camera movement and unpredictable environmental conditions like dust and wind
induce noise into video feeds. We observe that popular unsupervised MOT methods
are dependent on noise-free conditions. We show that the addition of a small
amount of artificial random noise causes a sharp degradation in model
performance on benchmark metrics. We resolve this problem by introducing a
robust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed
single-head attention model helps limit the negative impact of noise by
learning visual representations at different segment scales. AttU-Net shows
better unsupervised MOT tracking performance over variational inference-based
state-of-the-art baselines. We evaluate our method in the MNIST and the Atari
game video benchmark. We also provide two extended video datasets consisting of
complex visual patterns that include Kuzushiji characters and fashion images to
validate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakam_T/0/1/0/all/0/1"&gt;Tomokazu Murakam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction. (arXiv:2105.10446v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10446</id>
        <link href="http://arxiv.org/abs/2105.10446"/>
        <updated>2021-05-24T05:08:40.710Z</updated>
        <summary type="html"><![CDATA[This work attempts to provide a plausible theoretical framework that aims to
interpret modern deep (convolutional) networks from the principles of data
compression and discriminative representation. We show that for
high-dimensional multi-class data, the optimal linear discriminative
representation maximizes the coding rate difference between the whole dataset
and the average of all the subsets. We show that the basic iterative gradient
ascent scheme for optimizing the rate reduction objective naturally leads to a
multi-layer deep network, named ReduNet, that shares common characteristics of
modern deep networks. The deep layered architectures, linear and nonlinear
operators, and even parameters of the network are all explicitly constructed
layer-by-layer via forward propagation, instead of learned via back
propagation. All components of so-obtained "white-box" network have precise
optimization, statistical, and geometric interpretation. Moreover, all linear
operators of the so-derived network naturally become multi-channel convolutions
when we enforce classification to be rigorously shift-invariant. The derivation
also indicates that such a deep convolution network is significantly more
efficient to construct and learn in the spectral domain. Our preliminary
simulations and experiments clearly verify the effectiveness of both the rate
reduction objective and the associated ReduNet. All code and data are available
at https://github.com/Ma-Lab-Berkeley.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Kwan Ho Ryan Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haozhi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1"&gt;John Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial-Temporal Conv-sequence Learning with Accident Encoding for Traffic Flow Prediction. (arXiv:2105.10478v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10478</id>
        <link href="http://arxiv.org/abs/2105.10478"/>
        <updated>2021-05-24T05:08:40.703Z</updated>
        <summary type="html"><![CDATA[In intelligent transportation system, the key problem of traffic forecasting
is how to extract the periodic temporal dependencies and complex spatial
correlation. Current state-of-the-art methods for traffic flow prediction are
based on graph architectures and sequence learning models, but they do not
fully exploit spatial-temporal dynamic information in traffic system.
Specifically, the temporal dependence of short-range is diluted by recurrent
neural networks, and existing sequence model ignores local spatial information
because the convolution operation uses global average pooling. Besides, there
will be some traffic accidents during the transitions of objects causing
congestion in the real world that trigger increased prediction deviation. To
overcome these challenges, we propose the Spatial-Temporal Conv-sequence
Learning (STCL), in which a focused temporal block uses unidirectional
convolution to effectively capture short-term periodic temporal dependence, and
a spatial-temporal fusion module is able to extract the dependencies of both
interactions and decrease the feature dimensions. Moreover, the accidents
features impact on local traffic congestion and position encoding is employed
to detect anomalies in complex traffic situations. We conduct extensive
experiments on large-scale real-world tasks and verify the effectiveness of our
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zichuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hongbo Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations. (arXiv:1804.07209v4 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1804.07209</id>
        <link href="http://arxiv.org/abs/1804.07209"/>
        <updated>2021-05-24T05:08:40.619Z</updated>
        <summary type="html"><![CDATA[This paper introduces Non-Autonomous Input-Output Stable Network(NAIS-Net), a
very deep architecture where each stacked processing block is derived from a
time-invariant non-autonomous dynamical system. Non-autonomy is implemented by
skip connections from the block input to each of the unrolled processing stages
and allows stability to be enforced so that blocks can be unrolled adaptively
to a pattern-dependent processing depth. NAIS-Net induces non-trivial,
Lipschitz input-output maps, even for an infinite unroll length. We prove that
the network is globally asymptotically stable so that for every initial
condition there is exactly one input-dependent equilibrium assuming $tanh$
units, and incrementally stable for ReL units. An efficient implementation that
enforces the stability under derived conditions for both fully-connected and
convolutional layers is also presented. Experimental results show how NAIS-Net
exhibits stability in practice, yielding a significant reduction in
generalization gap compared to ResNets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1"&gt;Marco Ciccone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallieri_M/0/1/0/all/0/1"&gt;Marco Gallieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masci_J/0/1/0/all/0/1"&gt;Jonathan Masci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osendorfer_C/0/1/0/all/0/1"&gt;Christian Osendorfer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_F/0/1/0/all/0/1"&gt;Faustino Gomez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Statistical Inference for Parameters Estimation with Linear-Equality Constraints. (arXiv:2105.10315v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.10315</id>
        <link href="http://arxiv.org/abs/2105.10315"/>
        <updated>2021-05-24T05:08:40.607Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) and projected stochastic gradient descent
(PSGD) are scalable algorithms to compute model parameters in unconstrained and
constrained optimization problems. In comparison with stochastic gradient
descent (SGD), PSGD forces its iterative values into the constrained parameter
space via projection. The convergence rate of PSGD-type estimates has been
exhaustedly studied, while statistical properties such as asymptotic
distribution remain less explored. From a purely statistical point of view,
this paper studies the limiting distribution of PSGD-based estimate when the
true parameters satisfying some linear-equality constraints. Our theoretical
findings reveal the role of projection played in the uncertainty of the PSGD
estimate. As a byproduct, we propose an online hypothesis testing procedure to
test the linear-equality constraints. Simulation studies on synthetic data and
an application to a real-world dataset confirm our theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruiqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Mingao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shang_Z/0/1/0/all/0/1"&gt;Zuofeng Shang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Machine Learning with Prior Knowledge: An Overview. (arXiv:2105.10172v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10172</id>
        <link href="http://arxiv.org/abs/2105.10172"/>
        <updated>2021-05-24T05:08:40.595Z</updated>
        <summary type="html"><![CDATA[This survey presents an overview of integrating prior knowledge into machine
learning systems in order to improve explainability. The complexity of machine
learning models has elicited research to make them more explainable. However,
most explainability methods cannot provide insight beyond the given data,
requiring additional information about the context. We propose to harness prior
knowledge to improve upon the explanation capabilities of machine learning
models. In this paper, we present a categorization of current research into
three main categories which either integrate knowledge into the machine
learning pipeline, into the explainability method or derive knowledge from
explanations. To classify the papers, we build upon the existing taxonomy of
informed machine learning and extend it from the perspective of explainability.
We conclude with open challenges and research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beckh_K/0/1/0/all/0/1"&gt;Katharina Beckh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1"&gt;Sebastian M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakobs_M/0/1/0/all/0/1"&gt;Matthias Jakobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toborek_V/0/1/0/all/0/1"&gt;Vanessa Toborek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hanxiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_R/0/1/0/all/0/1"&gt;Raphael Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welke_P/0/1/0/all/0/1"&gt;Pascal Welke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houben_S/0/1/0/all/0/1"&gt;Sebastian Houben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueden_L/0/1/0/all/0/1"&gt;Laura von Rueden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BELT: Blockwise Missing Embedding Learning Transfomer. (arXiv:2105.10360v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.10360</id>
        <link href="http://arxiv.org/abs/2105.10360"/>
        <updated>2021-05-24T05:08:40.586Z</updated>
        <summary type="html"><![CDATA[Matrix completion has attracted a lot of attention in many fields including
statistics, applied mathematics and electrical engineering. Most of works focus
on the independent sampling models under which the individual observed entries
are sampled independently. Motivated by applications in the integration of
multiple (point-wise mutual information) PMI matrices, we propose the model
{\bf B}lockwise missing {\bf E}mbedding {\bf L}earning {\bf T}ransformer (BELT)
to treat row-wise/column-wise missingness. Specifically, our proposed method
aims at efficient matrix recovery when every pair of matrices from multiple
sources has an overlap. We provide theoretical justification for the proposed
BELT method. Simulation studies show that the method performs well in finite
sample under a variety of configurations. The method is applied to integrate
several PMI matrices built by EHR data and Chinese medical text data, which
enables us to construct a comprehensive embedding set for CUI and Chinese with
high quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Doudou Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianxi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lu_J/0/1/0/all/0/1"&gt;Junwei Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unit Test Case Generation with Transformers and Focal Context. (arXiv:2009.05617v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05617</id>
        <link href="http://arxiv.org/abs/2009.05617"/>
        <updated>2021-05-24T05:08:40.578Z</updated>
        <summary type="html"><![CDATA[Automated unit test case generation tools facilitate test-driven development
and support developers by suggesting tests intended to identify flaws in their
code. Existing approaches are usually guided by the test coverage criteria,
generating synthetic test cases that are often difficult for developers to read
or understand. In this paper we propose AthenaTest, an approach that aims to
generate unit test cases by learning from real-world focal methods and
developer-written testcases. We formulate unit test case generation as a
sequence-to-sequence learning task, adopting a two-step training procedure
consisting of denoising pretraining on a large unsupervised Java corpus, and
supervised finetuning for a downstream translation task of generating unit
tests. We investigate the impact of natural language and source code
pretraining, as well as the focal context information surrounding the focal
method. Both techniques provide improvements in terms of validation loss, with
pretraining yielding 25% relative improvement and focal context providing
additional 11.1% improvement. We also introduce Methods2Test, the largest
publicly available supervised parallel corpus of unit test case methods and
corresponding focal methods in Java, which comprises 780K test cases mined from
91K open-source repositories from GitHub. We evaluate AthenaTest on five
defects4j projects, generating 25K passing test cases covering 43.7% of the
focal methods with only 30 attempts. We execute the test cases, collect test
coverage information, and compare them with test cases generated by EvoSuite
and GPT-3, finding that our approach outperforms GPT-3 and has comparable
coverage w.r.t. EvoSuite. Finally, we survey professional developers on their
preference in terms of readability, understandability, and testing
effectiveness of the generated tests, showing overwhelmingly preference towards
AthenaTest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tufano_M/0/1/0/all/0/1"&gt;Michele Tufano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1"&gt;Dawn Drain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1"&gt;Alexey Svyatkovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shao Kun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1"&gt;Neel Sundaresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy-based Discovery of Summary Causal Graphs in Time Series. (arXiv:2105.10381v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.10381</id>
        <link href="http://arxiv.org/abs/2105.10381"/>
        <updated>2021-05-24T05:08:40.560Z</updated>
        <summary type="html"><![CDATA[We address in this study the problem of learning a summary causal graph on
time series with potentially different sampling rates. To do so, we first
propose a new temporal mutual information measure defined on a window-based
representation of time series. We then show how this measure relates to an
entropy reduction principle that can be seen as a special case of the
Probabilistic Raising Principle. We finally combine these two ingredients in a
PC-like algorithm to construct the summary causal graph. This algorithm is
evaluated on several datasets that shows both its efficacy and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assaad_K/0/1/0/all/0/1"&gt;Karim Assaad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devijver_E/0/1/0/all/0/1"&gt;Emilie Devijver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaussier_E/0/1/0/all/0/1"&gt;Eric Gaussier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ait_Bachir_A/0/1/0/all/0/1"&gt;Ali Ait-Bachir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization. (arXiv:1908.06077v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.06077</id>
        <link href="http://arxiv.org/abs/1908.06077"/>
        <updated>2021-05-24T05:08:40.555Z</updated>
        <summary type="html"><![CDATA[As the size and complexity of models and datasets grow, so does the need for
communication-efficient variants of stochastic gradient descent that can be
deployed to perform parallel model training. One popular
communication-compression method for data-parallel SGD is QSGD (Alistarh et
al., 2017), which quantizes and encodes gradients to reduce communication
costs. The baseline variant of QSGD provides strong theoretical guarantees,
however, for practical purposes, the authors proposed a heuristic variant which
we call QSGDinf, which demonstrated impressive empirical gains for distributed
training of large neural networks. In this paper, we build on this work to
propose a new gradient quantization scheme, and show that it has both stronger
theoretical guarantees than QSGD, and matches and exceeds the empirical
performance of the QSGDinf heuristic and of other compression methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramezani_Kebrya_A/0/1/0/all/0/1"&gt;Ali Ramezani-Kebrya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1"&gt;Fartash Faghri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Ilya Markov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aksenov_V/0/1/0/all/0/1"&gt;Vitalii Aksenov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1"&gt;Dan Alistarh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1"&gt;Daniel M. Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Precise Performance Analysis of Support Vector Regression. (arXiv:2105.10373v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10373</id>
        <link href="http://arxiv.org/abs/2105.10373"/>
        <updated>2021-05-24T05:08:40.533Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the hard and soft support vector regression
techniques applied to a set of $n$ linear measurements of the form
$y_i=\boldsymbol{\beta}_\star^{T}{\bf x}_i +n_i$ where
$\boldsymbol{\beta}_\star$ is an unknown vector, $\left\{{\bf
x}_i\right\}_{i=1}^n$ are the feature vectors and
$\left\{{n}_i\right\}_{i=1}^n$ model the noise. Particularly, under some
plausible assumptions on the statistical distribution of the data, we
characterize the feasibility condition for the hard support vector regression
in the regime of high dimensions and, when feasible, derive an asymptotic
approximation for its risk. Similarly, we study the test risk for the soft
support vector regression as a function of its parameters. Our results are then
used to optimally tune the parameters intervening in the design of hard and
soft support vector regression algorithms. Based on our analysis, we illustrate
that adding more samples may be harmful to the test performance of support
vector regression, while it is always beneficial when the parameters are
optimally selected. Such a result reminds a similar phenomenon observed in
modern learning architectures according to which optimally tuned architectures
present a decreasing test performance curve with respect to the number of
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sifaou_H/0/1/0/all/0/1"&gt;Houssem Sifaou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+kammoun_A/0/1/0/all/0/1"&gt;Abla kammoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alouini_M/0/1/0/all/0/1"&gt;Mohamed-Slim Alouini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Error Bound for Hyperbolic Ordinal Embedding. (arXiv:2105.10475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10475</id>
        <link href="http://arxiv.org/abs/2105.10475"/>
        <updated>2021-05-24T05:08:40.494Z</updated>
        <summary type="html"><![CDATA[Hyperbolic ordinal embedding (HOE) represents entities as points in
hyperbolic space so that they agree as well as possible with given constraints
in the form of entity i is more similar to entity j than to entity k. It has
been experimentally shown that HOE can obtain representations of hierarchical
data such as a knowledge base and a citation network effectively, owing to
hyperbolic space's exponential growth property. However, its theoretical
analysis has been limited to ideal noiseless settings, and its generalization
error in compensation for hyperbolic space's exponential representation ability
has not been guaranteed. The difficulty is that existing generalization error
bound derivations for ordinal embedding based on the Gramian matrix do not work
in HOE, since hyperbolic space is not inner-product space. In this paper,
through our novel characterization of HOE with decomposed Lorentz Gramian
matrices, we provide a generalization error bound of HOE for the first time,
which is at most exponential with respect to the embedding space's radius. Our
comparison between the bounds of HOE and Euclidean ordinal embedding shows that
HOE's generalization error is reasonable as a cost for its exponential
representation ability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_A/0/1/0/all/0/1"&gt;Atsushi Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nitanda_A/0/1/0/all/0/1"&gt;Atsushi Nitanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Linchuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavazza_M/0/1/0/all/0/1"&gt;Marc Cavazza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamanishi_K/0/1/0/all/0/1"&gt;Kenji Yamanishi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stance Detection with BERT Embeddings for Credibility Analysis of Information on Social Media. (arXiv:2105.10272v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.10272</id>
        <link href="http://arxiv.org/abs/2105.10272"/>
        <updated>2021-05-24T05:08:40.476Z</updated>
        <summary type="html"><![CDATA[The evolution of electronic media is a mixed blessing. Due to the easy
access, low cost, and faster reach of the information, people search out and
devour news from online social networks. In contrast, the increasing acceptance
of social media reporting leads to the spread of fake news. This is a minacious
problem that causes disputes and endangers societal stability and harmony. Fake
news spread has gained attention from researchers due to its vicious nature.
proliferation of misinformation in all media, from the internet to cable news,
paid advertising and local news outlets, has made it essential for people to
identify the misinformation and sort through the facts. Researchers are trying
to analyze the credibility of information and curtail false information on such
platforms. Credibility is the believability of the piece of information at
hand. Analyzing the credibility of fake news is challenging due to the intent
of its creation and the polychromatic nature of the news. In this work, we
propose a model for detecting fake news. Our method investigates the content of
the news at the early stage i.e. when the news is published but is yet to be
disseminated through social media. Our work interprets the content with
automatic feature extraction and the relevance of the text pieces. In summary,
we introduce stance as one of the features along with the content of the
article and employ the pre-trained contextualized word embeddings BERT to
obtain the state-of-art results for fake news detection. The experiment
conducted on the real-world dataset indicates that our model outperforms the
previous work and enables fake news detection with an accuracy of 95.32%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karande_H/0/1/0/all/0/1"&gt;Hema Karande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walambe_R/0/1/0/all/0/1"&gt;Rahee Walambe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benjamin_V/0/1/0/all/0/1"&gt;Victor Benjamin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotecha_K/0/1/0/all/0/1"&gt;Ketan Kotecha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghu_T/0/1/0/all/0/1"&gt;T. S. Raghu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Quantum Classifiers Through the Lens of the Hessian. (arXiv:2105.10162v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2105.10162</id>
        <link href="http://arxiv.org/abs/2105.10162"/>
        <updated>2021-05-24T05:08:40.469Z</updated>
        <summary type="html"><![CDATA[In quantum computing, the variational quantum algorithms (VQAs) are well
suited for finding optimal combinations of things in specific applications
ranging from chemistry all the way to finance. The training of VQAs with
gradient descent optimization algorithm has shown a good convergence. At an
early stage, the simulation of variational quantum circuits on noisy
intermediate-scale quantum (NISQ) devices suffers from noisy outputs. Just like
classical deep learning, it also suffers from vanishing gradient problems. It
is a realistic goal to study the topology of loss landscape, to visualize the
curvature information and trainability of these circuits in the existence of
vanishing gradients. In this paper, we calculated the Hessian and visualized
the loss landscape of variational quantum classifiers at different points in
parameter space. The curvature information of variational quantum classifiers
(VQC) is interpreted and the loss function's convergence is shown. It helps us
better understand the behavior of variational quantum circuits to tackle
optimization problems efficiently. We investigated the variational quantum
classifiers via Hessian on quantum computers, started with a simple 4-bit
parity problem to gain insight into the practical behavior of Hessian, then
thoroughly analyzed the behavior of Hessian's eigenvalues on training the
variational quantum classifier for the Diabetes dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sen_P/0/1/0/all/0/1"&gt;Pinaki Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bhatia_A/0/1/0/all/0/1"&gt;Amandeep Singh Bhatia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Quantile Networks: Uncertainty-Aware Reinforcement Learning with Applications in Autonomous Driving. (arXiv:2105.10266v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10266</id>
        <link href="http://arxiv.org/abs/2105.10266"/>
        <updated>2021-05-24T05:08:40.462Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) can be used to create a decision-making agent for
autonomous driving. However, previous approaches provide only black-box
solutions, which do not offer information on how confident the agent is about
its decisions. An estimate of both the aleatoric and epistemic uncertainty of
the agent's decisions is fundamental for real-world applications of autonomous
driving. Therefore, this paper introduces the Ensemble Quantile Networks (EQN)
method, which combines distributional RL with an ensemble approach, to obtain a
complete uncertainty estimate. The distribution over returns is estimated by
learning its quantile function implicitly, which gives the aleatoric
uncertainty, whereas an ensemble of agents is trained on bootstrapped data to
provide a Bayesian estimation of the epistemic uncertainty. A criterion for
classifying which decisions that have an unacceptable uncertainty is also
introduced. The results show that the EQN method can balance risk and time
efficiency in different occluded intersection scenarios, by considering the
estimated aleatoric uncertainty. Furthermore, it is shown that the trained
agent can use the epistemic uncertainty information to identify situations that
the agent has not been trained for and thereby avoid making unfounded,
potentially dangerous, decisions outside of the training distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoel_C/0/1/0/all/0/1"&gt;Carl-Johan Hoel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolff_K/0/1/0/all/0/1"&gt;Krister Wolff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laine_L/0/1/0/all/0/1"&gt;Leo Laine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behind the leaves -- Estimation of occluded grapevine berries with conditional generative adversarial networks. (arXiv:2105.10325v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10325</id>
        <link href="http://arxiv.org/abs/2105.10325"/>
        <updated>2021-05-24T05:08:40.438Z</updated>
        <summary type="html"><![CDATA[The need for accurate yield estimates for viticulture is becoming more
important due to increasing competition in the wine market worldwide. One of
the most promising methods to estimate the harvest is berry counting, as it can
be approached non-destructively, and its process can be automated. In this
article, we present a method that addresses the challenge of occluded berries
with leaves to obtain a more accurate estimate of the number of berries that
will enable a better estimate of the harvest. We use generative adversarial
networks, a deep learning-based approach that generates a likely scenario
behind the leaves exploiting learned patterns from images with non-occluded
berries. Our experiments show that the estimate of the number of berries after
applying our method is closer to the manually counted reference. In contrast to
applying a factor to the berry count, our approach better adapts to local
conditions by directly involving the appearance of the visible berries.
Furthermore, we show that our approach can identify which areas in the image
should be changed by adding new berries without explicitly requiring
information about hidden areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1"&gt;Jana Kierdorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1"&gt;Immanuel Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kicherer_A/0/1/0/all/0/1"&gt;Anna Kicherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zabawa_L/0/1/0/all/0/1"&gt;Laura Zabawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drees_L/0/1/0/all/0/1"&gt;Lukas Drees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1"&gt;Ribana Roscher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word-level Text Highlighting of Medical Texts forTelehealth Services. (arXiv:2105.10400v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10400</id>
        <link href="http://arxiv.org/abs/2105.10400"/>
        <updated>2021-05-24T05:08:40.432Z</updated>
        <summary type="html"><![CDATA[The medical domain is often subject to information overload. The digitization
of healthcare, constant updates to online medical repositories, and increasing
availability of biomedical datasets make it challenging to effectively analyze
the data. This creates additional work for medical professionals who are
heavily dependent on medical data to complete their research and consult their
patients. This paper aims to show how different text highlighting techniques
can capture relevant medical context. This would reduce the doctors' cognitive
load and response time to patients by facilitating them in making faster
decisions, thus improving the overall quality of online medical services. Three
different word-level text highlighting methodologies are implemented and
evaluated. The first method uses TF-IDF scores directly to highlight important
parts of the text. The second method is a combination of TF-IDF scores and the
application of Local Interpretable Model-Agnostic Explanations to
classification models. The third method uses neural networks directly to make
predictions on whether or not a word should be highlighted. The results of our
experiments show that the neural network approach is successful in highlighting
medically-relevant terms and its performance is improved as the size of the
input segment increases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozyegen_O/0/1/0/all/0/1"&gt;Ozan Ozyegen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabe_D/0/1/0/all/0/1"&gt;Devika Kabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cevik_M/0/1/0/all/0/1"&gt;Mucahit Cevik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Definite Non-Ancestral Relations and Structure Learning. (arXiv:2105.10350v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10350</id>
        <link href="http://arxiv.org/abs/2105.10350"/>
        <updated>2021-05-24T05:08:40.424Z</updated>
        <summary type="html"><![CDATA[In causal graphical models based on directed acyclic graphs (DAGs), directed
paths represent causal pathways between the corresponding variables. The
variable at the beginning of such a path is referred to as an ancestor of the
variable at the end of the path. Ancestral relations between variables play an
important role in causal modeling. In existing literature on structure
learning, these relations are usually deduced from learned structures and used
for orienting edges or formulating constraints of the space of possible DAGs.
However, they are usually not posed as immediate target of inference. In this
work we investigate the graphical characterization of ancestral relations via
CPDAGs and d-separation relations. We propose a framework that can learn
definite non-ancestral relations without first learning the skeleton. This
frame-work yields structural information that can be used in both score- and
constraint-based algorithms to learn causal DAGs more efficiently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drton_M/0/1/0/all/0/1"&gt;Mathias Drton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shojaie_A/0/1/0/all/0/1"&gt;Ali Shojaie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distinguishing artefacts: evaluating the saturation point of convolutional neural networks. (arXiv:2105.10448v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10448</id>
        <link href="http://arxiv.org/abs/2105.10448"/>
        <updated>2021-05-24T05:08:40.405Z</updated>
        <summary type="html"><![CDATA[Prior work has shown Convolutional Neural Networks (CNNs) trained on
surrogate Computer Aided Design (CAD) models are able to detect and classify
real-world artefacts from photographs. The applications of which support
twinning of digital and physical assets in design, including rapid extraction
of part geometry from model repositories, information search \& retrieval and
identifying components in the field for maintenance, repair, and recording. The
performance of CNNs in classification tasks have been shown dependent on
training data set size and number of classes. Where prior works have used
relatively small surrogate model data sets ($<100$ models), the question
remains as to the ability of a CNN to differentiate between models in
increasingly large model repositories. This paper presents a method for
generating synthetic image data sets from online CAD model repositories, and
further investigates the capacity of an off-the-shelf CNN architecture trained
on synthetic data to classify models as class size increases. 1,000 CAD models
were curated and processed to generate large scale surrogate data sets,
featuring model coverage at steps of 10$^{\circ}$, 30$^{\circ}$, 60$^{\circ}$,
and 120$^{\circ}$ degrees. The findings demonstrate the capability of computer
vision algorithms to classify artefacts in model repositories of up to 200,
beyond this point the CNN's performance is observed to deteriorate
significantly, limiting its present ability for automated twinning of physical
to digital artefacts. Although, a match is more often found in the top-5
results showing potential for information search and retrieval on large
repositories of surrogate models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Real_R/0/1/0/all/0/1"&gt;Ric Real&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopsill_J/0/1/0/all/0/1"&gt;James Gopsill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1"&gt;David Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snider_C/0/1/0/all/0/1"&gt;Chris Snider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hicks_B/0/1/0/all/0/1"&gt;Ben Hicks&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Instrumental Variable Regression for Deep Offline Policy Evaluation. (arXiv:2105.10148v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10148</id>
        <link href="http://arxiv.org/abs/2105.10148"/>
        <updated>2021-05-24T05:08:40.397Z</updated>
        <summary type="html"><![CDATA[We show that the popular reinforcement learning (RL) strategy of estimating
the state-action value (Q-function) by minimizing the mean squared Bellman
error leads to a regression problem with confounding, the inputs and output
noise being correlated. Hence, direct minimization of the Bellman error can
result in significantly biased Q-function estimates. We explain why fixing the
target Q-network in Deep Q-Networks and Fitted Q Evaluation provides a way of
overcoming this confounding, thus shedding new light on this popular but not
well understood trick in the deep RL literature. An alternative approach to
address confounding is to leverage techniques developed in the causality
literature, notably instrumental variables (IV). We bring together here the
literature on IV and RL by investigating whether IV approaches can lead to
improved Q-function estimates. This paper analyzes and compares a wide range of
recent IV methods in the context of offline policy evaluation (OPE), where the
goal is to estimate the value of a policy using logged data only. By applying
different IV techniques to OPE, we are not only able to recover previously
proposed OPE methods such as model-based techniques but also to obtain
competitive new techniques. We find empirically that state-of-the-art OPE
methods are closely matched in performance by some IV methods such as AGMM,
which were not developed for OPE. We open-source all our code and datasets at
https://github.com/liyuan9988/IVOPEwithACME.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yutian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1"&gt;Caglar Gulcehre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paine_T/0/1/0/all/0/1"&gt;Tom Le Paine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1"&gt;Nando de Freitas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AC-CovidNet: Attention Guided Contrastive CNN for Recognition of Covid-19 in Chest X-Ray Images. (arXiv:2105.10239v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10239</id>
        <link href="http://arxiv.org/abs/2105.10239"/>
        <updated>2021-05-24T05:08:40.383Z</updated>
        <summary type="html"><![CDATA[Covid-19 global pandemic continues to devastate health care systems across
the world. In many countries, the 2nd wave is very severe. Economical and rapid
testing, as well as diagnosis, is urgently needed to control the pandemic. At
present, the Covid-19 testing is costly and time-consuming. Chest X-Ray (CXR)
testing can be the fastest, scalable, and non-invasive method. The existing
methods suffer due to the limited CXR samples available from Covid-19. Thus,
inspired by the limitations of the open-source work in this field, we propose
attention guided contrastive CNN architecture (AC-CovidNet) for Covid-19
detection in CXR images. The proposed method learns the robust and
discriminative features with the help of contrastive loss. Moreover, the
proposed method gives more importance to the infected regions as guided by the
attention mechanism. We compute the sensitivity of the proposed method over the
publicly available Covid-19 dataset. It is observed that the proposed
AC-CovidNet exhibits very promising performance as compared to the existing
methods even with limited training data. It can tackle the bottleneck of CXR
Covid-19 datasets being faced by the researchers. The code used in this paper
is released publicly at \url{https://github.com/shivram1987/AC-CovidNet/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ambati_A/0/1/0/all/0/1"&gt;Anirudh Ambati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dubey_S/0/1/0/all/0/1"&gt;Shiv Ram Dubey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Gaussian equivalence of generative models for learning with shallow neural networks. (arXiv:2006.14709v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14709</id>
        <link href="http://arxiv.org/abs/2006.14709"/>
        <updated>2021-05-24T05:08:40.355Z</updated>
        <summary type="html"><![CDATA[Understanding the impact of data structure on the computational tractability
of learning is a key challenge for the theory of neural networks. Many
theoretical works do not explicitly model training data, or assume that inputs
are drawn component-wise independently from some simple probability
distribution. Here, we go beyond this simple paradigm by studying the
performance of neural networks trained on data drawn from pre-trained
generative models. This is possible due to a Gaussian equivalence stating that
the key metrics of interest, such as the training and test errors, can be fully
captured by an appropriately chosen Gaussian model. We provide three strands of
rigorous, analytical and numerical evidence corroborating this equivalence.
First, we establish rigorous conditions for the Gaussian equivalence to hold in
the case of single-layer generative models, as well as deterministic rates for
convergence in distribution. Second, we leverage this equivalence to derive a
closed set of equations describing the generalisation performance of two widely
studied machine learning problems: two-layer neural networks trained using
one-pass stochastic gradient descent, and full-batch pre-learned features or
kernel methods. Finally, we perform experiments demonstrating how our theory
applies to deep, pre-trained generative models. These results open a viable
path to the theoretical study of machine learning models with realistic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1"&gt;Sebastian Goldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Loureiro_B/0/1/0/all/0/1"&gt;Bruno Loureiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Reeves_G/0/1/0/all/0/1"&gt;Galen Reeves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Krzakala_F/0/1/0/all/0/1"&gt;Florent Krzakala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mezard_M/0/1/0/all/0/1"&gt;Marc M&amp;#xe9;zard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zdeborova_L/0/1/0/all/0/1"&gt;Lenka Zdeborov&amp;#xe1;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Non-Linear Structural Probe. (arXiv:2105.10185v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10185</id>
        <link href="http://arxiv.org/abs/2105.10185"/>
        <updated>2021-05-24T05:08:40.348Z</updated>
        <summary type="html"><![CDATA[Probes are models devised to investigate the encoding of knowledge -- e.g.
syntactic structure -- in contextual representations. Probes are often designed
for simplicity, which has led to restrictions on probe design that may not
allow for the full exploitation of the structure of encoded information; one
such restriction is linearity. We examine the case of a structural probe
(Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic
structure in contextual representations through learning only linear
transformations. By observing that the structural probe learns a metric, we are
able to kernelize it and develop a novel non-linear variant with an identical
number of parameters. We test on 6 languages and find that the radial-basis
function (RBF) kernel, in conjunction with regularization, achieves a
statistically significant improvement over the baseline in all languages --
implying that at least part of the syntactic knowledge is encoded non-linearly.
We conclude by discussing how the RBF kernel resembles BERT's self-attention
layers and speculate that this resemblance leads to the RBF-based probe's
stronger performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1"&gt;Jennifer C. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1"&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1"&gt;Naomi Saphra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Mining -- Past, Present and Future. (arXiv:2105.10077v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10077</id>
        <link href="http://arxiv.org/abs/2105.10077"/>
        <updated>2021-05-24T05:08:40.331Z</updated>
        <summary type="html"><![CDATA[Anomaly mining is an important problem that finds numerous applications in
various real world domains such as environmental monitoring, cybersecurity,
finance, healthcare and medicine, to name a few. In this article, I focus on
two areas, (1) point-cloud and (2) graph-based anomaly mining. I aim to present
a broad view of each area, and discuss classes of main research problems,
recent trends and future directions. I conclude with key take-aways and
overarching open problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1"&gt;Leman Akoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks. (arXiv:2105.10190v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10190</id>
        <link href="http://arxiv.org/abs/2105.10190"/>
        <updated>2021-05-24T05:08:40.312Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) are trained using stochastic gradient
descent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam)
optimizer has become very popular due to its adaptive momentum, which tackles
the dying gradient problem of SGD. Nevertheless, existing optimizers are still
unable to exploit the optimization curvature information efficiently. This
paper proposes a new AngularGrad optimizer that considers the behavior of the
direction/angle of consecutive gradients. This is the first attempt in the
literature to exploit the gradient angular information apart from its
magnitude. The proposed AngularGrad generates a score to control the step size
based on the gradient angular information of previous iterations. Thus, the
optimization steps become smoother as a more accurate step size of immediate
past gradients is captured through the angular information. Two variants of
AngularGrad are developed based on the use of Tangent or Cosine functions for
computing the gradient angular information. Theoretically, AngularGrad exhibits
the same regret bound as Adam for convergence purposes. Nevertheless, extensive
experiments conducted on benchmark data sets against state-of-the-art methods
reveal a superior performance of AngularGrad. The source code will be made
publicly available at: https://github.com/mhaut/AngularGrad.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;S.K. Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paoletti_M/0/1/0/all/0/1"&gt;M.E. Paoletti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haut_J/0/1/0/all/0/1"&gt;J.M. Haut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1"&gt;S.R. Dubey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;P. Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1"&gt;A. Plaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_B/0/1/0/all/0/1"&gt;B.B. Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Robust Misclassifications of Neural Networks to Enhance Adversarial Attacks. (arXiv:2105.10304v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10304</id>
        <link href="http://arxiv.org/abs/2105.10304"/>
        <updated>2021-05-24T05:08:40.277Z</updated>
        <summary type="html"><![CDATA[Progress in making neural networks more robust against adversarial attacks is
mostly marginal, despite the great efforts of the research community. Moreover,
the robustness evaluation is often imprecise, making it difficult to identify
promising approaches. We analyze the classification decisions of 19 different
state-of-the-art neural networks trained to be robust against adversarial
attacks. Our findings suggest that current untargeted adversarial attacks
induce misclassification towards only a limited amount of different classes.
Additionally, we observe that both over- and under-confidence in model
predictions result in an inaccurate assessment of model robustness. Based on
these observations, we propose a novel loss function for adversarial attacks
that consistently improves attack success rate compared to prior loss functions
for 19 out of 19 analyzed models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwinn_L/0/1/0/all/0/1"&gt;Leo Schwinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raab_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Raab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;An Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1"&gt;Dario Zanca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1"&gt;Bjoern Eskofier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Have you tried Neural Topic Models? Comparative Analysis of Neural and Non-Neural Topic Models with Application to COVID-19 Twitter Data. (arXiv:2105.10165v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10165</id>
        <link href="http://arxiv.org/abs/2105.10165"/>
        <updated>2021-05-24T05:08:40.269Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used in studying social phenomena. We conduct a
comparative study examining state-of-the-art neural versus non-neural topic
models, performing a rigorous quantitative and qualitative assessment on a
dataset of tweets about the COVID-19 pandemic. Our results show that not only
do neural topic models outperform their classical counterparts on standard
evaluation metrics, but they also produce more coherent topics, which are of
great benefit when studying complex social problems. We also propose a novel
regularization term for neural topic models, which is designed to address the
well-documented problem of mode collapse, and demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1"&gt;Andrew Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1"&gt;Nga Than&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rule Augmented Unsupervised Constituency Parsing. (arXiv:2105.10193v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10193</id>
        <link href="http://arxiv.org/abs/2105.10193"/>
        <updated>2021-05-24T05:08:40.263Z</updated>
        <summary type="html"><![CDATA[Recently, unsupervised parsing of syntactic trees has gained considerable
attention. A prototypical approach to such unsupervised parsing employs
reinforcement learning and auto-encoders. However, no mechanism ensures that
the learnt model leverages the well-understood language grammar. We propose an
approach that utilizes very generic linguistic knowledge of the language
present in the form of syntactic rules, thus inducing better syntactic
structures. We introduce a novel formulation that takes advantage of the
syntactic grammar rules and is independent of the base system. We achieve new
state-of-the-art results on two benchmarks datasets, MNLI and WSJ. The source
code of the paper is available at https://github.com/anshuln/Diora_with_rules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sahay_A/0/1/0/all/0/1"&gt;Atul Sahay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1"&gt;Anshul Nasery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1"&gt;Ayush Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN pretraining for deep convolutional autoencoders applied to Software-based Fingerprint Presentation Attack Detection. (arXiv:2105.10213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10213</id>
        <link href="http://arxiv.org/abs/2105.10213"/>
        <updated>2021-05-24T05:08:40.241Z</updated>
        <summary type="html"><![CDATA[The need for reliable systems to determine fingerprint presentation attacks
grows with the rising use of the fingerprint for authentication. This work
presents a new approach to single-class classification for software-based
fingerprint presentation attach detection. The described method utilizes a
Wasserstein GAN to apply transfer learning to a deep convolutional autoencoder.
By doing so, the autoencoder could be pretrained and finetuned on the
LivDet2021 Dermalog sensor dataset with only 1122 bona fide training samples.
Without making use of any presentation attack samples, the model could archive
an average classification error rate of 16.79%. The Wasserstein GAN implemented
to pretrain the autoencoders weights can further be used to generate
realistic-looking artificial fingerprint patches. Extensive testing of
different autoencoder architectures and hyperparameters led to coarse
architectural guidelines as well as multiple implementations which can be
utilized for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rohrer_T/0/1/0/all/0/1"&gt;Tobias Rohrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolberg_J/0/1/0/all/0/1"&gt;Jascha Kolberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trimming Feature Extraction and Inference for MCU-based Edge NILM: a Systematic Approach. (arXiv:2105.10302v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10302</id>
        <link href="http://arxiv.org/abs/2105.10302"/>
        <updated>2021-05-24T05:08:40.168Z</updated>
        <summary type="html"><![CDATA[Non-Intrusive Load Monitoring (NILM) enables the disaggregation of the global
power consumption of multiple loads, taken from a single smart electrical
meter, into appliance-level details. State-of-the-Art approaches are based on
Machine Learning methods and exploit the fusion of time- and frequency-domain
features from current and voltage sensors. Unfortunately, these methods are
compute-demanding and memory-intensive. Therefore, running low-latency NILM on
low-cost, resource-constrained MCU-based meters is currently an open challenge.
This paper addresses the optimization of the feature spaces as well as the
computational and storage cost reduction needed for executing State-of-the-Art
(SoA) NILM algorithms on memory- and compute-limited MCUs. We compare four
supervised learning techniques on different classification scenarios and
characterize the overall NILM pipeline's implementation on a MCU-based Smart
Measurement Node. Experimental results demonstrate that optimizing the feature
space enables edge MCU-based NILM with 95.15% accuracy, resulting in a small
drop compared to the most-accurate feature vector deployment (96.19%) while
achieving up to 5.45x speed-up and 80.56% storage reduction. Furthermore, we
show that low-latency NILM relying only on current measurements reaches almost
80% accuracy, allowing a major cost reduction by removing voltage sensors from
the hardware design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tabanelli_E/0/1/0/all/0/1"&gt;Enrico Tabanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1"&gt;Davide Brunelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acquaviva_A/0/1/0/all/0/1"&gt;Andrea Acquaviva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma. (arXiv:2105.10238v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10238</id>
        <link href="http://arxiv.org/abs/2105.10238"/>
        <updated>2021-05-24T05:08:40.152Z</updated>
        <summary type="html"><![CDATA[Pelvic ring disruptions result from blunt injury mechanisms and are often
found in patients with multi-system trauma. To grade pelvic fracture severity
in trauma victims based on whole-body CT, the Tile AO/OTA classification is
frequently used. Due to the high volume of whole-body trauma CTs generated in
busy trauma centers, an automated approach to Tile classification would provide
substantial value, e.,g., to prioritize the reading queue of the attending
trauma radiologist. In such scenario, an automated method should perform
grading based on a transparent process and based on interpretable features to
enable interaction with human readers and lower their workload by offering
insights from a first automated read of the scan. This paper introduces an
automated yet interpretable pelvic trauma decision support system to assist
radiologists in fracture detection and Tile grade classification. The method
operates similarly to human interpretation of CT scans and first detects
distinct pelvic fractures on CT with high specificity using a Faster-RCNN model
that are then interpreted using a structural causal model based on clinical
best practices to infer an initial Tile grade. The Bayesian causal model and
finally, the object detector are then queried for likely co-occurring fractures
that may have been rejected initially due to the highly specific operating
point of the detector, resulting in an updated list of detected fractures and
corresponding final Tile grade. Our method is transparent in that it provides
finding location and type using the object detector, as well as information on
important counterfactuals that would invalidate the system's recommendation and
achieves an AUC of 83.3%/85.1% for translational/rotational instability.
Despite being designed for human-machine teaming, our approach does not
compromise on performance compared to previous black-box approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zapaishchykova_A/0/1/0/all/0/1"&gt;Anna Zapaishchykova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dreizin_D/0/1/0/all/0/1"&gt;David Dreizin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhaoshuo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jie Ying Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roohi_S/0/1/0/all/0/1"&gt;Shahrooz Faghih Roohi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Unberath_M/0/1/0/all/0/1"&gt;Mathias Unberath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yes We Care! -- Certification for Machine Learning Methods through the Care Label Framework. (arXiv:2105.10197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10197</id>
        <link href="http://arxiv.org/abs/2105.10197"/>
        <updated>2021-05-24T05:08:40.117Z</updated>
        <summary type="html"><![CDATA[Machine learning applications have become ubiquitous. Their applications from
machine embedded control in production over process optimization in diverse
areas (e.g., traffic, finance, sciences) to direct user interactions like
advertising and recommendations. This has led to an increased effort of making
machine learning trustworthy. Explainable and fair AI have already matured.
They address knowledgeable users and application engineers. However, there are
users that want to deploy a learned model in a similar way as their washing
machine. These stakeholders do not want to spend time understanding the model.
Instead, they want to rely on guaranteed properties. What are the relevant
properties? How can they be expressed to stakeholders without presupposing
machine learning knowledge? How can they be guaranteed for a certain
implementation of a model? These questions move far beyond the current
state-of-the-art and we want to address them here. We propose a unified
framework that certifies learning methods via care labels. They are easy to
understand and draw inspiration from well-known certificates like textile
labels or property cards of electronic devices. Our framework considers both,
the machine learning theory and a given implementation. We test the
implementation's compliance with theoretical properties and bounds. In this
paper, we illustrate care labels by a prototype implementation of a
certification suite for a selection of probabilistic graphical models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morik_K/0/1/0/all/0/1"&gt;Katharina Morik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1"&gt;Helena Kotthaus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heppe_L/0/1/0/all/0/1"&gt;Lukas Heppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinrich_D/0/1/0/all/0/1"&gt;Danny Heinrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_R/0/1/0/all/0/1"&gt;Raphael Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mucke_S/0/1/0/all/0/1"&gt;Sascha M&amp;#xfc;cke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauly_A/0/1/0/all/0/1"&gt;Andreas Pauly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakobs_M/0/1/0/all/0/1"&gt;Matthias Jakobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piatkowski_N/0/1/0/all/0/1"&gt;Nico Piatkowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certification of Iterative Predictions in Bayesian Neural Networks. (arXiv:2105.10134v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10134</id>
        <link href="http://arxiv.org/abs/2105.10134"/>
        <updated>2021-05-24T05:08:40.074Z</updated>
        <summary type="html"><![CDATA[We consider the problem of computing reach-avoid probabilities for iterative
predictions made with Bayesian neural network (BNN) models. Specifically, we
leverage bound propagation techniques and backward recursion to compute lower
bounds for the probability that trajectories of the BNN model reach a given set
of states while avoiding a set of unsafe states. We use the lower bounds in the
context of control and reinforcement learning to provide safety certification
for given control policies, as well as to synthesize control policies that
improve the certification bounds. On a set of benchmarks, we demonstrate that
our framework can be employed to certify policies over BNNs predictions for
problems of more than $10$ dimensions, and to effectively synthesize policies
that significantly increase the lower bound on the satisfaction probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1"&gt;Matthew Wicker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laurenti_L/0/1/0/all/0/1"&gt;Luca Laurenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patane_A/0/1/0/all/0/1"&gt;Andrea Patane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paoletti_N/0/1/0/all/0/1"&gt;Nicola Paoletti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1"&gt;Alessandro Abate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1"&gt;Marta Kwiatkowska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A GAN-Like Approach for Physics-Based Imitation Learning and Interactive Character Control. (arXiv:2105.10066v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2105.10066</id>
        <link href="http://arxiv.org/abs/2105.10066"/>
        <updated>2021-05-24T05:08:40.063Z</updated>
        <summary type="html"><![CDATA[We present a simple and intuitive approach for interactive control of
physically simulated characters. Our work builds upon generative adversarial
networks (GAN) and reinforcement learning, and introduces an imitation learning
framework where an ensemble of classifiers and an imitation policy are trained
in tandem given pre-processed reference clips. The classifiers are trained to
discriminate the reference motion from the motion generated by the imitation
policy, while the policy is rewarded for fooling the discriminators. Using our
GAN-based approach, multiple motor control policies can be trained separately
to imitate different behaviors. In runtime, our system can respond to external
control signal provided by the user and interactively switch between different
policies. Compared to existing methods, our proposed approach has the following
attractive properties: 1) achieves state-of-the-art imitation performance
without manually designing and fine tuning a reward function; 2) directly
controls the character without having to track any target reference pose
explicitly or implicitly through a phase state; and 3) supports interactive
policy switching without requiring any motion generation or motion matching
mechanism. We highlight the applicability of our approach in a range of
imitation and interactive control tasks, while also demonstrating its ability
to withstand external perturbations as well as to recover balance. Overall, our
approach generates high-fidelity motion, has low runtime cost, and can be
easily integrated into interactive applications and games.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Pei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1"&gt;Ioannis Karamouzas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safety Metrics for Semantic Segmentation in Autonomous Driving. (arXiv:2105.10142v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10142</id>
        <link href="http://arxiv.org/abs/2105.10142"/>
        <updated>2021-05-24T05:08:40.037Z</updated>
        <summary type="html"><![CDATA[Within the context of autonomous driving, safety-related metrics for deep
neural networks have been widely studied for image classification and object
detection. In this paper, we further consider safety-aware correctness and
robustness metrics specialized for semantic segmentation. The novelty of our
proposal is to move beyond pixel-level metrics: Given two images with each
having N pixels being class-flipped, the designed metrics should, depending on
the clustering of pixels being class-flipped or the location of occurrence,
reflect a different level of safety criticality. The result evaluated on an
autonomous driving dataset demonstrates the validity and practicality of our
proposed methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chih-Hong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1"&gt;Hsuan-Cheng Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Cross-Sectional Currency Strategies by Ranking Refinement with Transformer-based Architectures. (arXiv:2105.10019v1 [q-fin.PM])]]></title>
        <id>http://arxiv.org/abs/2105.10019</id>
        <link href="http://arxiv.org/abs/2105.10019"/>
        <updated>2021-05-24T05:08:40.020Z</updated>
        <summary type="html"><![CDATA[The performance of a cross-sectional currency strategy depends crucially on
accurately ranking instruments prior to portfolio construction. While this
ranking step is traditionally performed using heuristics, or by sorting outputs
produced by pointwise regression or classification models, Learning to Rank
algorithms have recently presented themselves as competitive and viable
alternatives. Despite improving ranking accuracy on average however, these
techniques do not account for the possibility that assets positioned at the
extreme ends of the ranked list -- which are ultimately used to construct the
long/short portfolios -- can assume different distributions in the input space,
and thus lead to sub-optimal strategy performance. Drawing from research in
Information Retrieval that demonstrates the utility of contextual information
embedded within top-ranked documents to learn the query's characteristics to
improve ranking, we propose an analogous approach: exploiting the features of
both out- and under-performing instruments to learn a model for refining the
original ranked list. Under a re-ranking framework, we adapt the Transformer
architecture to encode the features of extreme assets for refining our
selection of long/short instruments obtained with an initial retrieval.
Backtesting on a set of 31 currencies, our proposed methodology significantly
boosts Sharpe ratios -- by approximately 20% over the original LTR algorithms
and double that of traditional baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Poh_D/0/1/0/all/0/1"&gt;Daniel Poh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Lim_B/0/1/0/all/0/1"&gt;Bryan Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Stein Discrepancy Descent. (arXiv:2105.09994v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09994</id>
        <link href="http://arxiv.org/abs/2105.09994"/>
        <updated>2021-05-24T05:08:40.003Z</updated>
        <summary type="html"><![CDATA[Among dissimilarities between probability distributions, the Kernel Stein
Discrepancy (KSD) has received much interest recently. We investigate the
properties of its Wasserstein gradient flow to approximate a target probability
distribution $\pi$ on $\mathbb{R}^d$, known up to a normalization constant.
This leads to a straightforwardly implementable, deterministic score-based
method to sample from $\pi$, named KSD Descent, which uses a set of particles
to approximate $\pi$. Remarkably, owing to a tractable loss function, KSD
Descent can leverage robust parameter-free optimization schemes such as L-BFGS;
this contrasts with other popular particle-based schemes such as the Stein
Variational Gradient Descent algorithm. We study the convergence properties of
KSD Descent and demonstrate its practical relevance. However, we also highlight
failure cases by showing that the algorithm can get stuck in spurious local
minima.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Korba_A/0/1/0/all/0/1"&gt;Anna Korba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aubin_Frankowski_P/0/1/0/all/0/1"&gt;Pierre-Cyril Aubin-Frankowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Majewski_S/0/1/0/all/0/1"&gt;Szymon Majewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1"&gt;Pierre Ablin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TestRank: Bringing Order into Unlabeled Test Instances for Deep Learning Tasks. (arXiv:2105.10113v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10113</id>
        <link href="http://arxiv.org/abs/2105.10113"/>
        <updated>2021-05-24T05:08:39.980Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) has achieved unprecedented success in a variety of tasks.
However, DL systems are notoriously difficult to test and debug due to the lack
of explainability of DL models and the huge test input space to cover.
Generally speaking, it is relatively easy to collect a massive amount of test
data, but the labeling cost can be quite high. Consequently, it is essential to
conduct test selection and label only those selected "high quality"
bug-revealing test inputs for test cost reduction.

In this paper, we propose a novel test prioritization technique that brings
order into the unlabeled test instances according to their bug-revealing
capabilities, namely TestRank. Different from existing solutions, TestRank
leverages both intrinsic attributes and contextual attributes of test instances
when prioritizing them. To be specific, we first build a similarity graph on
test instances and training samples, and we conduct graph-based semi-supervised
learning to extract contextual features. Then, for a particular test instance,
the contextual features extracted from the graph neural network (GNN) and the
intrinsic features obtained with the DL model itself are combined to predict
its bug-revealing probability. Finally, TestRank prioritizes unlabeled test
instances in descending order of the above probability value. We evaluate the
performance of TestRank on a variety of image classification datasets.
Experimental results show that the debugging efficiency of our method
significantly outperforms existing test prioritization techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Min Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1"&gt;Qiuxia Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yannan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comment on Stochastic Polyak Step-Size: Performance of ALI-G. (arXiv:2105.10011v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10011</id>
        <link href="http://arxiv.org/abs/2105.10011"/>
        <updated>2021-05-24T05:08:39.973Z</updated>
        <summary type="html"><![CDATA[This is a short note on the performance of the ALI-G algorithm (Berrada et
al., 2020) as reported in (Loizou et al., 2021). ALI-G (Berrada et al., 2020)
and SPS (Loizou et al., 2021) are both adaptations of the Polyak step-size to
optimize machine learning models that can interpolate the training data. The
main algorithmic differences are that (1) SPS employs a multiplicative constant
in the denominator of the learning-rate while ALI-G uses an additive constant,
and (2) SPS uses an iteration-dependent maximal learning-rate while ALI-G uses
a constant one. There are also differences in the analysis provided by the two
works, with less restrictive assumptions proposed in (Loizou et al., 2021). In
their experiments, (Loizou et al., 2021) did not use momentum for ALI-G (which
is a standard part of the algorithm) or standard hyper-parameter tuning (for
e.g. learning-rate and regularization). Hence this note as a reference for the
improved performance that ALI-G can obtain with well-chosen hyper-parameters.
In particular, we show that when training a ResNet-34 on CIFAR-10 and
CIFAR-100, the performance of ALI-G can reach respectively 93.5% (+6%) and 76%
(+8%) with a very small amount of tuning. Thus ALI-G remains a very competitive
method for training interpolating neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berrada_L/0/1/0/all/0/1"&gt;Leonard Berrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping Saddle Points with Compressed SGD. (arXiv:2105.10090v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10090</id>
        <link href="http://arxiv.org/abs/2105.10090"/>
        <updated>2021-05-24T05:08:39.954Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) is a prevalent optimization technique for
large-scale distributed machine learning. While SGD computation can be
efficiently divided between multiple machines, communication typically becomes
a bottleneck in the distributed setting. Gradient compression methods can be
used to alleviate this problem, and a recent line of work shows that SGD
augmented with gradient compression converges to an $\varepsilon$-first-order
stationary point. In this paper we extend these results to convergence to an
$\varepsilon$-second-order stationary point ($\varepsilon$-SOSP), which is to
the best of our knowledge the first result of this type. In addition, we show
that, when the stochastic gradient is not Lipschitz, compressed SGD with
RandomK compressor converges to an $\varepsilon$-SOSP with the same number of
iterations as uncompressed SGD [Jin et al.,2021] (JACM), while improving the
total communication by a factor of $\tilde \Theta(\sqrt{d}
\varepsilon^{-3/4})$, where $d$ is the dimension of the optimization problem.
We present additional results for the cases when the compressor is arbitrary
and when the stochastic gradient is Lipschitz.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avdiukhin_D/0/1/0/all/0/1"&gt;Dmitrii Avdiukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaroslavtsev_G/0/1/0/all/0/1"&gt;Grigory Yaroslavtsev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XGBoost energy consumption prediction based on multi-system data HVAC. (arXiv:2105.09945v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09945</id>
        <link href="http://arxiv.org/abs/2105.09945"/>
        <updated>2021-05-24T05:08:39.944Z</updated>
        <summary type="html"><![CDATA[The energy consumption of the HVAC system accounts for a significant portion
of the energy consumption of the public building system, and using an efficient
energy consumption prediction model can assist it in carrying out effective
energy-saving transformation. Unlike the traditional energy consumption
prediction model, this paper extracts features from large data sets using
XGBoost, trains them separately to obtain multiple models, then fuses them with
LightGBM's independent prediction results using MAE, infers energy consumption
related variables, and successfully applies this model to the self-developed
Internet of Things platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunlong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yiming Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dengzheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_Y/0/1/0/all/0/1"&gt;Yingan Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_Z/0/1/0/all/0/1"&gt;Zhengrong Ruan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Free Knowledge Distillation for Heterogeneous Federated Learning. (arXiv:2105.10056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10056</id>
        <link href="http://arxiv.org/abs/2105.10056"/>
        <updated>2021-05-24T05:08:39.924Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a decentralized machine-learning paradigm, in
which a global server iteratively averages the model parameters of local users
without accessing their data. User heterogeneity has imposed significant
challenges to FL, which can incur drifted global models that are slow to
converge. Knowledge Distillation has recently emerged to tackle this issue, by
refining the server model using aggregated knowledge from heterogeneous users,
other than directly averaging their model parameters. This approach, however,
depends on a proxy dataset, making it impractical unless such a prerequisite is
satisfied. Moreover, the ensemble knowledge is not fully utilized to guide
local model learning, which may in turn affect the quality of the aggregated
model. Inspired by the prior art, we propose a data-free knowledge
distillation} approach to address heterogeneous FL, where the server learns a
lightweight generator to ensemble user information in a data-free manner, which
is then broadcasted to users, regulating local training using the learned
knowledge as an inductive bias. Empirical studies powered by theoretical
implications show that, our approach facilitates FL with better generalization
performance using fewer communication rounds, compared with the
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhuangdi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Junyuan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiayu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Probabilistic Approach to Neural Network Pruning. (arXiv:2105.10065v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10065</id>
        <link href="http://arxiv.org/abs/2105.10065"/>
        <updated>2021-05-24T05:08:39.917Z</updated>
        <summary type="html"><![CDATA[Neural network pruning techniques reduce the number of parameters without
compromising predicting ability of a network. Many algorithms have been
developed for pruning both over-parameterized fully-connected networks (FCNs)
and convolutional neural networks (CNNs), but analytical studies of
capabilities and compression ratios of such pruned sub-networks are lacking. We
theoretically study the performance of two pruning techniques (random and
magnitude-based) on FCNs and CNNs. Given a target network {whose weights are
independently sampled from appropriate distributions}, we provide a universal
approach to bound the gap between a pruned and the target network in a
probabilistic sense. The results establish that there exist pruned networks
with expressive power within any specified bound from the target network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xin Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1"&gt;Diego Klabjan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Robustness over High Level Driving Instruction for Autonomous Driving. (arXiv:2105.10014v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10014</id>
        <link href="http://arxiv.org/abs/2105.10014"/>
        <updated>2021-05-24T05:08:39.892Z</updated>
        <summary type="html"><![CDATA[In recent years, we have witnessed increasingly high performance in the field
of autonomous end-to-end driving. In particular, more and more research is
being done on driving in urban environments, where the car has to follow high
level commands to navigate. However, few evaluations are made on the ability of
these agents to react in an unexpected situation. Specifically, no evaluations
are conducted on the robustness of driving agents in the event of a bad
high-level command. We propose here an evaluation method, namely a benchmark
that allows to assess the robustness of an agent, and to appreciate its
understanding of the environment through its ability to keep a safe behavior,
regardless of the instruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carton_F/0/1/0/all/0/1"&gt;Florence Carton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1"&gt;David Filliat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabarisoa_J/0/1/0/all/0/1"&gt;Jaonary Rabarisoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1"&gt;Quoc Cuong Pham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Modular Robot Control Policies. (arXiv:2105.10049v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10049</id>
        <link href="http://arxiv.org/abs/2105.10049"/>
        <updated>2021-05-24T05:08:39.886Z</updated>
        <summary type="html"><![CDATA[To make a modular robotic system both capable and scalable, the controller
must be equally as modular as the mechanism. Given the large number of designs
that can be generated from even a small set of modules, it becomes impractical
to create a new system-wide controller for each design. Instead, we construct a
modular control policy that handles a broad class of designs. We take the view
that a module is both form and function, i.e. both mechanism and controller. As
the modules are physically re-configured, the policy automatically
re-configures to match the kinematic structure. This novel policy is trained
with a new model-based reinforcement learning algorithm, which interleaves
model learning and trajectory optimization to guide policy learning for
multiple designs simultaneously. Training the policy on a varied set of designs
teaches it how to adapt its behavior to the design. We show that the policy can
then generalize to a larger set of designs not seen during training. We
demonstrate one policy controlling many designs with different combinations of
legs and wheels to locomote both in simulation and on real robots.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitman_J/0/1/0/all/0/1"&gt;Julian Whitman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Travers_M/0/1/0/all/0/1"&gt;Matthew Travers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choset_H/0/1/0/all/0/1"&gt;Howie Choset&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10013</id>
        <link href="http://arxiv.org/abs/2105.10013"/>
        <updated>2021-05-24T05:08:39.877Z</updated>
        <summary type="html"><![CDATA[Image classification methods are usually trained to perform predictions
taking into account a predefined group of known classes. Real-world problems,
however, may not allow for a full knowledge of the input and label spaces,
making failures in recognition a hazard to deep visual learning. Open set
recognition methods are characterized by the ability to correctly identifying
inputs of known and unknown classes. In this context, we propose GeMOS: simple
and plug-and-play open set recognition modules that can be attached to
pretrained Deep Neural Networks for visual recognition. The GeMOS framework
pairs pre-trained Convolutional Neural Networks with generative models for open
set recognition to extract open set scores for each sample, allowing for
failure recognition in object recognition tasks. We conduct a thorough
evaluation of the proposed method in comparison with state-of-the-art open set
algorithms, finding that GeMOS either outperforms or is statistically
indistinguishable from more complex and costly models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1"&gt;Marcos Vendramini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1"&gt;Alexei Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide & Deep neural network model for patch aggregation in CNN-based prostate cancer detection systems. (arXiv:2105.09974v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09974</id>
        <link href="http://arxiv.org/abs/2105.09974"/>
        <updated>2021-05-24T05:08:39.858Z</updated>
        <summary type="html"><![CDATA[Prostate cancer (PCa) is one of the most commonly diagnosed cancer and one of
the leading causes of death among men, with almost 1.41 million new cases and
around 375,000 deaths in 2020. Artificial Intelligence algorithms have had a
huge impact in medical image analysis, including digital histopathology, where
Convolutional Neural Networks (CNNs) are used to provide a fast and accurate
diagnosis, supporting experts in this task. To perform an automatic diagnosis,
prostate tissue samples are first digitized into gigapixel-resolution
whole-slide images. Due to the size of these images, neural networks cannot use
them as input and, therefore, small subimages called patches are extracted and
predicted, obtaining a patch-level classification. In this work, a novel patch
aggregation method based on a custom Wide & Deep neural network model is
presented, which performs a slide-level classification using the patch-level
classes obtained from a CNN. The malignant tissue ratio, a 10-bin malignant
probability histogram, the least squares regression line of the histogram, and
the number of malignant connected components are used by the proposed model to
perform the classification. An accuracy of 94.24% and a sensitivity of 98.87%
were achieved, proving that the proposed system could aid pathologists by
speeding up the screening process and, thus, contribute to the fight against
PCa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duran_Lopez_L/0/1/0/all/0/1"&gt;Lourdes Duran-Lopez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dominguez_Morales_J/0/1/0/all/0/1"&gt;Juan P. Dominguez-Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_Galan_D/0/1/0/all/0/1"&gt;Daniel Gutierrez-Galan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rios_Navarro_A/0/1/0/all/0/1"&gt;Antonio Rios-Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jimenez_Fernandez_A/0/1/0/all/0/1"&gt;Angel Jimenez-Fernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_Diaz_S/0/1/0/all/0/1"&gt;Saturnino Vicente-Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linares_Barranco_A/0/1/0/all/0/1"&gt;Alejandro Linares-Barranco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-domain Imitation from Observations. (arXiv:2105.10037v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10037</id>
        <link href="http://arxiv.org/abs/2105.10037"/>
        <updated>2021-05-24T05:08:39.851Z</updated>
        <summary type="html"><![CDATA[Imitation learning seeks to circumvent the difficulty in designing proper
reward functions for training agents by utilizing expert behavior. With
environments modeled as Markov Decision Processes (MDP), most of the existing
imitation algorithms are contingent on the availability of expert
demonstrations in the same MDP as the one in which a new imitation policy is to
be learned. In this paper, we study the problem of how to imitate tasks when
there exist discrepancies between the expert and agent MDP. These discrepancies
across domains could include differing dynamics, viewpoint, or morphology; we
present a novel framework to learn correspondences across such domains.
Importantly, in contrast to prior works, we use unpaired and unaligned
trajectories containing only states in the expert domain, to learn this
correspondence. We utilize a cycle-consistency constraint on both the state
space and a domain agnostic latent space to do this. In addition, we enforce
consistency on the temporal position of states via a normalized position
estimator function, to align the trajectories across the two domains. Once this
correspondence is found, we can directly transfer the demonstrations on one
domain to the other and use it for imitation. Experiments across a wide variety
of challenging domains demonstrate the efficacy of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1"&gt;Dripta S. Raychaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sujoy Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baar_J/0/1/0/all/0/1"&gt;Jeroen van Baar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Multi-Robot System for Non-myopic Spatial Sampling. (arXiv:2105.10018v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10018</id>
        <link href="http://arxiv.org/abs/2105.10018"/>
        <updated>2021-05-24T05:08:39.795Z</updated>
        <summary type="html"><![CDATA[This paper presents a distributed scalable multi-robot planning algorithm for
non-uniform sampling of quasi-static spatial fields. We address the problem of
efficient data collection using multiple autonomous vehicles. In this paper, we
are interested in analyzing the effect of communication between multiple
robots, acting independently, on the overall sampling performance of the team.
Our focus is on distributed sampling problem where the robots are operating
independent of their teammates, but have the ability to communicate their
states to other neighbors with a constraint on the communication range. We
design and apply an informed non-myopic path planning technique on multiple
robotic platforms to efficiently collect measurements from a spatial field. Our
proposed approach is highly adaptive to challenging environments, growing team
size, and runs in real-time, which are the key features for any real-world
scenario. The results show that our distributed sampling approach is able to
achieve efficient sampling with minimal communication between the robots. We
evaluate our approach in simulation over multiple distributions commonly
occurring in nature and on the real-world data collected during a field trial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manjanna_S/0/1/0/all/0/1"&gt;Sandeep Manjanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_A/0/1/0/all/0/1"&gt;Ani Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1"&gt;Gregory Dudek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal prediction of oxygen uptake dynamics from wearable sensors during low-, moderate-, and heavy-intensity exercise. (arXiv:2105.09987v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09987</id>
        <link href="http://arxiv.org/abs/2105.09987"/>
        <updated>2021-05-24T05:08:39.767Z</updated>
        <summary type="html"><![CDATA[Oxygen consumption (VO$_2$) provides established clinical and physiological
indicators of cardiorespiratory function and exercise capacity. However, VO$_2$
monitoring is largely limited to specialized laboratory settings, making its
widespread monitoring elusive. Here, we investigate temporal prediction of
VO$_2$ from wearable sensors during cycle ergometer exercise using a temporal
convolutional network (TCN). Cardiorespiratory signals were acquired from a
smart shirt with integrated textile sensors alongside ground-truth VO$_2$ from
a metabolic system on twenty-two young healthy adults. Participants performed
one ramp-incremental and three pseudorandom binary sequence exercise protocols
to assess a range of VO$_2$ dynamics. A TCN model was developed using causal
convolutions across an effective history length to model the time-dependent
nature of VO$_2$. Optimal history length was determined through minimum
validation loss across hyperparameter values. The best performing model encoded
218 s history length (TCN-VO$_2$ A), with 187 s, 97 s, and 76 s yielding less
than 3% deviation from the optimal validation loss. TCN-VO$_2$ A showed strong
prediction accuracy (mean, 95% CI) across all exercise intensities (-22
ml.min$^{-1}$, [-262, 218]), spanning transitions from low-moderate (-23
ml.min$^{-1}$, [-250, 204]), low-heavy (14 ml.min$^{-1}$, [-252, 280]),
ventilatory threshold-heavy (-49 ml.min$^{-1}$, [-274, 176]), and maximal (-32
ml.min$^{-1}$, [-261, 197]) exercise. Second-by-second classification of
physical activity across 16090 s of predicted VO$_2$ was able to discern
between vigorous, moderate, and light activity with high accuracy (94.1%). This
system enables quantitative aerobic activity monitoring in non-laboratory
settings across a range of exercise intensities using wearable sensors for
monitoring exercise prescription adherence and personal fitness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amelard_R/0/1/0/all/0/1"&gt;Robert Amelard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hedge_E/0/1/0/all/0/1"&gt;Eric T Hedge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughson_R/0/1/0/all/0/1"&gt;Richard L Hughson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Universal NLG for Dialogue Systems and Simulators with Future Bridging. (arXiv:2105.10267v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10267</id>
        <link href="http://arxiv.org/abs/2105.10267"/>
        <updated>2021-05-24T05:08:39.757Z</updated>
        <summary type="html"><![CDATA[In a dialogue system pipeline, a natural language generation (NLG) unit
converts the dialogue direction and content to a corresponding natural language
realization. A recent trend for dialogue systems is to first pre-train on large
datasets and then fine-tune in a supervised manner using datasets annotated
with application-specific features. Though novel behaviours can be learned from
custom annotation, the required effort severely bounds the quantity of the
training set, and the application-specific nature limits the reuse. In light of
the recent success of data-driven approaches, we propose the novel future
bridging NLG (FBNLG) concept for dialogue systems and simulators. The critical
step is for an FBNLG to accept a future user or system utterance to bridge the
present context towards. Future bridging enables self supervised training over
annotation-free datasets, decoupled the training of NLG from the rest of the
system. An FBNLG, pre-trained with massive datasets, is expected to apply in
classical or new dialogue scenarios with minimal adaptation effort. We evaluate
a prototype FBNLG to show that future bridging can be a viable approach to a
universal few-shot NLG for task-oriented and chit-chat dialogues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ennen_P/0/1/0/all/0/1"&gt;Philipp Ennen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yen-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozbay_A/0/1/0/all/0/1"&gt;Ali Girayhan Ozbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Insalata_F/0/1/0/all/0/1"&gt;Ferdinando Insalata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Ye Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalali_S/0/1/0/all/0/1"&gt;Sepehr Jalali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-group Agnostic PAC Learnability. (arXiv:2105.09989v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09989</id>
        <link href="http://arxiv.org/abs/2105.09989"/>
        <updated>2021-05-24T05:08:39.739Z</updated>
        <summary type="html"><![CDATA[An agnostic PAC learning algorithm finds a predictor that is competitive with
the best predictor in a benchmark hypothesis class, where competitiveness is
measured with respect to a given loss function. However, its predictions might
be quite sub-optimal for structured subgroups of individuals, such as protected
demographic groups. Motivated by such fairness concerns, we study "multi-group
agnostic PAC learnability": fixing a measure of loss, a benchmark class $\H$
and a (potentially) rich collection of subgroups $\G$, the objective is to
learn a single predictor such that the loss experienced by every group $g \in
\G$ is not much larger than the best possible loss for this group within $\H$.
Under natural conditions, we provide a characterization of the loss functions
for which such a predictor is guaranteed to exist. For any such loss function
we construct a learning algorithm whose sample complexity is logarithmic in the
size of the collection $\G$. Our results unify and extend previous positive and
negative results from the multi-group fairness literature, which applied for
specific loss functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rothblum_G/0/1/0/all/0/1"&gt;Guy N Rothblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yona_G/0/1/0/all/0/1"&gt;Gal Yona&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Answer Validation for Knowledge-Based VQA. (arXiv:2103.12248v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12248</id>
        <link href="http://arxiv.org/abs/2103.12248"/>
        <updated>2021-05-24T05:08:39.733Z</updated>
        <summary type="html"><![CDATA[The problem of knowledge-based visual question answering involves answering
questions that require external knowledge in addition to the content of the
image. Such knowledge typically comes in a variety of forms, including visual,
textual, and commonsense knowledge. The use of more knowledge sources, however,
also increases the chance of retrieving more irrelevant or noisy facts, making
it difficult to comprehend the facts and find the answer. To address this
challenge, we propose Multi-modal Answer Validation using External knowledge
(MAVEx), where the idea is to validate a set of promising answer candidates
based on answer-specific knowledge retrieval. This is in contrast to existing
approaches that search for the answer in a vast collection of often irrelevant
facts. Our approach aims to learn which knowledge source should be trusted for
each answer candidate and how to validate the candidate using that source. We
consider a multi-modal setting, relying on both textual and visual knowledge
resources, including images searched using Google, sentences from Wikipedia
articles, and concepts from ConceptNet. Our experiments with OK-VQA, a
challenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jialin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiasen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Representation for Dialogue Modeling. (arXiv:2105.10188v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10188</id>
        <link href="http://arxiv.org/abs/2105.10188"/>
        <updated>2021-05-24T05:08:39.722Z</updated>
        <summary type="html"><![CDATA[Although neural models have achieved competitive results in dialogue systems,
they have shown limited ability in representing core semantics, such as
ignoring important entities. To this end, we exploit Abstract Meaning
Representation (AMR) to help dialogue modeling. Compared with the textual
input, AMR explicitly provides core semantic knowledge and reduces data
sparsity. We develop an algorithm to construct dialogue-level AMR graphs from
sentence-level AMRs and explore two ways to incorporate AMRs into dialogue
systems. Experimental results on both dialogue understanding and response
generation tasks show the superiority of our model. To our knowledge, we are
the first to leverage a formal semantic representation into neural dialogue
modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xuefeng Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yulong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Linfeng Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Neural Network Weights using Nature-Inspired Algorithms. (arXiv:2105.09983v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09983</id>
        <link href="http://arxiv.org/abs/2105.09983"/>
        <updated>2021-05-24T05:08:39.696Z</updated>
        <summary type="html"><![CDATA[This study aims to optimize Deep Feedforward Neural Networks (DFNNs) training
using nature-inspired optimization algorithms, such as PSO, MTO, and its
variant called MTOCL. We show how these algorithms efficiently update the
weights of DFNNs when learning from data. We evaluate the performance of DFNN
fused with optimization algorithms using three Wisconsin breast cancer
datasets, Original, Diagnostic, and Prognosis, under different experimental
scenarios. The empirical analysis demonstrates that MTOCL is the most
performing in most scenarios across the three datasets. Also, MTOCL is
comparable to past weight optimization algorithms for the original dataset, and
superior for the other datasets, especially for the challenging Prognostic
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korani_W/0/1/0/all/0/1"&gt;Wael Korani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouhoub_M/0/1/0/all/0/1"&gt;Malek Mouhoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadaoui_S/0/1/0/all/0/1"&gt;Samira Sadaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On planetary systems as ordered sequences. (arXiv:2105.09966v1 [astro-ph.EP])]]></title>
        <id>http://arxiv.org/abs/2105.09966</id>
        <link href="http://arxiv.org/abs/2105.09966"/>
        <updated>2021-05-24T05:08:39.628Z</updated>
        <summary type="html"><![CDATA[A planetary system consists of a host star and one or more planets, arranged
into a particular configuration. Here, we consider what information belongs to
the configuration, or ordering, of 4286 Kepler planets in their 3277 planetary
systems. First, we train a neural network model to predict the radius and
period of a planet based on the properties of its host star and the radii and
period of its neighbors. The mean absolute error of the predictions of the
trained model is a factor of 2.1 better than the MAE of the predictions of a
naive model which draws randomly from dynamically allowable periods and radii.
Second, we adapt a model used for unsupervised part-of-speech tagging in
computational linguistics to investigate whether planets or planetary systems
fall into natural categories with physically interpretable "grammatical rules."
The model identifies two robust groups of planetary systems: (1) compact
multi-planet systems and (2) systems around giant stars ($\log{g} \lesssim
4.0$), although the latter group is strongly sculpted by the selection bias of
the transit method. These results reinforce the idea that planetary systems are
not random sequences -- instead, as a population, they contain predictable
patterns that can provide insight into the formation and evolution of planetary
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sandford_E/0/1/0/all/0/1"&gt;Emily Sandford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kipping_D/0/1/0/all/0/1"&gt;David Kipping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Collins_M/0/1/0/all/0/1"&gt;Michael Collins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words. (arXiv:2101.00265v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00265</id>
        <link href="http://arxiv.org/abs/2101.00265"/>
        <updated>2021-05-24T05:08:39.544Z</updated>
        <summary type="html"><![CDATA[Text-to-image retrieval is an essential task in cross-modal information
retrieval, i.e., retrieving relevant images from a large and unlabelled dataset
given textual queries. In this paper, we propose VisualSparta, a novel
(Visual-text Sparse Transformer Matching) model that shows significant
improvement in terms of both accuracy and efficiency. VisualSparta is capable
of outperforming previous state-of-the-art scalable methods in MSCOCO and
Flickr30K. We also show that it achieves substantial retrieving speed
advantages, i.e., for a 1 million image index, VisualSparta using CPU gets
~391X speedup compared to CPU vector search and ~5.4X speedup compared to
vector search with GPU acceleration. Experiments show that this speed advantage
even gets bigger for larger datasets because VisualSparta can be efficiently
implemented as an inverted index. To the best of our knowledge, VisualSparta is
the first transformer-based text-to-image retrieval model that can achieve
real-time searching for large-scale datasets, with significant accuracy
improvement compared to previous state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyusong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven discovery of interpretable causal relations for deep learning material laws with uncertainty propagation. (arXiv:2105.09980v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09980</id>
        <link href="http://arxiv.org/abs/2105.09980"/>
        <updated>2021-05-24T05:08:39.522Z</updated>
        <summary type="html"><![CDATA[This paper presents a computational framework that generates ensemble
predictive mechanics models with uncertainty quantification (UQ). We first
develop a causal discovery algorithm to infer causal relations among
time-history data measured during each representative volume element (RVE)
simulation through a directed acyclic graph (DAG). With multiple plausible sets
of causal relationships estimated from multiple RVE simulations, the
predictions are propagated in the derived causal graph while using a deep
neural network equipped with dropout layers as a Bayesian approximation for
uncertainty quantification. We select two representative numerical examples
(traction-separation laws for frictional interfaces, elastoplasticity models
for granular assembles) to examine the accuracy and robustness of the proposed
causal discovery method for the common material law predictions in civil
engineering applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahmani_B/0/1/0/all/0/1"&gt;Bahador Bahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1"&gt;Nikolaos N. Vlassis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;WaiChing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanxun Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-color balancing for correctly adjusting the intensity of target colors. (arXiv:2102.01893v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01893</id>
        <link href="http://arxiv.org/abs/2102.01893"/>
        <updated>2021-05-24T05:08:39.515Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel multi-color balance method for reducing
color distortions caused by lighting effects. The proposed method allows us to
adjust three target-colors chosen by a user in an input image so that each
target color is the same as the corresponding destination (benchmark) one. In
contrast, white balancing is a typical technique for reducing the color
distortions, however, they cannot remove lighting effects on colors other than
white. In an experiment, the proposed method is demonstrated to be able to
remove lighting effects on selected three colors, and is compared with existing
white balance adjustments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akazawa_T/0/1/0/all/0/1"&gt;Teruaki Akazawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1"&gt;Yuma Kinoshita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. (arXiv:2006.03659v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03659</id>
        <link href="http://arxiv.org/abs/2006.03659"/>
        <updated>2021-05-24T05:08:39.454Z</updated>
        <summary type="html"><![CDATA[Sentence embeddings are an important component of many natural language
processing (NLP) systems. Like word embeddings, sentence embeddings are
typically learned on large text corpora and then transferred to various
downstream tasks, such as clustering and retrieval. Unlike word embeddings, the
highest performing solutions for learning sentence embeddings require labelled
data, limiting their usefulness to languages and domains where labelled data is
abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for
Unsupervised Textual Representations. Inspired by recent advances in deep
metric learning (DML), we carefully design a self-supervised objective for
learning universal sentence embeddings that does not require labelled training
data. When used to extend the pretraining of transformer-based language models,
our approach closes the performance gap between unsupervised and supervised
pretraining for universal sentence encoders. Importantly, our experiments
suggest that the quality of the learned embeddings scale with both the number
of trainable parameters and the amount of unlabelled training data, making
further improvements straightforward. Our code and pretrained models are
publicly available and can be easily adapted to new domains or used to embed
unseen text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1"&gt;John Giorgi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nitski_O/0/1/0/all/0/1"&gt;Osvald Nitski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bader_G/0/1/0/all/0/1"&gt;Gary Bader&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring the impact of spammers on e-mail and Twitter networks. (arXiv:2105.10256v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.10256</id>
        <link href="http://arxiv.org/abs/2105.10256"/>
        <updated>2021-05-24T05:08:39.442Z</updated>
        <summary type="html"><![CDATA[This paper investigates the research question if senders of large amounts of
irrelevant or unsolicited information - commonly called "spammers" - distort
the network structure of social networks. Two large social networks are
analyzed, the first extracted from the Twitter discourse about a big
telecommunication company, and the second obtained from three years of email
communication of 200 managers working for a large multinational company. This
work compares network robustness and the stability of centrality and
interaction metrics, as well as the use of language, after removing spammers
and the most and least connected nodes. The results show that spammers do not
significantly alter the structure of the information-carrying network, for most
of the social indicators. The authors additionally investigate the correlation
between e-mail subject line and content by tracking language sentiment,
emotionality, and complexity, addressing the cases where collecting email
bodies is not permitted for privacy reasons. The findings extend the research
about robustness and stability of social networks metrics, after the
application of graph simplification strategies. The results have practical
implication for network analysts and for those company managers who rely on
network analytics (applied to company emails and social media data) to support
their decision-making processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1"&gt;P. A. Gloor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting the Negative Data of Distantly Supervised Relation Extraction. (arXiv:2105.10158v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10158</id>
        <link href="http://arxiv.org/abs/2105.10158"/>
        <updated>2021-05-24T05:08:39.422Z</updated>
        <summary type="html"><![CDATA[Distantly supervision automatically generates plenty of training samples for
relation extraction. However, it also incurs two major problems: noisy labels
and imbalanced training data. Previous works focus more on reducing wrongly
labeled relations (false positives) while few explore the missing relations
that are caused by incompleteness of knowledge base (false negatives).
Furthermore, the quantity of negative labels overwhelmingly surpasses the
positive ones in previous problem formulations. In this paper, we first provide
a thorough analysis of the above challenges caused by negative data. Next, we
formulate the problem of relation extraction into as a positive unlabeled
learning task to alleviate false negative problem. Thirdly, we propose a
pipeline approach, dubbed \textsc{ReRe}, that performs sentence-level relation
detection then subject/object extraction to achieve sample-efficient training.
Experimental results show that the proposed method consistently outperforms
existing approaches and remains excellent performance even learned with a large
quantity of false positive samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chenhao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jiaqing Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chengsong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenhao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yanghua Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-05-24T05:08:39.414Z</updated>
        <summary type="html"><![CDATA[Camera movement and unpredictable environmental conditions like dust and wind
induce noise into video feeds. We observe that popular unsupervised MOT methods
are dependent on noise-free conditions. We show that the addition of a small
amount of artificial random noise causes a sharp degradation in model
performance on benchmark metrics. We resolve this problem by introducing a
robust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed
single-head attention model helps limit the negative impact of noise by
learning visual representations at different segment scales. AttU-Net shows
better unsupervised MOT tracking performance over variational inference-based
state-of-the-art baselines. We evaluate our method in the MNIST and the Atari
game video benchmark. We also provide two extended video datasets consisting of
complex visual patterns that include Kuzushiji characters and fashion images to
validate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakam_T/0/1/0/all/0/1"&gt;Tomokazu Murakam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Data Augmentation Approaches for NLP. (arXiv:2105.03075v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03075</id>
        <link href="http://arxiv.org/abs/2105.03075"/>
        <updated>2021-05-24T05:08:39.407Z</updated>
        <summary type="html"><![CDATA[Data augmentation has recently seen increased interest in NLP due to more
work in low-resource domains, new tasks, and the popularity of large-scale
neural networks that require large amounts of training data. Despite this
recent upsurge, this area is still relatively underexplored, perhaps due to the
challenges posed by the discrete nature of language data. In this paper, we
present a comprehensive and unifying survey of data augmentation for NLP by
summarizing the literature in a structured manner. We first introduce and
motivate data augmentation for NLP, and then discuss major methodologically
representative approaches. Next, we highlight techniques that are used for
popular NLP applications and tasks. We conclude by outlining current challenges
and directions for future research. Overall, our paper aims to clarify the
landscape of existing literature in data augmentation for NLP and motivate
additional work in this area. We also present a GitHub repository with a paper
list that will be continuously updated at
https://github.com/styfeng/DataAug4NLP]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Steven Y. Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1"&gt;Varun Gangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1"&gt;Teruko Mitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Emotions in Hindi-English Code Mixed Text Data. (arXiv:2105.09226v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09226</id>
        <link href="http://arxiv.org/abs/2105.09226"/>
        <updated>2021-05-24T05:08:39.399Z</updated>
        <summary type="html"><![CDATA[In recent times, we have seen an increased use of text chat for communication
on social networks and smartphones. This particularly involves the use of
Hindi-English code-mixed text which contains words which are not recognized in
English vocabulary. We have worked on detecting emotions in these mixed data
and classify the sentences in human emotions which are angry, fear, happy or
sad. We have used state of the art natural language processing models and
compared their performance on the dataset comprising sentences in this mixed
data. The dataset was collected and annotated from sources and then used to
train the models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1"&gt;Divyansh Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering. (arXiv:2101.01910v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01910</id>
        <link href="http://arxiv.org/abs/2101.01910"/>
        <updated>2021-05-24T05:08:39.389Z</updated>
        <summary type="html"><![CDATA[Although open-domain question answering (QA) draws great attention in recent
years, it requires large amounts of resources for building the full system and
is often difficult to reproduce previous results due to complex configurations.
In this paper, we introduce SF-QA: simple and fair evaluation framework for
open-domain QA. SF-QA framework modularizes the pipeline open-domain QA system,
which makes the task itself easily accessible and reproducible to research
groups without enough computing resources. The proposed evaluation framework is
publicly available and anyone can contribute to the code and evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyusong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiancheng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Block Design for Learned Fractional Downsampling. (arXiv:2105.09999v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09999</id>
        <link href="http://arxiv.org/abs/2105.09999"/>
        <updated>2021-05-24T05:08:39.334Z</updated>
        <summary type="html"><![CDATA[The layers of convolutional neural networks (CNNs) can be used to alter the
resolution of their inputs, but the scaling factors are limited to integer
values. However, in many image and video processing applications, the ability
to resize by a fractional factor would be advantageous. One example is
conversion between resolutions standardized for video compression, such as from
1080p to 720p. To solve this problem, we propose an alternative building block,
formulated as a conventional convolutional layer followed by a differentiable
resizer. More concretely, the convolutional layer preserves the resolution of
the input, while the resizing operation is fully handled by the resizer. In
this way, any CNN architecture can be adapted for non-integer resizing. As an
application, we replace the resizing convolutional layer of a modern deep
downsampling model by the proposed building block, and apply it to an adaptive
bitrate video streaming scenario. Our experimental results show that an
improvement in coding efficiency over the conventional Lanczos algorithm is
attained, in terms of PSNR, SSIM, and VMAF on test videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li-Heng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bampis_C/0/1/0/all/0/1"&gt;Christos G. Bampis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1"&gt;Alan C. Bovik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from My Friends: Few-Shot Personalized Conversation Systems via Social Networks. (arXiv:2105.10323v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10323</id>
        <link href="http://arxiv.org/abs/2105.10323"/>
        <updated>2021-05-24T05:08:39.325Z</updated>
        <summary type="html"><![CDATA[Personalized conversation models (PCMs) generate responses according to
speaker preferences. Existing personalized conversation tasks typically require
models to extract speaker preferences from user descriptions or their
conversation histories, which are scarce for newcomers and inactive users. In
this paper, we propose a few-shot personalized conversation task with an
auxiliary social network. The task requires models to generate personalized
responses for a speaker given a few conversations from the speaker and a social
network. Existing methods are mainly designed to incorporate descriptions or
conversation histories. Those methods can hardly model speakers with so few
conversations or connections between speakers. To better cater for newcomers
with few resources, we propose a personalized conversation model (PCM) that
learns to adapt to new speakers as well as enabling new speakers to learn from
resource-rich speakers. Particularly, based on a meta-learning based PCM, we
propose a task aggregator (TA) to collect other speakers' information from the
social network. The TA provides prior knowledge of the new speaker in its
meta-learning. Experimental results show our methods outperform all baselines
in appropriateness, diversity, and consistency with speakers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhiliang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1"&gt;Wei Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zihan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dongkyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yiping Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Nevin L. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partner Matters! An Empirical Study on Fusing Personas for Personalized Response Selection in Retrieval-Based Chatbots. (arXiv:2105.09050v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09050</id>
        <link href="http://arxiv.org/abs/2105.09050"/>
        <updated>2021-05-24T05:08:39.305Z</updated>
        <summary type="html"><![CDATA[Persona can function as the prior knowledge for maintaining the consistency
of dialogue systems. Most of previous studies adopted the self persona in
dialogue whose response was about to be selected from a set of candidates or
directly generated, but few have noticed the role of partner in dialogue. This
paper makes an attempt to thoroughly explore the impact of utilizing personas
that describe either self or partner speakers on the task of response selection
in retrieval-based chatbots. Four persona fusion strategies are designed, which
assume personas interact with contexts or responses in different ways. These
strategies are implemented into three representative models for response
selection, which are based on the Hierarchical Recurrent Encoder (HRE),
Interactive Matching Network (IMN) and Bidirectional Encoder Representations
from Transformers (BERT) respectively. Empirical studies on the Persona-Chat
dataset show that the partner personas neglected in previous studies can
improve the accuracy of response selection in the IMN- and BERT-based models.
Besides, our BERT-based model implemented with the context-response-aware
persona fusion strategy outperforms previous methods by margins larger than
2.7% on original personas and 4.6% on revised personas in terms of hits@1
(top-1 accuracy), achieving a new state-of-the-art performance on the
Persona-Chat dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zhen-Hua Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhigang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaodan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functionals in the Clouds: An abstract architecture of serverless Cloud-Native Apps. (arXiv:2105.10362v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10362</id>
        <link href="http://arxiv.org/abs/2105.10362"/>
        <updated>2021-05-24T05:08:39.287Z</updated>
        <summary type="html"><![CDATA[Cloud Native Application CNApp (as a distributed system) is a collection of
independent components (micro-services) interacting via communication
protocols. This gives rise to present an abstract architecture of CNApp as
dynamically re-configurable acyclic directed multi graph where vertices are
microservices, and edges are the protocols. Generic mechanisms for such
reconfigurations evidently correspond to higher-level functions (functionals).
This implies also internal abstract architecture of microservice as a
collection of event-triggered serverless functions (including functions
implementing the protocols) that are dynamically composed into event-dependent
data-flow graphs. Again, generic mechanisms for such compositions correspond to
calculus of functionals and relations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ambroszkiewicz_S/0/1/0/all/0/1"&gt;Stanislaw Ambroszkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartyna_W/0/1/0/all/0/1"&gt;Waldemar Bartyna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bylka_S/0/1/0/all/0/1"&gt;Stanislaw Bylka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretrained Language Models for Text Generation: A Survey. (arXiv:2105.10311v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10311</id>
        <link href="http://arxiv.org/abs/2105.10311"/>
        <updated>2021-05-24T05:08:39.249Z</updated>
        <summary type="html"><![CDATA[Text generation has become one of the most important yet challenging tasks in
natural language processing (NLP). The resurgence of deep learning has greatly
advanced this field by neural generation models, especially the paradigm of
pretrained language models (PLMs). In this paper, we present an overview of the
major advances achieved in the topic of PLMs for text generation. As the
preliminaries, we present the general task definition and briefly describe the
mainstream architectures of PLMs for text generation. As the core content, we
discuss how to adapt existing PLMs to model different input data and satisfy
special properties in the generated text. We further summarize several
important fine-tuning strategies for text generation. Finally, we present
several future directions and conclude this paper. Our survey aims to provide
text generation researchers a synthesis and pointer to related research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tianyi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unit Test Case Generation with Transformers and Focal Context. (arXiv:2009.05617v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05617</id>
        <link href="http://arxiv.org/abs/2009.05617"/>
        <updated>2021-05-24T05:08:39.225Z</updated>
        <summary type="html"><![CDATA[Automated unit test case generation tools facilitate test-driven development
and support developers by suggesting tests intended to identify flaws in their
code. Existing approaches are usually guided by the test coverage criteria,
generating synthetic test cases that are often difficult for developers to read
or understand. In this paper we propose AthenaTest, an approach that aims to
generate unit test cases by learning from real-world focal methods and
developer-written testcases. We formulate unit test case generation as a
sequence-to-sequence learning task, adopting a two-step training procedure
consisting of denoising pretraining on a large unsupervised Java corpus, and
supervised finetuning for a downstream translation task of generating unit
tests. We investigate the impact of natural language and source code
pretraining, as well as the focal context information surrounding the focal
method. Both techniques provide improvements in terms of validation loss, with
pretraining yielding 25% relative improvement and focal context providing
additional 11.1% improvement. We also introduce Methods2Test, the largest
publicly available supervised parallel corpus of unit test case methods and
corresponding focal methods in Java, which comprises 780K test cases mined from
91K open-source repositories from GitHub. We evaluate AthenaTest on five
defects4j projects, generating 25K passing test cases covering 43.7% of the
focal methods with only 30 attempts. We execute the test cases, collect test
coverage information, and compare them with test cases generated by EvoSuite
and GPT-3, finding that our approach outperforms GPT-3 and has comparable
coverage w.r.t. EvoSuite. Finally, we survey professional developers on their
preference in terms of readability, understandability, and testing
effectiveness of the generated tests, showing overwhelmingly preference towards
AthenaTest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tufano_M/0/1/0/all/0/1"&gt;Michele Tufano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1"&gt;Dawn Drain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1"&gt;Alexey Svyatkovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shao Kun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1"&gt;Neel Sundaresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fact-driven Logical Reasoning. (arXiv:2105.10334v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10334</id>
        <link href="http://arxiv.org/abs/2105.10334"/>
        <updated>2021-05-24T05:08:39.208Z</updated>
        <summary type="html"><![CDATA[Logical reasoning, which is closely related to human cognition, is of vital
importance in human's understanding of texts. Recent years have witnessed
increasing attentions on machine's logical reasoning abilities. However,
previous studies commonly apply ad-hoc methods to model pre-defined relation
patterns, such as linking named entities, which only considers global knowledge
components that are related to commonsense, without local perception of
complete facts or events. Such methodology is obviously insufficient to deal
with complicated logical structures. Therefore, we argue that the natural logic
units would be the group of backbone constituents of the sentence such as the
subject-verb-object formed "facts", covering both global and local knowledge
pieces that are necessary as the basis for logical reasoning. Beyond building
the ad-hoc graphs, we propose a more general and convenient fact-driven
approach to construct a supergraph on top of our newly defined fact units, and
enhance the supergraph with further explicit guidance of local question and
option interactions. Experiments on two challenging logical reasoning benchmark
datasets, ReClor and LogiQA, show that our proposed model, \textsc{Focal
Reasoner}, outperforms the baseline models dramatically. It can also be
smoothly applied to other downstream tasks such as MuTual, a dialogue reasoning
dataset, achieving competitive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1"&gt;Siru Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06977</id>
        <link href="http://arxiv.org/abs/2105.06977"/>
        <updated>2021-05-24T05:08:39.198Z</updated>
        <summary type="html"><![CDATA[Context-aware machine translation models are designed to leverage contextual
information, but often fail to do so. As a result, they inaccurately
disambiguate pronouns and polysemous words that require context for resolution.
In this paper, we ask several questions: What contexts do human translators use
to resolve ambiguous words? Are models paying large amounts of attention to the
same context? What if we explicitly train them to do so? To answer these
questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a
new English-French dataset comprising supporting context words for 14K
translations that professional translators found useful for pronoun
disambiguation. Using SCAT, we perform an in-depth analysis of the context used
to disambiguate, examining positional and lexical characteristics of the
supporting words. Furthermore, we measure the degree of alignment between the
model's attention scores and the supporting context from SCAT, and apply a
guided attention strategy to encourage agreement between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kayo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1"&gt;Patrick Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1"&gt;Danish Pruthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1"&gt;Aditi Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09996</id>
        <link href="http://arxiv.org/abs/2105.09996"/>
        <updated>2021-05-24T05:08:39.151Z</updated>
        <summary type="html"><![CDATA[We present a simplified, task-agnostic multi-modal pre-training approach that
can accept either video or text input, or both for a variety of end tasks.
Existing pre-training are task-specific by adopting either a single cross-modal
encoder that requires both modalities, limiting their use for retrieval-style
end tasks or more complex multitask learning with two unimodal encoders,
limiting early cross-modal fusion. We instead introduce new pretraining masking
schemes that better mix across modalities (e.g. by forcing masks for text to
predict the closest video embeddings) while also maintaining separability (e.g.
unimodal predictions are sometimes required, without using all the input).
Experimental results show strong performance across a wider range of tasks than
any previous methods, often outperforming task-specific pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1"&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Po-Yao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1"&gt;Prahal Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1"&gt;Masoumeh Aminzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1"&gt;Christoph Feichtenhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1"&gt;Florian Metze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Have you tried Neural Topic Models? Comparative Analysis of Neural and Non-Neural Topic Models with Application to COVID-19 Twitter Data. (arXiv:2105.10165v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10165</id>
        <link href="http://arxiv.org/abs/2105.10165"/>
        <updated>2021-05-24T05:08:39.143Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used in studying social phenomena. We conduct a
comparative study examining state-of-the-art neural versus non-neural topic
models, performing a rigorous quantitative and qualitative assessment on a
dataset of tweets about the COVID-19 pandemic. Our results show that not only
do neural topic models outperform their classical counterparts on standard
evaluation metrics, but they also produce more coherent topics, which are of
great benefit when studying complex social problems. We also propose a novel
regularization term for neural topic models, which is designed to address the
well-documented problem of mode collapse, and demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1"&gt;Andrew Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1"&gt;Nga Than&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rule Augmented Unsupervised Constituency Parsing. (arXiv:2105.10193v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10193</id>
        <link href="http://arxiv.org/abs/2105.10193"/>
        <updated>2021-05-24T05:08:39.000Z</updated>
        <summary type="html"><![CDATA[Recently, unsupervised parsing of syntactic trees has gained considerable
attention. A prototypical approach to such unsupervised parsing employs
reinforcement learning and auto-encoders. However, no mechanism ensures that
the learnt model leverages the well-understood language grammar. We propose an
approach that utilizes very generic linguistic knowledge of the language
present in the form of syntactic rules, thus inducing better syntactic
structures. We introduce a novel formulation that takes advantage of the
syntactic grammar rules and is independent of the base system. We achieve new
state-of-the-art results on two benchmarks datasets, MNLI and WSJ. The source
code of the paper is available at https://github.com/anshuln/Diora_with_rules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sahay_A/0/1/0/all/0/1"&gt;Atul Sahay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1"&gt;Anshul Nasery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1"&gt;Ayush Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversational Machine Reading Comprehension for Vietnamese Healthcare Texts. (arXiv:2105.01542v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01542</id>
        <link href="http://arxiv.org/abs/2105.01542"/>
        <updated>2021-05-24T05:08:38.979Z</updated>
        <summary type="html"><![CDATA[Machine reading comprehension (MRC) is a sub-field in natural language
processing that aims to help computers understand unstructured texts and then
answer questions related to them. In practice, conversation is an essential way
to communicate and transfer information. To help machines understand
conversation texts, we present UIT-ViCoQA - a new corpus for conversational
machine reading comprehension in the Vietnamese language. This corpus consists
of 10,000 questions with answers to over 2,000 conversations about health news
articles. Then, we evaluate several baseline approaches for conversational
machine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1
score of 45.27%, which is 30.91 points behind human performance (76.18%),
indicating that there is ample room for improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1"&gt;Son T. Luu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1"&gt;Mao Nguyen Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Loi Duc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1"&gt;Khiem Vinh Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Kiet Van Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngan Luu-Thuy Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Generation and Evaluation of Visual Stories via Semantic Consistency. (arXiv:2105.10026v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10026</id>
        <link href="http://arxiv.org/abs/2105.10026"/>
        <updated>2021-05-24T05:08:38.915Z</updated>
        <summary type="html"><![CDATA[Story visualization is an under-explored task that falls at the intersection
of many important research directions in both computer vision and natural
language processing. In this task, given a series of natural language captions
which compose a story, an agent must generate a sequence of images that
correspond to the captions. Prior work has introduced recurrent generative
models which outperform text-to-image synthesis models on this task. However,
there is room for improvement of generated images in terms of visual quality,
coherence and relevance. We present a number of improvements to prior modeling
approaches, including (1) the addition of a dual learning framework that
utilizes video captioning to reinforce the semantic alignment between the story
and generated images, (2) a copy-transform mechanism for
sequentially-consistent story visualization, and (3) MART-based transformers to
model complex interactions between frames. We present ablation studies to
demonstrate the effect of each of these techniques on the generative power of
the model for both individual images as well as the entire narrative.
Furthermore, due to the complexity and generative nature of the task, standard
evaluation metrics do not accurately reflect performance. Therefore, we also
provide an exploration of evaluation metrics for the model, focused on aspects
of the generated frames such as the presence/quality of generated characters,
the relevance to captions, and the diversity of the generated images. We also
present correlation experiments of our proposed automated metrics with human
evaluations. Code and data available at:
https://github.com/adymaharana/StoryViz]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1"&gt;Adyasha Maharana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hannan_D/0/1/0/all/0/1"&gt;Darryl Hannan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Bi-Encoders for Word Sense Disambiguation. (arXiv:2105.10146v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10146</id>
        <link href="http://arxiv.org/abs/2105.10146"/>
        <updated>2021-05-24T05:08:38.868Z</updated>
        <summary type="html"><![CDATA[Modern transformer-based neural architectures yield impressive results in
nearly every NLP task and Word Sense Disambiguation, the problem of discerning
the correct sense of a word in a given context, is no exception.
State-of-the-art approaches in WSD today leverage lexical information along
with pre-trained embeddings from these models to achieve results comparable to
human inter-annotator agreement on standard evaluation benchmarks. In the same
vein, we experiment with several strategies to optimize bi-encoders for this
specific task and propose alternative methods of presenting lexical information
to our model. Through our multi-stage pre-training and fine-tuning pipeline we
further the state of the art in Word Sense Disambiguation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kohli_H/0/1/0/all/0/1"&gt;Harsh Kohli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Non-Linear Structural Probe. (arXiv:2105.10185v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10185</id>
        <link href="http://arxiv.org/abs/2105.10185"/>
        <updated>2021-05-24T05:08:38.860Z</updated>
        <summary type="html"><![CDATA[Probes are models devised to investigate the encoding of knowledge -- e.g.
syntactic structure -- in contextual representations. Probes are often designed
for simplicity, which has led to restrictions on probe design that may not
allow for the full exploitation of the structure of encoded information; one
such restriction is linearity. We examine the case of a structural probe
(Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic
structure in contextual representations through learning only linear
transformations. By observing that the structural probe learns a metric, we are
able to kernelize it and develop a novel non-linear variant with an identical
number of parameters. We test on 6 languages and find that the radial-basis
function (RBF) kernel, in conjunction with regularization, achieves a
statistically significant improvement over the baseline in all languages --
implying that at least part of the syntactic knowledge is encoded non-linearly.
We conclude by discussing how the RBF kernel resembles BERT's self-attention
layers and speculate that this resemblance leads to the RBF-based probe's
stronger performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1"&gt;Jennifer C. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1"&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1"&gt;Naomi Saphra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Comparison of Data Privacy Documents: A Preliminary Experiment on GDPR-like Laws. (arXiv:2105.10117v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10117</id>
        <link href="http://arxiv.org/abs/2105.10117"/>
        <updated>2021-05-24T05:08:38.850Z</updated>
        <summary type="html"><![CDATA[General Data Protection Regulation (GDPR) becomes a standard law for data
protection in many countries. Currently, twelve countries adopt the regulation
and establish their GDPR-like regulation. However, to evaluate the differences
and similarities of these GDPR-like regulations is time-consuming and needs a
lot of manual effort from legal experts. Moreover, GDPR-like regulations from
different countries are written in their languages leading to a more difficult
task since legal experts who know both languages are essential. In this paper,
we investigate a simple natural language processing (NLP) approach to tackle
the problem. We first extract chunks of information from GDPR-like documents
and form structured data from natural language. Next, we use NLP methods to
compare documents to measure their similarity. Finally, we manually label a
small set of data to evaluate our approach. The empirical result shows that the
BERT model with cosine similarity outperforms other baselines. Our data and
code are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kawintiranon_K/0/1/0/all/0/1"&gt;Kornraphop Kawintiranon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yaguang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10042</id>
        <link href="http://arxiv.org/abs/2105.10042"/>
        <updated>2021-05-24T05:08:38.840Z</updated>
        <summary type="html"><![CDATA[End-to-end spoken language understanding (SLU) has recently attracted
increasing interest. Compared to the conventional tandem-based approach that
combines speech recognition and language understanding as separate modules, the
new approach extracts users' intentions directly from the speech signals,
resulting in joint optimization and low latency. Such an approach, however, is
typically designed to process one intention at a time, which leads users to
take multiple rounds to fulfill their requirements while interacting with a
dialogue system. In this paper, we propose a streaming end-to-end framework
that can process multiple intentions in an online and incremental way. The
backbone of our framework is a unidirectional RNN trained with the
connectionist temporal classification (CTC) criterion. By this design, an
intention can be identified when sufficient evidence has been accumulated, and
multiple intentions can be identified sequentially. We evaluate our solution on
the Fluent Speech Commands (FSC) dataset and the intent detection accuracy is
about 97 % on all multi-intent settings. This result is comparable to the
performance of the state-of-the-art non-streaming models, but is achieved in an
online and incremental way. We also employ our model to a keyword spotting task
using the Google Speech Commands dataset and the results are also highly
promising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1"&gt;Nihal Potdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1"&gt;Anderson R. Avila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1"&gt;Chao Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yiran Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter. (arXiv:2105.09967v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09967</id>
        <link href="http://arxiv.org/abs/2105.09967"/>
        <updated>2021-05-24T05:08:38.828Z</updated>
        <summary type="html"><![CDATA[Datasets with induced emotion labels are scarce but of utmost importance for
many NLP tasks. We present a new, automated method for collecting texts along
with their induced reaction labels. The method exploits the online use of
reaction GIFs, which capture complex affective states. We show how to augment
the data with induced emotion and induced sentiment labels. We use our method
to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K
tweets. We provide baselines for three new tasks, including induced sentiment
prediction and multilabel classification of induced emotions. Our method and
dataset open new research opportunities in emotion detection and affective
computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmueli_B/0/1/0/all/0/1"&gt;Boaz Shmueli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1"&gt;Soumya Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1"&gt;Lun-Wei Ku&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering. (arXiv:2101.01910v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01910</id>
        <link href="http://arxiv.org/abs/2101.01910"/>
        <updated>2021-05-24T05:08:38.800Z</updated>
        <summary type="html"><![CDATA[Although open-domain question answering (QA) draws great attention in recent
years, it requires large amounts of resources for building the full system and
is often difficult to reproduce previous results due to complex configurations.
In this paper, we introduce SF-QA: simple and fair evaluation framework for
open-domain QA. SF-QA framework modularizes the pipeline open-domain QA system,
which makes the task itself easily accessible and reproducible to research
groups without enough computing resources. The proposed evaluation framework is
publicly available and anyone can contribute to the code and evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyusong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiancheng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Aware Abstractive Summarization. (arXiv:2105.10155v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10155</id>
        <link href="http://arxiv.org/abs/2105.10155"/>
        <updated>2021-05-24T05:08:38.791Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach to summarization based on Bayesian deep learning.
We approximate Bayesian summary generation by first extending state-of-the-art
summarization models with Monte Carlo dropout and then using them to perform
multiple stochastic forward passes. This method allows us to improve
summarization performance by simply using the median of multiple stochastic
summaries. We show that our variational equivalents of BART and PEGASUS can
outperform their deterministic counterparts on multiple benchmark datasets. In
addition, we rely on Bayesian inference to measure the uncertainty of the model
when generating summaries. Having a reliable uncertainty measure, we can
improve the experience of the end user by filtering out generated summaries of
high uncertainty. Furthermore, our proposed metric could be used as a criterion
for selecting samples for annotation, and can be paired nicely with active
learning and human-in-the-loop approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gidiotis_A/0/1/0/all/0/1"&gt;Alexios Gidiotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1"&gt;Grigorios Tsoumakas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Span-based Joint Entity and Relation Extraction via Squence Tagging Mechanism. (arXiv:2105.10080v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10080</id>
        <link href="http://arxiv.org/abs/2105.10080"/>
        <updated>2021-05-24T05:08:38.783Z</updated>
        <summary type="html"><![CDATA[Span-based joint extraction simultaneously conducts named entity recognition
(NER) and relation extraction (RE) in text span form. Recent studies have shown
that token labels can convey crucial task-specific information and enrich token
semantics. However, as far as we know, due to completely abstain from sequence
tagging mechanism, all prior span-based work fails to use token label
in-formation. To solve this problem, we pro-pose Sequence Tagging enhanced
Span-based Network (STSN), a span-based joint extrac-tion network that is
enhanced by token BIO label information derived from sequence tag-ging based
NER. By stacking multiple atten-tion layers in depth, we design a deep neu-ral
architecture to build STSN, and each atten-tion layer consists of three basic
attention units. The deep neural architecture first learns seman-tic
representations for token labels and span-based joint extraction, and then
constructs in-formation interactions between them, which also realizes
bidirectional information interac-tions between span-based NER and RE.
Fur-thermore, we extend the BIO tagging scheme to make STSN can extract
overlapping en-tity. Experiments on three benchmark datasets show that our
model consistently outperforms previous optimal models by a large margin,
creating new state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1"&gt;Bin Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shasha Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huijun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A General Method For Automatic Discovery of Powerful Interactions In Click-Through Rate Prediction. (arXiv:2105.10484v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10484</id>
        <link href="http://arxiv.org/abs/2105.10484"/>
        <updated>2021-05-24T05:08:38.775Z</updated>
        <summary type="html"><![CDATA[Modeling powerful interactions is a critical challenge in Click-through rate
(CTR) prediction, which is one of the most typical machine learning tasks in
personalized advertising and recommender systems. Although developing
hand-crafted interactions is effective for a small number of datasets, it
generally requires laborious and tedious architecture engineering for extensive
scenarios. In recent years, several neural architecture search (NAS) methods
have been proposed for designing interactions automatically. However, existing
methods only explore limited types and connections of operators for interaction
generation, leading to low generalization ability. To address these problems,
we propose a more general automated method for building powerful interactions
named AutoPI. The main contributions of this paper are as follows: AutoPI
adopts a more general search space in which the computational graph is
generalized from existing network connections, and the interactive operators in
the edges of the graph are extracted from representative hand-crafted works. It
allows searching for various powerful feature interactions to produce higher
AUC and lower Logloss in a wide variety of applications. Besides, AutoPI
utilizes a gradient-based search strategy for exploration with a significantly
low computational cost. Experimentally, we evaluate AutoPI on a diverse suite
of benchmark datasets, demonstrating the generalizability and efficiency of
AutoPI over hand-crafted architectures and state-of-the-art NAS algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Ze Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinnian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yumeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiancheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1"&gt;Tanchao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lifeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal Sarcasm Detection and Humor Classification in Code-mixed Conversations. (arXiv:2105.09984v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09984</id>
        <link href="http://arxiv.org/abs/2105.09984"/>
        <updated>2021-05-24T05:08:38.766Z</updated>
        <summary type="html"><![CDATA[Sarcasm detection and humor classification are inherently subtle problems,
primarily due to their dependence on the contextual and non-verbal information.
Furthermore, existing studies in these two topics are usually constrained in
non-English languages such as Hindi, due to the unavailability of qualitative
annotated datasets. In this work, we make two major contributions considering
the above limitations: (1) we develop a Hindi-English code-mixed dataset,
MaSaC, for the multi-modal sarcasm detection and humor classification in
conversational dialog, which to our knowledge is the first dataset of its kind;
(2) we propose MSH-COMICS, a novel attention-rich neural architecture for the
utterance classification. We learn efficient utterance representation utilizing
a hierarchical attention mechanism that attends to a small portion of the input
sentence at a time. Further, we incorporate dialog-level contextual attention
mechanism to leverage the dialog history for the multi-modal classification. We
perform extensive experiments for both the tasks by varying multi-modal inputs
and various submodules of MSH-COMICS. We also conduct comparative analysis
against existing approaches. We observe that MSH-COMICS attains superior
performance over the existing models by > 1 F1-score point for the sarcasm
detection and 10 F1-score points in humor classification. We diagnose our model
and perform thorough analysis of the results to understand the superiority and
pitfalls.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bedi_M/0/1/0/all/0/1"&gt;Manjot Bedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Shivani Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1"&gt;Md Shad Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASQ: Automatically Generating Question-Answer Pairs using AMRs. (arXiv:2105.10023v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10023</id>
        <link href="http://arxiv.org/abs/2105.10023"/>
        <updated>2021-05-24T05:08:38.727Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce ASQ, a tool to automatically mine questions and
answers from a sentence, using its Abstract Meaning Representation (AMR).
Previous work has made a case for using question-answer pairs to specify
predicate-argument structure of a sentence using natural language, which does
not require linguistic expertise or training. This has resulted in the creation
of datasets such as QA-SRL and QAMR, for both of which, the question-answer
pair annotations were crowdsourced. Our approach has the same end-goal, but is
automatic, making it faster and cost-effective, without compromising on the
quality and validity of the question-answer pairs thus obtained. A qualitative
evaluation of the output generated by ASQ from the AMR 2.0 data shows that the
question-answer pairs are natural and valid, and demonstrate good coverage of
the content. We run ASQ on the sentences from the QAMR dataset, to observe that
the semantic roles in QAMR are also captured by ASQ.We intend to make this tool
and the results publicly available for others to use and build upon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rakshit_G/0/1/0/all/0/1"&gt;Geetanjali Rakshit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1"&gt;Jeffrey Flanigan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Non-sequential Approach to Deep User Interest Model for CTR Prediction. (arXiv:2104.06312v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06312</id>
        <link href="http://arxiv.org/abs/2104.06312"/>
        <updated>2021-05-24T05:08:38.657Z</updated>
        <summary type="html"><![CDATA[Click-Through Rate (CTR) prediction plays an important role in many
industrial applications, and recently a lot of attention is paid to the deep
interest models which use attention mechanism to capture user interests from
historical behaviors. However, most current models are based on sequential
models which truncate the behavior sequences by a fixed length, thus have
difficulties in handling very long behavior sequences. Another big problem is
that sequences with the same length can be quite different in terms of time,
carrying completely different meanings. In this paper, we propose a
non-sequential approach to tackle the above problems. Specifically, we first
represent the behavior data in a sparse key-vector format, where the vector
contains rich behavior info such as time, count and category. Next, we enhance
the Deep Interest Network to take such rich information into account by a novel
attention network. The sparse representation makes it practical to handle large
scale long behavior sequences. Finally, we introduce a multidimensional
partition framework to mine behavior interactions. The framework can partition
data into custom designed time buckets to capture the interactions among
information aggregated in different time buckets. Similarly, it can also
partition the data into different categories and capture the interactions among
them. Experiments are conducted on two public datasets: one is an advertising
dataset and the other is a production recommender dataset. Our models
outperform other state-of-the-art models on both datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Keke Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xing Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1"&gt;Qi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1"&gt;Linjian Mo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RFID-based Article-to-Fixture Predictions in Real-World Fashion Stores. (arXiv:2105.10216v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10216</id>
        <link href="http://arxiv.org/abs/2105.10216"/>
        <updated>2021-05-24T05:08:38.622Z</updated>
        <summary type="html"><![CDATA[In recent years, Radio Frequency Identification (RFID) technology has been
applied to improve numerous processes, such as inventory management in retail
stores. However, automatic localization of RFID-tagged goods in stores is still
a challenging problem. To address this issue, we equip fixtures (e.g., shelves)
with reference tags and use data we collect during RFID-based stocktakes to map
articles to fixtures. Knowing the location of goods enables the implementation
of several practical applications, such as automated Money Mapping (i.e., a
heat map of sales across fixtures). Specifically, we conduct controlled lab
experiments and a case-study in two fashion retail stores to evaluate our
article-to-fixture prediction approaches. The approaches are based on
calculating distances between read event time series using DTW, and clustering
of read events using DBSCAN. We find that, read events collected during
RFID-based stocktakes can be used to assign articles to fixtures with an
accuracy of more than 90%. Additionally, we conduct a pilot to investigate the
challenges related to the integration of such a localization system in the
day-to-day business of retail stores. Hence, in this paper we present an
exploratory venture into novel and practical RFID-based applications in fashion
retails stores, beyond the scope of stock management.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolbitsch_M/0/1/0/all/0/1"&gt;Matthias W&amp;#xf6;lbitsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasler_T/0/1/0/all/0/1"&gt;Thomas Hasler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasper_P/0/1/0/all/0/1"&gt;Patrick Kasper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helic_D/0/1/0/all/0/1"&gt;Denis Helic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walk_S/0/1/0/all/0/1"&gt;Simon Walk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversifying Multi-aspect Search Results Using Simpson's Diversity Index. (arXiv:2105.10075v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10075</id>
        <link href="http://arxiv.org/abs/2105.10075"/>
        <updated>2021-05-24T05:08:38.573Z</updated>
        <summary type="html"><![CDATA[In search and recommendation, diversifying the multi-aspect search results
could help with reducing redundancy, and promoting results that might not be
shown otherwise. Many previous methods have been proposed for this task.
However, previous methods do not explicitly consider the uniformity of the
number of the items' classes, or evenness, which could degrade the search and
recommendation quality. To address this problem, we introduce a novel method by
adapting the Simpson's Diversity Index from biology, which enables a more
effective and efficient quadratic search result diversification algorithm. We
also extend the method to balance the diversity between multiple aspects
through weighted factors and further improve computational complexity by
developing a fast approximation algorithm. We demonstrate the feasibility of
the proposed method using the openly available Kaggle shoes competition
dataset. Our experimental results show that our approach outperforms previous
state of the art diversification methods, while reducing computational
complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianghong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1"&gt;Eugene Agichtein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kallumadi_S/0/1/0/all/0/1"&gt;Surya Kallumadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Cross-Sectional Currency Strategies by Ranking Refinement with Transformer-based Architectures. (arXiv:2105.10019v1 [q-fin.PM])]]></title>
        <id>http://arxiv.org/abs/2105.10019</id>
        <link href="http://arxiv.org/abs/2105.10019"/>
        <updated>2021-05-24T05:08:38.317Z</updated>
        <summary type="html"><![CDATA[The performance of a cross-sectional currency strategy depends crucially on
accurately ranking instruments prior to portfolio construction. While this
ranking step is traditionally performed using heuristics, or by sorting outputs
produced by pointwise regression or classification models, Learning to Rank
algorithms have recently presented themselves as competitive and viable
alternatives. Despite improving ranking accuracy on average however, these
techniques do not account for the possibility that assets positioned at the
extreme ends of the ranked list -- which are ultimately used to construct the
long/short portfolios -- can assume different distributions in the input space,
and thus lead to sub-optimal strategy performance. Drawing from research in
Information Retrieval that demonstrates the utility of contextual information
embedded within top-ranked documents to learn the query's characteristics to
improve ranking, we propose an analogous approach: exploiting the features of
both out- and under-performing instruments to learn a model for refining the
original ranked list. Under a re-ranking framework, we adapt the Transformer
architecture to encode the features of extreme assets for refining our
selection of long/short instruments obtained with an initial retrieval.
Backtesting on a set of 31 currencies, our proposed methodology significantly
boosts Sharpe ratios -- by approximately 20% over the original LTR algorithms
and double that of traditional baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Poh_D/0/1/0/all/0/1"&gt;Daniel Poh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Lim_B/0/1/0/all/0/1"&gt;Bryan Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RLIRank: Learning to Rank with Reinforcement Learning for Dynamic Search. (arXiv:2105.10124v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10124</id>
        <link href="http://arxiv.org/abs/2105.10124"/>
        <updated>2021-05-24T05:08:38.302Z</updated>
        <summary type="html"><![CDATA[To support complex search tasks, where the initial information requirements
are complex or may change during the search, a search engine must adapt the
information delivery as the user's information requirements evolve. To support
this dynamic ranking paradigm effectively, search result ranking must
incorporate both the user feedback received, and the information displayed so
far. To address this problem, we introduce a novel reinforcement learning-based
approach, RLIrank. We first build an adapted reinforcement learning framework
to integrate the key components of the dynamic search. Then, we implement a new
Learning to Rank (LTR) model for each iteration of the dynamic search, using a
recurrent Long Short Term Memory neural network (LSTM), which estimates the
gain for each next result, learning from each previously ranked document. To
incorporate the user's feedback, we develop a word-embedding variation of the
classic Rocchio Algorithm, to help guide the ranking towards the high-value
documents. Those innovations enable RLIrank to outperform the previously
reported methods from the TREC Dynamic Domain Tracks 2017 and exceed all the
methods in 2016 TREC Dynamic Domain after multiple search iterations, advancing
the state of the art for dynamic search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianghong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1"&gt;Eugene Agichtein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring the impact of spammers on e-mail and Twitter networks. (arXiv:2105.10256v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.10256</id>
        <link href="http://arxiv.org/abs/2105.10256"/>
        <updated>2021-05-24T05:08:38.286Z</updated>
        <summary type="html"><![CDATA[This paper investigates the research question if senders of large amounts of
irrelevant or unsolicited information - commonly called "spammers" - distort
the network structure of social networks. Two large social networks are
analyzed, the first extracted from the Twitter discourse about a big
telecommunication company, and the second obtained from three years of email
communication of 200 managers working for a large multinational company. This
work compares network robustness and the stability of centrality and
interaction metrics, as well as the use of language, after removing spammers
and the most and least connected nodes. The results show that spammers do not
significantly alter the structure of the information-carrying network, for most
of the social indicators. The authors additionally investigate the correlation
between e-mail subject line and content by tracking language sentiment,
emotionality, and complexity, addressing the cases where collecting email
bodies is not permitted for privacy reasons. The findings extend the research
about robustness and stability of social networks metrics, after the
application of graph simplification strategies. The results have practical
implication for network analysts and for those company managers who rely on
network analytics (applied to company emails and social media data) to support
their decision-making processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1"&gt;P. A. Gloor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Comparison of Data Privacy Documents: A Preliminary Experiment on GDPR-like Laws. (arXiv:2105.10117v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10117</id>
        <link href="http://arxiv.org/abs/2105.10117"/>
        <updated>2021-05-24T05:08:38.223Z</updated>
        <summary type="html"><![CDATA[General Data Protection Regulation (GDPR) becomes a standard law for data
protection in many countries. Currently, twelve countries adopt the regulation
and establish their GDPR-like regulation. However, to evaluate the differences
and similarities of these GDPR-like regulations is time-consuming and needs a
lot of manual effort from legal experts. Moreover, GDPR-like regulations from
different countries are written in their languages leading to a more difficult
task since legal experts who know both languages are essential. In this paper,
we investigate a simple natural language processing (NLP) approach to tackle
the problem. We first extract chunks of information from GDPR-like documents
and form structured data from natural language. Next, we use NLP methods to
compare documents to measure their similarity. Finally, we manually label a
small set of data to evaluate our approach. The empirical result shows that the
BERT model with cosine similarity outperforms other baselines. Our data and
code are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kawintiranon_K/0/1/0/all/0/1"&gt;Kornraphop Kawintiranon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yaguang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[De-Biased Modelling of Search Click Behavior with Reinforcement Learning. (arXiv:2105.10072v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10072</id>
        <link href="http://arxiv.org/abs/2105.10072"/>
        <updated>2021-05-24T05:08:38.210Z</updated>
        <summary type="html"><![CDATA[Users' clicks on Web search results are one of the key signals for evaluating
and improving web search quality and have been widely used as part of current
state-of-the-art Learning-To-Rank(LTR) models. With a large volume of search
logs available for major search engines, effective models of searcher click
behavior have emerged to evaluate and train LTR models. However, when modeling
the users' click behavior, considering the bias of the behavior is imperative.
In particular, when a search result is not clicked, it is not necessarily
chosen as not relevant by the user, but instead could have been simply missed,
especially for lower-ranked results. These kinds of biases in the click log
data can be incorporated into the click models, propagating the errors to the
resulting LTR ranking models or evaluation metrics. In this paper, we propose
the De-biased Reinforcement Learning Click model (DRLC). The DRLC model relaxes
previously made assumptions about the users' examination behavior and resulting
latent states. To implement the DRLC model, convolutional neural networks are
used as the value networks for reinforcement learning, trained to learn a
policy to reduce bias in the click logs. To demonstrate the effectiveness of
the DRLC model, we first compare performance with the previous state-of-art
approaches using established click prediction metrics, including log-likelihood
and perplexity. We further show that DRLC also leads to improvements in ranking
performance. Our experiments demonstrate the effectiveness of the DRLC model in
learning to reduce bias in click logs, leading to improved modeling performance
and showing the potential for using DRLC for improving Web search quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianghong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zahiri_S/0/1/0/all/0/1"&gt;Sayyed M. Zahiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_S/0/1/0/all/0/1"&gt;Simon Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jadda_K/0/1/0/all/0/1"&gt;Khalifeh Al Jadda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kallumadi_S/0/1/0/all/0/1"&gt;Surya Kallumadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1"&gt;Eugene Agichtein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Load Balanced Recommendation Approach. (arXiv:2105.09981v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09981</id>
        <link href="http://arxiv.org/abs/2105.09981"/>
        <updated>2021-05-24T05:08:38.183Z</updated>
        <summary type="html"><![CDATA[Recommender systems (RSs) are software tools and algorithms developed to
alleviate the problem of information overload, which makes it difficult for a
user to make right decisions. Two main paradigms toward the recommendation
problem are collaborative filtering and content-based filtering, which try to
recommend the best items using ratings and content available. These methods
typically face infamous problems including cold-start, diversity, scalability,
and great computational expense. We argue that the uptake of deep learning and
reinforcement learning methods is also questionable due to their computational
complexities and uninterpretability. In this paper, we approach the
recommendation problem from a new prospective. We borrow ideas from cluster
head selection algorithms in wireless sensor networks and adapt them to the
recommendation problem. In particular, we propose Load Balanced Recommender
System (LBRS), which uses a probabilistic scheme for item recommendation.
Furthermore, we factor in the importance of items in the recommendation
process, which significantly improves the recommendation accuracy. We also
introduce a method that considers a heterogeneity among items, in order to
balance the similarity and diversity trade-off. Finally, we propose a new
metric for diversity, which emphasizes the importance of diversity not only
from an intra-list perspective, but also from a between-list point of view.
With experiments in a simulation study performed on RecSim, we show that LBRS
is effective and can outperform baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afsar_M/0/1/0/all/0/1"&gt;Mehdi Afsar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crump_T/0/1/0/all/0/1"&gt;Trafford Crump&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Far_B/0/1/0/all/0/1"&gt;Behrouz Far&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Mixed-Objective Pointing Decoders for Block-Level Optimization in Search Recommendation. (arXiv:2105.10152v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10152</id>
        <link href="http://arxiv.org/abs/2105.10152"/>
        <updated>2021-05-24T05:08:38.166Z</updated>
        <summary type="html"><![CDATA[Related or ideal follow-up suggestions to a web query in search engines are
often optimized based on several different parameters -- relevance to the
original query, diversity, click probability etc. One or many rankers may be
trained to score each suggestion from a candidate pool based on these factors.
These scorers are usually pairwise classification tasks where each training
example consists of a user query and a single suggestion from the list of
candidates. We propose an architecture that takes all candidate suggestions
associated with a given query and outputs a suggestion block. We discuss the
benefits of such an architecture over traditional approaches and experiment
with further enforcing each individual metric through mixed-objective training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kohli_H/0/1/0/all/0/1"&gt;Harsh Kohli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Have you tried Neural Topic Models? Comparative Analysis of Neural and Non-Neural Topic Models with Application to COVID-19 Twitter Data. (arXiv:2105.10165v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10165</id>
        <link href="http://arxiv.org/abs/2105.10165"/>
        <updated>2021-05-24T05:08:38.132Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used in studying social phenomena. We conduct a
comparative study examining state-of-the-art neural versus non-neural topic
models, performing a rigorous quantitative and qualitative assessment on a
dataset of tweets about the COVID-19 pandemic. Our results show that not only
do neural topic models outperform their classical counterparts on standard
evaluation metrics, but they also produce more coherent topics, which are of
great benefit when studying complex social problems. We also propose a novel
regularization term for neural topic models, which is designed to address the
well-documented problem of mode collapse, and demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1"&gt;Andrew Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1"&gt;Nga Than&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The MAMe Dataset: On the relevance of High Resolution and Variable Shape image properties. (arXiv:2007.13693v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13693</id>
        <link href="http://arxiv.org/abs/2007.13693"/>
        <updated>2021-05-23T06:10:40.919Z</updated>
        <summary type="html"><![CDATA[In the image classification task, the most common approach is to resize all
images in a dataset to a unique shape, while reducing their precision to a size
which facilitates experimentation at scale. This practice has benefits from a
computational perspective, but it entails negative side-effects on performance
due to loss of information and image deformation. In this work we introduce the
MAMe dataset, an image classification dataset with remarkable high resolution
and variable shape properties. The goal of MAMe is to provide a tool for
studying the impact of such properties in image classification, while
motivating research in the field. The MAMe dataset contains thousands of
artworks from three different museums, and proposes a classification task
consisting on differentiating between 29 mediums (i.e. materials and
techniques) supervised by art experts. After reviewing the singularity of MAMe
in the context of current image classification tasks, a thorough description of
the task is provided, together with dataset statistics. Experiments are
conducted to evaluate the impact of using high resolution images, variable
shape inputs and both properties at the same time. Results illustrate the
positive impact in performance when using high resolution images, while
highlighting the lack of solutions to exploit variable shapes. An additional
experiment exposes the distinctiveness between the MAMe dataset and the
prototypical ImageNet dataset. Finally, the baselines are inspected using
explainability methods and expert knowledge, to gain insights on the challenges
that remain ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pares_F/0/1/0/all/0/1"&gt;Ferran Par&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arias_Duart_A/0/1/0/all/0/1"&gt;Anna Arias-Duart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1"&gt;Dario Garcia-Gasulla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campo_Frances_G/0/1/0/all/0/1"&gt;Gema Campo-Franc&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viladrich_N/0/1/0/all/0/1"&gt;Nina Viladrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1"&gt;Eduard Ayguad&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Labarta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EiGLasso for Scalable Sparse Kronecker-Sum Inverse Covariance Estimation. (arXiv:2105.09872v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09872</id>
        <link href="http://arxiv.org/abs/2105.09872"/>
        <updated>2021-05-23T06:10:40.912Z</updated>
        <summary type="html"><![CDATA[In many real-world problems, complex dependencies are present both among
samples and among features. The Kronecker sum or the Cartesian product of two
graphs, each modeling dependencies across features and across samples, has been
used as an inverse covariance matrix for a matrix-variate Gaussian
distribution, as an alternative to a Kronecker-product inverse covariance
matrix, due to its more intuitive sparse structure. However, the existing
methods for sparse Kronecker-sum inverse covariance estimation are limited in
that they do not scale to more than a few hundred features and samples and that
the unidentifiable parameters pose challenges in estimation. In this paper, we
introduce EiGLasso, a highly scalable method for sparse Kronecker-sum inverse
covariance estimation, based on Newton's method combined with
eigendecomposition of the two graphs for exploiting the structure of Kronecker
sum. EiGLasso further reduces computation time by approximating the Hessian
based on the eigendecomposition of the sample and feature graphs. EiGLasso
achieves quadratic convergence with the exact Hessian and linear convergence
with the approximate Hessian. We describe a simple new approach to estimating
the unidentifiable parameters that generalizes the existing methods. On
simulated and real-world data, we demonstrate that EiGLasso achieves two to
three orders-of-magnitude speed-up compared to the existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jun Ho Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seyoung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Information in Stochastic Networks and Queues. (arXiv:2105.08769v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08769</id>
        <link href="http://arxiv.org/abs/2105.08769"/>
        <updated>2021-05-23T06:10:40.895Z</updated>
        <summary type="html"><![CDATA[We review the role of information and learning in the stability and
optimization of queueing systems. In recent years, techniques from supervised
learning, bandit learning and reinforcement learning have been applied to
queueing systems supported by increasing role of information in decision
making. We present observations and new results that help rationalize the
application of these areas to queueing systems.

We prove that the MaxWeight and BackPressure policies are an application of
Blackwell's Approachability Theorem. This connects queueing theoretic results
with adversarial learning. We then discuss the requirements of statistical
learning for service parameter estimation. As an example, we show how queue
size regret can be bounded when applying a perceptron algorithm to classify
service. Next, we discuss the role of state information in improved decision
making. Here we contrast the roles of epistemic information (information on
uncertain parameters) and aleatoric information (information on an uncertain
state). Finally we review recent advances in the theory of reinforcement
learning and queueing, as well as, provide discussion on current research
challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Walton_N/0/1/0/all/0/1"&gt;Neil Walton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kuang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Graph Neural Networks without explicit negative sampling. (arXiv:2103.14958v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14958</id>
        <link href="http://arxiv.org/abs/2103.14958"/>
        <updated>2021-05-23T06:10:40.871Z</updated>
        <summary type="html"><![CDATA[Real world data is mostly unlabeled or only few instances are labeled.
Manually labeling data is a very expensive and daunting task. This calls for
unsupervised learning techniques that are powerful enough to achieve comparable
results as semi-supervised/supervised techniques. Contrastive self-supervised
learning has emerged as a powerful direction, in some cases outperforming
supervised techniques. In this study, we propose, SelfGNN, a novel contrastive
self-supervised graph neural network (GNN) without relying on explicit
contrastive terms. We leverage Batch Normalization, which introduces implicit
contrastive terms, without sacrificing performance. Furthermore, as data
augmentation is key in contrastive learning, we introduce four feature
augmentation (FA) techniques for graphs. Though graph topological augmentation
(TA) is commonly used, our empirical findings show that FA perform as good as
TA. Moreover, FA incurs no computational overhead, unlike TA, which often has
O(N^3) time complexity, N-number of nodes. Our empirical evaluation on seven
publicly available real-world data shows that, SelfGNN is powerful and leads to
a performance comparable with SOTA supervised GNNs and always better than SOTA
semi-supervised and unsupervised GNNs. The source code is available at
https://github.com/zekarias-tilahun/SelfGNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kefato_Z/0/1/0/all/0/1"&gt;Zekarias T. Kefato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girdzijauskas_S/0/1/0/all/0/1"&gt;Sarunas Girdzijauskas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Offensive Language Identification for Low-resource Languages. (arXiv:2105.05996v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05996</id>
        <link href="http://arxiv.org/abs/2105.05996"/>
        <updated>2021-05-23T06:10:40.860Z</updated>
        <summary type="html"><![CDATA[Offensive content is pervasive in social media and a reason for concern to
companies and government organizations. Several studies have been recently
published investigating methods to detect the various forms of such content
(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of
these studies deal with English partially because most annotated datasets
available contain English data. In this paper, we take advantage of available
English datasets by applying cross-lingual contextual word embeddings and
transfer learning to make predictions in low-resource languages. We project
predictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,
Spanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in
TRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in
OffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513
F1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our
approach compares favourably to the best systems submitted to recent shared
tasks on these three languages. Additionally, we report competitive performance
on Arabic, and Turkish using the training and development sets of OffensEval
2020 shared task. The results for all languages confirm the robustness of
cross-lingual contextual embeddings and transfer learning for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1"&gt;Tharindu Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1"&gt;Marcos Zampieri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical One-Shot Federated Learning for Cross-Silo Setting. (arXiv:2010.01017v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01017</id>
        <link href="http://arxiv.org/abs/2010.01017"/>
        <updated>2021-05-23T06:10:40.852Z</updated>
        <summary type="html"><![CDATA[Federated learning enables multiple parties to collaboratively learn a model
without exchanging their data. While most existing federated learning
algorithms need many rounds to converge, one-shot federated learning (i.e.,
federated learning with a single communication round) is a promising approach
to make federated learning applicable in cross-silo setting in practice.
However, existing one-shot algorithms only support specific models and do not
provide any privacy guarantees, which significantly limit the applications in
practice. In this paper, we propose a practical one-shot federated learning
algorithm named FedKT. By utilizing the knowledge transfer technique, FedKT can
be applied to any classification models and can flexibly achieve differential
privacy guarantees. Our experiments on various tasks show that FedKT can
significantly outperform the other state-of-the-art federated learning
algorithms with a single communication round.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qinbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1"&gt;Bingsheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-Tuned Sim-to-Real Transfer. (arXiv:2104.07662v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07662</id>
        <link href="http://arxiv.org/abs/2104.07662"/>
        <updated>2021-05-23T06:10:40.837Z</updated>
        <summary type="html"><![CDATA[Policies trained in simulation often fail when transferred to the real world
due to the `reality gap' where the simulator is unable to accurately capture
the dynamics and visual properties of the real world. Current approaches to
tackle this problem, such as domain randomization, require prior knowledge and
engineering to determine how much to randomize system parameters in order to
learn a policy that is robust to sim-to-real transfer while also not being too
conservative. We propose a method for automatically tuning simulator system
parameters to match the real world using only raw RGB images of the real world
without the need to define rewards or estimate state. Our key insight is to
reframe the auto-tuning of parameters as a search problem where we iteratively
shift the simulation system parameters to approach the real-world system
parameters. We propose a Search Param Model (SPM) that, given a sequence of
observations and actions and a set of system parameters, predicts whether the
given parameters are higher or lower than the true parameters used to generate
the observations. We evaluate our method on multiple robotic control tasks in
both sim-to-sim and sim-to-real transfer, demonstrating significant improvement
over naive domain randomization. Project videos and code at
https://yuqingd.github.io/autotuned-sim2real/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuqing Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1"&gt;Olivia Watkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1"&gt;Deepak Pathak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variability of Artificial Neural Networks. (arXiv:2105.08911v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08911</id>
        <link href="http://arxiv.org/abs/2105.08911"/>
        <updated>2021-05-23T06:10:40.822Z</updated>
        <summary type="html"><![CDATA[What makes an artificial neural network easier to train and more likely to
produce desirable solutions than other comparable networks? In this paper, we
provide a new angle to study such issues under the setting of a fixed number of
model parameters which in general is the most dominant cost factor. We
introduce a notion of variability and show that it correlates positively to the
activation ratio and negatively to a phenomenon called {Collapse to Constants}
(or C2C), which is closely related but not identical to the phenomenon commonly
known as vanishing gradient. Experiments on a styled model problem empirically
verify that variability is indeed a key performance indicator for fully
connected neural networks. The insights gained from this variability study will
help the design of new and effective neural network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yueyao Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Support Recovery Using Very Few Measurements Per Sample. (arXiv:2105.09855v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2105.09855</id>
        <link href="http://arxiv.org/abs/2105.09855"/>
        <updated>2021-05-23T06:10:40.816Z</updated>
        <summary type="html"><![CDATA[In the problem of multiple support recovery, we are given access to linear
measurements of multiple sparse samples in $\mathbb{R}^{d}$. These samples can
be partitioned into $\ell$ groups, with samples having the same support
belonging to the same group. For a given budget of $m$ measurements per sample,
the goal is to recover the $\ell$ underlying supports, in the absence of the
knowledge of group labels. We study this problem with a focus on the
measurement-constrained regime where $m$ is smaller than the support size $k$
of each sample. We design a two-step procedure that estimates the union of the
underlying supports first, and then uses a spectral algorithm to estimate the
individual supports. Our proposed estimator can recover the supports with $m<k$
measurements per sample, from $\tilde{O}(k^{4}\ell^{4}/m^{4})$ samples. Our
guarantees hold for a general, generative model assumption on the samples and
measurement matrices. We also provide results from experiments conducted on
synthetic data and on the MNIST dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_L/0/1/0/all/0/1"&gt;Lekshmi Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murthy_C/0/1/0/all/0/1"&gt;Chandra R. Murthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1"&gt;Himanshu Tyagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches. (arXiv:2105.08506v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08506</id>
        <link href="http://arxiv.org/abs/2105.08506"/>
        <updated>2021-05-23T06:10:40.796Z</updated>
        <summary type="html"><![CDATA[Detecting COVID-19 in computed tomography (CT) or radiography images has been
proposed as a supplement to the definitive RT-PCR test. We present a deep
learning ensemble for detecting COVID-19 infection, combining slice-based (2D)
and volume-based (3D) approaches. The 2D system detects the infection on each
CT slice independently, combining them to obtain the patient-level decision via
different methods (averaging and long-short term memory networks). The 3D
system takes the whole CT volume to arrive to the patient-level decision in one
step. A new high resolution chest CT scan dataset, called the IST-C dataset, is
also collected in this work. The proposed ensemble, called IST-CovNet, obtains
90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting
COVID-19 among normal controls and other types of lung pathologies; and 93.69%
accuracy and 0.99 AUC score on the publicly available MosMed dataset that
consists of COVID-19 scans and normal controls only. The system is deployed at
Istanbul University Cerrahpasa School of Medicine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sara Atito Ali Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1"&gt;Mehmet Can Yavuz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sen_M/0/1/0/all/0/1"&gt;Mehmet Umut Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gulsen_F/0/1/0/all/0/1"&gt;Fatih Gulsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tutar_O/0/1/0/all/0/1"&gt;Onur Tutar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korkmazer_B/0/1/0/all/0/1"&gt;Bora Korkmazer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Samanci_C/0/1/0/all/0/1"&gt;Cesur Samanci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sirolu_S/0/1/0/all/0/1"&gt;Sabri Sirolu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamid_R/0/1/0/all/0/1"&gt;Rauf Hamid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eryurekli_A/0/1/0/all/0/1"&gt;Ali Ergun Eryurekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mammadov_T/0/1/0/all/0/1"&gt;Toghrul Mammadov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yanikoglu_B/0/1/0/all/0/1"&gt;Berrin Yanikoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04916</id>
        <link href="http://arxiv.org/abs/2105.04916"/>
        <updated>2021-05-23T06:10:40.788Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have been attached great importance due to
their biological plausibility and high energy-efficiency on neuromorphic chips.
As these chips are usually resource-constrained, the compression of SNNs is
thus crucial along the road of practical use of SNNs. Most existing methods
directly apply pruning approaches in artificial neural networks (ANNs) to SNNs,
which ignore the difference between ANNs and SNNs, thus limiting the
performance of the pruned SNNs. Besides, these methods are only suitable for
shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination
in the neural system, we propose gradient rewiring (Grad R), a joint learning
algorithm of connectivity and weight for SNNs, that enables us to seamlessly
optimize network structure without retrain. Our key innovation is to redefine
the gradient to a new synaptic parameter, allowing better exploration of
network structures by taking full advantage of the competition between pruning
and regrowth of connections. The experimental results show that the proposed
method achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset
so far. Moreover, it reaches a $\sim$3.5% accuracy loss under unprecedented
0.73% connectivity, which reveals remarkable structure refining capability in
SNNs. Our work suggests that there exists extremely high redundancy in deep
SNNs. Our codes are available at
https://github.com/Yanqi-Chen/Gradient-Rewiring .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Value Function is All You Need: A Unified Learning Framework for Ride Hailing Platforms. (arXiv:2105.08791v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08791</id>
        <link href="http://arxiv.org/abs/2105.08791"/>
        <updated>2021-05-23T06:10:40.781Z</updated>
        <summary type="html"><![CDATA[Large ride-hailing platforms, such as DiDi, Uber and Lyft, connect tens of
thousands of vehicles in a city to millions of ride demands throughout the day,
providing great promises for improving transportation efficiency through the
tasks of order dispatching and vehicle repositioning. Existing studies,
however, usually consider the two tasks in simplified settings that hardly
address the complex interactions between the two, the real-time fluctuations
between supply and demand, and the necessary coordinations due to the
large-scale nature of the problem. In this paper we propose a unified
value-based dynamic learning framework (V1D3) for tackling both tasks. At the
center of the framework is a globally shared value function that is updated
continuously using online experiences generated from real-time platform
transactions. To improve the sample-efficiency and the robustness, we further
propose a novel periodic ensemble method combining the fast online learning
with a large-scale offline training scheme that leverages the abundant
historical driver trajectory data. This allows the proposed framework to adapt
quickly to the highly dynamic environment, to generalize robustly to recurrent
patterns and to drive implicit coordinations among the population of managed
vehicles. Extensive experiments based on real-world datasets show considerably
improvements over other recently proposed methods on both tasks. Particularly,
V1D3 outperforms the first prize winners of both dispatching and repositioning
tracks in the KDD Cup 2020 RL competition, achieving state-of-the-art results
on improving both total driver income and user experience related metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yansheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dingyuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1"&gt;Bingchen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yongxin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jieping Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs. (arXiv:2105.08147v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08147</id>
        <link href="http://arxiv.org/abs/2105.08147"/>
        <updated>2021-05-23T06:10:40.774Z</updated>
        <summary type="html"><![CDATA[Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently
obtained to determine the extent of lung disease and are a valuable source of
data for creating artificial intelligence models. Most work to date assessing
disease severity on chest imaging has focused on segmenting computed tomography
(CT) images; however, given that CTs are performed much less frequently than
chest X-rays for COVID-19 patients, automated lung lesion segmentation on chest
X-rays could be clinically valuable. There currently exists a universal
shortage of chest X-rays with ground truth COVID-19 lung lesion annotations,
and manually contouring lung opacities is a tedious, labor-intensive task. To
accelerate severity detection and augment the amount of publicly available
chest X-ray training data for supervised deep learning (DL) models, we leverage
existing annotated CT images to generate frontal projection "chest X-ray"
images for training COVID-19 chest X-ray models. In this paper, we propose an
automated pipeline for segmentation of COVID-19 lung lesions on chest X-rays
comprised of a Mask R-CNN trained on a mixed dataset of open-source chest
X-rays and coronal X-ray projections computed from annotated volumetric CTs. On
a test set containing 40 chest X-rays of COVID-19 positive patients, our model
achieved IoU scores of 0.81 $\pm$ 0.03 and 0.79 $\pm$ 0.03 when trained on a
dataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50
projections from CTs, respectively. Our model far outperforms current baselines
with limited supervised training and may assist in automated COVID-19 severity
quantification on chest X-rays.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ramesh_V/0/1/0/all/0/1"&gt;Vignav Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rister_B/0/1/0/all/0/1"&gt;Blaine Rister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Coding Challenge Competence With APPS. (arXiv:2105.09938v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2105.09938</id>
        <link href="http://arxiv.org/abs/2105.09938"/>
        <updated>2021-05-23T06:10:40.767Z</updated>
        <summary type="html"><![CDATA[While programming is one of the most broadly applicable skills in modern
society, modern machine learning models still cannot code solutions to basic
problems. It can be difficult to accurately assess code generation performance,
and there has been surprisingly little work on evaluating code generation in a
way that is both flexible and rigorous. To meet this challenge, we introduce
APPS, a benchmark for code generation. Unlike prior work in more restricted
settings, our benchmark measures the ability of models to take an arbitrary
natural language specification and generate Python code fulfilling this
specification. Similar to how companies assess candidate software developers,
we then evaluate models by checking their generated code on test cases. Our
benchmark includes 10,000 problems, which range from having simple one-line
solutions to being substantial algorithmic challenges. We fine-tune large
language models on both GitHub and our training set, and we find that the
prevalence of syntax errors is decreasing exponentially. Recent models such as
GPT-Neo can pass approximately 15% of the test cases of introductory problems,
so we find that machine learning models are beginning to learn how to code. As
the social significance of automatic code generation increases over the coming
years, our benchmark can provide an important measure for tracking
advancements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1"&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1"&gt;Steven Basart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1"&gt;Saurav Kadavath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1"&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1"&gt;Akul Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1"&gt;Ethan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1"&gt;Collin Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puranik_S/0/1/0/all/0/1"&gt;Samir Puranik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Horace He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Be Causal: De-biasing Social Network Confounding in Recommendation. (arXiv:2105.07775v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07775</id>
        <link href="http://arxiv.org/abs/2105.07775"/>
        <updated>2021-05-23T06:10:40.748Z</updated>
        <summary type="html"><![CDATA[In recommendation systems, the existence of the missing-not-at-random (MNAR)
problem results in the selection bias issue, degrading the recommendation
performance ultimately. A common practice to address MNAR is to treat missing
entries from the so-called "exposure" perspective, i.e., modeling how an item
is exposed (provided) to a user. Most of the existing approaches use heuristic
models or re-weighting strategy on observed ratings to mimic the
missing-at-random setting. However, little research has been done to reveal how
the ratings are missing from a causal perspective. To bridge the gap, we
propose an unbiased and robust method called DENC (De-bias Network Confounding
in Recommendation) inspired by confounder analysis in causal inference. In
general, DENC provides a causal analysis on MNAR from both the inherent factors
(e.g., latent user or item factors) and auxiliary network's perspective.
Particularly, the proposed exposure model in DENC can control the social
network confounder meanwhile preserves the observed exposure information. We
also develop a deconfounding model through the balanced representation learning
to retain the primary user and item features, which enables DENC generalize
well on the rating prediction. Extensive experiments on three datasets validate
that our proposed model outperforms the state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiangmeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guandong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11156</id>
        <link href="http://arxiv.org/abs/2101.11156"/>
        <updated>2021-05-23T06:10:40.742Z</updated>
        <summary type="html"><![CDATA[We establish exact asymptotic expressions for the normalized mutual
information and minimum mean-square-error (MMSE) of sparse linear regression in
the sub-linear sparsity regime. Our result is achieved by a generalization of
the adaptive interpolation method in Bayesian inference for linear regimes to
sub-linear ones. A modification of the well-known approximate message passing
algorithm to approach the MMSE fundamental limit is also proposed, and its
state evolution is rigorously analysed. Our results show that the traditional
linear assumption between the signal dimension and number of observations in
the replica and adaptive interpolation methods is not necessary for sparse
signals. They also show how to modify the existing well-known AMP algorithms
for linear regimes to sub-linear ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1"&gt;Lan V. Truong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries. (arXiv:2105.09930v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09930</id>
        <link href="http://arxiv.org/abs/2105.09930"/>
        <updated>2021-05-23T06:10:40.735Z</updated>
        <summary type="html"><![CDATA[As more and more online search queries come from voice, automatic speech
recognition becomes a key component to deliver relevant search results. Errors
introduced by automatic speech recognition (ASR) lead to irrelevant search
results returned to the user, thus causing user dissatisfaction. In this paper,
we introduce an approach, Mondegreen, to correct voice queries in text space
without depending on audio signals, which may not always be available due to
system constraints or privacy or bandwidth (for example, some ASR systems run
on-device) considerations. We focus on voice queries transcribed via several
proprietary commercial ASR systems. These queries come from users making
internet, or online service search queries. We first present an analysis
showing how different the language distribution coming from user voice queries
is from that in traditional text corpora used to train off-the-shelf ASR
systems. We then demonstrate that Mondegreen can achieve significant
improvements in increased user interaction by correcting user voice queries in
one of the largest search systems in Google. Finally, we see Mondegreen as
complementing existing highly-optimized production ASR systems, which may not
be frequently retrained and thus lag behind due to vocabulary drifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1"&gt;Sukhdeep S. Sodhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chio_E/0/1/0/all/0/1"&gt;Ellie Ka-In Chio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1"&gt;Ambarish Jash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1"&gt;Santiago Onta&amp;#xf1;&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apte_A/0/1/0/all/0/1"&gt;Ajit Apte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ankit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeje_A/0/1/0/all/0/1"&gt;Ayooluwakunmi Jeje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1"&gt;Dima Kuzmin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_H/0/1/0/all/0/1"&gt;Harry Fung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Tze Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Effrat_J/0/1/0/all/0/1"&gt;Jon Effrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bali_T/0/1/0/all/0/1"&gt;Tarush Bali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1"&gt;Nitin Jindal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1"&gt;Pei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sarvjeet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Senqiang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1"&gt;Tameen Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wankhede_A/0/1/0/all/0/1"&gt;Amol Wankhede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alzantot_M/0/1/0/all/0/1"&gt;Moustafa Alzantot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Allen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_T/0/1/0/all/0/1"&gt;Tushar Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADASYN-Random Forest Based Intrusion Detection Model. (arXiv:2105.04301v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04301</id>
        <link href="http://arxiv.org/abs/2105.04301"/>
        <updated>2021-05-23T06:10:40.728Z</updated>
        <summary type="html"><![CDATA[Intrusion detection has been a key topic in the field of cyber security, and
the common network threats nowadays have the characteristics of varieties and
variation. Considering the serious imbalance of intrusion detection datasets
will result in low classification performance on attack behaviors of small
sample size and difficulty to detect network attacks accurately and
efficiently, using Adaptive Synthetic Sampling (ADASYN) method to balance
datasets was proposed in this paper. In addition, Random Forest algorithm was
used to train intrusion detection classifiers. Through the comparative
experiment of Intrusion detection on CICIDS 2017 dataset, it is found that
ADASYN with Random Forest performs better. Based on the experimental results,
the improvement of precision, recall, F1 scores and AUC values after ADASYN is
then analyzed. Experiments show that the proposed method can be applied to
intrusion detection with large data, and can effectively improve the
classification accuracy of network attack behaviors. Compared with traditional
machine learning models, it has better performance, generalization ability and
robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhewei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Linyue Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenwen Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incentivized Bandit Learning with Self-Reinforcing User Preferences. (arXiv:2105.08869v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08869</id>
        <link href="http://arxiv.org/abs/2105.08869"/>
        <updated>2021-05-23T06:10:40.722Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate a new multi-armed bandit (MAB) online learning
model that considers real-world phenomena in many recommender systems: (i) the
learning agent cannot pull the arms by itself and thus has to offer rewards to
users to incentivize arm-pulling indirectly; and (ii) if users with specific
arm preferences are well rewarded, they induce a "self-reinforcing" effect in
the sense that they will attract more users of similar arm preferences. Besides
addressing the tradeoff of exploration and exploitation, another key feature of
this new MAB model is to balance reward and incentivizing payment. The goal of
the agent is to maximize the total reward over a fixed time horizon $T$ with a
low total payment. Our contributions in this paper are two-fold: (i) We propose
a new MAB model with random arm selection that considers the relationship of
users' self-reinforcing preferences and incentives; and (ii) We leverage the
properties of a multi-color Polya urn with nonlinear feedback model to propose
two MAB policies termed "At-Least-$n$ Explore-Then-Commit" and "UCB-List". We
prove that both policies achieve $O(log T)$ expected regret with $O(log T)$
expected payment over a time horizon $T$. We conduct numerical simulations to
demonstrate and verify the performances of these two policies and study their
robustness under various settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianchen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chaosheng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jingyuan Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capsule GAN for Prostate MRI Super-Resolution. (arXiv:2105.07495v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07495</id>
        <link href="http://arxiv.org/abs/2105.07495"/>
        <updated>2021-05-23T06:10:40.703Z</updated>
        <summary type="html"><![CDATA[Prostate cancer is a very common disease among adult men. One in seven
Canadian men is diagnosed with this cancer in their lifetime. Super-Resolution
(SR) can facilitate early diagnosis and potentially save many lives. In this
paper, a robust and accurate model is proposed for prostate MRI SR. The model
is trained on the Prostate-Diagnosis and PROSTATEx datasets. The proposed model
outperformed the state-of-the-art prostate SR model in all similarity metrics
with notable margins. A new task-specific similarity assessment is introduced
as well. A classifier is trained for severe cancer detection and the drop in
the accuracy of this model when dealing with super-resolved images is used for
evaluating the ability of medical detail reconstruction of the SR models. The
proposed SR model is a step towards an efficient and accurate general medical
SR platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majdabadi_M/0/1/0/all/0/1"&gt;Mahdiyar Molahasani Majdabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Younhee Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deivalakshmi_S/0/1/0/all/0/1"&gt;S. Deivalakshmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1"&gt;Seokbum Ko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CURE: Code-Aware Neural Machine Translation for Automatic Program Repair. (arXiv:2103.00073v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00073</id>
        <link href="http://arxiv.org/abs/2103.00073"/>
        <updated>2021-05-23T06:10:40.696Z</updated>
        <summary type="html"><![CDATA[Automatic program repair (APR) is crucial to improve software reliability.
Recently, neural machine translation (NMT) techniques have been used to fix
software bugs automatically. While promising, these approaches have two major
limitations. Their search space often does not contain the correct fix, and
their search strategy ignores software knowledge such as strict code syntax.
Due to these limitations, existing NMT-based techniques underperform the best
template-based approaches.

We propose CURE, a new NMT-based APR technique with three major novelties.
First, CURE pre-trains a programming language (PL) model on a large software
codebase to learn developer-like source code before the APR task. Second, CURE
designs a new code-aware search strategy that finds more correct fixes by
focusing on compilable patches and patches that are close in length to the
buggy code. Finally, CURE uses a subword tokenization technique to generate a
smaller search space that contains more correct fixes.

Our evaluation on two widely-used benchmarks shows that CURE correctly fixes
57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR
techniques on both benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutellier_T/0/1/0/all/0/1"&gt;Thibaud Lutellier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1"&gt;Lin Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Estimation Is Not Optimal: How to Use Kalman Filter the Right Way. (arXiv:2104.02372v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02372</id>
        <link href="http://arxiv.org/abs/2104.02372"/>
        <updated>2021-05-23T06:10:40.667Z</updated>
        <summary type="html"><![CDATA[Determining the noise parameters of a Kalman Filter (KF) has been studied for
decades. A huge body of research focuses on the task of estimation of the noise
under various conditions, since precise noise estimation is considered
equivalent to minimization of the filtering errors. However, we show that even
a small violation of the KF assumptions can significantly modify the effective
noise, breaking the equivalence between the tasks and making noise estimation
an inferior strategy. We show that such violations are very common, and are
often not trivial to handle or even notice. Consequentially, we argue that a
robust solution is needed - rather than choosing a dedicated model per problem.
To that end, we apply gradient-based optimization to the filtering errors
directly, with relation to a simple and efficient parameterization of the
symmetric and positive-definite parameters of KF. In radar tracking and video
tracking, we show that the optimization improves both the accuracy of KF and
its robustness to design decisions. In addition, we demonstrate how an
optimized neural network model can seem to reduce the errors significantly
compared to a KF - and how this reduction vanishes once the KF is optimized
similarly. This indicates how complicated models can be wrongly identified as
superior to KF, while in fact they were merely more optimized.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greenberg_I/0/1/0/all/0/1"&gt;Ido Greenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yannay_N/0/1/0/all/0/1"&gt;Netanel Yannay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A data-driven approach to the forecasting of ground-level ozone concentration. (arXiv:2012.00685v3 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00685</id>
        <link href="http://arxiv.org/abs/2012.00685"/>
        <updated>2021-05-23T06:10:40.661Z</updated>
        <summary type="html"><![CDATA[The ability to forecast the concentration of air pollutants in an urban
region is crucial for decision-makers wishing to reduce the impact of pollution
on public health through active measures (e.g. temporary traffic closures). In
this study, we present a machine learning approach applied to the forecast of
the day-ahead maximum value of the ozone concentration for several geographical
locations in southern Switzerland. Due to the low density of measurement
stations and to the complex orography of the use case terrain, we adopted
feature selection methods instead of explicitly restricting relevant features
to a neighbourhood of the prediction sites, as common in spatio-temporal
forecasting methods. We then used Shapley values to assess the explainability
of the learned models in terms of feature importance and feature interactions
in relation to ozone predictions; our analysis suggests that the trained models
effectively learned explanatory cross-dependencies among atmospheric variables.
Finally, we show how weighting observations helps in increasing the accuracy of
the forecasts for specific ranges of ozone's daily peak values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Marvin_D/0/1/0/all/0/1"&gt;Dario Marvin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nespoli_L/0/1/0/all/0/1"&gt;Lorenzo Nespoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Strepparava_D/0/1/0/all/0/1"&gt;Davide Strepparava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Medici_V/0/1/0/all/0/1"&gt;Vasco Medici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization. (arXiv:2105.06129v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06129</id>
        <link href="http://arxiv.org/abs/2105.06129"/>
        <updated>2021-05-23T06:10:40.654Z</updated>
        <summary type="html"><![CDATA[Artistic style transfer aims to transfer the style characteristics of one
image onto another image while retaining its content. Existing approaches
commonly leverage various normalization techniques, although these face
limitations in adequately transferring diverse textures to different spatial
locations. Self-Attention-based approaches have tackled this issue with partial
success but suffer from unwanted artifacts. Motivated by these observations,
this paper aims to combine the best of both worlds: self-attention and
normalization. That yields a new plug-and-play module that we name
Self-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially
a spatially adaptive normalization module whose parameters are inferred through
attention on the content and style image. We demonstrate that plugging SAFIN
into the base network of another state-of-the-art method results in enhanced
stylization. We also develop a novel base network composed of Wavelet Transform
for multi-scale style transfer, which when combined with SAFIN, produces
visually appealing results with lesser unwanted textures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Aaditya Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hingane_S/0/1/0/all/0/1"&gt;Shreeshail Hingane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1"&gt;Xinyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction. (arXiv:2105.06709v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06709</id>
        <link href="http://arxiv.org/abs/2105.06709"/>
        <updated>2021-05-23T06:10:40.635Z</updated>
        <summary type="html"><![CDATA[The study of multi-type Protein-Protein Interaction (PPI) is fundamental for
understanding biological processes from a systematic perspective and revealing
disease mechanisms. Existing methods suffer from significant performance
degradation when tested in unseen dataset. In this paper, we investigate the
problem and find that it is mainly attributed to the poor performance for
inter-novel-protein interaction prediction. However, current evaluations
overlook the inter-novel-protein interactions, and thus fail to give an
instructive assessment. As a result, we propose to address the problem from
both the evaluation and the methodology. Firstly, we design a new evaluation
framework that fully respects the inter-novel-protein interactions and gives
consistent assessment across datasets. Secondly, we argue that correlations
between proteins must provide useful information for analysis of novel
proteins, and based on this, we propose a graph neural network based method
(GNN-PPI) for better inter-novel-protein interaction prediction. Experimental
results on real-world datasets of different scales demonstrate that GNN-PPI
significantly outperforms state-of-the-art PPI prediction methods, especially
for the inter-novel-protein interaction prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1"&gt;Guofeng Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiqiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1"&gt;Yanguang Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaoting Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoding Explanatory Knowledge for Zero-shot Science Question Answering. (arXiv:2105.05737v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05737</id>
        <link href="http://arxiv.org/abs/2105.05737"/>
        <updated>2021-05-23T06:10:40.486Z</updated>
        <summary type="html"><![CDATA[This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge
Transfer), a novel method for the automatic transfer of explanatory knowledge
through neural encoding mechanisms. We demonstrate that N-XKT is able to
improve accuracy and generalization on science Question Answering (QA).
Specifically, by leveraging facts from background explanatory knowledge
corpora, the N-XKT model shows a clear improvement on zero-shot QA.
Furthermore, we show that N-XKT can be fine-tuned on a target QA dataset,
enabling faster convergence and more accurate results. A systematic analysis is
conducted to quantitatively analyze the performance of the N-XKT model and the
impact of different categories of knowledge on the zero-shot generalization
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zili Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1"&gt;Marco Valentino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1"&gt;Donal Landers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1"&gt;Andre Freitas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization. (arXiv:2104.04785v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04785</id>
        <link href="http://arxiv.org/abs/2104.04785"/>
        <updated>2021-05-23T06:10:40.479Z</updated>
        <summary type="html"><![CDATA[As climate change increases the intensity of natural disasters, society needs
better tools for adaptation. Floods, for example, are the most frequent natural
disaster, and better tools for flood risk communication could increase the
support for flood-resilient infrastructure development. Our work aims to enable
more visual communication of large-scale climate impacts via visualizing the
output of coastal flood models as satellite imagery. We propose the first deep
learning pipeline to ensure physical-consistency in synthetic visual satellite
imagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it
produces imagery that is physically-consistent with the output of an
expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery
relative to physics-based flood maps, we find that our proposed framework
outperforms baseline models in both physical-consistency and photorealism. We
envision our work to be the first step towards a global visualization of how
climate change shapes our landscape. Continuing on this path, we show that the
proposed pipeline generalizes to visualize arctic sea ice melt. We also publish
a dataset of over 25k labelled image-pairs to study image-to-image translation
in Earth observation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leshchinskiy_B/0/1/0/all/0/1"&gt;Brandon Leshchinskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Requena_Mesa_C/0/1/0/all/0/1"&gt;Christian Requena-Mesa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chishtie_F/0/1/0/all/0/1"&gt;Farrukh Chishtie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1"&gt;Natalia D&amp;#xed;az-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1"&gt;Oc&amp;#xe9;ane Boulais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aruna Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pina_A/0/1/0/all/0/1"&gt;Aaron Pi&amp;#xf1;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raissi_C/0/1/0/all/0/1"&gt;Chedy Ra&amp;#xef;ssi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1"&gt;Alexander Lavin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1"&gt;Dava Newman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proximal Learning for Individualized Treatment Regimes Under Unmeasured Confounding. (arXiv:2105.01187v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01187</id>
        <link href="http://arxiv.org/abs/2105.01187"/>
        <updated>2021-05-23T06:10:40.465Z</updated>
        <summary type="html"><![CDATA[Data-driven individualized decision making has recently received increasing
research interests. Most existing methods rely on the assumption of no
unmeasured confounding, which unfortunately cannot be ensured in practice
especially in observational studies. Motivated by the recent proposed proximal
causal inference, we develop several proximal learning approaches to estimating
optimal individualized treatment regimes (ITRs) in the presence of unmeasured
confounding. In particular, we establish several identification results for
different classes of ITRs, exhibiting the trade-off between the risk of making
untestable assumptions and the value function improvement in decision making.
Based on these results, we propose several classification-based approaches to
finding a variety of restricted in-class optimal ITRs and develop their
theoretical properties. The appealing numerical performance of our proposed
methods is demonstrated via an extensive simulation study and one real data
application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhengling Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Miao_R/0/1/0/all/0/1"&gt;Rui Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoke Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03842</id>
        <link href="http://arxiv.org/abs/2105.03842"/>
        <updated>2021-05-23T06:10:40.453Z</updated>
        <summary type="html"><![CDATA[Error correction techniques have been used to refine the output sentences
from automatic speech recognition (ASR) models and achieve a lower word error
rate (WER) than original ASR outputs. Previous works usually use a
sequence-to-sequence model to correct an ASR output sentence autoregressively,
which causes large latency and cannot be deployed in online ASR services. A
straightforward solution to reduce latency, inspired by non-autoregressive
(NAR) neural machine translation, is to use an NAR sequence generation model
for ASR error correction, which, however, comes at the cost of significantly
increased ASR error rate. In this paper, observing distinctive error patterns
and correction operations (i.e., insertion, deletion, and substitution) in ASR,
we propose FastCorrect, a novel NAR error correction model based on edit
alignment. In training, FastCorrect aligns each source token from an ASR output
sentence to the target tokens from the corresponding ground-truth sentence
based on the edit distance between the source and target sentences, and
extracts the number of target tokens corresponding to each source token during
edition/correction, which is then used to train a length predictor and to
adjust the source tokens to match the length of the target sentence for
parallel generation. In inference, the token number predicted by the length
predictor is used to adjust the source tokens for target sequence generation.
Experiments on the public AISHELL-1 dataset and an internal industrial-scale
ASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)
it speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER
reduction) compared with the autoregressive correction model; and 2) it
outperforms the accuracy of popular NAR models adopted in neural machine
translation by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1"&gt;Yichong Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Linchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Renqian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Linquan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang-Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1"&gt;Ed Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models. (arXiv:2104.10683v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10683</id>
        <link href="http://arxiv.org/abs/2104.10683"/>
        <updated>2021-05-23T06:10:40.434Z</updated>
        <summary type="html"><![CDATA[(Artificial) neural networks have become increasingly popular in mechanics as
means to accelerate computations with model order reduction techniques and as
universal models for a wide variety of materials. However, the major
disadvantage of neural networks remains: their numerous parameters are
challenging to interpret and explain. Thus, neural networks are often labeled
as black boxes, and their results often elude human interpretation. In
mechanics, the new and active field of physics-informed neural networks
attempts to mitigate this disadvantage by designing deep neural networks on the
basis of mechanical knowledge. By using this a priori knowledge, deeper and
more complex neural networks became feasible, since the mechanical assumptions
could be explained. However, the internal reasoning and explanation of neural
network parameters remain mysterious.

Complementary to the physics-informed approach, we propose a first step
towards a physics-informing approach, which explains neural networks trained on
mechanical data a posteriori. This novel explainable artificial intelligence
approach aims at elucidating the black box of neural networks and their
high-dimensional representations. Therein, the principal component analysis
decorrelates the distributed representations in cell states of RNNs and allows
the comparison to known and fundamental functions. The novel approach is
supported by a systematic hyperparameter search strategy that identifies the
best neural network architectures and training parameters. The findings of
three case studies on fundamental constitutive models (hyperelasticity,
elastoplasticity, and viscoelasticity) imply that the proposed strategy can
help identify numerical and analytical closed-form solutions to characterize
new materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koeppe_A/0/1/0/all/0/1"&gt;Arnd Koeppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamer_F/0/1/0/all/0/1"&gt;Franz Bamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Selzer_M/0/1/0/all/0/1"&gt;Michael Selzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nestler_B/0/1/0/all/0/1"&gt;Britta Nestler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markert_B/0/1/0/all/0/1"&gt;Bernd Markert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications. (arXiv:2012.01059v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01059</id>
        <link href="http://arxiv.org/abs/2012.01059"/>
        <updated>2021-05-23T06:10:40.406Z</updated>
        <summary type="html"><![CDATA[Improving irradiance forecasting is critical to further increase the share of
solar in the energy mix. On a short time scale, fish-eye cameras on the ground
are used to capture cloud displacements causing the local variability of the
electricity production. As most of the solar radiation comes directly from the
Sun, current forecasting approaches use its position in the image as a
reference to interpret the cloud cover dynamics. However, existing Sun tracking
methods rely on external data and a calibration of the camera, which requires
access to the device. To address these limitations, this study introduces an
image-based Sun tracking algorithm to localise the Sun in the image when it is
visible and interpolate its daily trajectory from past observations. We
validate the method on a set of sky images collected over a year at SIRTA's
lab. Experimental results show that the proposed method provides robust smooth
Sun trajectories with a mean absolute error below 1% of the image size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1"&gt;Quentin Paletta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Abductive Knowledge Induction From Raw Data. (arXiv:2010.03514v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03514</id>
        <link href="http://arxiv.org/abs/2010.03514"/>
        <updated>2021-05-23T06:10:40.399Z</updated>
        <summary type="html"><![CDATA[For many reasoning-heavy tasks involving raw inputs, it is challenging to
design an appropriate end-to-end learning pipeline. Neuro-Symbolic Learning,
divide the process into sub-symbolic perception and symbolic reasoning, trying
to utilise data-driven machine learning and knowledge-driven reasoning
simultaneously. However, they suffer from the exponential computational
complexity within the interface between these two components, where the
sub-symbolic learning model lacks direct supervision, and the symbolic model
lacks accurate input facts. Hence, most of them assume the existence of a
strong symbolic knowledge base and only learn the perception model while
avoiding a crucial problem: where does the knowledge come from? In this paper,
we present Abductive Meta-Interpretive Learning ($Meta_{Abd}$) that unites
abduction and induction to learn neural networks and induce logic theories
jointly from raw data. Experimental results demonstrate that $Meta_{Abd}$ not
only outperforms the compared systems in predictive accuracy and data
efficiency but also induces logic programs that can be re-used as background
knowledge in subsequent learning tasks. To the best of our knowledge,
$Meta_{Abd}$ is the first system that can jointly learn neural networks from
scratch and induce recursive first-order logic theories with predicate
invention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wang-Zhou Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muggleton_S/0/1/0/all/0/1"&gt;Stephen H. Muggleton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Machine Learning on Graphs: A Survey. (arXiv:2103.00742v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00742</id>
        <link href="http://arxiv.org/abs/2103.00742"/>
        <updated>2021-05-23T06:10:40.390Z</updated>
        <summary type="html"><![CDATA[Machine learning on graphs has been extensively studied in both academic and
industry. However, as the literature on graph learning booms with a vast number
of emerging methods and techniques, it becomes increasingly difficult to
manually design the optimal machine learning algorithm for different
graph-related tasks. To solve this critical challenge, automated machine
learning (AutoML) on graphs which combines the strength of graph machine
learning and AutoML together, is gaining attention from the research community.
Therefore, we comprehensively survey AutoML on graphs in this paper, primarily
focusing on hyper-parameter optimization (HPO) and neural architecture search
(NAS) for graph machine learning. We further overview libraries related to
automated graph machine learning and in-depth discuss AutoGL, the first
dedicated open-source library for AutoML on graphs. In the end, we share our
insights on future research directions for automated graph machine learning.
This paper is the first systematic and comprehensive review of automated
machine learning on graphs to the best of our knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenwu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedMood: Federated Learning on Mobile Health Data for Mood Detection. (arXiv:2102.09342v6 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09342</id>
        <link href="http://arxiv.org/abs/2102.09342"/>
        <updated>2021-05-23T06:10:40.383Z</updated>
        <summary type="html"><![CDATA[Depression is one of the most common mental illness problems, and the
symptoms shown by patients are not consistent, making it difficult to diagnose
in the process of clinical practice and pathological research. Although
researchers hope that artificial intelligence can contribute to the diagnosis
and treatment of depression, the traditional centralized machine learning needs
to aggregate patient data, and the data privacy of patients with mental illness
needs to be strictly confidential, which hinders machine learning algorithms
clinical application. To solve the problem of privacy of the medical history of
patients with depression, we implement federated learning to analyze and
diagnose depression. First, we propose a general multi-view federated learning
framework using multi-source data, which can extend any traditional machine
learning model to support federated learning across different institutions or
parties. Secondly, we adopt late fusion methods to solve the problem of
inconsistent time series of multi-view data. Finally, we compare the federated
framework with other cooperative learning frameworks in performance and discuss
the related results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaohang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhuiyan_M/0/1/0/all/0/1"&gt;Md Zakirul Alam Bhuiyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lianzhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Human Trajectories by Learning and Matching Patterns. (arXiv:2104.10241v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10241</id>
        <link href="http://arxiv.org/abs/2104.10241"/>
        <updated>2021-05-23T06:10:40.362Z</updated>
        <summary type="html"><![CDATA[Thesis document of the degree of Master of Science in Robotics of Carnegie
Mellon University School of Computer Science.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dapeng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoML to Date and Beyond: Challenges and Opportunities. (arXiv:2010.10777v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10777</id>
        <link href="http://arxiv.org/abs/2010.10777"/>
        <updated>2021-05-23T06:10:40.354Z</updated>
        <summary type="html"><![CDATA[As big data becomes ubiquitous across domains, and more and more stakeholders
aspire to make the most of their data, demand for machine learning tools has
spurred researchers to explore the possibilities of automated machine learning
(AutoML). AutoML tools aim to make machine learning accessible for non-machine
learning experts (domain experts), to improve the efficiency of machine
learning, and to accelerate machine learning research. But although automation
and efficiency are among AutoML's main selling points, the process still
requires human involvement at a number of vital steps, including understanding
the attributes of domain-specific data, defining prediction problems, creating
a suitable training data set, and selecting a promising machine learning
technique. These steps often require a prolonged back-and-forth that makes this
process inefficient for domain experts and data scientists alike, and keeps
so-called AutoML systems from being truly automatic. In this review article, we
introduce a new classification system for AutoML systems, using a seven-tiered
schematic to distinguish these systems based on their level of autonomy. We
begin by describing what an end-to-end machine learning pipeline actually looks
like, and which subtasks of the machine learning pipeline have been automated
so far. We highlight those subtasks which are still done manually - generally
by a data scientist - and explain how this limits domain experts' access to
machine learning. Next, we introduce our novel level-based taxonomy for AutoML
systems and define each level according to the scope of automation support
provided. Finally, we lay out a roadmap for the future, pinpointing the
research required to further automate the end-to-end machine learning pipeline
and discussing important challenges that stand in the way of this ambitious
goal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santu_S/0/1/0/all/0/1"&gt;Shubhra Kanti Karmaker Santu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1"&gt;Md. Mahadi Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1"&gt;Micah J. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1"&gt;ChengXiang Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1"&gt;Kalyan Veeramachaneni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducibility Report: La-MAML: Look-ahead Meta Learning for Continual Learning. (arXiv:2102.05824v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05824</id>
        <link href="http://arxiv.org/abs/2102.05824"/>
        <updated>2021-05-23T06:10:40.345Z</updated>
        <summary type="html"><![CDATA[The Continual Learning (CL) problem involves performing well on a sequence of
tasks under limited compute. Current algorithms in the domain are either slow,
offline or sensitive to hyper-parameters. La-MAML, an optimization-based
meta-learning algorithm claims to be better than other replay-based,
prior-based and meta-learning based approaches. According to the MER paper [1],
metrics to measure performance in the continual learning arena are Retained
Accuracy (RA) and Backward Transfer-Interference (BTI). La-MAML claims to
perform better in these values when compared to the SOTA in the domain. This is
the main claim of the paper, which we shall be verifying in this report.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_J/0/1/0/all/0/1"&gt;Joel Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1"&gt;Alex Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Approximation Rates and Metric Entropy of ReLU$^k$ and Cosine Networks. (arXiv:2101.12365v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12365</id>
        <link href="http://arxiv.org/abs/2101.12365"/>
        <updated>2021-05-23T06:10:40.334Z</updated>
        <summary type="html"><![CDATA[This article addresses several fundamental issues associated with the
approximation theory of neural networks, including the characterization of
approximation spaces, the determination of the metric entropy of these spaces,
and approximation rates of neural networks. For any activation function
$\sigma$, we show that the largest Banach space of functions which can be
efficiently approximated by the corresponding shallow neural networks is the
space whose norm is given by the gauge of the closed convex hull of the set
$\{\pm\sigma(\omega\cdot x + b)\}$. We characterize this space for the ReLU$^k$
and cosine activation functions and, in particular, show that the resulting
gauge space is equivalent to the spectral Barron space if $\sigma=\cos$ and is
equivalent to the Barron space when $\sigma={\rm ReLU}$. Our main result
establishes the precise asymptotics of the $L^2$-metric entropy of the unit
ball of these guage spaces and, as a consequence, the optimal approximation
rates for shallow ReLU$^k$ networks. The sharpest previous results hold only in
the special case that $k=0$ and $d=2$, where the metric entropy has been
determined up to logarithmic factors. When $k > 0$ or $d > 2$, there is a
significant gap between the previous best upper and lower bounds. We close all
of these gaps and determine the precise asymptotics of the metric entropy for
all $k \geq 0$ and $d\geq 2$, including removing the logarithmic factors
previously mentioned. Finally, we use these results to quantify how much is
lost by Barron's spectral condition relative to the convex hull of
$\{\pm\sigma(\omega\cdot x + b)\}$ when $\sigma={\rm ReLU}^k$. Finally, we also
show that the orthogonal greedy algorithm can algorithmically realize the
improved approximation rates which have been derived.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1"&gt;Jonathan W. Siegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinchao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[tFold-TR: Combining Deep Learning Enhanced Hybrid Potential Energy for Template-Based Modelling Structure Refinement. (arXiv:2105.04350v2 [physics.bio-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04350</id>
        <link href="http://arxiv.org/abs/2105.04350"/>
        <updated>2021-05-23T06:10:40.318Z</updated>
        <summary type="html"><![CDATA[Proteins structure prediction has long been a grand challenge over the past
50 years, owing to its board scientific and application interests. There are
two major types of modelling algorithm, template-free modelling and
template-based modelling, which is suitable for easy prediction tasks, and is
widely adopted in computer aided drug discoveries for drug design and
screening. Although it has been several decades since its first edition, the
current template-based modeling approach suffers from two important problems:
1) there are many missing regions in the template-query sequence alignment, and
2) the accuracy of the distance pairs from different regions of the template
varies, and this information is not well introduced into the modeling. To solve
the two problems, we propose a structural optimization process based on
template modelling, introducing two neural network models predict the distance
information of the missing regions and the accuracy of the distance pairs of
different regions in the template modeling structure. The predicted distances
and residue pairwise specific accuracy information are incorporated into the
potential energy function for structural optimization, which significantly
improves the qualities of the original template modelling decoys.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liangzhen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lan_H/0/1/0/all/0/1"&gt;Haidong Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shen_T/0/1/0/all/0/1"&gt;Tao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiaxiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Thermal Anomaly Detection for Batteries using Unsupervised Shape Clustering. (arXiv:2103.08796v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08796</id>
        <link href="http://arxiv.org/abs/2103.08796"/>
        <updated>2021-05-23T06:10:40.311Z</updated>
        <summary type="html"><![CDATA[For electric vehicles (EV) and energy storage (ES) batteries, thermal runaway
is a critical issue as it can lead to uncontrollable fires or even explosions.
Thermal anomaly detection can identify problematic battery packs that may
eventually undergo thermal runaway. However, there are common challenges like
data unavailability, environment and configuration variations, and battery
aging. We propose a data-driven method to detect battery thermal anomaly based
on comparing shape-similarity between thermal measurements. Based on their
shapes, the measurements are continuously being grouped into different
clusters. Anomaly is detected by monitoring deviations within the clusters.
Unlike model-based or other data-driven methods, the proposed method is robust
to data loss and requires minimal reference data for different pack
configurations. As the initial experimental results show, the method not only
can be more accurate than the onboard BMS and but also can detect unforeseen
anomalies at the early stage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaojun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdollahi_A/0/1/0/all/0/1"&gt;Ali Abdollahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jones_T/0/1/0/all/0/1"&gt;Trevor Jones&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The World as a Graph: Improving El Ni\~no Forecasts with Graph Neural Networks. (arXiv:2104.05089v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05089</id>
        <link href="http://arxiv.org/abs/2104.05089"/>
        <updated>2021-05-23T06:10:40.304Z</updated>
        <summary type="html"><![CDATA[Deep learning-based models have recently outperformed state-of-the-art
seasonal forecasting models, such as for predicting El Ni\~no-Southern
Oscillation (ENSO). However, current deep learning models are based on
convolutional neural networks which are difficult to interpret and can fail to
model large-scale atmospheric patterns. In comparison, graph neural networks
(GNNs) are capable of modeling large-scale spatial dependencies and are more
interpretable due to the explicit modeling of information flow through edge
connections. We propose the first application of graph neural networks to
seasonal forecasting. We design a novel graph connectivity learning module that
enables our GNN model to learn large-scale spatial interactions jointly with
the actual ENSO forecasting task. Our model, \graphino, outperforms
state-of-the-art deep learning-based models for forecasts up to six months
ahead. Additionally, we show that our model is more interpretable as it learns
sensible connectivity structures that correlate with the ENSO anomaly pattern.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cachay_S/0/1/0/all/0/1"&gt;Salva R&amp;#xfc;hling Cachay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erickson_E/0/1/0/all/0/1"&gt;Emma Erickson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucker_A/0/1/0/all/0/1"&gt;Arthur Fender C. Bucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokropek_E/0/1/0/all/0/1"&gt;Ernest Pokropek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potosnak_W/0/1/0/all/0/1"&gt;Willa Potosnak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bire_S/0/1/0/all/0/1"&gt;Suyash Bire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1"&gt;Salomey Osei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Standard Backpropagation Forget Less Catastrophically Than Adam?. (arXiv:2102.07686v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07686</id>
        <link href="http://arxiv.org/abs/2102.07686"/>
        <updated>2021-05-23T06:10:40.297Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting remains a severe hindrance to the broad application
of artificial neural networks (ANNs), however, it continues to be a poorly
understood phenomenon. Despite the extensive amount of work on catastrophic
forgetting, we argue that it is still unclear how exactly the phenomenon should
be quantified, and, moreover, to what degree all of the choices we make when
designing learning systems affect the amount of catastrophic forgetting. We use
various testbeds from the reinforcement learning and supervised learning
literature to (1) provide evidence that the choice of which modern
gradient-based optimization algorithm is used to train an ANN has a significant
impact on the amount of catastrophic forgetting and show that-surprisingly-in
many instances classical algorithms such as vanilla SGD experience less
catastrophic forgetting than the more modern algorithms such as Adam. We
empirically compare four different existing metrics for quantifying
catastrophic forgetting and (2) show that the degree to which the learning
systems experience catastrophic forgetting is sufficiently sensitive to the
metric used that a change from one principled metric to another is enough to
change the conclusions of a study dramatically. Our results suggest that a much
more rigorous experimental methodology is required when looking at catastrophic
forgetting. Based on our results, we recommend inter-task forgetting in
supervised learning must be measured with both retention and relearning metrics
concurrently, and intra-task forgetting in reinforcement learning must-at the
very least-be measured with pairwise interference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1"&gt;Dylan R. Ashley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiassian_S/0/1/0/all/0/1"&gt;Sina Ghiassian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1"&gt;Richard S. Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible DenseNets with Concatenated LipSwish. (arXiv:2102.02694v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02694</id>
        <link href="http://arxiv.org/abs/2102.02694"/>
        <updated>2021-05-23T06:10:40.286Z</updated>
        <summary type="html"><![CDATA[We introduce Invertible Dense Networks (i-DenseNets), a more parameter
efficient extension of Residual Flows. The method relies on an analysis of the
Lipschitz continuity of the concatenation in DenseNets, where we enforce
invertibility of the network by satisfying the Lipschitz constant. Furthermore,
we propose a learnable weighted concatenation, which not only improves the
model performance but also indicates the importance of the concatenated
weighted representation. Additionally, we introduce the Concatenated LipSwish
as activation function, for which we show how to enforce the Lipschitz
condition and which boosts performance. The new architecture, i-DenseNet,
out-performs Residual Flow and other flow-based models on density estimation
evaluated in bits per dimension, where we utilize an equal parameter budget.
Moreover, we show that the proposed model out-performs Residual Flows when
trained as a hybrid model where the model is both a generative and a
discriminative model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Perugachi_Diaz_Y/0/1/0/all/0/1"&gt;Yura Perugachi-Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tomczak_J/0/1/0/all/0/1"&gt;Jakub M. Tomczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bhulai_S/0/1/0/all/0/1"&gt;Sandjai Bhulai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Data Assimilation with a Learned Inverse Observation Operator. (arXiv:2102.11192v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11192</id>
        <link href="http://arxiv.org/abs/2102.11192"/>
        <updated>2021-05-23T06:10:40.278Z</updated>
        <summary type="html"><![CDATA[Variational data assimilation optimizes for an initial state of a dynamical
system such that its evolution fits observational data. The physical model can
subsequently be evolved into the future to make predictions. This principle is
a cornerstone of large scale forecasting applications such as numerical weather
prediction. As such, it is implemented in current operational systems of
weather forecasting agencies across the globe. However, finding a good initial
state poses a difficult optimization problem in part due to the non-invertible
relationship between physical states and their corresponding observations. We
learn a mapping from observational data to physical states and show how it can
be used to improve optimizability. We employ this mapping in two ways: to
better initialize the non-convex optimization problem, and to reformulate the
objective function in better behaved physics space instead of observation
space. Our experimental results for the Lorenz96 model and a two-dimensional
turbulent fluid flow demonstrate that this procedure significantly improves
forecast quality for chaotic systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frerix_T/0/1/0/all/0/1"&gt;Thomas Frerix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kochkov_D/0/1/0/all/0/1"&gt;Dmitrii Kochkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1"&gt;Jamie A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1"&gt;Michael P. Brenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoyer_S/0/1/0/all/0/1"&gt;Stephan Hoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Rank and Sparse Enhanced Tucker Decomposition for Tensor Completion. (arXiv:2010.00359v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00359</id>
        <link href="http://arxiv.org/abs/2010.00359"/>
        <updated>2021-05-23T06:10:40.269Z</updated>
        <summary type="html"><![CDATA[Tensor completion refers to the task of estimating the missing data from an
incomplete measurement or observation, which is a core problem frequently
arising from the areas of big data analysis, computer vision, and network
engineering. Due to the multidimensional nature of high-order tensors, the
matrix approaches, e.g., matrix factorization and direct matricization of
tensors, are often not ideal for tensor completion and recovery. In this paper,
we introduce a unified low-rank and sparse enhanced Tucker decomposition model
for tensor completion. Our model possesses a sparse regularization term to
promote a sparse core tensor of the Tucker decomposition, which is beneficial
for tensor data compression. Moreover, we enforce low-rank regularization terms
on factor matrices of the Tucker decomposition for inducing the low-rankness of
the tensor with a cheap computational cost. Numerically, we propose a
customized ADMM with enough easy subproblems to solve the underlying model. It
is remarkable that our model is able to deal with different types of real-world
data sets, since it exploits the potential periodicity and inherent correlation
properties appeared in tensors. A series of computational experiments on
real-world data sets, including internet traffic data sets, color images, and
face recognition, demonstrate that our model performs better than many existing
state-of-the-art matricization and tensorization approaches in terms of
achieving higher recovery accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chenjian Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1"&gt;Chen Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hongjin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Liqun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Efficient Reinforcement Learning with Self-Predictive Representations. (arXiv:2007.05929v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05929</id>
        <link href="http://arxiv.org/abs/2007.05929"/>
        <updated>2021-05-23T06:10:40.251Z</updated>
        <summary type="html"><![CDATA[While deep reinforcement learning excels at solving tasks where large amounts
of data can be collected through virtually unlimited interaction with the
environment, learning from limited interaction remains a key challenge. We
posit that an agent can learn more efficiently if we augment reward
maximization with self-supervised objectives based on structure in its visual
input and sequential interaction with the environment. Our method,
Self-Predictive Representations(SPR), trains an agent to predict its own latent
state representations multiple steps into the future. We compute target
representations for future states using an encoder which is an exponential
moving average of the agent's parameters and we make predictions using a
learned transition model. On its own, this future prediction objective
outperforms prior methods for sample-efficient deep RL from pixels. We further
improve performance by adding data augmentation to the future prediction loss,
which forces the agent's representations to be consistent across multiple views
of an observation. Our full self-supervised objective, which combines future
prediction and data augmentation, achieves a median human-normalized score of
0.415 on Atari in a setting limited to 100k steps of environment interaction,
which represents a 55% relative improvement over the previous state-of-the-art.
Notably, even in this limited data regime, SPR exceeds expert human scores on 7
out of 26 games. The code associated with this work is available at
https://github.com/mila-iqia/spr]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1"&gt;Max Schwarzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Ankesh Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1"&gt;Rishab Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1"&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1"&gt;Philip Bachman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review on Modern Computational Optimal Transport Methods with Applications in Biomedical Research. (arXiv:2008.02995v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02995</id>
        <link href="http://arxiv.org/abs/2008.02995"/>
        <updated>2021-05-23T06:10:40.243Z</updated>
        <summary type="html"><![CDATA[Optimal transport has been one of the most exciting subjects in mathematics,
starting from the 18th century. As a powerful tool to transport between two
probability measures, optimal transport methods have been reinvigorated
nowadays in a remarkable proliferation of modern data science applications. To
meet the big data challenges, various computational tools have been developed
in the recent decade to accelerate the computation for optimal transport
methods. In this review, we present some cutting-edge computational optimal
transport methods with a focus on the regularization-based methods and the
projection-based methods. We discuss their real-world applications in
biomedical research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhong_W/0/1/0/all/0/1"&gt;Wenxuan Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ma_P/0/1/0/all/0/1"&gt;Ping Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based attacks in Cyber-Physical Systems: Exploration, Detection, and Control Cost trade-offs. (arXiv:2011.10718v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10718</id>
        <link href="http://arxiv.org/abs/2011.10718"/>
        <updated>2021-05-23T06:10:40.226Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning-based attacks in linear systems, where the
communication channel between the controller and the plant can be hijacked by a
malicious attacker. We assume the attacker learns the dynamics of the system
from observations, then overrides the controller's actuation signal, while
mimicking legitimate operation by providing fictitious sensor readings to the
controller. On the other hand, the controller is on a lookout to detect the
presence of the attacker and tries to enhance the detection performance by
carefully crafting its control signals. We study the trade-offs between the
information acquired by the attacker from observations, the detection
capabilities of the controller, and the control cost. Specifically, we provide
tight upper and lower bounds on the expected $\epsilon$-deception time, namely
the time required by the controller to make a decision regarding the presence
of an attacker with confidence at least $(1-\epsilon\log(1/\epsilon))$. We then
show a probabilistic lower bound on the time that must be spent by the attacker
learning the system, in order for the controller to have a given expected
$\epsilon$-deception time. We show that this bound is also order optimal, in
the sense that if the attacker satisfies it, then there exists a learning
algorithm with the given order expected deception time. Finally, we show a
lower bound on the expected energy expenditure required to guarantee detection
with confidence at least $1-\epsilon \log(1/\epsilon)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rangi_A/0/1/0/all/0/1"&gt;Anshuka Rangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khojasteh_M/0/1/0/all/0/1"&gt;Mohammad Javad Khojasteh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Franceschetti_M/0/1/0/all/0/1"&gt;Massimo Franceschetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus. (arXiv:2010.10391v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10391</id>
        <link href="http://arxiv.org/abs/2010.10391"/>
        <updated>2021-05-23T06:10:40.219Z</updated>
        <summary type="html"><![CDATA[Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have
achieved state-of-the-art results in biomedical natural language processing
tasks by focusing their pre-training process on domain-specific corpora.
However, such models do not take into consideration expert domain knowledge.

In this work, we introduced UmlsBERT, a contextual embedding model that
integrates domain knowledge during the pre-training process via a novel
knowledge augmentation strategy. More specifically, the augmentation on
UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was
performed in two ways: i) connecting words that have the same underlying
`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to
create clinically meaningful input embeddings. By applying these two
strategies, UmlsBERT can encode clinical domain knowledge into word embeddings
and outperform existing domain-specific models on common named-entity
recognition (NER) and clinical natural language inference clinical NLP tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1"&gt;George Michalopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuanxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaka_H/0/1/0/all/0/1"&gt;Hussam Kaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Helen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03814</id>
        <link href="http://arxiv.org/abs/2102.03814"/>
        <updated>2021-05-23T06:10:40.212Z</updated>
        <summary type="html"><![CDATA[Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)
allow control of several applications by decoding neurophysiological phenomena,
which are usually recorded by electroencephalography (EEG) using a non-invasive
technique. Despite great advances in MI-based BCI, EEG rhythms are specific to
a subject and various changes over time. These issues point to significant
challenges to enhance the classification performance, especially in a
subject-independent manner. To overcome these challenges, we propose MIN2Net, a
novel end-to-end multi-task learning to tackle this task. We integrate deep
metric learning into a multi-task autoencoder to learn a compact and
discriminative latent representation from EEG and perform classification
simultaneously. This approach reduces the complexity in pre-processing, results
in significant performance improvement on EEG classification. Experimental
results in a subject-independent manner show that MIN2Net outperforms the
state-of-the-art techniques, achieving an F1-score improvement of 6.72%, and
2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that
MIN2Net improves discriminative information in the latent representation. This
study indicates the possibility and practicality of using this model to develop
MI-based BCI applications for new users without the need for calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1"&gt;Phairot Autthasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1"&gt;Rattanaphon Chaisaen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1"&gt;Thapanun Sudhawiyangkul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1"&gt;Phurin Rangpong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1"&gt;Suktipol Kiatthaveephong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1"&gt;Nat Dilokthanakul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1"&gt;Gun Bhakdisongkhram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1"&gt;Huy Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1"&gt;Theerawit Wilaiprasitporn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility Mapping. (arXiv:2101.08413v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08413</id>
        <link href="http://arxiv.org/abs/2101.08413"/>
        <updated>2021-05-23T06:10:40.206Z</updated>
        <summary type="html"><![CDATA[Quantitative susceptibility mapping (QSM) has demonstrated great potential in
quantifying tissue susceptibility in various brain diseases. However, the
intrinsic ill-posed inverse problem relating the tissue phase to the underlying
susceptibility distribution affects the accuracy for quantifying tissue
susceptibility. Recently, deep learning has shown promising results to improve
accuracy by reducing the streaking artifacts. However, there exists a mismatch
between the observed phase and the theoretical forward phase estimated by the
susceptibility label. In this study, we proposed a model-based deep learning
architecture that followed the STI (susceptibility tensor imaging) physical
model, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the
relationship between STI-derived phase contrast induced by the susceptibility
tensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The
convolution neural networks are embedded into the physical model to learn a
regularization term containing prior information. ki33 and phase induced by
ki13 and ki23 terms were used as the labels for network training. Quantitative
evaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed
deep learning QSM methods. The results showed that MoDL-QSM achieved superior
performance, demonstrating its potential for future applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ruimin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jiayi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Baofeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jie Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuting Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chunlei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuyao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jie Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Hongjiang Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to send a real number using a single bit (and some shared randomness). (arXiv:2010.02331v4 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02331</id>
        <link href="http://arxiv.org/abs/2010.02331"/>
        <updated>2021-05-23T06:10:40.199Z</updated>
        <summary type="html"><![CDATA[We consider the fundamental problem of communicating an estimate of a real
number $x\in[0,1]$ using a single bit. A sender that knows $x$ chooses a value
$X\in\set{0,1}$ to transmit. In turn, a receiver estimates $x$ based on the
value of $X$. We consider both the biased and unbiased estimation problems and
aim to minimize the cost. For the biased case, the cost is the worst-case (over
the choice of $x$) expected squared error, which coincides with the variance if
the algorithm is required to be unbiased.

We first overview common biased and unbiased estimation approaches and prove
their optimality when no shared randomness is allowed. We then show how a small
amount of shared randomness, which can be as low as a single bit, reduces the
cost in both cases. Specifically, we derive lower bounds on the cost attainable
by any algorithm with unrestricted use of shared randomness and propose
near-optimal solutions that use a small number of shared random bits. Finally,
we discuss open problems and future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Basat_R/0/1/0/all/0/1"&gt;Ran Ben-Basat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitzenmacher_M/0/1/0/all/0/1"&gt;Michael Mitzenmacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1"&gt;Shay Vargaftik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Neural Networks Jamming on the Beat. (arXiv:2007.06284v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06284</id>
        <link href="http://arxiv.org/abs/2007.06284"/>
        <updated>2021-05-23T06:10:40.190Z</updated>
        <summary type="html"><![CDATA[This paper addresses the issue of long-scale correlations that is
characteristic for symbolic music and is a challenge for modern generative
algorithms. It suggests a very simple workaround for this challenge, namely,
generation of a drum pattern that could be further used as a foundation for
melody generation. The paper presents a large dataset of drum patterns
alongside with corresponding melodies. It explores two possible methods for
drum pattern generation. Exploring a latent space of drum patterns one could
generate new drum patterns with a given music style. Finally, the paper
demonstrates that a simple artificial neural network could be trained to
generate melodies corresponding with these drum patters used as inputs.
Resulting system could be used for end-to-end generation of symbolic music with
song-like structure and higher long-scale correlations between the notes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yamshchikov_I/0/1/0/all/0/1"&gt;Ivan P. Yamshchikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphReach: Position-Aware Graph Neural Network using Reachability Estimations. (arXiv:2008.09657v3 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09657</id>
        <link href="http://arxiv.org/abs/2008.09657"/>
        <updated>2021-05-23T06:10:40.181Z</updated>
        <summary type="html"><![CDATA[Majority of the existing graph neural networks (GNN) learn node embeddings
that encode their local neighborhoods but not their positions. Consequently,
two nodes that are vastly distant but located in similar local neighborhoods
map to similar embeddings in those networks. This limitation prevents accurate
performance in predictive tasks that rely on position information. In this
paper,we develop GraphReach, a position-aware inductive GNN that captures the
global positions of nodes through reachability estimations with respect to a
set of anchor nodes. The anchors are strategically selected so that
reachability estimations across all the nodes are maximized. We show that this
combinatorial anchor selection problem is NP-hard and, consequently, develop a
greedy (1-1/e) approximation heuristic. Empirical evaluation against
state-of-the-art GNN architectures reveal that GraphReach provides up to 40%
relative improvement in accuracy. In addition, it is more robust to adversarial
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nishad_S/0/1/0/all/0/1"&gt;Sunil Nishad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Shubhangi Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Arnab Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranu_S/0/1/0/all/0/1"&gt;Sayan Ranu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling the Field Value Variations and Field Interactions Simultaneously for Fraud Detection. (arXiv:2008.05600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05600</id>
        <link href="http://arxiv.org/abs/2008.05600"/>
        <updated>2021-05-23T06:10:40.174Z</updated>
        <summary type="html"><![CDATA[With the explosive growth of e-commerce, online transaction fraud has become
one of the biggest challenges for e-commerce platforms. The historical
behaviors of users provide rich information for digging into the users' fraud
risk. While considerable efforts have been made in this direction, a
long-standing challenge is how to effectively exploit internal user information
and provide explainable prediction results. In fact, the value variations of
same field from different events and the interactions of different fields
inside one event have proven to be strong indicators for fraudulent behaviors.
In this paper, we propose the Dual Importance-aware Factorization Machines
(DIFM), which exploits the internal field information among users' behavior
sequence from dual perspectives, i.e., field value variations and field
interactions simultaneously for fraud detection. The proposed model is deployed
in the risk management system of one of the world's largest e-commerce
platforms, which utilize it to provide real-time transaction fraud detection.
Experimental results on real industrial data from different regions in the
platform clearly demonstrate that our model achieves significant improvements
compared with various state-of-the-art baseline models. Moreover, the DIFM
could also give an insight into the explanation of the prediction results from
dual perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1"&gt;Dongbo Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1"&gt;Bowen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1"&gt;Fuzhen Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yongchun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1"&gt;Qing He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Head Attention: Collaborate Instead of Concatenate. (arXiv:2006.16362v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16362</id>
        <link href="http://arxiv.org/abs/2006.16362"/>
        <updated>2021-05-23T06:10:40.021Z</updated>
        <summary type="html"><![CDATA[Attention layers are widely used in natural language processing (NLP) and are
beginning to influence computer vision architectures. Training very large
transformer models allowed significant improvement in both fields, but once
trained, these networks show symptoms of over-parameterization. For instance,
it is known that many attention heads can be pruned without impacting accuracy.
This work aims to enhance current understanding on how multiple heads interact.
Motivated by the observation that attention heads learn redundant key/query
projections, we propose a collaborative multi-head attention layer that enables
heads to learn shared projections. Our scheme decreases the number of
parameters in an attention layer and can be used as a drop-in replacement in
any transformer architecture. Our experiments confirm that sharing key/query
dimensions can be exploited in language understanding, machine translation and
vision. We also show that it is possible to re-parametrize a pre-trained
multi-head attention layer into our collaborative attention layer.
Collaborative multi-head attention reduces the size of the key and query
projections by 4 for same accuracy and speed. Our code is public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cordonnier_J/0/1/0/all/0/1"&gt;Jean-Baptiste Cordonnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1"&gt;Andreas Loukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation: A Survey. (arXiv:2006.05525v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05525</id>
        <link href="http://arxiv.org/abs/2006.05525"/>
        <updated>2021-05-23T06:10:39.966Z</updated>
        <summary type="html"><![CDATA[In recent years, deep neural networks have been successful in both industry
and academia, especially for computer vision tasks. The great success of deep
learning is mainly due to its scalability to encode large-scale data and to
maneuver billions of model parameters. However, it is a challenge to deploy
these cumbersome deep models on devices with limited resources, e.g., mobile
phones and embedded devices, not only because of the high computational
complexity but also the large storage requirements. To this end, a variety of
model compression and acceleration techniques have been developed. As a
representative type of model compression and acceleration, knowledge
distillation effectively learns a small student model from a large teacher
model. It has received rapid increasing attention from the community. This
paper provides a comprehensive survey of knowledge distillation from the
perspectives of knowledge categories, training schemes, teacher-student
architecture, distillation algorithms, performance comparison and applications.
Furthermore, challenges in knowledge distillation are briefly reviewed and
comments on future research are discussed and forwarded.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gou_J/0/1/0/all/0/1"&gt;Jianping Gou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Baosheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1"&gt;Stephen John Maybank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate gradients for analog neuromorphic computing. (arXiv:2006.07239v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07239</id>
        <link href="http://arxiv.org/abs/2006.07239"/>
        <updated>2021-05-23T06:10:39.959Z</updated>
        <summary type="html"><![CDATA[To rapidly process temporal information at a low metabolic cost, biological
neurons integrate inputs as an analog sum but communicate with spikes, binary
events in time. Analog neuromorphic hardware uses the same principles to
emulate spiking neural networks with exceptional energy-efficiency. However,
instantiating high-performing spiking networks on such hardware remains a
significant challenge due to device mismatch and the lack of efficient training
algorithms. Here, we introduce a general in-the-loop learning framework based
on surrogate gradients that resolves these issues. Using the BrainScaleS-2
neuromorphic system, we show that learning self-corrects for device mismatch
resulting in competitive spiking network performance on both vision and speech
benchmarks. Our networks display sparse spiking activity with, on average, far
less than one spike per hidden neuron and input, perform inference at rates of
up to 85 k frames/second, and consume less than 200 mW. In summary, our work
sets several new benchmarks for low-energy spiking network processing on analog
neuromorphic hardware and paves the way for future on-chip learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cramer_B/0/1/0/all/0/1"&gt;Benjamin Cramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Billaudelle_S/0/1/0/all/0/1"&gt;Sebastian Billaudelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanya_S/0/1/0/all/0/1"&gt;Simeon Kanya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leibfried_A/0/1/0/all/0/1"&gt;Aron Leibfried&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grubl_A/0/1/0/all/0/1"&gt;Andreas Gr&amp;#xfc;bl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karasenko_V/0/1/0/all/0/1"&gt;Vitali Karasenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pehle_C/0/1/0/all/0/1"&gt;Christian Pehle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schreiber_K/0/1/0/all/0/1"&gt;Korbinian Schreiber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stradmann_Y/0/1/0/all/0/1"&gt;Yannik Stradmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weis_J/0/1/0/all/0/1"&gt;Johannes Weis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schemmel_J/0/1/0/all/0/1"&gt;Johannes Schemmel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zenke_F/0/1/0/all/0/1"&gt;Friedemann Zenke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural networks with superexpressive activations and integer weights. (arXiv:2105.09917v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09917</id>
        <link href="http://arxiv.org/abs/2105.09917"/>
        <updated>2021-05-23T06:10:39.952Z</updated>
        <summary type="html"><![CDATA[An example of an activation function $\sigma$ is given such that networks
with activations $\{\sigma, \lfloor\cdot\rfloor\}$, integer weights and a fixed
architecture depending on $d$ approximate continuous functions on $[0,1]^d$.
The range of integer weights required for $\varepsilon$-approximation of
H\"older continuous functions is derived, which leads to a convergence rate of
order $n^{\frac{-2\beta}{2\beta+d}}\log_2n$ for neural network regression
estimation of unknown $\beta$-H\"older continuous function with given $n$
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1"&gt;Aleksandr Beknazaryan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning high-dimensional probability distributions using tree tensor networks. (arXiv:1912.07913v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.07913</id>
        <link href="http://arxiv.org/abs/1912.07913"/>
        <updated>2021-05-23T06:10:39.944Z</updated>
        <summary type="html"><![CDATA[We consider the problem of the estimation of a high-dimensional probability
distribution from i.i.d. samples of the distribution using model classes of
functions in tree-based tensor formats, a particular case of tensor networks
associated with a dimension partition tree. The distribution is assumed to
admit a density with respect to a product measure, possibly discrete for
handling the case of discrete random variables.

After discussing the representation of classical model classes in tree-based
tensor formats, we present learning algorithms based on empirical risk
minimization using a $L^2$ contrast.

These algorithms exploit the multilinear parametrization of the formats to
recast the nonlinear minimization problem into a sequence of empirical risk
minimization problems with linear models. A suitable parametrization of the
tensor in tree-based tensor format allows to obtain a linear model with
orthogonal bases, so that each problem admits an explicit expression of the
solution and cross-validation risk estimates. These estimations of the risk
enable the model selection, for instance when exploiting sparsity in the
coefficients of the representation.

A strategy for the adaptation of the tensor format (dimension tree and
tree-based ranks) is provided, which allows to discover and exploit some
specific structures of high-dimensional probability distributions such as
independence or conditional independence.

We illustrate the performances of the proposed algorithms for the
approximation of classical probabilistic models (such as Gaussian distribution,
graphical models, Markov chain).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Grelier_E/0/1/0/all/0/1"&gt;Erwan Grelier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nouy_A/0/1/0/all/0/1"&gt;Anthony Nouy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lebrun_R/0/1/0/all/0/1"&gt;R&amp;#xe9;gis Lebrun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Convolutional Networks for Automatically Generating Image Masks to Train Mask R-CNN. (arXiv:2003.01383v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01383</id>
        <link href="http://arxiv.org/abs/2003.01383"/>
        <updated>2021-05-23T06:10:39.936Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel automatically generating image masks method for
the state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method
achieves the best results in object detection until now, however, it is very
time-consuming and laborious to get the object Masks for training, the proposed
method is composed by a two-stage design, to automatically generating image
masks, the first stage implements a fully convolutional networks (FCN) based
segmentation network, the second stage network, a Mask R-CNN based object
detection network, which is trained on the object image masks from FCN output,
the original input image, and additional label information. Through
experimentation, our proposed method can obtain the image masks automatically
to train Mask R-CNN, and it can achieve very high classification accuracy with
an over 90% mean of average precision (mAP) for segmentation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1"&gt;Jan Paul Siebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiangrong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Pruning at Initialization. (arXiv:2002.08797v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08797</id>
        <link href="http://arxiv.org/abs/2002.08797"/>
        <updated>2021-05-23T06:10:39.916Z</updated>
        <summary type="html"><![CDATA[Overparameterized Neural Networks (NN) display state-of-the-art performance.
However, there is a growing need for smaller, energy-efficient, neural networks
tobe able to use machine learning applications on devices with limited
computational resources. A popular approach consists of using pruning
techniques. While these techniques have traditionally focused on pruning
pre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et
al. (2018) has shown promising results when pruning at initialization. However,
for Deep NNs, such procedures remain unsatisfactory as the resulting pruned
networks can be difficult to train and, for instance, they do not prevent one
layer from being fully pruned. In this paper, we provide a comprehensive
theoretical analysis of Magnitude and Gradient based pruning at initialization
and training of sparse architectures. This allows us to propose novel
principled approaches which we validate experimentally on a variety of NN
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1"&gt;Soufiane Hayou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ton_J/0/1/0/all/0/1"&gt;Jean-Francois Ton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer Users Have Unique Yet Temporally Inconsistent Computer Usage Profiles. (arXiv:2105.09900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09900</id>
        <link href="http://arxiv.org/abs/2105.09900"/>
        <updated>2021-05-23T06:10:39.909Z</updated>
        <summary type="html"><![CDATA[This paper investigates whether computer usage profiles comprised of
process-, network-, mouse- and keystroke-related events are unique and
temporally consistent in a naturalistic setting, discussing challenges and
opportunities of using such profiles in applications of continuous
authentication. We collected ecologically-valid computer usage profiles from 28
MS Windows 10 computer users over 8 weeks and submitted this data to
comprehensive machine learning analysis involving a diverse set of online and
offline classifiers. We found that (i) computer usage profiles have the
potential to uniquely characterize computer users (with a maximum F-score of
99.94%); (ii) network-related events were the most useful features to properly
recognize profiles (95.14% of the top features distinguishing users being
network-related); (iii) user profiles were mostly inconsistent over the 8-week
data collection period, with 92.86% of users exhibiting drifts in terms of time
and usage habits; and (iv) online models are better suited to handle computer
usage profiles compared to offline models (maximum F-score for each approach
was 95.99% and 99.94%, respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1"&gt;Luiz Giovanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceschin_F/0/1/0/all/0/1"&gt;Fabr&amp;#xed;cio Ceschin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1"&gt;Mirela Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Aokun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1"&gt;Ramchandra Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banda_S/0/1/0/all/0/1"&gt;Sanjay Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lysaght_M/0/1/0/all/0/1"&gt;Madison Lysaght&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_H/0/1/0/all/0/1"&gt;Heng Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapountzis_N/0/1/0/all/0/1"&gt;Nikolaos Sapountzis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Ruimin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matthews_B/0/1/0/all/0/1"&gt;Brandon Matthews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dapeng Oliver Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregio_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Gr&amp;#xe9;gio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1"&gt;Daniela Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Graph-Based Behavior-Aware Recommendation for Interactive News. (arXiv:1812.00002v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.00002</id>
        <link href="http://arxiv.org/abs/1812.00002"/>
        <updated>2021-05-23T06:10:39.898Z</updated>
        <summary type="html"><![CDATA[Interactive news recommendation has been launched and attracted much
attention recently. In this scenario, user's behavior evolves from single click
behavior to multiple behaviors including like, comment, share etc. However,
most of the existing methods still use single click behavior as the unique
criterion of judging user's preferences. Further, although heterogeneous graphs
have been applied in different areas, a proper way to construct a heterogeneous
graph for interactive news data with an appropriate learning mechanism on it is
still desired. To address the above concerns, we propose a graph-based
behavior-aware network, which simultaneously considers six different types of
behaviors as well as user's demand on the news diversity. We have three main
steps. First, we build an interaction behavior graph for multi-level and
multi-category data. Second, we apply DeepWalk on the behavior graph to obtain
entity semantics, then build a graph-based convolutional neural network called
G-CNN to learn news representations, and an attention-based LSTM to learn
behavior sequence representations. Third, we introduce core and coritivity
features for the behavior graph, which measure the concentration degree of
user's interests. These features affect the trade-off between accuracy and
diversity of our personalized recommendation system. Taking these features into
account, our system finally achieves recommending news to different users at
their different levels of concentration degrees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Mingyuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1"&gt;Sen Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Congzhou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09908</id>
        <link href="http://arxiv.org/abs/2105.09908"/>
        <updated>2021-05-23T06:10:39.891Z</updated>
        <summary type="html"><![CDATA[There is a prevailing trend to study urban morphology quantitatively thanks
to the growing accessibility to various forms of spatial big data, increasing
computing power, and use cases benefiting from such information. The methods
developed up to now measure urban morphology with numerical indices describing
density, proportion, and mixture, but they do not directly represent
morphological features from human's visual and intuitive perspective. We take
the first step to bridge the gap by proposing a deep learning-based technique
to automatically classify road networks into four classes on a visual basis.
The method is implemented by generating an image of the street network (Colored
Road Hierarchy Diagram), which we introduce in this paper, and classifying it
using a deep convolutional neural network (ResNet-34). The model achieves an
overall classification accuracy of 0.875. Nine cities around the world are
selected as the study areas and their road networks are acquired from
OpenStreetMap. Latent subgroups among the cities are uncovered through a
clustering on the percentage of each road network category. In the subsequent
part of the paper, we focus on the usability of such classification: the
effectiveness of our human perception augmentation is examined by a case study
of urban vitality prediction. An advanced tree-based regression model is for
the first time designated to establish the relationship between morphological
indices and vitality indicators. A positive effect of human perception
augmentation is detected in the comparative experiment of baseline model and
augmented model. This work expands the toolkit of quantitative urban morphology
study with new techniques, supporting further studies in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wangyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Abraham Noah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1"&gt;Filip Biljecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Perspective Anomaly Detection. (arXiv:2105.09903v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09903</id>
        <link href="http://arxiv.org/abs/2105.09903"/>
        <updated>2021-05-23T06:10:39.885Z</updated>
        <summary type="html"><![CDATA[Multi-view classification is inspired by the behavior of humans, especially
when fine-grained features or in our case rarely occurring anomalies are to be
detected. Current contributions point to the problem of how high-dimensional
data can be fused. In this work, we build upon the deep support vector data
description algorithm and address multi-perspective anomaly detection using
three different fusion techniques i.e. early fusion, late fusion, and late
fusion with multiple decoders. We employ different augmentation techniques with
a denoising process to deal with scarce one-class data, which further improves
the performance (ROC AUC = 80\%). Furthermore, we introduce the dices dataset
that consists of over 2000 grayscale images of falling dices from multiple
perspectives, with 5\% of the images containing rare anomalies (e.g. drill
holes, sawing, or scratches). We evaluate our approach on the new dices dataset
using images from two different perspectives and also benchmark on the standard
MNIST dataset. Extensive experiments demonstrate that our proposed approach
exceeds the state-of-the-art on both the MNIST and dices datasets. To the best
of our knowledge, this is the first work that focuses on addressing
multi-perspective anomaly detection in images by jointly using different
perspectives together with one single objective function for anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1"&gt;Manav Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1"&gt;Peter Jakob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1"&gt;Tobias Schmid-Schirling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Complexity of Standard Benchmarking Datasets for Long-Term Human Trajectory Prediction. (arXiv:2005.13934v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.13934</id>
        <link href="http://arxiv.org/abs/2005.13934"/>
        <updated>2021-05-23T06:10:39.867Z</updated>
        <summary type="html"><![CDATA[Methods to quantify the complexity of trajectory datasets are still a missing
piece in benchmarking human trajectory prediction models. In order to gain a
better understanding of the complexity of trajectory prediction tasks and
following the intuition, that more complex datasets contain more information,
an approach for quantifying the amount of information contained in a dataset
from a prototype-based dataset representation is proposed. The dataset
representation is obtained by first employing a non-trivial spatial sequence
alignment, which enables a subsequent learning vector quantization (LVQ) stage.
A large-scale complexity analysis is conducted on several human trajectory
prediction benchmarking datasets, followed by a brief discussion on indications
for human trajectory prediction and benchmarking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1"&gt;Ronny Hug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1"&gt;Stefan Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1"&gt;Wolfgang H&amp;#xfc;bner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1"&gt;Michael Arens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Compositional Learning of Structured Visual Concepts. (arXiv:2105.09848v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09848</id>
        <link href="http://arxiv.org/abs/2105.09848"/>
        <updated>2021-05-23T06:10:39.860Z</updated>
        <summary type="html"><![CDATA[Humans are highly efficient learners, with the ability to grasp the meaning
of a new concept from just a few examples. Unlike popular computer vision
systems, humans can flexibly leverage the compositional structure of the visual
world, understanding new concepts as combinations of existing concepts. In the
current paper, we study how people learn different types of visual
compositions, using abstract visual forms with rich relational structure. We
find that people can make meaningful compositional generalizations from just a
few examples in a variety of scenarios, and we develop a Bayesian program
induction model that provides a close fit to the behavioral data. Unlike past
work examining special cases of compositionality, our work shows how a single
computational approach can account for many distinct types of compositional
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yanli Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1"&gt;Brenden M. Lake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidirectional LSTM-CRF Attention-based Model for Chinese Word Segmentation. (arXiv:2105.09681v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09681</id>
        <link href="http://arxiv.org/abs/2105.09681"/>
        <updated>2021-05-23T06:10:39.853Z</updated>
        <summary type="html"><![CDATA[Chinese word segmentation (CWS) is the basic of Chinese natural language
processing (NLP). The quality of word segmentation will directly affect the
rest of NLP tasks. Recently, with the artificial intelligence tide rising
again, Long Short-Term Memory (LSTM) neural network, as one of easily modeling
in sequence, has been widely utilized in various kinds of NLP tasks, and
functions well. Attention mechanism is an ingenious method to solve the memory
compression problem on LSTM. Furthermore, inspired by the powerful abilities of
bidirectional LSTM models for modeling sequence and CRF model for decoding, we
propose a Bidirectional LSTM-CRF Attention-based Model in this paper.
Experiments on PKU and MSRA benchmark datasets show that our model performs
better than the baseline methods modeling by other neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhuangwei Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weihua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yanbu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09858</id>
        <link href="http://arxiv.org/abs/2105.09858"/>
        <updated>2021-05-23T06:10:39.847Z</updated>
        <summary type="html"><![CDATA[This paper presents a low-latency real-time (LLRT) non-parallel voice
conversion (VC) framework based on cyclic variational autoencoder (CycleVAE)
and multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a
robust non-parallel multispeaker spectral model, which utilizes a
speaker-independent latent space and a speaker-dependent code to generate
reconstructed/converted spectral features given the spectral features of an
input speaker. On the other hand, MWDLP is an efficient and a high-quality
neural vocoder that can handle multispeaker data and generate speech waveform
for LLRT applications with CPU. To accommodate LLRT constraint with CPU, we
propose a novel CycleVAE framework that utilizes mel-spectrogram as spectral
features and is built with a sparse network architecture. Further, to improve
the modeling performance, we also propose a novel fine-tuning procedure that
refines the frame-rate CycleVAE network by utilizing the waveform loss from the
MWDLP network. The experimental results demonstrate that the proposed framework
achieves high-performance VC, while allowing for LLRT usage with a single-core
of $2.1$--$2.7$~GHz CPU on a real-time factor of $0.87$--$0.95$, including
input/output, feature extraction, on a frame shift of $10$ ms, a window length
of $27.5$ ms, and $2$ lookup frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1"&gt;Patrick Lumban Tobing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Personalized Fairness based on Causal Notion. (arXiv:2105.09829v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09829</id>
        <link href="http://arxiv.org/abs/2105.09829"/>
        <updated>2021-05-23T06:10:39.840Z</updated>
        <summary type="html"><![CDATA[Recommender systems are gaining increasing and critical impacts on human and
society since a growing number of users use them for information seeking and
decision making. Therefore, it is crucial to address the potential unfairness
problems in recommendations. Just like users have personalized preferences on
items, users' demands for fairness are also personalized in many scenarios.
Therefore, it is important to provide personalized fair recommendations for
users to satisfy their personalized fairness demands. Besides, previous works
on fair recommendation mainly focus on association-based fairness. However, it
is important to advance from associative fairness notions to causal fairness
notions for assessing fairness more properly in recommender systems. Based on
the above considerations, this paper focuses on achieving personalized
counterfactual fairness for users in recommender systems. To this end, we
introduce a framework for achieving counterfactually fair recommendations
through adversary learning by generating feature-independent user embeddings
for recommendation. The framework allows recommender systems to achieve
personalized fairness for users while also covering non-personalized
situations. Experiments on two real-world datasets with shallow and deep
recommendation algorithms show that our method can generate fairer
recommendations for users with a desirable recommendation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanxiong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning in physics: a study of dielectric quasi-cubic particles in a uniform electric field. (arXiv:2105.09866v1 [physics.class-ph])]]></title>
        <id>http://arxiv.org/abs/2105.09866</id>
        <link href="http://arxiv.org/abs/2105.09866"/>
        <updated>2021-05-23T06:10:39.834Z</updated>
        <summary type="html"><![CDATA[Solving physics problems for which we know the equations, boundary conditions
and symmetries can be done by deep learning. The constraints can be either
imposed as terms in a loss function or used to formulate a neural ansatz. In
the present case study, we calculate the induced field inside and outside a
dielectric cube placed in a uniform electric field, wherein the dielectric
mismatch at edges and corners of the cube makes accurate calculations
numerically challenging. The electric potential is expressed as an ansatz
incorporating neural networks with known leading order behaviors and symmetries
and the Laplace's equation is then solved with boundary conditions at the
dielectric interface by minimizing a loss function. The loss function ensures
that both Laplace's equation and boundary conditions are satisfied everywhere
inside a large solution domain. We study how the electric potential inside and
outside a quasi-cubic particle evolves through a sequence of shapes from a
sphere to a cube. The neural network being differentiable, it is
straightforward to calculate the electric field over the whole domain, the
induced surface charge distribution and the polarizability. The neural network
being retentive, one can efficiently follow how the field changes upon
particle's shape or dielectric constant by iterating from any previously
converged solution. The present work's objective is two-fold, first to show how
an a priori knowledge can be incorporated into neural networks to achieve
efficient learning and second to apply the method and study how the induced
field and polarizability change when a dielectric particle progressively
changes its shape from a sphere to a cube.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Guet_C/0/1/0/all/0/1"&gt;Claude Guet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09856</id>
        <link href="http://arxiv.org/abs/2105.09856"/>
        <updated>2021-05-23T06:10:39.814Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel high-fidelity and low-latency universal neural
vocoder framework based on multiband WaveRNN with data-driven linear prediction
for discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN
architecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit
with a relatively large size of hidden units is utilized, while the multiband
modeling is deployed to achieve real-time low-latency usage. A novel technique
for data-driven linear prediction (LP) with discrete waveform modeling is
proposed, where the LP coefficients are estimated in a data-driven manner.
Moreover, a novel loss function using short-time Fourier transform (STFT) for
discrete waveform modeling with Gumbel approximation is also proposed. The
experimental results demonstrate that the proposed MWDLP framework generates
high-fidelity synthetic speech for seen and unseen speakers and/or language on
300 speakers training data including clean and noisy/reverberant conditions,
where the number of training utterances is limited to 60 per speaker, while
allowing for real-time low-latency processing using a single core of $\sim\!$
2.1--2.7~GHz CPU with $\sim\!$ 0.57--0.64 real-time factor including
input/output and feature extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1"&gt;Patrick Lumban Tobing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEHB: Evolutionary Hyberband for Scalable, Robust and Efficient Hyperparameter Optimization. (arXiv:2105.09821v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09821</id>
        <link href="http://arxiv.org/abs/2105.09821"/>
        <updated>2021-05-23T06:10:39.793Z</updated>
        <summary type="html"><![CDATA[Modern machine learning algorithms crucially rely on several design decisions
to achieve strong performance, making the problem of Hyperparameter
Optimization (HPO) more important than ever. Here, we combine the advantages of
the popular bandit-based HPO method Hyperband (HB) and the evolutionary search
approach of Differential Evolution (DE) to yield a new HPO method which we call
DEHB. Comprehensive results on a very broad range of HPO problems, as well as a
wide range of tabular benchmarks from neural architecture search, demonstrate
that DEHB achieves strong performance far more robustly than all previous HPO
methods we are aware of, especially for high-dimensional problems with discrete
input dimensions. For example, DEHB is up to 1000x faster than random search.
It is also efficient in computational time, conceptually simple and easy to
implement, positioning it well to become a new default HPO method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awad_N/0/1/0/all/0/1"&gt;Noor Awad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1"&gt;Neeratyoy Mallik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monte Carlo Filtering Objectives: A New Family of Variational Objectives to Learn Generative Model and Neural Adaptive Proposal for Time Series. (arXiv:2105.09801v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09801</id>
        <link href="http://arxiv.org/abs/2105.09801"/>
        <updated>2021-05-23T06:10:39.786Z</updated>
        <summary type="html"><![CDATA[Learning generative models and inferring latent trajectories have shown to be
challenging for time series due to the intractable marginal likelihoods of
flexible generative models. It can be addressed by surrogate objectives for
optimization. We propose Monte Carlo filtering objectives (MCFOs), a family of
variational objectives for jointly learning parametric generative models and
amortized adaptive importance proposals of time series. MCFOs extend the
choices of likelihood estimators beyond Sequential Monte Carlo in
state-of-the-art objectives, possess important properties revealing the factors
for the tightness of objectives, and allow for less biased and variant gradient
estimates. We demonstrate that the proposed MCFOs and gradient estimations lead
to efficient and stable model learning, and learned generative models well
explain data and importance proposals are more sample efficient on various
kinds of time series data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuangshuang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Sihao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karayiannidis_Y/0/1/0/all/0/1"&gt;Yiannis Karayiannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bjorkman_M/0/1/0/all/0/1"&gt;M&amp;#xe5;rten Bj&amp;#xf6;rkman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry. (arXiv:2105.09899v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09899</id>
        <link href="http://arxiv.org/abs/2105.09899"/>
        <updated>2021-05-23T06:10:39.777Z</updated>
        <summary type="html"><![CDATA[The technology for Visual Odometry (VO) that estimates the position and
orientation of the moving object through analyzing the image sequences captured
by on-board cameras, has been well investigated with the rising interest in
autonomous driving. This paper studies monocular VO from the perspective of
Deep Learning (DL). Unlike most current learning-based methods, our approach,
called DeepAVO, is established on the intuition that features contribute
discriminately to different motion patterns. Specifically, we present a novel
four-branch network to learn the rotation and translation by leveraging
Convolutional Neural Networks (CNNs) to focus on different quadrants of optical
flow input. To enhance the ability of feature selection, we further introduce
an effective channel-spatial attention mechanism to force each branch to
explicitly distill related information for specific Frame to Frame (F2F) motion
estimation. Experiments on various datasets involving outdoor driving and
indoor walking scenarios show that the proposed DeepAVO outperforms the
state-of-the-art monocular methods by a large margin, demonstrating competitive
performance to the stereo VO algorithm and verifying promising potential for
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ran Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mingkun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Rujun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bo Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zhuoling Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-side Sparse Tensor Core. (arXiv:2105.09564v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2105.09564</id>
        <link href="http://arxiv.org/abs/2105.09564"/>
        <updated>2021-05-23T06:10:39.770Z</updated>
        <summary type="html"><![CDATA[Leveraging sparsity in deep neural network (DNN) models is promising for
accelerating model inference. Yet existing GPUs can only leverage the sparsity
from weights but not activations, which are dynamic, unpredictable, and hence
challenging to exploit. In this work, we propose a novel architecture to
efficiently harness the dual-side sparsity (i.e., weight and activation
sparsity). We take a systematic approach to understand the (dis)advantages of
previous sparsity-related architectures and propose a novel, unexplored
paradigm that combines outer-product computation primitive and bitmap-based
encoding format. We demonstrate the feasibility of our design with minimal
changes to the existing production-scale inner-product-based Tensor Core. We
propose a set of novel ISA extensions and co-design the matrix-matrix
multiplication and convolution algorithms, which are the two dominant
computation patterns in today's DNN models, to exploit our new dual-side sparse
Tensor Core. Our evaluation shows that our design can fully unleash the
dual-side DNN sparsity and improve the performance by up to one order of
magnitude with \hl{small} hardware overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhiqiang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Cong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yunxin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1"&gt;Jingwen Leng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks. (arXiv:2105.09720v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09720</id>
        <link href="http://arxiv.org/abs/2105.09720"/>
        <updated>2021-05-23T06:10:39.750Z</updated>
        <summary type="html"><![CDATA[The novel corona virus (Covid-19) has introduced significant challenges due
to its rapid spreading nature through respiratory transmission. As a result,
there is a huge demand for Artificial Intelligence (AI) based quick disease
diagnosis methods as an alternative to high demand tests such as Polymerase
Chain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective
radiography technique due to resource availability and quick screening. But, a
sufficient and systematic data collection that is required by complex deep
leaning (DL) models is more difficult and hence there are recent efforts that
utilize transfer learning to address this issue. Still these transfer learnt
models suffer from lack of generalization and increased bias to the training
dataset resulting poor performance for unseen data. Limited correlation of the
transferred features from the pre-trained model to a specific medical imaging
domain like X-ray and overfitting on fewer data can be reasons for this
circumstance. In this work, we propose a novel Graph Convolution Neural Network
(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR
images and meta information about patients. The proposed method exploits
important relational knowledge between data instances and their features using
graph representation and applies convolution to learn the graph data which is
not possible with conventional convolution on Euclidean domain. The results of
extensive experiments of proposed model on binary (Covid vs normal) and three
class (Covid, normal, other pneumonia) classification problems outperform
different benchmark transfer learnt models, hence overcoming the aforementioned
drawbacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mudiyanselage_T/0/1/0/all/0/1"&gt;Thosini Bamunu Mudiyanselage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Senanayake_N/0/1/0/all/0/1"&gt;Nipuna Senanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1"&gt;Chunyan Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanqing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised, Topology-Aware Segmentation of Tubular Structures from Live Imaging 3D Microscopy. (arXiv:2105.09737v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09737</id>
        <link href="http://arxiv.org/abs/2105.09737"/>
        <updated>2021-05-23T06:10:39.743Z</updated>
        <summary type="html"><![CDATA[Motivated by a challenging tubular network segmentation task, this paper
tackles two commonly encountered problems in biomedical imaging: Topological
consistency of the segmentation, and limited annotations. We propose a
topological score which measures both topological and geometric consistency
between the predicted and ground truth segmentations, applied for model
selection and validation. We apply our topological score in three scenarios: i.
a U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised
U-net architecture, which offers a straightforward approach to jointly training
the network both as an autoencoder and a segmentation algorithm. This allows us
to utilize un-annotated data for training a representation that generalizes
across test data variability, in spite of our annotated training data having
very limited variation. Our contributions are validated on a challenging
segmentation task, locating tubular structures in the fetal pancreas from noisy
live imaging confocal microscopy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arnavaz_K/0/1/0/all/0/1"&gt;Kasra Arnavaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1"&gt;Oswin Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krivokapic_J/0/1/0/all/0/1"&gt;Jelena M. Krivokapic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heilmann_S/0/1/0/all/0/1"&gt;Silja Heilmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baerentzen_J/0/1/0/all/0/1"&gt;Jakob Andreas B&amp;#xe6;rentzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nyeng_P/0/1/0/all/0/1"&gt;Pia Nyeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1"&gt;Aasa Feragen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on Gradient-Free ADMM framework. (arXiv:2105.09837v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09837</id>
        <link href="http://arxiv.org/abs/2105.09837"/>
        <updated>2021-05-23T06:10:39.688Z</updated>
        <summary type="html"><![CDATA[The Graph Augmented Multi-layer Perceptron (GA-MLP) model is an attractive
alternative to Graph Neural Networks (GNNs). This is because it is resistant to
the over-smoothing problem, and deeper GA-MLP models yield better performance.
GA-MLP models are traditionally optimized by the Stochastic Gradient Descent
(SGD). However, SGD suffers from the layer dependency problem, which prevents
the gradients of different layers of GA-MLP models from being calculated in
parallel. In this paper, we propose a parallel deep learning Alternating
Direction Method of Multipliers (pdADMM) framework to achieve model
parallelism: parameters in each layer of GA-MLP models can be updated in
parallel. The extended pdADMM-Q algorithm reduces communication cost by
utilizing the quantization technique. Theoretical convergence to a critical
point of the pdADMM algorithm and the pdADMM-Q algorithm is provided with a
sublinear convergence rate $o(1/k)$. Extensive experiments in six benchmark
datasets demonstrate that the pdADMM can lead to high speedup, and outperforms
all the existing state-of-the-art comparison methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1"&gt;Zheng Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yue Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos. (arXiv:2105.09783v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09783</id>
        <link href="http://arxiv.org/abs/2105.09783"/>
        <updated>2021-05-23T06:10:39.663Z</updated>
        <summary type="html"><![CDATA[The absence or abnormality of fidgety movements of joints or limbs is
strongly indicative of cerebral palsy in infants. Developing computer-based
methods for assessing infant movements in videos is pivotal for improved
cerebral palsy screening. Most existing methods use appearance-based features
and are thus sensitive to strong but irrelevant signals caused by background
clutter or a moving camera. Moreover, these features are computed over the
whole frame, thus they measure gross whole body movements rather than specific
joint/limb motion.

Addressing these challenges, we develop and validate a new method for fidgety
movement assessment from consumer-grade videos using human poses extracted from
short clips. Human poses capture only relevant motion profiles of joints and
limbs and are thus free from irrelevant appearance artifacts. The dynamics and
coordination between joints are modeled using spatio-temporal graph
convolutional networks. Frames and body parts that contain discriminative
information about fidgety movements are selected through a spatio-temporal
attention mechanism. We validate the proposed model on the cerebral palsy
screening task using a real-life consumer-grade video dataset collected at an
Australian hospital through the Cerebral Palsy Alliance, Australia. Our
experiments show that the proposed method achieves the ROC-AUC score of 81.87%,
significantly outperforming existing competing methods with better
interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Thai_B/0/1/0/all/0/1"&gt;Binh Nguyen-Thai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1"&gt;Vuong Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1"&gt;Catherine Morgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badawi_N/0/1/0/all/0/1"&gt;Nadia Badawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Activity Recognition for Smart Home Systems. (arXiv:2105.09787v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09787</id>
        <link href="http://arxiv.org/abs/2105.09787"/>
        <updated>2021-05-23T06:10:39.642Z</updated>
        <summary type="html"><![CDATA[Smart home environments are designed to provide services that help improve
the quality of life for the occupant via a variety of sensors and actuators
installed throughout the space. Many automated actions taken by a smart home
are governed by the output of an underlying activity recognition system.
However, activity recognition systems may not be perfectly accurate and
therefore inconsistencies in smart home operations can lead a user to wonder
"why did the smart home do that?" In this work, we build on insights from
Explainable Artificial Intelligence (XAI) techniques to contribute
computational methods for explainable activity recognition. Specifically, we
generate explanations for smart home activity recognition systems that explain
what about an activity led to the given classification. To do so, we introduce
four computational techniques for generating natural language explanations of
smart home data and compare their effectiveness at generating meaningful
explanations. Through a study with everyday users, we evaluate user preferences
towards the four explanation types. Our results show that the leading approach,
SHAP, has a 92% success rate in generating accurate explanations. Moreover, 84%
of sampled scenarios users preferred natural language explanations over a
simple activity label, underscoring the need for explainable activity
recognition systems. Finally, we show that explanations generated by some XAI
methods can lead users to lose confidence in the accuracy of the underlying
activity recognition model, while others lead users to gain confidence. Taking
all studied factors into consideration, we make a recommendation regarding
which existing XAI method leads to the best performance in the domain of smart
home automation, and discuss a range of topics for future work in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Devleena Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishimura_Y/0/1/0/all/0/1"&gt;Yasutaka Nishimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vivek_R/0/1/0/all/0/1"&gt;Rajan P. Vivek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeda_N/0/1/0/all/0/1"&gt;Naoto Takeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fish_S/0/1/0/all/0/1"&gt;Sean T. Fish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ploetz_T/0/1/0/all/0/1"&gt;Thomas Ploetz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1"&gt;Sonia Chernova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Adaptive Nearest Neighbor Classifier: Algorithm and Theory. (arXiv:2105.09788v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09788</id>
        <link href="http://arxiv.org/abs/2105.09788"/>
        <updated>2021-05-23T06:10:39.622Z</updated>
        <summary type="html"><![CDATA[When data is of an extraordinarily large size or physically stored in
different locations, the distributed nearest neighbor (NN) classifier is an
attractive tool for classification. We propose a novel distributed adaptive NN
classifier for which the number of nearest neighbors is a tuning parameter
stochastically chosen by a data-driven criterion. An early stopping rule is
proposed when searching for the optimal tuning parameter, which not only speeds
up the computation but also improves the finite sample performance of the
proposed Algorithm. Convergence rate of excess risk of the distributed adaptive
NN classifier is investigated under various sub-sample size compositions. In
particular, we show that when the sub-sample sizes are sufficiently large, the
proposed classifier achieves the nearly optimal convergence rate. Effectiveness
of the proposed approach is demonstrated through simulation studies as well as
an empirical application to a real-world dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruiqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_G/0/1/0/all/0/1"&gt;Ganggang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shang_Z/0/1/0/all/0/1"&gt;Zuofeng Shang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Transparent Adversarial Examples. (arXiv:2105.09685v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09685</id>
        <link href="http://arxiv.org/abs/2105.09685"/>
        <updated>2021-05-23T06:10:39.616Z</updated>
        <summary type="html"><![CDATA[There has been a rise in the use of Machine Learning as a Service (MLaaS)
Vision APIs as they offer multiple services including pre-built models and
algorithms, which otherwise take a huge amount of resources if built from
scratch. As these APIs get deployed for high-stakes applications, it's very
important that they are robust to different manipulations. Recent works have
only focused on typical adversarial attacks when evaluating the robustness of
vision APIs. We propose two new aspects of adversarial image generation methods
and evaluate them on the robustness of Google Cloud Vision API's optical
character recognition service and object detection APIs deployed in real-world
settings such as sightengine.com, picpurify.com, Google Cloud Vision API, and
Microsoft Azure's Computer Vision API. Specifically, we go beyond the
conventional small-noise adversarial attacks and introduce secret embedding and
transparent adversarial examples as a simpler way to evaluate robustness. These
methods are so straightforward that even non-specialists can craft such
attacks. As a result, they pose a serious threat where APIs are used for
high-stakes applications. Our transparent adversarial examples successfully
evade state-of-the art object detections APIs such as Azure Cloud Vision
(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).
90% of the images have a secret embedded text that successfully fools the
vision of time-limited humans but is detected by Google Cloud Vision API's
optical character recognition. Complementing to current research, our results
provide simple but unconventional methods on robustness evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_J/0/1/0/all/0/1"&gt;Jaydeep Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stochastic Composite Augmented Lagrangian Method For Reinforcement Learning. (arXiv:2105.09716v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2105.09716</id>
        <link href="http://arxiv.org/abs/2105.09716"/>
        <updated>2021-05-23T06:10:39.608Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the linear programming (LP) formulation for deep
reinforcement learning. The number of the constraints depends on the size of
state and action spaces, which makes the problem intractable in large or
continuous environments. The general augmented Lagrangian method suffers the
double-sampling obstacle in solving the LP. Namely, the conditional
expectations originated from the constraint functions and the quadratic
penalties in the augmented Lagrangian function impose difficulties in sampling
and evaluation. Motivated from the updates of the multipliers, we overcome the
obstacles in minimizing the augmented Lagrangian function by replacing the
intractable conditional expectations with the multipliers. Therefore, a deep
parameterized augment Lagrangian method is proposed. Furthermore, the
replacement provides a promising breakthrough to integrate the two steps in the
augmented Lagrangian method into a single constrained problem. A general
theoretical analysis shows that the solutions generated from a sequence of the
constrained optimizations converge to the optimal solution of the LP if the
error is controlled properly. A theoretical analysis on the quadratic penalty
algorithm under neural tangent kernel setting shows the residual can be
arbitrarily small if the parameter in network and optimization algorithm is
chosen suitably. Preliminary experiments illustrate that our method is
competitive to other state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yongfeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weijie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zaiwen Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Localization and Control of Magnetic Suture Needles in Cluttered Surgical Site with Blood and Tissue. (arXiv:2105.09481v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.09481</id>
        <link href="http://arxiv.org/abs/2105.09481"/>
        <updated>2021-05-23T06:10:39.583Z</updated>
        <summary type="html"><![CDATA[Real-time visual localization of needles is necessary for various surgical
applications, including surgical automation and visual feedback. In this study
we investigate localization and autonomous robotic control of needles in the
context of our magneto-suturing system. Our system holds the potential for
surgical manipulation with the benefit of minimal invasiveness and reduced
patient side effects. However, the non-linear magnetic fields produce
unintuitive forces and demand delicate position-based control that exceeds the
capabilities of direct human manipulation. This makes automatic needle
localization a necessity. Our localization method combines neural network-based
segmentation and classical techniques, and we are able to consistently locate
our needle with 0.73 mm RMS error in clean environments and 2.72 mm RMS error
in challenging environments with blood and occlusion. The average localization
RMS error is 2.16 mm for all environments we used in the experiments. We
combine this localization method with our closed-loop feedback control system
to demonstrate the further applicability of localization to autonomous control.
Our needle is able to follow a running suture path in (1) no blood, no tissue;
(2) heavy blood, no tissue; (3) no blood, with tissue; and (4) heavy blood,
with tissue environments. The tip position tracking error ranges from 2.6 mm to
3.7 mm RMS, opening the door towards autonomous suturing tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pryor_W/0/1/0/all/0/1"&gt;Will Pryor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnoy_Y/0/1/0/all/0/1"&gt;Yotam Barnoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raval_S/0/1/0/all/0/1"&gt;Suraj Raval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaolong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mair_L/0/1/0/all/0/1"&gt;Lamar Mair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerner_D/0/1/0/all/0/1"&gt;Daniel Lerner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erin_O/0/1/0/all/0/1"&gt;Onder Erin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1"&gt;Gregory D. Hager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_Mercado_Y/0/1/0/all/0/1"&gt;Yancy Diaz-Mercado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krieger_A/0/1/0/all/0/1"&gt;Axel Krieger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction. (arXiv:2105.09543v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09543</id>
        <link href="http://arxiv.org/abs/2105.09543"/>
        <updated>2021-05-23T06:10:39.576Z</updated>
        <summary type="html"><![CDATA[Distantly supervised (DS) relation extraction (RE) has attracted much
attention in the past few years as it can utilize large-scale auto-labeled
data. However, its evaluation has long been a problem: previous works either
took costly and inconsistent methods to manually examine a small sample of
model predictions, or directly test models on auto-labeled data -- which, by
our check, produce as much as 53% wrong labels at the entity pair level in the
popular NYT10 dataset. This problem has not only led to inaccurate evaluation,
but also made it hard to understand where we are and what's left to improve in
the research of DS-RE. To evaluate DS-RE models in a more credible way, we
build manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,
and thoroughly evaluate several competitive models, especially the latest
pre-trained ones. The experimental results show that the manual evaluation can
indicate very different conclusions from automatic ones, especially some
unexpected observations, e.g., pre-trained models can achieve dominating
performance while being more susceptible to false-positives compared to
previous methods. We hope that both our manual test sets and novel observations
can help advance future DS-RE research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tianyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1"&gt;Keyue Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yuzhuo Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhiyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yankai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Neuronal Ensemble Inference with Generative Model and MCMC. (arXiv:2105.09679v1 [cond-mat.dis-nn])]]></title>
        <id>http://arxiv.org/abs/2105.09679</id>
        <link href="http://arxiv.org/abs/2105.09679"/>
        <updated>2021-05-23T06:10:39.539Z</updated>
        <summary type="html"><![CDATA[Neuronal ensemble inference is a significant problem in the study of
biological neural networks. Various methods have been proposed for ensemble
inference from experimental data of neuronal activity. Among them, Bayesian
inference approach with generative model was proposed recently. However, this
method requires large computational cost for appropriate inference. In this
work, we give an improved Bayesian inference algorithm by modifying update rule
in Markov chain Monte Carlo method and introducing the idea of simulated
annealing for hyperparameter control. We compare the performance of ensemble
inference between our algorithm and the original one, and discuss the advantage
of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kimura_S/0/1/0/all/0/1"&gt;Shun Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Ota_K/0/1/0/all/0/1"&gt;Keisuke Ota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Takeda_K/0/1/0/all/0/1"&gt;Koujin Takeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Parameterized Complexity of Polytree Learning. (arXiv:2105.09675v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2105.09675</id>
        <link href="http://arxiv.org/abs/2105.09675"/>
        <updated>2021-05-23T06:10:39.519Z</updated>
        <summary type="html"><![CDATA[A Bayesian network is a directed acyclic graph that represents statistical
dependencies between variables of a joint probability distribution. A
fundamental task in data science is to learn a Bayesian network from observed
data. \textsc{Polytree Learning} is the problem of learning an optimal Bayesian
network that fulfills the additional property that its underlying undirected
graph is a forest. In this work, we revisit the complexity of \textsc{Polytree
Learning}. We show that \textsc{Polytree Learning} can be solved in $3^n \cdot
|I|^{\mathcal{O}(1)}$ time where $n$ is the number of variables and $|I|$ is
the total instance size. Moreover, we consider the influence of the number of
variables $d$ that might receive a nonempty parent set in the final DAG on the
complexity of \textsc{Polytree Learning}. We show that \textsc{Polytree
Learning} has no $f(d)\cdot |I|^{\mathcal{O}(1)}$-time algorithm, unlike
Bayesian network learning which can be solved in $2^d \cdot
|I|^{\mathcal{O}(1)}$ time. We show that, in contrast, if $d$ and the maximum
parent set size are bounded, then we can obtain efficient algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gruttemeier_N/0/1/0/all/0/1"&gt;Niels Gr&amp;#xfc;ttemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komusiewicz_C/0/1/0/all/0/1"&gt;Christian Komusiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morawietz_N/0/1/0/all/0/1"&gt;Nils Morawietz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble machine learning approach for screening of coronary heart disease based on echocardiography and risk factors. (arXiv:2105.09670v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09670</id>
        <link href="http://arxiv.org/abs/2105.09670"/>
        <updated>2021-05-23T06:10:39.511Z</updated>
        <summary type="html"><![CDATA[Background: Extensive clinical evidence suggests that a preventive screening
of coronary heart disease (CHD) at an earlier stage can greatly reduce the
mortality rate. We use 64 two-dimensional speckle tracking echocardiography
(2D-STE) features and seven clinical features to predict whether one has CHD.
Methods: We develop a machine learning approach that integrates a number of
popular classification methods together by model stacking, and generalize the
traditional stacking method to a two-step stacking method to improve the
diagnostic performance. Results: By borrowing strengths from multiple
classification models through the proposed method, we improve the CHD
classification accuracy from around 70% to 87.7% on the testing set. The
sensitivity of the proposed method is 0.903 and the specificity is 0.843, with
an AUC of 0.904, which is significantly higher than those of the individual
classification models. Conclusions: Our work lays a foundation for the
deployment of speckle tracking echocardiography-based screening tools for
coronary heart disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Huolan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yongkai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenguang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Huimin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhong_W/0/1/0/all/0/1"&gt;Wenxuan Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09632</id>
        <link href="http://arxiv.org/abs/2105.09632"/>
        <updated>2021-05-23T06:10:39.502Z</updated>
        <summary type="html"><![CDATA[Today, we are seeing an ever-increasing number of clinical notes that contain
clinical results, images, and textual descriptions of patient's health state.
All these data can be analyzed and employed to cater novel services that can
help people and domain experts with their common healthcare tasks. However,
many technologies such as Deep Learning and tools like Word Embeddings have
started to be investigated only recently, and many challenges remain open when
it comes to healthcare domain applications. To address these challenges, we
propose the use of Deep Learning and Word Embeddings for identifying sixteen
morbidity types within textual descriptions of clinical records. For this
purpose, we have used a Deep Learning model based on Bidirectional Long-Short
Term Memory (LSTM) layers which can exploit state-of-the-art vector
representations of data such as Word Embeddings. We have employed pre-trained
Word Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained
on the target domain. Furthermore, we have compared the performances of the
deep learning approaches against the traditional tf-idf using Support Vector
Machine and Multilayer perceptron (our baselines). From the obtained results it
seems that the latter outperforms the combination of Deep Learning approaches
using any word embeddings. Our preliminary results indicate that there are
specific features that make the dataset biased in favour of traditional machine
learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1"&gt;Danilo Dessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1"&gt;Rim Helaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vivek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1"&gt;Diego Reforgiato Recupero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1"&gt;Daniele Riboni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exact Poly-Time Membership-Queries Algorithm for Extraction a three-Layer ReLU Network. (arXiv:2105.09673v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09673</id>
        <link href="http://arxiv.org/abs/2105.09673"/>
        <updated>2021-05-23T06:10:39.448Z</updated>
        <summary type="html"><![CDATA[As machine learning increasingly becomes more prevalent in our everyday life,
many organizations offer neural-networks based services as a black-box. The
reasons for hiding a learning model may vary: e.g., preventing copying of its
behavior or keeping back an adversarial from reverse-engineering its mechanism
and revealing sensitive information about its training data.

However, even as a black-box, some information can still be discovered by
specific queries. In this work, we show a polynomial-time algorithm that uses a
polynomial number of queries to mimic precisely the behavior of a three-layer
neural network that uses ReLU activation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daniely_A/0/1/0/all/0/1"&gt;Amit Daniely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granot_E/0/1/0/all/0/1"&gt;Elad Granot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Hawkes Process with Gaussian Process Self Effects. (arXiv:2105.09618v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09618</id>
        <link href="http://arxiv.org/abs/2105.09618"/>
        <updated>2021-05-23T06:10:39.438Z</updated>
        <summary type="html"><![CDATA[Traditionally, Hawkes processes are used to model time--continuous point
processes with history dependence. Here we propose an extended model where the
self--effects are of both excitatory and inhibitory type and follow a Gaussian
Process. Whereas previous work either relies on a less flexible
parameterization of the model, or requires a large amount of data, our
formulation allows for both a flexible model and learning when data are scarce.
We continue the line of work of Bayesian inference for Hawkes processes, and
our approach dispenses with the necessity of estimating a branching structure
for the posterior, as we perform inference on an aggregated sum of Gaussian
Processes. Efficient approximate Bayesian inference is achieved via data
augmentation, and we describe a mean--field variational inference approach to
learn the model parameters. To demonstrate the flexibility of the model we
apply our methodology on data from three different domains and compare it to
previously reported results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Malem_Shinitski_N/0/1/0/all/0/1"&gt;Noa Malem-Shinitski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ojeda_C/0/1/0/all/0/1"&gt;Cesar Ojeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Opper_M/0/1/0/all/0/1"&gt;Manfred Opper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation. (arXiv:2105.09637v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.09637</id>
        <link href="http://arxiv.org/abs/2105.09637"/>
        <updated>2021-05-23T06:10:39.418Z</updated>
        <summary type="html"><![CDATA[A key challenge on the path to developing agents that learn complex
human-like behavior is the need to quickly and accurately quantify
human-likeness. While human assessments of such behavior can be highly
accurate, speed and scalability are limited. We address these limitations
through a novel automated Navigation Turing Test (ANTT) that learns to predict
human judgments of human-likeness. We demonstrate the effectiveness of our
automated NTT on a navigation task in a complex 3D environment. We investigate
six classification models to shed light on the types of architectures best
suited to this task, and validate them against data collected through a human
NTT. Our best models achieve high accuracy when distinguishing true human and
agent behavior. At the same time, we show that predicting finer-grained human
assessment of agents' progress towards human-like behavior remains unsolved.
Our work takes an important step towards agents that more effectively learn
complex human-like behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1"&gt;Sam Devlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1"&gt;Raluca Georgescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1"&gt;Ida Momennejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rzepecki_J/0/1/0/all/0/1"&gt;Jaroslaw Rzepecki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuniga_E/0/1/0/all/0/1"&gt;Evelyn Zuniga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costello_G/0/1/0/all/0/1"&gt;Gavin Costello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1"&gt;Guy Leroy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaw_A/0/1/0/all/0/1"&gt;Ali Shaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1"&gt;Katja Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09601</id>
        <link href="http://arxiv.org/abs/2105.09601"/>
        <updated>2021-05-23T06:10:39.393Z</updated>
        <summary type="html"><![CDATA[In recent years, abstractive text summarization with multimodal inputs has
started drawing attention due to its ability to accumulate information from
different source modalities and generate a fluent textual summary. However,
existing methods use short videos as the visual modality and short summary as
the ground-truth, therefore, perform poorly on lengthy videos and long
ground-truth summary. Additionally, there exists no benchmark dataset to
generalize this task on videos of varying lengths. In this paper, we introduce
AVIATE, the first large-scale dataset for abstractive text summarization with
videos of diverse duration, compiled from presentations in well-known academic
conferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding
research papers as the reference summaries, which ensure adequate quality and
uniformity of the ground-truth. We then propose {\name}, a factorized
multi-modal Transformer based decoder-only language model, which inherently
captures the intra-modal and inter-modal dynamics within various input
modalities for the text summarization task. {\name} utilizes an increasing
number of self-attentions to capture multimodality and performs significantly
better than traditional encoder-decoder based networks. Extensive experiments
illustrate that {\name} achieves significant improvement over the baselines in
both qualitative and quantitative evaluations on the existing How2 dataset for
short videos and newly introduced AVIATE dataset for videos with diverse
duration, beating the best baseline on the two datasets by $1.39$ and $2.74$
ROUGE-L points respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1"&gt;Yash Kumar Atri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1"&gt;Shraman Pramanick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1"&gt;Vikram Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution Agnostic Symbolic Representations for Time Series Dimensionality Reduction and Online Anomaly Detection. (arXiv:2105.09592v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09592</id>
        <link href="http://arxiv.org/abs/2105.09592"/>
        <updated>2021-05-23T06:10:39.382Z</updated>
        <summary type="html"><![CDATA[Due to the importance of the lower bounding distances and the attractiveness
of symbolic representations, the family of symbolic aggregate approximations
(SAX) has been used extensively for encoding time series data. However, typical
SAX-based methods rely on two restrictive assumptions; the Gaussian
distribution and equiprobable symbols. This paper proposes two novel
data-driven SAX-based symbolic representations, distinguished by their
discretization steps. The first representation, oriented for general data
compaction and indexing scenarios, is based on the combination of kernel
density estimation and Lloyd-Max quantization to minimize the information loss
and mean squared error in the discretization step. The second method, oriented
for high-level mining tasks, employs the Mean-Shift clustering method and is
shown to enhance anomaly detection in the lower-dimensional space. Besides, we
verify on a theoretical basis a previously observed phenomenon of the intrinsic
process that results in a lower than the expected variance of the intermediate
piecewise aggregate approximation. This phenomenon causes an additional
information loss but can be avoided with a simple modification. The proposed
representations possess all the attractive properties of the conventional SAX
method. Furthermore, experimental evaluation on real-world datasets
demonstrates their superiority compared to the traditional SAX and an
alternative data-driven SAX variant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bountrogiannis_K/0/1/0/all/0/1"&gt;Konstantinos Bountrogiannis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzagkarakis_G/0/1/0/all/0/1"&gt;George Tzagkarakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsakalides_P/0/1/0/all/0/1"&gt;Panagiotis Tsakalides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09540</id>
        <link href="http://arxiv.org/abs/2105.09540"/>
        <updated>2021-05-23T06:10:39.363Z</updated>
        <summary type="html"><![CDATA[The increasing concerns about data privacy and security drives the emergence
of a new field of studying privacy-preserving machine learning from isolated
data sources, i.e., \textit{federated learning}. Vertical federated learning,
where different parties hold different features for common users, has a great
potential of driving a more variety of business cooperation among enterprises
in different fields. Decision tree models especially decision tree ensembles
are a class of widely applied powerful machine learning models with high
interpretability and modeling efficiency. However, the interpretability are
compromised in these works such as SecureBoost since the feature names are not
exposed to avoid possible data breaches due to the unprotected decision path.
In this paper, we shall propose Fed-EINI, an efficient and interpretable
inference framework for federated decision tree models with only one round of
multi-party communication. We shall compute the candidate sets of leaf nodes
based on the local data at each party in parallel, followed by securely
computing the weight of the only leaf node in the intersection of the candidate
sets. We propose to protect the decision path by the efficient additively
homomorphic encryption method, which allows the disclosure of feature names and
thus makes the federated decision trees interpretable. The advantages of
Fed-EINI will be demonstrated through theoretical analysis and extensive
numerical results. Experiments show that the inference efficiency is improved
by over $50\%$ in average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaolin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuai Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zejin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongji Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Negational Symmetry of Quantum Neural Networks for Binary Pattern Classification. (arXiv:2105.09580v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09580</id>
        <link href="http://arxiv.org/abs/2105.09580"/>
        <updated>2021-05-23T06:10:39.355Z</updated>
        <summary type="html"><![CDATA[Entanglement is a physical phenomenon, which has fueled recent successes of
quantum algorithms. Although quantum neural networks (QNNs) have shown
promising results in solving simple machine learning tasks recently, for the
time being, the effect of entanglement in QNNs and the behavior of QNNs in
binary pattern classification are still underexplored. In this work, we provide
some theoretical insight into the properties of QNNs by presenting and
analyzing a new form of invariance embedded in QNNs for both quantum binary
classification and quantum representation learning, which we term negational
symmetry. Given a quantum binary signal and its negational counterpart where a
bitwise NOT operation is applied to each quantum bit of the binary signal, a
QNN outputs the same logits. That is to say, QNNs cannot differentiate a
quantum binary signal and its negational counterpart in a binary classification
task. We further empirically evaluate the negational symmetry of QNNs in binary
pattern classification tasks using Google's quantum computing framework. The
theoretical and experimental results suggest that negational symmetry is a
fundamental property of QNNs, which is not shared by classical models. Our
findings also imply that negational symmetry is a double-edged sword in
practical quantum applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1"&gt;Nanqing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1"&gt;Michael Kampffmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voiculescu_I/0/1/0/all/0/1"&gt;Irina Voiculescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric Xing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logarithmic landscape and power-law escape rate of SGD. (arXiv:2105.09557v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09557</id>
        <link href="http://arxiv.org/abs/2105.09557"/>
        <updated>2021-05-23T06:10:39.348Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) undergoes complicated multiplicative noise
for the mean-square loss. We use this property of the SGD noise to derive a
stochastic differential equation (SDE) with simpler additive noise by
performing a non-uniform transformation of the time variable. In the SDE, the
gradient of the loss is replaced by that of the logarithmized loss.
Consequently, we show that, near a local or global minimum, the stationary
distribution $P_\mathrm{ss}(\theta)$ of the network parameters $\theta$ follows
a power-law with respect to the loss function $L(\theta)$, i.e.
$P_\mathrm{ss}(\theta)\propto L(\theta)^{-\phi}$ with the exponent $\phi$
specified by the mini-batch size, the learning rate, and the Hessian at the
minimum. We obtain the escape rate formula from a local minimum, which is
determined not by the loss barrier height $\Delta L=L(\theta^s)-L(\theta^*)$
between a minimum $\theta^*$ and a saddle $\theta^s$ but by the logarithmized
loss barrier height $\Delta\log L=\log[L(\theta^s)/L(\theta^*)]$. Our
escape-rate formula explains an empirical fact that SGD prefers flat minima
with low effective dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1"&gt;Takashi Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1"&gt;Liu Ziyin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masahito Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Kronecker neural networks: A general framework for neural networks with adaptive activation functions. (arXiv:2105.09513v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09513</id>
        <link href="http://arxiv.org/abs/2105.09513"/>
        <updated>2021-05-23T06:10:39.334Z</updated>
        <summary type="html"><![CDATA[We propose a new type of neural networks, Kronecker neural networks (KNNs),
that form a general framework for neural networks with adaptive activation
functions. KNNs employ the Kronecker product, which provides an efficient way
of constructing a very wide network while keeping the number of parameters low.
Our theoretical analysis reveals that under suitable conditions, KNNs induce a
faster decay of the loss than that by the feed-forward networks. This is also
empirically verified through a set of computational examples. Furthermore,
under certain technical assumptions, we establish global convergence of
gradient descent for KNNs. As a specific case, we propose the Rowdy activation
function that is designed to get rid of any saturation region by injecting
sinusoidal fluctuations, which include trainable parameters. The proposed Rowdy
activation function can be employed in any neural network architecture like
feed-forward neural networks, Recurrent neural networks, Convolutional neural
networks etc. The effectiveness of KNNs with Rowdy activation is demonstrated
through various computational experiments including function approximation
using feed-forward neural networks, solution inference of partial differential
equations using the physics-informed neural networks, and standard deep
learning benchmark problems using convolutional and fully-connected neural
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jagtap_A/0/1/0/all/0/1"&gt;Ameya D. Jagtap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yeonjong Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the $\alpha$-lazy version of Markov chains in estimation and testing problems. (arXiv:2105.09536v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09536</id>
        <link href="http://arxiv.org/abs/2105.09536"/>
        <updated>2021-05-23T06:10:39.325Z</updated>
        <summary type="html"><![CDATA[We formulate extendibility of the minimax one-trajectory length of several
statistical Markov chains inference problems and give sufficient conditions for
both the possibility and impossibility of such extensions. We follow up and
apply this framework to recently published results on learning and identity
testing of ergodic Markov chains. In particular, we show that for some of the
aforementioned results, we can omit the aperiodicity requirement by simulating
an $\alpha$-lazy version of the original process, and quantify the incurred
cost of removing this assumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fried_S/0/1/0/all/0/1"&gt;Sela Fried&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wolfer_G/0/1/0/all/0/1"&gt;Geoffrey Wolfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregate Learning for Mixed Frequency Data. (arXiv:2105.09579v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09579</id>
        <link href="http://arxiv.org/abs/2105.09579"/>
        <updated>2021-05-23T06:10:39.318Z</updated>
        <summary type="html"><![CDATA[Large and acute economic shocks such as the 2007-2009 financial crisis and
the current COVID-19 infections rapidly change the economic environment. In
such a situation, the importance of real-time economic analysis using
alternative datais emerging. Alternative data such as search query and location
data are closer to real-time and richer than official statistics that are
typically released once a month in an aggregated form. We take advantage of
spatio-temporal granularity of alternative data and propose a
mixed-FrequencyAggregate Learning (MF-AGL)model that predicts economic
indicators for the smaller areas in real-time. We apply the model for the
real-world problem; prediction of the number of job applicants which is closely
related to the unemployment rates. We find that the proposed model predicts (i)
the regional heterogeneity of the labor market condition and (ii) the rapidly
changing economic status. The model can be applied to various tasks, especially
economic analysis]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Takamichi Toda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moriwaki_D/0/1/0/all/0/1"&gt;Daisuke Moriwaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ota_K/0/1/0/all/0/1"&gt;Kazuhiro Ota&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-informed neural networks (PINNs) for fluid mechanics: A review. (arXiv:2105.09506v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2105.09506</id>
        <link href="http://arxiv.org/abs/2105.09506"/>
        <updated>2021-05-23T06:10:39.309Z</updated>
        <summary type="html"><![CDATA[Despite the significant progress over the last 50 years in simulating flow
problems using numerical discretization of the Navier-Stokes equations (NSE),
we still cannot incorporate seamlessly noisy data into existing algorithms,
mesh-generation is complex, and we cannot tackle high-dimensional problems
governed by parametrized NSE. Moreover, solving inverse flow problems is often
prohibitively expensive and requires complex and expensive formulations and new
computer codes. Here, we review flow physics-informed learning, integrating
seamlessly data and mathematical models, and implementing them using
physics-informed neural networks (PINNs). We demonstrate the effectiveness of
PINNs for inverse problems related to three-dimensional wake flows, supersonic
flows, and biomedical flows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shengze Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mao_Z/0/1/0/all/0/1"&gt;Zhiping Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yin_M/0/1/0/all/0/1"&gt;Minglang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Preference Random Walk Algorithm for Link Prediction through Mutual Influence Nodes in Complex Networks. (arXiv:2105.09494v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.09494</id>
        <link href="http://arxiv.org/abs/2105.09494"/>
        <updated>2021-05-23T06:10:39.302Z</updated>
        <summary type="html"><![CDATA[Predicting links in complex networks has been one of the essential topics
within the realm of data mining and science discovery over the past few years.
This problem remains an attempt to identify future, deleted, and redundant
links using the existing links in a graph. Local random walk is considered to
be one of the most well-known algorithms in the category of quasi-local
methods. It traverses the network using the traditional random walk with a
limited number of steps, randomly selecting one adjacent node in each step
among the nodes which have equal importance. Then this method uses the
transition probability between node pairs to calculate the similarity between
them. However, in most datasets, this method is not able to perform accurately
in scoring remarkably similar nodes. In the present article, an efficient
method is proposed for improving local random walk by encouraging random walk
to move, in every step, towards the node which has a stronger influence.
Therefore, the next node is selected according to the influence of the source
node. To do so, using mutual information, the concept of the asymmetric mutual
influence of nodes is presented. A comparison between the proposed method and
other similarity-based methods (local, quasi-local, and global) has been
performed, and results have been reported for 11 real-world networks. It had a
higher prediction accuracy compared with other link prediction approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berahmand_K/0/1/0/all/0/1"&gt;Kamal Berahmand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasiri_E/0/1/0/all/0/1"&gt;Elahe Nasiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forouzandeh_S/0/1/0/all/0/1"&gt;Saman Forouzandeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuefeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09501</id>
        <link href="http://arxiv.org/abs/2105.09501"/>
        <updated>2021-05-23T06:10:39.262Z</updated>
        <summary type="html"><![CDATA[Existing multilingual machine translation approaches mainly focus on
English-centric directions, while the non-English directions still lag behind.
In this work, we aim to build a many-to-many translation system with an
emphasis on the quality of non-English language directions. Our intuition is
based on the hypothesis that a universal cross-language representation leads to
better multilingual translation performance. To this end, we propose \method, a
training method to obtain a single unified multilingual translation model.
mCOLT is empowered by two techniques: (i) a contrastive learning scheme to
close the gap among representations of different languages, and (ii) data
augmentation on both multiple parallel and monolingual data to further align
token representations. For English-centric directions, mCOLT achieves
competitive or even better performance than a strong pre-trained model mBART on
tens of WMT benchmarks. For non-English directions, mCOLT achieves an
improvement of average 10+ BLEU compared with the multilingual baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decomposing reverse-mode automatic differentiation. (arXiv:2105.09469v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2105.09469</id>
        <link href="http://arxiv.org/abs/2105.09469"/>
        <updated>2021-05-23T06:10:39.240Z</updated>
        <summary type="html"><![CDATA[We decompose reverse-mode automatic differentiation into (forward-mode)
linearization followed by transposition. Doing so isolates the essential
difference between forward- and reverse-mode AD, and simplifies their joint
implementation. In particular, once forward-mode AD rules are defined for every
primitive operation in a source language, only linear primitives require an
additional transposition rule in order to arrive at a complete reverse-mode AD
implementation. This is how reverse-mode AD is written in JAX and Dex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frostig_R/0/1/0/all/0/1"&gt;Roy Frostig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"&gt;Matthew J. Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maclaurin_D/0/1/0/all/0/1"&gt;Dougal Maclaurin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paszke_A/0/1/0/all/0/1"&gt;Adam Paszke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radul_A/0/1/0/all/0/1"&gt;Alexey Radul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for solution and inversion of structural mechanics and vibrations. (arXiv:2105.09477v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09477</id>
        <link href="http://arxiv.org/abs/2105.09477"/>
        <updated>2021-05-23T06:10:39.180Z</updated>
        <summary type="html"><![CDATA[Deep learning has been the most popular machine learning method in the last
few years. In this chapter, we present the application of deep learning and
physics-informed neural networks concerning structural mechanics and vibration
problems. Demonstration problems involve de-noising data, solution to
time-dependent ordinary and partial differential equations, and characterizing
the system's response for a given data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haghighat_E/0/1/0/all/0/1"&gt;Ehsan Haghighat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekar_A/0/1/0/all/0/1"&gt;Ali Can Bekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madenci_E/0/1/0/all/0/1"&gt;Erdogan Madenci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juanes_R/0/1/0/all/0/1"&gt;Ruben Juanes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-Decision Support System Interface Using Cancer Related Data for Lung Cancer Prognosis. (arXiv:2105.09471v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09471</id>
        <link href="http://arxiv.org/abs/2105.09471"/>
        <updated>2021-05-23T06:10:39.153Z</updated>
        <summary type="html"><![CDATA[Until the beginning of 2021, lung cancer is known to be the most common
cancer in the world. The disease is common due to factors such as occupational
exposure, smoking and environmental pollution. The early diagnosis and
treatment of the disease is of great importance as well as the prevention of
the causes that cause the disease. The study was planned to create a web
interface that works with machine learning algorithms to predict prognosis
using lung cancer clinical and gene expression in the GDC data portal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leblebici_A/0/1/0/all/0/1"&gt;Asim Leblebici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gesoglu_O/0/1/0/all/0/1"&gt;Omer Gesoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basbinar_Y/0/1/0/all/0/1"&gt;Yasemin Basbinar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying sources of uncertainty in drug discovery predictions with probabilistic models. (arXiv:2105.09474v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09474</id>
        <link href="http://arxiv.org/abs/2105.09474"/>
        <updated>2021-05-23T06:10:39.144Z</updated>
        <summary type="html"><![CDATA[Knowing the uncertainty in a prediction is critical when making expensive
investment decisions and when patient safety is paramount, but machine learning
(ML) models in drug discovery typically provide only a single best estimate and
ignore all sources of uncertainty. Predictions from these models may therefore
be over-confident, which can put patients at risk and waste resources when
compounds that are destined to fail are further developed. Probabilistic
predictive models (PPMs) can incorporate uncertainty in both the data and
model, and return a distribution of predicted values that represents the
uncertainty in the prediction. PPMs not only let users know when predictions
are uncertain, but the intuitive output from these models makes communicating
risk easier and decision making better. Many popular machine learning methods
have a PPM or Bayesian analogue, making PPMs easy to fit into current
workflows. We use toxicity prediction as a running example, but the same
principles apply for all prediction models used in drug discovery. The
consequences of ignoring uncertainty and how PPMs account for uncertainty are
also described. We aim to make the discussion accessible to a broad
non-mathematical audience. Equations are provided to make ideas concrete for
mathematical readers (but can be skipped without loss of understanding) and
code is available for computational researchers
(https://github.com/stanlazic/ML_uncertainty_quantification).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lazic_S/0/1/0/all/0/1"&gt;Stanley E. Lazic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_D/0/1/0/all/0/1"&gt;Dominic P. Williams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Physics-Constrained Deep Learning Model for Simulating Multiphase Flow in 3D Heterogeneous Porous Media. (arXiv:2105.09467v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2105.09467</id>
        <link href="http://arxiv.org/abs/2105.09467"/>
        <updated>2021-05-23T06:10:39.136Z</updated>
        <summary type="html"><![CDATA[In this work, an efficient physics-constrained deep learning model is
developed for solving multiphase flow in 3D heterogeneous porous media. The
model fully leverages the spatial topology predictive capability of
convolutional neural networks, and is coupled with an efficient
continuity-based smoother to predict flow responses that need spatial
continuity. Furthermore, the transient regions are penalized to steer the
training process such that the model can accurately capture flow in these
regions. The model takes inputs including properties of porous media, fluid
properties and well controls, and predicts the temporal-spatial evolution of
the state variables (pressure and saturation). While maintaining the continuity
of fluid flow, the 3D spatial domain is decomposed into 2D images for reducing
training cost, and the decomposition results in an increased number of training
data samples and better training efficiency. Additionally, a surrogate model is
separately constructed as a postprocessor to calculate well flow rate based on
the predictions of state variables from the deep learning model. We use the
example of CO2 injection into saline aquifers, and apply the
physics-constrained deep learning model that is trained from physics-based
simulation data and emulates the physics process. The model performs prediction
with a speedup of ~1400 times compared to physics-based simulations, and the
average temporal errors of predicted pressure and saturation plumes are 0.27%
and 0.099% respectively. Furthermore, water production rate is efficiently
predicted by a surrogate model for well flow rate, with a mean error less than
5%. Therefore, with its unique scheme to cope with the fidelity in fluid flow
in porous media, the physics-constrained deep learning model can become an
efficient predictive model for computationally demanding inverse problems or
other coupled processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bicheng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Harp_D/0/1/0/all/0/1"&gt;Dylan Robert Harp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bailian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pawar_R/0/1/0/all/0/1"&gt;Rajesh Pawar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning-Accelerated Data Assimilation and Forecasting Workflow for Commercial-Scale Geologic Carbon Storage. (arXiv:2105.09468v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2105.09468</id>
        <link href="http://arxiv.org/abs/2105.09468"/>
        <updated>2021-05-23T06:10:39.125Z</updated>
        <summary type="html"><![CDATA[Fast assimilation of monitoring data to update forecasts of pressure buildup
and carbon dioxide (CO2) plume migration under geologic uncertainties is a
challenging problem in geologic carbon storage. The high computational cost of
data assimilation with a high-dimensional parameter space impedes fast
decision-making for commercial-scale reservoir management. We propose to
leverage physical understandings of porous medium flow behavior with deep
learning techniques to develop a fast history matching-reservoir response
forecasting workflow. Applying an Ensemble Smoother Multiple Data Assimilation
framework, the workflow updates geologic properties and predicts reservoir
performance with quantified uncertainty from pressure history and CO2 plumes
interpreted through seismic inversion. As the most computationally expensive
component in such a workflow is reservoir simulation, we developed surrogate
models to predict dynamic pressure and CO2 plume extents under multi-well
injection. The surrogate models employ deep convolutional neural networks,
specifically, a wide residual network and a residual U-Net. The workflow is
validated against a flat three-dimensional reservoir model representative of a
clastic shelf depositional environment. Intelligent treatments are applied to
bridge between quantities in a true-3D reservoir model and those in a
single-layer reservoir model. The workflow can complete history matching and
reservoir forecasting with uncertainty quantification in less than one hour on
a mainstream personal workstation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hewei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Fu_P/0/1/0/all/0/1"&gt;Pengcheng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sherman_C/0/1/0/all/0/1"&gt;Christopher S. Sherman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jize Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ju_X/0/1/0/all/0/1"&gt;Xin Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hamon_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Hamon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Azzolina_N/0/1/0/all/0/1"&gt;Nicholas A. Azzolina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Burton_Kelly_M/0/1/0/all/0/1"&gt;Matthew Burton-Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Morris_J/0/1/0/all/0/1"&gt;Joseph P. Morris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An IoT-Based Framework for Remote Fall Monitoring. (arXiv:2105.09461v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2105.09461</id>
        <link href="http://arxiv.org/abs/2105.09461"/>
        <updated>2021-05-23T06:10:39.102Z</updated>
        <summary type="html"><![CDATA[Fall detection is a serious healthcare issue that needs to be solved. Falling
without quick medical intervention would lower the chances of survival for the
elderly, especially if living alone. Hence, the need is there for developing
fall detection algorithms with high accuracy. This paper presents a novel
IoT-based system for fall detection that includes a sensing device transmitting
data to a mobile application through a cloud-connected gateway device. Then,
the focus is shifted to the algorithmic aspect where multiple features are
extracted from 3-axis accelerometer data taken from existing datasets. The
results emphasize on the significance of Continuous Wavelet Transform (CWT) as
an influential feature for determining falls. CWT, Signal Energy (SE), Signal
Magnitude Area (SMA), and Signal Vector Magnitude (SVM) features have shown
promising classification results using K-Nearest Neighbors (KNN) and E-Nearest
Neighbors (ENN). For all performance metrics (accuracy, recall, precision,
specificity, and F1 Score), the achieved results are higher than 95% for a
dataset of small size, while more than 98.47% score is achieved in the
aforementioned criteria over the UniMiB-SHAR dataset by the same algorithms,
where the classification time for a single test record is extremely efficient
and is real-time]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Kababji_A/0/1/0/all/0/1"&gt;Ayman Al-Kababji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amira_A/0/1/0/all/0/1"&gt;Abbes Amira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bensaali_F/0/1/0/all/0/1"&gt;Faycal Bensaali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jarouf_A/0/1/0/all/0/1"&gt;Abdulah Jarouf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shidqi_L/0/1/0/all/0/1"&gt;Lisan Shidqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djelouat_H/0/1/0/all/0/1"&gt;Hamza Djelouat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09492</id>
        <link href="http://arxiv.org/abs/2105.09492"/>
        <updated>2021-05-23T06:10:39.091Z</updated>
        <summary type="html"><![CDATA[Deep generative models of 3D shapes have received a great deal of research
interest. Yet, almost all of them generate discrete shape representations, such
as voxels, point clouds, and polygon meshes. We present the first 3D generative
model for a drastically different shape representation -- describing a shape as
a sequence of computer-aided design (CAD) operations. Unlike meshes and point
clouds, CAD models encode the user creation process of 3D shapes, widely used
in numerous industrial and engineering design tasks. However, the sequential
and irregular structure of CAD operations poses significant challenges for
existing 3D generative models. Drawing an analogy between CAD operations and
natural language, we propose a CAD generative network based on the Transformer.
We demonstrate the performance of our model for both shape autoencoding and
random shape generation. To train our network, we create a new CAD dataset
consisting of 179,133 models and their CAD construction sequences. We have made
this dataset publicly available to promote future research on this topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rundi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chang Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Changxi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Superpixel-based Domain-Knowledge Infusion in Computer Vision. (arXiv:2105.09448v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09448</id>
        <link href="http://arxiv.org/abs/2105.09448"/>
        <updated>2021-05-23T06:10:39.076Z</updated>
        <summary type="html"><![CDATA[Superpixels are higher-order perceptual groups of pixels in an image, often
carrying much more information than raw pixels. There is an inherent relational
structure to the relationship among different superpixels of an image. This
relational information can convey some form of domain information about the
image, e.g. relationship between superpixels representing two eyes in a cat
image. Our interest in this paper is to construct computer vision models,
specifically those based on Deep Neural Networks (DNNs) to incorporate these
superpixels information. We propose a methodology to construct a hybrid model
that leverages (a) Convolutional Neural Network (CNN) to deal with spatial
information in an image, and (b) Graph Neural Network (GNN) to deal with
relational superpixel information in the image. The proposed deep model is
learned using a generic hybrid loss function that we call a `hybrid' loss. We
evaluate the predictive performance of our proposed hybrid vision model on four
popular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.
Moreover, we evaluate our method on three real-world classification tasks:
COVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint
Identification. The results demonstrate that the relational superpixel
information provided via a GNN could improve the performance of standard
CNN-based vision systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1"&gt;Gunjan Chhablani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1"&gt;Harshit Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1"&gt;Tirtharaj Dash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection. (arXiv:2105.09452v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09452</id>
        <link href="http://arxiv.org/abs/2105.09452"/>
        <updated>2021-05-23T06:10:39.068Z</updated>
        <summary type="html"><![CDATA[Non-stationary environments are challenging for reinforcement learning
algorithms. If the state transition and/or reward functions change based on
latent factors, the agent is effectively tasked with optimizing a behavior that
maximizes performance over a possibly infinite random sequence of Markov
Decision Processes (MDPs), each of which drawn from some unknown distribution.
We call each such MDP a context. Most related works make strong assumptions
such as knowledge about the distribution over contexts, the existence of
pre-training phases, or a priori knowledge about the number, sequence, or
boundaries between contexts. We introduce an algorithm that efficiently learns
policies in non-stationary environments. It analyzes a possibly infinite stream
of data and computes, in real-time, high-confidence change-point detection
statistics that reflect whether novel, specialized policies need to be created
and deployed to tackle novel contexts, or whether previously-optimized ones
might be reused. We show that (i) this algorithm minimizes the delay until
unforeseen changes to a context are detected, thereby allowing for rapid
responses; and (ii) it bounds the rate of false alarm, which is important in
order to minimize regret. Our method constructs a mixture model composed of a
(possibly infinite) ensemble of probabilistic dynamics predictors that model
the different modes of the distribution over underlying latent MDPs. We
evaluate our algorithm on high-dimensional continuous reinforcement learning
problems and show that it outperforms state-of-the-art (model-free and
model-based) RL algorithms, as well as state-of-the-art meta-learning methods
specially designed to deal with non-stationarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alegre_L/0/1/0/all/0/1"&gt;Lucas N. Alegre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazzan_A/0/1/0/all/0/1"&gt;Ana L. C. Bazzan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1"&gt;Bruno C. da Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Sanitation with Application to Node Classification. (arXiv:2105.09384v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09384</id>
        <link href="http://arxiv.org/abs/2105.09384"/>
        <updated>2021-05-23T06:10:38.908Z</updated>
        <summary type="html"><![CDATA[The past decades have witnessed the prosperity of graph mining, with a
multitude of sophisticated models and algorithms designed for various mining
tasks, such as ranking, classification, clustering and anomaly detection.
Generally speaking, the vast majority of the existing works aim to answer the
following question, that is, given a graph, what is the best way to mine it? In
this paper, we introduce the graph sanitation problem, to answer an orthogonal
question. That is, given a mining task and an initial graph, what is the best
way to improve the initially provided graph? By learning a better graph as part
of the input of the mining model, it is expected to benefit graph mining in a
variety of settings, ranging from denoising, imputation to defense. We
formulate the graph sanitation problem as a bilevel optimization problem, and
further instantiate it by semi-supervised node classification, together with an
effective solver named GaSoliNe. Extensive experimental results demonstrate
that the proposed method is (1) broadly applicable with respect to different
graph neural network models and flexible graph modification strategies, (2)
effective in improving the node classification accuracy on both the original
and contaminated graphs in various perturbation scenarios. In particular, it
brings up to 25% performance improvement over the existing robust graph neural
network methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1"&gt;Hanghang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation. (arXiv:2105.09365v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09365</id>
        <link href="http://arxiv.org/abs/2105.09365"/>
        <updated>2021-05-23T06:10:38.879Z</updated>
        <summary type="html"><![CDATA[Retinal Vessel Segmentation is important for diagnosis of various diseases.
The research on retinal vessel segmentation focuses mainly on improvement of
the segmentation model which is usually based on U-Net architecture. In our
study we use the U-Net architecture and we rely on heavy data augmentation in
order to achieve better performance. The success of the data augmentation
relies on successfully addressing the problem of input images. By analyzing
input images and performing the augmentation accordingly we show that the
performance of the U-Net model can be increased dramatically. Results are
reported using the most widely used retina dataset, DRIVE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Uysal_E/0/1/0/all/0/1"&gt;Enes Sadi Uysal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilici_M/0/1/0/all/0/1"&gt;M.&amp;#x15e;afak Bilici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaza_B/0/1/0/all/0/1"&gt;B. Selin Zaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozgenc_M/0/1/0/all/0/1"&gt;M. Yi&amp;#x11f;it &amp;#xd6;zgen&amp;#xe7;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boyar_O/0/1/0/all/0/1"&gt;Onur Boyar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Autonomous Road Vehicle Integrated Approaches to an Emergency Obstacle Avoidance Maneuver. (arXiv:2105.09446v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.09446</id>
        <link href="http://arxiv.org/abs/2105.09446"/>
        <updated>2021-05-23T06:10:38.865Z</updated>
        <summary type="html"><![CDATA[As passenger vehicle technologies have advanced, so have their capabilities
to avoid obstacles, especially with developments in tires, suspensions,
steering, as well as safety technologies like ABS, ESC, and more recently, ADAS
systems. However, environments around passenger vehicles have also become more
complex, and dangerous. There have previously been studies that outline driver
tendencies and performance capabilities when attempting to avoid obstacles
while driving passenger vehicles. Now that autonomous vehicles are being
developed with obstacle avoidance capabilities, it is important to target
performance that meets or exceeds that of human drivers. This manuscript
highlights systems that are crucial for an emergency obstacle avoidance
maneuver (EOAM) and identifies the state-of-the-art for each of the related
systems, while considering the nuances of traveling at highway speeds. Some of
the primary EOAM-related systems/areas that are discussed in this review are:
general path planning methods, system hierarchies, decision-making, trajectory
generation, and trajectory-tracking control methods. After concluding remarks,
suggestions for future work which could lead to an ideal EOAM development, are
discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lowe_E/0/1/0/all/0/1"&gt;Evan Lowe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guvenc_L/0/1/0/all/0/1"&gt;Levent Guven&amp;#xe7;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DistTune: Distributed Fine-Grained Adaptive Traffic Speed Prediction for Growing Transportation Networks. (arXiv:2105.09421v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09421</id>
        <link href="http://arxiv.org/abs/2105.09421"/>
        <updated>2021-05-23T06:10:38.850Z</updated>
        <summary type="html"><![CDATA[Over the past decade, many approaches have been introduced for traffic speed
prediction. However, providing fine-grained, accurate, time-efficient, and
adaptive traffic speed prediction for a growing transportation network where
the size of the network keeps increasing and new traffic detectors are
constantly deployed has not been well studied. To address this issue, this
paper presents DistTune based on Long Short-Term Memory (LSTM) and the
Nelder-Mead method. Whenever encountering an unprocessed detector, DistTune
decides if it should customize an LSTM model for this detector by comparing the
detector with other processed detectors in terms of the normalized traffic
speed patterns they have observed. If similarity is found, DistTune directly
shares an existing LSTM model with this detector to achieve time-efficient
processing. Otherwise, DistTune customizes an LSTM model for the detector to
achieve fine-grained prediction. To make DistTune even more time-efficient,
DistTune performs on a cluster of computing nodes in parallel. To achieve
adaptive traffic speed prediction, DistTune also provides LSTM re-customization
for detectors that suffer from unsatisfactory prediction accuracy due to for
instance traffic speed pattern change. Extensive experiments based on traffic
data collected from freeway I5-N in California are conducted to evaluate the
performance of DistTune. The results demonstrate that DistTune provides
fine-grained, accurate, time-efficient, and adaptive traffic speed prediction
for a growing transportation network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Ming-Chang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jia-Chun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gran_E/0/1/0/all/0/1"&gt;Ernst Gunnar Gran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separation of Powers in Federated Learning. (arXiv:2105.09400v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.09400</id>
        <link href="http://arxiv.org/abs/2105.09400"/>
        <updated>2021-05-23T06:10:38.835Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) enables collaborative training among mutually
distrusting parties. Model updates, rather than training data, are concentrated
and fused in a central aggregation server. A key security challenge in FL is
that an untrustworthy or compromised aggregation process might lead to
unforeseeable information leakage. This challenge is especially acute due to
recently demonstrated attacks that have reconstructed large fractions of
training data from ostensibly "sanitized" model updates.

In this paper, we introduce TRUDA, a new cross-silo FL system, employing a
trustworthy and decentralized aggregation architecture to break down
information concentration with regard to a single aggregator. Based on the
unique computational properties of model-fusion algorithms, all exchanged model
updates in TRUDA are disassembled at the parameter-granularity and re-stitched
to random partitions designated for multiple TEE-protected aggregators. Thus,
each aggregator only has a fragmentary and shuffled view of model updates and
is oblivious to the model architecture. Our new security mechanisms can
fundamentally mitigate training reconstruction attacks, while still preserving
the final accuracy of trained models and keeping performance overheads low.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pau-Chen Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eykholt_K/0/1/0/all/0/1"&gt;Kevin Eykholt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1"&gt;Zhongshu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamjoom_H/0/1/0/all/0/1"&gt;Hani Jamjoom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayaram_K/0/1/0/all/0/1"&gt;K. R. Jayaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valdez_E/0/1/0/all/0/1"&gt;Enriquillo Valdez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1"&gt;Ashish Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech & Song Emotion Recognition Using Multilayer Perceptron and Standard Vector Machine. (arXiv:2105.09406v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09406</id>
        <link href="http://arxiv.org/abs/2105.09406"/>
        <updated>2021-05-23T06:10:38.820Z</updated>
        <summary type="html"><![CDATA[Herein, we have compared the performance of SVM and MLP in emotion
recognition using speech and song channels of the RAVDESS dataset. We have
undertaken a journey to extract various audio features, identify optimal
scaling strategy and hyperparameter for our models. To increase sample size, we
have performed audio data augmentation and addressed data imbalance using
SMOTE. Our data indicate that optimised SVM outperforms MLP with an accuracy of
82 compared to 75%. Following data augmentation, the performance of both
algorithms was identical at ~79%, however, overfitting was evident for the SVM.
Our final exploration indicated that the performance of both SVM and MLP were
similar in which both resulted in lower accuracy for the speech channel
compared to the song channel. Our findings suggest that both SVM and MLP are
powerful classifiers for emotion recognition in a vocal-dependent manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Javaheri_B/0/1/0/all/0/1"&gt;Behzad Javaheri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder. (arXiv:2105.09428v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09428</id>
        <link href="http://arxiv.org/abs/2105.09428"/>
        <updated>2021-05-23T06:10:38.807Z</updated>
        <summary type="html"><![CDATA[In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an
Artificial Intelligence (AI) Health Outcomes Challenge seeking solutions to
predict risk in value-based care for incorporation into CMS Innovation Center
payment and service delivery models. Recently, modern language models have
played key roles in a number of health related tasks. This paper presents, to
the best of our knowledge, the first application of these models to patient
readmission prediction. To facilitate this, we create a dataset of 1.2 million
medical history samples derived from the Limited Dataset (LDS) issued by CMS.
Moreover, we propose a comprehensive modeling solution centered on a deep
learning framework for this data. To demonstrate the framework, we train an
attention-based Transformer to learn Medicare semantics in support of
performing downstream prediction tasks thereby achieving 0.91 AUC and 0.91
recall on readmission classification. We also introduce a novel data
pre-processing pipeline and discuss pertinent deployment considerations
surrounding model explainability and bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahlou_C/0/1/0/all/0/1"&gt;Chuhong Lahlou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crayton_A/0/1/0/all/0/1"&gt;Ancil Crayton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trier_C/0/1/0/all/0/1"&gt;Caroline Trier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willett_E/0/1/0/all/0/1"&gt;Evan Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[L1 Regression with Lewis Weights Subsampling. (arXiv:2105.09433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09433</id>
        <link href="http://arxiv.org/abs/2105.09433"/>
        <updated>2021-05-23T06:10:38.786Z</updated>
        <summary type="html"><![CDATA[We consider the problem of finding an approximate solution to $\ell_1$
regression while only observing a small number of labels. Given an $n \times d$
unlabeled data matrix $X$, we must choose a small set of $m \ll n$ rows to
observe the labels of, then output an estimate $\widehat{\beta}$ whose error on
the original problem is within a $1 + \varepsilon$ factor of optimal. We show
that sampling from $X$ according to its Lewis weights and outputting the
empirical minimizer succeeds with probability $1-\delta$ for $m >
O(\frac{1}{\varepsilon^2} d \log \frac{d}{\varepsilon \delta})$. This is
analogous to the performance of sampling according to leverage scores for
$\ell_2$ regression, but with exponentially better dependence on $\delta$. We
also give a corresponding lower bound of $\Omega(\frac{d}{\varepsilon^2} + (d +
\frac{1}{\varepsilon^2}) \log\frac{1}{\delta})$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1"&gt;Aditya Parulekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1"&gt;Advait Parulekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1"&gt;Eric Price&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Contrastive Learning. (arXiv:2105.09401v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09401</id>
        <link href="http://arxiv.org/abs/2105.09401"/>
        <updated>2021-05-23T06:10:38.751Z</updated>
        <summary type="html"><![CDATA[With the advent of big data across multiple high-impact applications, we are
often facing the challenge of complex heterogeneity. The newly collected data
usually consist of multiple modalities and characterized with multiple labels,
thus exhibiting the co-existence of multiple types of heterogeneity. Although
state-of-the-art techniques are good at modeling the complex heterogeneity with
sufficient label information, such label information can be quite expensive to
obtain in real applications, leading to sub-optimal performance using these
techniques. Inspired by the capability of contrastive learning to utilize rich
unlabeled data for improving performance, in this paper, we propose a unified
heterogeneous learning framework, which combines both weighted unsupervised
contrastive loss and weighted supervised contrastive loss to model multiple
types of heterogeneity. We also provide theoretical analyses showing that the
proposed weighted supervised contrastive loss is the lower bound of the mutual
information of two samples from the same class and the weighted unsupervised
contrastive loss is the lower bound of the mutual information between the
hidden representation of two views of the same sample. Experimental results on
real-world data sets demonstrate the effectiveness and the efficiency of the
proposed method modeling multiple types of heterogeneity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lecheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yada Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Balancing Robustness and Sensitivity using Feature Contrastive Learning. (arXiv:2105.09394v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09394</id>
        <link href="http://arxiv.org/abs/2105.09394"/>
        <updated>2021-05-23T06:10:38.598Z</updated>
        <summary type="html"><![CDATA[It is generally believed that robust training of extremely large networks is
critical to their success in real-world applications. However, when taken to
the extreme, methods that promote robustness can hurt the model's sensitivity
to rare or underrepresented patterns. In this paper, we discuss this trade-off
between sensitivity and robustness to natural (non-adversarial) perturbations
by introducing two notions: contextual feature utility and contextual feature
sensitivity. We propose Feature Contrastive Learning (FCL) that encourages a
model to be more sensitive to the features that have higher contextual utility.
Empirical results demonstrate that models trained with FCL achieve a better
balance of robustness and sensitivity, leading to improved generalization in
the presence of noise on both vision and NLP datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seungyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1"&gt;Daniel Glasner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1"&gt;Srikumar Ramalingam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papineni_K/0/1/0/all/0/1"&gt;Kishore Papineni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Learning Techniques to Identify Key Risk Factors for Diabetes and Undiagnosed Diabetes. (arXiv:2105.09379v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09379</id>
        <link href="http://arxiv.org/abs/2105.09379"/>
        <updated>2021-05-23T06:10:38.585Z</updated>
        <summary type="html"><![CDATA[This paper reviews a wide selection of machine learning models built to
predict both the presence of diabetes and the presence of undiagnosed diabetes
using eight years of National Health and Nutrition Examination Survey (NHANES)
data. Models are tuned and compared via their Brier Scores. The most relevant
variables of the best performing models are then compared. A Support Vector
Machine with a linear kernel performed best for predicting diabetes, returning
a Brier score of 0.0654 and an AUROC of 0.9235 on the test set. An elastic net
regression performed best for predicting undiagnosed diabetes with a Brier
score of 0.0294 and an AUROC of 0.9439 on the test set. Similar features appear
prominently in the models for both sets of models. Blood osmolality, family
history, the prevalance of various compounds, and hypertension are key
indicators for all diabetes risk. For undiagnosed diabetes in particular, there
are ethnicity or genetic components which arise as strong correlates as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1"&gt;Avraham Adler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation. (arXiv:2105.09371v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.09371</id>
        <link href="http://arxiv.org/abs/2105.09371"/>
        <updated>2021-05-23T06:10:38.567Z</updated>
        <summary type="html"><![CDATA[While imitation learning for vision based autonomous mobile robot navigation
has recently received a great deal of attention in the research community,
existing approaches typically require state action demonstrations that were
gathered using the deployment platform. However, what if one cannot easily
outfit their platform to record these demonstration signals or worse yet the
demonstrator does not have access to the platform at all? Is imitation learning
for vision based autonomous navigation even possible in such scenarios? In this
work, we hypothesize that the answer is yes and that recent ideas from the
Imitation from Observation (IfO) literature can be brought to bear such that a
robot can learn to navigate using only ego centric video collected by a
demonstrator, even in the presence of viewpoint mismatch. To this end, we
introduce a new algorithm, Visual Observation only Imitation Learning for
Autonomous navigation (VOILA), that can successfully learn navigation policies
from a single video demonstration collected from a physically different agent.
We evaluate VOILA in the photorealistic AirSim simulator and show that VOILA
not only successfully imitates the expert, but that it also learns navigation
policies that can generalize to novel environments. Further, we demonstrate the
effectiveness of VOILA in a real world setting by showing that it allows a
wheeled Jackal robot to successfully imitate a human walking in an environment
using a video recorded using a mobile phone camera.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1"&gt;Haresh Karnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1"&gt;Garrett Warnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xuesu Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1"&gt;Peter Stone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09356</id>
        <link href="http://arxiv.org/abs/2105.09356"/>
        <updated>2021-05-23T06:10:38.549Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Seyed Saeed Changiz Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fred X. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1"&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Keith Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1"&gt;Shuo Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User Label Leakage from Gradients in Federated Learning. (arXiv:2105.09369v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.09369</id>
        <link href="http://arxiv.org/abs/2105.09369"/>
        <updated>2021-05-23T06:10:38.516Z</updated>
        <summary type="html"><![CDATA[Federated learning enables multiple users to build a joint model by sharing
their model updates (gradients), while their raw data remains local on their
devices. In contrast to the common belief that this provides privacy benefits,
we here add to the very recent results on privacy risks when sharing gradients.
Specifically, we propose Label Leakage from Gradients (LLG), a novel attack to
extract the labels of the users' training data from their shared gradients. The
attack exploits the direction and magnitude of gradients to determine the
presence or absence of any label. LLG is simple yet effective, capable of
leaking potential sensitive information represented by labels, and scales well
to arbitrary batch sizes and multiple classes. We empirically and
mathematically demonstrate the validity of our attack under different settings.
Moreover, empirical results show that LLG successfully extracts labels with
high accuracy at the early stages of model training. We also discuss different
defense mechanisms against such leakage. Our findings suggest that gradient
compression is a practical technique to prevent our attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wainakh_A/0/1/0/all/0/1"&gt;Aidmar Wainakh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1"&gt;Fabrizio Ventola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mussig_T/0/1/0/all/0/1"&gt;Till M&amp;#xfc;&amp;#xdf;ig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keim_J/0/1/0/all/0/1"&gt;Jens Keim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cordero_C/0/1/0/all/0/1"&gt;Carlos Garcia Cordero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_E/0/1/0/all/0/1"&gt;Ephraim Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grube_T/0/1/0/all/0/1"&gt;Tim Grube&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muhlhauser_M/0/1/0/all/0/1"&gt;Max M&amp;#xfc;hlh&amp;#xe4;user&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepDebug: Fixing Python Bugs Using Stack Traces, Backtranslation, and Code Skeletons. (arXiv:2105.09352v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2105.09352</id>
        <link href="http://arxiv.org/abs/2105.09352"/>
        <updated>2021-05-23T06:10:38.347Z</updated>
        <summary type="html"><![CDATA[The joint task of bug localization and program repair is an integral part of
the software development process. In this work we present DeepDebug, an
approach to automated debugging using large, pretrained transformers. We begin
by training a bug-creation model on reversed commit data for the purpose of
generating synthetic bugs. We apply these synthetic bugs toward two ends.
First, we directly train a backtranslation model on all functions from 200K
repositories. Next, we focus on 10K repositories for which we can execute
tests, and create buggy versions of all functions in those repositories that
are covered by passing tests. This provides us with rich debugging information
such as stack traces and print statements, which we use to finetune our model
which was pretrained on raw source code. Finally, we strengthen all our models
by expanding the context window beyond the buggy function itself, and adding a
skeleton consisting of that function's parent class, imports, signatures,
docstrings, and method bodies, in order of priority. On the QuixBugs benchmark,
we increase the total number of fixes found by over 50%, while also decreasing
the false positive rate from 35% to 5% and decreasing the timeout from six
hours to one minute. On our own benchmark of executable tests, our model fixes
68% of all bugs on its first attempt without using traces, and after adding
traces it fixes 75% on first attempt. We will open-source our framework and
validation set for evaluating on executable tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1"&gt;Dawn Drain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1"&gt;Colin B. Clement&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serrato_G/0/1/0/all/0/1"&gt;Guillermo Serrato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1"&gt;Neel Sundaresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Objects as Extreme Points. (arXiv:2104.14066v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14066</id>
        <link href="http://arxiv.org/abs/2104.14066"/>
        <updated>2021-05-23T06:08:18.236Z</updated>
        <summary type="html"><![CDATA[Object detection can be regarded as a pixel clustering task, and its boundary
is determined by four extreme points (leftmost, top, rightmost, and bottom).
However, most studies focus on the center or corner points of the object, which
are actually conditional results of the extreme points. In this paper, we
present an Extreme-Point-Prediction-Based object detector (EPP-Net), which
directly regresses the relative displacement vector between each pixel and the
four extreme points. We also propose a new metric to measure the similarity
between two groups of extreme points, namely, Extreme Intersection over Union
(EIoU), and incorporate this EIoU as a new regression loss. Moreover, we
propose a novel branch to predict the EIoU between the ground-truth and the
prediction results, and combine it with the classification confidence as the
ranking keyword in non-maximum suppression. On the MS-COCO dataset, our method
achieves an average precision (AP) of 44.0% with ResNet-50 and an AP of 48.3%
with ResNeXt-101-DCN. The proposed EPP-Net provides a new method to detect
objects and outperforms state-of-the-art anchor-free detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Min Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1"&gt;Bo Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zihao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Junxing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1"&gt;Degang Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization. (arXiv:2104.04785v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04785</id>
        <link href="http://arxiv.org/abs/2104.04785"/>
        <updated>2021-05-23T06:08:18.215Z</updated>
        <summary type="html"><![CDATA[As climate change increases the intensity of natural disasters, society needs
better tools for adaptation. Floods, for example, are the most frequent natural
disaster, and better tools for flood risk communication could increase the
support for flood-resilient infrastructure development. Our work aims to enable
more visual communication of large-scale climate impacts via visualizing the
output of coastal flood models as satellite imagery. We propose the first deep
learning pipeline to ensure physical-consistency in synthetic visual satellite
imagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it
produces imagery that is physically-consistent with the output of an
expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery
relative to physics-based flood maps, we find that our proposed framework
outperforms baseline models in both physical-consistency and photorealism. We
envision our work to be the first step towards a global visualization of how
climate change shapes our landscape. Continuing on this path, we show that the
proposed pipeline generalizes to visualize arctic sea ice melt. We also publish
a dataset of over 25k labelled image-pairs to study image-to-image translation
in Earth observation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leshchinskiy_B/0/1/0/all/0/1"&gt;Brandon Leshchinskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Requena_Mesa_C/0/1/0/all/0/1"&gt;Christian Requena-Mesa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chishtie_F/0/1/0/all/0/1"&gt;Farrukh Chishtie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1"&gt;Natalia D&amp;#xed;az-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1"&gt;Oc&amp;#xe9;ane Boulais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aruna Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pina_A/0/1/0/all/0/1"&gt;Aaron Pi&amp;#xf1;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raissi_C/0/1/0/all/0/1"&gt;Chedy Ra&amp;#xef;ssi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1"&gt;Alexander Lavin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1"&gt;Dava Newman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning for Bone Mineral Density Estimation in Hip X-ray Images. (arXiv:2103.13482v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13482</id>
        <link href="http://arxiv.org/abs/2103.13482"/>
        <updated>2021-05-23T06:08:18.206Z</updated>
        <summary type="html"><![CDATA[Bone mineral density (BMD) is a clinically critical indicator of
osteoporosis, usually measured by dual-energy X-ray absorptiometry (DEXA). Due
to the limited accessibility of DEXA machines and examinations, osteoporosis is
often under-diagnosed and under-treated, leading to increased fragility
fracture risks. Thus it is highly desirable to obtain BMDs with alternative
cost-effective and more accessible medical imaging examinations such as X-ray
plain films. In this work, we formulate the BMD estimation from plain hip X-ray
images as a regression problem. Specifically, we propose a new semi-supervised
self-training algorithm to train the BMD regression model using images coupled
with DEXA measured BMDs and unlabeled images with pseudo BMDs. Pseudo BMDs are
generated and refined iteratively for unlabeled images during self-training. We
also present a novel adaptive triplet loss to improve the model's regression
accuracy. On an in-house dataset of 1,090 images (819 unique patients), our BMD
estimation method achieves a high Pearson correlation coefficient of 0.8805 to
ground-truth BMDs. It offers good feasibility to use the more accessible and
cheaper X-ray imaging for opportunistic osteoporosis screening.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yirui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiaoyun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fakai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1"&gt;Le Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chihung Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lingyun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1"&gt;Guotong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jing Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1"&gt;Chang-Fu Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miao_S/0/1/0/all/0/1"&gt;Shun Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility Mapping. (arXiv:2101.08413v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08413</id>
        <link href="http://arxiv.org/abs/2101.08413"/>
        <updated>2021-05-23T06:08:18.199Z</updated>
        <summary type="html"><![CDATA[Quantitative susceptibility mapping (QSM) has demonstrated great potential in
quantifying tissue susceptibility in various brain diseases. However, the
intrinsic ill-posed inverse problem relating the tissue phase to the underlying
susceptibility distribution affects the accuracy for quantifying tissue
susceptibility. Recently, deep learning has shown promising results to improve
accuracy by reducing the streaking artifacts. However, there exists a mismatch
between the observed phase and the theoretical forward phase estimated by the
susceptibility label. In this study, we proposed a model-based deep learning
architecture that followed the STI (susceptibility tensor imaging) physical
model, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the
relationship between STI-derived phase contrast induced by the susceptibility
tensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The
convolution neural networks are embedded into the physical model to learn a
regularization term containing prior information. ki33 and phase induced by
ki13 and ki23 terms were used as the labels for network training. Quantitative
evaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed
deep learning QSM methods. The results showed that MoDL-QSM achieved superior
performance, demonstrating its potential for future applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ruimin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jiayi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Baofeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jie Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuting Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chunlei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuyao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jie Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Hongjiang Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Person Extreme Motion Prediction with Cross-Interaction Attention. (arXiv:2105.08825v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08825</id>
        <link href="http://arxiv.org/abs/2105.08825"/>
        <updated>2021-05-23T06:08:18.192Z</updated>
        <summary type="html"><![CDATA[Human motion prediction aims to forecast future human poses given a sequence
of past 3D skeletons. While this problem has recently received increasing
attention, it has mostly been tackled for single humans in isolation. In this
paper we explore this problem from a novel perspective, involving humans
performing collaborative tasks. We assume that the input of our system are two
sequences of past skeletons for two interacting persons, and we aim to predict
the future motion for each of them. For this purpose, we devise a novel cross
interaction attention mechanism that exploits historical information of both
persons and learns to predict cross dependencies between self poses and the
poses of the other person in spite of their spatial or temporal distance. Since
no dataset to train such interactive situations is available, we have captured
ExPI (Extreme Pose Interaction), a new lab-based person interaction dataset of
professional dancers performing acrobatics. ExPI contains 115 sequences with
30k frames and 60k instances with annotated 3D body poses and shapes. We
thoroughly evaluate our cross-interaction network on this dataset and show that
both in short-term and long-term predictions, it consistently outperforms
baselines that independently reason for each person. We plan to release our
code jointly with the dataset and the train/test splits to spur future research
on the topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1"&gt;Xiaoyu Bie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1"&gt;Xavier Alameda-Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1"&gt;Francesc Moreno-Noguer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00641</id>
        <link href="http://arxiv.org/abs/2012.00641"/>
        <updated>2021-05-23T06:08:18.175Z</updated>
        <summary type="html"><![CDATA[The content based image retrieval aims to find the similar images from a
large scale dataset against a query image. Generally, the similarity between
the representative features of the query image and dataset images is used to
rank the images for retrieval. In early days, various hand designed feature
descriptors have been investigated based on the visual cues such as color,
texture, shape, etc. that represent the images. However, the deep learning has
emerged as a dominating alternative of hand-designed feature engineering from a
decade. It learns the features automatically from the data. This paper presents
a comprehensive survey of deep learning based developments in the past decade
for content based image retrieval. The categorization of existing
state-of-the-art methods from different perspectives is also performed for
greater understanding of the progress. The taxonomy used in this survey covers
different supervision, different networks, different descriptor type and
different retrieval type. A performance analysis is also performed using the
state-of-the-art methods. The insights are also presented for the benefit of
the researchers to observe the progress and to make the best choices. The
survey presented in this paper will help in further research progress in image
retrieval using deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1"&gt;Shiv Ram Dubey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs. (arXiv:2105.08147v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08147</id>
        <link href="http://arxiv.org/abs/2105.08147"/>
        <updated>2021-05-23T06:08:18.169Z</updated>
        <summary type="html"><![CDATA[Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently
obtained to determine the extent of lung disease and are a valuable source of
data for creating artificial intelligence models. Most work to date assessing
disease severity on chest imaging has focused on segmenting computed tomography
(CT) images; however, given that CTs are performed much less frequently than
chest X-rays for COVID-19 patients, automated lung lesion segmentation on chest
X-rays could be clinically valuable. There currently exists a universal
shortage of chest X-rays with ground truth COVID-19 lung lesion annotations,
and manually contouring lung opacities is a tedious, labor-intensive task. To
accelerate severity detection and augment the amount of publicly available
chest X-ray training data for supervised deep learning (DL) models, we leverage
existing annotated CT images to generate frontal projection "chest X-ray"
images for training COVID-19 chest X-ray models. In this paper, we propose an
automated pipeline for segmentation of COVID-19 lung lesions on chest X-rays
comprised of a Mask R-CNN trained on a mixed dataset of open-source chest
X-rays and coronal X-ray projections computed from annotated volumetric CTs. On
a test set containing 40 chest X-rays of COVID-19 positive patients, our model
achieved IoU scores of 0.81 $\pm$ 0.03 and 0.79 $\pm$ 0.03 when trained on a
dataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50
projections from CTs, respectively. Our model far outperforms current baselines
with limited supervised training and may assist in automated COVID-19 severity
quantification on chest X-rays.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ramesh_V/0/1/0/all/0/1"&gt;Vignav Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rister_B/0/1/0/all/0/1"&gt;Blaine Rister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization. (arXiv:2105.06129v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06129</id>
        <link href="http://arxiv.org/abs/2105.06129"/>
        <updated>2021-05-23T06:08:18.162Z</updated>
        <summary type="html"><![CDATA[Artistic style transfer aims to transfer the style characteristics of one
image onto another image while retaining its content. Existing approaches
commonly leverage various normalization techniques, although these face
limitations in adequately transferring diverse textures to different spatial
locations. Self-Attention-based approaches have tackled this issue with partial
success but suffer from unwanted artifacts. Motivated by these observations,
this paper aims to combine the best of both worlds: self-attention and
normalization. That yields a new plug-and-play module that we name
Self-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially
a spatially adaptive normalization module whose parameters are inferred through
attention on the content and style image. We demonstrate that plugging SAFIN
into the base network of another state-of-the-art method results in enhanced
stylization. We also develop a novel base network composed of Wavelet Transform
for multi-scale style transfer, which when combined with SAFIN, produces
visually appealing results with lesser unwanted textures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Aaditya Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hingane_S/0/1/0/all/0/1"&gt;Shreeshail Hingane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1"&gt;Xinyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remote Pulse Estimation in the Presence of Face Masks. (arXiv:2101.04096v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04096</id>
        <link href="http://arxiv.org/abs/2101.04096"/>
        <updated>2021-05-23T06:08:18.154Z</updated>
        <summary type="html"><![CDATA[Remote photoplethysmography (rPPG), a family of techniques for monitoring
blood volume changes, may be especially useful for widespread contactless
health monitoring using face video from consumer-grade visible-light cameras.
The COVID-19 pandemic has caused the widespread use of protective face masks.
We found that occlusions from cloth face masks increased the mean absolute
error of heart rate estimation by more than 80\% when deploying methods
designed on unmasked faces. We show that augmenting unmasked face videos by
adding patterned synthetic face masks forces the model to attend to the
periocular and forehead regions, improving performance and closing the gap
between masked and unmasked pulse estimation. To our knowledge, this paper is
the first to analyse the impact of face masks on the accuracy of pulse
estimation and offers several novel contributions: (a) 3D CNN-based method
designed for remote photoplethysmography in a presence of face masks, (b) two
publicly available pulse estimation datasets acquired from 86 unmasked and 61
masked subjects, (c) evaluations of handcrafted algorithms and a 3D CNN trained
on videos of unmasked faces and with masks synthetically added, and (d) data
augmentation method to add a synthetic mask to a face video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Speth_J/0/1/0/all/0/1"&gt;Jeremy Speth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vance_N/0/1/0/all/0/1"&gt;Nathan Vance&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1"&gt;Patrick Flynn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1"&gt;Kevin Bowyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1"&gt;Adam Czajka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio-Driven Emotional Video Portraits. (arXiv:2104.07452v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07452</id>
        <link href="http://arxiv.org/abs/2104.07452"/>
        <updated>2021-05-23T06:08:18.147Z</updated>
        <summary type="html"><![CDATA[Despite previous success in generating audio-driven talking heads, most of
the previous studies focus on the correlation between speech content and the
mouth shape. Facial emotion, which is one of the most important features on
natural human faces, is always neglected in their methods. In this work, we
present Emotional Video Portraits (EVP), a system for synthesizing high-quality
video portraits with vivid emotional dynamics driven by audios. Specifically,
we propose the Cross-Reconstructed Emotion Disentanglement technique to
decompose speech into two decoupled spaces, i.e., a duration-independent
emotion space and a duration dependent content space. With the disentangled
features, dynamic 2D emotional facial landmarks can be deduced. Then we propose
the Target-Adaptive Face Synthesis technique to generate the final high-quality
video portraits, by bridging the gap between the deduced landmarks and the
natural head poses of target videos. Extensive experiments demonstrate the
effectiveness of our method both qualitatively and quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xinya Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaisiyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wayne Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Feng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Object Recognition in Indoor Environments Using Topologically Persistent Features. (arXiv:2010.03196v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03196</id>
        <link href="http://arxiv.org/abs/2010.03196"/>
        <updated>2021-05-23T06:08:18.140Z</updated>
        <summary type="html"><![CDATA[Object recognition in unseen indoor environments remains a challenging
problem for visual perception of mobile robots. In this letter, we propose the
use of topologically persistent features, which rely on the objects' shape
information, to address this challenge. In particular, we extract two kinds of
features, namely, sparse persistence image (PI) and amplitude, by applying
persistent homology to multi-directional height function-based filtrations of
the cubical complexes representing the object segmentation maps. The features
are then used to train a fully connected network for recognition. For
performance evaluation, in addition to a widely used shape dataset and a
benchmark indoor scenes dataset, we collect a new dataset, comprising scene
images from two different environments, namely, a living room and a mock
warehouse. The scenes are captured using varying camera poses under different
illumination conditions and include up to five different objects from a given
set of fourteen objects. On the benchmark indoor scenes dataset, sparse PI
features show better recognition performance in unseen environments than the
features learned using the widely used ResNetV2-56 and EfficientNet-B4 models.
Further, they provide slightly higher recall and accuracy values than Faster
R-CNN, an end-to-end object detection method, and its state-of-the-art variant,
Domain Adaptive Faster R-CNN. The performance of our methods also remains
relatively unchanged from the training environment (living room) to the unseen
environment (mock warehouse) in the new dataset. In contrast, the performance
of the object detection methods drops substantially. We also implement the
proposed method on a real-world robot to demonstrate its usefulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1"&gt;Ekta U. Samani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingjian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1"&gt;Ashis G. Banerjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches. (arXiv:2105.08506v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08506</id>
        <link href="http://arxiv.org/abs/2105.08506"/>
        <updated>2021-05-23T06:08:18.131Z</updated>
        <summary type="html"><![CDATA[Detecting COVID-19 in computed tomography (CT) or radiography images has been
proposed as a supplement to the definitive RT-PCR test. We present a deep
learning ensemble for detecting COVID-19 infection, combining slice-based (2D)
and volume-based (3D) approaches. The 2D system detects the infection on each
CT slice independently, combining them to obtain the patient-level decision via
different methods (averaging and long-short term memory networks). The 3D
system takes the whole CT volume to arrive to the patient-level decision in one
step. A new high resolution chest CT scan dataset, called the IST-C dataset, is
also collected in this work. The proposed ensemble, called IST-CovNet, obtains
90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting
COVID-19 among normal controls and other types of lung pathologies; and 93.69%
accuracy and 0.99 AUC score on the publicly available MosMed dataset that
consists of COVID-19 scans and normal controls only. The system is deployed at
Istanbul University Cerrahpasa School of Medicine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sara Atito Ali Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1"&gt;Mehmet Can Yavuz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sen_M/0/1/0/all/0/1"&gt;Mehmet Umut Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gulsen_F/0/1/0/all/0/1"&gt;Fatih Gulsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tutar_O/0/1/0/all/0/1"&gt;Onur Tutar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korkmazer_B/0/1/0/all/0/1"&gt;Bora Korkmazer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Samanci_C/0/1/0/all/0/1"&gt;Cesur Samanci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sirolu_S/0/1/0/all/0/1"&gt;Sabri Sirolu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamid_R/0/1/0/all/0/1"&gt;Rauf Hamid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eryurekli_A/0/1/0/all/0/1"&gt;Ali Ergun Eryurekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mammadov_T/0/1/0/all/0/1"&gt;Toghrul Mammadov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yanikoglu_B/0/1/0/all/0/1"&gt;Berrin Yanikoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Simultaneous Pseudo Image Classification with Random Fields and a Deep Belief Network for Disease Indication. (arXiv:2104.10762v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10762</id>
        <link href="http://arxiv.org/abs/2104.10762"/>
        <updated>2021-05-23T06:08:18.110Z</updated>
        <summary type="html"><![CDATA[We show how to use random field theory in a supervised, energy-based model
for multiple pseudo image classification of 2D integer matrices. In the model,
each row of a 2D integer matrix is a pseudo image where a local receptive field
focuses on multiple portions of individual rows for simultaneous learning. The
model is used for a classification task consisting of presence of patient
biomarkers indicative of a particular disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1"&gt;Robert A. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which Parts determine the Impression of the Font?. (arXiv:2103.14216v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14216</id>
        <link href="http://arxiv.org/abs/2103.14216"/>
        <updated>2021-05-23T06:08:18.103Z</updated>
        <summary type="html"><![CDATA[Various fonts give different impressions, such as legible, rough, and
comic-text.This paper aims to analyze the correlation between the local shapes,
or parts, and the impression of fonts. By focusing on local shapes instead of
the whole letter shape, we can realize letter-shape independent and more
general analysis. The analysis is performed by newly combining SIFT and
DeepSets, to extract an arbitrary number of essential parts from a particular
font and aggregate them to infer the font impressions by nonlinear regression.
Our qualitative and quantitative analyses prove that (1)fonts with similar
parts have similar impressions, (2)many impressions, such as legible and rough,
largely depend on specific parts, (3)several impressions are very irrelevant to
parts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masaya Ueda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1"&gt;Akisato Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04916</id>
        <link href="http://arxiv.org/abs/2105.04916"/>
        <updated>2021-05-23T06:08:18.096Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have been attached great importance due to
their biological plausibility and high energy-efficiency on neuromorphic chips.
As these chips are usually resource-constrained, the compression of SNNs is
thus crucial along the road of practical use of SNNs. Most existing methods
directly apply pruning approaches in artificial neural networks (ANNs) to SNNs,
which ignore the difference between ANNs and SNNs, thus limiting the
performance of the pruned SNNs. Besides, these methods are only suitable for
shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination
in the neural system, we propose gradient rewiring (Grad R), a joint learning
algorithm of connectivity and weight for SNNs, that enables us to seamlessly
optimize network structure without retrain. Our key innovation is to redefine
the gradient to a new synaptic parameter, allowing better exploration of
network structures by taking full advantage of the competition between pruning
and regrowth of connections. The experimental results show that the proposed
method achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset
so far. Moreover, it reaches a $\sim$3.5% accuracy loss under unprecedented
0.73% connectivity, which reveals remarkable structure refining capability in
SNNs. Our work suggests that there exists extremely high redundancy in deep
SNNs. Our codes are available at
https://github.com/Yanqi-Chen/Gradient-Rewiring .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications. (arXiv:2012.01059v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01059</id>
        <link href="http://arxiv.org/abs/2012.01059"/>
        <updated>2021-05-23T06:08:18.079Z</updated>
        <summary type="html"><![CDATA[Improving irradiance forecasting is critical to further increase the share of
solar in the energy mix. On a short time scale, fish-eye cameras on the ground
are used to capture cloud displacements causing the local variability of the
electricity production. As most of the solar radiation comes directly from the
Sun, current forecasting approaches use its position in the image as a
reference to interpret the cloud cover dynamics. However, existing Sun tracking
methods rely on external data and a calibration of the camera, which requires
access to the device. To address these limitations, this study introduces an
image-based Sun tracking algorithm to localise the Sun in the image when it is
visible and interpolate its daily trajectory from past observations. We
validate the method on a set of sky images collected over a year at SIRTA's
lab. Experimental results show that the proposed method provides robust smooth
Sun trajectories with a mean absolute error below 1% of the image size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1"&gt;Quentin Paletta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-Tuned Sim-to-Real Transfer. (arXiv:2104.07662v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07662</id>
        <link href="http://arxiv.org/abs/2104.07662"/>
        <updated>2021-05-23T06:08:18.072Z</updated>
        <summary type="html"><![CDATA[Policies trained in simulation often fail when transferred to the real world
due to the `reality gap' where the simulator is unable to accurately capture
the dynamics and visual properties of the real world. Current approaches to
tackle this problem, such as domain randomization, require prior knowledge and
engineering to determine how much to randomize system parameters in order to
learn a policy that is robust to sim-to-real transfer while also not being too
conservative. We propose a method for automatically tuning simulator system
parameters to match the real world using only raw RGB images of the real world
without the need to define rewards or estimate state. Our key insight is to
reframe the auto-tuning of parameters as a search problem where we iteratively
shift the simulation system parameters to approach the real-world system
parameters. We propose a Search Param Model (SPM) that, given a sequence of
observations and actions and a set of system parameters, predicts whether the
given parameters are higher or lower than the true parameters used to generate
the observations. We evaluate our method on multiple robotic control tasks in
both sim-to-sim and sim-to-real transfer, demonstrating significant improvement
over naive domain randomization. Project videos and code at
https://yuqingd.github.io/autotuned-sim2real/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuqing Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1"&gt;Olivia Watkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1"&gt;Deepak Pathak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Analysis of Image Caption Generation using Deep Learning. (arXiv:2105.09906v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09906</id>
        <link href="http://arxiv.org/abs/2105.09906"/>
        <updated>2021-05-23T06:08:18.065Z</updated>
        <summary type="html"><![CDATA[Automated image captioning is one of the applications of Deep Learning which
involves fusion of work done in computer vision and natural language
processing, and it is typically performed using Encoder-Decoder architectures.
In this project, we have implemented and experimented with various flavors of
multi-modal image captioning networks where ResNet101, DenseNet121 and VGG19
based CNN Encoders and Attention based LSTM Decoders were explored. We have
studied the effect of beam size and the use of pretrained word embeddings and
compared them to baseline CNN encoder and RNN decoder architecture. The goal is
to analyze the performance of each approach using various evaluation metrics
including BLEU, CIDEr, ROUGE and METEOR. We have also explored model
explainability using Visual Attention Maps (VAM) to highlight parts of the
images which has maximum contribution for predicting each word of the generated
caption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Aditya Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girishekar_E/0/1/0/all/0/1"&gt;Eshwar Shamanna Girishekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1"&gt;Padmakar Anil Deshpande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seismic Fault Segmentation via 3D-CNN Training by a Few 2D Slices Labels. (arXiv:2105.03857v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03857</id>
        <link href="http://arxiv.org/abs/2105.03857"/>
        <updated>2021-05-23T06:08:18.058Z</updated>
        <summary type="html"><![CDATA[Detection faults in seismic data is a crucial step for seismic structural
interpretation, reservoir characterization and well placement. Some recent
works regard it as an image segmentation task. The task of image segmentation
requires huge labels, especially 3D seismic data, which has a complex structure
and lots of noise. Therefore, its annotation requires expert experience and a
huge workload. In this study, we present {\lambda}-BCE and {\lambda}-smooth
L1loss to effectively train 3D-CNN by some slices from 3D seismic data, so that
the model can learn the segmentation of 3D seismic data from a few 2D slices.
In order to fully extract information from limited data and suppress seismic
noise, we propose an attention module that can be used for active supervision
training and embedded in the network. The attention heatmap target is generated
by the original label, and letting it supervise the attention module using the
{\lambda}-smooth L1loss. The experiment proves the effectiveness of our loss
function and attention module, it also shows that our method can extract 3D
seismic features from a few 2D slices labels, and the segmentation effect
achieves state-of-the-art. We only use 3.3% of the all labels, and we can
achieve similar performance as using all labels. This work has been submitted
to the IEEE for possible publication. Copyright may be transferred without
notice, after which this version may no longer be accessible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1"&gt;YiMin Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kewen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianbing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1"&gt;Yingjie Xi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Camouflaged Instance Segmentation In-The-Wild: Dataset And Benchmark Suite. (arXiv:2103.17123v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17123</id>
        <link href="http://arxiv.org/abs/2103.17123"/>
        <updated>2021-05-23T06:08:18.024Z</updated>
        <summary type="html"><![CDATA[This paper pushes the envelope on camouflaged regions to decompose them into
meaningful components, namely, camouflaged instances. To promote the new task
of camouflaged instance segmentation in-the-wild, we introduce a new dataset,
namely CAMO++, by extending our preliminary CAMO dataset (camouflaged object
segmentation) in terms of quantity and diversity. The new dataset substantially
increases the number of images with hierarchical pixel-wise ground-truths. We
also provide a benchmark suite for the task of camouflaged instance
segmentation. In particular, we conduct extensive evaluation of
state-of-the-art instance segmentation methods on our newly constructed CAMO++
dataset in various scenarios. We also propose Camouflage Fusion Learning (CFL)
framework for camouflaged instance segmentation to further improve the
state-of-the-art performance. The dataset, model, evaluation suite, and
benchmark will be publicly available at our project page.
\url{https://sites.google.com/view/ltnghia/research/camo\_plus\_plus}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Trung-Nghia Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yubo Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tan-Cong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1"&gt;Minh-Quan Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khanh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1"&gt;Thanh-Toan Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh-Triet Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tam V. Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Discriminative Learning of Sounds for Audio Event Classification. (arXiv:2105.09279v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09279</id>
        <link href="http://arxiv.org/abs/2105.09279"/>
        <updated>2021-05-23T06:08:18.017Z</updated>
        <summary type="html"><![CDATA[Recent progress in network-based audio event classification has shown the
benefit of pre-training models on visual data such as ImageNet. While this
process allows knowledge transfer across different domains, training a model on
large-scale visual datasets is time consuming. On several audio event
classification benchmarks, we show a fast and effective alternative that
pre-trains the model unsupervised, only on audio data and yet delivers on-par
performance with ImageNet pre-training. Furthermore, we show that our
discriminative audio learning can be used to transfer knowledge across audio
datasets and optionally include ImageNet pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hornauer_S/0/1/0/all/0/1"&gt;Sascha Hornauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Stella X. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaffarzadegan_S/0/1/0/all/0/1"&gt;Shabnam Ghaffarzadegan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1"&gt;Liu Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03814</id>
        <link href="http://arxiv.org/abs/2102.03814"/>
        <updated>2021-05-23T06:08:18.005Z</updated>
        <summary type="html"><![CDATA[Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)
allow control of several applications by decoding neurophysiological phenomena,
which are usually recorded by electroencephalography (EEG) using a non-invasive
technique. Despite great advances in MI-based BCI, EEG rhythms are specific to
a subject and various changes over time. These issues point to significant
challenges to enhance the classification performance, especially in a
subject-independent manner. To overcome these challenges, we propose MIN2Net, a
novel end-to-end multi-task learning to tackle this task. We integrate deep
metric learning into a multi-task autoencoder to learn a compact and
discriminative latent representation from EEG and perform classification
simultaneously. This approach reduces the complexity in pre-processing, results
in significant performance improvement on EEG classification. Experimental
results in a subject-independent manner show that MIN2Net outperforms the
state-of-the-art techniques, achieving an F1-score improvement of 6.72%, and
2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that
MIN2Net improves discriminative information in the latent representation. This
study indicates the possibility and practicality of using this model to develop
MI-based BCI applications for new users without the need for calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1"&gt;Phairot Autthasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1"&gt;Rattanaphon Chaisaen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1"&gt;Thapanun Sudhawiyangkul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1"&gt;Phurin Rangpong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1"&gt;Suktipol Kiatthaveephong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1"&gt;Nat Dilokthanakul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1"&gt;Gun Bhakdisongkhram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1"&gt;Huy Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1"&gt;Theerawit Wilaiprasitporn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anchor-based Plain Net for Mobile Image Super-Resolution. (arXiv:2105.09750v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09750</id>
        <link href="http://arxiv.org/abs/2105.09750"/>
        <updated>2021-05-23T06:08:17.998Z</updated>
        <summary type="html"><![CDATA[Along with the rapid development of real-world applications, higher
requirements on the accuracy and efficiency of image super-resolution (SR) are
brought forward. Though existing methods have achieved remarkable success, the
majority of them demand plenty of computational resources and large amount of
RAM, and thus they can not be well applied to mobile device. In this paper, we
aim at designing efficient architecture for 8-bit quantization and deploy it on
mobile device. First, we conduct an experiment about meta-node latency by
decomposing lightweight SR architectures, which determines the portable
operations we can utilize. Then, we dig deeper into what kind of architecture
is beneficial to 8-bit quantization and propose anchor-based plain net (ABPN).
Finally, we adopt quantization-aware training strategy to further boost the
performance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in
terms of PSNR, while satisfying realistic needs at the same time. Code is
avaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zongcai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gangshan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Robust LiDAR-Based End-to-End Navigation. (arXiv:2105.09932v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.09932</id>
        <link href="http://arxiv.org/abs/2105.09932"/>
        <updated>2021-05-23T06:08:17.961Z</updated>
        <summary type="html"><![CDATA[Deep learning has been used to demonstrate end-to-end neural network learning
for autonomous vehicle control from raw sensory input. While LiDAR sensors
provide reliably accurate information, existing end-to-end driving solutions
are mainly based on cameras since processing 3D data requires a large memory
footprint and computation cost. On the other hand, increasing the robustness of
these systems is also critical; however, even estimating the model's
uncertainty is very challenging due to the cost of sampling-based methods. In
this paper, we present an efficient and robust LiDAR-based end-to-end
navigation framework. We first introduce Fast-LiDARNet that is based on sparse
convolution kernel optimization and hardware-aware model design. We then
propose Hybrid Evidential Fusion that directly estimates the uncertainty of the
prediction from only a single forward pass and then fuses the control
predictions intelligently. We evaluate our system on a full-scale vehicle and
demonstrate lane-stable as well as navigation capabilities. In the presence of
out-of-distribution events (e.g., sensor failures), our system significantly
improves robustness and reduces the number of takeovers in the real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhijian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1"&gt;Alexander Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Sibo Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1"&gt;Sertac Karaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine-learning based methodologies for 3d x-ray measurement, characterization and optimization for buried structures in advanced ic packages. (arXiv:2103.04838v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04838</id>
        <link href="http://arxiv.org/abs/2103.04838"/>
        <updated>2021-05-23T06:08:17.947Z</updated>
        <summary type="html"><![CDATA[For over 40 years lithographic silicon scaling has driven circuit integration
and performance improvement in the semiconductor industry. As silicon scaling
slows down, the industry is increasingly dependent on IC package technologies
to contribute to further circuit integration and performance improvements. This
is a paradigm shift and requires the IC package industry to reduce the size and
increase the density of internal interconnects on a scale which has never been
done before. Traditional package characterization and process optimization
relies on destructive techniques such as physical cross-sections and delayering
to extract data from internal package features. These destructive techniques
are not practical with today's advanced packages. In this paper we will
demonstrate how data acquired non-destructively with a 3D X-ray microscope can
be enhanced and optimized using machine learning, and can then be used to
measure, characterize and optimize the design and production of buried
interconnects in advanced IC packages. Test vehicles replicating 2.5D and HBM
construction were designed and fabricated, and digital data was extracted from
these test vehicles using 3D X-ray and machine learning techniques. The
extracted digital data was used to characterize and optimize the design and
production of the interconnects and demonstrates a superior alternative to
destructive physical analysis. We report an mAP of 0.96 for 3D object
detection, a dice score of 0.92 for 3D segmentation, and an average of 2.1um
error for 3D metrology on the test dataset. This paper is the first part of a
multi-part report.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pahwa_R/0/1/0/all/0/1"&gt;Ramanpreet S Pahwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1"&gt;Soon Wee Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ren Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1"&gt;Richard Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_O/0/1/0/all/0/1"&gt;Oo Zaw Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jie_W/0/1/0/all/0/1"&gt;Wang Jie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1"&gt;Vempati Srinivasa Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nwe_T/0/1/0/all/0/1"&gt;Tin Lay Nwe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yanjing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_J/0/1/0/all/0/1"&gt;Jens Timo Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pichumani_R/0/1/0/all/0/1"&gt;Ramani Pichumani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregorich_T/0/1/0/all/0/1"&gt;Thomas Gregorich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pedestrian Intention Prediction: A Multi-task Perspective. (arXiv:2010.10270v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10270</id>
        <link href="http://arxiv.org/abs/2010.10270"/>
        <updated>2021-05-23T06:08:17.928Z</updated>
        <summary type="html"><![CDATA[In order to be globally deployed, autonomous cars must guarantee the safety
of pedestrians. This is the reason why forecasting pedestrians' intentions
sufficiently in advance is one of the most critical and challenging tasks for
autonomous vehicles. This work tries to solve this problem by jointly
predicting the intention and visual states of pedestrians. In terms of visual
states, whereas previous work focused on x-y coordinates, we will also predict
the size and indeed the whole bounding box of the pedestrian. The method is a
recurrent neural network in a multi-task learning approach. It has one head
that predicts the intention of the pedestrian for each one of its future
position and another one predicting the visual states of the pedestrian.
Experiments on the JAAD dataset show the superiority of the performance of our
method compared to previous works for intention prediction. Also, although its
simple architecture (more than 2 times faster), the performance of the bounding
box prediction is comparable to the ones yielded by much more complex
architectures. Our code is available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bouhsain_S/0/1/0/all/0/1"&gt;Smail Ait Bouhsain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saadatnejad_S/0/1/0/all/0/1"&gt;Saeed Saadatnejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1"&gt;Alexandre Alahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Compositional Learning of Structured Visual Concepts. (arXiv:2105.09848v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09848</id>
        <link href="http://arxiv.org/abs/2105.09848"/>
        <updated>2021-05-23T06:08:17.922Z</updated>
        <summary type="html"><![CDATA[Humans are highly efficient learners, with the ability to grasp the meaning
of a new concept from just a few examples. Unlike popular computer vision
systems, humans can flexibly leverage the compositional structure of the visual
world, understanding new concepts as combinations of existing concepts. In the
current paper, we study how people learn different types of visual
compositions, using abstract visual forms with rich relational structure. We
find that people can make meaningful compositional generalizations from just a
few examples in a variety of scenarios, and we develop a Bayesian program
induction model that provides a close fit to the behavioral data. Unlike past
work examining special cases of compositionality, our work shows how a single
computational approach can account for many distinct types of compositional
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yanli Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1"&gt;Brenden M. Lake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face, Body, Voice: Video Person-Clustering with Multiple Modalities. (arXiv:2105.09939v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09939</id>
        <link href="http://arxiv.org/abs/2105.09939"/>
        <updated>2021-05-23T06:08:17.901Z</updated>
        <summary type="html"><![CDATA[The objective of this work is person-clustering in videos -- grouping
characters according to their identity. Previous methods focus on the narrower
task of face-clustering, and for the most part ignore other cues such as the
person's voice, their overall appearance (hair, clothes, posture), and the
editing structure of the videos. Similarly, most current datasets evaluate only
the task of face-clustering, rather than person-clustering. This limits their
applicability to downstream applications such as story understanding which
require person-level, rather than only face-level, reasoning. In this paper we
make contributions to address both these deficiencies: first, we introduce a
Multi-Modal High-Precision Clustering algorithm for person-clustering in videos
using cues from several modalities (face, body, and voice). Second, we
introduce a Video Person-Clustering dataset, for evaluating multi-modal
person-clustering. It contains body-tracks for each annotated character,
face-tracks when visible, and voice-tracks when speaking, with their associated
features. The dataset is by far the largest of its kind, and covers films and
TV-shows representing a wide range of demographics. Finally, we show the
effectiveness of using multiple modalities for person-clustering, explore the
use of this new broad task for story understanding through character
co-occurrences, and achieve a new state of the art on all available datasets
for face and person-clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1"&gt;Andrew Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1"&gt;Vicky Kalogeiton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biologically Inspired Semantic Lateral Connectivity for Convolutional Neural Networks. (arXiv:2105.09830v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09830</id>
        <link href="http://arxiv.org/abs/2105.09830"/>
        <updated>2021-05-23T06:08:17.895Z</updated>
        <summary type="html"><![CDATA[Lateral connections play an important role for sensory processing in visual
cortex by supporting discriminable neuronal responses even to highly similar
features. In the present work, we show that establishing a biologically
inspired Mexican hat lateral connectivity profile along the filter domain can
significantly improve the classification accuracy of a variety of lightweight
convolutional neural networks without the addition of trainable network
parameters. Moreover, we demonstrate that it is possible to analytically
determine the stationary distribution of modulated filter activations and
thereby avoid using recurrence for modeling temporal dynamics. We furthermore
reveal that the Mexican hat connectivity profile has the effect of ordering
filters in a sequence resembling the topographic organization of feature
selectivity in early visual cortex. In an ordered filter sequence, this profile
then sharpens the filters' tuning curves.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weidler_T/0/1/0/all/0/1"&gt;Tonio Weidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehnen_J/0/1/0/all/0/1"&gt;Julian Lehnen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denman_Q/0/1/0/all/0/1"&gt;Quinton Denman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebok_D/0/1/0/all/0/1"&gt;D&amp;#xe1;vid Seb&amp;#x151;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1"&gt;Gerhard Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Driessens_K/0/1/0/all/0/1"&gt;Kurt Driessens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senden_M/0/1/0/all/0/1"&gt;Mario Senden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PLSM: A Parallelized Liquid State Machine for Unintentional Action Detection. (arXiv:2105.09909v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09909</id>
        <link href="http://arxiv.org/abs/2105.09909"/>
        <updated>2021-05-23T06:08:17.884Z</updated>
        <summary type="html"><![CDATA[Reservoir Computing (RC) offers a viable option to deploy AI algorithms on
low-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired
RC model that mimics the cortical microcircuits and uses spiking neural
networks (SNN) that can be directly realized on neuromorphic hardware. In this
paper, we present a novel Parallelized LSM (PLSM) architecture that
incorporates spatio-temporal read-out layer and semantic constraints on model
output. To the best of our knowledge, such a formulation has been done for the
first time in literature, and it offers a computationally lighter alternative
to traditional deep-learning models. Additionally, we also present a
comprehensive algorithm for the implementation of parallelizable SNNs and LSMs
that are GPU-compatible. We implement the PLSM model to classify
unintentional/accidental video clips, using the Oops dataset. From the
experimental results on detecting unintentional action in video, it can be
observed that our proposed model outperforms a self-supervised model and a
fully supervised traditional deep learning model. All the implemented codes can
be found at our repository
https://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Dipayan Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1"&gt;Saumik Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1"&gt;Umapada Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1"&gt;Sukalpa Chanda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AnaXNet: Anatomy Aware Multi-label Finding Classification in Chest X-ray. (arXiv:2105.09937v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09937</id>
        <link href="http://arxiv.org/abs/2105.09937"/>
        <updated>2021-05-23T06:08:17.877Z</updated>
        <summary type="html"><![CDATA[Radiologists usually observe anatomical regions of chest X-ray images as well
as the overall image before making a decision. However, most existing deep
learning models only look at the entire X-ray image for classification, failing
to utilize important anatomical information. In this paper, we propose a novel
multi-label chest X-ray classification model that accurately classifies the
image finding and also localizes the findings to their correct anatomical
regions. Specifically, our model consists of two modules, the detection module
and the anatomical dependency module. The latter utilizes graph convolutional
networks, which enable our model to learn not only the label dependency but
also the relationship between the anatomical regions in the chest X-ray. We
further utilize a method to efficiently create an adjacency matrix for the
anatomical regions using the correlation of the label across the different
regions. Detailed experiments and analysis of our results show the
effectiveness of our method when compared to the current state-of-the-art
multi-label chest X-ray image classification methods while also providing
accurate location information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1"&gt;Nkechinyere N. Agu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Joy T. Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hanqing Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1"&gt;Ismini Lourentzou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Arjun Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1"&gt;Mehdi Moradi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1"&gt;Pingkun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1"&gt;James Hendler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09908</id>
        <link href="http://arxiv.org/abs/2105.09908"/>
        <updated>2021-05-23T06:08:17.864Z</updated>
        <summary type="html"><![CDATA[There is a prevailing trend to study urban morphology quantitatively thanks
to the growing accessibility to various forms of spatial big data, increasing
computing power, and use cases benefiting from such information. The methods
developed up to now measure urban morphology with numerical indices describing
density, proportion, and mixture, but they do not directly represent
morphological features from human's visual and intuitive perspective. We take
the first step to bridge the gap by proposing a deep learning-based technique
to automatically classify road networks into four classes on a visual basis.
The method is implemented by generating an image of the street network (Colored
Road Hierarchy Diagram), which we introduce in this paper, and classifying it
using a deep convolutional neural network (ResNet-34). The model achieves an
overall classification accuracy of 0.875. Nine cities around the world are
selected as the study areas and their road networks are acquired from
OpenStreetMap. Latent subgroups among the cities are uncovered through a
clustering on the percentage of each road network category. In the subsequent
part of the paper, we focus on the usability of such classification: the
effectiveness of our human perception augmentation is examined by a case study
of urban vitality prediction. An advanced tree-based regression model is for
the first time designated to establish the relationship between morphological
indices and vitality indicators. A positive effect of human perception
augmentation is detected in the comparative experiment of baseline model and
augmented model. This work expands the toolkit of quantitative urban morphology
study with new techniques, supporting further studies in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wangyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Abraham Noah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1"&gt;Filip Biljecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where Do Deep Fakes Look? Synthetic Face Detection via Gaze Tracking. (arXiv:2101.01165v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01165</id>
        <link href="http://arxiv.org/abs/2101.01165"/>
        <updated>2021-05-23T06:08:17.845Z</updated>
        <summary type="html"><![CDATA[Following the recent initiatives for the democratization of AI, deep fake
generators have become increasingly popular and accessible, causing dystopian
scenarios towards social erosion of trust. A particular domain, such as
biological signals, attracted attention towards detection methods that are
capable of exploiting authenticity signatures in real videos that are not yet
faked by generative approaches. In this paper, we first propose several
prominent eye and gaze features that deep fakes exhibit differently. Second, we
compile those features into signatures and analyze and compare those of real
and fake videos, formulating geometric, visual, metric, temporal, and spectral
variations. Third, we generalize this formulation to the deep fake detection
problem by a deep neural network, to classify any video in the wild as fake or
real. We evaluate our approach on several deep fake datasets, achieving 92.48%
accuracy on FaceForensics++, 80.0% on Deep Fakes (in the wild), 88.35% on
CelebDF, and 99.27% on DeeperForensics datasets. Our approach outperforms most
deep and biological fake detectors with complex network architectures without
the proposed gaze signatures. We conduct ablation studies involving different
features, architectures, sequence durations, and post-processing artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1"&gt;Ilke Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciftci_U/0/1/0/all/0/1"&gt;Umur A. Ciftci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry. (arXiv:2105.09899v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09899</id>
        <link href="http://arxiv.org/abs/2105.09899"/>
        <updated>2021-05-23T06:08:17.838Z</updated>
        <summary type="html"><![CDATA[The technology for Visual Odometry (VO) that estimates the position and
orientation of the moving object through analyzing the image sequences captured
by on-board cameras, has been well investigated with the rising interest in
autonomous driving. This paper studies monocular VO from the perspective of
Deep Learning (DL). Unlike most current learning-based methods, our approach,
called DeepAVO, is established on the intuition that features contribute
discriminately to different motion patterns. Specifically, we present a novel
four-branch network to learn the rotation and translation by leveraging
Convolutional Neural Networks (CNNs) to focus on different quadrants of optical
flow input. To enhance the ability of feature selection, we further introduce
an effective channel-spatial attention mechanism to force each branch to
explicitly distill related information for specific Frame to Frame (F2F) motion
estimation. Experiments on various datasets involving outdoor driving and
indoor walking scenarios show that the proposed DeepAVO outperforms the
state-of-the-art monocular methods by a large margin, demonstrating competitive
performance to the stereo VO algorithm and verifying promising potential for
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ran Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mingkun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Rujun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bo Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zhuoling Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Complexity of Standard Benchmarking Datasets for Long-Term Human Trajectory Prediction. (arXiv:2005.13934v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.13934</id>
        <link href="http://arxiv.org/abs/2005.13934"/>
        <updated>2021-05-23T06:08:17.832Z</updated>
        <summary type="html"><![CDATA[Methods to quantify the complexity of trajectory datasets are still a missing
piece in benchmarking human trajectory prediction models. In order to gain a
better understanding of the complexity of trajectory prediction tasks and
following the intuition, that more complex datasets contain more information,
an approach for quantifying the amount of information contained in a dataset
from a prototype-based dataset representation is proposed. The dataset
representation is obtained by first employing a non-trivial spatial sequence
alignment, which enables a subsequent learning vector quantization (LVQ) stage.
A large-scale complexity analysis is conducted on several human trajectory
prediction benchmarking datasets, followed by a brief discussion on indications
for human trajectory prediction and benchmarking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1"&gt;Ronny Hug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1"&gt;Stefan Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1"&gt;Wolfgang H&amp;#xfc;bner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1"&gt;Michael Arens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Convolutional Networks for Automatically Generating Image Masks to Train Mask R-CNN. (arXiv:2003.01383v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01383</id>
        <link href="http://arxiv.org/abs/2003.01383"/>
        <updated>2021-05-23T06:08:17.823Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel automatically generating image masks method for
the state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method
achieves the best results in object detection until now, however, it is very
time-consuming and laborious to get the object Masks for training, the proposed
method is composed by a two-stage design, to automatically generating image
masks, the first stage implements a fully convolutional networks (FCN) based
segmentation network, the second stage network, a Mask R-CNN based object
detection network, which is trained on the object image masks from FCN output,
the original input image, and additional label information. Through
experimentation, our proposed method can obtain the image masks automatically
to train Mask R-CNN, and it can achieve very high classification accuracy with
an over 90% mean of average precision (mAP) for segmentation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1"&gt;Jan Paul Siebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiangrong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trained Trajectory based Automated Parking System using Visual SLAM on Surround View Cameras. (arXiv:2001.02161v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.02161</id>
        <link href="http://arxiv.org/abs/2001.02161"/>
        <updated>2021-05-23T06:08:17.817Z</updated>
        <summary type="html"><![CDATA[Automated Parking is becoming a standard feature in modern vehicles. Existing
parking systems build a local map to be able to plan for maneuvering towards a
detected slot. Next generation parking systems have an use case where they
build a persistent map of the environment where the car is frequently parked,
say for example, home parking or office parking. The pre-built map helps in
re-localizing the vehicle better when its trying to park the next time. This is
achieved by augmenting the parking system with a Visual SLAM pipeline and the
feature is called trained trajectory parking in the automotive industry. In
this paper, we discuss the use cases, design and implementation of a trained
trajectory automated parking system. The proposed system is deployed on
commercial vehicles and the consumer application is illustrated in
\url{https://youtu.be/nRWF5KhyJZU}. The focus of this paper is on the
application and the details of vision algorithms are kept at high level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_N/0/1/0/all/0/1"&gt;Nivedita Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The MAMe Dataset: On the relevance of High Resolution and Variable Shape image properties. (arXiv:2007.13693v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13693</id>
        <link href="http://arxiv.org/abs/2007.13693"/>
        <updated>2021-05-23T06:08:17.797Z</updated>
        <summary type="html"><![CDATA[In the image classification task, the most common approach is to resize all
images in a dataset to a unique shape, while reducing their precision to a size
which facilitates experimentation at scale. This practice has benefits from a
computational perspective, but it entails negative side-effects on performance
due to loss of information and image deformation. In this work we introduce the
MAMe dataset, an image classification dataset with remarkable high resolution
and variable shape properties. The goal of MAMe is to provide a tool for
studying the impact of such properties in image classification, while
motivating research in the field. The MAMe dataset contains thousands of
artworks from three different museums, and proposes a classification task
consisting on differentiating between 29 mediums (i.e. materials and
techniques) supervised by art experts. After reviewing the singularity of MAMe
in the context of current image classification tasks, a thorough description of
the task is provided, together with dataset statistics. Experiments are
conducted to evaluate the impact of using high resolution images, variable
shape inputs and both properties at the same time. Results illustrate the
positive impact in performance when using high resolution images, while
highlighting the lack of solutions to exploit variable shapes. An additional
experiment exposes the distinctiveness between the MAMe dataset and the
prototypical ImageNet dataset. Finally, the baselines are inspected using
explainability methods and expert knowledge, to gain insights on the challenges
that remain ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pares_F/0/1/0/all/0/1"&gt;Ferran Par&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arias_Duart_A/0/1/0/all/0/1"&gt;Anna Arias-Duart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1"&gt;Dario Garcia-Gasulla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campo_Frances_G/0/1/0/all/0/1"&gt;Gema Campo-Franc&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viladrich_N/0/1/0/all/0/1"&gt;Nina Viladrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1"&gt;Eduard Ayguad&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Labarta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos. (arXiv:2105.09783v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09783</id>
        <link href="http://arxiv.org/abs/2105.09783"/>
        <updated>2021-05-23T06:08:17.790Z</updated>
        <summary type="html"><![CDATA[The absence or abnormality of fidgety movements of joints or limbs is
strongly indicative of cerebral palsy in infants. Developing computer-based
methods for assessing infant movements in videos is pivotal for improved
cerebral palsy screening. Most existing methods use appearance-based features
and are thus sensitive to strong but irrelevant signals caused by background
clutter or a moving camera. Moreover, these features are computed over the
whole frame, thus they measure gross whole body movements rather than specific
joint/limb motion.

Addressing these challenges, we develop and validate a new method for fidgety
movement assessment from consumer-grade videos using human poses extracted from
short clips. Human poses capture only relevant motion profiles of joints and
limbs and are thus free from irrelevant appearance artifacts. The dynamics and
coordination between joints are modeled using spatio-temporal graph
convolutional networks. Frames and body parts that contain discriminative
information about fidgety movements are selected through a spatio-temporal
attention mechanism. We validate the proposed model on the cerebral palsy
screening task using a real-life consumer-grade video dataset collected at an
Australian hospital through the Cerebral Palsy Alliance, Australia. Our
experiments show that the proposed method achieves the ROC-AUC score of 81.87%,
significantly outperforming existing competing methods with better
interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Thai_B/0/1/0/all/0/1"&gt;Binh Nguyen-Thai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1"&gt;Vuong Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1"&gt;Catherine Morgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badawi_N/0/1/0/all/0/1"&gt;Nadia Badawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Face Image Restoration and Frontalization for Recognition. (arXiv:2105.09907v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09907</id>
        <link href="http://arxiv.org/abs/2105.09907"/>
        <updated>2021-05-23T06:08:17.782Z</updated>
        <summary type="html"><![CDATA[In real-world scenarios, many factors may harm face recognition performance,
e.g., large pose, bad illumination,low resolution, blur and noise. To address
these challenges, previous efforts usually first restore the low-quality faces
to high-quality ones and then perform face recognition. However, most of these
methods are stage-wise, which is sub-optimal and deviates from the reality. In
this paper, we address all these challenges jointly for unconstrained face
recognition. We propose an Multi-Degradation Face Restoration (MDFR) model to
restore frontalized high-quality faces from the given low-quality ones under
arbitrary facial poses, with three distinct novelties. First, MDFR is a
well-designed encoder-decoder architecture which extracts feature
representation from an input face image with arbitrary low-quality factors and
restores it to a high-quality counterpart. Second, MDFR introduces a pose
residual learning strategy along with a 3D-based Pose Normalization Module
(PNM), which can perceive the pose gap between the input initial pose and its
real-frontal pose to guide the face frontalization. Finally, MDFR can generate
frontalized high-quality face images by a single unified network, showing a
strong capability of preserving face identity. Qualitative and quantitative
experiments on both controlled and in-the-wild benchmarks demonstrate the
superiority of MDFR over state-of-the-art methods on both face frontalization
and face restoration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1"&gt;Xiaoguang Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiankun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1"&gt;Wenjie Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BodyPressure -- Inferring Body Pose and Contact Pressure from a Depth Image. (arXiv:2105.09936v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09936</id>
        <link href="http://arxiv.org/abs/2105.09936"/>
        <updated>2021-05-23T06:08:17.774Z</updated>
        <summary type="html"><![CDATA[Contact pressure between the human body and its surroundings has important
implications. For example, it plays a role in comfort, safety, posture, and
health. We present a method that infers contact pressure between a human body
and a mattress from a depth image. Specifically, we focus on using a depth
image from a downward facing camera to infer pressure on a body at rest in bed
occluded by bedding, which is directly applicable to the prevention of pressure
injuries in healthcare. Our approach involves augmenting a real dataset with
synthetic data generated via a soft-body physics simulation of a human body, a
mattress, a pressure sensing mat, and a blanket. We introduce a novel deep
network that we trained on an augmented dataset and evaluated with real data.
The network contains an embedded human body mesh model and uses a white-box
model of depth and pressure image generation. Our network successfully infers
body pose, outperforming prior work. It also infers contact pressure across a
3D mesh model of the human body, which is a novel capability, and does so in
the presence of occlusion from blankets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Clever_H/0/1/0/all/0/1"&gt;Henry M. Clever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grady_P/0/1/0/all/0/1"&gt;Patrick Grady&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turk_G/0/1/0/all/0/1"&gt;Greg Turk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1"&gt;Charles C. Kemp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Pruning at Initialization. (arXiv:2002.08797v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08797</id>
        <link href="http://arxiv.org/abs/2002.08797"/>
        <updated>2021-05-23T06:08:17.767Z</updated>
        <summary type="html"><![CDATA[Overparameterized Neural Networks (NN) display state-of-the-art performance.
However, there is a growing need for smaller, energy-efficient, neural networks
tobe able to use machine learning applications on devices with limited
computational resources. A popular approach consists of using pruning
techniques. While these techniques have traditionally focused on pruning
pre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et
al. (2018) has shown promising results when pruning at initialization. However,
for Deep NNs, such procedures remain unsatisfactory as the resulting pruned
networks can be difficult to train and, for instance, they do not prevent one
layer from being fully pruned. In this paper, we provide a comprehensive
theoretical analysis of Magnitude and Gradient based pruning at initialization
and training of sparse architectures. This allows us to propose novel
principled approaches which we validate experimentally on a variety of NN
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1"&gt;Soufiane Hayou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ton_J/0/1/0/all/0/1"&gt;Jean-Francois Ton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing the Effect of Selection Bias on NN Generalization with a Thought Experiment. (arXiv:2105.09934v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09934</id>
        <link href="http://arxiv.org/abs/2105.09934"/>
        <updated>2021-05-23T06:08:17.749Z</updated>
        <summary type="html"><![CDATA[Learned networks in the domain of visual recognition and cognition impress in
part because even though they are trained with datasets many orders of
magnitude smaller than the full population of possible images, they exhibit
sufficient generalization to be applicable to new and previously unseen data.
Although many have examined issues regarding generalization from several
perspectives, we wondered If a network is trained with a biased dataset that
misses particular samples corresponding to some defining domain attribute, can
it generalize to the full domain from which that training dataset was
extracted? It is certainly true that in vision, no current training set fully
captures all visual information and this may lead to Selection Bias. Here, we
try a novel approach in the tradition of the Thought Experiment. We run this
thought experiment on a real domain of visual objects that we can fully
characterize and look at specific gaps in training data and their impact on
performance requirements. Our thought experiment points to three conclusions:
first, that generalization behavior is dependent on how sufficiently the
particular dimensions of the domain are represented during training; second,
that the utility of any generalization is completely dependent on the
acceptable system error; and third, that specific visual features of objects,
such as pose orientations out of the imaging plane or colours, may not be
recoverable if not represented sufficiently in a training set. Any currently
observed generalization in modern deep learning networks may be more the result
of coincidental alignments and whose utility needs to be confirmed with respect
to a system's performance specification. Our Thought Experiment Probe approach,
coupled with the resulting Bias Breakdown can be very informative towards
understanding the impact of biases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1"&gt;John K. Tsotsos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jun Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised, Topology-Aware Segmentation of Tubular Structures from Live Imaging 3D Microscopy. (arXiv:2105.09737v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09737</id>
        <link href="http://arxiv.org/abs/2105.09737"/>
        <updated>2021-05-23T06:08:17.742Z</updated>
        <summary type="html"><![CDATA[Motivated by a challenging tubular network segmentation task, this paper
tackles two commonly encountered problems in biomedical imaging: Topological
consistency of the segmentation, and limited annotations. We propose a
topological score which measures both topological and geometric consistency
between the predicted and ground truth segmentations, applied for model
selection and validation. We apply our topological score in three scenarios: i.
a U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised
U-net architecture, which offers a straightforward approach to jointly training
the network both as an autoencoder and a segmentation algorithm. This allows us
to utilize un-annotated data for training a representation that generalizes
across test data variability, in spite of our annotated training data having
very limited variation. Our contributions are validated on a challenging
segmentation task, locating tubular structures in the fetal pancreas from noisy
live imaging confocal microscopy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arnavaz_K/0/1/0/all/0/1"&gt;Kasra Arnavaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1"&gt;Oswin Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krivokapic_J/0/1/0/all/0/1"&gt;Jelena M. Krivokapic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heilmann_S/0/1/0/all/0/1"&gt;Silja Heilmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baerentzen_J/0/1/0/all/0/1"&gt;Jakob Andreas B&amp;#xe6;rentzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nyeng_P/0/1/0/all/0/1"&gt;Pia Nyeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1"&gt;Aasa Feragen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Physically Unconstrained Gaze Estimation. (arXiv:2105.09803v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09803</id>
        <link href="http://arxiv.org/abs/2105.09803"/>
        <updated>2021-05-23T06:08:17.735Z</updated>
        <summary type="html"><![CDATA[A major challenge for physically unconstrained gaze estimation is acquiring
training data with 3D gaze annotations for in-the-wild and outdoor scenarios.
In contrast, videos of human interactions in unconstrained environments are
abundantly available and can be much more easily annotated with frame-level
activity labels. In this work, we tackle the previously unexplored problem of
weakly-supervised gaze estimation from videos of human interactions. We
leverage the insight that strong gaze-related geometric constraints exist when
people perform the activity of "looking at each other" (LAEO). To acquire
viable 3D gaze supervision from LAEO labels, we propose a training algorithm
along with several novel loss functions especially designed for the task. With
weak supervision from two large scale CMU-Panoptic and AVA-LAEO activity
datasets, we show significant improvements in (a) the accuracy of
semi-supervised gaze estimation and (b) cross-domain generalization on the
state-of-the-art physically unconstrained in-the-wild Gaze360 gaze estimation
benchmark. We open source our code at
https://github.com/NVlabs/weakly-supervised-gaze.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1"&gt;Rakshit Kothari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1"&gt;Shalini De Mello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1"&gt;Umar Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1"&gt;Wonmin Byeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seonwook Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Perspective Anomaly Detection. (arXiv:2105.09903v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09903</id>
        <link href="http://arxiv.org/abs/2105.09903"/>
        <updated>2021-05-23T06:08:17.679Z</updated>
        <summary type="html"><![CDATA[Multi-view classification is inspired by the behavior of humans, especially
when fine-grained features or in our case rarely occurring anomalies are to be
detected. Current contributions point to the problem of how high-dimensional
data can be fused. In this work, we build upon the deep support vector data
description algorithm and address multi-perspective anomaly detection using
three different fusion techniques i.e. early fusion, late fusion, and late
fusion with multiple decoders. We employ different augmentation techniques with
a denoising process to deal with scarce one-class data, which further improves
the performance (ROC AUC = 80\%). Furthermore, we introduce the dices dataset
that consists of over 2000 grayscale images of falling dices from multiple
perspectives, with 5\% of the images containing rare anomalies (e.g. drill
holes, sawing, or scratches). We evaluate our approach on the new dices dataset
using images from two different perspectives and also benchmark on the standard
MNIST dataset. Extensive experiments demonstrate that our proposed approach
exceeds the state-of-the-art on both the MNIST and dices datasets. To the best
of our knowledge, this is the first work that focuses on addressing
multi-perspective anomaly detection in images by jointly using different
perspectives together with one single objective function for anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1"&gt;Manav Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1"&gt;Peter Jakob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1"&gt;Tobias Schmid-Schirling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M4Depth: A motion-based approach for monocular depth estimation on video sequences. (arXiv:2105.09847v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09847</id>
        <link href="http://arxiv.org/abs/2105.09847"/>
        <updated>2021-05-23T06:08:17.646Z</updated>
        <summary type="html"><![CDATA[Getting the distance to objects is crucial for autonomous vehicles. In
instances where depth sensors cannot be used, this distance has to be estimated
from RGB cameras. As opposed to cars, the task of estimating depth from
on-board mounted cameras is made complex on drones because of the lack of
constrains on motion during flights. %In the case of drones, this task is even
more complex than for car-mounted cameras since the camera motion is
unconstrained. In this paper, we present a method to estimate the distance of
objects seen by an on-board mounted camera by using its RGB video stream and
drone motion information. Our method is built upon a pyramidal convolutional
neural network architecture and uses time recurrence in pair with geometric
constraints imposed by motion to produce pixel-wise depth maps. %from a RGB
video stream of a camera attached to the drone In our architecture, each level
of the pyramid is designed to produce its own depth estimate based on past
observations and information provided by the previous level in the pyramid. We
introduce a spatial reprojection layer to maintain the spatio-temporal
consistency of the data between the levels. We analyse the performance of our
approach on Mid-Air, a public drone dataset featuring synthetic drone
trajectories recorded in a wide variety of unstructured outdoor environments.
Our experiments show that our network outperforms state-of-the-art depth
estimation methods and that the use of motion information is the main
contributing factor for this improvement. The code of our method is publicly
available on GitHub; see
$\href{https://github.com/michael-fonder/M4Depth}{\text{https://github.com/michael-fonder/M4Depth}}$]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fonder_M/0/1/0/all/0/1"&gt;Micha&amp;#xeb;l Fonder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1"&gt;Damien Ernst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1"&gt;Marc Van Droogenbroeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[POCFormer: A Lightweight Transformer Architecture for Detection of COVID-19 Using Point of Care Ultrasound. (arXiv:2105.09913v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09913</id>
        <link href="http://arxiv.org/abs/2105.09913"/>
        <updated>2021-05-23T06:08:17.622Z</updated>
        <summary type="html"><![CDATA[The rapid and seemingly endless expansion of COVID-19 can be traced back to
the inefficiency and shortage of testing kits that offer accurate results in a
timely manner. An emerging popular technique, which adopts improvements made in
mobile ultrasound technology, allows for healthcare professionals to conduct
rapid screenings on a large scale. We present an image-based solution that aims
at automating the testing process which allows for rapid mass testing to be
conducted with or without a trained medical professional that can be applied to
rural environments and third world countries. Our contributions towards rapid
large-scale testing include a novel deep learning architecture capable of
analyzing ultrasound data that can run in real-time and significantly improve
the current state-of-the-art detection accuracies using image-based COVID-19
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perera_S/0/1/0/all/0/1"&gt;Shehan Perera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adhikari_S/0/1/0/all/0/1"&gt;Srikar Adhikari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yilmaz_A/0/1/0/all/0/1"&gt;Alper Yilmaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepDarts: Modeling Keypoints as Objects for Automatic Scorekeeping in Darts using a Single Camera. (arXiv:2105.09880v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09880</id>
        <link href="http://arxiv.org/abs/2105.09880"/>
        <updated>2021-05-23T06:08:17.580Z</updated>
        <summary type="html"><![CDATA[Existing multi-camera solutions for automatic scorekeeping in steel-tip darts
are very expensive and thus inaccessible to most players. Motivated to develop
a more accessible low-cost solution, we present a new approach to keypoint
detection and apply it to predict dart scores from a single image taken from
any camera angle. This problem involves detecting multiple keypoints that may
be of the same class and positioned in close proximity to one another. The
widely adopted framework for regressing keypoints using heatmaps is not
well-suited for this task. To address this issue, we instead propose to model
keypoints as objects. We develop a deep convolutional neural network around
this idea and use it to predict dart locations and dartboard calibration points
within an overall pipeline for automatic dart scoring, which we call DeepDarts.
Additionally, we propose several task-specific data augmentation strategies to
improve the generalization of our method. As a proof of concept, two datasets
comprising 16k images originating from two different dartboard setups were
manually collected and annotated to evaluate the system. In the primary dataset
containing 15k images captured from a face-on view of the dartboard using a
smartphone, DeepDarts predicted the total score correctly in 94.7% of the test
images. In a second more challenging dataset containing limited training data
(830 images) and various camera angles, we utilize transfer learning and
extensive data augmentation to achieve a test accuracy of 84.0%. Because
DeepDarts relies only on single images, it has the potential to be deployed on
edge devices, giving anyone with a smartphone access to an automatic dart
scoring system for steel-tip darts. The code and datasets are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McNally_W/0/1/0/all/0/1"&gt;William McNally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walters_P/0/1/0/all/0/1"&gt;Pascale Walters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1"&gt;Kanav Vats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McPhee_J/0/1/0/all/0/1"&gt;John McPhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Transparent Adversarial Examples. (arXiv:2105.09685v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09685</id>
        <link href="http://arxiv.org/abs/2105.09685"/>
        <updated>2021-05-23T06:08:17.573Z</updated>
        <summary type="html"><![CDATA[There has been a rise in the use of Machine Learning as a Service (MLaaS)
Vision APIs as they offer multiple services including pre-built models and
algorithms, which otherwise take a huge amount of resources if built from
scratch. As these APIs get deployed for high-stakes applications, it's very
important that they are robust to different manipulations. Recent works have
only focused on typical adversarial attacks when evaluating the robustness of
vision APIs. We propose two new aspects of adversarial image generation methods
and evaluate them on the robustness of Google Cloud Vision API's optical
character recognition service and object detection APIs deployed in real-world
settings such as sightengine.com, picpurify.com, Google Cloud Vision API, and
Microsoft Azure's Computer Vision API. Specifically, we go beyond the
conventional small-noise adversarial attacks and introduce secret embedding and
transparent adversarial examples as a simpler way to evaluate robustness. These
methods are so straightforward that even non-specialists can craft such
attacks. As a result, they pose a serious threat where APIs are used for
high-stakes applications. Our transparent adversarial examples successfully
evade state-of-the art object detections APIs such as Azure Cloud Vision
(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).
90% of the images have a secret embedded text that successfully fools the
vision of time-limited humans but is detected by Google Cloud Vision API's
optical character recognition. Complementing to current research, our results
provide simple but unconventional methods on robustness evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_J/0/1/0/all/0/1"&gt;Jaydeep Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of Vehicle Re-Identification on the AI City Challenge. (arXiv:2105.09701v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09701</id>
        <link href="http://arxiv.org/abs/2105.09701"/>
        <updated>2021-05-23T06:08:17.562Z</updated>
        <summary type="html"><![CDATA[This paper introduces our solution for the Track2 in AI City Challenge 2021
(AICITY21). The Track2 is a vehicle re-identification (ReID) task with both the
real-world data and synthetic data. We mainly focus on four points, i.e.
training data, unsupervised domain-adaptive (UDA) training, post-processing,
model ensembling in this challenge. (1) Both cropping training data and using
synthetic data can help the model learn more discriminative features. (2) Since
there is a new scenario in the test set that dose not appear in the training
set, UDA methods perform well in the challenge. (3) Post-processing techniques
including re-ranking, image-to-track retrieval, inter-camera fusion, etc,
significantly improve final performance. (4) We ensemble CNN-based models and
transformer-based models which provide different representation diversity. With
aforementioned techniques, our method finally achieves 0.7445 mAP score,
yielding the first place in the competition. Codes are available at
https://github.com/michuanhaohao/AICITY2021_Track2_DMT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weihua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xianzhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jianyang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yiqi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shuting He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content-adaptive Representation Learning for Fast Image Super-resolution. (arXiv:2105.09645v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09645</id>
        <link href="http://arxiv.org/abs/2105.09645"/>
        <updated>2021-05-23T06:08:17.554Z</updated>
        <summary type="html"><![CDATA[Deep convolutional networks have attracted great attention in image
restoration and enhancement. Generally, restoration quality has been improved
by building more and more convolutional block. However, these methods mostly
learn a specific model to handle all images and ignore difficulty diversity. In
other words, an area in the image with high frequency tend to lose more
information during compressing while an area with low frequency tends to lose
less. In this article, we adrress the efficiency issue in image SR by
incorporating a patch-wise rolling network(PRN) to content-adaptively recover
images according to difficulty levels. In contrast to existing studies that
ignore difficulty diversity, we adopt different stage of a neural network to
perform image restoration. In addition, we propose a rolling strategy that
utilizes the parameters of each stage more flexible. Extensive experiments
demonstrate that our model not only shows a significant acceleration but also
maintain state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yukai Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jinghui Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Image Segmentation using Squeeze-and-Expansion Transformers. (arXiv:2105.09511v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09511</id>
        <link href="http://arxiv.org/abs/2105.09511"/>
        <updated>2021-05-23T06:08:17.533Z</updated>
        <summary type="html"><![CDATA[Medical image segmentation is important for computer-aided diagnosis. Good
segmentation demands the model to see the big picture and fine details
simultaneously, i.e., to learn image features that incorporate large context
while keep high spatial resolutions. To approach this goal, the most widely
used methods -- U-Net and variants, extract and fuse multi-scale features.
However, the fused features still have small "effective receptive fields" with
a focus on local image cues, limiting their performance. In this work, we
propose Segtran, an alternative segmentation framework based on transformers,
which have unlimited "effective receptive fields" even at high feature
resolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer:
a squeezed attention block regularizes the self attention of transformers, and
an expansion block learns diversified representations. Additionally, we propose
a new positional encoding scheme for transformers, imposing a continuity
inductive bias for images. Experiments were performed on 2D and 3D medical
image segmentation tasks: optic disc/cup segmentation in fundus images
(REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain
tumor segmentation in MRI scans (BraTS'19 challenge). Compared with
representative existing methods, Segtran consistently achieved the highest
segmentation accuracy, and exhibited good cross-domain generalization
capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaohua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sui_X/0/1/0/all/0/1"&gt;Xiuchao Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiangde Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xinxing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Goh_R/0/1/0/all/0/1"&gt;Rick Siow Mong Goh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks. (arXiv:2105.09720v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09720</id>
        <link href="http://arxiv.org/abs/2105.09720"/>
        <updated>2021-05-23T06:08:17.526Z</updated>
        <summary type="html"><![CDATA[The novel corona virus (Covid-19) has introduced significant challenges due
to its rapid spreading nature through respiratory transmission. As a result,
there is a huge demand for Artificial Intelligence (AI) based quick disease
diagnosis methods as an alternative to high demand tests such as Polymerase
Chain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective
radiography technique due to resource availability and quick screening. But, a
sufficient and systematic data collection that is required by complex deep
leaning (DL) models is more difficult and hence there are recent efforts that
utilize transfer learning to address this issue. Still these transfer learnt
models suffer from lack of generalization and increased bias to the training
dataset resulting poor performance for unseen data. Limited correlation of the
transferred features from the pre-trained model to a specific medical imaging
domain like X-ray and overfitting on fewer data can be reasons for this
circumstance. In this work, we propose a novel Graph Convolution Neural Network
(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR
images and meta information about patients. The proposed method exploits
important relational knowledge between data instances and their features using
graph representation and applies convolution to learn the graph data which is
not possible with conventional convolution on Euclidean domain. The results of
extensive experiments of proposed model on binary (Covid vs normal) and three
class (Covid, normal, other pneumonia) classification problems outperform
different benchmark transfer learnt models, hence overcoming the aforementioned
drawbacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mudiyanselage_T/0/1/0/all/0/1"&gt;Thosini Bamunu Mudiyanselage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Senanayake_N/0/1/0/all/0/1"&gt;Nipuna Senanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1"&gt;Chunyan Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanqing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DPN-SENet:A self-attention mechanism neural network for detection and diagnosis of COVID-19 from chest x-ray images. (arXiv:2105.09683v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09683</id>
        <link href="http://arxiv.org/abs/2105.09683"/>
        <updated>2021-05-23T06:08:17.519Z</updated>
        <summary type="html"><![CDATA[Background and Objective: The new type of coronavirus is also called
COVID-19. It began to spread at the end of 2019 and has now spread across the
world. Until October 2020, It has infected around 37 million people and claimed
about 1 million lives. We propose a deep learning model that can help
radiologists and clinicians use chest X-rays to diagnose COVID-19 cases and
show the diagnostic features of pneumonia. Methods: The approach in this study
is: 1) we propose a data enhancement method to increase the diversity of the
data set, thereby improving the generalization performance of the model. 2) Our
deep convolution neural network model DPN-SE adds a self-attention mechanism to
the DPN network. The addition of a self-attention mechanism has greatly
improved the performance of the network. 3) Use the Lime interpretable library
to mark the feature regions on the X-ray medical image that helps doctors more
quickly diagnose COVID-19 in people. Results: Under the same network model, the
data with and without data enhancement is put into the model for training
respectively. At last, comparing two experimental results: among the 10 network
models with different structures, 7 network models have improved their effects
after using data enhancement, with an average improvement of 1% in recognition
accuracy. We propose that the accuracy and recall rates of the DPN-SE network
are 93% and 98% of cases (COVID vs. pneumonia bacteria vs. viral pneumonia vs.
normal). Compared with the original DPN, the respective accuracy is improved by
2%. Conclusion: The data augmentation method we used has achieved effective
results on a small amount of data set, showing that a reasonable data
augmentation method can improve the recognition accuracy without changing the
sample size and model structure. Overall, the proposed method and model can
effectively become a very useful tool for clinical radiologists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_B/0/1/0/all/0/1"&gt;Bo Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xue_R/0/1/0/all/0/1"&gt;Ruhui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Laili Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiang_W/0/1/0/all/0/1"&gt;Wei Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FVC: A New Framework towards Deep Video Compression in Feature Space. (arXiv:2105.09600v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09600</id>
        <link href="http://arxiv.org/abs/2105.09600"/>
        <updated>2021-05-23T06:08:17.511Z</updated>
        <summary type="html"><![CDATA[Learning based video compression attracts increasing attention in the past
few years. The previous hybrid coding approaches rely on pixel space operations
to reduce spatial and temporal redundancy, which may suffer from inaccurate
motion estimation or less effective motion compensation. In this work, we
propose a feature-space video coding network (FVC) by performing all major
operations (i.e., motion estimation, motion compression, motion compensation
and residual compression) in the feature space. Specifically, in the proposed
deformable compensation module, we first apply motion estimation in the feature
space to produce motion information (i.e., the offset maps), which will be
compressed by using the auto-encoder style network. Then we perform motion
compensation by using deformable convolution and generate the predicted
feature. After that, we compress the residual feature between the feature from
the current frame and the predicted feature from our deformable compensation
module. For better frame reconstruction, the reference features from multiple
previous reconstructed frames are also fused by using the non-local attention
mechanism in the multi-frame feature fusion module. Comprehensive experimental
results demonstrate that the proposed framework achieves the state-of-the-art
performance on four benchmark datasets including HEVC, UVG, VTL and MCL-JCV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhihao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Connected Component Labelling algorithm for multi-pixel per clock cycle video strea. (arXiv:2105.09658v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09658</id>
        <link href="http://arxiv.org/abs/2105.09658"/>
        <updated>2021-05-23T06:08:17.504Z</updated>
        <summary type="html"><![CDATA[This work describes the hardware implementation of a connected component
labelling (CCL) module in reprogammable logic. The main novelty of the design
is the "full", i.e. without any simplifications, support of a 4 pixel per clock
format (4 ppc) and real-time processing of a 4K/UltraHD video stream (3840 x
2160 pixels) at 60 frames per second. To achieve this, a special labelling
method was designed and a functionality that stops the input data stream in
order to process pixel groups which require writing more than one merger into
the equivalence table. The proposed module was verified in simulation and in
hardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the ZCU104 evaluation
board.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kowalczyk_M/0/1/0/all/0/1"&gt;Marcin Kowalczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1"&gt;Tomasz Kryjak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09624</id>
        <link href="http://arxiv.org/abs/2105.09624"/>
        <updated>2021-05-23T06:08:17.485Z</updated>
        <summary type="html"><![CDATA[Photoacoustic imaging has the potential to revolutionise healthcare due to
the valuable information on tissue physiology that is contained in
multispectral photoacoustic measurements. Clinical translation of the
technology requires conversion of the high-dimensional acquired data into
clinically relevant and interpretable information. In this work, we present a
deep learning-based approach to semantic segmentation of multispectral
photoacoustic images to facilitate the interpretability of recorded images.
Manually annotated multispectral photoacoustic imaging data are used as gold
standard reference annotations and enable the training of a deep learning-based
segmentation algorithm in a supervised manner. Based on a validation study with
experimentally acquired data of healthy human volunteers, we show that
automatic tissue segmentation can be used to create powerful analyses and
visualisations of multispectral photoacoustic images. Due to the intuitive
representation of high-dimensional information, such a processing algorithm
could be a valuable means to facilitate the clinical translation of
photoacoustic imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Grohl_J/0/1/0/all/0/1"&gt;Janek Gr&amp;#xf6;hl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1"&gt;Melanie Schellenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dreher_K/0/1/0/all/0/1"&gt;Kris Dreher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Holzwarth_N/0/1/0/all/0/1"&gt;Niklas Holzwarth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu D. Tizabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seitel_A/0/1/0/all/0/1"&gt;Alexander Seitel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised learning of text line segmentationby differentiating coarse patterns. (arXiv:2105.09405v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09405</id>
        <link href="http://arxiv.org/abs/2105.09405"/>
        <updated>2021-05-23T06:08:17.479Z</updated>
        <summary type="html"><![CDATA[Despite recent advances in the field of supervised deep learning for text
line segmentation, unsupervised deep learning solutions are beginning to gain
popularity. In this paper, we present an unsupervised deep learning method that
embeds document image patches to a compact Euclidean space where distances
correspond to a coarse text line pattern similarity. Once this space has been
produced, text line segmentation can be easily implemented using standard
techniques with the embedded feature vectors. To train the model, we extract
random pairs of document image patches with the assumption that neighbour
patches contain a similar coarse trend of text lines, whereas if one of them is
rotated, they contain different coarse trends of text lines. Doing well on this
task requires the model to learn to recognize the text lines and their salient
parts. The benefit of our approach is zero manual labelling effort. We evaluate
the method qualitatively and quantitatively on several variants of text line
segmentation datasets to demonstrate its effectivity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barakat_B/0/1/0/all/0/1"&gt;Berat Kurar Barakat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droby_A/0/1/0/all/0/1"&gt;Ahmad Droby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saabni_R/0/1/0/all/0/1"&gt;Raid Saabni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sana_J/0/1/0/all/0/1"&gt;Jihad El-Sana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intra-Model Collaborative Learning of Neural Networks. (arXiv:2105.09590v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09590</id>
        <link href="http://arxiv.org/abs/2105.09590"/>
        <updated>2021-05-23T06:08:17.472Z</updated>
        <summary type="html"><![CDATA[Recently, collaborative learning proposed by Song and Chai has achieved
remarkable improvements in image classification tasks by simultaneously
training multiple classifier heads. However, huge memory footprints required by
such multi-head structures may hinder the training of large-capacity baseline
models. The natural question is how to achieve collaborative learning within a
single network without duplicating any modules. In this paper, we propose four
ways of collaborative learning among different parts of a single network with
negligible engineering efforts. To improve the robustness of the network, we
leverage the consistency of the output layer and intermediate layers for
training under the collaborative learning framework. Besides, the similarity of
intermediate representation and convolution kernel is also introduced to reduce
the reduce redundant in a neural network. Compared to the method of Song and
Chai, our framework further considers the collaboration inside a single model
and takes smaller overhead. Extensive experiments on Cifar-10, Cifar-100,
ImageNet32 and STL-10 corroborate the effectiveness of these four ways
separately while combining them leads to further improvements. In particular,
test errors on the STL-10 dataset are decreased by $9.28\%$ and $5.45\%$ for
ResNet-18 and VGG-16 respectively. Moreover, our method is proven to be robust
to label noise with experiments on Cifar-10 dataset. For example, our method
has $3.53\%$ higher performance under $50\%$ noise ratio setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Shijie Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tong Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Few-Shot Object Detection without Forgetting. (arXiv:2105.09491v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09491</id>
        <link href="http://arxiv.org/abs/2105.09491"/>
        <updated>2021-05-23T06:08:17.466Z</updated>
        <summary type="html"><![CDATA[Recently few-shot object detection is widely adopted to deal with
data-limited situations. While most previous works merely focus on the
performance on few-shot categories, we claim that detecting all classes is
crucial as test samples may contain any instances in realistic applications,
which requires the few-shot detector to learn new concepts without forgetting.
Through analysis on transfer learning based methods, some neglected but
beneficial properties are utilized to design a simple yet effective few-shot
detector, Retentive R-CNN. It consists of Bias-Balanced RPN to debias the
pretrained RPN and Re-detector to find few-shot class objects without
forgetting previous knowledge. Extensive experiments on few-shot detection
benchmarks show that Retentive R-CNN significantly outperforms state-of-the-art
methods on overall performance among all settings as it can achieve competitive
results on few-shot classes and does not degrade the base class performance at
all. Our approach has demonstrated that the long desired never-forgetting
learner is available in object detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zhibo Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yuchen Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zeming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VTNet: Visual Transformer Network for Object Goal Navigation. (arXiv:2105.09447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09447</id>
        <link href="http://arxiv.org/abs/2105.09447"/>
        <updated>2021-05-23T06:08:17.458Z</updated>
        <summary type="html"><![CDATA[Object goal navigation aims to steer an agent towards a target object based
on observations of the agent. It is of pivotal importance to design effective
visual representations of the observed scene in determining navigation actions.
In this paper, we introduce a Visual Transformer Network (VTNet) for learning
informative visual representation in navigation. VTNet is a highly effective
structure that embodies two key properties for visual representations: First,
the relationships among all the object instances in a scene are exploited;
Second, the spatial locations of objects and image regions are emphasized so
that directional navigation signals can be learned. Furthermore, we also
develop a pre-training scheme to associate the visual representations with
navigation signals, and thus facilitate navigation policy learning. In a
nutshell, VTNet embeds object and region features with their location cues as
spatial-aware descriptors and then incorporates all the encoded descriptors
through attention operations to achieve informative representation for
navigation. Given such visual representations, agents are able to explore the
correlations between visual observations and navigation actions. For example,
an agent would prioritize "turning right" over "turning left" when the visual
representation emphasizes on the right side of activation map. Experiments in
the artificial environment AI2-Thor demonstrate that VTNet significantly
outperforms state-of-the-art methods in unseen testing environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1"&gt;Heming Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crowd Counting by Self-supervised Transfer Colorization Learning and Global Prior Classification. (arXiv:2105.09684v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09684</id>
        <link href="http://arxiv.org/abs/2105.09684"/>
        <updated>2021-05-23T06:08:17.452Z</updated>
        <summary type="html"><![CDATA[Labeled crowd scene images are expensive and scarce. To significantly reduce
the requirement of the labeled images, we propose ColorCount, a novel CNN-based
approach by combining self-supervised transfer colorization learning and global
prior classification to leverage the abundantly available unlabeled data. The
self-supervised colorization branch learns the semantics and surface texture of
the image by using its color components as pseudo labels. The classification
branch extracts global group priors by learning correlations among image
clusters. Their fused resultant discriminative features (global priors,
semantics and textures) provide ample priors for counting, hence significantly
reducing the requirement of labeled images. We conduct extensive experiments on
four challenging benchmarks. ColorCount achieves much better performance as
compared with other unsupervised approaches. Its performance is close to the
supervised baseline with substantially less labeled data (10\% of the original
one).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoyue Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1"&gt;Song Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1"&gt;S.-H. Gary Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying concepts via visual properties. (arXiv:2105.09422v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.09422</id>
        <link href="http://arxiv.org/abs/2105.09422"/>
        <updated>2021-05-23T06:08:17.426Z</updated>
        <summary type="html"><![CDATA[We assume that substances in the world are represented by two types of
concepts, namely substance concepts and classification concepts, the former
instrumental to (visual) perception, the latter to (language based)
classification. Based on this distinction, we introduce a general methodology
for building lexico-semantic hierarchies of substance concepts, where nodes are
annotated with the media, e.g.,videos or photos, from which substance concepts
are extracted, and are associated with the corresponding classification
concepts. The methodology is based on Ranganathan's original faceted approach,
contextualized to the problem of classifying substance concepts. The key
novelty is that the hierarchy is built exploiting the visual properties of
substance concepts, while the linguistically defined properties of
classification concepts are only used to describe substance concepts. The
validity of the approach is exemplified by providing some highlights of an
ongoing project whose goal is to build a large scale multimedia multilingual
concept hierarchy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1"&gt;Fausto Giunchiglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1"&gt;Mayukh Bagchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction. (arXiv:2105.09711v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09711</id>
        <link href="http://arxiv.org/abs/2105.09711"/>
        <updated>2021-05-23T06:08:17.418Z</updated>
        <summary type="html"><![CDATA[Joint relation modeling is a curial component in human motion prediction.
Most existing methods tend to design skeletal-based graphs to build the
relations among joints, where local interactions between joint pairs are well
learned. However, the global coordination of all joints, which reflects human
motion's balance property, is usually weakened because it is learned from part
to whole progressively and asynchronously. Thus, the final predicted motions
are sometimes unnatural. To tackle this issue, we learn a medium, called
balance attractor (BA), from the spatiotemporal features of motion to
characterize the global motion features, which is subsequently used to build
new joint relations. Through the BA, all joints are related synchronously, and
thus the global coordination of all joints can be better learned. Based on the
BA, we propose our framework, referred to Attractor-Guided Neural Network,
mainly including Attractor-Based Joint Relation Extractor (AJRE) and
Multi-timescale Dynamics Extractor (MTDE). The AJRE mainly includes Global
Coordination Extractor (GCE) and Local Interaction Extractor (LIE). The former
presents the global coordination of all joints, and the latter encodes local
interactions between joint pairs. The MTDE is designed to extract dynamic
information from raw position information for effective prediction. Extensive
experiments show that the proposed framework outperforms state-of-the-art
methods in both short and long-term predictions in H3.6M, CMU-Mocap, and 3DPW.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1"&gt;Pengxiang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianqin Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anabranch Network for Camouflaged Object Segmentation. (arXiv:2105.09451v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09451</id>
        <link href="http://arxiv.org/abs/2105.09451"/>
        <updated>2021-05-23T06:08:17.411Z</updated>
        <summary type="html"><![CDATA[Camouflaged objects attempt to conceal their texture into the background and
discriminating them from the background is hard even for human beings. The main
objective of this paper is to explore the camouflaged object segmentation
problem, namely, segmenting the camouflaged object(s) for a given image. This
problem has not been well studied in spite of a wide range of potential
applications including the preservation of wild animals and the discovery of
new species, surveillance systems, search-and-rescue missions in the event of
natural disasters such as earthquakes, floods or hurricanes. This paper
addresses a new challenging problem of camouflaged object segmentation. To
address this problem, we provide a new image dataset of camouflaged objects for
benchmarking purposes. In addition, we propose a general end-to-end network,
called the Anabranch Network, that leverages both classification and
segmentation tasks. Different from existing networks for segmentation, our
proposed network possesses the second branch for classification to predict the
probability of containing camouflaged object(s) in an image, which is then
fused into the main branch for segmentation to boost up the segmentation
accuracy. Extensive experiments conducted on the newly built dataset
demonstrate the effectiveness of our network using various fully convolutional
networks. \url{https://sites.google.com/view/ltnghia/research/camo}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Trung-Nghia Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tam V. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1"&gt;Zhongliang Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh-Triet Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1"&gt;Akihiro Sugimoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09492</id>
        <link href="http://arxiv.org/abs/2105.09492"/>
        <updated>2021-05-23T06:08:17.404Z</updated>
        <summary type="html"><![CDATA[Deep generative models of 3D shapes have received a great deal of research
interest. Yet, almost all of them generate discrete shape representations, such
as voxels, point clouds, and polygon meshes. We present the first 3D generative
model for a drastically different shape representation -- describing a shape as
a sequence of computer-aided design (CAD) operations. Unlike meshes and point
clouds, CAD models encode the user creation process of 3D shapes, widely used
in numerous industrial and engineering design tasks. However, the sequential
and irregular structure of CAD operations poses significant challenges for
existing 3D generative models. Drawing an analogy between CAD operations and
natural language, we propose a CAD generative network based on the Transformer.
We demonstrate the performance of our model for both shape autoencoding and
random shape generation. To train our network, we create a new CAD dataset
consisting of 179,133 models and their CAD construction sequences. We have made
this dataset publicly available to promote future research on this topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rundi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chang Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Changxi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A low-rank representation for unsupervised registration of medical images. (arXiv:2105.09548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09548</id>
        <link href="http://arxiv.org/abs/2105.09548"/>
        <updated>2021-05-23T06:08:17.397Z</updated>
        <summary type="html"><![CDATA[Registration networks have shown great application potentials in medical
image analysis. However, supervised training methods have a great demand for
large and high-quality labeled datasets, which is time-consuming and sometimes
impractical due to data sharing issues. Unsupervised image registration
algorithms commonly employ intensity-based similarity measures as loss
functions without any manual annotations. These methods estimate the
parameterized transformations between pairs of moving and fixed images through
the optimization of the network parameters during training. However, these
methods become less effective when the image quality varies, e.g., some images
are corrupted by substantial noise or artifacts. In this work, we propose a
novel approach based on a low-rank representation, i.e., Regnet-LRR, to tackle
the problem. We project noisy images into a noise-free low-rank space, and then
compute the similarity between the images. Based on the low-rank similarity
measure, we train the registration network to predict the dense deformation
fields of noisy image pairs. We highlight that the low-rank projection is
reformulated in a way that the registration network can successfully update
gradients. With two tasks, i.e., cardiac and abdominal intra-modality
registration, we demonstrate that the low-rank representation can boost the
generalization ability and robustness of models as well as bring significant
improvements in noisy data registration scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1"&gt;Dengqiang Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shangqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qunlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xinzhe Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Than Just Attention: Learning Cross-Modal Attentions with Contrastive Constraints. (arXiv:2105.09597v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09597</id>
        <link href="http://arxiv.org/abs/2105.09597"/>
        <updated>2021-05-23T06:08:17.378Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms have been widely applied to cross-modal tasks such as
image captioning and information retrieval, and have achieved remarkable
improvements due to its capability to learn fine-grained relevance across
different modalities. However, existing attention models could be sub-optimal
and lack preciseness because there is no direct supervision involved during
training. In this work, we propose Contrastive Content Re-sourcing (CCR) and
Contrastive Content Swapping (CCS) constraints to address such limitation.
These constraints supervise the training of attention models in a contrastive
learning manner without requiring explicit attention annotations. Additionally,
we introduce three metrics, namely Attention Precision, Recall and F1-Score, to
quantitatively evaluate the attention quality. We evaluate the proposed
constraints with cross-modal retrieval (image-text matching) task. The
experiments on both Flickr30k and MS-COCO datasets demonstrate that integrating
these attention constraints into two state-of-the-art attention-based models
improves the model performance in terms of both retrieval accuracy and
attention metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jianbo Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Long Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Rui Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1"&gt;Dimitris N. Metaxas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Superpixel-based Domain-Knowledge Infusion in Computer Vision. (arXiv:2105.09448v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09448</id>
        <link href="http://arxiv.org/abs/2105.09448"/>
        <updated>2021-05-23T06:08:17.371Z</updated>
        <summary type="html"><![CDATA[Superpixels are higher-order perceptual groups of pixels in an image, often
carrying much more information than raw pixels. There is an inherent relational
structure to the relationship among different superpixels of an image. This
relational information can convey some form of domain information about the
image, e.g. relationship between superpixels representing two eyes in a cat
image. Our interest in this paper is to construct computer vision models,
specifically those based on Deep Neural Networks (DNNs) to incorporate these
superpixels information. We propose a methodology to construct a hybrid model
that leverages (a) Convolutional Neural Network (CNN) to deal with spatial
information in an image, and (b) Graph Neural Network (GNN) to deal with
relational superpixel information in the image. The proposed deep model is
learned using a generic hybrid loss function that we call a `hybrid' loss. We
evaluate the predictive performance of our proposed hybrid vision model on four
popular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.
Moreover, we evaluate our method on three real-world classification tasks:
COVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint
Identification. The results demonstrate that the relational superpixel
information provided via a GNN could improve the performance of standard
CNN-based vision systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1"&gt;Gunjan Chhablani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1"&gt;Harshit Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1"&gt;Tirtharaj Dash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content-Augmented Feature Pyramid Network with Light Linear Transformers. (arXiv:2105.09464v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09464</id>
        <link href="http://arxiv.org/abs/2105.09464"/>
        <updated>2021-05-23T06:08:17.363Z</updated>
        <summary type="html"><![CDATA[Recently, plenty of work has tried to introduce transformers into computer
vision tasks, with good results. Unlike classic convolution networks, which
extract features within a local receptive field, transformers can adaptively
aggregate similar features from a global view using self-attention mechanism.
For object detection, Feature Pyramid Network (FPN) proposes feature
interaction across layers and proves its extremely importance. However, its
interaction is still in a local manner, which leaves a lot of room for
improvement. Since transformer was originally designed for NLP tasks, adapting
processing subject directly from text to image will cause unaffordable
computation and space overhead. In this paper, we utilize a linearized
attention function to overcome above problems and build a novel architecture,
named Content-Augmented Feature Pyramid Network (CA-FPN), which proposes a
global content extraction module and deeply combines with FPN through light
linear transformers. What's more, light transformers can further make the
application of multi-head attention mechanism easier. Most importantly, our
CA-FPN can be readily plugged into existing FPN-based models. Extensive
experiments on the challenging COCO object detection dataset demonstrated that
our CA-FPN significantly outperforms competitive baselines without bells and
whistles. Code will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yongxiang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xiaolin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yuncong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGSFCOS: Based on attention mechanism and Scale-Equalizing pyramid network of object detection. (arXiv:2105.09596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09596</id>
        <link href="http://arxiv.org/abs/2105.09596"/>
        <updated>2021-05-23T06:08:17.355Z</updated>
        <summary type="html"><![CDATA[Recently, the anchor-free object detection model has shown great potential
for accuracy and speed to exceed anchor-based object detection. Therefore, two
issues are mainly studied in this article: (1) How to let the backbone network
in the anchor-free object detection model learn feature extraction? (2) How to
make better use of the feature pyramid network? In order to solve the above
problems, Experiments show that our model has a certain improvement in accuracy
compared with the current popular detection models on the COCO dataset, the
designed attention mechanism module can capture contextual information well,
improve detection accuracy, and use sepc network to help balance abstract and
detailed information, and reduce the problem of semantic gap in the feature
pyramid network. Whether it is anchor-based network model YOLOv3, Faster RCNN,
or anchor-free network model Foveabox, FSAF, FCOS. Our optimal model can get
39.5% COCO AP under the background of ResNet50.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1"&gt;Wei Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_R/0/1/0/all/0/1"&gt;Ruhui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1"&gt;Kaida Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Laili Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Egocentric Activity Recognition and Localization on a 3D Map. (arXiv:2105.09544v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09544</id>
        <link href="http://arxiv.org/abs/2105.09544"/>
        <updated>2021-05-23T06:08:17.347Z</updated>
        <summary type="html"><![CDATA[Given a video captured from a first person perspective and recorded in a
familiar environment, can we recognize what the person is doing and identify
where the action occurs in the 3D space? We address this challenging problem of
jointly recognizing and localizing actions of a mobile user on a known 3D map
from egocentric videos. To this end, we propose a novel deep probabilistic
model. Our model takes the inputs of a Hierarchical Volumetric Representation
(HVR) of the environment and an egocentric video, infers the 3D action location
as a latent variable, and recognizes the action based on the video and
contextual cues surrounding its potential locations. To evaluate our model, we
conduct extensive experiments on a newly collected egocentric video dataset, in
which both human naturalistic actions and photo-realistic 3D environment
reconstructions are captured. Our method demonstrates strong results on both
action recognition and 3D action localization across seen and unseen
environments. We believe our work points to an exciting research direction in
the intersection of egocentric vision, and 3D scene understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Miao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lingni Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somasundaram_K/0/1/0/all/0/1"&gt;Kiran Somasundaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1"&gt;James M. Rehg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Unsupervised Document Image Blind Denoising. (arXiv:2105.09437v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09437</id>
        <link href="http://arxiv.org/abs/2105.09437"/>
        <updated>2021-05-23T06:08:17.320Z</updated>
        <summary type="html"><![CDATA[Removing noise from scanned pages is a vital step before their submission to
optical character recognition (OCR) system. Most available image denoising
methods are supervised where the pairs of noisy/clean pages are required.
However, this assumption is rarely met in real settings. Besides, there is no
single model that can remove various noise types from documents. Here, we
propose a unified end-to-end unsupervised deep learning model, for the first
time, that can effectively remove multiple types of noise, including salt \&
pepper noise, blurred and/or faded text, as well as watermarks from documents
at various levels of intensity. We demonstrate that the proposed model
significantly improves the quality of scanned images and the OCR of the pages
on several test datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gangeh_M/0/1/0/all/0/1"&gt;Mehrdad J Gangeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plata_M/0/1/0/all/0/1"&gt;Marcin Plata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Motahari_H/0/1/0/all/0/1"&gt;Hamid Motahari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1"&gt;Nigel P Duffy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Contrastive Learning. (arXiv:2105.09401v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09401</id>
        <link href="http://arxiv.org/abs/2105.09401"/>
        <updated>2021-05-23T06:08:17.301Z</updated>
        <summary type="html"><![CDATA[With the advent of big data across multiple high-impact applications, we are
often facing the challenge of complex heterogeneity. The newly collected data
usually consist of multiple modalities and characterized with multiple labels,
thus exhibiting the co-existence of multiple types of heterogeneity. Although
state-of-the-art techniques are good at modeling the complex heterogeneity with
sufficient label information, such label information can be quite expensive to
obtain in real applications, leading to sub-optimal performance using these
techniques. Inspired by the capability of contrastive learning to utilize rich
unlabeled data for improving performance, in this paper, we propose a unified
heterogeneous learning framework, which combines both weighted unsupervised
contrastive loss and weighted supervised contrastive loss to model multiple
types of heterogeneity. We also provide theoretical analyses showing that the
proposed weighted supervised contrastive loss is the lower bound of the mutual
information of two samples from the same class and the weighted unsupervised
contrastive loss is the lower bound of the mutual information between the
hidden representation of two views of the same sample. Experimental results on
real-world data sets demonstrate the effectiveness and the efficiency of the
proposed method modeling multiple types of heterogeneity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lecheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yada Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Birds of a Feather: Capturing Avian Shape Models from Images. (arXiv:2105.09396v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09396</id>
        <link href="http://arxiv.org/abs/2105.09396"/>
        <updated>2021-05-23T06:08:17.274Z</updated>
        <summary type="html"><![CDATA[Animals are diverse in shape, but building a deformable shape model for a new
species is not always possible due to the lack of 3D data. We present a method
to capture new species using an articulated template and images of that
species. In this work, we focus mainly on birds. Although birds represent
almost twice the number of species as mammals, no accurate shape model is
available. To capture a novel species, we first fit the articulated template to
each training sample. By disentangling pose and shape, we learn a shape space
that captures variation both among species and within each species from image
evidence. We learn models of multiple species from the CUB dataset, and
contribute new species-specific and multi-species shape models that are useful
for downstream reconstruction tasks. Using a low-dimensional embedding, we show
that our learned 3D shape space better reflects the phylogenetic relationships
among birds than learned perceptual features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yufu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolotouros_N/0/1/0/all/0/1"&gt;Nikos Kolotouros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1"&gt;Kostas Daniilidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badger_M/0/1/0/all/0/1"&gt;Marc Badger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust partial Fourier reconstruction for diffusion-weighted imaging using a recurrent convolutional neural network. (arXiv:2105.09378v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09378</id>
        <link href="http://arxiv.org/abs/2105.09378"/>
        <updated>2021-05-23T06:08:17.262Z</updated>
        <summary type="html"><![CDATA[Purpose: To develop an algorithm for robust partial Fourier (PF)
reconstruction applicable to diffusion-weighted (DW) images with non-smooth
phase variations.

Methods: Based on an unrolled proximal splitting algorithm, a neural network
architecture is derived which alternates between data consistency operations
and regularization implemented by recurrent convolutions. In order to exploit
correlations, multiple repetitions of the same slice are jointly reconstructed
under consideration of permutation-equivariance. The proposed method is trained
on DW liver data of 60 volunteers and evaluated on retrospectively and
prospectively sub-sampled data of different anatomies and resolutions. In
addition, the benefits of using a recurrent network over other unrolling
strategies is investigated.

Results: Conventional PF techniques can be significantly outperformed in
terms of quantitative measures as well as perceptual image quality. The
proposed method is able to generalize well to brain data with contrasts and
resolution not present in the training set. The reduction in echo time (TE)
associated with prospective PF-sampling enables DW imaging with higher signal.
Also, the TE increase in acquisitions with higher resolution can be compensated
for. It can be shown that unrolling by means of a recurrent network produced
better results than using a weight-shared network or a cascade of networks.

Conclusion: This work demonstrates that robust PF reconstruction of DW data
is feasible even at strong PF factors in applications with severe phase
variations. Since the proposed method does not rely on smoothness priors of the
phase but uses learned recurrent convolutions instead, artifacts of
conventional PF methods can be avoided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gadjimuradov_F/0/1/0/all/0/1"&gt;Fasil Gadjimuradov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benkert_T/0/1/0/all/0/1"&gt;Thomas Benkert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nickel_M/0/1/0/all/0/1"&gt;Marcel Dominik Nickel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Endless Loops: Detecting and Animating Periodic Patterns in Still Images. (arXiv:2105.09374v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09374</id>
        <link href="http://arxiv.org/abs/2105.09374"/>
        <updated>2021-05-23T06:08:17.231Z</updated>
        <summary type="html"><![CDATA[We present an algorithm for producing a seamless animated loop from a single
image. The algorithm detects periodic structures, such as the windows of a
building or the steps of a staircase, and generates a non-trivial displacement
vector field that maps each segment of the structure onto a neighboring segment
along a user- or auto-selected main direction of motion. This displacement
field is used, together with suitable temporal and spatial smoothing, to warp
the image and produce the frames of a continuous animation loop. Our
cinemagraphs are created in under a second on a mobile device. Over 140,000
users downloaded our app and exported over 350,000 cinemagraphs. Moreover, we
conducted two user studies that show that users prefer our method for creating
surreal and structured cinemagraphs compared to more manual approaches and
compared to previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Halperin_T/0/1/0/all/0/1"&gt;Tavi Halperin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakim_H/0/1/0/all/0/1"&gt;Hanit Hakim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vantzos_O/0/1/0/all/0/1"&gt;Orestis Vantzos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochman_G/0/1/0/all/0/1"&gt;Gershon Hochman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benaim_N/0/1/0/all/0/1"&gt;Netai Benaim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sassy_L/0/1/0/all/0/1"&gt;Lior Sassy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kupchik_M/0/1/0/all/0/1"&gt;Michael Kupchik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_O/0/1/0/all/0/1"&gt;Ofir Bibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1"&gt;Ohad Fried&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation. (arXiv:2105.09365v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09365</id>
        <link href="http://arxiv.org/abs/2105.09365"/>
        <updated>2021-05-23T06:08:17.203Z</updated>
        <summary type="html"><![CDATA[Retinal Vessel Segmentation is important for diagnosis of various diseases.
The research on retinal vessel segmentation focuses mainly on improvement of
the segmentation model which is usually based on U-Net architecture. In our
study we use the U-Net architecture and we rely on heavy data augmentation in
order to achieve better performance. The success of the data augmentation
relies on successfully addressing the problem of input images. By analyzing
input images and performing the augmentation accordingly we show that the
performance of the U-Net model can be increased dramatically. Results are
reported using the most widely used retina dataset, DRIVE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Uysal_E/0/1/0/all/0/1"&gt;Enes Sadi Uysal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilici_M/0/1/0/all/0/1"&gt;M.&amp;#x15e;afak Bilici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaza_B/0/1/0/all/0/1"&gt;B. Selin Zaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozgenc_M/0/1/0/all/0/1"&gt;M. Yi&amp;#x11f;it &amp;#xd6;zgen&amp;#xe7;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boyar_O/0/1/0/all/0/1"&gt;Onur Boyar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation. (arXiv:2105.09371v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.09371</id>
        <link href="http://arxiv.org/abs/2105.09371"/>
        <updated>2021-05-23T06:08:17.176Z</updated>
        <summary type="html"><![CDATA[While imitation learning for vision based autonomous mobile robot navigation
has recently received a great deal of attention in the research community,
existing approaches typically require state action demonstrations that were
gathered using the deployment platform. However, what if one cannot easily
outfit their platform to record these demonstration signals or worse yet the
demonstrator does not have access to the platform at all? Is imitation learning
for vision based autonomous navigation even possible in such scenarios? In this
work, we hypothesize that the answer is yes and that recent ideas from the
Imitation from Observation (IfO) literature can be brought to bear such that a
robot can learn to navigate using only ego centric video collected by a
demonstrator, even in the presence of viewpoint mismatch. To this end, we
introduce a new algorithm, Visual Observation only Imitation Learning for
Autonomous navigation (VOILA), that can successfully learn navigation policies
from a single video demonstration collected from a physically different agent.
We evaluate VOILA in the photorealistic AirSim simulator and show that VOILA
not only successfully imitates the expert, but that it also learns navigation
policies that can generalize to novel environments. Further, we demonstrate the
effectiveness of VOILA in a real world setting by showing that it allows a
wheeled Jackal robot to successfully imitate a human walking in an environment
using a video recorded using a mobile phone camera.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1"&gt;Haresh Karnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1"&gt;Garrett Warnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xuesu Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1"&gt;Peter Stone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimate The Efficiency Of Multiprocessor's Cash Memory Work Algorithms. (arXiv:2102.03848v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03848</id>
        <link href="http://arxiv.org/abs/2102.03848"/>
        <updated>2021-05-23T06:08:17.168Z</updated>
        <summary type="html"><![CDATA[Many computer systems for calculating the proper organization of memory are
among the most critical issues. Using a tier cache memory (along with branching
prediction) is an effective means of increasing modern multi-core processors'
performance. Designing high-performance processors is a complex task and
requires preliminary verification and analysis of the model level, usually used
in analytical and simulation modeling. The refinement of extreme programming is
an unfortunate challenge. Few experts disagree with the synthesis of access
points. This article demonstrates that Internet QoS and 16-bit architectures
are always incompatible, but it's the same situation for write-back caches. The
solution to this problem can be implemented by analyzing simulation models of
different complexity in combination with the analytical evaluation of
individual algorithms. This work is devoted to designing a multi-parameter
simulation model of a multi-process for evaluating the performance of cache
memory algorithms and the optimality of the structure. Optimization of the
structures and algorithms of the cache memory allows you to accelerate the
interaction of the memory process and improve the performance of the entire
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamada_M/0/1/0/all/0/1"&gt;Mohamed A. Hamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1"&gt;Abdelrahman Abdallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09356</id>
        <link href="http://arxiv.org/abs/2105.09356"/>
        <updated>2021-05-23T06:08:17.161Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Seyed Saeed Changiz Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fred X. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1"&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Keith Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1"&gt;Shuo Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Graph-Based Behavior-Aware Recommendation for Interactive News. (arXiv:1812.00002v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.00002</id>
        <link href="http://arxiv.org/abs/1812.00002"/>
        <updated>2021-05-23T06:08:17.152Z</updated>
        <summary type="html"><![CDATA[Interactive news recommendation has been launched and attracted much
attention recently. In this scenario, user's behavior evolves from single click
behavior to multiple behaviors including like, comment, share etc. However,
most of the existing methods still use single click behavior as the unique
criterion of judging user's preferences. Further, although heterogeneous graphs
have been applied in different areas, a proper way to construct a heterogeneous
graph for interactive news data with an appropriate learning mechanism on it is
still desired. To address the above concerns, we propose a graph-based
behavior-aware network, which simultaneously considers six different types of
behaviors as well as user's demand on the news diversity. We have three main
steps. First, we build an interaction behavior graph for multi-level and
multi-category data. Second, we apply DeepWalk on the behavior graph to obtain
entity semantics, then build a graph-based convolutional neural network called
G-CNN to learn news representations, and an attention-based LSTM to learn
behavior sequence representations. Third, we introduce core and coritivity
features for the behavior graph, which measure the concentration degree of
user's interests. These features affect the trade-off between accuracy and
diversity of our personalized recommendation system. Taking these features into
account, our system finally achieves recommending news to different users at
their different levels of concentration degrees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Mingyuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1"&gt;Sen Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Congzhou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning. (arXiv:2105.09710v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09710</id>
        <link href="http://arxiv.org/abs/2105.09710"/>
        <updated>2021-05-23T06:08:17.142Z</updated>
        <summary type="html"><![CDATA[Conversational recommender systems (CRS) enable the traditional recommender
systems to explicitly acquire user preferences towards items and attributes
through interactive conversations. Reinforcement learning (RL) is widely
adopted to learn conversational recommendation policies to decide what
attributes to ask, which items to recommend, and when to ask or recommend, at
each conversation turn. However, existing methods mainly target at solving one
or two of these three decision-making problems in CRS with separated
conversation and recommendation components, which restrict the scalability and
generality of CRS and fall short of preserving a stable training procedure. In
the light of these challenges, we propose to formulate these three
decision-making problems in CRS as a unified policy learning task. In order to
systematically integrate conversation and recommendation components, we develop
a dynamic weighted graph based RL method to learn a policy to select the action
at each conversation turn, either asking an attribute or recommending items.
Further, to deal with the sample efficiency issue, we propose two action
selection strategies for reducing the candidate action space according to the
preference and entropy information. Experimental results on two benchmark CRS
datasets and a real-world E-Commerce application show that the proposed method
not only significantly outperforms state-of-the-art methods but also enhances
the scalability and stability of CRS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Fei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1"&gt;Bolin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Query Formulation using Query By Navigation. (arXiv:2105.09562v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09562</id>
        <link href="http://arxiv.org/abs/2105.09562"/>
        <updated>2021-05-23T06:08:17.118Z</updated>
        <summary type="html"><![CDATA[Effective information disclosure in the context of databases with a large
conceptual schema is known to be a non-trivial problem. In particular the
formulation of ad-hoc queries is a major problem in such contexts. Existing
approaches for tackling this problem include graphical query interfaces, query
by navigation, query by construction, and point to point queries. In this
report we propose an adoption of the query by navigation mechanism that is
especially geared towards the InfoAssistant product. Query by navigation is
based on ideas from the information retrieval world, in particular on the
stratified hypermedia architecture. When using our approach to the formulations
of queries, a user will first formulate a number of simple queries
corresponding to linear paths through the information structure. The
formulation of the linear paths is the result of the {\em explorative phase} of
the query formulation. Once users have specified a number of these linear
paths, they may combine them to form more complex queries. Examples of such
combinations are: concatenation, union, intersection and selection. This last
process is referred to as {\em query by construction}, and is the {\em
constructive phase} of the query formulation process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Proper_H/0/1/0/all/0/1"&gt;H. A. Proper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution Agnostic Symbolic Representations for Time Series Dimensionality Reduction and Online Anomaly Detection. (arXiv:2105.09592v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09592</id>
        <link href="http://arxiv.org/abs/2105.09592"/>
        <updated>2021-05-23T06:08:17.108Z</updated>
        <summary type="html"><![CDATA[Due to the importance of the lower bounding distances and the attractiveness
of symbolic representations, the family of symbolic aggregate approximations
(SAX) has been used extensively for encoding time series data. However, typical
SAX-based methods rely on two restrictive assumptions; the Gaussian
distribution and equiprobable symbols. This paper proposes two novel
data-driven SAX-based symbolic representations, distinguished by their
discretization steps. The first representation, oriented for general data
compaction and indexing scenarios, is based on the combination of kernel
density estimation and Lloyd-Max quantization to minimize the information loss
and mean squared error in the discretization step. The second method, oriented
for high-level mining tasks, employs the Mean-Shift clustering method and is
shown to enhance anomaly detection in the lower-dimensional space. Besides, we
verify on a theoretical basis a previously observed phenomenon of the intrinsic
process that results in a lower than the expected variance of the intermediate
piecewise aggregate approximation. This phenomenon causes an additional
information loss but can be avoided with a simple modification. The proposed
representations possess all the attractive properties of the conventional SAX
method. Furthermore, experimental evaluation on real-world datasets
demonstrates their superiority compared to the traditional SAX and an
alternative data-driven SAX variant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bountrogiannis_K/0/1/0/all/0/1"&gt;Konstantinos Bountrogiannis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzagkarakis_G/0/1/0/all/0/1"&gt;George Tzagkarakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsakalides_P/0/1/0/all/0/1"&gt;Panagiotis Tsakalides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Personalized Fairness based on Causal Notion. (arXiv:2105.09829v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09829</id>
        <link href="http://arxiv.org/abs/2105.09829"/>
        <updated>2021-05-23T06:08:17.079Z</updated>
        <summary type="html"><![CDATA[Recommender systems are gaining increasing and critical impacts on human and
society since a growing number of users use them for information seeking and
decision making. Therefore, it is crucial to address the potential unfairness
problems in recommendations. Just like users have personalized preferences on
items, users' demands for fairness are also personalized in many scenarios.
Therefore, it is important to provide personalized fair recommendations for
users to satisfy their personalized fairness demands. Besides, previous works
on fair recommendation mainly focus on association-based fairness. However, it
is important to advance from associative fairness notions to causal fairness
notions for assessing fairness more properly in recommender systems. Based on
the above considerations, this paper focuses on achieving personalized
counterfactual fairness for users in recommender systems. To this end, we
introduce a framework for achieving counterfactually fair recommendations
through adversary learning by generating feature-independent user embeddings
for recommendation. The framework allows recommender systems to achieve
personalized fairness for users while also covering non-personalized
situations. Experiments on two real-world datasets with shallow and deep
recommendation algorithms show that our method can generate fairer
recommendations for users with a desirable recommendation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanxiong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter. (arXiv:2105.07148v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07148</id>
        <link href="http://arxiv.org/abs/2105.07148"/>
        <updated>2021-05-23T06:08:17.061Z</updated>
        <summary type="html"><![CDATA[Lexicon information and pre-trained models, such as BERT, have been combined
to explore Chinese sequence labelling tasks due to their respective strengths.
However, existing methods solely fuse lexicon features via a shallow and random
initialized sequence layer and do not integrate them into the bottom layers of
BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese
sequence labelling, which integrates external lexicon knowledge into BERT
layers directly by a Lexicon Adapter layer. Compared with the existing methods,
our model facilitates deep lexicon knowledge fusion at the lower layers of
BERT. Experiments on ten Chinese datasets of three tasks including Named Entity
Recognition, Word Segmentation, and Part-of-Speech tagging, show that LEBERT
achieves the state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiyan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wenming Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FreshDiskANN: A Fast and Accurate Graph-Based ANN Index for Streaming Similarity Search. (arXiv:2105.09613v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09613</id>
        <link href="http://arxiv.org/abs/2105.09613"/>
        <updated>2021-05-23T06:08:17.044Z</updated>
        <summary type="html"><![CDATA[Approximate nearest neighbor search (ANNS) is a fundamental building block in
information retrieval with graph-based indices being the current
state-of-the-art and widely used in the industry. Recent advances in
graph-based indices have made it possible to index and search billion-point
datasets with high recall and millisecond-level latency on a single commodity
machine with an SSD.

However, existing graph algorithms for ANNS support only static indices that
cannot reflect real-time changes to the corpus required by many key real-world
scenarios (e.g. index of sentences in documents, email, or a news index). To
overcome this drawback, the current industry practice for manifesting updates
into such indices is to periodically re-build these indices, which can be
prohibitively expensive.

In this paper, we present the first graph-based ANNS index that reflects
corpus updates into the index in real-time without compromising on search
performance. Using update rules for this index, we design FreshDiskANN, a
system that can index over a billion points on a workstation with an SSD and
limited memory, and support thousands of concurrent real-time inserts, deletes
and searches per second each, while retaining $>95\%$ 5-recall@5. This
represents a 5-10x reduction in the cost of maintaining freshness in indices
when compared to existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Aditi Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanya_S/0/1/0/all/0/1"&gt;Suhas Jayaram Subramanya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_R/0/1/0/all/0/1"&gt;Ravishankar Krishnaswamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simhadri_H/0/1/0/all/0/1"&gt;Harsha Vardhan Simhadri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Offensive Language Identification for Low-resource Languages. (arXiv:2105.05996v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05996</id>
        <link href="http://arxiv.org/abs/2105.05996"/>
        <updated>2021-05-23T06:08:17.026Z</updated>
        <summary type="html"><![CDATA[Offensive content is pervasive in social media and a reason for concern to
companies and government organizations. Several studies have been recently
published investigating methods to detect the various forms of such content
(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of
these studies deal with English partially because most annotated datasets
available contain English data. In this paper, we take advantage of available
English datasets by applying cross-lingual contextual word embeddings and
transfer learning to make predictions in low-resource languages. We project
predictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,
Spanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in
TRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in
OffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513
F1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our
approach compares favourably to the best systems submitted to recent shared
tasks on these three languages. Additionally, we report competitive performance
on Arabic, and Turkish using the training and development sets of OffensEval
2020 shared task. The results for all languages confirm the robustness of
cross-lingual contextual embeddings and transfer learning for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1"&gt;Tharindu Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1"&gt;Marcos Zampieri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking. (arXiv:2105.09816v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09816</id>
        <link href="http://arxiv.org/abs/2105.09816"/>
        <updated>2021-05-23T06:08:17.014Z</updated>
        <summary type="html"><![CDATA[An emerging recipe for achieving state-of-the-art effectiveness in neural
document re-ranking involves utilizing large pre-trained language models -
e.g., BERT - to evaluate all individual passages in the document and then
aggregating the outputs by pooling or additional Transformer layers. A major
drawback of this approach is high query latency due to the cost of evaluating
every passage in the document with BERT. To make matters worse, this high
inference cost and latency varies based on the length of the document, with
longer documents requiring more time and computation. To address this
challenge, we adopt an intra-document cascading strategy, which prunes passages
of a candidate document using a less expensive model, called ESM, before
running a scoring model that is more expensive and effective, called ETM. We
found it best to train ESM (short for Efficient Student Model) via knowledge
distillation from the ETM (short for Effective Teacher Model) e.g., BERT. This
pruning allows us to only run the ETM model on a smaller set of passages whose
size does not vary by document length. Our experiments on the MS MARCO and TREC
Deep Learning Track benchmarks suggest that the proposed Intra-Document
Cascaded Ranking Model (IDCM) leads to over 400% lower query latency by
providing essentially the same effectiveness as the state-of-the-art BERT-based
document ranking models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1"&gt;Sebastian Hofst&amp;#xe4;tter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1"&gt;Bhaskar Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1"&gt;Hamed Zamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1"&gt;Nick Craswell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1"&gt;Allan Hanbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic and Variational Recommendation Denoising. (arXiv:2105.09605v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09605</id>
        <link href="http://arxiv.org/abs/2105.09605"/>
        <updated>2021-05-23T06:08:16.990Z</updated>
        <summary type="html"><![CDATA[Learning from implicit feedback is one of the most common cases in the
application of recommender systems. Generally speaking, interacted examples are
considered as positive while negative examples are sampled from uninteracted
ones. However, noisy examples are prevalent in real-world implicit feedback. A
noisy positive example could be interacted but it actually leads to negative
user preference. A noisy negative example which is uninteracted because of
unawareness of the user could also denote potential positive user preference.
Conventional training methods overlook these noisy examples, leading to
sub-optimal recommendation. In this work, we propose probabilistic and
variational recommendation denoising for implicit feedback. Through an
empirical study, we find that different models make relatively similar
predictions on clean examples which denote the real user preference, while the
predictions on noisy examples vary much more across different models. Motivated
by this observation, we propose denoising with probabilistic inference (DPI)
which aims to minimize the KL-divergence between the real user preference
distributions parameterized by two recommendation models while maximize the
likelihood of data observation. We then show that DPI recovers the evidence
lower bound of an variational auto-encoder when the real user preference is
considered as the latent variables. This leads to our second learning framework
denoising with variational autoencoder (DVAE). We employ the proposed DPI and
DVAE on four state-of-the-art recommendation models and conduct experiments on
three datasets. Experimental results demonstrate that DPI and DVAE
significantly improve recommendation performance compared with normal training
and other denoising methods. Codes will be open-sourced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zaiqiao Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1"&gt;Joemon Jose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction. (arXiv:2105.06965v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06965</id>
        <link href="http://arxiv.org/abs/2105.06965"/>
        <updated>2021-05-23T06:08:16.967Z</updated>
        <summary type="html"><![CDATA[When language models process syntactically complex sentences, do they use
abstract syntactic information present in these sentences in a manner that is
consistent with the grammar of English, or do they rely solely on a set of
heuristics? We propose a method to tackle this question, AlterRep. For any
linguistic feature in the sentence, AlterRep allows us to generate
counterfactual representations by altering how this feature is encoded, while
leaving all other aspects of the original representation intact. Then, by
measuring the change in a models' word prediction with these counterfactual
representations in different sentences, we can draw causal conclusions about
the contexts in which the model uses the linguistic feature (if any). Applying
this method to study how BERT uses relative clause (RC) span information, we
found that BERT uses information about RC spans during agreement prediction
using the linguistically correct strategy. We also found that counterfactual
representations generated for a specific RC subtype influenced the number
prediction in sentences with other RC subtypes, suggesting that information
about RC boundaries was encoded abstractly in BERT's representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1"&gt;Shauli Ravfogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1"&gt;Grusha Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1"&gt;Tal Linzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoding Explanatory Knowledge for Zero-shot Science Question Answering. (arXiv:2105.05737v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05737</id>
        <link href="http://arxiv.org/abs/2105.05737"/>
        <updated>2021-05-23T06:08:16.959Z</updated>
        <summary type="html"><![CDATA[This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge
Transfer), a novel method for the automatic transfer of explanatory knowledge
through neural encoding mechanisms. We demonstrate that N-XKT is able to
improve accuracy and generalization on science Question Answering (QA).
Specifically, by leveraging facts from background explanatory knowledge
corpora, the N-XKT model shows a clear improvement on zero-shot QA.
Furthermore, we show that N-XKT can be fine-tuned on a target QA dataset,
enabling faster convergence and more accurate results. A systematic analysis is
conducted to quantitatively analyze the performance of the N-XKT model and the
impact of different categories of knowledge on the zero-shot generalization
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zili Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1"&gt;Marco Valentino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1"&gt;Donal Landers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1"&gt;Andre Freitas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03842</id>
        <link href="http://arxiv.org/abs/2105.03842"/>
        <updated>2021-05-23T06:08:16.945Z</updated>
        <summary type="html"><![CDATA[Error correction techniques have been used to refine the output sentences
from automatic speech recognition (ASR) models and achieve a lower word error
rate (WER) than original ASR outputs. Previous works usually use a
sequence-to-sequence model to correct an ASR output sentence autoregressively,
which causes large latency and cannot be deployed in online ASR services. A
straightforward solution to reduce latency, inspired by non-autoregressive
(NAR) neural machine translation, is to use an NAR sequence generation model
for ASR error correction, which, however, comes at the cost of significantly
increased ASR error rate. In this paper, observing distinctive error patterns
and correction operations (i.e., insertion, deletion, and substitution) in ASR,
we propose FastCorrect, a novel NAR error correction model based on edit
alignment. In training, FastCorrect aligns each source token from an ASR output
sentence to the target tokens from the corresponding ground-truth sentence
based on the edit distance between the source and target sentences, and
extracts the number of target tokens corresponding to each source token during
edition/correction, which is then used to train a length predictor and to
adjust the source tokens to match the length of the target sentence for
parallel generation. In inference, the token number predicted by the length
predictor is used to adjust the source tokens for target sequence generation.
Experiments on the public AISHELL-1 dataset and an internal industrial-scale
ASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)
it speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER
reduction) compared with the autoregressive correction model; and 2) it
outperforms the accuracy of popular NAR models adopted in neural machine
translation by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1"&gt;Yichong Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Linchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Renqian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Linquan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang-Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1"&gt;Ed Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders. (arXiv:2105.03505v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03505</id>
        <link href="http://arxiv.org/abs/2105.03505"/>
        <updated>2021-05-23T06:08:16.936Z</updated>
        <summary type="html"><![CDATA[Learning prerequisite chains is an essential task for efficiently acquiring
knowledge in both known and unknown domains. For example, one may be an expert
in the natural language processing (NLP) domain but want to determine the best
order to learn new concepts in an unfamiliar Computer Vision domain (CV). Both
domains share some common concepts, such as machine learning basics and deep
learning models. In this paper, we propose unsupervised cross-domain concept
prerequisite chain learning using an optimized variational graph autoencoder.
Our model learns to transfer concept prerequisite relations from an
information-rich domain (source domain) to an information-poor domain (target
domain), substantially surpassing other baseline models. Also, we expand an
existing dataset by introducing two new domains: CV and Bioinformatics (BIO).
The annotated data and resources, as well as the code, will be made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1"&gt;Irene Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1"&gt;Vanessa Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_R/0/1/0/all/0/1"&gt;Rihao Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1"&gt;Dragomir Radev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering. (arXiv:2012.00955v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00955</id>
        <link href="http://arxiv.org/abs/2012.00955"/>
        <updated>2021-05-23T06:08:16.916Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that language models (LM) capture different types of
knowledge regarding facts or common sense. However, because no model is
perfect, they still fail to provide appropriate answers in many cases. In this
paper, we ask the question "how can we know when language models know, with
confidence, the answer to a particular query?" We examine this question from
the point of view of calibration, the property of a probabilistic model's
predicted probabilities actually being well correlated with the probabilities
of correctness. We examine three strong generative models -- T5, BART, and
GPT-2 -- and study whether their probabilities on QA tasks are well calibrated,
finding the answer is a relatively emphatic no. We then examine methods to
calibrate such models to make their confidence scores correlate better with the
likelihood of correctness through fine-tuning, post-hoc probability
modification, or adjustment of the predicted outputs or inputs. Experiments on
a diverse range of datasets demonstrate the effectiveness of our methods. We
also perform analysis to study the strengths and limitations of these methods,
shedding light on further improvements that may be made in methods for
calibrating LMs. We have released the code at
https://github.com/jzbjyb/lm-calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengbao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araki_J/0/1/0/all/0/1"&gt;Jun Araki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Haibo Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning. (arXiv:2012.15409v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15409</id>
        <link href="http://arxiv.org/abs/2012.15409"/>
        <updated>2021-05-23T06:08:16.907Z</updated>
        <summary type="html"><![CDATA[Existed pre-training methods either focus on single-modal tasks or
multi-modal tasks, and cannot effectively adapt to each other. They can only
utilize single-modal data (i.e. text or image) or limited multi-modal data
(i.e. image-text pairs). In this work, we propose a unified-modal pre-training
architecture, namely UNIMO, which can effectively adapt to both single-modal
and multi-modal understanding and generation tasks. Large scale of free text
corpus and image collections can be utilized to improve the capability of
visual and textual understanding, and cross-modal contrastive learning (CMCL)
is leveraged to align the textual and visual information into a unified
semantic space over a corpus of image-text pairs. As the non-paired
single-modal data is very rich, our model can utilize much larger scale of data
to learn more generalizable representations. Moreover, the textual knowledge
and visual knowledge can enhance each other in the unified semantic space. The
experimental results show that UNIMO significantly improves the performance of
several single-modal and multi-modal downstream tasks. Our code and pre-trained
models are public at
https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Can Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guocheng Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xinyan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiachen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Subverting the Jewtocracy": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05947</id>
        <link href="http://arxiv.org/abs/2104.05947"/>
        <updated>2021-05-23T06:08:16.899Z</updated>
        <summary type="html"><![CDATA[The exponential rise of online social media has enabled the creation,
distribution, and consumption of information at an unprecedented rate. However,
it has also led to the burgeoning of various forms of online abuse. Increasing
cases of online antisemitism have become one of the major concerns because of
its socio-political consequences. Unlike other major forms of online abuse like
racism, sexism, etc., online antisemitism has not been studied much from a
machine learning perspective. To the best of our knowledge, we present the
first work in the direction of automated multimodal detection of online
antisemitism. The task poses multiple challenges that include extracting
signals across multiple modalities, contextual references, and handling
multiple aspects of antisemitism. Unfortunately, there does not exist any
publicly available benchmark corpus for this critical task. Hence, we collect
and label two datasets with 3,102 and 3,509 social media posts from Twitter and
Gab respectively. Further, we present a multimodal deep learning system that
detects the presence of antisemitic content and its specific antisemitism
category using text and images from posts. We perform an extensive set of
experiments on the two datasets to evaluate the efficacy of the proposed
system. Finally, we also present a qualitative analysis of our study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1"&gt;Mohit Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1"&gt;Dheeraj Pailla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1"&gt;Himanshu Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1"&gt;Aadilmehdi Sanchawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"&gt;Manish Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1"&gt;Ponnurangam Kumaraguru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Infinite use of finite means: Zero-Shot Generalization using Compositional Emergent Protocols. (arXiv:2012.05011v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05011</id>
        <link href="http://arxiv.org/abs/2012.05011"/>
        <updated>2021-05-23T06:08:16.830Z</updated>
        <summary type="html"><![CDATA[Human language has been described as a system that makes \textit{use of
finite means to express an unlimited array of thoughts}. Of particular interest
is the aspect of compositionality, whereby, the meaning of a compound language
expression can be deduced from the meaning of its constituent parts. If
artificial agents can develop compositional communication protocols akin to
human language, they can be made to seamlessly generalize to unseen
combinations. However, the real question is, how do we induce compositionality
in emergent communication? Studies have recognized the role of curiosity in
enabling linguistic development in children. It is this same intrinsic urge
that drives us to master complex tasks with decreasing amounts of explicit
reward. In this paper, we seek to use this intrinsic feedback in inducing a
systematic and unambiguous protolanguage in artificial agents. We show how
these rewards can be leveraged in training agents to induce compositionality in
absence of any external feedback. Additionally, we introduce gComm, an
environment for investigating grounded language acquisition in 2D-grid
environments. Using this, we demonstrate how compositionality can enable agents
to not only interact with unseen objects but also transfer skills from one task
to another in a zero-shot setting: \textit{Can an agent, trained to `pull' and
`push twice', `pull twice'?}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1"&gt;Rishi Hazra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixit_S/0/1/0/all/0/1"&gt;Sonu Dixit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1"&gt;Sayambhu Sen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting News Article Structure for Automatic Corpus Generation. (arXiv:2010.11574v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11574</id>
        <link href="http://arxiv.org/abs/2010.11574"/>
        <updated>2021-05-23T06:08:16.822Z</updated>
        <summary type="html"><![CDATA[Transformers represent the state-of-the-art in Natural Language Processing
(NLP) in recent years, proving effective even in tasks done in low-resource
languages. While pretrained transformers for these languages can be made, it is
challenging to measure their true performance and capacity due to the lack of
hard benchmark datasets, as well as the difficulty and cost of producing them.
In this paper, we present three contributions: First, we propose a methodology
for automatically producing Natural Language Inference (NLI) benchmark datasets
for low-resource languages using published news articles. Through this, we
create and release NewsPH-NLI, the first sentence entailment benchmark dataset
in the low-resource Filipino language. Second, we produce new pretrained
transformers based on the ELECTRA technique to further alleviate the resource
scarcity in Filipino, benchmarking them on our dataset against other
commonly-used transfer learning techniques. Lastly, we perform analyses on
transfer learning techniques to shed light on their true performance when
operating in low-data domains through the use of degradation tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1"&gt;Jan Christian Blaise Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resabal_J/0/1/0/all/0/1"&gt;Jose Kristian Resabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;James Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velasco_D/0/1/0/all/0/1"&gt;Dan John Velasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Charibeth Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Coding Challenge Competence With APPS. (arXiv:2105.09938v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2105.09938</id>
        <link href="http://arxiv.org/abs/2105.09938"/>
        <updated>2021-05-23T06:08:16.792Z</updated>
        <summary type="html"><![CDATA[While programming is one of the most broadly applicable skills in modern
society, modern machine learning models still cannot code solutions to basic
problems. It can be difficult to accurately assess code generation performance,
and there has been surprisingly little work on evaluating code generation in a
way that is both flexible and rigorous. To meet this challenge, we introduce
APPS, a benchmark for code generation. Unlike prior work in more restricted
settings, our benchmark measures the ability of models to take an arbitrary
natural language specification and generate Python code fulfilling this
specification. Similar to how companies assess candidate software developers,
we then evaluate models by checking their generated code on test cases. Our
benchmark includes 10,000 problems, which range from having simple one-line
solutions to being substantial algorithmic challenges. We fine-tune large
language models on both GitHub and our training set, and we find that the
prevalence of syntax errors is decreasing exponentially. Recent models such as
GPT-Neo can pass approximately 15% of the test cases of introductory problems,
so we find that machine learning models are beginning to learn how to code. As
the social significance of automatic code generation increases over the coming
years, our benchmark can provide an important measure for tracking
advancements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1"&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1"&gt;Steven Basart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1"&gt;Saurav Kadavath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1"&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1"&gt;Akul Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1"&gt;Ethan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1"&gt;Collin Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puranik_S/0/1/0/all/0/1"&gt;Samir Puranik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Horace He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Head Attention: Collaborate Instead of Concatenate. (arXiv:2006.16362v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16362</id>
        <link href="http://arxiv.org/abs/2006.16362"/>
        <updated>2021-05-23T06:08:16.782Z</updated>
        <summary type="html"><![CDATA[Attention layers are widely used in natural language processing (NLP) and are
beginning to influence computer vision architectures. Training very large
transformer models allowed significant improvement in both fields, but once
trained, these networks show symptoms of over-parameterization. For instance,
it is known that many attention heads can be pruned without impacting accuracy.
This work aims to enhance current understanding on how multiple heads interact.
Motivated by the observation that attention heads learn redundant key/query
projections, we propose a collaborative multi-head attention layer that enables
heads to learn shared projections. Our scheme decreases the number of
parameters in an attention layer and can be used as a drop-in replacement in
any transformer architecture. Our experiments confirm that sharing key/query
dimensions can be exploited in language understanding, machine translation and
vision. We also show that it is possible to re-parametrize a pre-trained
multi-head attention layer into our collaborative attention layer.
Collaborative multi-head attention reduces the size of the key and query
projections by 4 for same accuracy and speed. Our code is public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cordonnier_J/0/1/0/all/0/1"&gt;Jean-Baptiste Cordonnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1"&gt;Andreas Loukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus. (arXiv:2010.10391v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10391</id>
        <link href="http://arxiv.org/abs/2010.10391"/>
        <updated>2021-05-23T06:08:16.766Z</updated>
        <summary type="html"><![CDATA[Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have
achieved state-of-the-art results in biomedical natural language processing
tasks by focusing their pre-training process on domain-specific corpora.
However, such models do not take into consideration expert domain knowledge.

In this work, we introduced UmlsBERT, a contextual embedding model that
integrates domain knowledge during the pre-training process via a novel
knowledge augmentation strategy. More specifically, the augmentation on
UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was
performed in two ways: i) connecting words that have the same underlying
`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to
create clinically meaningful input embeddings. By applying these two
strategies, UmlsBERT can encode clinical domain knowledge into word embeddings
and outperform existing domain-specific models on common named-entity
recognition (NER) and clinical natural language inference clinical NLP tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1"&gt;George Michalopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuanxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaka_H/0/1/0/all/0/1"&gt;Hussam Kaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Helen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Cross-Dataset Generalization in Automatic Detection of Online Abuse. (arXiv:2010.07414v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07414</id>
        <link href="http://arxiv.org/abs/2010.07414"/>
        <updated>2021-05-23T06:08:16.758Z</updated>
        <summary type="html"><![CDATA[NLP research has attained high performances in abusive language detection as
a supervised classification task. While in research settings, training and test
datasets are usually obtained from similar data samples, in practice systems
are often applied on data that are different from the training set in topic and
class distributions. Also, the ambiguity in class definitions inherited in this
task aggravates the discrepancies between source and target datasets. We
explore the topic bias and the task formulation bias in cross-dataset
generalization. We show that the benign examples in the Wikipedia Detox dataset
are biased towards platform-specific topics. We identify these examples using
unsupervised topic modeling and manual inspection of topics' keywords. Removing
these topics increases cross-dataset generalization, without reducing in-domain
classification performance. For a robust dataset design, we suggest applying
inexpensive unsupervised methods to inspect the collected data and downsize the
non-generalizable content before manually annotating for class labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1"&gt;Isar Nejadgholi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1"&gt;Svetlana Kiritchenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BRUMS at SemEval-2020 Task 3: Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity. (arXiv:2010.06269v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06269</id>
        <link href="http://arxiv.org/abs/2010.06269"/>
        <updated>2021-05-23T06:08:16.745Z</updated>
        <summary type="html"><![CDATA[This paper presents the team BRUMS submission to SemEval-2020 Task 3: Graded
Word Similarity in Context. The system utilises state-of-the-art contextualised
word embeddings, which have some task-specific adaptations, including stacked
embeddings and average embeddings. Overall, the approach achieves good
evaluation scores across all the languages, while maintaining simplicity.
Following the final rankings, our approach is ranked within the top 5 solutions
of each language while preserving the 1st position of Finnish subtask 2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1"&gt;Hansi Hettiarachchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1"&gt;Tharindu Ranasinghe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries. (arXiv:2105.09930v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09930</id>
        <link href="http://arxiv.org/abs/2105.09930"/>
        <updated>2021-05-23T06:08:16.721Z</updated>
        <summary type="html"><![CDATA[As more and more online search queries come from voice, automatic speech
recognition becomes a key component to deliver relevant search results. Errors
introduced by automatic speech recognition (ASR) lead to irrelevant search
results returned to the user, thus causing user dissatisfaction. In this paper,
we introduce an approach, Mondegreen, to correct voice queries in text space
without depending on audio signals, which may not always be available due to
system constraints or privacy or bandwidth (for example, some ASR systems run
on-device) considerations. We focus on voice queries transcribed via several
proprietary commercial ASR systems. These queries come from users making
internet, or online service search queries. We first present an analysis
showing how different the language distribution coming from user voice queries
is from that in traditional text corpora used to train off-the-shelf ASR
systems. We then demonstrate that Mondegreen can achieve significant
improvements in increased user interaction by correcting user voice queries in
one of the largest search systems in Google. Finally, we see Mondegreen as
complementing existing highly-optimized production ASR systems, which may not
be frequently retrained and thus lag behind due to vocabulary drifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1"&gt;Sukhdeep S. Sodhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chio_E/0/1/0/all/0/1"&gt;Ellie Ka-In Chio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1"&gt;Ambarish Jash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1"&gt;Santiago Onta&amp;#xf1;&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apte_A/0/1/0/all/0/1"&gt;Ajit Apte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ankit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeje_A/0/1/0/all/0/1"&gt;Ayooluwakunmi Jeje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1"&gt;Dima Kuzmin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_H/0/1/0/all/0/1"&gt;Harry Fung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Tze Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Effrat_J/0/1/0/all/0/1"&gt;Jon Effrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bali_T/0/1/0/all/0/1"&gt;Tarush Bali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1"&gt;Nitin Jindal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1"&gt;Pei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sarvjeet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Senqiang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1"&gt;Tameen Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wankhede_A/0/1/0/all/0/1"&gt;Amol Wankhede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alzantot_M/0/1/0/all/0/1"&gt;Moustafa Alzantot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Allen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_T/0/1/0/all/0/1"&gt;Tushar Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simplifying Paragraph-level Question Generation via Transformer Language Models. (arXiv:2005.01107v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01107</id>
        <link href="http://arxiv.org/abs/2005.01107"/>
        <updated>2021-05-23T06:08:16.708Z</updated>
        <summary type="html"><![CDATA[Question generation (QG) is a natural language generation task where a model
is trained to ask questions corresponding to some input text. Most recent
approaches frame QG as a sequence-to-sequence problem and rely on additional
features and mechanisms to increase performance; however, these often increase
model complexity, and can rely on auxiliary data unavailable in practical use.
A single Transformer-based unidirectional language model leveraging transfer
learning can be used to produce high quality questions while disposing of
additional task-specific complexity. Our QG model, finetuned from GPT-2 Small,
outperforms several paragraph-level QG baselines on the SQuAD dataset by 0.95
METEOR points. Human evaluators rated questions as easy to answer, relevant to
their context paragraph, and corresponding well to natural human speech. Also
introduced is a new set of baseline scores on the RACE dataset, which has not
previously been used for QG tasks. Further experimentation with varying model
capacities and datasets with non-identification type questions is recommended
in order to further verify the robustness of pretrained Transformer-based LMs
as question generators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1"&gt;Luis Enrico Lopez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_D/0/1/0/all/0/1"&gt;Diane Kathryn Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1"&gt;Jan Christian Blaise Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Charibeth Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A practical introduction to the Rational Speech Act modeling framework. (arXiv:2105.09867v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09867</id>
        <link href="http://arxiv.org/abs/2105.09867"/>
        <updated>2021-05-23T06:08:16.699Z</updated>
        <summary type="html"><![CDATA[Recent advances in computational cognitive science (i.e., simulation-based
probabilistic programs) have paved the way for significant progress in formal,
implementable models of pragmatics. Rather than describing a pragmatic
reasoning process in prose, these models formalize and implement one, deriving
both qualitative and quantitative predictions of human behavior -- predictions
that consistently prove correct, demonstrating the viability and value of the
framework. The current paper provides a practical introduction to and critical
assessment of the Bayesian Rational Speech Act modeling framework, unpacking
theoretical foundations, exploring technological innovations, and drawing
connections to issues beyond current applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scontras_G/0/1/0/all/0/1"&gt;Gregory Scontras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1"&gt;Michael Henry Tessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1"&gt;Michael Franke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09858</id>
        <link href="http://arxiv.org/abs/2105.09858"/>
        <updated>2021-05-23T06:08:16.671Z</updated>
        <summary type="html"><![CDATA[This paper presents a low-latency real-time (LLRT) non-parallel voice
conversion (VC) framework based on cyclic variational autoencoder (CycleVAE)
and multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a
robust non-parallel multispeaker spectral model, which utilizes a
speaker-independent latent space and a speaker-dependent code to generate
reconstructed/converted spectral features given the spectral features of an
input speaker. On the other hand, MWDLP is an efficient and a high-quality
neural vocoder that can handle multispeaker data and generate speech waveform
for LLRT applications with CPU. To accommodate LLRT constraint with CPU, we
propose a novel CycleVAE framework that utilizes mel-spectrogram as spectral
features and is built with a sparse network architecture. Further, to improve
the modeling performance, we also propose a novel fine-tuning procedure that
refines the frame-rate CycleVAE network by utilizing the waveform loss from the
MWDLP network. The experimental results demonstrate that the proposed framework
achieves high-performance VC, while allowing for LLRT usage with a single-core
of $2.1$--$2.7$~GHz CPU on a real-time factor of $0.87$--$0.95$, including
input/output, feature extraction, on a frame shift of $10$ ms, a window length
of $27.5$ ms, and $2$ lookup frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1"&gt;Patrick Lumban Tobing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09856</id>
        <link href="http://arxiv.org/abs/2105.09856"/>
        <updated>2021-05-23T06:08:16.662Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel high-fidelity and low-latency universal neural
vocoder framework based on multiband WaveRNN with data-driven linear prediction
for discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN
architecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit
with a relatively large size of hidden units is utilized, while the multiband
modeling is deployed to achieve real-time low-latency usage. A novel technique
for data-driven linear prediction (LP) with discrete waveform modeling is
proposed, where the LP coefficients are estimated in a data-driven manner.
Moreover, a novel loss function using short-time Fourier transform (STFT) for
discrete waveform modeling with Gumbel approximation is also proposed. The
experimental results demonstrate that the proposed MWDLP framework generates
high-fidelity synthetic speech for seen and unseen speakers and/or language on
300 speakers training data including clean and noisy/reverberant conditions,
where the number of training utterances is limited to 60 per speaker, while
allowing for real-time low-latency processing using a single core of $\sim\!$
2.1--2.7~GHz CPU with $\sim\!$ 0.57--0.64 real-time factor including
input/output and feature extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1"&gt;Patrick Lumban Tobing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking. (arXiv:2105.09816v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09816</id>
        <link href="http://arxiv.org/abs/2105.09816"/>
        <updated>2021-05-23T06:08:16.636Z</updated>
        <summary type="html"><![CDATA[An emerging recipe for achieving state-of-the-art effectiveness in neural
document re-ranking involves utilizing large pre-trained language models -
e.g., BERT - to evaluate all individual passages in the document and then
aggregating the outputs by pooling or additional Transformer layers. A major
drawback of this approach is high query latency due to the cost of evaluating
every passage in the document with BERT. To make matters worse, this high
inference cost and latency varies based on the length of the document, with
longer documents requiring more time and computation. To address this
challenge, we adopt an intra-document cascading strategy, which prunes passages
of a candidate document using a less expensive model, called ESM, before
running a scoring model that is more expensive and effective, called ETM. We
found it best to train ESM (short for Efficient Student Model) via knowledge
distillation from the ETM (short for Effective Teacher Model) e.g., BERT. This
pruning allows us to only run the ETM model on a smaller set of passages whose
size does not vary by document length. Our experiments on the MS MARCO and TREC
Deep Learning Track benchmarks suggest that the proposed Intra-Document
Cascaded Ranking Model (IDCM) leads to over 400% lower query latency by
providing essentially the same effectiveness as the state-of-the-art BERT-based
document ranking models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1"&gt;Sebastian Hofst&amp;#xe4;tter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1"&gt;Bhaskar Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1"&gt;Hamed Zamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1"&gt;Nick Craswell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1"&gt;Allan Hanbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Head-driven Phrase Structure Parsing in O($n^3$) Time Complexity. (arXiv:2105.09835v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09835</id>
        <link href="http://arxiv.org/abs/2105.09835"/>
        <updated>2021-05-23T06:08:16.627Z</updated>
        <summary type="html"><![CDATA[Constituent and dependency parsing, the two classic forms of syntactic
parsing, have been found to benefit from joint training and decoding under a
uniform formalism, Head-driven Phrase Structure Grammar (HPSG). However,
decoding this unified grammar has a higher time complexity ($O(n^5)$) than
decoding either form individually ($O(n^3)$) since more factors have to be
considered during decoding. We thus propose an improved head scorer that helps
achieve a novel performance-preserved parser in $O$($n^3$) time complexity.
Furthermore, on the basis of this proposed practical HPSG parser, we
investigated the strengths of HPSG-based parsing and explored the general
method of training an HPSG-based parser from only a constituent or dependency
annotations in a multilingual scenario. We thus present a more effective, more
in-depth, and general work on HPSG parsing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Junru Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parnow_K/0/1/0/all/0/1"&gt;Kevin Parnow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KLUE: Korean Language Understanding Evaluation. (arXiv:2105.09680v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09680</id>
        <link href="http://arxiv.org/abs/2105.09680"/>
        <updated>2021-05-23T06:08:16.503Z</updated>
        <summary type="html"><![CDATA[We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE
is a collection of 8 Korean natural language understanding (NLU) tasks,
including Topic Classification, Semantic Textual Similarity, Natural Language
Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing,
Machine Reading Comprehension, and Dialogue State Tracking. We build all of the
tasks from scratch from diverse source corpora while respecting copyrights, to
ensure accessibility for anyone without any restrictions. With ethical
considerations in mind, we carefully design annotation protocols. Along with
the benchmark tasks and data, we provide suitable evaluation metrics and
fine-tuning recipes for pretrained language models for each task. We
furthermore release the pretrained language models (PLM), KLUE-BERT and
KLUE-RoBERTa, to help reproduce baseline models on KLUE and thereby facilitate
future research. We make a few interesting observations from the preliminary
experiments using the proposed KLUE benchmark suite, already demonstrating the
usefulness of this new benchmark suite. First, we find KLUE-RoBERTa-large
outperforms other baselines, including multilingual PLMs and existing
open-source Korean PLMs. Second, we see minimal degradation in performance even
when we replace personally identifiable information from the pretraining
corpus, suggesting that privacy and NLU capability are not at odds with each
other. Lastly, we find that using BPE tokenization in combination with
morpheme-level pre-tokenization is effective in tasks involving morpheme-level
tagging, detection and generation. In addition to accelerating Korean NLP
research, our comprehensive documentation on creating KLUE will facilitate
creating similar resources for other languages in the future. KLUE is available
at this https URL (https://klue-benchmark.com/).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungjoon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1"&gt;Jihyung Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungdong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1"&gt;Won Ik Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiyoon Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jangwon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chisung Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junseong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yongsook Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1"&gt;Taehwan Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joohong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Juhyun Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Sungwon Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1"&gt;Younghoon Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1"&gt;Inkwon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Sangwoo Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dongjun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Myeonghwa Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1"&gt;Seongbo Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_S/0/1/0/all/0/1"&gt;Seungwon Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sunkyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kyungtae Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jongwon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyumin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jamin Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seonghyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1"&gt;Lucy Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1"&gt;Alice Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1"&gt;Jungwoo Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho Alice Oh Jungwoo Ha Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction. (arXiv:2105.09543v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09543</id>
        <link href="http://arxiv.org/abs/2105.09543"/>
        <updated>2021-05-23T06:08:16.493Z</updated>
        <summary type="html"><![CDATA[Distantly supervised (DS) relation extraction (RE) has attracted much
attention in the past few years as it can utilize large-scale auto-labeled
data. However, its evaluation has long been a problem: previous works either
took costly and inconsistent methods to manually examine a small sample of
model predictions, or directly test models on auto-labeled data -- which, by
our check, produce as much as 53% wrong labels at the entity pair level in the
popular NYT10 dataset. This problem has not only led to inaccurate evaluation,
but also made it hard to understand where we are and what's left to improve in
the research of DS-RE. To evaluate DS-RE models in a more credible way, we
build manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,
and thoroughly evaluate several competitive models, especially the latest
pre-trained ones. The experimental results show that the manual evaluation can
indicate very different conclusions from automatic ones, especially some
unexpected observations, e.g., pre-trained models can achieve dominating
performance while being more susceptible to false-positives compared to
previous methods. We hope that both our manual test sets and novel observations
can help advance future DS-RE research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tianyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1"&gt;Keyue Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yuzhuo Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhiyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yankai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Target-dependent Sentiment Classification in News Articles. (arXiv:2105.09660v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09660</id>
        <link href="http://arxiv.org/abs/2105.09660"/>
        <updated>2021-05-23T06:08:16.482Z</updated>
        <summary type="html"><![CDATA[Extensive research on target-dependent sentiment classification (TSC) has led
to strong classification performances in domains where authors tend to
explicitly express sentiment about specific entities or topics, such as in
reviews or on social media. We investigate TSC in news articles, a much less
researched domain, despite the importance of news as an essential information
source in individual and societal decision making. This article introduces
NewsTSC, a manually annotated dataset to explore TSC on news articles.
Investigating characteristics of sentiment in news and contrasting them to
popular TSC domains, we find that sentiment in the news is expressed less
explicitly, is more dependent on context and readership, and requires a greater
degree of interpretation. In an extensive evaluation, we find that the state of
the art in TSC performs worse on news articles than on other domains (average
recall AvgRec = 69.8 on NewsTSC compared to AvgRev = [75.6, 82.2] on
established TSC datasets). Reasons include incorrectly resolved relation of
target and sentiment-bearing phrases and off-context dependence. As a major
improvement over previous news TSC, we find that BERT's natural language
understanding capabilities capture the less explicit sentiment used in news
articles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1"&gt;Felix Hamborg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1"&gt;Karsten Donnay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1"&gt;Bela Gipp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Case Study on Pros and Cons of Regular Expression Detection and Dependency Parsing for Negation Extraction from German Medical Documents. Technical Report. (arXiv:2105.09702v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09702</id>
        <link href="http://arxiv.org/abs/2105.09702"/>
        <updated>2021-05-23T06:08:16.455Z</updated>
        <summary type="html"><![CDATA[We describe our work on information extraction in medical documents written
in German, especially detecting negations using an architecture based on the
UIMA pipeline. Based on our previous work on software modules to cover medical
concepts like diagnoses, examinations, etc. we employ a version of the NegEx
regular expression algorithm with a large set of triggers as a baseline. We
show how a significantly smaller trigger set is sufficient to achieve similar
results, in order to reduce adaptation times to new text types. We elaborate on
the question whether dependency parsing (based on the Stanford CoreNLP model)
is a good alternative and describe the potentials and shortcomings of both
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Profitlich_H/0/1/0/all/0/1"&gt;Hans-J&amp;#xfc;rgen Profitlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1"&gt;Daniel Sonntag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness of end-to-end Automatic Speech Recognition Models -- A Case Study using Mozilla DeepSpeech. (arXiv:2105.09742v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09742</id>
        <link href="http://arxiv.org/abs/2105.09742"/>
        <updated>2021-05-23T06:08:16.444Z</updated>
        <summary type="html"><![CDATA[When evaluating the performance of automatic speech recognition models,
usually word error rate within a certain dataset is used. Special care must be
taken in understanding the dataset in order to report realistic performance
numbers. We argue that many performance numbers reported probably underestimate
the expected error rate. We conduct experiments controlling for selection bias,
gender as well as overlap (between training and test data) in content, voices,
and recording conditions. We find that content overlap has the biggest impact,
but other factors like gender also play a role.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Aashish Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zesch_T/0/1/0/all/0/1"&gt;Torsten Zesch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The impact of virtual mirroring on customer satisfaction. (arXiv:2105.09571v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.09571</id>
        <link href="http://arxiv.org/abs/2105.09571"/>
        <updated>2021-05-23T06:08:16.435Z</updated>
        <summary type="html"><![CDATA[We investigate the impact of a novel method called "virtual mirroring" to
promote employee self-reflection and impact customer satisfaction. The method
is based on measuring communication patterns, through social network and
semantic analysis, and mirroring them back to the individual. Our goal is to
demonstrate that self-reflection can trigger a change in communication
behaviors, which lead to increased customer satisfaction. We illustrate and
test our approach analyzing e-mails of a large global services company by
comparing changes in customer satisfaction associated with team leaders exposed
to virtual mirroring (the experimental group). We find an increase in customer
satisfaction in the experimental group and a decrease in the control group
(team leaders not involved in the virtual mirroring process). With regard to
the individual communication indicators, we find that customer satisfaction is
higher when employees are more responsive, use a simpler language, are embedded
in less centralized communication networks, and show more stable leadership
patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1"&gt;P. Gloor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giacomelli_G/0/1/0/all/0/1"&gt;G. Giacomelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saran_T/0/1/0/all/0/1"&gt;T. Saran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grippa_F/0/1/0/all/0/1"&gt;F. Grippa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comprehensive comparative evaluation and analysis of Distributional Semantic Models. (arXiv:2105.09825v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09825</id>
        <link href="http://arxiv.org/abs/2105.09825"/>
        <updated>2021-05-23T06:08:16.424Z</updated>
        <summary type="html"><![CDATA[Distributional semantics has deeply changed in the last decades. First,
predict models stole the thunder from traditional count ones, and more recently
both of them were replaced in many NLP applications by contextualized vectors
produced by Transformer neural language models. Although an extensive body of
research has been devoted to Distributional Semantic Model (DSM) evaluation, we
still lack a thorough comparison with respect to tested models, semantic tasks,
and benchmark datasets. Moreover, previous work has mostly focused on
task-driven evaluation, instead of exploring the differences between the way
models represent the lexical semantic space. In this paper, we perform a
comprehensive evaluation of type distributional vectors, either produced by
static DSMs or obtained by averaging the contextualized vectors generated by
BERT. First of all, we investigate the performance of embeddings in several
semantic tasks, carrying out an in-depth statistical analysis to identify the
major factors influencing the behavior of DSMs. The results show that i.) the
alleged superiority of predict based models is more apparent than real, and
surely not ubiquitous and ii.) static DSMs surpass contextualized
representations in most out-of-context semantic tasks and datasets.
Furthermore, we borrow from cognitive neuroscience the methodology of
Representational Similarity Analysis (RSA) to inspect the semantic spaces
generated by distributional models. RSA reveals important differences related
to the frequency and part-of-speech of lexical items.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1"&gt;Alessandro Lenci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahlgren_M/0/1/0/all/0/1"&gt;Magnus Sahlgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeuniaux_P/0/1/0/all/0/1"&gt;Patrick Jeuniaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gyllensten_A/0/1/0/all/0/1"&gt;Amaru Cuba Gyllensten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miliani_M/0/1/0/all/0/1"&gt;Martina Miliani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09632</id>
        <link href="http://arxiv.org/abs/2105.09632"/>
        <updated>2021-05-23T06:08:16.414Z</updated>
        <summary type="html"><![CDATA[Today, we are seeing an ever-increasing number of clinical notes that contain
clinical results, images, and textual descriptions of patient's health state.
All these data can be analyzed and employed to cater novel services that can
help people and domain experts with their common healthcare tasks. However,
many technologies such as Deep Learning and tools like Word Embeddings have
started to be investigated only recently, and many challenges remain open when
it comes to healthcare domain applications. To address these challenges, we
propose the use of Deep Learning and Word Embeddings for identifying sixteen
morbidity types within textual descriptions of clinical records. For this
purpose, we have used a Deep Learning model based on Bidirectional Long-Short
Term Memory (LSTM) layers which can exploit state-of-the-art vector
representations of data such as Word Embeddings. We have employed pre-trained
Word Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained
on the target domain. Furthermore, we have compared the performances of the
deep learning approaches against the traditional tf-idf using Support Vector
Machine and Multilayer perceptron (our baselines). From the obtained results it
seems that the latter outperforms the combination of Deep Learning approaches
using any word embeddings. Our preliminary results indicate that there are
specific features that make the dataset biased in favour of traditional machine
learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1"&gt;Danilo Dessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1"&gt;Rim Helaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vivek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1"&gt;Diego Reforgiato Recupero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1"&gt;Daniele Riboni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Detecting Need for Empathetic Response in Motivational Interviewing. (arXiv:2105.09649v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09649</id>
        <link href="http://arxiv.org/abs/2105.09649"/>
        <updated>2021-05-23T06:08:16.391Z</updated>
        <summary type="html"><![CDATA[Empathetic response from the therapist is key to the success of clinical
psychotherapy, especially motivational interviewing. Previous work on
computational modelling of empathy in motivational interviewing has focused on
offline, session-level assessment of therapist empathy, where empathy captures
all efforts that the therapist makes to understand the client's perspective and
convey that understanding to the client. In this position paper, we propose a
novel task of turn-level detection of client need for empathy. Concretely, we
propose to leverage pre-trained language models and empathy-related general
conversation corpora in a unique labeller-detector framework, where the
labeller automatically annotates a motivational interviewing conversation
corpus with empathy labels to train the detector that determines the need for
therapist empathy. We also lay out our strategies of extending the detector
with additional-input and multi-task setups to improve its detection and
explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zixiu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1"&gt;Rim Helaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vivek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1"&gt;Diego Reforgiato Recupero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1"&gt;Daniele Riboni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLBiNet: A Cross-Sentence Collective Event Detection Network. (arXiv:2105.09458v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09458</id>
        <link href="http://arxiv.org/abs/2105.09458"/>
        <updated>2021-05-23T06:08:16.366Z</updated>
        <summary type="html"><![CDATA[We consider the problem of collectively detecting multiple events,
particularly in cross-sentence settings. The key to dealing with the problem is
to encode semantic information and model event inter-dependency at a
document-level. In this paper, we reformulate it as a Seq2Seq task and propose
a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level
association of events and semantic information simultaneously. Specifically, a
bidirectional decoder is firstly devised to model event inter-dependency within
a sentence when decoding the event tag vector sequence. Secondly, an
information aggregation module is employed to aggregate sentence-level semantic
and event tag information. Finally, we stack multiple bidirectional decoders
and feed cross-sentence information, forming a multi-layer bidirectional
tagging architecture to iteratively propagate information across sentences. We
show that our approach provides significant improvement in performance compared
to the current state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lou_D/0/1/0/all/0/1"&gt;Dongfang Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1"&gt;Zhilin Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LAST at SemEval-2021 Task 1: Improving Multi-Word Complexity Prediction Using Bigram Association Measures. (arXiv:2105.09653v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09653</id>
        <link href="http://arxiv.org/abs/2105.09653"/>
        <updated>2021-05-23T06:08:16.352Z</updated>
        <summary type="html"><![CDATA[This paper describes the system developed by the Laboratoire d'analyse
statistique des textes (LAST) for the Lexical Complexity Prediction shared task
at SemEval-2021. The proposed system is made up of a LightGBM model fed with
features obtained from many word frequency lists, published lexical norms and
psychometric data. For tackling the specificity of the multi-word task, it uses
bigram association measures. Despite that the only contextual feature used was
sentence length, the system achieved an honorable performance in the multi-word
task, but poorer in the single word task. The bigram association measures were
found useful, but to a limited extent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1"&gt;Yves Bestgen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09501</id>
        <link href="http://arxiv.org/abs/2105.09501"/>
        <updated>2021-05-23T06:08:16.338Z</updated>
        <summary type="html"><![CDATA[Existing multilingual machine translation approaches mainly focus on
English-centric directions, while the non-English directions still lag behind.
In this work, we aim to build a many-to-many translation system with an
emphasis on the quality of non-English language directions. Our intuition is
based on the hypothesis that a universal cross-language representation leads to
better multilingual translation performance. To this end, we propose \method, a
training method to obtain a single unified multilingual translation model.
mCOLT is empowered by two techniques: (i) a contrastive learning scheme to
close the gap among representations of different languages, and (ii) data
augmentation on both multiple parallel and monolingual data to further align
token representations. For English-centric directions, mCOLT achieves
competitive or even better performance than a strong pre-trained model mBART on
tens of WMT benchmarks. For non-English directions, mCOLT achieves an
improvement of average 10+ BLEU compared with the multilingual baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder. (arXiv:2105.09428v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09428</id>
        <link href="http://arxiv.org/abs/2105.09428"/>
        <updated>2021-05-23T06:08:16.214Z</updated>
        <summary type="html"><![CDATA[In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an
Artificial Intelligence (AI) Health Outcomes Challenge seeking solutions to
predict risk in value-based care for incorporation into CMS Innovation Center
payment and service delivery models. Recently, modern language models have
played key roles in a number of health related tasks. This paper presents, to
the best of our knowledge, the first application of these models to patient
readmission prediction. To facilitate this, we create a dataset of 1.2 million
medical history samples derived from the Limited Dataset (LDS) issued by CMS.
Moreover, we propose a comprehensive modeling solution centered on a deep
learning framework for this data. To demonstrate the framework, we train an
attention-based Transformer to learn Medicare semantics in support of
performing downstream prediction tasks thereby achieving 0.91 AUC and 0.91
recall on readmission classification. We also introduce a novel data
pre-processing pipeline and discuss pertinent deployment considerations
surrounding model explainability and bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahlou_C/0/1/0/all/0/1"&gt;Chuhong Lahlou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crayton_A/0/1/0/all/0/1"&gt;Ancil Crayton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trier_C/0/1/0/all/0/1"&gt;Caroline Trier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willett_E/0/1/0/all/0/1"&gt;Evan Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dependency Parsing with Bottom-up Hierarchical Pointer Networks. (arXiv:2105.09611v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09611</id>
        <link href="http://arxiv.org/abs/2105.09611"/>
        <updated>2021-05-23T06:08:16.202Z</updated>
        <summary type="html"><![CDATA[Dependency parsing is a crucial step towards deep language understanding and,
therefore, widely demanded by numerous Natural Language Processing
applications. In particular, left-to-right and top-down transition-based
algorithms that rely on Pointer Networks are among the most accurate approaches
for performing dependency parsing. Additionally, it has been observed for the
top-down algorithm that Pointer Networks' sequential decoding can be improved
by implementing a hierarchical variant, more adequate to model dependency
structures. Considering all this, we develop a bottom-up-oriented Hierarchical
Pointer Network for the left-to-right parser and propose two novel
transition-based alternatives: an approach that parses a sentence in
right-to-left order and a variant that does it from the outside in. We
empirically test the proposed neural architecture with the different algorithms
on a wide variety of languages, outperforming the original approach in
practically all of them and setting new state-of-the-art results on the English
and Chinese Penn Treebanks for non-contextualized and BERT-based embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1"&gt;Daniel Fern&amp;#xe1;ndez-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1"&gt;Carlos G&amp;#xf3;mez-Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computational Morphology with Neural Network Approaches. (arXiv:2105.09404v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09404</id>
        <link href="http://arxiv.org/abs/2105.09404"/>
        <updated>2021-05-23T06:08:16.168Z</updated>
        <summary type="html"><![CDATA[Neural network approaches have been applied to computational morphology with
great success, improving the performance of most tasks by a large margin and
providing new perspectives for modeling. This paper starts with a brief
introduction to computational morphology, followed by a review of recent work
on computational morphology with neural network approaches, to provide an
overview of the area. In the end, we will analyze the advantages and problems
of neural network approaches to computational morphology, and point out some
directions to be explored by future research and study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09601</id>
        <link href="http://arxiv.org/abs/2105.09601"/>
        <updated>2021-05-23T06:08:16.148Z</updated>
        <summary type="html"><![CDATA[In recent years, abstractive text summarization with multimodal inputs has
started drawing attention due to its ability to accumulate information from
different source modalities and generate a fluent textual summary. However,
existing methods use short videos as the visual modality and short summary as
the ground-truth, therefore, perform poorly on lengthy videos and long
ground-truth summary. Additionally, there exists no benchmark dataset to
generalize this task on videos of varying lengths. In this paper, we introduce
AVIATE, the first large-scale dataset for abstractive text summarization with
videos of diverse duration, compiled from presentations in well-known academic
conferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding
research papers as the reference summaries, which ensure adequate quality and
uniformity of the ground-truth. We then propose {\name}, a factorized
multi-modal Transformer based decoder-only language model, which inherently
captures the intra-modal and inter-modal dynamics within various input
modalities for the text summarization task. {\name} utilizes an increasing
number of self-attentions to capture multimodality and performs significantly
better than traditional encoder-decoder based networks. Extensive experiments
illustrate that {\name} achieves significant improvement over the baselines in
both qualitative and quantitative evaluations on the existing How2 dataset for
short videos and newly introduced AVIATE dataset for videos with diverse
duration, beating the best baseline on the two datasets by $1.39$ and $2.74$
ROUGE-L points respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1"&gt;Yash Kumar Atri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1"&gt;Shraman Pramanick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1"&gt;Vikram Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geographic Question Answering: Challenges, Uniqueness, Classification, and Future Directions. (arXiv:2105.09392v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09392</id>
        <link href="http://arxiv.org/abs/2105.09392"/>
        <updated>2021-05-23T06:08:16.136Z</updated>
        <summary type="html"><![CDATA[As an important part of Artificial Intelligence (AI), Question Answering (QA)
aims at generating answers to questions phrased in natural language. While
there has been substantial progress in open-domain question answering, QA
systems are still struggling to answer questions which involve geographic
entities or concepts and that require spatial operations. In this paper, we
discuss the problem of geographic question answering (GeoQA). We first
investigate the reasons why geographic questions are difficult to answer by
analyzing challenges of geographic questions. We discuss the uniqueness of
geographic questions compared to general QA. Then we review existing work on
GeoQA and classify them by the types of questions they can address. Based on
this survey, we provide a generic classification framework for geographic
questions. Finally, we conclude our work by pointing out unique future research
directions for GeoQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1"&gt;Gengchen Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janowicz_K/0/1/0/all/0/1"&gt;Krzysztof Janowicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1"&gt;Ling Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1"&gt;Ni Lao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection. (arXiv:2105.09509v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09509</id>
        <link href="http://arxiv.org/abs/2105.09509"/>
        <updated>2021-05-23T06:08:16.101Z</updated>
        <summary type="html"><![CDATA[Event detection (ED) aims at detecting event trigger words in sentences and
classifying them into specific event types. In real-world applications, ED
typically does not have sufficient labelled data, thus can be formulated as a
few-shot learning problem. To tackle the issue of low sample diversity in
few-shot ED, we propose a novel knowledge-based few-shot event detection method
which uses a definition-based encoder to introduce external event knowledge as
the knowledge prior of event types. Furthermore, as external knowledge
typically provides limited and imperfect coverage of event types, we introduce
an adaptive knowledge-enhanced Bayesian meta-learning method to dynamically
adjust the knowledge prior of event types. Experiments show our method
consistently and substantially outperforms a number of baselines by at least 15
absolute F1 points under the same few-shot settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Shirong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tongtong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1"&gt;Guilin Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan-Fang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1"&gt;Gholamreza Haffari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1"&gt;Sheng Bi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Dual-view Cognitive Model for Interpretable Claim Verification. (arXiv:2105.09567v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09567</id>
        <link href="http://arxiv.org/abs/2105.09567"/>
        <updated>2021-05-23T06:08:16.080Z</updated>
        <summary type="html"><![CDATA[Recent studies constructing direct interactions between the claim and each
single user response (a comment or a relevant article) to capture evidence have
shown remarkable success in interpretable claim verification. Owing to
different single responses convey different cognition of individual users
(i.e., audiences), the captured evidence belongs to the perspective of
individual cognition. However, individuals' cognition of social things is not
always able to truly reflect the objective. There may be one-sided or biased
semantics in their opinions on a claim. The captured evidence correspondingly
contains some unobjective and biased evidence fragments, deteriorating task
performance. In this paper, we propose a Dual-view model based on the views of
Collective and Individual Cognition (CICD) for interpretable claim
verification. From the view of the collective cognition, we not only capture
the word-level semantics based on individual users, but also focus on
sentence-level semantics (i.e., the overall responses) among all users and
adjust the proportion between them to generate global evidence. From the view
of individual cognition, we select the top-$k$ articles with high degree of
difference and interact with the claim to explore the local key evidence
fragments. To weaken the bias of individual cognition-view evidence, we devise
inconsistent loss to suppress the divergence between global and local evidence
for strengthening the consistent shared evidence between the both. Experiments
on three benchmark datasets confirm that CICD achieves state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lianwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yuan Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yuqian Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Ling Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhaoyin Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00641</id>
        <link href="http://arxiv.org/abs/2012.00641"/>
        <updated>2021-05-23T06:08:15.920Z</updated>
        <summary type="html"><![CDATA[The content based image retrieval aims to find the similar images from a
large scale dataset against a query image. Generally, the similarity between
the representative features of the query image and dataset images is used to
rank the images for retrieval. In early days, various hand designed feature
descriptors have been investigated based on the visual cues such as color,
texture, shape, etc. that represent the images. However, the deep learning has
emerged as a dominating alternative of hand-designed feature engineering from a
decade. It learns the features automatically from the data. This paper presents
a comprehensive survey of deep learning based developments in the past decade
for content based image retrieval. The categorization of existing
state-of-the-art methods from different perspectives is also performed for
greater understanding of the progress. The taxonomy used in this survey covers
different supervision, different networks, different descriptor type and
different retrieval type. A performance analysis is also performed using the
state-of-the-art methods. The insights are also presented for the benefit of
the researchers to observe the progress and to make the best choices. The
survey presented in this paper will help in further research progress in image
retrieval using deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1"&gt;Shiv Ram Dubey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Subverting the Jewtocracy": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05947</id>
        <link href="http://arxiv.org/abs/2104.05947"/>
        <updated>2021-05-23T06:08:15.873Z</updated>
        <summary type="html"><![CDATA[The exponential rise of online social media has enabled the creation,
distribution, and consumption of information at an unprecedented rate. However,
it has also led to the burgeoning of various forms of online abuse. Increasing
cases of online antisemitism have become one of the major concerns because of
its socio-political consequences. Unlike other major forms of online abuse like
racism, sexism, etc., online antisemitism has not been studied much from a
machine learning perspective. To the best of our knowledge, we present the
first work in the direction of automated multimodal detection of online
antisemitism. The task poses multiple challenges that include extracting
signals across multiple modalities, contextual references, and handling
multiple aspects of antisemitism. Unfortunately, there does not exist any
publicly available benchmark corpus for this critical task. Hence, we collect
and label two datasets with 3,102 and 3,509 social media posts from Twitter and
Gab respectively. Further, we present a multimodal deep learning system that
detects the presence of antisemitic content and its specific antisemitism
category using text and images from posts. We perform an extensive set of
experiments on the two datasets to evaluate the efficacy of the proposed
system. Finally, we also present a qualitative analysis of our study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1"&gt;Mohit Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1"&gt;Dheeraj Pailla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1"&gt;Himanshu Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1"&gt;Aadilmehdi Sanchawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"&gt;Manish Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1"&gt;Ponnurangam Kumaraguru&lt;/a&gt;</name>
        </author>
    </entry>
</feed>