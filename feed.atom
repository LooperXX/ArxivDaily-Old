<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-07-01T01:59:34.801Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Pretrained Transformers as Universal Computation Engines. (arXiv:2103.05247v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05247</id>
        <link href="http://arxiv.org/abs/2103.05247"/>
        <updated>2021-07-01T01:59:34.732Z</updated>
        <summary type="html"><![CDATA[We investigate the capability of a transformer pretrained on natural language
to generalize to other modalities with minimal finetuning -- in particular,
without finetuning of the self-attention and feedforward layers of the residual
blocks. We consider such a model, which we call a Frozen Pretrained Transformer
(FPT), and study finetuning it on a variety of sequence classification tasks
spanning numerical computation, vision, and protein fold prediction. In
contrast to prior works which investigate finetuning on the same modality as
the pretraining dataset, we show that pretraining on natural language can
improve performance and compute efficiency on non-language downstream tasks.
Additionally, we perform an analysis of the architecture, comparing the
performance of a random initialized transformer to a random LSTM. Combining the
two insights, we find language-pretrained transformers can obtain strong
performance on a variety of non-language tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kevin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1"&gt;Aditya Grover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear-Mapping based Variational Ensemble Kalman Filter. (arXiv:2103.06315v3 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06315</id>
        <link href="http://arxiv.org/abs/2103.06315"/>
        <updated>2021-07-01T01:59:34.726Z</updated>
        <summary type="html"><![CDATA[We propose a linear-mapping based variational Ensemble Kalman filter for
sequential Bayesian filtering problems with generic observation models.
Specifically, the proposed method is formulated as to construct a linear
mapping from the prior ensemble to the posterior one, and the linear mapping is
computed via a variational Bayesian formulation, i.e., by minimizing the
Kullback-Leibler divergence between the transformed distribution by the linear
mapping and the actual posterior. A gradient descent scheme is proposed to
solve the resulting optimization problem. With numerical examples we
demonstrate that the method has competitive performance against existing
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wen_L/0/1/0/all/0/1"&gt;Linjie Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinglai Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Utility of Gradient Compression in Distributed Training Systems. (arXiv:2103.00543v3 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00543</id>
        <link href="http://arxiv.org/abs/2103.00543"/>
        <updated>2021-07-01T01:59:34.720Z</updated>
        <summary type="html"><![CDATA[A rich body of prior work has highlighted the existence of communication
bottlenecks in synchronous data-parallel training. To alleviate these
bottlenecks, a long line of recent work proposes gradient and model compression
methods. In this work, we evaluate the efficacy of gradient compression methods
and compare their scalability with optimized implementations of synchronous
data-parallel SGD across more than 200 different setups. Surprisingly, we
observe that only in 6 cases out of more than 200, gradient compression methods
provide speedup over optimized synchronous data-parallel training in the
typical data-center setting. We conduct an extensive investigation to identify
the root causes of this phenomenon, and offer a performance model that can be
used to identify the benefits of gradient compression for a variety of system
setups. Based on our analysis, we propose a list of desirable properties that
gradient compression methods should satisfy, in order for them to provide a
meaningful end-to-end speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Saurabh Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1"&gt;Shivaram Venkataraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1"&gt;Dimitris Papailiopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gym-ANM: Reinforcement Learning Environments for Active Network Management Tasks in Electricity Distribution Systems. (arXiv:2103.07932v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07932</id>
        <link href="http://arxiv.org/abs/2103.07932"/>
        <updated>2021-07-01T01:59:34.714Z</updated>
        <summary type="html"><![CDATA[Active network management (ANM) of electricity distribution networks include
many complex stochastic sequential optimization problems. These problems need
to be solved for integrating renewable energies and distributed storage into
future electrical grids. In this work, we introduce Gym-ANM, a framework for
designing reinforcement learning (RL) environments that model ANM tasks in
electricity distribution networks. These environments provide new playgrounds
for RL research in the management of electricity networks that do not require
an extensive knowledge of the underlying dynamics of such systems. Along with
this work, we are releasing an implementation of an introductory
toy-environment, ANM6-Easy, designed to emphasize common challenges in ANM. We
also show that state-of-the-art RL algorithms can already achieve good
performance on ANM6-Easy when compared against a model predictive control (MPC)
approach. Finally, we provide guidelines to create new Gym-ANM environments
differing in terms of (a) the distribution network topology and parameters, (b)
the observation space, (c) the modelling of the stochastic processes present in
the system, and (d) a set of hyperparameters influencing the reward signal.
Gym-ANM can be downloaded at https://github.com/robinhenry/gym-anm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henry_R/0/1/0/all/0/1"&gt;Robin Henry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1"&gt;Damien Ernst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Power of Saturated Transformers: A View from Circuit Complexity. (arXiv:2106.16213v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16213</id>
        <link href="http://arxiv.org/abs/2106.16213"/>
        <updated>2021-07-01T01:59:34.708Z</updated>
        <summary type="html"><![CDATA[Transformers have become a standard architecture for many NLP problems. This
has motivated theoretically analyzing their capabilities as models of language,
in order to understand what makes them successful, and what their potential
weaknesses might be. Recent work has shown that transformers with hard
attention are quite limited in capacity, and in fact can be simulated by
constant-depth circuits. However, hard attention is a restrictive assumption,
which may complicate the relevance of these results for practical transformers.
In this work, we analyze the circuit complexity of transformers with saturated
attention: a generalization of hard attention that more closely captures the
attention patterns learnable in practical transformers. We show that saturated
transformers transcend the limitations of hard-attention transformers. With
some minor assumptions, we prove that the number of bits needed to represent a
saturated transformer memory vector is $O(\log n)$, which implies saturated
transformers can be simulated by log-depth circuits. Thus, the jump from hard
to saturated attention can be understood as increasing the transformer's
effective circuit depth by a factor of $O(\log n)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1"&gt;William Merrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1"&gt;Roy Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation. (arXiv:2103.12923v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12923</id>
        <link href="http://arxiv.org/abs/2103.12923"/>
        <updated>2021-07-01T01:59:34.692Z</updated>
        <summary type="html"><![CDATA[Policy optimization methods are popular reinforcement learning algorithms,
because their incremental and on-policy nature makes them more stable than the
value-based counterparts. However, the same properties also make them slow to
converge and sample inefficient, as the on-policy requirement precludes data
reuse and the incremental updates couple large iteration complexity into the
sample complexity. These characteristics have been observed in experiments as
well as in theory in the recent work of~\citet{agarwal2020pc}, which provides a
policy optimization method PCPG that can robustly find near optimal polices for
approximately linear Markov decision processes but suffers from an extremely
poor sample complexity compared with value-based techniques.

In this paper, we propose a new algorithm, COPOE, that overcomes the sample
complexity issue of PCPG while retaining its robustness to model
misspecification. Compared with PCPG, COPOE makes several important algorithmic
enhancements, such as enabling data reuse, and uses more refined analysis
techniques, which we expect to be more broadly applicable to designing new
reinforcement learning algorithms. The result is an improvement in sample
complexity from $\widetilde{O}(1/\epsilon^{11})$ for PCPG to
$\widetilde{O}(1/\epsilon^3)$ for PCPG, nearly bridging the gap with
value-based techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1"&gt;Andrea Zanette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Ching-An Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Alekh Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nearly-Tight and Oblivious Algorithms for Explainable Clustering. (arXiv:2106.16147v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.16147</id>
        <link href="http://arxiv.org/abs/2106.16147"/>
        <updated>2021-07-01T01:59:34.686Z</updated>
        <summary type="html"><![CDATA[We study the problem of explainable clustering in the setting first
formalized by Moshkovitz, Dasgupta, Rashtchian, and Frost (ICML 2020). A
$k$-clustering is said to be explainable if it is given by a decision tree
where each internal node splits data points with a threshold cut in a single
dimension (feature), and each of the $k$ leaves corresponds to a cluster. We
give an algorithm that outputs an explainable clustering that loses at most a
factor of $O(\log^2 k)$ compared to an optimal (not necessarily explainable)
clustering for the $k$-medians objective, and a factor of $O(k \log^2 k)$ for
the $k$-means objective. This improves over the previous best upper bounds of
$O(k)$ and $O(k^2)$, respectively, and nearly matches the previous $\Omega(\log
k)$ lower bound for $k$-medians and our new $\Omega(k)$ lower bound for
$k$-means. The algorithm is remarkably simple. In particular, given an initial
not necessarily explainable clustering in $\mathbb{R}^d$, it is oblivious to
the data points and runs in time $O(dk \log^2 k)$, independent of the number of
data points $n$. Our upper and lower bounds also generalize to objectives given
by higher $\ell_p$-norms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gamlath_B/0/1/0/all/0/1"&gt;Buddhima Gamlath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xinrui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polak_A/0/1/0/all/0/1"&gt;Adam Polak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svensson_O/0/1/0/all/0/1"&gt;Ola Svensson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MAGIC: Learning Macro-Actions for Online POMDP Planning. (arXiv:2011.03813v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03813</id>
        <link href="http://arxiv.org/abs/2011.03813"/>
        <updated>2021-07-01T01:59:34.680Z</updated>
        <summary type="html"><![CDATA[The partially observable Markov decision process (POMDP) is a principled
general framework for robot decision making under uncertainty, but POMDP
planning suffers from high computational complexity, when long-term planning is
required. While temporally-extended macro-actions help to cut down the
effective planning horizon and significantly improve computational efficiency,
how do we acquire good macro-actions? This paper proposes Macro-Action
Generator-Critic (MAGIC), which performs offline learning of macro-actions
optimized for online POMDP planning. Specifically, MAGIC learns a macro-action
generator end-to-end, using an online planner's performance as the feedback.
During online planning, the generator generates on the fly situation-aware
macro-actions conditioned on the robot's belief and the environment context. We
evaluated MAGIC on several long-horizon planning tasks both in simulation and
on a real robot. The experimental results show that the learned macro-actions
offer significant benefits in online planning performance, compared with
primitive actions and handcrafted macro-actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yiyuan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1"&gt;Panpan Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1"&gt;David Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information. (arXiv:2106.16038v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16038</id>
        <link href="http://arxiv.org/abs/2106.16038"/>
        <updated>2021-07-01T01:59:34.663Z</updated>
        <summary type="html"><![CDATA[Recent pretraining models in Chinese neglect two important aspects specific
to the Chinese language: glyph and pinyin, which carry significant syntax and
semantic information for language understanding. In this work, we propose
ChineseBERT, which incorporates both the {\it glyph} and {\it pinyin}
information of Chinese characters into language model pretraining. The glyph
embedding is obtained based on different fonts of a Chinese character, being
able to capture character semantics from the visual features, and the pinyin
embedding characterizes the pronunciation of Chinese characters, which handles
the highly prevalent heteronym phenomenon in Chinese (the same character has
different pronunciations with different meanings). Pretrained on large-scale
unlabeled Chinese corpus, the proposed ChineseBERT model yields significant
performance boost over baseline models with fewer training steps. The porpsoed
model achieves new SOTA performances on a wide range of Chinese NLP tasks,
including machine reading comprehension, natural language inference, text
classification, sentence pair matching, and competitive performances in named
entity recognition. Code and pretrained models are publicly available at
https://github.com/ShannonAI/ChineseBert.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zijun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoya Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiaofei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yuxian Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1"&gt;Xiang Ao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1"&gt;Qing He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiwei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGD Generalizes Better Than GD (And Regularization Doesn't Help). (arXiv:2102.01117v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01117</id>
        <link href="http://arxiv.org/abs/2102.01117"/>
        <updated>2021-07-01T01:59:34.658Z</updated>
        <summary type="html"><![CDATA[We give a new separation result between the generalization performance of
stochastic gradient descent (SGD) and of full-batch gradient descent (GD) in
the fundamental stochastic convex optimization model. While for SGD it is
well-known that $O(1/\epsilon^2)$ iterations suffice for obtaining a solution
with $\epsilon$ excess expected risk, we show that with the same number of
steps GD may overfit and emit a solution with $\Omega(1)$ generalization error.
Moreover, we show that in fact $\Omega(1/\epsilon^4)$ iterations are necessary
for GD to match the generalization performance of SGD, which is also tight due
to recent work by Bassily et al. (2020). We further discuss how regularizing
the empirical risk minimized by GD essentially does not change the above
result, and revisit the concepts of stability, implicit bias and the role of
the learning algorithm in generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amir_I/0/1/0/all/0/1"&gt;Idan Amir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1"&gt;Roi Livni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges and Opportunities in High-dimensional Variational Inference. (arXiv:2103.01085v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01085</id>
        <link href="http://arxiv.org/abs/2103.01085"/>
        <updated>2021-07-01T01:59:34.652Z</updated>
        <summary type="html"><![CDATA[Current black-box variational inference (BBVI) methods require the user to
make numerous design choices -- such as the selection of variational objective
and approximating family -- yet there is little principled guidance on how to
do so. We develop a conceptual framework and set of experimental tools to
understand the effects of these choices, which we leverage to propose best
practices for maximizing posterior approximation accuracy. Our approach is
based on studying the pre-asymptotic tail behavior of the density ratios
between the joint distribution and the variational approximation, then
exploiting insights and tools from the importance sampling literature. Our
framework and supporting experiments help to distinguish between the behavior
of BBVI methods for approximating low-dimensional versus
moderate-to-high-dimensional posteriors. In the latter case, we show that
mass-covering variational objectives are difficult to optimize and do not
improve accuracy, but flexible variational families can improve accuracy and
the effectiveness of importance sampling -- at the cost of additional
optimization challenges. Therefore, for moderate-to-high-dimensional posteriors
we recommend using the (mode-seeking) exclusive KL divergence since it is the
easiest to optimize, and improving the variational family or using model
parameter transformations to make the posterior and optimal variational
approximation more similar. On the other hand, in low-dimensional settings, we
show that heavy-tailed variational families and mass-covering divergences are
effective and can increase the chances that the approximation can be improved
by importance sampling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhaka_A/0/1/0/all/0/1"&gt;Akash Kumar Dhaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catalina_A/0/1/0/all/0/1"&gt;Alejandro Catalina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welandawe_M/0/1/0/all/0/1"&gt;Manushi Welandawe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andersen_M/0/1/0/all/0/1"&gt;Michael Riis Andersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huggins_J/0/1/0/all/0/1"&gt;Jonathan Huggins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vehtari_A/0/1/0/all/0/1"&gt;Aki Vehtari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. (arXiv:2102.02888v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02888</id>
        <link href="http://arxiv.org/abs/2102.02888"/>
        <updated>2021-07-01T01:59:34.646Z</updated>
        <summary type="html"><![CDATA[Scalable training of large models (like BERT and GPT-3) requires careful
optimization rooted in model design, architecture, and system capabilities.
From a system standpoint, communication has become a major bottleneck,
especially on commodity systems with standard TCP interconnects that offer
limited network bandwidth. Communication compression is an important technique
to reduce training time on such systems. One of the most effective methods is
error-compensated compression, which offers robust convergence speed even under
1-bit compression. However, state-of-the-art error compensation techniques only
work with basic optimizers like SGD and momentum SGD, which are linearly
dependent on the gradients. They do not work with non-linear gradient-based
optimizers like Adam, which offer state-of-the-art convergence efficiency and
accuracy for models like BERT. In this paper, we propose 1-bit Adam that
reduces the communication volume by up to $5\times$, offers much better
scalability, and provides the same convergence speed as uncompressed Adam. Our
key finding is that Adam's variance (non-linear term) becomes stable (after a
warmup phase) and can be used as a fixed precondition for the rest of the
training (compression phase). Experiments on up to 256 GPUs show that 1-bit
Adam enables up to $3.3\times$ higher throughput for BERT-Large pre-training
and up to $2.9\times$ higher throughput for SQuAD fine-tuning. In addition, we
provide theoretical analysis for our proposed work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hanlin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_S/0/1/0/all/0/1"&gt;Shaoduo Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1"&gt;Ammar Ahmad Awan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1"&gt;Samyam Rajbhandari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Conglong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1"&gt;Xiangru Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuxiong He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long Short-term Cognitive Networks. (arXiv:2106.16233v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16233</id>
        <link href="http://arxiv.org/abs/2106.16233"/>
        <updated>2021-07-01T01:59:34.640Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a recurrent neural system named Long Short-term
Cognitive Networks (LSTCNs) as a generalisation of the Short-term Cognitive
Network (STCN) model. Such a generalisation is motivated by the difficulty of
forecasting very long time series in an efficient, greener fashion. The LSTCN
model can be defined as a collection of STCN blocks, each processing a specific
time patch of the (multivariate) time series being modelled. In this neural
ensemble, each block passes information to the subsequent one in the form of a
weight matrix referred to as the prior knowledge matrix. As a second
contribution, we propose a deterministic learning algorithm to compute the
learnable weights while preserving the prior knowledge resulting from previous
learning processes. As a third contribution, we introduce a feature influence
score as a proxy to explain the forecasting process in multivariate time
series. The simulations using three case studies show that our neural system
reports small forecasting errors while being up to thousands of times faster
than state-of-the-art recurrent models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1"&gt;Gonzalo N&amp;#xe1;poles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grau_I/0/1/0/all/0/1"&gt;Isel Grau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jastrzebska_A/0/1/0/all/0/1"&gt;Agnieszka Jastrzebska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salgueiro_Y/0/1/0/all/0/1"&gt;Yamisleydi Salgueiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for MU-MIMO Receive Processing in OFDM Systems. (arXiv:2012.08177v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08177</id>
        <link href="http://arxiv.org/abs/2012.08177"/>
        <updated>2021-07-01T01:59:34.634Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) starts to be widely used to enhance the performance of
multi-user multiple-input multiple-output (MU-MIMO) receivers. However, it is
still unclear if such methods are truly competitive with respect to
conventional methods in realistic scenarios and under practical constraints. In
addition to enabling accurate signal reconstruction on realistic channel
models, MU-MIMO receive algorithms must allow for easy adaptation to a varying
number of users without the need for retraining. In contrast to existing work,
we propose an ML-enhanced MU-MIMO receiver that builds on top of a conventional
linear minimum mean squared error (LMMSE) architecture. It preserves the
interpretability and scalability of the LMMSE receiver, while improving its
accuracy in two ways. First, convolutional neural networks (CNNs) are used to
compute an approximation of the second-order statistics of the channel
estimation error which are required for accurate equalization. Second, a
CNN-based demapper jointly processes a large number of orthogonal
frequency-division multiplexing (OFDM) symbols and subcarriers, which allows it
to compute better log likelihood ratios (LLRs) by compensating for channel
aging. The resulting architecture can be used in the up- and downlink and is
trained in an end-to-end manner, removing the need for hard-to-get perfect
channel state information (CSI) during the training phase. Simulation results
demonstrate consistent performance improvements over the baseline which are
especially pronounced in high mobility scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1"&gt;Mathieu Goutay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1"&gt;Fay&amp;#xe7;al Ait Aoudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1"&gt;Jakob Hoydis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1"&gt;Jean-Marie Gorce&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational VAE: A Continuous Latent Variable Model for Graph Structured Data. (arXiv:2106.16049v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2106.16049</id>
        <link href="http://arxiv.org/abs/2106.16049"/>
        <updated>2021-07-01T01:59:34.628Z</updated>
        <summary type="html"><![CDATA[Graph Networks (GNs) enable the fusion of prior knowledge and relational
reasoning with flexible function approximations. In this work, a general
GN-based model is proposed which takes full advantage of the relational
modeling capabilities of GNs and extends these to probabilistic modeling with
Variational Bayes (VB). To that end, we combine complementary pre-existing
approaches on VB for graph data and propose an approach that relies on
graph-structured latent and conditioning variables. It is demonstrated that
Neural Processes can also be viewed through the lens of the proposed model. We
show applications on the problem of structured probability density modeling for
simulated and real wind farm monitoring data, as well as on the meta-learning
of simulated Gaussian Process data. We release the source code, along with the
simulated datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mylonas_C/0/1/0/all/0/1"&gt;Charilaos Mylonas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdallah_I/0/1/0/all/0/1"&gt;Imad Abdallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzi_E/0/1/0/all/0/1"&gt;Eleni Chatzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Koopman Spectrum Nonlinear Regulator and Provably Efficient Online Learning. (arXiv:2106.15775v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15775</id>
        <link href="http://arxiv.org/abs/2106.15775"/>
        <updated>2021-07-01T01:59:34.622Z</updated>
        <summary type="html"><![CDATA[Most modern reinforcement learning algorithms optimize a cumulative
single-step cost along a trajectory. The optimized motions are often
'unnatural', representing, for example, behaviors with sudden accelerations
that waste energy and lack predictability. In this work, we present a novel
paradigm of controlling nonlinear systems via the minimization of the Koopman
spectrum cost: a cost over the Koopman operator of the controlled dynamics.
This induces a broader class of dynamical behaviors that evolve over stable
manifolds such as nonlinear oscillators, closed loops, and smooth movements. We
demonstrate that some dynamics realizations that are not possible with a
cumulative cost are feasible in this paradigm. Moreover, we present a provably
efficient online learning algorithm for our problem that enjoys a sub-linear
regret bound under some structural assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ohnishi_M/0/1/0/all/0/1"&gt;Motoya Ohnishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishikawa_I/0/1/0/all/0/1"&gt;Isao Ishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lowrey_K/0/1/0/all/0/1"&gt;Kendall Lowrey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikeda_M/0/1/0/all/0/1"&gt;Masahiro Ikeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1"&gt;Yoshinobu Kawahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ppAURORA: Privacy Preserving Area Under Receiver Operating Characteristic and Precision-Recall Curves with Secure 3-Party Computation. (arXiv:2102.08788v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08788</id>
        <link href="http://arxiv.org/abs/2102.08788"/>
        <updated>2021-07-01T01:59:34.605Z</updated>
        <summary type="html"><![CDATA[Computing an AUC as a performance measure to compare the quality of different
machine learning models is one of the final steps of many research projects.
Many of these methods are trained on privacy-sensitive data and there are
several different approaches like $\epsilon$-differential privacy, federated
machine learning and methods based on cryptographic approaches if the datasets
cannot be shared or evaluated jointly at one place. In this setting, it can
also be a problem to compute the global performance measure like an AUC, since
the labels might also contain privacy-sensitive information. There have been
approaches based on $\epsilon$-differential privacy to deal with this problem,
but to the best of our knowledge, no exact privacy preserving solution has been
introduced. In this paper, we propose an MPC-based framework, called \fw{},
with private merging of sorted lists and novel methods for comparing two
secret-shared values, selecting between two secret-shared values, converting
the modulus, and performing division to compute the exact AUC as one could
obtain on the pooled original test samples. With \fw{} computation of the exact
area under precision-recall curve and receiver operating characteristic curve
is even possible when ties between prediction confidence values exist. To show
the applicability of \fw{}, we use it to evaluate a model trained to predict
acute myeloid leukemia therapy response and we also assess its scalability via
experiments on synthetic data. The experiments show that we efficiently compute
exactly the same AUC with both evaluation metrics in a privacy preserving
manner as one can obtain on the pooled test samples in the plaintext domain.
Our solution provides security against semi-honest corruption of at most one of
the servers performing the secure computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Unal_A/0/1/0/all/0/1"&gt;Ali Burak &amp;#xdc;nal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1"&gt;Nico Pfeifer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akgun_M/0/1/0/all/0/1"&gt;Mete Akg&amp;#xfc;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Particle Filtering via Entropy-Regularized Optimal Transport. (arXiv:2102.07850v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07850</id>
        <link href="http://arxiv.org/abs/2102.07850"/>
        <updated>2021-07-01T01:59:34.588Z</updated>
        <summary type="html"><![CDATA[Particle Filtering (PF) methods are an established class of procedures for
performing inference in non-linear state-space models. Resampling is a key
ingredient of PF, necessary to obtain low variance likelihood and states
estimates. However, traditional resampling methods result in PF-based loss
functions being non-differentiable with respect to model and PF parameters. In
a variational inference context, resampling also yields high variance gradient
estimates of the PF-based evidence lower bound. By leveraging optimal transport
ideas, we introduce a principled differentiable particle filter and provide
convergence results. We demonstrate this novel method on a variety of
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Corenflos_A/0/1/0/all/0/1"&gt;Adrien Corenflos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thornton_J/0/1/0/all/0/1"&gt;James Thornton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1"&gt;George Deligiannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Reinforcement Learning Approach to the Orienteering Problem with Time Windows. (arXiv:2011.03647v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03647</id>
        <link href="http://arxiv.org/abs/2011.03647"/>
        <updated>2021-07-01T01:59:34.575Z</updated>
        <summary type="html"><![CDATA[The Orienteering Problem with Time Windows (OPTW) is a combinatorial
optimization problem where the goal is to maximize the total score collected
from different visited locations. The application of neural network models to
combinatorial optimization has recently shown promising results in dealing with
similar problems, like the Travelling Salesman Problem. A neural network allows
learning solutions using reinforcement learning or supervised learning,
depending on the available data. After the learning stage, it can be
generalized and quickly fine-tuned to further improve performance and
personalization. The advantages are evident since, for real-world applications,
solution quality, personalization, and execution times are all important
factors that should be taken into account.

This study explores the use of Pointer Network models trained using
reinforcement learning to solve the OPTW problem. We propose a modified
architecture that leverages Pointer Networks to better address problems related
with dynamic time-dependent constraints. Among its various applications, the
OPTW can be used to model the Tourist Trip Design Problem (TTDP). We train the
Pointer Network with the TTDP problem in mind, by sampling variables that can
change across tourists visiting a particular instance-region: starting
position, starting time, available time, and the scores given to each point of
interest. Once a model-region is trained, it can infer a solution for a
particular tourist using beam search. We based the assessment of our approach
on several existing benchmark OPTW instances. We show that it generalizes
across different tourists that visit each region and that it generally
outperforms the most commonly used heuristic, while computing the solution in
realistic times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gama_R/0/1/0/all/0/1"&gt;Ricardo Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_H/0/1/0/all/0/1"&gt;Hugo L. Fernandes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exponential Savings in Agnostic Active Learning through Abstention. (arXiv:2102.00451v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00451</id>
        <link href="http://arxiv.org/abs/2102.00451"/>
        <updated>2021-07-01T01:59:34.558Z</updated>
        <summary type="html"><![CDATA[We show that in pool-based active classification without assumptions on the
underlying distribution, if the learner is given the power to abstain from some
predictions by paying the price marginally smaller than the average loss $1/2$
of a random guess, exponential savings in the number of label requests are
possible whenever they are possible in the corresponding realizable problem. We
extend this result to provide a necessary and sufficient condition for
exponential savings in pool-based active classification under the model
misspecification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puchkin_N/0/1/0/all/0/1"&gt;Nikita Puchkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhivotovskiy_N/0/1/0/all/0/1"&gt;Nikita Zhivotovskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed points of monotonic and (weakly) scalable neural networks. (arXiv:2106.16239v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.16239</id>
        <link href="http://arxiv.org/abs/2106.16239"/>
        <updated>2021-07-01T01:59:34.553Z</updated>
        <summary type="html"><![CDATA[We derive conditions for the existence of fixed points of neural networks, an
important research objective to understand their behavior in modern
applications involving autoencoders and loop unrolling techniques, among
others. In particular, we focus on networks with nonnegative inputs and
nonnegative network parameters, as often considered in the literature. We show
that such networks can be recognized as monotonic and (weakly) scalable
functions within the framework of nonlinear Perron-Frobenius theory. This fact
enables us to derive conditions for the existence of a nonempty fixed point set
of the neural networks, and these conditions are weaker than those obtained
recently using arguments in convex analysis, which are typically based on the
assumption of nonexpansivity of the activation functions. Furthermore, we prove
that the shape of the fixed point set of monotonic and weakly scalable neural
networks is often an interval, which degenerates to a point for the case of
scalable networks. The chief results of this paper are verified in numerical
simulations, where we consider an autoencoder-type network that first
compresses angular power spectra in massive MIMO systems, and, second,
reconstruct the input spectra from the compressed signal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Piotrowski_T/0/1/0/all/0/1"&gt;Tomasz Piotrowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cavalcante_R/0/1/0/all/0/1"&gt;Renato L. G. Cavalcante&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Context Modeling Techniques on the Spatiotemporal Crowd Flow Prediction. (arXiv:2106.16046v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16046</id>
        <link href="http://arxiv.org/abs/2106.16046"/>
        <updated>2021-07-01T01:59:34.547Z</updated>
        <summary type="html"><![CDATA[In the big data and AI era, context is widely exploited as extra information
which makes it easier to learn a more complex pattern in machine learning
systems. However, most of the existing related studies seldom take context into
account. The difficulty lies in the unknown generalization ability of both
context and its modeling techniques across different scenarios. To fill the
above gaps, we conduct a large-scale analytical and empirical study on the
spatiotemporal crowd prediction (STCFP) problem that is a widely-studied and
hot research topic. We mainly make three efforts:(i) we develop new taxonomy
about both context features and context modeling techniques based on extensive
investigations in prevailing STCFP research; (ii) we conduct extensive
experiments on seven datasets with hundreds of millions of records to
quantitatively evaluate the generalization ability of both distinct context
features and context modeling techniques; (iii) we summarize some guidelines
for researchers to conveniently utilize context in diverse applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liyue Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Leye Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Refinement for Importance Sampling Using the Forward Kullback-Leibler Divergence. (arXiv:2106.15980v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15980</id>
        <link href="http://arxiv.org/abs/2106.15980"/>
        <updated>2021-07-01T01:59:34.541Z</updated>
        <summary type="html"><![CDATA[Variational Inference (VI) is a popular alternative to asymptotically exact
sampling in Bayesian inference. Its main workhorse is optimization over a
reverse Kullback-Leibler divergence (RKL), which typically underestimates the
tail of the posterior leading to miscalibration and potential degeneracy.
Importance sampling (IS), on the other hand, is often used to fine-tune and
de-bias the estimates of approximate Bayesian inference procedures. The quality
of IS crucially depends on the choice of the proposal distribution. Ideally,
the proposal distribution has heavier tails than the target, which is rarely
achievable by minimizing the RKL. We thus propose a novel combination of
optimization and sampling techniques for approximate Bayesian inference by
constructing an IS proposal distribution through the minimization of a forward
KL (FKL) divergence. This approach guarantees asymptotic consistency and a fast
convergence towards both the optimal IS estimator and the optimal variational
approximation. We empirically demonstrate on real data that our method is
competitive with variational boosting and MCMC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jerfel_G/0/1/0/all/0/1"&gt;Ghassen Jerfel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Serena Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fannjiang_C/0/1/0/all/0/1"&gt;Clara Fannjiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heller_K/0/1/0/all/0/1"&gt;Katherine A. Heller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoLAW: Augmented Legal Reasoning through Legal Precedent Prediction. (arXiv:2106.16034v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16034</id>
        <link href="http://arxiv.org/abs/2106.16034"/>
        <updated>2021-07-01T01:59:34.535Z</updated>
        <summary type="html"><![CDATA[This paper demonstrate how NLP can be used to address an unmet need of the
legal community and increase access to justice. The paper introduces Legal
Precedent Prediction (LPP), the task of predicting relevant passages from
precedential court decisions given the context of a legal argument. To this
end, the paper showcases a BERT model, trained on 530,000 examples of legal
arguments made by U.S. federal judges, to predict relevant passages from
precedential court decisions given the context of a legal argument. In 96% of
unseen test examples the correct target passage is among the top-10 predicted
passages. The same model is able to predict relevant precedent given a short
summary of a complex and unseen legal brief, predicting the precedent that was
actually cited by the brief's co-author, former U.S. Solicitor General and
current U.S. Supreme Court Justice Elena Kagan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahari_R/0/1/0/all/0/1"&gt;Robert Zev Mahari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interventional Assays for the Latent Space of Autoencoders. (arXiv:2106.16091v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16091</id>
        <link href="http://arxiv.org/abs/2106.16091"/>
        <updated>2021-07-01T01:59:34.491Z</updated>
        <summary type="html"><![CDATA[The encoders and decoders of autoencoders effectively project the input onto
learned manifolds in the latent space and data space respectively. We propose a
framework, called latent responses, for probing the learned data manifold using
interventions in the latent space. Using this framework, we investigate "holes"
in the representation to quantitatively ascertain to what extent the latent
space of a trained VAE is consistent with the chosen prior. Furthermore, we use
the identified structure to improve interpolation between latent vectors. We
evaluate how our analyses improve the quality of the generated samples using
the VAE on a variety of benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1"&gt;Felix Leeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1"&gt;Stefan Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Hidden Structure in Self-Supervised Learning. (arXiv:2106.16060v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16060</id>
        <link href="http://arxiv.org/abs/2106.16060"/>
        <updated>2021-07-01T01:59:34.477Z</updated>
        <summary type="html"><![CDATA[This work considers the problem of learning structured representations from
raw images using self-supervised learning. We propose a principled framework
based on a mutual information objective, which integrates self-supervised and
structure learning. Furthermore, we devise a post-hoc procedure to interpret
the meaning of the learnt representations. Preliminary experiments on CIFAR-10
show that the proposed framework achieves higher generalization performance in
downstream classification tasks and provides more interpretable representations
compared to the ones learnt through traditional self-supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1"&gt;Emanuele Sansone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymptotically Optimal Information-Directed Sampling. (arXiv:2011.05944v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05944</id>
        <link href="http://arxiv.org/abs/2011.05944"/>
        <updated>2021-07-01T01:59:34.464Z</updated>
        <summary type="html"><![CDATA[We introduce a simple and efficient algorithm for stochastic linear bandits
with finitely many actions that is asymptotically optimal and (nearly)
worst-case optimal in finite time. The approach is based on the frequentist
information-directed sampling (IDS) framework, with a surrogate for the
information gain that is informed by the optimization problem that defines the
asymptotic lower bound. Our analysis sheds light on how IDS balances the
trade-off between regret and information and uncovers a surprising connection
between the recently proposed primal-dual methods and the IDS algorithm. We
demonstrate empirically that IDS is competitive with UCB in finite-time, and
can be significantly better in the asymptotic regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kirschner_J/0/1/0/all/0/1"&gt;Johannes Kirschner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lattimore_T/0/1/0/all/0/1"&gt;Tor Lattimore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vernade_C/0/1/0/all/0/1"&gt;Claire Vernade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reservoir Based Edge Training on RF Data To Deliver Intelligent and Efficient IoT Spectrum Sensors. (arXiv:2106.16087v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.16087</id>
        <link href="http://arxiv.org/abs/2106.16087"/>
        <updated>2021-07-01T01:59:34.417Z</updated>
        <summary type="html"><![CDATA[Current radio frequency (RF) sensors at the Edge lack the computational
resources to support practical, in-situ training for intelligent spectrum
monitoring, and sensor data classification in general. We propose a solution
via Deep Delay Loop Reservoir Computing (DLR), a processing architecture that
supports general machine learning algorithms on compact mobile devices by
leveraging delay-loop reservoir computing in combination with innovative
electrooptical hardware. With both digital and photonic realizations of our
design of the loops, DLR delivers reductions in form factor, hardware
complexity and latency, compared to the State-of-the-Art (SoA). The main impact
of the reservoir is to project the input data into a higher dimensional space
of reservoir state vectors in order to linearly separate the input classes.
Once the classes are well separated, traditionally complex, power-hungry
classification models are no longer needed for the learning process. Yet, even
with simple classifiers based on Ridge regression (RR), the complexity grows at
least quadratically with the input size. Hence, the hardware reduction required
for training on compact devices is in contradiction with the large dimension of
state vectors. DLR employs a RR-based classifier to exceed the SoA accuracy,
while further reducing power consumption by leveraging the architecture of
parallel (split) loops. We present DLR architectures composed of multiple
smaller loops whose state vectors are linearly combined to create a lower
dimensional input into Ridge regression. We demonstrate the advantages of using
DLR for two distinct applications: RF Specific Emitter Identification (SEI) for
IoT authentication, and wireless protocol recognition for IoT situational
awareness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kokalj_Filipovic_S/0/1/0/all/0/1"&gt;Silvija Kokalj-Filipovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Toliver_P/0/1/0/all/0/1"&gt;Paul Toliver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Johnson_W/0/1/0/all/0/1"&gt;William Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miller_R/0/1/0/all/0/1"&gt;Rob Miller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Factual Consistency of Abstractive Summarization on Customer Feedback. (arXiv:2106.16188v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16188</id>
        <link href="http://arxiv.org/abs/2106.16188"/>
        <updated>2021-07-01T01:59:34.392Z</updated>
        <summary type="html"><![CDATA[E-commerce stores collect customer feedback to let sellers learn about
customer concerns and enhance customer order experience. Because customer
feedback often contains redundant information, a concise summary of the
feedback can be generated to help sellers better understand the issues causing
customer dissatisfaction. Previous state-of-the-art abstractive text
summarization models make two major types of factual errors when producing
summaries from customer feedback, which are wrong entity detection (WED) and
incorrect product-defect description (IPD). In this work, we introduce a set of
methods to enhance the factual consistency of abstractive summarization on
customer feedback. We augment the training data with artificially corrupted
summaries, and use them as counterparts of the target summaries. We add a
contrastive loss term into the training objective so that the model learns to
avoid certain factual errors. Evaluation results show that a large portion of
WED and IPD errors are alleviated for BART and T5. Furthermore, our approaches
do not depend on the structure of the summarization model and thus are
generalizable to any abstractive summarization systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yifei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1"&gt;Vincent Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COKE: Communication-Censored Decentralized Kernel Learning. (arXiv:2001.10133v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10133</id>
        <link href="http://arxiv.org/abs/2001.10133"/>
        <updated>2021-07-01T01:59:34.375Z</updated>
        <summary type="html"><![CDATA[This paper studies the decentralized optimization and learning problem where
multiple interconnected agents aim to learn an optimal decision function
defined over a reproducing kernel Hilbert space by jointly minimizing a global
objective function, with access to their own locally observed dataset. As a
non-parametric approach, kernel learning faces a major challenge in distributed
implementation: the decision variables of local objective functions are
data-dependent and thus cannot be optimized under the decentralized consensus
framework without any raw data exchange among agents. To circumvent this major
challenge, we leverage the random feature (RF) approximation approach to enable
consensus on the function modeled in the RF space by data-independent
parameters across different agents. We then design an iterative algorithm,
termed DKLA, for fast-convergent implementation via ADMM. Based on DKLA, we
further develop a communication-censored kernel learning (COKE) algorithm that
reduces the communication load of DKLA by preventing an agent from transmitting
at every iteration unless its local updates are deemed informative. Theoretical
results in terms of linear convergence guarantee and generalization performance
analysis of DKLA and COKE are provided. Comprehensive tests on both synthetic
and real datasets are conducted to verify the communication efficiency and
learning effectiveness of COKE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Ping Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HybridDeepRx: Deep Learning Receiver for High-EVM Signals. (arXiv:2106.16079v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.16079</id>
        <link href="http://arxiv.org/abs/2106.16079"/>
        <updated>2021-07-01T01:59:34.369Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a machine learning (ML) based physical layer
receiver solution for demodulating OFDM signals that are subject to a high
level of nonlinear distortion. Specifically, a novel deep learning based
convolutional neural network receiver is devised, containing layers in both
time- and frequency domains, allowing to demodulate and decode the transmitted
bits reliably despite the high error vector magnitude (EVM) in the transmit
signal. Extensive set of numerical results is provided, in the context of 5G NR
uplink incorporating also measured terminal power amplifier characteristics.
The obtained results show that the proposed receiver system is able to clearly
outperform classical linear receivers as well as existing ML receiver
approaches, especially when the EVM is high in comparison with modulation
order. The proposed ML receiver can thus facilitate pushing the terminal power
amplifier (PA) systems deeper into saturation, and thereon improve the terminal
power-efficiency, radiated power and network coverage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pihlajasalo_J/0/1/0/all/0/1"&gt;Jaakko Pihlajasalo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korpi_D/0/1/0/all/0/1"&gt;Dani Korpi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Honkala_M/0/1/0/all/0/1"&gt;Mikko Honkala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huttunen_J/0/1/0/all/0/1"&gt;Janne M.J. Huttunen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riihonen_T/0/1/0/all/0/1"&gt;Taneli Riihonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Talvitie_J/0/1/0/all/0/1"&gt;Jukka Talvitie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brihuega_A/0/1/0/all/0/1"&gt;Alberto Brihuega&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Uusitalo_M/0/1/0/all/0/1"&gt;Mikko A. Uusitalo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Valkama_M/0/1/0/all/0/1"&gt;Mikko Valkama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analytic Insights into Structure and Rank of Neural Network Hessian Maps. (arXiv:2106.16225v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16225</id>
        <link href="http://arxiv.org/abs/2106.16225"/>
        <updated>2021-07-01T01:59:34.364Z</updated>
        <summary type="html"><![CDATA[The Hessian of a neural network captures parameter interactions through
second-order derivatives of the loss. It is a fundamental object of study,
closely tied to various problems in deep learning, including model design,
optimization, and generalization. Most prior work has been empirical, typically
focusing on low-rank approximations and heuristics that are blind to the
network structure. In contrast, we develop theoretical tools to analyze the
range of the Hessian map, providing us with a precise understanding of its rank
deficiency as well as the structural reasons behind it. This yields exact
formulas and tight upper bounds for the Hessian rank of deep linear networks,
allowing for an elegant interpretation in terms of rank deficiency. Moreover,
we demonstrate that our bounds remain faithful as an estimate of the numerical
Hessian rank, for a larger class of models such as rectified and hyperbolic
tangent networks. Further, we also investigate the implications of model
architecture (e.g.~width, depth, bias) on the rank deficiency. Overall, our
work provides novel insights into the source and extent of redundancy in
overparameterized networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sidak Pal Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachmann_G/0/1/0/all/0/1"&gt;Gregor Bachmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified View of Stochastic Hamiltonian Sampling. (arXiv:2106.16200v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16200</id>
        <link href="http://arxiv.org/abs/2106.16200"/>
        <updated>2021-07-01T01:59:34.358Z</updated>
        <summary type="html"><![CDATA[In this work, we revisit the theoretical properties of Hamiltonian stochastic
differential equations (SDEs) for Bayesian posterior sampling, and we study the
two types of errors that arise from numerical SDE simulation: the
discretization error and the error due to noisy gradient estimates in the
context of data subsampling. We consider overlooked results describing the
ergodic convergence rates of numerical integration schemes, and we produce a
novel analysis for the effect of mini-batches through the lens of differential
operator splitting. In our analysis, the stochastic component of the proposed
Hamiltonian SDE is decoupled from the gradient noise, for which we make no
normality assumptions. This allows us to derive interesting connections among
different sampling schemes, including the original Hamiltonian Monte Carlo
(HMC) algorithm, and explain their performance. We show that for a careful
selection of numerical integrators, both errors vanish at a rate
$\mathcal{O}(\eta^2)$, where $\eta$ is the integrator step size. Our
theoretical results are supported by an empirical study on a variety of
regression and classification tasks for Bayesian neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Franzese_G/0/1/0/all/0/1"&gt;Giulio Franzese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milios_D/0/1/0/all/0/1"&gt;Dimitrios Milios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filippone_M/0/1/0/all/0/1"&gt;Maurizio Filippone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1"&gt;Pietro Michiardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tuning Mixed Input Hyperparameters on the Fly for Efficient Population Based AutoRL. (arXiv:2106.15883v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15883</id>
        <link href="http://arxiv.org/abs/2106.15883"/>
        <updated>2021-07-01T01:59:34.352Z</updated>
        <summary type="html"><![CDATA[Despite a series of recent successes in reinforcement learning (RL), many RL
algorithms remain sensitive to hyperparameters. As such, there has recently
been interest in the field of AutoRL, which seeks to automate design decisions
to create more general algorithms. Recent work suggests that population based
approaches may be effective AutoRL algorithms, by learning hyperparameter
schedules on the fly. In particular, the PB2 algorithm is able to achieve
strong performance in RL tasks by formulating online hyperparameter
optimization as time varying GP-bandit problem, while also providing
theoretical guarantees. However, PB2 is only designed to work for continuous
hyperparameters, which severely limits its utility in practice. In this paper
we introduce a new (provably) efficient hierarchical approach for optimizing
both continuous and categorical variables, using a new time-varying bandit
algorithm specifically designed for the population based training regime. We
evaluate our approach on the challenging Procgen benchmark, where we show that
explicitly modelling dependence between data augmentation and other
hyperparameters improves generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Vu Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Shaan Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Efficiency of Transformers for Resource-Constrained Devices. (arXiv:2106.16006v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16006</id>
        <link href="http://arxiv.org/abs/2106.16006"/>
        <updated>2021-07-01T01:59:34.333Z</updated>
        <summary type="html"><![CDATA[Transformers provide promising accuracy and have become popular and used in
various domains such as natural language processing and computer vision.
However, due to their massive number of model parameters, memory and
computation requirements, they are not suitable for resource-constrained
low-power devices. Even with high-performance and specialized devices, the
memory bandwidth can become a performance-limiting bottleneck. In this paper,
we present a performance analysis of state-of-the-art vision transformers on
several devices. We propose to reduce the overall memory footprint and memory
transfers by clustering the model parameters. We show that by using only 64
clusters to represent model parameters, it is possible to reduce the data
transfer from the main memory by more than 4x, achieve up to 22% speedup and
39% energy savings on mobile devices with less than 0.1% accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tabani_H/0/1/0/all/0/1"&gt;Hamid Tabani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramaniam_A/0/1/0/all/0/1"&gt;Ajay Balasubramaniam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1"&gt;Shabbir Marzban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Space Model for Higher-order Networks and Generalized Tensor Decomposition. (arXiv:2106.16042v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16042</id>
        <link href="http://arxiv.org/abs/2106.16042"/>
        <updated>2021-07-01T01:59:34.324Z</updated>
        <summary type="html"><![CDATA[We introduce a unified framework, formulated as general latent space models,
to study complex higher-order network interactions among multiple entities. Our
framework covers several popular models in recent network analysis literature,
including mixture multi-layer latent space model and hypergraph latent space
model. We formulate the relationship between the latent positions and the
observed data via a generalized multilinear kernel as the link function. While
our model enjoys decent generality, its maximum likelihood parameter estimation
is also convenient via a generalized tensor decomposition procedure.We propose
a novel algorithm using projected gradient descent on Grassmannians. We also
develop original theoretical guarantees for our algorithm. First, we show its
linear convergence under mild conditions. Second, we establish finite-sample
statistical error rates of latent position estimation, determined by the signal
strength, degrees of freedom and the smoothness of link function, for both
general and specific latent space models. We demonstrate the effectiveness of
our method on synthetic data. We also showcase the merit of our method on two
real-world datasets that are conventionally described by different specific
models in producing meaningful and interpretable parameter estimations and
accurate link prediction. We demonstrate the effectiveness of our method on
synthetic data. We also showcase the merit of our method on two real-world
datasets that are conventionally described by different specific models in
producing meaningful and interpretable parameter estimations and accurate link
prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1"&gt;Zhongyuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_D/0/1/0/all/0/1"&gt;Dong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Minimize Age of Information over an Unreliable Channel with Energy Harvesting. (arXiv:2106.16037v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.16037</id>
        <link href="http://arxiv.org/abs/2106.16037"/>
        <updated>2021-07-01T01:59:34.307Z</updated>
        <summary type="html"><![CDATA[The time average expected age of information (AoI) is studied for status
updates sent over an error-prone channel from an energy-harvesting transmitter
with a finite-capacity battery. Energy cost of sensing new status updates is
taken into account as well as the transmission energy cost better capturing
practical systems. The optimal scheduling policy is first studied under the
hybrid automatic repeat request (HARQ) protocol when the channel and energy
harvesting statistics are known, and the existence of a threshold-based optimal
policy is shown. For the case of unknown environments, average-cost
reinforcement-learning algorithms are proposed that learn the system parameters
and the status update policy in real-time. The effectiveness of the proposed
methods is demonstrated through numerical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ceran_E/0/1/0/all/0/1"&gt;Elif Tugce Ceran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1"&gt;Andras Gyorgy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resilient UAV Swarm Communications with Graph Convolutional Neural Network. (arXiv:2106.16048v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.16048</id>
        <link href="http://arxiv.org/abs/2106.16048"/>
        <updated>2021-07-01T01:59:34.290Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the self-healing problem of unmanned aerial vehicle
(UAV) swarm network (USNET) that is required to quickly rebuild the
communication connectivity under unpredictable external disruptions (UEDs).
Firstly, to cope with the one-off UEDs, we propose a graph convolutional neural
network (GCN) and find the recovery topology of the USNET in an on-line manner.
Secondly, to cope with general UEDs, we develop a GCN based trajectory planning
algorithm that can make UAVs rebuild the communication connectivity during the
self-healing process. We also design a meta learning scheme to facilitate the
on-line executions of the GCN. Numerical results show that the proposed
algorithms can rebuild the communication connectivity of the USNET more quickly
than the existing algorithms under both one-off UEDs and general UEDs. The
simulation results also show that the meta learning scheme can not only enhance
the performance of the GCN but also reduce the time complexity of the on-line
executions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mou_Z/0/1/0/all/0/1"&gt;Zhiyu Mou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1"&gt;Feifei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qihui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Likelihoods and Parameter Priors for Bayesian Networks. (arXiv:2105.06241v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06241</id>
        <link href="http://arxiv.org/abs/2105.06241"/>
        <updated>2021-07-01T01:59:34.181Z</updated>
        <summary type="html"><![CDATA[We develop simple methods for constructing likelihoods and parameter priors
for learning about the parameters and structure of a Bayesian network. In
particular, we introduce several assumptions that permit the construction of
likelihoods and parameter priors for a large number of Bayesian-network
structures from a small set of assessments. The most notable assumption is that
of likelihood equivalence, which says that data can not help to discriminate
network structures that encode the same assertions of conditional independence.
We describe the constructions that follow from these assumptions, and also
present a method for directly computing the marginal likelihood of a random
sample with no missing observations. Also, we show how these assumptions lead
to a general framework for characterizing parameter priors of multivariate
distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1"&gt;David Heckerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1"&gt;Dan Geiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIDER: Commonsense Inference for Dialogue Explanation and Reasoning. (arXiv:2106.00510v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00510</id>
        <link href="http://arxiv.org/abs/2106.00510"/>
        <updated>2021-07-01T01:59:34.166Z</updated>
        <summary type="html"><![CDATA[Commonsense inference to understand and explain human language is a
fundamental research problem in natural language processing. Explaining human
conversations poses a great challenge as it requires contextual understanding,
planning, inference, and several aspects of reasoning including causal,
temporal, and commonsense reasoning. In this work, we introduce CIDER -- a
manually curated dataset that contains dyadic dialogue explanations in the form
of implicit and explicit knowledge triplets inferred using contextual
commonsense inference. Extracting such rich explanations from conversations can
be conducive to improving several downstream applications. The annotated
triplets are categorized by the type of commonsense knowledge present (e.g.,
causal, conditional, temporal). We set up three different tasks conditioned on
the annotated dataset: Dialogue-level Natural Language Inference, Span
Extraction, and Multi-choice Span Selection. Baseline results obtained with
transformer-based models reveal that the tasks are difficult, paving the way
for promising future research. The dataset and the baseline implementations are
publicly available at https://cider-task.github.io/cider/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1"&gt;Pengfei Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Siqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monte Carlo Variational Auto-Encoders. (arXiv:2106.15921v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15921</id>
        <link href="http://arxiv.org/abs/2106.15921"/>
        <updated>2021-07-01T01:59:34.160Z</updated>
        <summary type="html"><![CDATA[Variational auto-encoders (VAE) are popular deep latent variable models which
are trained by maximizing an Evidence Lower Bound (ELBO). To obtain tighter
ELBO and hence better variational approximations, it has been proposed to use
importance sampling to get a lower variance estimate of the evidence. However,
importance sampling is known to perform poorly in high dimensions. While it has
been suggested many times in the literature to use more sophisticated
algorithms such as Annealed Importance Sampling (AIS) and its Sequential
Importance Sampling (SIS) extensions, the potential benefits brought by these
advanced techniques have never been realized for VAE: the AIS estimate cannot
be easily differentiated, while SIS requires the specification of carefully
chosen backward Markov kernels. In this paper, we address both issues and
demonstrate the performance of the resulting Monte Carlo VAEs on a variety of
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Thin_A/0/1/0/all/0/1"&gt;Achille Thin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kotelevskii_N/0/1/0/all/0/1"&gt;Nikita Kotelevskii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1"&gt;Alain Durmus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1"&gt;Eric Moulines&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Panov_M/0/1/0/all/0/1"&gt;Maxim Panov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grey-box models for wave loading prediction. (arXiv:2105.13813v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13813</id>
        <link href="http://arxiv.org/abs/2105.13813"/>
        <updated>2021-07-01T01:59:34.129Z</updated>
        <summary type="html"><![CDATA[The quantification of wave loading on offshore structures and components is a
crucial element in the assessment of their useful remaining life. In many
applications the well-known Morison's equation is employed to estimate the
forcing from waves with assumed particle velocities and accelerations. This
paper develops a grey-box modelling approach to improve the predictions of the
force on structural members. A grey-box model intends to exploit the enhanced
predictive capabilities of data-based modelling whilst retaining physical
insight into the behaviour of the system; in the context of the work carried
out here, this can be considered as physics-informed machine learning. There
are a number of possible approaches to establish a grey-box model. This paper
demonstrates two means of combining physics (white box) and data-based (black
box) components; one where the model is a simple summation of the two
components, the second where the white-box prediction is fed into the black box
as an additional input. Here Morison's equation is used as the physics-based
component in combination with a data-based Gaussian process NARX - a dynamic
variant of the more well-known Gaussian process regression. Two key challenges
with employing the GP-NARX formulation that are addressed here are the
selection of appropriate lag terms and the proper treatment of uncertainty
propagation within the dynamic GP. The best performing grey-box model, the
residual modelling GP-NARX, was able to achieve a 29.13\% and 5.48\% relative
reduction in NMSE over Morison's Equation and a black-box GP-NARX respectively,
alongside significant benefits in extrapolative capabilities of the model, in
circumstances of low dataset coverage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pitchforth_D/0/1/0/all/0/1"&gt;Daniel J Pitchforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1"&gt;Timothy J Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tygesen_U/0/1/0/all/0/1"&gt;Ulf T Tygesen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cross_E/0/1/0/all/0/1"&gt;Elizabeth J Cross&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIDER: Commonsense Inference for Dialogue Explanation and Reasoning. (arXiv:2106.00510v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00510</id>
        <link href="http://arxiv.org/abs/2106.00510"/>
        <updated>2021-07-01T01:59:34.123Z</updated>
        <summary type="html"><![CDATA[Commonsense inference to understand and explain human language is a
fundamental research problem in natural language processing. Explaining human
conversations poses a great challenge as it requires contextual understanding,
planning, inference, and several aspects of reasoning including causal,
temporal, and commonsense reasoning. In this work, we introduce CIDER -- a
manually curated dataset that contains dyadic dialogue explanations in the form
of implicit and explicit knowledge triplets inferred using contextual
commonsense inference. Extracting such rich explanations from conversations can
be conducive to improving several downstream applications. The annotated
triplets are categorized by the type of commonsense knowledge present (e.g.,
causal, conditional, temporal). We set up three different tasks conditioned on
the annotated dataset: Dialogue-level Natural Language Inference, Span
Extraction, and Multi-choice Span Selection. Baseline results obtained with
transformer-based models reveal that the tasks are difficult, paving the way
for promising future research. The dataset and the baseline implementations are
publicly available at https://cider-task.github.io/cider/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1"&gt;Pengfei Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Siqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task. (arXiv:2106.16055v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16055</id>
        <link href="http://arxiv.org/abs/2106.16055"/>
        <updated>2021-07-01T01:59:34.116Z</updated>
        <summary type="html"><![CDATA[This paper describes the submission to the IWSLT 2021 Low-Resource Speech
Translation Shared Task by IMS team. We utilize state-of-the-art models
combined with several data augmentation, multi-task and transfer learning
approaches for the automatic speech recognition (ASR) and machine translation
(MT) steps of our cascaded system. Moreover, we also explore the feasibility of
a full end-to-end speech translation (ST) model in the case of very constrained
amount of ground truth labeled data. Our best system achieves the best
performance among all submitted systems for Congolese Swahili to English and
French with BLEU scores 7.7 and 13.7 respectively, and the second best result
for Coastal Swahili to English with BLEU score 14.9.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1"&gt;Pavel Denisov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mager_M/0/1/0/all/0/1"&gt;Manuel Mager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1"&gt;Ngoc Thang Vu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separation Results between Fixed-Kernel and Feature-Learning Probability Metrics. (arXiv:2106.05739v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05739</id>
        <link href="http://arxiv.org/abs/2106.05739"/>
        <updated>2021-07-01T01:59:34.100Z</updated>
        <summary type="html"><![CDATA[Several works in implicit and explicit generative modeling empirically
observed that feature-learning discriminators outperform fixed-kernel
discriminators in terms of the sample quality of the models. We provide
separation results between probability metrics with fixed-kernel and
feature-learning discriminators using the function classes $\mathcal{F}_2$ and
$\mathcal{F}_1$ respectively, which were developed to study overparametrized
two-layer neural networks. In particular, we construct pairs of distributions
over hyper-spheres that can not be discriminated by fixed kernel
$(\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)
in high dimensions, but that can be discriminated by their feature learning
($\mathcal{F}_1$) counterparts. To further study the separation we provide
links between the $\mathcal{F}_1$ and $\mathcal{F}_2$ IPMs with sliced
Wasserstein distances. Our work suggests that fixed-kernel discriminators
perform worse than their feature learning counterparts because their
corresponding metrics are weaker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Domingo_Enrich_C/0/1/0/all/0/1"&gt;Carles Domingo-Enrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1"&gt;Youssef Mroueh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Broadcasted Residual Learning for Efficient Keyword Spotting. (arXiv:2106.04140v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04140</id>
        <link href="http://arxiv.org/abs/2106.04140"/>
        <updated>2021-07-01T01:59:34.094Z</updated>
        <summary type="html"><![CDATA[Keyword spotting is an important research field because it plays a key role
in device wake-up and user interaction on smart devices. However, it is
challenging to minimize errors while operating efficiently in devices with
limited resources such as mobile phones. We present a broadcasted residual
learning method to achieve high accuracy with small model size and
computational load. Our method configures most of the residual functions as 1D
temporal convolution while still allows 2D convolution together using a
broadcasted-residual connection that expands temporal output to
frequency-temporal dimension. This residual mapping enables the network to
effectively represent useful audio features with much less computation than
conventional convolutional neural networks. We also propose a novel network
architecture, Broadcasting-residual network (BC-ResNet), based on broadcasted
residual learning and describe how to scale up the model according to the
target device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%
top-1 accuracy on Google speech command datasets v1 and v2, respectively, and
consistently outperform previous approaches, using fewer computations and
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Byeonggeun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Simyung Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jinkyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_D/0/1/0/all/0/1"&gt;Dooyong Sung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo. (arXiv:2106.16118v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.16118</id>
        <link href="http://arxiv.org/abs/2106.16118"/>
        <updated>2021-07-01T01:59:34.088Z</updated>
        <summary type="html"><![CDATA[Robot manipulation of unknown objects in unstructured environments is a
challenging problem due to the variety of shapes, materials, arrangements and
lighting conditions. Even with large-scale real-world data collection, robust
perception and manipulation of transparent and reflective objects across
various lighting conditions remain challenging. To address these challenges we
propose an approach to performing sim-to-real transfer of robotic perception.
The underlying model, SimNet, is trained as a single multi-headed neural
network using simulated stereo data as input and simulated object segmentation
masks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as
output. A key component of SimNet is the incorporation of a learned stereo
sub-network that predicts disparity. SimNet is evaluated on 2D car detection,
unknown object detection, and deformable object keypoint detection and
significantly outperforms a baseline that uses a structured light RGB-D sensor.
By inferring grasp positions using the OBB and keypoint predictions, SimNet can
be used to perform end-to-end manipulation of unknown objects in both easy and
hard scenarios using our fleet of Toyota HSR robots in four home environments.
In unknown object grasping experiments, the predictions from the baseline RGB-D
network and SimNet enable successful grasps of most of the easy objects.
However, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)
objects, while SimNet grasps 95%, suggesting that SimNet can enable robust
manipulation of unknown objects, including transparent objects, in unknown
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1"&gt;Thomas Kollar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1"&gt;Michael Laskey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1"&gt;Kevin Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tjersland_M/0/1/0/all/0/1"&gt;Mark Tjersland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Genre determining prediction: Non-standard TAM marking in football language. (arXiv:2106.15872v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15872</id>
        <link href="http://arxiv.org/abs/2106.15872"/>
        <updated>2021-07-01T01:59:34.081Z</updated>
        <summary type="html"><![CDATA[German and French football language display tense-aspect-mood (TAM) forms
which differ from the TAM use in other genres. In German football talk, the
present indicative may replace the pluperfect subjunctive. In French reports of
football matches, the imperfective past may occur instead of a perfective past
tense-aspect form. We argue that the two phenomena share a functional core and
are licensed in the same way, which is a direct result of the genre they occur
in. More precisely, football match reports adhere to a precise script and
specific events are temporally determined in terms of objective time. This
allows speakers to exploit a secondary function of TAM forms, namely, they
shift the temporal perspective. We argue that it is on the grounds of the genre
that comprehenders predict the deviating forms and are also able to decode
them. We present various corpus studies where we explore the functioning of
these phenomena in order to gain insights into their distribution,
grammaticalization and their functioning in discourse. Relevant factors are
Aktionsart properties, rhetorical relations and their interaction with other
TAM forms. This allows us to discuss coping mechanisms on the part of the
comprehender. We broaden our understanding of the phenomena, which have only
been partly covered for French and up to now seem to have been ignored in
German.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Egetenmeyer_J/0/1/0/all/0/1"&gt;Jakob Egetenmeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Application of deep reinforcement learning for Indian stock trading automation. (arXiv:2106.16088v1 [q-fin.TR])]]></title>
        <id>http://arxiv.org/abs/2106.16088</id>
        <link href="http://arxiv.org/abs/2106.16088"/>
        <updated>2021-07-01T01:59:34.076Z</updated>
        <summary type="html"><![CDATA[In stock trading, feature extraction and trading strategy design are the two
important tasks to achieve long-term benefits using machine learning
techniques. Several methods have been proposed to design trading strategy by
acquiring trading signals to maximize the rewards. In the present paper the
theory of deep reinforcement learning is applied for stock trading strategy and
investment decisions to Indian markets. The experiments are performed
systematically with three classical Deep Reinforcement Learning models Deep
Q-Network, Double Deep Q-Network and Dueling Double Deep Q-Network on ten
Indian stock datasets. The performance of the models are evaluated and
comparison is made.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Bajpai_S/0/1/0/all/0/1"&gt;Supriya Bajpai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Domain Adaptation for Object Detection. (arXiv:2106.15793v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15793</id>
        <link href="http://arxiv.org/abs/2106.15793"/>
        <updated>2021-07-01T01:59:34.067Z</updated>
        <summary type="html"><![CDATA[To reduce annotation labor associated with object detection, an increasing
number of studies focus on transferring the learned knowledge from a labeled
source domain to another unlabeled target domain. However, existing methods
assume that the labeled data are sampled from a single source domain, which
ignores a more generalized scenario, where labeled data are from multiple
source domains. For the more challenging task, we propose a unified Faster
R-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which
can simultaneously enhance domain invariance and preserve discriminative power.
Specifically, the framework contains multiple source subnets and a pseudo
target subnet. First, we propose a hierarchical feature alignment strategy to
conduct strong and weak alignments for low- and high-level features,
respectively, considering their different effects for object detection. Second,
we develop a novel pseudo subnet learning algorithm to approximate optimal
parameters of pseudo target subset by weighted combination of parameters in
different source subnets. Finally, a consistency regularization for region
proposal network is proposed to facilitate each subnet to learn more abstract
invariances. Extensive experiments on different adaptation scenarios
demonstrate the effectiveness of the proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xingxu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sicheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Pengfei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jufeng Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Protein-Ligand Docking Surrogate Models: A SARS-CoV-2 Benchmark for Deep Learning Accelerated Virtual Screening. (arXiv:2106.07036v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07036</id>
        <link href="http://arxiv.org/abs/2106.07036"/>
        <updated>2021-07-01T01:59:34.051Z</updated>
        <summary type="html"><![CDATA[We propose a benchmark to study surrogate model accuracy for protein-ligand
docking. We share a dataset consisting of 200 million 3D complex structures and
2D structure scores across a consistent set of 13 million "in-stock" molecules
over 15 receptors, or binding sites, across the SARS-CoV-2 proteome. Our work
shows surrogate docking models have six orders of magnitude more throughput
than standard docking protocols on the same supercomputer node types. We
demonstrate the power of high-speed surrogate models by running each target
against 1 billion molecules in under a day (50k predictions per GPU seconds).
We showcase a workflow for docking utilizing surrogate ML models as a
pre-filter. Our workflow is ten times faster at screening a library of
compounds than the standard technique, with an error rate less than 0.01\% of
detecting the underlying best scoring 0.1\% of compounds. Our analysis of the
speedup explains that to screen more molecules under a docking paradigm,
another order of magnitude speedup must come from model accuracy rather than
computing speed (which, if increased, will not anymore alter our throughput to
screen molecules). We believe this is strong evidence for the community to
begin focusing on improving the accuracy of surrogate models to improve the
ability to screen massive compound libraries 100x or even 1000x faster than
current techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Clyde_A/0/1/0/all/0/1"&gt;Austin Clyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Brettin_T/0/1/0/all/0/1"&gt;Thomas Brettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Partin_A/0/1/0/all/0/1"&gt;Alexander Partin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yoo_H/0/1/0/all/0/1"&gt;Hyunseung Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Babuji_Y/0/1/0/all/0/1"&gt;Yadu Babuji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Blaiszik_B/0/1/0/all/0/1"&gt;Ben Blaiszik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Merzky_A/0/1/0/all/0/1"&gt;Andre Merzky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Turilli_M/0/1/0/all/0/1"&gt;Matteo Turilli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Jha_S/0/1/0/all/0/1"&gt;Shantenu Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ramanathan_A/0/1/0/all/0/1"&gt;Arvind Ramanathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Stevens_R/0/1/0/all/0/1"&gt;Rick Stevens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[10-mega pixel snapshot compressive imaging with a hybrid coded aperture. (arXiv:2106.15765v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.15765</id>
        <link href="http://arxiv.org/abs/2106.15765"/>
        <updated>2021-07-01T01:59:34.045Z</updated>
        <summary type="html"><![CDATA[High resolution images are widely used in our daily life, whereas high-speed
video capture is challenging due to the low frame rate of cameras working at
the high resolution mode. Digging deeper, the main bottleneck lies in the low
throughput of existing imaging systems. Towards this end, snapshot compressive
imaging (SCI) was proposed as a promising solution to improve the throughput of
imaging systems by compressive sampling and computational reconstruction.
During acquisition, multiple high-speed images are encoded and collapsed to a
single measurement. After this, algorithms are employed to retrieve the video
frames from the coded snapshot. Recently developed Plug-and-Play (PnP)
algorithms make it possible for SCI reconstruction in large-scale problems.
However, the lack of high-resolution encoding systems still precludes SCI's
wide application. In this paper, we build a novel hybrid coded aperture
snapshot compressive imaging (HCA-SCI) system by incorporating a dynamic liquid
crystal on silicon and a high-resolution lithography mask. We further implement
a PnP reconstruction algorithm with cascaded denoisers for high quality
reconstruction. Based on the proposed HCA-SCI system and algorithm, we achieve
a 10-mega pixel SCI system to capture high-speed scenes, leading to a high
throughput of 4.6G voxels per second. Both simulation and real data experiments
verify the feasibility and performance of our proposed HCA-SCI scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhihong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_C/0/1/0/all/0/1"&gt;Chao Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Suo_J/0/1/0/all/0/1"&gt;Jinli Suo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qionghai Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partially-Connected Differentiable Architecture Search for Deepfake and Spoofing Detection. (arXiv:2104.03123v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03123</id>
        <link href="http://arxiv.org/abs/2104.03123"/>
        <updated>2021-07-01T01:59:34.038Z</updated>
        <summary type="html"><![CDATA[This paper reports the first successful application of a differentiable
architecture search (DARTS) approach to the deepfake and spoofing detection
problems. An example of neural architecture search, DARTS operates upon a
continuous, differentiable search space which enables both the architecture and
parameters to be optimised via gradient descent. Solutions based on
partially-connected DARTS use random channel masking in the search space to
reduce GPU time and automatically learn and optimise complex neural
architectures composed of convolutional operations and residual blocks. Despite
being learned quickly with little human effort, the resulting networks are
competitive with the best performing systems reported in the literature. Some
are also far less complex, containing 85% fewer parameters than a Res2Net
competitor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1"&gt;Wanying Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panariello_M/0/1/0/all/0/1"&gt;Michele Panariello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patino_J/0/1/0/all/0/1"&gt;Jose Patino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1"&gt;Massimiliano Todisco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1"&gt;Nicholas Evans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning on a Budget via Teacher Imitation. (arXiv:2104.08440v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08440</id>
        <link href="http://arxiv.org/abs/2104.08440"/>
        <updated>2021-07-01T01:59:34.032Z</updated>
        <summary type="html"><![CDATA[Deep Reinforcement Learning (RL) techniques can benefit greatly from
leveraging prior experience, which can be either self-generated or acquired
from other entities. Action advising is a framework that provides a flexible
way to transfer such knowledge in the form of actions between teacher-student
peers. However, due to the realistic concerns, the number of these interactions
is limited with a budget; therefore, it is crucial to perform these in the most
appropriate moments. There have been several promising studies recently that
address this problem setting especially from the student's perspective. Despite
their success, they have some shortcomings when it comes to the practical
applicability and integrity as an overall solution to the learning from advice
challenge. In this paper, we extend the idea of advice reusing via teacher
imitation to construct a unified approach that addresses both advice collection
and advice utilisation problems. We also propose a method to automatically tune
the relevant hyperparameters of these components on-the-fly to make it able to
adapt to any task with minimal human intervention. The experiments we performed
in 5 different Atari games verify that our algorithm either surpasses or
performs on-par with its top competitors while being far simpler to be
employed. Furthermore, its individual components are also found to be providing
significant advantages alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilhan_E/0/1/0/all/0/1"&gt;Ercument Ilhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gow_J/0/1/0/all/0/1"&gt;Jeremy Gow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Liebana_D/0/1/0/all/0/1"&gt;Diego Perez-Liebana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Human-Machine Teaming for Medical Prognosis Through Neural Ordinary Differential Equations (NODEs). (arXiv:2102.04121v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04121</id>
        <link href="http://arxiv.org/abs/2102.04121"/>
        <updated>2021-07-01T01:59:34.016Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) has recently been demonstrated to rival expert-level
human accuracy in prediction and detection tasks in a variety of domains,
including medicine. Despite these impressive findings, however, a key barrier
to the full realization of ML's potential in medical prognoses is technology
acceptance. Recent efforts to produce explainable AI (XAI) have made progress
in improving the interpretability of some ML models, but these efforts suffer
from limitations intrinsic to their design: they work best at identifying why a
system fails, but do poorly at explaining when and why a model's prediction is
correct. We posit that the acceptability of ML predictions in expert domains is
limited by two key factors: the machine's horizon of prediction that extends
beyond human capability, and the inability for machine predictions to
incorporate human intuition into their models. We propose the use of a novel ML
architecture, Neural Ordinary Differential Equations (NODEs) to enhance human
understanding and encourage acceptability. Our approach prioritizes human
cognitive intuition at the center of the algorithm design, and offers a
distribution of predictions rather than single outputs. We explain how this
approach may significantly improve human-machine collaboration in prediction
tasks in expert domains such as medical prognoses. We propose a model and
demonstrate, by expanding a concrete example from the literature, how our model
advances the vision of future hybrid Human-AI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fompeyrine_D/0/1/0/all/0/1"&gt;D. Fompeyrine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vorm_E/0/1/0/all/0/1"&gt;E. S. Vorm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricka_N/0/1/0/all/0/1"&gt;N. Ricka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rose_F/0/1/0/all/0/1"&gt;F. Rose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pellegrin_G/0/1/0/all/0/1"&gt;G. Pellegrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05704</id>
        <link href="http://arxiv.org/abs/2104.05704"/>
        <updated>2021-07-01T01:59:34.010Z</updated>
        <summary type="html"><![CDATA[With the rise of Transformers as the standard for language processing, and
their advancements in computer vision, along with their unprecedented size and
amounts of training data, many have come to believe that they are not suitable
for small sets of data. This trend leads to great concerns, including but not
limited to: limited availability of data in certain scientific domains and the
exclusion of those with limited resource from research in the field. In this
paper, we dispel the myth that transformers are "data hungry" and therefore can
only be applied to large sets of data. We show for the first time that with the
right size and tokenization, transformers can perform head-to-head with
state-of-the-art CNNs on small datasets. Our model eliminates the requirement
for class token and positional embeddings through a novel sequence pooling
strategy and the use of convolutions. We show that compared to CNNs, our
compact transformers have fewer parameters and MACs, while obtaining similar
accuracies. Our method is flexible in terms of model size, and can have as
little as 0.28M parameters and achieve reasonable results. It can reach an
accuracy of 95.29 % when training from scratch on CIFAR-10, which is comparable
with modern CNN based approaches, and a significant improvement over previous
Transformer based models. Our simple and compact design democratizes
transformers by making them accessible to those equipped with basic computing
resources and/or dealing with important small datasets. Our method works on
larger datasets, such as ImageNet (80.28% accuracy with 29% parameters of ViT),
and NLP tasks as well. Our code and pre-trained models are publicly available
at https://github.com/SHI-Labs/Compact-Transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1"&gt;Ali Hassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1"&gt;Steven Walton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Nikhil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1"&gt;Abulikemu Abuduweili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Humphrey Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10479</id>
        <link href="http://arxiv.org/abs/2106.10479"/>
        <updated>2021-07-01T01:59:34.004Z</updated>
        <summary type="html"><![CDATA[Transferability estimation is an essential problem in transfer learning to
predict how good the performance is when transferring a source model (or source
task) to a target task. Recent analytical transferability metrics have been
widely used for source model selection and multi-task learning. A major
challenge is how to make transfereability estimation robust under the
cross-domain cross-task settings. The recently proposed OTCE score solves this
problem by considering both domain and task differences, with the help of
transfer experiences on auxiliary tasks, which causes an efficiency overhead.
In this work, we propose a practical transferability metric called JC-NCE score
that dramatically improves the robustness of the task difference estimation in
OTCE, thus removing the need for auxiliary tasks. Specifically, we build the
joint correspondences between source and target data via solving an optimal
transport problem with a ground cost considering both the sample distance and
label distance, and then compute the transferability score as the negative
conditional entropy of the matched labels. Extensive validations under the
intra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE
score outperforms the auxiliary-task free version of OTCE for 7% and 12%,
respectively, and is also more robust than other existing transferability
metrics on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shao-Lun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Regular Conditional Distributions. (arXiv:2105.07743v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07743</id>
        <link href="http://arxiv.org/abs/2105.07743"/>
        <updated>2021-07-01T01:59:33.997Z</updated>
        <summary type="html"><![CDATA[We introduce a general framework for approximating regular conditional
distributions (RCDs). Our approximations of these RCDs are implemented by a new
class of geometric deep learning models with inputs in $\mathbb{R}^d$ and
outputs in the Wasserstein-$1$ space $\mathcal{P}_1(\mathbb{R}^D)$. We find
that the models built using our framework can approximate any continuous
functions from $\mathbb{R}^d$ to $\mathcal{P}_1(\mathbb{R}^D)$ uniformly on
compacts, and quantitative rates are obtained. We identify two methods for
avoiding the "curse of dimensionality"; i.e.: the number of parameters
determining the approximating neural network depends only polynomially on the
involved dimension and the approximation error. The first solution describes
functions in $C(\mathbb{R}^d,\mathcal{P}_1(\mathbb{R}^D))$ which can be
efficiently approximated on any compact subset of $\mathbb{R}^d$. Conversely,
the second approach describes sets in $\mathbb{R}^d$, on which any function in
$C(\mathbb{R}^d,\mathcal{P}_1(\mathbb{R}^D))$ can be efficiently approximated.
Our framework is used to obtain an affirmative answer to the open conjecture of
Bishop (1994); namely: mixture density networks are universal regular
conditional distributions. The predictive performance of the proposed models is
evaluated against comparable learning models on various probabilistic
predictions tasks in the context of ELMs, model uncertainty, and
heteroscedastic regression. All the results are obtained for more general input
and output spaces and thus apply to geometric deep learning contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1"&gt;Anastasis Kratsios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Joint Chance Constrained Optimization: Approximations and Statistical Consistency. (arXiv:2106.12199v2 [math.ST] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.12199</id>
        <link href="http://arxiv.org/abs/2106.12199"/>
        <updated>2021-07-01T01:59:33.992Z</updated>
        <summary type="html"><![CDATA[This paper considers data-driven chance-constrained stochastic optimization
problems in a Bayesian framework. Bayesian posteriors afford a principled
mechanism to incorporate data and prior knowledge into stochastic optimization
problems. However, the computation of Bayesian posteriors is typically an
intractable problem, and has spawned a large literature on approximate Bayesian
computation. Here, in the context of chance-constrained optimization, we focus
on the question of statistical consistency (in an appropriate sense) of the
optimal value, computed using an approximate posterior distribution. To this
end, we rigorously prove a frequentist consistency result demonstrating the
convergence of the optimal value to the optimal value of a fixed, parameterized
constrained optimization problem. We augment this by also establishing a
probabilistic rate of convergence of the optimal value. We also prove the
convex feasibility of the approximate Bayesian stochastic optimization problem.
Finally, we demonstrate the utility of our approach on an optimal staffing
problem for an M/M/c queueing model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Jaiswal_P/0/1/0/all/0/1"&gt;Prateek Jaiswal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Honnappa_H/0/1/0/all/0/1"&gt;Harsha Honnappa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rao_V/0/1/0/all/0/1"&gt;Vinayak A. Rao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11396</id>
        <link href="http://arxiv.org/abs/2106.11396"/>
        <updated>2021-07-01T01:59:33.986Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization recently has attracted increased interest in machine
learning due to its many applications such as hyper-parameter optimization and
policy optimization. Although some methods recently have been proposed to solve
the bilevel problems, these methods do not consider using adaptive learning
rates. To fill this gap, in the paper, we propose a class of fast and effective
adaptive methods for solving bilevel optimization problems that the outer
problem is possibly nonconvex and the inner problem is strongly-convex.
Specifically, we propose a fast single-loop BiAdam algorithm based on the basic
momentum technique, which achieves a sample complexity of
$\tilde{O}(\epsilon^{-4})$ for finding an $\epsilon$-stationary point. At the
same time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by
using variance reduced technique, which reaches the best known sample
complexity of $\tilde{O}(\epsilon^{-3})$. To further reduce computation in
estimating derivatives, we propose a fast single-loop stochastic approximated
BiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still
achieves a sample complexity of $\tilde{O}(\epsilon^{-4})$ without large
batches. We further present an accelerated version of saBiAdam algorithm
(VR-saBiAdam), which also reaches the best known sample complexity of
$\tilde{O}(\epsilon^{-3})$. We apply the unified adaptive matrices to our
methods as the SUPER-ADAM \citep{huang2021super}, which including many types of
adaptive learning rates. Moreover, our framework can flexibly use the momentum
and variance reduced techniques. In particular, we provide a useful convergence
analysis framework for both the constrained and unconstrained bilevel
optimization. To the best of our knowledge, we first study the adaptive bilevel
optimization methods with adaptive learning rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual GNNs: Graph Neural Network Learning with Limited Supervision. (arXiv:2106.15755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15755</id>
        <link href="http://arxiv.org/abs/2106.15755"/>
        <updated>2021-07-01T01:59:33.968Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) require a relatively large number of labeled
nodes and a reliable/uncorrupted graph connectivity structure in order to
obtain good performance on the semi-supervised node classification task. The
performance of GNNs can degrade significantly as the number of labeled nodes
decreases or the graph connectivity structure is corrupted by adversarial
attacks or due to noises in data measurement /collection. Therefore, it is
important to develop GNN models that are able to achieve good performance when
there is limited supervision knowledge -- a few labeled nodes and noisy graph
structures. In this paper, we propose a novel Dual GNN learning framework to
address this challenge task. The proposed framework has two GNN based node
prediction modules. The primary module uses the input graph structure to induce
regular node embeddings and predictions with a regular GNN baseline, while the
auxiliary module constructs a new graph structure through fine-grained spectral
clusterings and learns new node embeddings and predictions. By integrating the
two modules in a dual GNN learning framework, we perform joint learning in an
end-to-end fashion. This general framework can be applied on many GNN baseline
models. The experimental results validate that the proposed dual GNN framework
can greatly outperform the GNN baseline methods when the labeled nodes are
scarce and the graph connectivity structure is noisy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alchihabi_A/0/1/0/all/0/1"&gt;Abdullah Alchihabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuhong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data. (arXiv:2106.14503v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14503</id>
        <link href="http://arxiv.org/abs/2106.14503"/>
        <updated>2021-07-01T01:59:33.961Z</updated>
        <summary type="html"><![CDATA[Federated Learning allows training of data stored in distributed devices
without the need for centralizing training data, thereby maintaining data
privacy. Addressing the ability to handle data heterogeneity (non-identical and
independent distribution or non-IID) is a key enabler for the wider deployment
of Federated Learning. In this paper, we propose a novel Divide-and-Conquer
training methodology that enables the use of the popular FedAvg aggregation
algorithm by overcoming the acknowledged FedAvg limitations in non-IID
environments. We propose a novel use of Cosine-distance based Weight Divergence
metric to determine the exact point where a Deep Learning network can be
divided into class agnostic initial layers and class-specific deep layers for
performing a Divide and Conquer training. We show that the methodology achieves
trained model accuracy at par (and in certain cases exceeding) with numbers
achieved by state-of-the-art Aggregation algorithms like FedProx, FedMA, etc.
Also, we show that this methodology leads to compute and bandwidth
optimizations under certain documented conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1"&gt;Pravin Chandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1"&gt;Raghavendra Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_A/0/1/0/all/0/1"&gt;Avinash Chakravarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Srikanth Chandar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10587</id>
        <link href="http://arxiv.org/abs/2106.10587"/>
        <updated>2021-07-01T01:59:33.955Z</updated>
        <summary type="html"><![CDATA[Existing computer vision research in categorization struggles with
fine-grained attributes recognition due to the inherently high intra-class
variances and low inter-class variances. SOTA methods tackle this challenge by
locating the most informative image regions and rely on them to classify the
complete image. The most recent work, Vision Transformer (ViT), shows its
strong performance in both traditional and fine-grained classification tasks.
In this work, we propose a multi-stage ViT framework for fine-grained image
classification tasks, which localizes the informative image regions without
requiring architectural changes using the inherent multi-head self-attention
mechanism. We also introduce attention-guided augmentations for improving the
model's capabilities. We demonstrate the value of our approach by experimenting
with four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,
Stanford Dogs, and FGVC7 Plant Pathology. We also prove our model's
interpretability via qualitative results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1"&gt;Marcos V. Conde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1"&gt;Kerem Turgutlu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust and Interpretable Temporal Convolution Network for Event Detection in Lung Sound Recordings. (arXiv:2106.15835v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.15835</id>
        <link href="http://arxiv.org/abs/2106.15835"/>
        <updated>2021-07-01T01:59:33.939Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel framework for lung sound event detection,
segmenting continuous lung sound recordings into discrete events and performing
recognition on each event. Exploiting the lightweight nature of Temporal
Convolution Networks (TCNs) and their superior results compared to their
recurrent counterparts, we propose a lightweight, yet robust, and completely
interpretable framework for lung sound event detection. We propose the use of a
multi-branch TCN architecture and exploit a novel fusion strategy to combine
the resultant features from these branches. This not only allows the network to
retain the most salient information across different temporal granularities and
disregards irrelevant information, but also allows our network to process
recordings of arbitrary length. Results: The proposed method is evaluated on
multiple public and in-house benchmarks of irregular and noisy recordings of
the respiratory auscultation process for the identification of numerous
auscultation events including inhalation, exhalation, crackles, wheeze,
stridor, and rhonchi. We exceed the state-of-the-art results in all
evaluations. Furthermore, we empirically analyse the effect of the proposed
multi-branch TCN architecture and the feature fusion strategy and provide
quantitative and qualitative evaluations to illustrate their efficiency.
Moreover, we provide an end-to-end model interpretation pipeline that
interprets the operations of all the components of the proposed framework. Our
analysis of different feature fusion strategies shows that the proposed feature
concatenation method leads to better suppression of non-informative features,
which drastically reduces the classifier overhead resulting in a robust
lightweight network.The lightweight nature of our model allows it to be
deployed in end-user devices such as smartphones, and it has the ability to
generate predictions in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1"&gt;Tharindu Fernando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1"&gt;Sridha Sridharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1"&gt;Simon Denman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaemmaghami_H/0/1/0/all/0/1"&gt;Houman Ghaemmaghami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1"&gt;Clinton Fookes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08903</id>
        <link href="http://arxiv.org/abs/2008.08903"/>
        <updated>2021-07-01T01:59:33.933Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have shown remarkable success in
producing realistic-looking images in the computer vision area. Recently,
GAN-based techniques are shown to be promising for spatio-temporal-based
applications such as trajectory prediction, events generation and time-series
data imputation. While several reviews for GANs in computer vision have been
presented, no one has considered addressing the practical applications and
challenges relevant to spatio-temporal data. In this paper, we have conducted a
comprehensive review of the recent developments of GANs for spatio-temporal
data. We summarise the application of popular GAN architectures for
spatio-temporal data and the common practices for evaluating the performance of
spatio-temporal applications with GANs. Finally, we point out future research
directions to benefit researchers in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1"&gt;Nan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1"&gt;Wei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sichen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1"&gt;Kyle Kai Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1"&gt;Arian Prabowo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1"&gt;Mohammad Saiedur Rahaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1"&gt;Flora D. Salim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep generative modeling for probabilistic forecasting in power systems. (arXiv:2106.09370v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09370</id>
        <link href="http://arxiv.org/abs/2106.09370"/>
        <updated>2021-07-01T01:59:33.927Z</updated>
        <summary type="html"><![CDATA[Greater direct electrification of end-use sectors with a higher share of
renewables is one of the pillars to power a carbon-neutral society by 2050.
This study uses a recent deep learning technique, the normalizing flows, to
produce accurate probabilistic forecasts that are crucial for decision-makers
to face the new challenges in power systems applications. Through comprehensive
empirical evaluations using the open data of the Global Energy Forecasting
Competition 2014, we demonstrate that our methodology is competitive with other
state-of-the-art deep learning generative models: generative adversarial
networks and variational autoencoders. The models producing weather-based wind,
solar power, and load scenarios are properly compared both in terms of forecast
value, by considering the case study of an energy retailer, and quality using
several complementary metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1"&gt;Jonathan Dumas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1"&gt;Antoine Wehenkel Damien Lanaspeze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1"&gt;Bertrand Corn&amp;#xe9;lusse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1"&gt;Antonio Sutera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Numerical Composition of Differential Privacy. (arXiv:2106.02848v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02848</id>
        <link href="http://arxiv.org/abs/2106.02848"/>
        <updated>2021-07-01T01:59:33.903Z</updated>
        <summary type="html"><![CDATA[We give a fast algorithm to optimally compose privacy guarantees of
differentially private (DP) algorithms to arbitrary accuracy. Our method is
based on the notion of privacy loss random variables to quantify the privacy
loss of DP algorithms. The running time and memory needed for our algorithm to
approximate the privacy curve of a DP algorithm composed with itself $k$ times
is $\tilde{O}(\sqrt{k})$. This improves over the best prior method by Koskela
et al. (2020) which requires $\tilde{\Omega}(k^{1.5})$ running time. We
demonstrate the utility of our algorithm by accurately computing the privacy
loss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm
speeds up the privacy computations by a few orders of magnitude compared to
prior work, while maintaining similar accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1"&gt;Sivakanth Gopi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yin Tat Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1"&gt;Lukas Wutschitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Normalizing Flows for Permutation Invariant Densities. (arXiv:2010.03242v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03242</id>
        <link href="http://arxiv.org/abs/2010.03242"/>
        <updated>2021-07-01T01:59:33.897Z</updated>
        <summary type="html"><![CDATA[Modeling sets is an important problem in machine learning since this type of
data can be found in many domains. A promising approach defines a family of
permutation invariant densities with continuous normalizing flows. This allows
us to maximize the likelihood directly and sample new realizations with ease.
In this work, we demonstrate how calculating the trace, a crucial step in this
method, raises issues that occur both during training and inference, limiting
its practicality. We propose an alternative way of defining permutation
equivariant transformations that give closed form trace. This leads not only to
improvements while training, but also to better final performance. We
demonstrate the benefits of our approach on point processes and general set
modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bilos_M/0/1/0/all/0/1"&gt;Marin Bilo&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration. (arXiv:1809.11165v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1809.11165</id>
        <link href="http://arxiv.org/abs/1809.11165"/>
        <updated>2021-07-01T01:59:33.888Z</updated>
        <summary type="html"><![CDATA[Despite advances in scalable models, the inference tools used for Gaussian
processes (GPs) have yet to fully capitalize on developments in computing
hardware. We present an efficient and general approach to GP inference based on
Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified
batched version of the conjugate gradients algorithm to derive all terms for
training and inference in a single call. BBMM reduces the asymptotic complexity
of exact GP inference from $O(n^3)$ to $O(n^2)$. Adapting this algorithm to
scalable approximations and complex GP models simply requires a routine for
efficient matrix-matrix multiplication with the kernel and its derivative. In
addition, BBMM uses a specialized preconditioner to substantially speed up
convergence. In experiments we show that BBMM effectively uses GPU hardware to
dramatically accelerate both exact GP inference and scalable approximations.
Additionally, we provide GPyTorch, a software platform for scalable GP
inference via BBMM, built on PyTorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1"&gt;Jacob R. Gardner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1"&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bindel_D/0/1/0/all/0/1"&gt;David Bindel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1"&gt;Kilian Q. Weinberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Operator-valued formulas for Riemannian Gradient and Hessian and families of tractable metrics. (arXiv:2009.10159v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10159</id>
        <link href="http://arxiv.org/abs/2009.10159"/>
        <updated>2021-07-01T01:59:33.882Z</updated>
        <summary type="html"><![CDATA[We provide an explicit formula for the Levi-Civita connection and Riemannian
Hessian for a Riemannian manifold that is a quotient of a manifold embedded in
an inner product space with a non-constant metric function. Together with a
classical formula for projection, this allows us to evaluate Riemannian
gradient and Hessian for several families of metrics on classical manifolds,
including a family of metrics on Stiefel manifolds connecting both the constant
and canonical ambient metrics with closed-form geodesics. Using these formulas,
we derive Riemannian optimization frameworks on quotients of Stiefel manifolds,
including flag manifolds, and a new family of complete quotient metrics on the
manifold of positive-semidefinite matrices of fixed rank, considered as a
quotient of a product of Stiefel and positive-definite matrix manifold with
affine-invariant metrics. The method is procedural, and in many instances, the
Riemannian gradient and Hessian formulas could be derived by symbolic calculus.
The method extends the list of potential metrics that could be used in manifold
optimization and machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Du Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning-enhanced Receive Processing for MU-MIMO OFDM Systems. (arXiv:2106.16074v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.16074</id>
        <link href="http://arxiv.org/abs/2106.16074"/>
        <updated>2021-07-01T01:59:33.856Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) can be used in various ways to improve multi-user
multiple-input multiple-output (MU-MIMO) receive processing. Typical approaches
either augment a single processing step, such as symbol detection, or replace
multiple steps jointly by a single neural network (NN). These techniques
demonstrate promising results but often assume perfect channel state
information (CSI) or fail to satisfy the interpretability and scalability
constraints imposed by practical systems. In this paper, we propose a new
strategy which preserves the benefits of a conventional receiver, but enhances
specific parts with ML components. The key idea is to exploit the orthogonal
frequency-division multiplexing (OFDM) signal structure to improve both the
demapping and the computation of the channel estimation error statistics.
Evaluation results show that the proposed ML-enhanced receiver beats practical
baselines on all considered scenarios, with significant gains at high speeds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1"&gt;Mathieu Goutay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1"&gt;Fay&amp;#xe7;al Ait Aoudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1"&gt;Jakob Hoydis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1"&gt;Jean-Marie Gorce&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distill on the Go: Online knowledge distillation in self-supervised learning. (arXiv:2104.09866v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09866</id>
        <link href="http://arxiv.org/abs/2104.09866"/>
        <updated>2021-07-01T01:59:33.843Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning solves pretext prediction tasks that do not require
annotations to learn feature representations. For vision tasks, pretext tasks
such as predicting rotation, solving jigsaw are solely created from the input
data. Yet, predicting this known information helps in learning representations
useful for downstream tasks. However, recent works have shown that wider and
deeper models benefit more from self-supervised learning than smaller models.
To address the issue of self-supervised pre-training of smaller models, we
propose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using
single-stage online knowledge distillation to improve the representation
quality of the smaller models. We employ deep mutual learning strategy in which
two models collaboratively learn from each other to improve one another.
Specifically, each model is trained using self-supervised learning along with
distillation that aligns each model's softmax probabilities of similarity
scores with that of the peer model. We conduct extensive experiments on
multiple benchmark datasets, learning objectives, and architectures to
demonstrate the potential of our proposed method. Our results show significant
performance gain in the presence of noisy and limited labels and generalization
to out-of-distribution data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1"&gt;Prashant Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Inertial Newton Algorithm for Deep Learning. (arXiv:1905.12278v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.12278</id>
        <link href="http://arxiv.org/abs/1905.12278"/>
        <updated>2021-07-01T01:59:33.817Z</updated>
        <summary type="html"><![CDATA[We introduce a new second-order inertial optimization method for machine
learning called INNA. It exploits the geometry of the loss function while only
requiring stochastic approximations of the function values and the generalized
gradients. This makes INNA fully implementable and adapted to large-scale
optimization problems such as the training of deep neural networks. The
algorithm combines both gradient-descent and Newton-like behaviors as well as
inertia. We prove the convergence of INNA for most deep learning problems. To
do so, we provide a well-suited framework to analyze deep learning loss
functions involving tame optimization in which we study a continuous dynamical
system together with its discrete stochastic approximations. We prove sublinear
convergence for the continuous-time differential inclusion which underlies our
algorithm. Additionally, we also show how standard optimization mini-batch
methods applied to non-smooth non-convex problems can yield a certain type of
spurious stationary points never discussed before. We address this issue by
providing a theoretical framework around the new idea of $D$-criticality; we
then give a simple asymptotic analysis of INNA. Our algorithm allows for using
an aggressive learning rate of $o(1/\log k)$. From an empirical viewpoint, we
show that INNA returns competitive results with respect to state of the art
(stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castera_C/0/1/0/all/0/1"&gt;Camille Castera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Bolte&lt;/a&gt; (UT1), &lt;a href="http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric F&amp;#xe9;votte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1"&gt;Edouard Pauwels&lt;/a&gt; (UT3)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias-reduced Multi-step Hindsight Experience Replay for Efficient Multi-goal Reinforcement Learning. (arXiv:2102.12962v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12962</id>
        <link href="http://arxiv.org/abs/2102.12962"/>
        <updated>2021-07-01T01:59:33.811Z</updated>
        <summary type="html"><![CDATA[Multi-goal reinforcement learning is widely applied in planning and robot
manipulation. Two main challenges in multi-goal reinforcement learning are
sparse rewards and sample inefficiency. Hindsight Experience Replay (HER) aims
to tackle the two challenges via goal relabeling. However, HER-related works
still need millions of samples and a huge computation. In this paper, we
propose Multi-step Hindsight Experience Replay (MHER), incorporating multi-step
relabeled returns based on $n$-step relabeling to improve sample efficiency.
Despite the advantages of $n$-step relabeling, we theoretically and
experimentally prove the off-policy $n$-step bias introduced by $n$-step
relabeling may lead to poor performance in many environments. To address the
above issue, two bias-reduced MHER algorithms, MHER($\lambda$) and Model-based
MHER (MMHER) are presented. MHER($\lambda$) exploits the $\lambda$ return while
MMHER benefits from model-based value expansions. Experimental results on
numerous multi-goal robotic tasks show that our solutions can successfully
alleviate off-policy $n$-step bias and achieve significantly higher sample
efficiency than HER and Curriculum-guided HER with little additional
computation beyond HER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Rui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1"&gt;Jiafei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ya_J/0/1/0/all/0/1"&gt;Jiangpeng Ya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1"&gt;Feng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1"&gt;Dijun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lanqing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08208</id>
        <link href="http://arxiv.org/abs/2106.08208"/>
        <updated>2021-07-01T01:59:33.793Z</updated>
        <summary type="html"><![CDATA[Adaptive gradient methods have shown excellent performance for solving many
machine learning problems. Although multiple adaptive methods were recently
studied, they mainly focus on either empirical or theoretical aspects and also
only work for specific problems by using specific adaptive learning rates. It
is desired to design a universal framework for practical algorithms of adaptive
gradients with theoretical guarantee to solve general problems. To fill this
gap, we propose a faster and universal framework of adaptive gradients (i.e.,
SUPER-ADAM) by introducing a universal adaptive matrix that includes most
existing adaptive gradient forms. Moreover, our framework can flexibly
integrates the momentum and variance reduced techniques. In particular, our
novel framework provides the convergence analysis support for adaptive gradient
methods under the nonconvex setting. In theoretical analysis, we prove that our
new algorithm can achieve the best known complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of
nonconvex optimization, which matches the lower bound for stochastic smooth
nonconvex optimization. In numerical experiments, we employ various deep
learning tasks to validate that our algorithm consistently outperforms the
existing adaptive algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon. (arXiv:2009.13503v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13503</id>
        <link href="http://arxiv.org/abs/2009.13503"/>
        <updated>2021-07-01T01:59:33.785Z</updated>
        <summary type="html"><![CDATA[Episodic reinforcement learning and contextual bandits are two widely studied
sequential decision-making problems. Episodic reinforcement learning
generalizes contextual bandits and is often perceived to be more difficult due
to long planning horizon and unknown state-dependent transitions. The current
paper shows that the long planning horizon and the unknown state-dependent
transitions (at most) pose little additional difficulty on sample complexity.

We consider the episodic reinforcement learning with $S$ states, $A$ actions,
planning horizon $H$, total reward bounded by $1$, and the agent plays for $K$
episodes. We propose a new algorithm, \textbf{M}onotonic \textbf{V}alue
\textbf{P}ropagation (MVP), which relies on a new Bernstein-type bonus.
Compared to existing bonus constructions, the new bonus is tighter since it is
based on a well-designed monotonic value function. In particular, the
\emph{constants} in the bonus should be subtly setting to ensure optimism and
monotonicity.

We show MVP enjoys an $O\left(\left(\sqrt{SAK} + S^2A\right) \poly\log
\left(SAHK\right)\right)$ regret, approaching the
$\Omega\left(\sqrt{SAK}\right)$ lower bound of \emph{contextual bandits} up to
logarithmic terms. Notably, this result 1) \emph{exponentially} improves the
state-of-the-art polynomial-time algorithms by Dann et al. [2019] and Zanette
et al. [2019] in terms of the dependency on $H$, and 2) \emph{exponentially}
improves the running time in [Wang et al. 2020] and significantly improves the
dependency on $S$, $A$ and $K$ in sample complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zihan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiangyang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CatBoost model with synthetic features in application to loan risk assessment of small businesses. (arXiv:2106.07954v3 [cs.CE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07954</id>
        <link href="http://arxiv.org/abs/2106.07954"/>
        <updated>2021-07-01T01:59:33.778Z</updated>
        <summary type="html"><![CDATA[Loan risk for small businesses has long been a complex problem worthy of
exploring. Predicting the loan risk can benefit entrepreneurship by developing
more jobs for the society. CatBoost (Categorical Boosting) is a powerful
machine learning algorithm suitable for dataset with many categorical variables
like the dataset for forecasting loan risk. In this paper, we identify the
important risk factors that contribute to loan status classification problem.
Then we compare the performance between boosting-type algorithms(especially
CatBoost) with other traditional yet popular ones. The dataset we adopt in the
research comes from the U.S. Small Business Administration (SBA) and holds a
very large sample size (899,164 observations and 27 features). In order to make
the best use of the important features in the dataset, we propose a technique
named "synthetic generation" to develop more combined features based on
arithmetic operation, which ends up improving the accuracy and AUC of the
original CatBoost model. We obtain a high accuracy of 95.84% and well-performed
AUC of 98.80% compared with the existent literature of related research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoxue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Liexin Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible Manifold Learning for Dimension Reduction. (arXiv:2010.04012v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04012</id>
        <link href="http://arxiv.org/abs/2010.04012"/>
        <updated>2021-07-01T01:59:33.772Z</updated>
        <summary type="html"><![CDATA[Dimension reduction (DR) aims to learn low-dimensional representations of
high-dimensional data with the preservation of essential information. In the
context of manifold learning, we define that the representation after
information-lossless DR preserves the topological and geometric properties of
data manifolds formally, and propose a novel two-stage DR method, called
invertible manifold learning (inv-ML) to bridge the gap between theoretical
information-lossless and practical DR. The first stage includes a homeomorphic
sparse coordinate transformation to learn low-dimensional representations
without destroying topology and a local isometry constraint to preserve local
geometry. In the second stage, a linear compression is implemented for the
trade-off between the target dimension and the incurred information loss in
excessive DR scenarios. Experiments are conducted on seven datasets with a
neural network implementation of inv-ML, called i-ML-Enc. Empirically, i-ML-Enc
achieves invertible DR in comparison with typical existing methods as well as
reveals the characteristics of the learned manifolds. Through latent space
interpolation on real-world datasets, we find that the reliability of tangent
space approximated by the local neighborhood is the key to the success of
manifold-based DR algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Siyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haitao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1"&gt;Zelin Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lirong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1"&gt;Jun Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan Z. Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributionally Robust Learning with Stable Adversarial Training. (arXiv:2106.15791v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15791</id>
        <link href="http://arxiv.org/abs/2106.15791"/>
        <updated>2021-07-01T01:59:33.756Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms with empirical risk minimization are vulnerable
under distributional shifts due to the greedy adoption of all the correlations
found in training data. There is an emerging literature on tackling this
problem by minimizing the worst-case risk over an uncertainty set. However,
existing methods mostly construct ambiguity sets by treating all variables
equally regardless of the stability of their correlations with the target,
resulting in the overwhelmingly-large uncertainty set and low confidence of the
learner. In this paper, we propose a novel Stable Adversarial Learning (SAL)
algorithm that leverages heterogeneous data sources to construct a more
practical uncertainty set and conduct differentiated robustness optimization,
where covariates are differentiated according to the stability of their
correlations with the target. We theoretically show that our method is
tractable for stochastic gradient-based optimization and provide the
performance guarantees for our method. Empirical studies on both simulation and
real datasets validate the effectiveness of our method in terms of uniformly
good performance across unknown distributional shifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiashuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zheyan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1"&gt;Peng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Linjun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1"&gt;Kun Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Representation Drift in Online Continual Learning. (arXiv:2104.05025v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05025</id>
        <link href="http://arxiv.org/abs/2104.05025"/>
        <updated>2021-07-01T01:59:33.750Z</updated>
        <summary type="html"><![CDATA[In the online continual learning paradigm, agents must learn from a changing
distribution while respecting memory and compute constraints. Previous work in
this setting often tries to reduce catastrophic forgetting by limiting changes
in the space of model parameters. In this work we instead focus on the change
in representations of observed data that arises when previously unobserved
classes appear in the incoming data stream, and new classes must be
distinguished from previous ones. Starting from a popular approach, experience
replay, we consider metric learning based loss functions which, when adjusted
to appropriately select negative samples, can effectively nudge the learned
representations to be more robust to new future classes. We show that this
selection of negatives is in fact critical for reducing representation drift of
previously observed data. Motivated by this we further introduce a simple
adjustment to the standard cross entropy loss used in prior experience replay
that achieves similar effect. Our approach directly improves the performance of
experience replay for this setting, obtaining state-of-the-art results on
several existing benchmarks in online continual learning, while remaining
efficient in both memory and compute. We release an implementation of our
experiments at https://github.com/naderAsadi/AML]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1"&gt;Lucas Caccia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1"&gt;Rahaf Aljundi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asadi_N/0/1/0/all/0/1"&gt;Nader Asadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1"&gt;Tinne Tuytelaars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1"&gt;Eugene Belilovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Linear Networks Dynamics: Low-Rank Biases Induced by Initialization Scale and L2 Regularization. (arXiv:2106.15933v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15933</id>
        <link href="http://arxiv.org/abs/2106.15933"/>
        <updated>2021-07-01T01:59:33.744Z</updated>
        <summary type="html"><![CDATA[For deep linear networks (DLN), various hyperparameters alter the dynamics of
training dramatically. We investigate how the rank of the linear map found by
gradient descent is affected by (1) the initialization norm and (2) the
addition of $L_{2}$ regularization on the parameters. For (1), we study two
regimes: (1a) the linear/lazy regime, for large norm initialization; (1b) a
\textquotedbl saddle-to-saddle\textquotedbl{} regime for small initialization
norm. In the (1a) setting, the dynamics of a DLN of any depth is similar to
that of a standard linear model, without any low-rank bias. In the (1b)
setting, we conjecture that throughout training, gradient descent approaches a
sequence of saddles, each corresponding to linear maps of increasing rank,
until reaching a minimal rank global minimum. We support this conjecture with a
partial proof and some numerical experiments. For (2), we show that adding a
$L_{2}$ regularization on the parameters corresponds to the addition to the
cost of a $L_{p}$-Schatten (quasi)norm on the linear map with $p=\frac{2}{L}$
(for a depth-$L$ network), leading to a stronger low-rank bias as $L$ grows.
The effect of $L_{2}$ regularization on the loss surface depends on the depth:
for shallow networks, all critical points are either strict saddles or global
minima, whereas for deep networks, some local minima appear. We numerically
observe that these local minima can generalize better than global ones in some
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jacot_A/0/1/0/all/0/1"&gt;Arthur Jacot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ged_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Ged&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gabriel_F/0/1/0/all/0/1"&gt;Franck Gabriel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Simsek_B/0/1/0/all/0/1"&gt;Berfin &amp;#x15e;im&amp;#x15f;ek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hongler_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Hongler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions. (arXiv:1301.6697v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1301.6697</id>
        <link href="http://arxiv.org/abs/1301.6697"/>
        <updated>2021-07-01T01:59:33.737Z</updated>
        <summary type="html"><![CDATA[We show that the only parameter prior for complete Gaussian DAG models that
satisfies global parameter independence, complete model equivalence, and some
weak regularity assumptions, is the normal-Wishart distribution. Our analysis
is based on the following new characterization of the Wishart distribution: let
W be an n x n, n >= 3, positive-definite symmetric matrix of random variables
and f(W) be a pdf of W. Then, f(W) is a Wishart distribution if and only if
W_{11}-W_{12}W_{22}^{-1}W_{12}' is independent of {W_{12}, W_{22}} for every
block partitioning W_{11}, W_{12}, W_{12}', W_{22} of W. Similar
characterizations of the normal and normal-Wishart distributions are provided
as well. We also show how to construct a prior for every DAG model over X from
the prior of a single regression model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1"&gt;Dan Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1"&gt;David Heckerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11866</id>
        <link href="http://arxiv.org/abs/2105.11866"/>
        <updated>2021-07-01T01:59:33.731Z</updated>
        <summary type="html"><![CDATA[Factorization machine (FM) is a prevalent approach to modeling pairwise
(second-order) feature interactions when dealing with high-dimensional sparse
data. However, on the one hand, FM fails to capture higher-order feature
interactions suffering from combinatorial expansion, on the other hand, taking
into account interaction between every pair of features may introduce noise and
degrade prediction accuracy. To solve the problems, we propose a novel approach
Graph Factorization Machine (GraphFM) by naturally representing features in the
graph structure. In particular, a novel mechanism is designed to select the
beneficial feature interactions and formulate them as edges between features.
Then our proposed model which integrates the interaction function of FM into
the feature aggregation strategy of Graph Neural Network (GNN), can model
arbitrary-order feature interactions on the graph-structured features by
stacking layers. Experimental results on several real-world datasets has
demonstrated the rationality and effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zeyu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information. (arXiv:2106.15991v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15991</id>
        <link href="http://arxiv.org/abs/2106.15991"/>
        <updated>2021-07-01T01:59:33.725Z</updated>
        <summary type="html"><![CDATA[This article presents a novel approach to incorporate visual cues from
video-data from a wide-angle stereo camera system mounted at an urban
intersection into the forecast of cyclist trajectories. We extract features
from image and optical flow (OF) sequences using 3D convolutional neural
networks (3D-ConvNet) and combine them with features extracted from the
cyclist's past trajectory to forecast future cyclist positions. By the use of
additional information, we are able to improve positional accuracy by about 7.5
% for our test dataset and by up to 22 % for specific motion types compared to
a method solely based on past trajectories. Furthermore, we compare the use of
image sequences to the use of OF sequences as additional information, showing
that OF alone leads to significant improvements in positional accuracy. By
training and testing our methods using a real-world dataset recorded at a
heavily frequented public intersection and evaluating the methods' runtimes, we
demonstrate the applicability in real traffic scenarios. Our code and parts of
our dataset are made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1"&gt;Stefan Zernetsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trupp_O/0/1/0/all/0/1"&gt;Oliver Trupp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1"&gt;Viktor Kress&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1"&gt;Konrad Doll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1"&gt;Bernhard Sick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recognizing Facial Expressions in the Wild using Multi-Architectural Representations based Ensemble Learning with Distillation. (arXiv:2106.16126v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16126</id>
        <link href="http://arxiv.org/abs/2106.16126"/>
        <updated>2021-07-01T01:59:33.706Z</updated>
        <summary type="html"><![CDATA[Facial expressions are the most universal forms of body language and
automatic facial expression recognition is one of the challenging tasks due to
different uncertainties. However, it has been an active field of research for
many years. Nevertheless, efficiency and performance are yet essential aspects
for building robust systems. We proposed two models, EmoXNet which is an
ensemble learning technique for learning convoluted facial representations, and
EmoXNetLite which is a distillation technique that is useful for transferring
the knowledge from our ensemble model to an efficient deep neural network using
label-smoothen soft labels for able to effectively detect expressions in
real-time. Both of the techniques performed quite well, where the ensemble
model (EmoXNet) helped to achieve 85.07% test accuracy on FER2013 with FER+
annotations and 86.25% test accuracy on RAF-DB. Moreover, the distilled model
(EmoXNetLite) showed 82.07% test accuracy on FER2013 with FER+ annotations and
81.78% test accuracy on RAF-DB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Momin_R/0/1/0/all/0/1"&gt;Rauf Momin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momin_A/0/1/0/all/0/1"&gt;Ali Shan Momin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1"&gt;Khalid Rasheed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms. (arXiv:2106.16174v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.16174</id>
        <link href="http://arxiv.org/abs/2106.16174"/>
        <updated>2021-07-01T01:59:33.700Z</updated>
        <summary type="html"><![CDATA[The cells and their spatial patterns in the tumor microenvironment (TME) play
a key role in tumor evolution, and yet remains an understudied topic in
computational pathology. This study, to the best of our knowledge, is among the
first to hybrid local and global graph methods to profile orchestration and
interaction of cellular components. To address the challenge in hematolymphoid
cancers where the cell classes in TME are unclear, we first implemented cell
level unsupervised learning and identified two new cell subtypes. Local cell
graphs or supercells were built for each image by considering the individual
cell's geospatial location and classes. Then, we applied supercell level
clustering and identified two new cell communities. In the end, we built global
graphs to abstract spatial interaction patterns and extract features for
disease diagnosis. We evaluate the proposed algorithm on H\&E slides of 60
hematolymphoid neoplasm patients and further compared it with three cell level
graph-based algorithms, including the global cell graph, cluster cell graph,
and FLocK. The proposed algorithm achieves a mean diagnosis accuracy of 0.703
with the repeated 5-fold cross-validation scheme. In conclusion, our algorithm
shows superior performance over the existing methods and can be potentially
applied to other cancer types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pingjun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Aminu_M/0/1/0/all/0/1"&gt;Muhammad Aminu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hussein_S/0/1/0/all/0/1"&gt;Siba El Hussein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Khoury_J/0/1/0/all/0/1"&gt;Joseph Khoury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jia Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Shortcuts for Vision Transformers. (arXiv:2106.15941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15941</id>
        <link href="http://arxiv.org/abs/2106.15941"/>
        <updated>2021-07-01T01:59:33.694Z</updated>
        <summary type="html"><![CDATA[Transformer models have achieved great progress on computer vision tasks
recently. The rapid development of vision transformers is mainly contributed by
their high representation ability for extracting informative features from
input images. However, the mainstream transformer models are designed with deep
architectures, and the feature diversity will be continuously reduced as the
depth increases, i.e., feature collapse. In this paper, we theoretically
analyze the feature collapse phenomenon and study the relationship between
shortcuts and feature diversity in these transformer models. Then, we present
an augmented shortcut scheme, which inserts additional paths with learnable
parameters in parallel on the original shortcuts. To save the computational
costs, we further explore an efficient approach that uses the block-circulant
projection to implement augmented shortcuts. Extensive experiments conducted on
benchmark datasets demonstrate the effectiveness of the proposed method, which
brings about 1% accuracy increase of the state-of-the-art visual transformers
without obviously increasing their parameters and FLOPs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;An Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yiping Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Scale Spectrogram Modelling for Neural Text-to-Speech. (arXiv:2106.15649v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.15649</id>
        <link href="http://arxiv.org/abs/2106.15649"/>
        <updated>2021-07-01T01:59:33.688Z</updated>
        <summary type="html"><![CDATA[We propose a novel Multi-Scale Spectrogram (MSS) modelling approach to
synthesise speech with an improved coarse and fine-grained prosody. We present
a generic multi-scale spectrogram prediction mechanism where the system first
predicts coarser scale mel-spectrograms that capture the suprasegmental
information in speech, and later uses these coarser scale mel-spectrograms to
predict finer scale mel-spectrograms capturing fine-grained prosody.

We present details for two specific versions of MSS called Word-level MSS and
Sentence-level MSS where the scales in our system are motivated by the
linguistic units. The Word-level MSS models word, phoneme, and frame-level
spectrograms while Sentence-level MSS models sentence-level spectrogram in
addition.

Subjective evaluations show that Word-level MSS performs statistically
significantly better compared to the baseline on two voices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1"&gt;Ammar Abbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bollepalli_B/0/1/0/all/0/1"&gt;Bajibabu Bollepalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1"&gt;Alexis Moinet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1"&gt;Arnaud Joly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1"&gt;Penny Karanasou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Makarov_P/0/1/0/all/0/1"&gt;Peter Makarov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Slangens_S/0/1/0/all/0/1"&gt;Simon Slangens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karlapati_S/0/1/0/all/0/1"&gt;Sri Karlapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1"&gt;Thomas Drugman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faithful Edge Federated Learning: Scalability and Privacy. (arXiv:2106.15905v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15905</id>
        <link href="http://arxiv.org/abs/2106.15905"/>
        <updated>2021-07-01T01:59:33.670Z</updated>
        <summary type="html"><![CDATA[Federated learning enables machine learning algorithms to be trained over a
network of multiple decentralized edge devices without requiring the exchange
of local datasets. Successfully deploying federated learning requires ensuring
that agents (e.g., mobile devices) faithfully execute the intended algorithm,
which has been largely overlooked in the literature. In this study, we first
use risk bounds to analyze how the key feature of federated learning,
unbalanced and non-i.i.d. data, affects agents' incentives to voluntarily
participate and obediently follow traditional federated learning algorithms.

To be more specific, our analysis reveals that agents with less typical data
distributions and relatively more samples are more likely to opt out of or
tamper with federated learning algorithms. To this end, we formulate the first
faithful implementation problem of federated learning and design two faithful
federated learning mechanisms which satisfy economic properties, scalability,
and privacy. Further, the time complexity of computing all agents' payments in
the number of agents is $\mathcal{O}(1)$. First, we design a Faithful Federated
Learning (FFL) mechanism which approximates the Vickrey-Clarke-Groves (VCG)
payments via an incremental computation. We show that it achieves (probably
approximate) optimality, faithful implementation, voluntary participation, and
some other economic properties (such as budget balance). Second, by
partitioning agents into several subsets, we present a scalable VCG mechanism
approximation. We further design a scalable and Differentially Private FFL
(DP-FFL) mechanism, the first differentially private faithful mechanism, that
maintains the economic properties. Our mechanism enables one to make three-way
performance tradeoffs among privacy, the iterations needed, and payment
accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Meng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_E/0/1/0/all/0/1"&gt;Ermin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berry_R/0/1/0/all/0/1"&gt;Randall Berry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Generative Utility of Cyclic Conditionals. (arXiv:2106.15962v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15962</id>
        <link href="http://arxiv.org/abs/2106.15962"/>
        <updated>2021-07-01T01:59:33.664Z</updated>
        <summary type="html"><![CDATA[We study whether and how can we model a joint distribution $p(x,z)$ using two
conditional models $p(x|z)$ and $q(z|x)$ that form a cycle. This is motivated
by the observation that deep generative models, in addition to a likelihood
model $p(x|z)$, often also use an inference model $q(z|x)$ for data
representation, but they rely on a usually uninformative prior distribution
$p(z)$ to define a joint distribution, which may render problems like posterior
collapse and manifold mismatch. To explore the possibility to model a joint
distribution using only $p(x|z)$ and $q(z|x)$, we study their compatibility and
determinacy, corresponding to the existence and uniqueness of a joint
distribution whose conditional distributions coincide with them. We develop a
general theory for novel and operable equivalence criteria for compatibility,
and sufficient conditions for determinacy. Based on the theory, we propose the
CyGen framework for cyclic-conditional generative modeling, including methods
to enforce compatibility and use the determined distribution to fit and
generate data. With the prior constraint removed, CyGen better fits data and
captures more representative features, supported by experiments showing better
generation and downstream classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haoyue Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10013</id>
        <link href="http://arxiv.org/abs/2105.10013"/>
        <updated>2021-07-01T01:59:33.658Z</updated>
        <summary type="html"><![CDATA[Image classification methods are usually trained to perform predictions
taking into account a predefined group of known classes. Real-world problems,
however, may not allow for a full knowledge of the input and label spaces,
making failures in recognition a hazard to deep visual learning. Open set
recognition methods are characterized by the ability to correctly identify
inputs of known and unknown classes. In this context, we propose GeMOS: simple
and plug-and-play open set recognition modules that can be attached to
pretrained Deep Neural Networks for visual recognition. The GeMOS framework
pairs pre-trained Convolutional Neural Networks with generative models for open
set recognition to extract open set scores for each sample, allowing for
failure recognition in object recognition tasks. We conduct a thorough
evaluation of the proposed method in comparison with state-of-the-art open set
algorithms, finding that GeMOS either outperforms or is statistically
indistinguishable from more complex and costly models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1"&gt;Marcos Vendramini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1"&gt;Alexei Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Super-Resolution via Iterative Refinement. (arXiv:2104.07636v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07636</id>
        <link href="http://arxiv.org/abs/2104.07636"/>
        <updated>2021-07-01T01:59:33.652Z</updated>
        <summary type="html"><![CDATA[We present SR3, an approach to image Super-Resolution via Repeated
Refinement. SR3 adapts denoising diffusion probabilistic models to conditional
image generation and performs super-resolution through a stochastic denoising
process. Inference starts with pure Gaussian noise and iteratively refines the
noisy output using a U-Net model trained on denoising at various noise levels.
SR3 exhibits strong performance on super-resolution tasks at different
magnification factors, on faces and natural images. We conduct human evaluation
on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA
GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic
outputs, while GANs do not exceed a fool rate of 34%. We further show the
effectiveness of SR3 in cascaded image generation, where generative models are
chained with super-resolution models, yielding a competitive FID score of 11.3
on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saharia_C/0/1/0/all/0/1"&gt;Chitwan Saharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1"&gt;William Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salimans_T/0/1/0/all/0/1"&gt;Tim Salimans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions. (arXiv:2105.03248v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03248</id>
        <link href="http://arxiv.org/abs/2105.03248"/>
        <updated>2021-07-01T01:59:33.646Z</updated>
        <summary type="html"><![CDATA[We develop simple methods for constructing parameter priors for model choice
among Directed Acyclic Graphical (DAG) models. In particular, we introduce
several assumptions that permit the construction of parameter priors for a
large number of DAG models from a small set of assessments. We then present a
method for directly computing the marginal likelihood of every DAG model given
a random sample with no missing observations. We apply this methodology to
Gaussian DAG models which consist of a recursive set of linear regression
models. We show that the only parameter prior for complete Gaussian DAG models
that satisfies our assumptions is the normal-Wishart distribution. Our analysis
is based on the following new characterization of the Wishart distribution: let
$W$ be an $n \times n$, $n \ge 3$, positive-definite symmetric matrix of random
variables and $f(W)$ be a pdf of $W$. Then, f$(W)$ is a Wishart distribution if
and only if $W_{11} - W_{12} W_{22}^{-1} W'_{12}$ is independent of
$\{W_{12},W_{22}\}$ for every block partitioning $W_{11},W_{12}, W'_{12},
W_{22}$ of $W$. Similar characterizations of the normal and normal-Wishart
distributions are provided as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Geiger_D/0/1/0/all/0/1"&gt;Dan Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heckerman_D/0/1/0/all/0/1"&gt;David Heckerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DAEMA: Denoising Autoencoder with Mask Attention. (arXiv:2106.16057v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16057</id>
        <link href="http://arxiv.org/abs/2106.16057"/>
        <updated>2021-07-01T01:59:33.629Z</updated>
        <summary type="html"><![CDATA[Missing data is a recurrent and challenging problem, especially when using
machine learning algorithms for real-world applications. For this reason,
missing data imputation has become an active research area, in which recent
deep learning approaches have achieved state-of-the-art results. We propose
DAEMA (Denoising Autoencoder with Mask Attention), an algorithm based on a
denoising autoencoder architecture with an attention mechanism. While most
imputation algorithms use incomplete inputs as they would use complete data -
up to basic preprocessing (e.g. mean imputation) - DAEMA leverages a mask-based
attention mechanism to focus on the observed values of its inputs. We evaluate
DAEMA both in terms of reconstruction capabilities and downstream prediction
and show that it achieves superior performance to state-of-the-art algorithms
on several publicly available real-world datasets under various missingness
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tihon_S/0/1/0/all/0/1"&gt;Simon Tihon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javaid_M/0/1/0/all/0/1"&gt;Muhammad Usama Javaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fourure_D/0/1/0/all/0/1"&gt;Damien Fourure&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Posocco_N/0/1/0/all/0/1"&gt;Nicolas Posocco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peel_T/0/1/0/all/0/1"&gt;Thomas Peel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Signal Restoration Using Nested Deep Algorithm Unrolling. (arXiv:2106.15910v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.15910</id>
        <link href="http://arxiv.org/abs/2106.15910"/>
        <updated>2021-07-01T01:59:33.623Z</updated>
        <summary type="html"><![CDATA[Graph signal processing is a ubiquitous task in many applications such as
sensor, social, transportation and brain networks, point cloud processing, and
graph neural networks. Graph signals are often corrupted through sensing
processes, and need to be restored for the above applications. In this paper,
we propose two graph signal restoration methods based on deep algorithm
unrolling (DAU). First, we present a graph signal denoiser by unrolling
iterations of the alternating direction method of multiplier (ADMM). We then
propose a general restoration method for linear degradation by unrolling
iterations of Plug-and-Play ADMM (PnP-ADMM). In the second method, the unrolled
ADMM-based denoiser is incorporated as a submodule. Therefore, our restoration
method has a nested DAU structure. Thanks to DAU, parameters in the proposed
denoising/restoration methods are trainable in an end-to-end manner. Since the
proposed restoration methods are based on iterations of a (convex) optimization
algorithm, the method is interpretable and keeps the number of parameters small
because we only need to tune graph-independent regularization parameters. We
solve two main problems in existing graph signal restoration methods: 1)
limited performance of convex optimization algorithms due to fixed parameters
which are often determined manually. 2) large number of parameters of graph
neural networks that result in difficulty of training. Several experiments for
graph signal denoising and interpolation are performed on synthetic and
real-world data. The proposed methods show performance improvements to several
existing methods in terms of root mean squared error in both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nagahama_M/0/1/0/all/0/1"&gt;Masatoshi Nagahama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yamada_K/0/1/0/all/0/1"&gt;Koki Yamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tanaka_Y/0/1/0/all/0/1"&gt;Yuichi Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1"&gt;Stanley H. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles. (arXiv:2105.06456v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06456</id>
        <link href="http://arxiv.org/abs/2105.06456"/>
        <updated>2021-07-01T01:59:33.617Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce a corpus for satire detection in Romanian news. We
gathered 55,608 public news articles from multiple real and satirical news
sources, composing one of the largest corpora for satire detection regardless
of language and the only one for the Romanian language. We provide an official
split of the text samples, such that training news articles belong to different
sources than test news articles, thus ensuring that models do not achieve high
performance simply due to overfitting. We conduct experiments with two
state-of-the-art deep neural models, resulting in a set of strong baselines for
our novel corpus. Our results show that the machine-level accuracy for satire
detection in Romanian is quite low (under 73% on the test set) compared to the
human-level accuracy (87%), leaving enough room for improvement in future
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rogoz_A/0/1/0/all/0/1"&gt;Ana-Cristina Rogoz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1"&gt;Mihaela Gaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Informants: Relations between Learning Success Criteria. (arXiv:1801.10502v5 [cs.FL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1801.10502</id>
        <link href="http://arxiv.org/abs/1801.10502"/>
        <updated>2021-07-01T01:59:33.610Z</updated>
        <summary type="html"><![CDATA[Learning from positive and negative information, so-called \emph{informants},
being one of the models for human and machine learning introduced by
E.~M.~Gold, is investigated. Particularly, naturally arising questions about
this learning setting, originating in results on learning from solely positive
information, are answered. By a carefully arranged argument learners can be
assumed to only change their hypothesis in case it is inconsistent with the
data (such a learning behavior is called \emph{conservative}). The deduced main
theorem states the relations between the most important delayable learning
success criteria, being the ones not ruined by a delayed in time hypothesis
output. Additionally, our investigations concerning the non-delayable
requirement of consistent learning underpin the claim for \emph{delayability}
being the right structural property to gain a deeper understanding concerning
the nature of learning success criteria. Moreover, we obtain an anomalous
\emph{hierarchy} when allowing for an increasing finite number of
\emph{anomalies} of the hypothesized language by the learner compared with the
language to be learned. In contrast to the vacillatory hierarchy for learning
from solely positive information, we observe a \emph{duality} depending on
whether infinitely many \emph{vacillations} between different (almost) correct
hypotheses are still considered a successful learning behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aschenbach_M/0/1/0/all/0/1"&gt;Martin Aschenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotzing_T/0/1/0/all/0/1"&gt;Timo K&amp;#xf6;tzing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidel_K/0/1/0/all/0/1"&gt;Karen Seidel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. (arXiv:2106.05953v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05953</id>
        <link href="http://arxiv.org/abs/2106.05953"/>
        <updated>2021-07-01T01:59:33.603Z</updated>
        <summary type="html"><![CDATA[Acquiring accurate 3D annotated data for hand pose estimation is a
notoriously difficult problem. This typically requires complex multi-camera
setups and controlled conditions, which in turn creates a domain gap that is
hard to bridge to fully unconstrained settings. Encouraged by the success of
contrastive learning on image classification tasks, we propose a new
self-supervised method for the structured regression task of 3D hand pose
estimation. Contrastive learning makes use of unlabeled data for the purpose of
representation learning via a loss formulation that encourages the learned
feature representations to be invariant under any image transformation. For 3D
hand pose estimation, it too is desirable to have invariance to appearance
transformation such as color jitter. However, the task requires equivariance
under affine transformations, such as rotation and translation. To address this
issue, we propose an equivariant contrastive objective and demonstrate its
effectiveness in the context of 3D hand pose estimation. We experimentally
investigate the impact of invariant and equivariant contrastive objectives and
show that learning equivariant features leads to better representations for the
task of 3D hand pose estimation. Furthermore, we show that a standard
ResNet-152, trained on additional unlabeled data, attains an improvement of
$7.6\%$ in PA-EPE on FreiHAND and thus achieves state-of-the-art performance
without any task specific, specialized architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1"&gt;Adrian Spurr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_A/0/1/0/all/0/1"&gt;Aneesh Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xucong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1"&gt;Otmar Hilliges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATOM: Robustifying Out-of-distribution Detection Using Outlier Mining. (arXiv:2006.15207v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15207</id>
        <link href="http://arxiv.org/abs/2006.15207"/>
        <updated>2021-07-01T01:59:33.587Z</updated>
        <summary type="html"><![CDATA[Detecting out-of-distribution (OOD) inputs is critical for safely deploying
deep learning models in an open-world setting. However, existing OOD detection
solutions can be brittle in the open world, facing various types of adversarial
OOD inputs. While methods leveraging auxiliary OOD data have emerged, our
analysis on illuminative examples reveals a key insight that the majority of
auxiliary OOD examples may not meaningfully improve or even hurt the decision
boundary of the OOD detector, which is also observed in empirical results on
real data. In this paper, we provide a theoretically motivated method,
Adversarial Training with informative Outlier Mining (ATOM), which improves the
robustness of OOD detection. We show that, by mining informative auxiliary OOD
data, one can significantly improve OOD detection performance, and somewhat
surprisingly, generalize to unseen adversarial attacks. ATOM achieves
state-of-the-art performance under a broad family of classic and adversarial
OOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset,
ATOM reduces the FPR (at TPR 95%) by up to 57.99% under adversarial OOD inputs,
surpassing the previous best baseline by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingyu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Adversarial Attacks on Observations in Deep Reinforcement Learning. (arXiv:2106.15860v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15860</id>
        <link href="http://arxiv.org/abs/2106.15860"/>
        <updated>2021-07-01T01:59:33.581Z</updated>
        <summary type="html"><![CDATA[Recent works demonstrate that deep reinforcement learning (DRL) models are
vulnerable to adversarial attacks which can decrease the victim's total reward
by manipulating the observations. Compared with adversarial attacks in
supervised learning, it is much more challenging to deceive a DRL model since
the adversary has to infer the environmental dynamics. To address this issue,
we reformulate the problem of adversarial attacks in function space and
separate the previous gradient based attacks into several subspace. Following
the analysis of the function space, we design a generic two-stage framework in
the subspace where the adversary lures the agent to a target trajectory or a
deceptive policy. In the first stage, we train a deceptive policy by hacking
the environment, and discover a set of trajectories routing to the lowest
reward. The adversary then misleads the victim to imitate the deceptive policy
by perturbing the observations. Our method provides a tighter theoretical upper
bound for the attacked agent's performance than the existing approaches.
Extensive experiments demonstrate the superiority of our method and we achieve
the state-of-the-art performance on both Atari and MuJoCo environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiaoben_Y/0/1/0/all/0/1"&gt;You Qiaoben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1"&gt;Chengyang Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xinning Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transductive Zero-Shot Hashing for Multilabel Image Retrieval. (arXiv:1911.07192v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07192</id>
        <link href="http://arxiv.org/abs/1911.07192"/>
        <updated>2021-07-01T01:59:33.575Z</updated>
        <summary type="html"><![CDATA[Hash coding has been widely used in approximate nearest neighbor search for
large-scale image retrieval. Given semantic annotations such as class labels
and pairwise similarities of the training data, hashing methods can learn and
generate effective and compact binary codes. While some newly introduced images
may contain undefined semantic labels, which we call unseen images, zeor-shot
hashing techniques have been studied. However, existing zeor-shot hashing
methods focus on the retrieval of single-label images, and cannot handle
multi-label images. In this paper, for the first time, a novel transductive
zero-shot hashing method is proposed for multi-label unseen image retrieval. In
order to predict the labels of the unseen/target data, a visual-semantic bridge
is built via instance-concept coherence ranking on the seen/source data. Then,
pairwise similarity loss and focal quantization loss are constructed for
training a hashing model using both the seen/source and unseen/target data.
Extensive evaluations on three popular multi-label datasets demonstrate that,
the proposed hashing method achieves significantly better results than the
competing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1"&gt;Qin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Ling Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Song Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Testing under Superspreading Dynamics. (arXiv:2106.15988v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2106.15988</id>
        <link href="http://arxiv.org/abs/2106.15988"/>
        <updated>2021-07-01T01:59:33.568Z</updated>
        <summary type="html"><![CDATA[Testing is recommended for all close contacts of confirmed COVID-19 patients.
However, existing group testing methods are oblivious to the circumstances of
contagion provided by contact tracing. Here, we build upon a well-known
semi-adaptive pool testing method, Dorfman's method with imperfect tests, and
derive a simple group testing method based on dynamic programming that is
specifically designed to use the information provided by contact tracing.
Experiments using a variety of reproduction numbers and dispersion levels,
including those estimated in the context of the COVID-19 pandemic, show that
the pools found using our method result in a significantly lower number of
tests than those found using standard Dorfman's method, especially when the
number of contacts of an infected individual is small. Moreover, our results
show that our method can be more beneficial when the secondary infections are
highly overdispersed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tsirtsis_S/0/1/0/all/0/1"&gt;Stratis Tsirtsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+De_A/0/1/0/all/0/1"&gt;Abir De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lorch_L/0/1/0/all/0/1"&gt;Lars Lorch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1"&gt;Manuel Gomez-Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Align Yourself: Self-supervised Pre-training for Fine-grained Recognition via Saliency Alignment. (arXiv:2106.15788v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15788</id>
        <link href="http://arxiv.org/abs/2106.15788"/>
        <updated>2021-07-01T01:59:33.560Z</updated>
        <summary type="html"><![CDATA[Self-supervised contrastive learning has demonstrated great potential in
learning visual representations. Despite their success on various downstream
tasks such as image classification and object detection, self-supervised
pre-training for fine-grained scenarios is not fully explored. In this paper,
we first point out that current contrastive methods are prone to memorizing
background/foreground texture and therefore have a limitation in localizing the
foreground object. Analysis suggests that learning to extract discriminative
texture information and localization are equally crucial for self-supervised
pre-training under fine-grained scenarios. Based on our findings, we introduce
Cross-view Saliency Alignment (CVSA), a contrastive learning framework that
first crops and swaps saliency regions of images as a novel view generation and
then guides the model to localize on the foreground object via a cross-view
alignment loss. Extensive experiments on four popular fine-grained
classification benchmarks show that CVSA significantly improves the learned
representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Siyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1"&gt;Zelin Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1"&gt;Lei Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Baigui Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan Z. Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advantages and Bottlenecks of Quantum Machine Learning for Remote Sensing. (arXiv:2101.10657v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10657</id>
        <link href="http://arxiv.org/abs/2101.10657"/>
        <updated>2021-07-01T01:59:33.543Z</updated>
        <summary type="html"><![CDATA[This concept paper aims to provide a brief outline of quantum computers,
explore existing methods of quantum image classification techniques, so
focusing on remote sensing applications, and discuss the bottlenecks of
performing these algorithms on currently available open source platforms.
Initial results demonstrate feasibility. Next steps include expanding the size
of the quantum hidden layer and increasing the variety of output image options.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zaidenberg_D/0/1/0/all/0/1"&gt;Daniela A. Zaidenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sebastianelli_A/0/1/0/all/0/1"&gt;Alessandro Sebastianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Spiller_D/0/1/0/all/0/1"&gt;Dario Spiller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Saux_B/0/1/0/all/0/1"&gt;Bertrand Le Saux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ullo_S/0/1/0/all/0/1"&gt;Silvia Liberata Ullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09199</id>
        <link href="http://arxiv.org/abs/2006.09199"/>
        <updated>2021-07-01T01:59:33.537Z</updated>
        <summary type="html"><![CDATA[Current methods for learning visually grounded language from videos often
rely on text annotation, such as human generated captions or machine generated
automatic speech recognition (ASR) transcripts. In this work, we introduce the
Audio-Video Language Network (AVLnet), a self-supervised network that learns a
shared audio-visual embedding space directly from raw video inputs. To
circumvent the need for text annotation, we learn audio-visual representations
from randomly segmented video clips and their raw audio waveforms. We train
AVLnet on HowTo100M, a large corpus of publicly available instructional videos,
and evaluate on image retrieval and video retrieval tasks, achieving
state-of-the-art performance. We perform analysis of AVLnet's learned
representations, showing our model utilizes speech and natural sounds to learn
audio-visual concepts. Further, we propose a tri-modal model that jointly
processes raw audio, video, and text captions from videos to learn a
multi-modal semantic embedding space useful for text-video retrieval. Our code,
data, and trained models will be released at avlnet.csail.mit.edu]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1"&gt;Andrew Rouditchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1"&gt;Angie Boggust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1"&gt;David Harwath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Brian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1"&gt;Dhiraj Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1"&gt;Samuel Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1"&gt;Kartik Audhkhasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1"&gt;Hilde Kuehne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1"&gt;Brian Kingsbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1"&gt;Michael Picheny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1"&gt;James Glass&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Belief Learning. (arXiv:2103.04000v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04000</id>
        <link href="http://arxiv.org/abs/2103.04000"/>
        <updated>2021-07-01T01:59:33.530Z</updated>
        <summary type="html"><![CDATA[The standard problem setting in Dec-POMDPs is self-play, where the goal is to
find a set of policies that play optimally together. Policies learned through
self-play may adopt arbitrary conventions and implicitly rely on multi-step
reasoning based on fragile assumptions about other agents' actions and thus
fail when paired with humans or independently trained agents at test time. To
address this, we present off-belief learning (OBL). At each timestep OBL agents
follow a policy $\pi_1$ that is optimized assuming past actions were taken by a
given, fixed policy ($\pi_0$), but assuming that future actions will be taken
by $\pi_1$. When $\pi_0$ is uniform random, OBL converges to an optimal policy
that does not rely on inferences based on other agents' behavior (an optimal
grounded policy). OBL can be iterated in a hierarchy, where the optimal policy
from one level becomes the input to the next, thereby introducing multi-level
cognitive reasoning in a controlled manner. Unlike existing approaches, which
may converge to any equilibrium policy, OBL converges to a unique policy,
making it suitable for zero-shot coordination (ZSC). OBL can be scaled to
high-dimensional settings with a fictitious transition mechanism and shows
strong performance in both a toy-setting and the benchmark human-AI & ZSC
problem Hanabi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hengyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1"&gt;Adam Lerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Brandon Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;David Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1"&gt;Luis Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1"&gt;Noam Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1"&gt;Jakob Foerster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Modeling with Reduced Densities. (arXiv:2007.03834v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03834</id>
        <link href="http://arxiv.org/abs/2007.03834"/>
        <updated>2021-07-01T01:59:33.524Z</updated>
        <summary type="html"><![CDATA[This work originates from the observation that today's state of the art
statistical language models are impressive not only for their performance, but
also - and quite crucially - because they are built entirely from correlations
in unstructured text data. The latter observation prompts a fundamental
question that lies at the heart of this paper: What mathematical structure
exists in unstructured text data? We put forth enriched category theory as a
natural answer. We show that sequences of symbols from a finite alphabet, such
as those found in a corpus of text, form a category enriched over
probabilities. We then address a second fundamental question: How can this
information be stored and modeled in a way that preserves the categorical
structure? We answer this by constructing a functor from our enriched category
of text to a particular enriched category of reduced density operators. The
latter leverages the Loewner order on positive semidefinite operators, which
can further be interpreted as a toy example of entailment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bradley_T/0/1/0/all/0/1"&gt;Tai-Danae Bradley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlassopoulos_Y/0/1/0/all/0/1"&gt;Yiannis Vlassopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Famous Companies Use More Letters in Logo:A Large-Scale Analysis of Text Area in Logo. (arXiv:2104.00327v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00327</id>
        <link href="http://arxiv.org/abs/2104.00327"/>
        <updated>2021-07-01T01:59:33.518Z</updated>
        <summary type="html"><![CDATA[This paper analyzes a large number of logo images from the LLD-logo dataset,
by recent deep learning-based techniques, to understand not only design trends
of logo images and but also the correlation to their owner company. Especially,
we focus on three correlations between logo images and their text areas,
between the text areas and the number of followers on Twitter, and between the
logo images and the number of followers. Various findings include the weak
positive correlation between the text area ratio and the number of followers of
the company. In addition, deep regression and deep ranking methods can catch
correlations between the logo images and the number of followers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nishi_S/0/1/0/all/0/1"&gt;Shintaro Nishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadota_T/0/1/0/all/0/1"&gt;Takeaki Kadota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16245</id>
        <link href="http://arxiv.org/abs/2106.16245"/>
        <updated>2021-07-01T01:59:33.501Z</updated>
        <summary type="html"><![CDATA[Model-agnostic meta-learning (MAML) is arguably the most popular
meta-learning algorithm nowadays, given its flexibility to incorporate various
model architectures and to be applied to different problems. Nevertheless, its
performance on few-shot classification is far behind many recent algorithms
dedicated to the problem. In this paper, we point out several key facets of how
to train MAML to excel in few-shot classification. First, we find that a large
number of gradient steps are needed for the inner loop update, which
contradicts the common usage of MAML for few-shot classification. Second, we
find that MAML is sensitive to the permutation of class assignments in
meta-testing: for a few-shot task of $N$ classes, there are exponentially many
ways to assign the learned initialization of the $N$-way classifier to the $N$
classes, leading to an unavoidably huge variance. Third, we investigate several
ways for permutation invariance and find that learning a shared classifier
initialization for all the classes performs the best. On benchmark datasets
such as MiniImageNet and TieredImageNet, our approach, which we name
UNICORN-MAML, performs on a par with or even outperforms state-of-the-art
algorithms, while keeping the simplicity of MAML without adding any extra
sub-networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1"&gt;Wei-Lun Chao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What can linear interpolation of neural network loss landscapes tell us?. (arXiv:2106.16004v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16004</id>
        <link href="http://arxiv.org/abs/2106.16004"/>
        <updated>2021-07-01T01:59:33.495Z</updated>
        <summary type="html"><![CDATA[Studying neural network loss landscapes provides insights into the nature of
the underlying optimization problems. Unfortunately, loss landscapes are
notoriously difficult to visualize in a human-comprehensible fashion. One
common way to address this problem is to plot linear slices of the landscape,
for example from the initial state of the network to the final state after
optimization. On the basis of this analysis, prior work has drawn broader
conclusions about the difficulty of the optimization problem. In this paper, we
put inferences of this kind to the test, systematically evaluating how linear
interpolation and final performance vary when altering the data, choice of
initialization, and other optimizer and architecture design choices. Further,
we use linear interpolation to study the role played by individual layers and
substructures of the network. We find that certain layers are more sensitive to
the choice of initialization and optimizer hyperparameter settings, and we
exploit these observations to design custom optimization schemes. However, our
results cast doubt on the broader intuition that the presence or absence of
barriers when interpolating necessarily relates to the success of optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1"&gt;Tiffany Vlaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1"&gt;Jonathan Frankle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Small in-distribution changes in 3D perspective and lighting fool both CNNs and Transformers. (arXiv:2106.16198v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16198</id>
        <link href="http://arxiv.org/abs/2106.16198"/>
        <updated>2021-07-01T01:59:33.489Z</updated>
        <summary type="html"><![CDATA[Neural networks are susceptible to small transformations including 2D
rotations and shifts, image crops, and even changes in object colors. This is
often attributed to biases in the training dataset, and the lack of 2D
shift-invariance due to not respecting the sampling theorem. In this paper, we
challenge this hypothesis by training and testing on unbiased datasets, and
showing that networks are brittle to both small 3D perspective changes and
lighting variations which cannot be explained by dataset bias or lack of
shift-invariance. To find these in-distribution errors, we introduce an
evolution strategies (ES) based approach, which we call CMA-Search. Despite
training with a large-scale (0.5 million images), unbiased dataset of camera
and light variations, in over 71% cases CMA-Search can find camera parameters
in the vicinity of a correctly classified image which lead to in-distribution
misclassifications with < 3.6% change in parameters. With lighting changes,
CMA-Search finds misclassifications in 33% cases with < 11.6% change in
parameters. Finally, we extend this method to find misclassifications in the
vicinity of ImageNet images for both ResNet and OpenAI's CLIP model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1"&gt;Spandan Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tzu-Mao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1"&gt;Hanspeter Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FROCC: Fast Random projection-based One-Class Classification. (arXiv:2011.14317v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14317</id>
        <link href="http://arxiv.org/abs/2011.14317"/>
        <updated>2021-07-01T01:59:33.483Z</updated>
        <summary type="html"><![CDATA[We present Fast Random projection-based One-Class Classification (FROCC), an
extremely efficient method for one-class classification. Our method is based on
a simple idea of transforming the training data by projecting it onto a set of
random unit vectors that are chosen uniformly and independently from the unit
sphere, and bounding the regions based on separation of the data. FROCC can be
naturally extended with kernels. We theoretically prove that FROCC generalizes
well in the sense that it is stable and has low bias. FROCC achieves up to 3.1
percent points better ROC, with 1.2--67.8x speedup in training and test times
over a range of state-of-the-art benchmarks including the SVM and the deep
learning based models for the OCC task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Arindam Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1"&gt;Sumanth Varambally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1"&gt;Amitabha Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1"&gt;Srikanta Bedathur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving Metric Learning for Incremental and Decremental Features. (arXiv:2006.15334v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15334</id>
        <link href="http://arxiv.org/abs/2006.15334"/>
        <updated>2021-07-01T01:59:33.465Z</updated>
        <summary type="html"><![CDATA[Online metric learning has been widely exploited for large-scale data
classification due to the low computational cost. However, amongst online
practical scenarios where the features are evolving (e.g., some features are
vanished and some new features are augmented), most metric learning models
cannot be successfully applied to these scenarios, although they can tackle the
evolving instances efficiently. To address the challenge, we develop a new
online Evolving Metric Learning (EML) model for incremental and decremental
features, which can handle the instance and feature evolutions simultaneously
by incorporating with a smoothed Wasserstein metric distance. Specifically, our
model contains two essential stages: a Transforming stage (T-stage) and a
Inheriting stage (I-stage). For the T-stage, we propose to extract important
information from vanished features while neglecting non-informative knowledge,
and forward it into survived features by transforming them into a low-rank
discriminative metric space. It further explores the intrinsic low-rank
structure of heterogeneous samples to reduce the computation and memory burden
especially for highly-dimensional large-scale data. For the I-stage, we inherit
the metric performance of survived features from the T-stage and then expand to
include the new augmented features. Moreover, a smoothed Wasserstein distance
is utilized to characterize the similarity relationships among the
heterogeneous and complex samples, since the evolving features are not strictly
aligned in the different stages. In addition to tackling the challenges in
one-shot case, we also extend our model into multishot scenario. After deriving
an efficient optimization strategy for both T-stage and I-stage, extensive
experiments on several datasets verify the superior performance of our EML
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jiahua Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1"&gt;Yang Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Gan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaowei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning. (arXiv:2106.12407v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12407</id>
        <link href="http://arxiv.org/abs/2106.12407"/>
        <updated>2021-07-01T01:59:33.459Z</updated>
        <summary type="html"><![CDATA[Fetal motion is unpredictable and rapid on the scale of conventional MR scan
times. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and
dynamics of fetal function, is limited to fast imaging techniques with
compromises in image quality and resolution. Super-resolution for dynamic fetal
MRI is still a challenge, especially when multi-oriented stacks of image slices
for oversampling are not available and high temporal resolution for recording
the dynamics of the fetus or placenta is desired. Further, fetal motion makes
it difficult to acquire high-resolution images for supervised learning methods.
To address this problem, in this work, we propose STRESS (Spatio-Temporal
Resolution Enhancement with Simulated Scans), a self-supervised
super-resolution framework for dynamic fetal MRI with interleaved slice
acquisitions. Our proposed method simulates an interleaved slice acquisition
along the high-resolution axis on the originally acquired data to generate
pairs of low- and high-resolution images. Then, it trains a super-resolution
network by exploiting both spatial and temporal correlations in the MR time
series, which is used to enhance the resolution of the original data.
Evaluations on both simulated and in utero data show that our proposed method
outperforms other self-supervised super-resolution methods and improves image
quality, which is beneficial to other downstream tasks and evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junshen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turk_E/0/1/0/all/0/1"&gt;Esra Abaci Turk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grant_P/0/1/0/all/0/1"&gt;P. Ellen Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1"&gt;Elfar Adalsteinsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concentration of Non-Isotropic Random Tensors with Applications to Learning and Empirical Risk Minimization. (arXiv:2102.04259v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04259</id>
        <link href="http://arxiv.org/abs/2102.04259"/>
        <updated>2021-07-01T01:59:33.418Z</updated>
        <summary type="html"><![CDATA[Dimension is an inherent bottleneck to some modern learning tasks, where
optimization methods suffer from the size of the data. In this paper, we study
non-isotropic distributions of data and develop tools that aim at reducing
these dimensional costs by a dependency on an effective dimension rather than
the ambient one. Based on non-asymptotic estimates of the metric entropy of
ellipsoids -- that prove to generalize to infinite dimensions -- and on a
chaining argument, our uniform concentration bounds involve an effective
dimension instead of the global dimension, improving over existing results. We
show the importance of taking advantage of non-isotropic properties in learning
problems with the following applications: i) we improve state-of-the-art
results in statistical preconditioning for communication-efficient distributed
optimization, ii) we introduce a non-isotropic randomized smoothing for
non-smooth optimization. Both applications cover a class of functions that
encompasses empirical risk minization (ERM) for linear models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Even_M/0/1/0/all/0/1"&gt;Mathieu Even&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1"&gt;Laurent Massouli&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pros and Cons of GAN Evaluation Measures: New Developments. (arXiv:2103.09396v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09396</id>
        <link href="http://arxiv.org/abs/2103.09396"/>
        <updated>2021-07-01T01:59:33.411Z</updated>
        <summary type="html"><![CDATA[This work is an update of a previous paper on the same topic published a few
years ago. With the dramatic progress in generative modeling, a suite of new
quantitative and qualitative techniques to evaluate models has emerged.
Although some measures such as Inception Score, Frechet Inception Distance,
Precision-Recall, and Perceptual Path Length are relatively more popular, GAN
evaluation is not a settled issue and there is still room for improvement.
Here, I describe new dimensions that are becoming important in assessing models
(e.g. bias and fairness) and discuss the connection between GAN evaluation and
deepfakes. These are important areas of concern in the machine learning
community today and progress in GAN evaluation can help mitigate them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1"&gt;Ali Borji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explanation-Guided Diagnosis of Machine Learning Evasion Attacks. (arXiv:2106.15820v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.15820</id>
        <link href="http://arxiv.org/abs/2106.15820"/>
        <updated>2021-07-01T01:59:33.406Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) models are susceptible to evasion attacks. Evasion
accuracy is typically assessed using aggregate evasion rate, and it is an open
question whether aggregate evasion rate enables feature-level diagnosis on the
effect of adversarial perturbations on evasive predictions. In this paper, we
introduce a novel framework that harnesses explainable ML methods to guide
high-fidelity assessment of ML evasion attacks. Our framework enables
explanation-guided correlation analysis between pre-evasion perturbations and
post-evasion explanations. Towards systematic assessment of ML evasion attacks,
we propose and evaluate a novel suite of model-agnostic metrics for
sample-level and dataset-level correlation analysis. Using malware and image
classifiers, we conduct comprehensive evaluations across diverse model
architectures and complementary feature representations. Our explanation-guided
correlation analysis reveals correlation gaps between adversarial samples and
the corresponding perturbations performed on them. Using a case study on
explanation-guided evasion, we show the broader usage of our methodology for
assessing robustness of ML models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amich_A/0/1/0/all/0/1"&gt;Abderrahmen Amich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eshete_B/0/1/0/all/0/1"&gt;Birhanu Eshete&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge Proposal Sets for Link Prediction. (arXiv:2106.15810v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.15810</id>
        <link href="http://arxiv.org/abs/2106.15810"/>
        <updated>2021-07-01T01:59:33.389Z</updated>
        <summary type="html"><![CDATA[Graphs are a common model for complex relational data such as social networks
and protein interactions, and such data can evolve over time (e.g., new
friendships) and be noisy (e.g., unmeasured interactions). Link prediction aims
to predict future edges or infer missing edges in the graph, and has diverse
applications in recommender systems, experimental design, and complex systems.
Even though link prediction algorithms strongly depend on the set of edges in
the graph, existing approaches typically do not modify the graph topology to
improve performance. Here, we demonstrate how simply adding a set of edges,
which we call a \emph{proposal set}, to the graph as a pre-processing step can
improve the performance of several link prediction algorithms. The underlying
idea is that if the edges in the proposal set generally align with the
structure of the graph, link prediction algorithms are further guided towards
predicting the right edges; in other words, adding a proposal set of edges is a
signal-boosting pre-processing step. We show how to use existing link
prediction algorithms to generate effective proposal sets and evaluate this
approach on various synthetic and empirical datasets. We find that proposal
sets meaningfully improve the accuracy of link prediction algorithms based on
both neighborhood heuristics and graph neural networks. Code is available at
\url{https://github.com/CUAI/Edge-Proposal-Sets}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Abhay Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qian Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Sijia Linda Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhalerao_O/0/1/0/all/0/1"&gt;Omkar Bhalerao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Horace He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"&gt;Ser-Nam Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1"&gt;Austin R. Benson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Threat of Offensive AI to Organizations. (arXiv:2106.15764v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.15764</id>
        <link href="http://arxiv.org/abs/2106.15764"/>
        <updated>2021-07-01T01:59:33.382Z</updated>
        <summary type="html"><![CDATA[AI has provided us with the ability to automate tasks, extract information
from vast amounts of data, and synthesize media that is nearly
indistinguishable from the real thing. However, positive tools can also be used
for negative purposes. In particular, cyber adversaries can use AI (such as
machine learning) to enhance their attacks and expand their campaigns.

Although offensive AI has been discussed in the past, there is a need to
analyze and understand the threat in the context of organizations. For example,
how does an AI-capable adversary impact the cyber kill chain? Does AI benefit
the attacker more than the defender? What are the most significant AI threats
facing organizations today and what will be their impact on the future?

In this survey, we explore the threat of offensive AI on organizations.
First, we present the background and discuss how AI changes the adversary's
methods, strategies, goals, and overall attack model. Then, through a
literature review, we identify 33 offensive AI capabilities which adversaries
can use to enhance their attacks. Finally, through a user study spanning
industry and academia, we rank the AI threats and provide insights on the
adversaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirsky_Y/0/1/0/all/0/1"&gt;Yisroel Mirsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1"&gt;Ambra Demontis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotak_J/0/1/0/all/0/1"&gt;Jaidip Kotak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_R/0/1/0/all/0/1"&gt;Ram Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelei_D/0/1/0/all/0/1"&gt;Deng Gelei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Liu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1"&gt;Wenke Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1"&gt;Yuval Elovici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1"&gt;Battista Biggio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11396</id>
        <link href="http://arxiv.org/abs/2106.11396"/>
        <updated>2021-07-01T01:59:33.376Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization recently has attracted increased interest in machine
learning due to its many applications such as hyper-parameter optimization and
policy optimization. Although some methods recently have been proposed to solve
the bilevel problems, these methods do not consider using adaptive learning
rates. To fill this gap, in the paper, we propose a class of fast and effective
adaptive methods for solving bilevel optimization problems that the outer
problem is possibly nonconvex and the inner problem is strongly-convex.
Specifically, we propose a fast single-loop BiAdam algorithm based on the basic
momentum technique, which achieves a sample complexity of
$\tilde{O}(\epsilon^{-4})$ for finding an $\epsilon$-stationary point. At the
same time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by
using variance reduced technique, which reaches the best known sample
complexity of $\tilde{O}(\epsilon^{-3})$. To further reduce computation in
estimating derivatives, we propose a fast single-loop stochastic approximated
BiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still
achieves a sample complexity of $\tilde{O}(\epsilon^{-4})$ without large
batches. We further present an accelerated version of saBiAdam algorithm
(VR-saBiAdam), which also reaches the best known sample complexity of
$\tilde{O}(\epsilon^{-3})$. We apply the unified adaptive matrices to our
methods as the SUPER-ADAM \citep{huang2021super}, which including many types of
adaptive learning rates. Moreover, our framework can flexibly use the momentum
and variance reduced techniques. In particular, we provide a useful convergence
analysis framework for both the constrained and unconstrained bilevel
optimization. To the best of our knowledge, we first study the adaptive bilevel
optimization methods with adaptive learning rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Aspect Self-Attention based on Transformer for Remaining Useful Life Prediction. (arXiv:2106.15842v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.15842</id>
        <link href="http://arxiv.org/abs/2106.15842"/>
        <updated>2021-07-01T01:59:33.370Z</updated>
        <summary type="html"><![CDATA[Remaining useful life prediction (RUL) is one of the key technologies of
condition-based maintenance, which is important to maintain the reliability and
safety of industrial equipments. While deep learning has achieved great success
in RUL prediction, existing methods have difficulties in processing long
sequences and extracting information from the sensor and time step aspects. In
this paper, we propose Dual Aspect Self-attention based on Transformer (DAST),
a novel deep RUL prediction method. DAST consists of two encoders, which work
in parallel to simultaneously extract features of different sensors and time
steps. Solely based on self-attention, the DAST encoders are more effective in
processing long data sequences, and are capable of adaptively learning to focus
on more important parts of input. Moreover, the parallel feature extraction
design avoids mutual influence of information from two aspects. Experimental
results on two real turbofan engine datasets show that our method significantly
outperforms state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_W/0/1/0/all/0/1"&gt;Wen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiqiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSD Representations for Effective Probability Models. (arXiv:2106.16116v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16116</id>
        <link href="http://arxiv.org/abs/2106.16116"/>
        <updated>2021-07-01T01:59:33.364Z</updated>
        <summary type="html"><![CDATA[Finding a good way to model probability densities is key to probabilistic
inference. An ideal model should be able to concisely approximate any
probability, while being also compatible with two main operations:
multiplications of two models (product rule) and marginalization with respect
to a subset of the random variables (sum rule). In this work, we show that a
recently proposed class of positive semi-definite (PSD) models for non-negative
functions is particularly suited to this end. In particular, we characterize
both approximation and generalization capabilities of PSD models, showing that
they enjoy strong theoretical guarantees. Moreover, we show that we can perform
efficiently both sum and product rule in closed form via matrix operations,
enjoying the same versatility of mixture models. Our results open the way to
applications of PSD models to density estimation, decision theory and
inference. Preliminary empirical evaluation supports our findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciliberto_C/0/1/0/all/0/1"&gt;Carlo Ciliberto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge-Based Learning of Nonlinear Dynamics and Chaos. (arXiv:2010.03415v3 [nlin.CD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03415</id>
        <link href="http://arxiv.org/abs/2010.03415"/>
        <updated>2021-07-01T01:59:33.358Z</updated>
        <summary type="html"><![CDATA[Extracting predictive models from nonlinear systems is a central task in
scientific machine learning. One key problem is the reconciliation between
modern data-driven approaches and first principles. Despite rapid advances in
machine learning techniques, embedding domain knowledge into data-driven models
remains a challenge. In this work, we present a universal learning framework
for extracting predictive models from nonlinear systems based on observations.
Our framework can readily incorporate first principle knowledge because it
naturally models nonlinear systems as continuous-time systems. This both
improves the extracted models' extrapolation power and reduces the amount of
data needed for training. In addition, our framework has the advantages of
robustness to observational noise and applicability to irregularly sampled
data. We demonstrate the effectiveness of our scheme by learning predictive
models for a wide variety of systems including a stiff Van der Pol oscillator,
the Lorenz system, and the Kuramoto-Sivashinsky equation. For the Lorenz
system, different types of domain knowledge are incorporated to demonstrate the
strength of knowledge embedding in data-driven system identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/nlin/1/au:+Jiahao_T/0/1/0/all/0/1"&gt;Tom Z. Jiahao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/nlin/1/au:+Hsieh_M/0/1/0/all/0/1"&gt;M. Ani Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/nlin/1/au:+Forgoston_E/0/1/0/all/0/1"&gt;Eric Forgoston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering conservation laws from trajectories via machine learning. (arXiv:2102.04008v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04008</id>
        <link href="http://arxiv.org/abs/2102.04008"/>
        <updated>2021-07-01T01:59:33.342Z</updated>
        <summary type="html"><![CDATA[Invariants and conservation laws convey critical information about the
underlying dynamics of a system, yet it is generally infeasible to find them
from large-scale data without any prior knowledge or human insight. We propose
ConservNet to achieve this goal, a neural network that spontaneously discovers
a conserved quantity from grouped data where the members of each group share
invariants, similar to a general experimental setting where trajectories from
different trials are observed. As a neural network trained with a novel and
intuitive loss function called noise-variance loss, ConservNet learns the
hidden invariants in each group of multi-dimensional observables in a
data-driven, end-to-end manner. Our model successfully discovers underlying
invariants from the simulated systems having invariants as well as a real-world
double pendulum trajectory. Since the model is robust to various noises and
data conditions compared to baseline, our approach is directly applicable to
experimental data for discovering hidden conservation laws and further, general
relationships between variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1"&gt;Seungwoong Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1"&gt;Hawoong Jeong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Priors In Variational Autoencoders. (arXiv:2106.15671v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15671</id>
        <link href="http://arxiv.org/abs/2106.15671"/>
        <updated>2021-07-01T01:59:33.284Z</updated>
        <summary type="html"><![CDATA[Among likelihood-based approaches for deep generative modelling, variational
autoencoders (VAEs) offer scalable amortized posterior inference and fast
sampling. However, VAEs are also more and more outperformed by competing models
such as normalizing flows (NFs), deep-energy models, or the new denoising
diffusion probabilistic models (DDPMs). In this preliminary work, we improve
VAEs by demonstrating how DDPMs can be used for modelling the prior
distribution of the latent variables. The diffusion prior model improves upon
Gaussian priors of classical VAEs and is competitive with NF-based priors.
Finally, we hypothesize that hierarchical VAEs could similarly benefit from the
enhanced capacity of diffusion priors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wehenkel_A/0/1/0/all/0/1"&gt;Antoine Wehenkel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louppe_G/0/1/0/all/0/1"&gt;Gilles Louppe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning based Disease Progression Model for Alzheimer's Disease. (arXiv:2106.16187v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16187</id>
        <link href="http://arxiv.org/abs/2106.16187"/>
        <updated>2021-07-01T01:59:33.202Z</updated>
        <summary type="html"><![CDATA[We model Alzheimer's disease (AD) progression by combining differential
equations (DEs) and reinforcement learning (RL) with domain knowledge. DEs
provide relationships between some, but not all, factors relevant to AD. We
assume that the missing relationships must satisfy general criteria about the
working of the brain, for e.g., maximizing cognition while minimizing the cost
of supporting cognition. This allows us to extract the missing relationships by
using RL to optimize an objective (reward) function that captures the above
criteria. We use our model consisting of DEs (as a simulator) and the trained
RL agent to predict individualized 10-year AD progression using baseline (year
0) features on synthetic and real data. The model was comparable or better at
predicting 10-year cognition trajectories than state-of-the-art learning-based
models. Our interpretable model demonstrated, and provided insights into,
"recovery/compensatory" processes that mitigate the effect of AD, even though
those processes were not explicitly encoded in the model. Our framework
combines DEs with RL for modelling AD progression and has broad applicability
for understanding other neurological disorders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saboo_K/0/1/0/all/0/1"&gt;Krishnakant V. Saboo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_A/0/1/0/all/0/1"&gt;Anirudh Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yurui Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worrell_G/0/1/0/all/0/1"&gt;Gregory A. Worrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1"&gt;David T. Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Ravishankar K. Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transductive Zero-Shot Hashing for Multilabel Image Retrieval. (arXiv:1911.07192v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07192</id>
        <link href="http://arxiv.org/abs/1911.07192"/>
        <updated>2021-07-01T01:59:33.183Z</updated>
        <summary type="html"><![CDATA[Hash coding has been widely used in approximate nearest neighbor search for
large-scale image retrieval. Given semantic annotations such as class labels
and pairwise similarities of the training data, hashing methods can learn and
generate effective and compact binary codes. While some newly introduced images
may contain undefined semantic labels, which we call unseen images, zeor-shot
hashing techniques have been studied. However, existing zeor-shot hashing
methods focus on the retrieval of single-label images, and cannot handle
multi-label images. In this paper, for the first time, a novel transductive
zero-shot hashing method is proposed for multi-label unseen image retrieval. In
order to predict the labels of the unseen/target data, a visual-semantic bridge
is built via instance-concept coherence ranking on the seen/source data. Then,
pairwise similarity loss and focal quantization loss are constructed for
training a hashing model using both the seen/source and unseen/target data.
Extensive evaluations on three popular multi-label datasets demonstrate that,
the proposed hashing method achieves significantly better results than the
competing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1"&gt;Qin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Ling Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Song Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.16036</id>
        <link href="http://arxiv.org/abs/2106.16036"/>
        <updated>2021-07-01T01:59:33.177Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel way of doing audio synthesis at the waveform
level using Transformer architectures. We propose a deep neural network for
generating waveforms, similar to wavenet \cite{oord2016wavenet}. This is fully
probabilistic, auto-regressive, and causal, i.e. each sample generated depends
only on the previously observed samples. Our approach outperforms a widely used
wavenet architecture by up to 9\% on a similar dataset for predicting the next
step. Using the attention mechanism, we enable the architecture to learn which
audio samples are important for the prediction of the future sample. We show
how causal transformer generative models can be used for raw waveform
synthesis. We also show that this performance can be improved by another 2\% by
conditioning samples over a wider context. The flexibility of the current model
to synthesize audio from latent representations suggests a large number of
potential applications. The novel approach of using generative transformer
architectures for raw audio synthesis is, however, still far away from
generating any meaningful music, without using latent codes/meta-data to aid
the generation process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1"&gt;Prateek Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1"&gt;Chris Chafe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Muti-view Mouse Social Behaviour Recognition with Deep Graphical Model. (arXiv:2011.02451v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02451</id>
        <link href="http://arxiv.org/abs/2011.02451"/>
        <updated>2021-07-01T01:59:33.160Z</updated>
        <summary type="html"><![CDATA[Home-cage social behaviour analysis of mice is an invaluable tool to assess
therapeutic efficacy of neurodegenerative diseases. Despite tremendous efforts
made within the research community, single-camera video recordings are mainly
used for such analysis. Because of the potential to create rich descriptions of
mouse social behaviors, the use of multi-view video recordings for rodent
observations is increasingly receiving much attention. However, identifying
social behaviours from various views is still challenging due to the lack of
correspondence across data sources. To address this problem, we here propose a
novel multiview latent-attention and dynamic discriminative model that jointly
learns view-specific and view-shared sub-structures, where the former captures
unique dynamics of each view whilst the latter encodes the interaction between
the views. Furthermore, a novel multi-view latent-attention variational
autoencoder model is introduced in learning the acquired features, enabling us
to learn discriminative features in each view. Experimental results on the
standard CRMI13 and our multi-view Parkinson's Disease Mouse Behaviour (PDMB)
datasets demonstrate that our model outperforms the other state of the arts
technologies and effectively deals with the imbalanced data problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zheheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feixiang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1"&gt;Aite Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Ling Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Adversarial Training of Self-Attention Based Networks for Land Cover Classification using Multi-temporal Sentinel-2 Satellite Imagery. (arXiv:2104.00564v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00564</id>
        <link href="http://arxiv.org/abs/2104.00564"/>
        <updated>2021-07-01T01:59:33.154Z</updated>
        <summary type="html"><![CDATA[The increasing availability of large-scale remote sensing labeled data has
prompted researchers to develop increasingly precise and accurate data-driven
models for land cover and crop classification (LC&CC). Moreover, with the
introduction of self-attention and introspection mechanisms, deep learning
approaches have shown promising results in processing long temporal sequences
in the multi-spectral domain with a contained computational request.
Nevertheless, most practical applications cannot rely on labeled data, and in
the field, surveys are a time consuming solution that poses strict limitations
to the number of collected samples. Moreover, atmospheric conditions and
specific geographical region characteristics constitute a relevant domain gap
that does not allow direct applicability of a trained model on the available
dataset to the area of interest. In this paper, we investigate adversarial
training of deep neural networks to bridge the domain discrepancy between
distinct geographical zones. In particular, we perform a thorough analysis of
domain adaptation applied to challenging multi-spectral, multi-temporal data,
accurately highlighting the advantages of adapting state-of-the-art
self-attention based models for LC&CC to different target zones where labeled
data are not available. Extensive experimentation demonstrated significant
performance and generalization gain in applying domain-adversarial training to
source and target regions with marked dissimilarities between the distribution
of extracted features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martini_M/0/1/0/all/0/1"&gt;Mauro Martini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1"&gt;Vittorio Mazzia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khaliq_A/0/1/0/all/0/1"&gt;Aleem Khaliq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1"&gt;Marcello Chiaberge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Learning of OFDM Waveforms with PAPR and ACLR Constraints. (arXiv:2106.16039v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.16039</id>
        <link href="http://arxiv.org/abs/2106.16039"/>
        <updated>2021-07-01T01:59:33.148Z</updated>
        <summary type="html"><![CDATA[Orthogonal frequency-division multiplexing (OFDM) is widely used in modern
wireless networks thanks to its efficient handling of multipath environment.
However, it suffers from a poor peak-to-average power ratio (PAPR) which
requires a large power backoff, degrading the power amplifier (PA) efficiency.
In this work, we propose to use a neural network (NN) at the transmitter to
learn a high-dimensional modulation scheme allowing to control the PAPR and
adjacent channel leakage ratio (ACLR). On the receiver side, a NN-based
receiver is implemented to carry out demapping of the transmitted bits. The two
NNs operate on top of OFDM, and are jointly optimized in and end-to-end manner
using a training algorithm that enforces constraints on the PAPR and ACLR.
Simulation results show that the learned waveforms enable higher information
rates than a tone reservation baseline, while satisfying predefined PAPR and
ACLR targets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1"&gt;Mathieu Goutay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1"&gt;Fay&amp;#xe7;al Ait Aoudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1"&gt;Jakob Hoydis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1"&gt;Jean-Marie Gorce&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning More for Free - A Multi Task Learning Approach for Improved Pathology Classification in Capsule Endoscopy. (arXiv:2106.16162v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16162</id>
        <link href="http://arxiv.org/abs/2106.16162"/>
        <updated>2021-07-01T01:59:33.142Z</updated>
        <summary type="html"><![CDATA[The progress in Computer Aided Diagnosis (CADx) of Wireless Capsule Endoscopy
(WCE) is thwarted by the lack of data. The inadequacy in richly representative
healthy and abnormal conditions results in isolated analyses of pathologies,
that can not handle realistic multi-pathology scenarios. In this work, we
explore how to learn more for free, from limited data through solving a WCE
multicentric, multi-pathology classification problem. Learning more implies to
learning more than full supervision would allow with the same data. This is
done by combining self supervision with full supervision, under multi task
learning. Additionally, we draw inspiration from the Human Visual System (HVS)
in designing self supervision tasks and investigate if seemingly ineffectual
signals within the data itself can be exploited to gain performance, if so,
which signals would be better than others. Further, we present our analysis of
the high level features as a stepping stone towards more robust multi-pathology
CADx in WCE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vats_A/0/1/0/all/0/1"&gt;Anuja Vats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedersen_M/0/1/0/all/0/1"&gt;Marius Pedersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammed_A/0/1/0/all/0/1"&gt;Ahmed Mohammed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovde_O/0/1/0/all/0/1"&gt;&amp;#xd8;istein Hovde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I Want This Product but Different : Multimodal Retrieval with Synthetic Query Expansion. (arXiv:2102.08871v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08871</id>
        <link href="http://arxiv.org/abs/2102.08871"/>
        <updated>2021-07-01T01:59:33.136Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of media retrieval using a multimodal query
(a query which combines visual input with additional semantic information in
natural language feedback). We propose a SynthTriplet GAN framework which
resolves this task by expanding the multimodal query with a synthetically
generated image that captures semantic information from both image and text
input. We introduce a novel triplet mining method that uses a synthetic image
as an anchor to directly optimize for embedding distances of generated and
target images. We demonstrate that apart from the added value of retrieval
illustration with synthetic image with the focus on customization and user
feedback, the proposed method greatly surpasses other multimodal generation
methods and achieves state of the art results in the multimodal retrieval task.
We also show that in contrast to other retrieval methods, our method provides
explainable embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tautkute_I/0/1/0/all/0/1"&gt;Ivona Tautkute&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1"&gt;Tomasz Trzcinski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monocular 3D Object Detection: An Extrinsic Parameter Free Approach. (arXiv:2106.15796v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15796</id>
        <link href="http://arxiv.org/abs/2106.15796"/>
        <updated>2021-07-01T01:59:33.129Z</updated>
        <summary type="html"><![CDATA[Monocular 3D object detection is an important task in autonomous driving. It
can be easily intractable where there exists ego-car pose change w.r.t. ground
plane. This is common due to the slight fluctuation of road smoothness and
slope. Due to the lack of insight in industrial application, existing methods
on open datasets neglect the camera pose information, which inevitably results
in the detector being susceptible to camera extrinsic parameters. The
perturbation of objects is very popular in most autonomous driving cases for
industrial products. To this end, we propose a novel method to capture camera
pose to formulate the detector free from extrinsic perturbation. Specifically,
the proposed framework predicts camera extrinsic parameters by detecting
vanishing point and horizon change. A converter is designed to rectify
perturbative features in the latent space. By doing so, our 3D detector works
independent of the extrinsic parameter variations and produces accurate results
in realistic cases, e.g., potholed and uneven roads, where almost all existing
monocular detectors fail to handle. Experiments demonstrate our method yields
the best performance compared with the other state-of-the-arts by a large
margin on both KITTI 3D and nuScenes datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yunsong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongzi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1"&gt;Qinhong Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limited-Fronthaul Cell-Free Hybrid Beamforming with Distributed Deep Neural Network. (arXiv:2106.16194v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.16194</id>
        <link href="http://arxiv.org/abs/2106.16194"/>
        <updated>2021-07-01T01:59:33.112Z</updated>
        <summary type="html"><![CDATA[Cell-free massive MIMO (CF-mMIMO) systems represent a promising approach to
increase the spectral efficiency of wireless communication systems. However,
near-optimal solutions require a large amount of signaling exchange between
access points (APs) and the network controller (NC). In addition, the use of
hybrid beamforming in each AP reduces the number of power hungry RF chains, but
imposes a large computational complexity to find near-optimal precoders. In
this letter, we propose two unsupervised deep neural networks (DNN)
architectures, fully and partially distributed, that can perform coordinated
hybrid beamforming with zero or limited communication overhead between APs and
NC, while achieving near-optimal sum-rate with a reduced computational
complexity compared to conventional near-optimal solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hojatian_H/0/1/0/all/0/1"&gt;Hamed Hojatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nadal_J/0/1/0/all/0/1"&gt;Jeremy Nadal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Frigon_J/0/1/0/all/0/1"&gt;Jean-Francois Frigon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leduc_Primeau_F/0/1/0/all/0/1"&gt;Francois Leduc-Primeau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source domain adaptation via supervised contrastive learning and confident consistency regularization. (arXiv:2106.16093v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16093</id>
        <link href="http://arxiv.org/abs/2106.16093"/>
        <updated>2021-07-01T01:59:33.106Z</updated>
        <summary type="html"><![CDATA[Multi-Source Unsupervised Domain Adaptation (multi-source UDA) aims to learn
a model from several labeled source domains while performing well on a
different target domain where only unlabeled data are available at training
time. To align source and target features distributions, several recent works
use source and target explicit statistics matching such as features moments or
class centroids. Yet, these approaches do not guarantee class conditional
distributions alignment across domains. In this work, we propose a new
framework called Contrastive Multi-Source Domain Adaptation (CMSDA) for
multi-source UDA that addresses this limitation. Discriminative features are
learned from interpolated source examples via cross entropy minimization and
from target examples via consistency regularization and hard pseudo-labeling.
Simultaneously, interpolated source examples are leveraged to align source
class conditional distributions through an interpolated version of the
supervised contrastive loss. This alignment leads to more general and
transferable features which further improve the generalization on the target
domain. Extensive experiments have been carried out on three standard
multi-source UDA datasets where our method reports state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scalbert_M/0/1/0/all/0/1"&gt;Marin Scalbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1"&gt;Maria Vakalopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couzinie_Devy_F/0/1/0/all/0/1"&gt;Florent Couzini&amp;#xe9;-Devy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotions in Macroeconomic News and their Impact on the European Bond Market. (arXiv:2106.15698v1 [econ.GN])]]></title>
        <id>http://arxiv.org/abs/2106.15698</id>
        <link href="http://arxiv.org/abs/2106.15698"/>
        <updated>2021-07-01T01:59:33.100Z</updated>
        <summary type="html"><![CDATA[We show how emotions extracted from macroeconomic news can be used to explain
and forecast future behaviour of sovereign bond yield spreads in Italy and
Spain. We use a big, open-source, database known as Global Database of Events,
Language and Tone to construct emotion indicators of bond market affective
states. We find that negative emotions extracted from news improve the
forecasting power of government yield spread models during distressed periods
even after controlling for the number of negative words present in the text. In
addition, stronger negative emotions, such as panic, reveal useful information
for predicting changes in spread at the short-term horizon, while milder
emotions, such as distress, are useful at longer time horizons. Emotions
generated by the Italian political turmoil propagate to the Spanish news
affecting this neighbourhood market.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Consoli_S/0/1/0/all/0/1"&gt;Sergio Consoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Pezzoli_L/0/1/0/all/0/1"&gt;Luca Tiozzo Pezzoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Tosetti_E/0/1/0/all/0/1"&gt;Elisa Tosetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Landscape of One-hidden-layer Sparse Networks and Beyond. (arXiv:2009.07439v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07439</id>
        <link href="http://arxiv.org/abs/2009.07439"/>
        <updated>2021-07-01T01:59:33.085Z</updated>
        <summary type="html"><![CDATA[Sparse neural networks have received increasing interests due to their small
size compared to dense networks. Nevertheless, most existing works on neural
network theory have focused on dense neural networks, and our understanding of
sparse networks is very limited. In this paper, we study the loss landscape of
one-hidden-layer sparse networks. We first consider sparse networks with linear
activations. We show that sparse linear networks can have spurious strict
minima, which is in sharp contrast to dense linear networks which do not even
have spurious minima. Second, we show that spurious valleys can exist for wide
sparse non-linear networks. This is different from wide dense networks which do
not have spurious valleys under mild assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dachao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Ruoyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhihua Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection: How to Artificially Increase your F1-Score with a Biased Evaluation Protocol. (arXiv:2106.16020v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16020</id>
        <link href="http://arxiv.org/abs/2106.16020"/>
        <updated>2021-07-01T01:59:33.079Z</updated>
        <summary type="html"><![CDATA[Anomaly detection is a widely explored domain in machine learning. Many
models are proposed in the literature, and compared through different metrics
measured on various datasets. The most popular metrics used to compare
performances are F1-score, AUC and AVPR. In this paper, we show that F1-score
and AVPR are highly sensitive to the contamination rate. One consequence is
that it is possible to artificially increase their values by modifying the
train-test split procedure. This leads to misleading comparisons between
algorithms in the literature, especially when the evaluation protocol is not
well detailed. Moreover, we show that the F1-score and the AVPR cannot be used
to compare performances on different datasets as they do not reflect the
intrinsic difficulty of modeling such data. Based on these observations, we
claim that F1-score and AVPR should not be used as metrics for anomaly
detection. We recommend a generic evaluation procedure for unsupervised anomaly
detection, including the use of other metrics such as the AUC, which are more
robust to arbitrary choices in the evaluation protocol.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fourure_D/0/1/0/all/0/1"&gt;Damien Fourure&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javaid_M/0/1/0/all/0/1"&gt;Muhammad Usama Javaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Posocco_N/0/1/0/all/0/1"&gt;Nicolas Posocco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tihon_S/0/1/0/all/0/1"&gt;Simon Tihon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Reconstruction Methods from RGB Images to Hyperspectral Imaging: A Survey. (arXiv:2106.15944v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.15944</id>
        <link href="http://arxiv.org/abs/2106.15944"/>
        <updated>2021-07-01T01:59:33.073Z</updated>
        <summary type="html"><![CDATA[Hyperspectral imaging enables versatile applications due to its competence in
capturing abundant spatial and spectral information, which are crucial for
identifying substances. However, the devices for acquiring hyperspectral images
are expensive and complicated. Therefore, many alternative spectral imaging
methods have been proposed by directly reconstructing the hyperspectral
information from lower-cost, more available RGB images. We present a thorough
investigation of these state-of-the-art spectral reconstruction methods from
the widespread RGB images. A systematic study and comparison of more than 25
methods has revealed that most of the data-driven deep learning methods are
superior to prior-based methods in terms of reconstruction accuracy and quality
despite lower speeds. This comprehensive review can serve as a fruitful
reference source for peer researchers, thus further inspiring future
development directions in related domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Su_R/0/1/0/all/0/1"&gt;Runmu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ren_W/0/1/0/all/0/1"&gt;Wenqi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_Q/0/1/0/all/0/1"&gt;Qiang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Yunfeng Nie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Reweighting Domain Generalization for Face Presentation Attack Detection. (arXiv:2106.16128v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16128</id>
        <link href="http://arxiv.org/abs/2106.16128"/>
        <updated>2021-07-01T01:59:33.066Z</updated>
        <summary type="html"><![CDATA[Face anti-spoofing approaches based on domain generalization (DG) have drawn
growing attention due to their robustness for unseen scenarios. Previous
methods treat each sample from multiple domains indiscriminately during the
training process, and endeavor to extract a common feature space to improve the
generalization. However, due to complex and biased data distribution, directly
treating them equally will corrupt the generalization ability. To settle the
issue, we propose a novel Dual Reweighting Domain Generalization (DRDG)
framework which iteratively reweights the relative importance between samples
to further improve the generalization. Concretely, Sample Reweighting Module is
first proposed to identify samples with relatively large domain bias, and
reduce their impact on the overall optimization. Afterwards, Feature
Reweighting Module is introduced to focus on these samples and extract more
domain-irrelevant features via a self-distilling mechanism. Combined with the
domain discriminator, the iteration of the two modules promotes the extraction
of generalized features. Extensive experiments and visualizations are presented
to demonstrate the effectiveness and interpretability of our method against the
state-of-the-art competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shubao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Ke-Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Taiping Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1"&gt;Kekai Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shouhong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Robust Classification-autoencoder to Defend Outliers and Adversaries. (arXiv:2106.15927v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15927</id>
        <link href="http://arxiv.org/abs/2106.15927"/>
        <updated>2021-07-01T01:59:33.060Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a robust classification-autoencoder (CAE) which has
strong ability to recognize outliers and defend adversaries. The basic idea is
to change the autoencoder from an unsupervised learning method into a
classifier. The CAE is a modified autoencoder, where the encoder is used to
compress samples with different labels into disjoint compression spaces and the
decoder is used to recover a sample with a given label from the corresponding
compression space. The encoder is used as a classifier and the decoder is used
to decide whether the classification given by the encoder is correct by
comparing the input sample with the output. Since adversary samples are seeming
inevitable for the current DNN framework, we introduce the list classification
based on CAE to defend adversaries, which outputs several labels and the
corresponding samples recovered by the CAE. The CAE is evaluated using the
MNIST dataset in great detail. It is shown that the CAE network can recognize
almost all outliers and the list classification contains the correct label for
almost all adversaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lijia Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiao-Shan Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Adversarial Image Synthesis. (arXiv:2106.16056v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16056</id>
        <link href="http://arxiv.org/abs/2106.16056"/>
        <updated>2021-07-01T01:59:33.054Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have been extremely successful in
various application domains. Adversarial image synthesis has drawn increasing
attention and made tremendous progress in recent years because of its wide
range of applications in many computer vision and image processing problems.
Among the many applications of GAN, image synthesis is the most well-studied
one, and research in this area has already demonstrated the great potential of
using GAN in image synthesis. In this paper, we provide a taxonomy of methods
used in image synthesis, review different models for text-to-image synthesis
and image-to-image translation, and discuss some evaluation metrics as well as
possible future research directions in image synthesis with GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_W/0/1/0/all/0/1"&gt;William Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_G/0/1/0/all/0/1"&gt;Glen Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1"&gt;Robert Leer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricardo_F/0/1/0/all/0/1"&gt;Frederick Ricardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images. (arXiv:2106.15753v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.15753</id>
        <link href="http://arxiv.org/abs/2106.15753"/>
        <updated>2021-07-01T01:59:33.047Z</updated>
        <summary type="html"><![CDATA[Robust and accurate nuclei centroid detection is important for the
understanding of biological structures in fluorescence microscopy images.
Existing automated nuclei localization methods face three main challenges: (1)
Most of object detection methods work only on 2D images and are difficult to
extend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes
but it is computational expensive for large microscopy volumes and they have
difficulty distinguishing different instances of objects; (3) Hand annotated
ground truth is limited for 3D microscopy volumes. To address these issues, we
present a scalable approach for nuclei centroid detection of 3D microscopy
volumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each
slice of the volume from different directions and 3D agglomerative hierarchical
clustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.
The model was trained with the synthetic microscopy data generated using
Spatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and
tested on different types of real 3D microscopy data. Extensive experimental
results demonstrate that our proposed method can accurately count and detect
the nuclei centroids in a 3D microscopy volume.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1"&gt;Shuo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1"&gt;Alain Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salama_P/0/1/0/all/0/1"&gt;Paul Salama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dunn_K/0/1/0/all/0/1"&gt;Kenneth W. Dunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1"&gt;Edward J. Delp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning-based Lie Detector applied to a Novel Annotated Game Dataset. (arXiv:2104.12345v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12345</id>
        <link href="http://arxiv.org/abs/2104.12345"/>
        <updated>2021-07-01T01:59:33.040Z</updated>
        <summary type="html"><![CDATA[Lie detection is considered a concern for everyone in their day to day life
given its impact on human interactions. Thus, people normally pay attention to
both what their interlocutors are saying and also to their visual appearances,
including faces, to try to find any signs that indicate whether the person is
telling the truth or not. While automatic lie detection may help us to
understand this lying characteristics, current systems are still fairly
limited, partly due to lack of adequate datasets to evaluate their performance
in realistic scenarios. In this work, we have collected an annotated dataset of
facial images, comprising both 2D and 3D information of several participants
during a card game that encourages players to lie. Using our collected dataset,
We evaluated several types of machine learning-based lie detectors in terms of
their generalization, person-specific and cross-domain experiments. Our results
show that models based on deep learning achieve the best accuracy, reaching up
to 57\% for the generalization task and 63\% when dealing with a single
participant. Finally, we also highlight the limitation of the deep learning
based lie detector when dealing with cross-domain lie detection tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Diaz_N/0/1/0/all/0/1"&gt;Nuria Rodriguez-Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aspandi_D/0/1/0/all/0/1"&gt;Decky Aspandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukno_F/0/1/0/all/0/1"&gt;Federico Sukno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binefa_X/0/1/0/all/0/1"&gt;Xavier Binefa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-Step Adversarial Training for Semantic Segmentation. (arXiv:2106.15998v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15998</id>
        <link href="http://arxiv.org/abs/2106.15998"/>
        <updated>2021-07-01T01:59:33.024Z</updated>
        <summary type="html"><![CDATA[Even though deep neural networks succeed on many different tasks including
semantic segmentation, they lack on robustness against adversarial examples. To
counteract this exploit, often adversarial training is used. However, it is
known that adversarial training with weak adversarial attacks (e.g. using the
Fast Gradient Method) does not improve the robustness against stronger attacks.
Recent research shows that it is possible to increase the robustness of such
single-step methods by choosing an appropriate step size during the training.
Finding such a step size, without increasing the computational effort of
single-step adversarial training, is still an open challenge. In this work we
address the computationally particularly demanding task of semantic
segmentation and propose a new step size control algorithm that increases the
robustness of single-step adversarial training. The proposed algorithm does not
increase the computational effort of single-step adversarial training
considerably and also simplifies training, because it is free of
meta-parameter. We show that the robustness of our approach can compete with
multi-step adversarial training on two popular benchmarks for semantic
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wiens_D/0/1/0/all/0/1"&gt;Daniel Wiens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1"&gt;Barbara Hammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Whose Opinions Matter? Perspective-aware Models to Identify Opinions of Hate Speech Victims in Abusive Language Detection. (arXiv:2106.15896v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15896</id>
        <link href="http://arxiv.org/abs/2106.15896"/>
        <updated>2021-07-01T01:59:33.016Z</updated>
        <summary type="html"><![CDATA[Social media platforms provide users the freedom of expression and a medium
to exchange information and express diverse opinions. Unfortunately, this has
also resulted in the growth of abusive content with the purpose of
discriminating people and targeting the most vulnerable communities such as
immigrants, LGBT, Muslims, Jews and women. Because abusive language is
subjective in nature, there might be highly polarizing topics or events
involved in the annotation of abusive contents such as hate speech (HS).
Therefore, we need novel approaches to model conflicting perspectives and
opinions coming from people with different personal and demographic
backgrounds. In this paper, we present an in-depth study to model polarized
opinions coming from different communities under the hypothesis that similar
characteristics (ethnicity, social background, culture etc.) can influence the
perspectives of annotators on a certain phenomenon. We believe that by relying
on this information, we can divide the annotators into groups sharing similar
perspectives. We can create separate gold standards, one for each group, to
train state-of-the-art deep learning models. We can employ an ensemble approach
to combine the perspective-aware classifiers from different groups to an
inclusive model. We also propose a novel resource, a multi-perspective English
language dataset annotated according to different sub-categories relevant for
characterising online abuse: hate speech, aggressiveness, offensiveness and
stereotype. By training state-of-the-art deep learning models on this novel
resource, we show how our approach improves the prediction performance of a
state-of-the-art supervised classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_S/0/1/0/all/0/1"&gt;Sohail Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basile_V/0/1/0/all/0/1"&gt;Valerio Basile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patti_V/0/1/0/all/0/1"&gt;Viviana Patti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo. (arXiv:2106.16118v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.16118</id>
        <link href="http://arxiv.org/abs/2106.16118"/>
        <updated>2021-07-01T01:59:32.988Z</updated>
        <summary type="html"><![CDATA[Robot manipulation of unknown objects in unstructured environments is a
challenging problem due to the variety of shapes, materials, arrangements and
lighting conditions. Even with large-scale real-world data collection, robust
perception and manipulation of transparent and reflective objects across
various lighting conditions remain challenging. To address these challenges we
propose an approach to performing sim-to-real transfer of robotic perception.
The underlying model, SimNet, is trained as a single multi-headed neural
network using simulated stereo data as input and simulated object segmentation
masks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as
output. A key component of SimNet is the incorporation of a learned stereo
sub-network that predicts disparity. SimNet is evaluated on 2D car detection,
unknown object detection, and deformable object keypoint detection and
significantly outperforms a baseline that uses a structured light RGB-D sensor.
By inferring grasp positions using the OBB and keypoint predictions, SimNet can
be used to perform end-to-end manipulation of unknown objects in both easy and
hard scenarios using our fleet of Toyota HSR robots in four home environments.
In unknown object grasping experiments, the predictions from the baseline RGB-D
network and SimNet enable successful grasps of most of the easy objects.
However, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)
objects, while SimNet grasps 95%, suggesting that SimNet can enable robust
manipulation of unknown objects, including transparent objects, in unknown
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1"&gt;Thomas Kollar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1"&gt;Michael Laskey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1"&gt;Kevin Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tjersland_M/0/1/0/all/0/1"&gt;Mark Tjersland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Bounds for Open-Set Learning. (arXiv:2106.15792v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15792</id>
        <link href="http://arxiv.org/abs/2106.15792"/>
        <updated>2021-07-01T01:59:32.980Z</updated>
        <summary type="html"><![CDATA[Traditional supervised learning aims to train a classifier in the closed-set
world, where training and test samples share the same label space. In this
paper, we target a more challenging and realistic setting: open-set learning
(OSL), where there exist test samples from the classes that are unseen during
training. Although researchers have designed many methods from the algorithmic
perspectives, there are few methods that provide generalization guarantees on
their ability to achieve consistent performance on different training samples
drawn from the same distribution. Motivated by the transfer learning and
probably approximate correct (PAC) theory, we make a bold attempt to study OSL
by proving its generalization error-given training samples with size n, the
estimation error will get close to order O_p(1/\sqrt{n}). This is the first
study to provide a generalization bound for OSL, which we do by theoretically
investigating the risk of the target classifier on unknown classes. According
to our theory, a novel algorithm, called auxiliary open-set risk (AOSR) is
proposed to address the OSL problem. Experiments verify the efficacy of AOSR.
The code is available at github.com/Anjin-Liu/Openset_Learning_AOSR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zhen Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Anjin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guangquan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Robustness of Neural Networks through Graph Measures. (arXiv:2106.15850v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15850</id>
        <link href="http://arxiv.org/abs/2106.15850"/>
        <updated>2021-07-01T01:59:32.967Z</updated>
        <summary type="html"><![CDATA[Motivated by graph theory, artificial neural networks (ANNs) are
traditionally structured as layers of neurons (nodes), which learn useful
information by the passage of data through interconnections (edges). In the
machine learning realm, graph structures (i.e., neurons and connections) of
ANNs have recently been explored using various graph-theoretic measures linked
to their predictive performance. On the other hand, in network science
(NetSci), certain graph measures including entropy and curvature are known to
provide insight into the robustness and fragility of real-world networks. In
this work, we use these graph measures to explore the robustness of various
ANNs to adversarial attacks. To this end, we (1) explore the design space of
inter-layer and intra-layers connectivity regimes of ANNs in the graph domain
and record their predictive performance after training under different types of
adversarial attacks, (2) use graph representations for both inter-layer and
intra-layers connectivity regimes to calculate various graph-theoretic
measures, including curvature and entropy, and (3) analyze the relationship
between these graph measures and the adversarial performance of ANNs. We show
that curvature and entropy, while operating in the graph domain, can quantify
the robustness of ANNs without having to train these ANNs. Our results suggest
that the real-world networks, including brain networks, financial networks, and
social networks may provide important clues to the neural architecture search
for robust ANNs. We propose a search strategy that efficiently finds robust
ANNs amongst a set of well-performing ANNs without having a need to train all
of these ANNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Waqas_A/0/1/0/all/0/1"&gt;Asim Waqas&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1"&gt;Ghulam Rasool&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Farooq_H/0/1/0/all/0/1"&gt;Hamza Farooq&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1"&gt;Nidhal C. Bouaynaya&lt;/a&gt; (1), ((1) Rowan University, (2) University of Minnesota)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. (arXiv:2106.15831v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15831</id>
        <link href="http://arxiv.org/abs/2106.15831"/>
        <updated>2021-07-01T01:59:32.949Z</updated>
        <summary type="html"><![CDATA[Although machine learning models typically experience a drop in performance
on out-of-distribution data, accuracies on in- versus out-of-distribution data
are widely observed to follow a single linear trend when evaluated across a
testbed of models. Models that are more accurate on the out-of-distribution
data relative to this baseline exhibit "effective robustness" and are
exceedingly rare. Identifying such models, and understanding their properties,
is key to improving out-of-distribution performance. We conduct a thorough
empirical investigation of effective robustness during fine-tuning and
surprisingly find that models pre-trained on larger datasets exhibit effective
robustness during training that vanishes at convergence. We study how
properties of the data influence effective robustness, and we show that it
increases with the larger size, more diversity, and higher example difficulty
of the dataset. We also find that models that display effective robustness are
able to correctly classify 10% of the examples that no other current testbed
model gets correct. Finally, we discuss several strategies for scaling
effective robustness to the high-accuracy regime to improve the
out-of-distribution accuracy of state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1"&gt;Anders Andreassen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_Y/0/1/0/all/0/1"&gt;Yasaman Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1"&gt;Behnam Neyshabur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1"&gt;Rebecca Roelofs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaGDA: Faster Adaptive Gradient Descent Ascent Methods for Minimax Optimization. (arXiv:2106.16101v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.16101</id>
        <link href="http://arxiv.org/abs/2106.16101"/>
        <updated>2021-07-01T01:59:32.935Z</updated>
        <summary type="html"><![CDATA[In the paper, we propose a class of faster adaptive gradient descent ascent
methods for solving the nonconvex-strongly-concave minimax problems by using
unified adaptive matrices used in the SUPER-ADAM \citep{huang2021super}.
Specifically, we propose a fast adaptive gradient decent ascent (AdaGDA) method
based on the basic momentum technique, which reaches a low sample complexity of
$O(\kappa^4\epsilon^{-4})$ for finding an $\epsilon$-stationary point without
large batches, which improves the existing result of adaptive minimax
optimization method by a factor of $O(\sqrt{\kappa})$. Moreover, we present an
accelerated version of AdaGDA (VR-AdaGDA) method based on the momentum-based
variance reduced technique, which achieves the best known sample complexity of
$O(\kappa^3\epsilon^{-3})$ for finding an $\epsilon$-stationary point without
large batches. Further assume the bounded Lipschitz parameter of objective
function, we prove that our VR-AdaGDA method reaches a lower sample complexity
of $O(\kappa^{2.5}\epsilon^{-3})$ with the mini-batch size $O(\kappa)$. In
particular, we provide an effective convergence analysis framework for our
adaptive methods based on unified adaptive matrices, which include almost
existing adaptive learning rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-Sample Analysis of Stochastic Approximation Using Smooth Convex Envelopes. (arXiv:2002.00874v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00874</id>
        <link href="http://arxiv.org/abs/2002.00874"/>
        <updated>2021-07-01T01:59:32.917Z</updated>
        <summary type="html"><![CDATA[Stochastic Approximation (SA) is a popular approach for solving fixed-point
equations where the information is corrupted by noise. In this paper, we
consider an SA involving a contraction mapping with respect to an arbitrary
norm, and show its finite-sample error bounds while using different stepsizes.
The idea is to construct a smooth Lyapunov function using the generalized
Moreau envelope, and show that the iterates of SA have negative drift with
respect to that Lyapunov function. Our result is applicable in Reinforcement
Learning (RL). In particular, we use it to establish the first-known
convergence rate of the V-trace algorithm for off-policy TD-learning. Moreover,
we also use it to study TD-learning in the on-policy setting, and recover the
existing state-of-the-art results for $Q$-learning. Importantly, our
construction results in only a logarithmic dependence of the convergence bound
on the size of the state-space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zaiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1"&gt;Siva Theja Maguluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Shortcuts for Vision Transformers. (arXiv:2106.15941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15941</id>
        <link href="http://arxiv.org/abs/2106.15941"/>
        <updated>2021-07-01T01:59:32.909Z</updated>
        <summary type="html"><![CDATA[Transformer models have achieved great progress on computer vision tasks
recently. The rapid development of vision transformers is mainly contributed by
their high representation ability for extracting informative features from
input images. However, the mainstream transformer models are designed with deep
architectures, and the feature diversity will be continuously reduced as the
depth increases, i.e., feature collapse. In this paper, we theoretically
analyze the feature collapse phenomenon and study the relationship between
shortcuts and feature diversity in these transformer models. Then, we present
an augmented shortcut scheme, which inserts additional paths with learnable
parameters in parallel on the original shortcuts. To save the computational
costs, we further explore an efficient approach that uses the block-circulant
projection to implement augmented shortcuts. Extensive experiments conducted on
benchmark datasets demonstrate the effectiveness of the proposed method, which
brings about 1% accuracy increase of the state-of-the-art visual transformers
without obviously increasing their parameters and FLOPs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;An Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yiping Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Hidden Structure in Self-Supervised Learning. (arXiv:2106.16060v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16060</id>
        <link href="http://arxiv.org/abs/2106.16060"/>
        <updated>2021-07-01T01:59:32.900Z</updated>
        <summary type="html"><![CDATA[This work considers the problem of learning structured representations from
raw images using self-supervised learning. We propose a principled framework
based on a mutual information objective, which integrates self-supervised and
structure learning. Furthermore, we devise a post-hoc procedure to interpret
the meaning of the learnt representations. Preliminary experiments on CIFAR-10
show that the proposed framework achieves higher generalization performance in
downstream classification tasks and provides more interpretable representations
compared to the ones learnt through traditional self-supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1"&gt;Emanuele Sansone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Reweighting for Adversarial Training. (arXiv:2106.15776v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15776</id>
        <link href="http://arxiv.org/abs/2106.15776"/>
        <updated>2021-07-01T01:59:32.895Z</updated>
        <summary type="html"><![CDATA[Instances-reweighted adversarial training (IRAT) can significantly boost the
robustness of trained models, where data being less/more vulnerable to the
given attack are assigned smaller/larger weights during training. However, when
tested on attacks different from the given attack simulated in training, the
robustness may drop significantly (e.g., even worse than no reweighting). In
this paper, we study this problem and propose our solution--locally reweighted
adversarial training (LRAT). The rationale behind IRAT is that we do not need
to pay much attention to an instance that is already safe under the attack. We
argue that the safeness should be attack-dependent, so that for the same
instance, its weight can change given different attacks based on the same
model. Thus, if the attack simulated in training is mis-specified, the weights
of IRAT are misleading. To this end, LRAT pairs each instance with its
adversarial variants and performs local reweighting inside each pair, while
performing no global reweighting--the rationale is to fit the instance itself
if it is immune to the attack, but not to skip the pair, in order to passively
defend different attacks in future. Experiments show that LRAT works better
than both IRAT (i.e., global reweighting) and the standard AT (i.e., no
reweighting) when trained with an attack and tested on different attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruize Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kaiwen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;James Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay. (arXiv:2106.15739v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15739</id>
        <link href="http://arxiv.org/abs/2106.15739"/>
        <updated>2021-07-01T01:59:32.889Z</updated>
        <summary type="html"><![CDATA[Despite the conventional wisdom that using batch normalization with weight
decay may improve neural network training, some recent works show their joint
usage may cause instabilities at the late stages of training. Other works, in
contrast, show convergence to the equilibrium, i.e., the stabilization of
training metrics. In this paper, we study this contradiction and show that
instead of converging to a stable equilibrium, the training dynamics converge
to consistent periodic behavior. That is, the training process regularly
exhibits instabilities which, however, do not lead to complete training
failure, but cause a new period of training. We rigorously investigate the
mechanism underlying this discovered periodic behavior both from an empirical
and theoretical point of view and show that this periodic behavior is indeed
caused by the interaction between batch normalization and weight decay.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1"&gt;Ekaterina Lobacheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1"&gt;Maxim Kodryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1"&gt;Nadezhda Chirkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1"&gt;Andrey Malinin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Improving Early Stopping for Learning with Noisy Labels. (arXiv:2106.15853v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15853</id>
        <link href="http://arxiv.org/abs/2106.15853"/>
        <updated>2021-07-01T01:59:32.871Z</updated>
        <summary type="html"><![CDATA[The memorization effect of deep neural network (DNN) plays a pivotal role in
many state-of-the-art label-noise learning methods. To exploit this property,
the early stopping trick, which stops the optimization at the early stage of
training, is usually adopted. Current methods generally decide the early
stopping point by considering a DNN as a whole. However, a DNN can be
considered as a composition of a series of layers, and we find that the latter
layers in a DNN are much more sensitive to label noise, while their former
counterparts are quite robust. Therefore, selecting a stopping point for the
whole network may make different DNN layers antagonistically affected each
other, thus degrading the final performance. In this paper, we propose to
separate a DNN into different parts and progressively train them to address
this problem. Instead of the early stopping, which trains a whole DNN all at
once, we initially train former DNN layers by optimizing the DNN with a
relatively large number of epochs. During training, we progressively train the
latter DNN layers by using a smaller number of epochs with the preceding layers
fixed to counteract the impact of noisy labels. We term the proposed method as
progressive early stopping (PES). Despite its simplicity, compared with the
early stopping, PES can help to obtain more promising and stable results.
Furthermore, by combining PES with existing approaches on noisy label training,
we achieve state-of-the-art performance on image classification benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yingbin Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Erkun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yanhua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiatong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yinian Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Domain Adaptation for Object Detection. (arXiv:2106.15793v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15793</id>
        <link href="http://arxiv.org/abs/2106.15793"/>
        <updated>2021-07-01T01:59:32.864Z</updated>
        <summary type="html"><![CDATA[To reduce annotation labor associated with object detection, an increasing
number of studies focus on transferring the learned knowledge from a labeled
source domain to another unlabeled target domain. However, existing methods
assume that the labeled data are sampled from a single source domain, which
ignores a more generalized scenario, where labeled data are from multiple
source domains. For the more challenging task, we propose a unified Faster
R-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which
can simultaneously enhance domain invariance and preserve discriminative power.
Specifically, the framework contains multiple source subnets and a pseudo
target subnet. First, we propose a hierarchical feature alignment strategy to
conduct strong and weak alignments for low- and high-level features,
respectively, considering their different effects for object detection. Second,
we develop a novel pseudo subnet learning algorithm to approximate optimal
parameters of pseudo target subset by weighted combination of parameters in
different source subnets. Finally, a consistency regularization for region
proposal network is proposed to facilitate each subnet to learn more abstract
invariances. Extensive experiments on different adaptation scenarios
demonstrate the effectiveness of the proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xingxu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sicheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Pengfei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jufeng Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Modeling with Reduced Densities. (arXiv:2007.03834v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03834</id>
        <link href="http://arxiv.org/abs/2007.03834"/>
        <updated>2021-07-01T01:59:32.858Z</updated>
        <summary type="html"><![CDATA[This work originates from the observation that today's state of the art
statistical language models are impressive not only for their performance, but
also - and quite crucially - because they are built entirely from correlations
in unstructured text data. The latter observation prompts a fundamental
question that lies at the heart of this paper: What mathematical structure
exists in unstructured text data? We put forth enriched category theory as a
natural answer. We show that sequences of symbols from a finite alphabet, such
as those found in a corpus of text, form a category enriched over
probabilities. We then address a second fundamental question: How can this
information be stored and modeled in a way that preserves the categorical
structure? We answer this by constructing a functor from our enriched category
of text to a particular enriched category of reduced density operators. The
latter leverages the Loewner order on positive semidefinite operators, which
can further be interpreted as a toy example of entailment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bradley_T/0/1/0/all/0/1"&gt;Tai-Danae Bradley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlassopoulos_Y/0/1/0/all/0/1"&gt;Yiannis Vlassopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XLM-E: Cross-lingual Language Model Pre-training via ELECTRA. (arXiv:2106.16138v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16138</id>
        <link href="http://arxiv.org/abs/2106.16138"/>
        <updated>2021-07-01T01:59:32.852Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce ELECTRA-style tasks to cross-lingual language
model pre-training. Specifically, we present two pre-training tasks, namely
multilingual replaced token detection, and translation replaced token
detection. Besides, we pretrain the model, named as XLM-E, on both multilingual
and parallel corpora. Our model outperforms the baseline models on various
cross-lingual understanding tasks with much less computation cost. Moreover,
analysis shows that XLM-E tends to obtain better cross-lingual transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1"&gt;Zewen Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaohan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shuming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1"&gt;Saksham Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1"&gt;Payal Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xia Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in Authorship Verification. (arXiv:2106.15825v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15825</id>
        <link href="http://arxiv.org/abs/2106.15825"/>
        <updated>2021-07-01T01:59:32.847Z</updated>
        <summary type="html"><![CDATA[The PAN 2021 authorship verification (AV) challenge is part of a three-year
strategy, moving from a cross-topic/closed-set to a cross-topic/open-set AV
task over a collection of fanfiction texts. In this work, we present our
modified hybrid neural-probabilistic framework. It is based on our 2020 winning
submission, with updates to significantly reduce sensitivities to topical
variations and to further improve the system's calibration by means of an
uncertainty-adaptation layer. Our framework additionally includes an
Out-Of-Distribution Detector (O2D2) for defining non-responses, outperforming
all other systems that participated in the PAN 2021 AV task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boenninghoff_B/0/1/0/all/0/1"&gt;Benedikt Boenninghoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nickel_R/0/1/0/all/0/1"&gt;Robert M. Nickel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1"&gt;Dorothea Kolossa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking Outside the Window: Wider-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15754</id>
        <link href="http://arxiv.org/abs/2106.15754"/>
        <updated>2021-07-01T01:59:32.830Z</updated>
        <summary type="html"><![CDATA[Long-range context information is crucial for the semantic segmentation of
High-Resolution (HR) Remote Sensing Images (RSIs). The image cropping
operations, commonly used for training neural networks, limit the perception of
long-range context information in large RSIs. To break this limitation, we
propose a Wider-Context Network (WiCNet) for the semantic segmentation of HR
RSIs. In the WiCNet, apart from a conventional feature extraction network to
aggregate the local information, an extra context branch is designed to
explicitly model the context information in a larger image area. The
information between the two branches is communicated through a Context
Transformer, which is a novel design derived from the Vision Transformer to
model the long-range context correlations. Ablation studies and comparative
experiments conducted on several benchmark datasets prove the effectiveness of
the proposed method. Additionally, we present a new Beijing Land-Use (BLU)
dataset. This is a large-scale HR satellite dataset provided with high-quality
and fine-grained reference labels, which we hope will boost future studies in
this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Lei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shaofu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiaojie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuebin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1"&gt;Lorenzo Bruzzone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles. (arXiv:2105.06456v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06456</id>
        <link href="http://arxiv.org/abs/2105.06456"/>
        <updated>2021-07-01T01:59:32.824Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce a corpus for satire detection in Romanian news. We
gathered 55,608 public news articles from multiple real and satirical news
sources, composing one of the largest corpora for satire detection regardless
of language and the only one for the Romanian language. We provide an official
split of the text samples, such that training news articles belong to different
sources than test news articles, thus ensuring that models do not achieve high
performance simply due to overfitting. We conduct experiments with two
state-of-the-art deep neural models, resulting in a set of strong baselines for
our novel corpus. Our results show that the machine-level accuracy for satire
detection in Romanian is quite low (under 73% on the test set) compared to the
human-level accuracy (87%), leaving enough room for improvement in future
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rogoz_A/0/1/0/all/0/1"&gt;Ana-Cristina Rogoz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1"&gt;Mihaela Gaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word-level Sign Language Recognition with Multi-stream Neural Networks Focusing on Local Regions. (arXiv:2106.15989v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15989</id>
        <link href="http://arxiv.org/abs/2106.15989"/>
        <updated>2021-07-01T01:59:32.811Z</updated>
        <summary type="html"><![CDATA[In recent years, Word-level Sign Language Recognition (WSLR) research has
gained popularity in the computer vision community, and thus various approaches
have been proposed. Among these approaches, the method using I3D network
achieves the highest recognition accuracy on large public datasets for WSLR.
However, the method with I3D only utilizes appearance information of the upper
body of the signers to recognize sign language words. On the other hand, in
WSLR, the information of local regions, such as the hand shape and facial
expression, and the positional relationship among the body and both hands are
important. Thus in this work, we utilized local region images of both hands and
face, along with skeletal information to capture local information and the
positions of both hands relative to the body, respectively. In other words, we
propose a novel multi-stream WSLR framework, in which a stream with local
region images and a stream with skeletal information are introduced by
extending I3D network to improve the recognition accuracy of WSLR. From the
experimental results on WLASL dataset, it is evident that the proposed method
has achieved about 15% improvement in the Top-1 accuracy than the existing
conventional methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maruyama_M/0/1/0/all/0/1"&gt;Mizuki Maruyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Shuvozit Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1"&gt;Katsufumi Inoue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1"&gt;Partha Pratim Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwamura_M/0/1/0/all/0/1"&gt;Masakazu Iwamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1"&gt;Michifumi Yoshioka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unaware Fairness: Hierarchical Random Forest for Protected Classes. (arXiv:2106.15767v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15767</id>
        <link href="http://arxiv.org/abs/2106.15767"/>
        <updated>2021-07-01T01:59:32.805Z</updated>
        <summary type="html"><![CDATA[Procedural fairness has been a public concern, which leads to controversy
when making decisions with respect to protected classes, such as race, social
status, and disability. Some protected classes can be inferred according to
some safe proxies like surname and geolocation for the race. Hence, implicitly
utilizing the predicted protected classes based on the related proxies when
making decisions is an efficient approach to circumvent this issue and seek
just decisions. In this article, we propose a hierarchical random forest model
for prediction without explicitly involving protected classes. Simulation
experiments are conducted to show the performance of the hierarchical random
forest model. An example is analyzed from Boston police interview records to
illustrate the usefulness of the proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xian Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Epidemic Control as a Contextual Combinatorial Bandit with Budget. (arXiv:2106.15808v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15808</id>
        <link href="http://arxiv.org/abs/2106.15808"/>
        <updated>2021-07-01T01:59:32.799Z</updated>
        <summary type="html"><![CDATA[In light of the COVID-19 pandemic, it is an open challenge and critical
practical problem to find a optimal way to dynamically prescribe the best
policies that balance both the governmental resources and epidemic control in
different countries and regions. To solve this multi-dimensional tradeoff of
exploitation and exploration, we formulate this technical challenge as a
contextual combinatorial bandit problem that jointly optimizes a multi-criteria
reward function. Given the historical daily cases in a region and the past
intervention plans in place, the agent should generate useful intervention
plans that policy makers can implement in real time to minimizing both the
number of daily COVID-19 cases and the stringency of the recommended
interventions. We prove this concept with simulations of multiple realistic
policy making scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Baihan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1"&gt;Djallel Bouneffouf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Assessment of Dialog Evaluation Metrics. (arXiv:2106.03706v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03706</id>
        <link href="http://arxiv.org/abs/2106.03706"/>
        <updated>2021-07-01T01:59:32.792Z</updated>
        <summary type="html"><![CDATA[Automatic evaluation metrics are a crucial component of dialog systems
research. Standard language evaluation metrics are known to be ineffective for
evaluating dialog. As such, recent research has proposed a number of novel,
dialog-specific metrics that correlate better with human judgements. Due to the
fast pace of research, many of these metrics have been assessed on different
datasets and there has as yet been no time for a systematic comparison between
them. To this end, this paper provides a comprehensive assessment of recently
proposed dialog evaluation metrics on a number of datasets. In this paper, 17
different automatic evaluation metrics are evaluated on 10 different datasets.
Furthermore, the metrics are assessed in different settings, to better qualify
their respective strengths and weaknesses. Metrics are assessed (1) on both the
turn level and the dialog level, (2) for different dialog lengths, (3) for
different dialog qualities (e.g., coherence, engaging), (4) for different types
of response generation models (i.e., generative, retrieval, simple models and
state-of-the-art models), (5) taking into account the similarity of different
metrics and (6) exploring combinations of different metrics. This comprehensive
assessment offers several takeaways pertaining to dialog evaluation metrics in
general. It also suggests how to best assess evaluation metrics and indicates
promising directions for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1"&gt;Yi-Ting Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1"&gt;Maxine Eskenazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1"&gt;Shikib Mehri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge Representation Learning with Hypergraphs. (arXiv:2106.15845v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15845</id>
        <link href="http://arxiv.org/abs/2106.15845"/>
        <updated>2021-07-01T01:59:32.775Z</updated>
        <summary type="html"><![CDATA[Graph neural networks have recently achieved remarkable success in
representing graph-structured data, with rapid progress in both the node
embedding and graph pooling methods. Yet, they mostly focus on capturing
information from the nodes considering their connectivity, and not much work
has been done in representing the edges, which are essential components of a
graph. However, for tasks such as graph reconstruction and generation, as well
as graph classification tasks for which the edges are important for
discrimination, accurately representing edges of a given graph is crucial to
the success of the graph representation learning. To this end, we propose a
novel edge representation learning framework based on Dual Hypergraph
Transformation (DHT), which transforms the edges of a graph into the nodes of a
hypergraph. This dual hypergraph construction allows us to apply message
passing techniques for node representations to edges. After obtaining edge
representations from the hypergraphs, we then cluster or drop edges to obtain
holistic graph-level edge representations. We validate our edge representation
learning method with hypergraphs on diverse graph datasets for graph
representation and generation performance, on which our method largely
outperforms existing graph representation learning methods. Moreover, our edge
representation learning and pooling method also largely outperforms
state-of-the-art graph pooling methods on graph classification, not only
because of its accurate edge representation learning, but also due to its
lossless compression of the nodes and removal of irrelevant edges for effective
message passing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1"&gt;Jaehyeong Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1"&gt;Jinheon Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seul Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dongki Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Minki Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[News Article Retrieval in Context for Event-centric Narrative Creation. (arXiv:2106.16053v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16053</id>
        <link href="http://arxiv.org/abs/2106.16053"/>
        <updated>2021-07-01T01:59:32.765Z</updated>
        <summary type="html"><![CDATA[Writers such as journalists often use automatic tools to find relevant
content to include in their narratives. In this paper, we focus on supporting
writers in the news domain to develop event-centric narratives. Given an
incomplete narrative that specifies a main event and a context, we aim to
retrieve news articles that discuss relevant events that would enable the
continuation of the narrative. We formally define this task and propose a
retrieval dataset construction procedure that relies on existing news articles
to simulate incomplete narratives and relevant articles. Experiments on two
datasets derived from this procedure show that state-of-the-art lexical and
semantic rankers are not sufficient for this task. We show that combining those
with a ranker that ranks articles by reverse chronological order outperforms
those rankers alone. We also perform an in-depth quantitative and qualitative
analysis of the results that sheds light on the characteristics of this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1"&gt;Nikos Voskarides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meij_E/0/1/0/all/0/1"&gt;Edgar Meij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sauer_S/0/1/0/all/0/1"&gt;Sabrina Sauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Conditional Splitting Framework for Efficient Constituency Parsing. (arXiv:2106.15760v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15760</id>
        <link href="http://arxiv.org/abs/2106.15760"/>
        <updated>2021-07-01T01:59:32.756Z</updated>
        <summary type="html"><![CDATA[We introduce a generic seq2seq parsing framework that casts constituency
parsing problems (syntactic and discourse parsing) into a series of conditional
splitting decisions. Our parsing model estimates the conditional probability
distribution of possible splitting points in a given text span and supports
efficient top-down decoding, which is linear in number of nodes. The
conditional splitting formulation together with efficient beam search inference
facilitate structural consistency without relying on expensive structured
inference. Crucially, for discourse analysis we show that in our formulation,
discourse segmentation can be framed as a special case of parsing which allows
us to perform discourse parsing without requiring segmentation as a
pre-requisite. Experiments show that our model achieves good results on the
standard syntactic parsing tasks under settings with/without pre-trained
representations and rivals state-of-the-art (SoTA) methods that are more
computationally expensive than ours. In discourse parsing, our method
outperforms SoTA by a good margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thanh-Tung Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1"&gt;Xuan-Phi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UAV-assisted Online Machine Learning over Multi-Tiered Networks: A Hierarchical Nested Personalized Federated Learning Approach. (arXiv:2106.15734v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15734</id>
        <link href="http://arxiv.org/abs/2106.15734"/>
        <updated>2021-07-01T01:59:32.744Z</updated>
        <summary type="html"><![CDATA[We consider distributed machine learning (ML) through unmanned aerial
vehicles (UAVs) for geo-distributed device clusters. We propose five new
technologies/techniques: (i) stratified UAV swarms with leader, worker, and
coordinator UAVs, (ii) hierarchical nested personalized federated learning
(HN-PFL): a holistic distributed ML framework for personalized model training
across the worker-leader-core network hierarchy, (iii) cooperative UAV resource
pooling for distributed ML using the UAVs' local computational capabilities,
(iv) aerial data caching and relaying for efficient data relaying to conduct
ML, and (v) concept/model drift, capturing online data variations at the
devices. We split the UAV-enabled model training problem as two parts. (a)
Network-aware HN-PFL, where we optimize a tradeoff between energy consumption
and ML model performance by configuring data offloading among devices-UAVs and
UAV-UAVs, UAVs' CPU frequencies, and mini-batch sizes subject to
communication/computation network heterogeneity. We tackle this optimization
problem via the method of posynomial condensation and propose a distributed
algorithm with a performance guarantee. (b) Macro-trajectory and learning
duration design, which we formulate as a sequential decision making problem,
tackled via deep reinforcement learning. Our simulations demonstrate the
superiority of our methodology with regards to the distributed ML performance,
the optimization of network resources, and the swarm trajectory efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Su Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1"&gt;Seyyedali Hosseinalipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorlatova_M/0/1/0/all/0/1"&gt;Maria Gorlatova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1"&gt;Christopher G. Brinton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1"&gt;Mung Chiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curvature Graph Neural Network. (arXiv:2106.15762v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15762</id>
        <link href="http://arxiv.org/abs/2106.15762"/>
        <updated>2021-07-01T01:59:32.711Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have achieved great success in many graph-based
tasks. Much work is dedicated to empowering GNNs with the adaptive locality
ability, which enables measuring the importance of neighboring nodes to the
target node by a node-specific mechanism. However, the current node-specific
mechanisms are deficient in distinguishing the importance of nodes in the
topology structure. We believe that the structural importance of neighboring
nodes is closely related to their importance in aggregation. In this paper, we
introduce discrete graph curvature (the Ricci curvature) to quantify the
strength of structural connection of pairwise nodes. And we propose Curvature
Graph Neural Network (CGNN), which effectively improves the adaptive locality
ability of GNNs by leveraging the structural property of graph curvature. To
improve the adaptability of curvature to various datasets, we explicitly
transform curvature into the weights of neighboring nodes by the necessary
Negative Curvature Processing Module and Curvature Normalization Module. Then,
we conduct numerous experiments on various synthetic datasets and real-world
datasets. The experimental results on synthetic datasets show that CGNN
effectively exploits the topology structure information, and the performance is
improved significantly. CGNN outperforms the baselines on 5 dense node
classification benchmark datasets. This study deepens the understanding of how
to utilize advanced topology information and assign the importance of
neighboring nodes from the perspective of graph curvature and encourages us to
bridge the gap between graph theory and neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiawei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Guohua Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-stream Network for Visual Recognition. (arXiv:2105.14734v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14734</id>
        <link href="http://arxiv.org/abs/2105.14734"/>
        <updated>2021-07-01T01:59:32.694Z</updated>
        <summary type="html"><![CDATA[Transformers with remarkable global representation capacities achieve
competitive results for visual tasks, but fail to consider high-level local
pattern information in input images. In this paper, we present a generic
Dual-stream Network (DS-Net) to fully explore the representation capacity of
local and global pattern features for image classification. Our DS-Net can
simultaneously calculate fine-grained and integrated features and efficiently
fuse them. Specifically, we propose an Intra-scale Propagation module to
process two different resolutions in each block and an Inter-Scale Alignment
module to perform information interaction across features at dual scales.
Besides, we also design a Dual-stream FPN (DS-FPN) to further enhance
contextual information for downstream dense predictions. Without bells and
whistles, the propsed DS-Net outperforms Deit-Small by 2.4% in terms of top-1
accuracy on ImageNet-1k and achieves state-of-the-art performance over other
Vision Transformers and ResNets. For object detection and instance
segmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 %
in terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art
scheme, which significantly demonstrates its potential to be a general backbone
in vision tasks. The code will be released soon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1"&gt;Mingyuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Renrui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Honghui Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Teli Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Baochang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shumin Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Graphical Models and Tensor Networks: A Hybrid Framework. (arXiv:2106.15666v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15666</id>
        <link href="http://arxiv.org/abs/2106.15666"/>
        <updated>2021-07-01T01:59:32.688Z</updated>
        <summary type="html"><![CDATA[We investigate a correspondence between two formalisms for discrete
probabilistic modeling: probabilistic graphical models (PGMs) and tensor
networks (TNs), a powerful modeling framework for simulating complex quantum
systems. The graphical calculus of PGMs and TNs exhibits many similarities,
with discrete undirected graphical models (UGMs) being a special case of TNs.
However, more general probabilistic TN models such as Born machines (BMs)
employ complex-valued hidden states to produce novel forms of correlation among
the probabilities. While representing a new modeling resource for capturing
structure in discrete probability distributions, this behavior also renders the
direct application of standard PGM tools impossible. We aim to bridge this gap
by introducing a hybrid PGM-TN formalism that integrates quantum-like
correlations into PGM models in a principled manner, using the
physically-motivated concept of decoherence. We first prove that applying
decoherence to the entirety of a BM model converts it into a discrete UGM, and
conversely, that any subgraph of a discrete UGM can be represented as a
decohered BM. This method allows a broad family of probabilistic TN models to
be encoded as partially decohered BMs, a fact we leverage to combine the
representational strengths of both model families. We experimentally verify the
performance of such hybrid models in a sequential modeling task, and identify
promising uses of our method within the context of existing applications of
graphical models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Miller_J/0/1/0/all/0/1"&gt;Jacob Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roeder_G/0/1/0/all/0/1"&gt;Geoffrey Roeder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bradley_T/0/1/0/all/0/1"&gt;Tai-Danae Bradley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10479</id>
        <link href="http://arxiv.org/abs/2106.10479"/>
        <updated>2021-07-01T01:59:32.668Z</updated>
        <summary type="html"><![CDATA[Transferability estimation is an essential problem in transfer learning to
predict how good the performance is when transferring a source model (or source
task) to a target task. Recent analytical transferability metrics have been
widely used for source model selection and multi-task learning. A major
challenge is how to make transfereability estimation robust under the
cross-domain cross-task settings. The recently proposed OTCE score solves this
problem by considering both domain and task differences, with the help of
transfer experiences on auxiliary tasks, which causes an efficiency overhead.
In this work, we propose a practical transferability metric called JC-NCE score
that dramatically improves the robustness of the task difference estimation in
OTCE, thus removing the need for auxiliary tasks. Specifically, we build the
joint correspondences between source and target data via solving an optimal
transport problem with a ground cost considering both the sample distance and
label distance, and then compute the transferability score as the negative
conditional entropy of the matched labels. Extensive validations under the
intra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE
score outperforms the auxiliary-task free version of OTCE for 7% and 12%,
respectively, and is also more robust than other existing transferability
metrics on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shao-Lun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2C2 - An orthogonal method for Semi-Supervised Learning on fuzzy labels. (arXiv:2106.16209v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16209</id>
        <link href="http://arxiv.org/abs/2106.16209"/>
        <updated>2021-07-01T01:59:32.652Z</updated>
        <summary type="html"><![CDATA[Semi-Supervised Learning (SSL) can decrease the amount of required labeled
image data and thus the cost for deep learning. Most SSL methods only consider
a clear distinction between classes but in many real-world datasets, this clear
distinction is not given due to intra- or interobserver variability. This
variability can lead to different annotations per image. Thus many images have
ambiguous annotations and their label needs to be considered "fuzzy". This
fuzziness of labels must be addressed as it will limit the performance of
Semi-Supervised Learning (SSL) and deep learning in general. We propose
Semi-Supervised Classification & Clustering (S2C2) which can extend many deep
SSL algorithms. S2C2 can estimate the fuzziness of a label and applies SSL as a
classification to certainly labeled data while creating distinct clusters for
images with similar but fuzzy labels. We show that S2C2 results in median 7.4%
better F1-score for classifications and 5.4% lower inner distance of clusters
across multiple SSL algorithms and datasets while being more interpretable due
to the fuzziness estimation of our method. Overall, a combination of
Semi-Supervised Learning with our method S2C2 leads to better handling of the
fuzziness of labels and thus real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1"&gt;Lars Schmarje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1"&gt;Monty Santarossa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1"&gt;Simon-Martin Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelenka_C/0/1/0/all/0/1"&gt;Claudius Zelenka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiko_R/0/1/0/all/0/1"&gt;Rainer Kiko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stracke_J/0/1/0/all/0/1"&gt;Jenny Stracke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Volkmann_N/0/1/0/all/0/1"&gt;Nina Volkmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1"&gt;Reinhard Koch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Onychomycosis Detection Using Deep Neural Networks. (arXiv:2106.16139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16139</id>
        <link href="http://arxiv.org/abs/2106.16139"/>
        <updated>2021-07-01T01:59:32.643Z</updated>
        <summary type="html"><![CDATA[Clinical dermatology, still relies heavily on manual introspection of fungi
within a Potassium Hydroxide (KOH) solution using a brightfield microscope.
However, this method takes a long time, is based on the experience of the
clinician, and has a low accuracy. With the increase of neural network
applications in the field of clinical microscopy it is now possible to automate
such manual processes increasing both efficiency and accuracy. This study
presents a deep neural network structure that enables the rapid solutions for
these problems and can perform automatic fungi detection in grayscale images
without colorants. Microscopic images of 81 fungi and 235 ceratine were
collected. Then, smaller patches were extracted containing 2062 fungi and 2142
ceratine. In order to detect fungus and ceratine, two models were created one
of which was a custom neural network and the other was based on the VGG16
architecture. The developed custom model had 99.84% accuracy, and an area under
the curve (AUC) value of 1.00, while the VGG16 model had 98.89% accuracy and an
AUC value of 0.99. However, average accuracy and AUC value of clinicians is
72.8% and 0.87 respectively. This deep learning model allows the development of
an automated system that can detect fungi within microscopic images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1"&gt;Abdurrahim Yilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varol_R/0/1/0/all/0/1"&gt;Rahmetullah Varol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goktay_F/0/1/0/all/0/1"&gt;Fatih Goktay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gencoglan_G/0/1/0/all/0/1"&gt;Gulsum Gencoglan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demircali_A/0/1/0/all/0/1"&gt;Ali Anil Demircali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilsizoglu_B/0/1/0/all/0/1"&gt;Berk Dilsizoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uvet_H/0/1/0/all/0/1"&gt;Huseyin Uvet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Temporal Adjacent Network for Language Grounding. (arXiv:2106.16136v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16136</id>
        <link href="http://arxiv.org/abs/2106.16136"/>
        <updated>2021-07-01T01:59:32.625Z</updated>
        <summary type="html"><![CDATA[Temporal language grounding (TLG) is a fundamental and challenging problem
for vision and language understanding. Existing methods mainly focus on fully
supervised setting with temporal boundary labels for training, which, however,
suffers expensive cost of annotation. In this work, we are dedicated to weakly
supervised TLG, where multiple description sentences are given to an untrimmed
video without temporal boundary labels. In this task, it is critical to learn a
strong cross-modal semantic alignment between sentence semantics and visual
content. To this end, we introduce a novel weakly supervised temporal adjacent
network (WSTAN) for temporal language grounding. Specifically, WSTAN learns
cross-modal semantic alignment by exploiting temporal adjacent network in a
multiple instance learning (MIL) paradigm, with a whole description paragraph
as input. Moreover, we integrate a complementary branch into the framework,
which explicitly refines the predictions with pseudo supervision from the MIL
stage. An additional self-discriminating loss is devised on both the MIL branch
and the complementary branch, aiming to enhance semantic discrimination by
self-supervising. Extensive experiments are conducted on three widely used
benchmark datasets, \emph{i.e.}, ActivityNet-Captions, Charades-STA, and
DiDeMo, and the results demonstrate the effectiveness of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuechen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiajun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wengang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Video Classification Meets Incremental Classes. (arXiv:2106.15827v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15827</id>
        <link href="http://arxiv.org/abs/2106.15827"/>
        <updated>2021-07-01T01:59:32.619Z</updated>
        <summary type="html"><![CDATA[With the rapid development of social media, tremendous videos with new
classes are generated daily, which raise an urgent demand for video
classification methods that can continuously update new classes while
maintaining the knowledge of old videos with limited storage and computing
resources. In this paper, we summarize this task as \textit{Class-Incremental
Video Classification (CIVC)} and propose a novel framework to address it. As a
subarea of incremental learning tasks, the challenge of \textit{catastrophic
forgetting} is unavoidable in CIVC. To better alleviate it, we utilize some
characteristics of videos. First, we decompose the spatio-temporal knowledge
before distillation rather than treating it as a whole in the knowledge
transfer process; trajectory is also used to refine the decomposition. Second,
we propose a dual granularity exemplar selection method to select and store
representative video instances of old classes and key-frames inside videos
under a tight storage budget. We benchmark our method and previous SOTA
class-incremental learning methods on Something-Something V2 and Kinetics
datasets, and our method outperforms previous methods significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hanbin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1"&gt;Shihao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zibo Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CodeVIO: Visual-Inertial Odometry with Learned Optimizable Dense Depth. (arXiv:2012.10133v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10133</id>
        <link href="http://arxiv.org/abs/2012.10133"/>
        <updated>2021-07-01T01:59:32.614Z</updated>
        <summary type="html"><![CDATA[In this work, we present a lightweight, tightly-coupled deep depth network
and visual-inertial odometry (VIO) system, which can provide accurate state
estimates and dense depth maps of the immediate surroundings. Leveraging the
proposed lightweight Conditional Variational Autoencoder (CVAE) for depth
inference and encoding, we provide the network with previously marginalized
sparse features from VIO to increase the accuracy of initial depth prediction
and generalization capability. The compact encoded depth maps are then updated
jointly with navigation states in a sliding window estimator in order to
provide the dense local scene geometry. We additionally propose a novel method
to obtain the CVAE's Jacobian which is shown to be more than an order of
magnitude faster than previous works, and we additionally leverage
First-Estimate Jacobian (FEJ) to avoid recalculation. As opposed to previous
works relying on completely dense residuals, we propose to only provide sparse
measurements to update the depth code and show through careful experimentation
that our choice of sparse measurements and FEJs can still significantly improve
the estimated depth maps. Our full system also exhibits state-of-the-art pose
estimation accuracy, and we show that it can run in real-time with
single-thread execution while utilizing GPU acceleration only for the network
and code Jacobian.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xingxing Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merrill_N/0/1/0/all/0/1"&gt;Nathaniel Merrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guoquan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resolution learning in deep convolutional networks using scale-space theory. (arXiv:2106.03412v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03412</id>
        <link href="http://arxiv.org/abs/2106.03412"/>
        <updated>2021-07-01T01:59:32.606Z</updated>
        <summary type="html"><![CDATA[Resolution in deep convolutional neural networks (CNNs) is typically bounded
by the receptive field size through filter sizes, and subsampling layers or
strided convolutions on feature maps. The optimal resolution may vary
significantly depending on the dataset. Modern CNNs hard-code their resolution
hyper-parameters in the network architecture which makes tuning such
hyper-parameters cumbersome. We propose to do away with hard-coded resolution
hyper-parameters and aim to learn the appropriate resolution from data. We use
scale-space theory to obtain a self-similar parametrization of filters and make
use of the N-Jet: a truncated Taylor series to approximate a filter by a
learned combination of Gaussian derivative filters. The parameter sigma of the
Gaussian basis controls both the amount of detail the filter encodes and the
spatial extent of the filter. Since sigma is a continuous parameter, we can
optimize it with respect to the loss. The proposed N-Jet layer achieves
comparable performance when used in state-of-the art architectures, while
learning the correct resolution in each layer automatically. We evaluate our
N-Jet layer on both classification and segmentation, and we show that learning
sigma is especially beneficial for inputs at multiple sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1"&gt;Silvia L.Pintea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1"&gt;Nergis Tomen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goes_S/0/1/0/all/0/1"&gt;Stanley F. Goes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1"&gt;Marco Loog&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan C. van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring. (arXiv:2106.16028v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16028</id>
        <link href="http://arxiv.org/abs/2106.16028"/>
        <updated>2021-07-01T01:59:32.594Z</updated>
        <summary type="html"><![CDATA[Real-time video deblurring still remains a challenging task due to the
complexity of spatially and temporally varying blur itself and the requirement
of low computational cost. To improve the network efficiency, we adopt residual
dense blocks into RNN cells, so as to efficiently extract the spatial features
of the current frame. Furthermore, a global spatio-temporal attention module is
proposed to fuse the effective hierarchical features from past and future
frames to help better deblur the current frame. Another issue needs to be
addressed urgently is the lack of a real-world benchmark dataset. Thus, we
contribute a novel dataset (BSD) to the community, by collecting paired
blurry/sharp video clips using a co-axis beam splitter acquisition system.
Experimental results show that the proposed method (ESTRNN) can achieve better
deblurring performance both quantitatively and qualitatively with less
computational cost against state-of-the-art video deblurring methods. In
addition, cross-validation experiments between datasets illustrate the high
generality of BSD over the synthetic datasets. The code and dataset are
released at https://github.com/zzh-tech/ESTRNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhihang Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Ye Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yinqiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bo Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1"&gt;Imari Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09199</id>
        <link href="http://arxiv.org/abs/2006.09199"/>
        <updated>2021-07-01T01:59:32.577Z</updated>
        <summary type="html"><![CDATA[Current methods for learning visually grounded language from videos often
rely on text annotation, such as human generated captions or machine generated
automatic speech recognition (ASR) transcripts. In this work, we introduce the
Audio-Video Language Network (AVLnet), a self-supervised network that learns a
shared audio-visual embedding space directly from raw video inputs. To
circumvent the need for text annotation, we learn audio-visual representations
from randomly segmented video clips and their raw audio waveforms. We train
AVLnet on HowTo100M, a large corpus of publicly available instructional videos,
and evaluate on image retrieval and video retrieval tasks, achieving
state-of-the-art performance. We perform analysis of AVLnet's learned
representations, showing our model utilizes speech and natural sounds to learn
audio-visual concepts. Further, we propose a tri-modal model that jointly
processes raw audio, video, and text captions from videos to learn a
multi-modal semantic embedding space useful for text-video retrieval. Our code,
data, and trained models will be released at avlnet.csail.mit.edu]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1"&gt;Andrew Rouditchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1"&gt;Angie Boggust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1"&gt;David Harwath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Brian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1"&gt;Dhiraj Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1"&gt;Samuel Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1"&gt;Kartik Audhkhasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1"&gt;Hilde Kuehne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1"&gt;Brian Kingsbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1"&gt;Michael Picheny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1"&gt;James Glass&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrently Estimating Reflective Symmetry Planes from Partial Pointclouds. (arXiv:2106.16129v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16129</id>
        <link href="http://arxiv.org/abs/2106.16129"/>
        <updated>2021-07-01T01:59:32.570Z</updated>
        <summary type="html"><![CDATA[Many man-made objects are characterised by a shape that is symmetric along
one or more planar directions. Estimating the location and orientation of such
symmetry planes can aid many tasks such as estimating the overall orientation
of an object of interest or performing shape completion, where a partial scan
of an object is reflected across the estimated symmetry plane in order to
obtain a more detailed shape. Many methods processing 3D data rely on expensive
3D convolutions. In this paper we present an alternative novel encoding that
instead slices the data along the height dimension and passes it sequentially
to a 2D convolutional recurrent regression scheme. The method also comprises a
differentiable least squares step, allowing for end-to-end accurate and fast
processing of both full and partial scans of symmetric objects. We use this
approach to efficiently handle 3D inputs to design a method to estimate planar
reflective symmetries. We show that our approach has an accuracy comparable to
state-of-the-art techniques on the task of planar reflective symmetry
estimation on full synthetic objects. Additionally, we show that it can be
deployed on partial scans of objects in a real-world pipeline to improve the
outputs of a 3D object detector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stoian_M/0/1/0/all/0/1"&gt;Mihaela C&amp;#x103;t&amp;#x103;lina Stoian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallari_T/0/1/0/all/0/1"&gt;Tommaso Cavallari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diff2Dist: Learning Spectrally Distinct Edge Functions, with Applications to Cell Morphology Analysis. (arXiv:2106.15716v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15716</id>
        <link href="http://arxiv.org/abs/2106.15716"/>
        <updated>2021-07-01T01:59:32.560Z</updated>
        <summary type="html"><![CDATA[We present a method for learning "spectrally descriptive" edge weights for
graphs. We generalize a previously known distance measure on graphs (Graph
Diffusion Distance), thereby allowing it to be tuned to minimize an arbitrary
loss function. Because all steps involved in calculating this modified GDD are
differentiable, we demonstrate that it is possible for a small neural network
model to learn edge weights which minimize loss. GDD alone does not effectively
discriminate between graphs constructed from shoot apical meristem images of
wild-type vs. mutant \emph{Arabidopsis thaliana} specimens. However, training
edge weights and kernel parameters with contrastive loss produces a learned
distance metric with large margins between these graph categories. We
demonstrate this by showing improved performance of a simple
k-nearest-neighbors classifier on the learned distance matrix. We also
demonstrate a further application of this method to biological image analysis:
once trained, we use our model to compute the distance between the biological
graphs and a set of graphs output by a cell division simulator. This allows us
to identify simulation parameter regimes which are similar to each class of
graph in our original dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1"&gt;Cory Braker Scott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mjolsness_E/0/1/0/all/0/1"&gt;Eric Mjolsness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1"&gt;Diane Oyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kodera_C/0/1/0/all/0/1"&gt;Chie Kodera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouchez_D/0/1/0/all/0/1"&gt;David Bouchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uyttewaal_M/0/1/0/all/0/1"&gt;Magalie Uyttewaal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diff2Dist: Learning Spectrally Distinct Edge Functions, with Applications to Cell Morphology Analysis. (arXiv:2106.15716v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15716</id>
        <link href="http://arxiv.org/abs/2106.15716"/>
        <updated>2021-07-01T01:59:32.553Z</updated>
        <summary type="html"><![CDATA[We present a method for learning "spectrally descriptive" edge weights for
graphs. We generalize a previously known distance measure on graphs (Graph
Diffusion Distance), thereby allowing it to be tuned to minimize an arbitrary
loss function. Because all steps involved in calculating this modified GDD are
differentiable, we demonstrate that it is possible for a small neural network
model to learn edge weights which minimize loss. GDD alone does not effectively
discriminate between graphs constructed from shoot apical meristem images of
wild-type vs. mutant \emph{Arabidopsis thaliana} specimens. However, training
edge weights and kernel parameters with contrastive loss produces a learned
distance metric with large margins between these graph categories. We
demonstrate this by showing improved performance of a simple
k-nearest-neighbors classifier on the learned distance matrix. We also
demonstrate a further application of this method to biological image analysis:
once trained, we use our model to compute the distance between the biological
graphs and a set of graphs output by a cell division simulator. This allows us
to identify simulation parameter regimes which are similar to each class of
graph in our original dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1"&gt;Cory Braker Scott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mjolsness_E/0/1/0/all/0/1"&gt;Eric Mjolsness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1"&gt;Diane Oyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kodera_C/0/1/0/all/0/1"&gt;Chie Kodera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouchez_D/0/1/0/all/0/1"&gt;David Bouchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uyttewaal_M/0/1/0/all/0/1"&gt;Magalie Uyttewaal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exponential Weights Algorithms for Selective Learning. (arXiv:2106.15662v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15662</id>
        <link href="http://arxiv.org/abs/2106.15662"/>
        <updated>2021-07-01T01:59:32.547Z</updated>
        <summary type="html"><![CDATA[We study the selective learning problem introduced by Qiao and Valiant
(2019), in which the learner observes $n$ labeled data points one at a time. At
a time of its choosing, the learner selects a window length $w$ and a model
$\hat\ell$ from the model class $\mathcal{L}$, and then labels the next $w$
data points using $\hat\ell$. The excess risk incurred by the learner is
defined as the difference between the average loss of $\hat\ell$ over those $w$
data points and the smallest possible average loss among all models in
$\mathcal{L}$ over those $w$ data points.

We give an improved algorithm, termed the hybrid exponential weights
algorithm, that achieves an expected excess risk of $O((\log\log|\mathcal{L}| +
\log\log n)/\log n)$. This result gives a doubly exponential improvement in the
dependence on $|\mathcal{L}|$ over the best known bound of
$O(\sqrt{|\mathcal{L}|/\log n})$. We complement the positive result with an
almost matching lower bound, which suggests the worst-case optimality of the
algorithm.

We also study a more restrictive family of learning algorithms that are
bounded-recall in the sense that when a prediction window of length $w$ is
chosen, the learner's decision only depends on the most recent $w$ data points.
We analyze an exponential weights variant of the ERM algorithm in Qiao and
Valiant (2019). This new algorithm achieves an expected excess risk of
$O(\sqrt{\log |\mathcal{L}|/\log n})$, which is shown to be nearly optimal
among all bounded-recall learners. Our analysis builds on a generalized version
of the selective mean prediction problem in Drucker (2013); Qiao and Valiant
(2019), which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1"&gt;Mingda Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1"&gt;Gregory Valiant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain adaptation for person re-identification on new unlabeled data using AlignedReID++. (arXiv:2106.15693v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15693</id>
        <link href="http://arxiv.org/abs/2106.15693"/>
        <updated>2021-07-01T01:59:32.530Z</updated>
        <summary type="html"><![CDATA[In the world where big data reigns and there is plenty of hardware prepared
to gather a huge amount of non structured data, data acquisition is no longer a
problem. Surveillance cameras are ubiquitous and they capture huge numbers of
people walking across different scenes. However, extracting value from this
data is challenging, specially for tasks that involve human images, such as
face recognition and person re-identification. Annotation of this kind of data
is a challenging and expensive task. In this work we propose a domain
adaptation workflow to allow CNNs that were trained in one domain to be applied
to another domain without the need for new annotation of the target data. Our
method uses AlignedReID++ as the baseline, trained using a Triplet loss with
batch hard. Domain adaptation is done by using pseudo-labels generated using an
unsupervised learning strategy. Our results show that domain adaptation
techniques really improve the performance of the CNN when applied in the target
domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_T/0/1/0/all/0/1"&gt;Tiago de C. G. Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campos_T/0/1/0/all/0/1"&gt;Teofilo E. de Campos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAT Based Analogy Evaluation Framework for Persian Word Embeddings. (arXiv:2106.15674v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15674</id>
        <link href="http://arxiv.org/abs/2106.15674"/>
        <updated>2021-07-01T01:59:32.518Z</updated>
        <summary type="html"><![CDATA[In recent years there has been a special interest in word embeddings as a new
approach to convert words to vectors. It has been a focal point to understand
how much of the semantics of the the words has been transferred into embedding
vectors. This is important as the embedding is going to be used as the basis
for downstream NLP applications and it will be costly to evaluate the
application end-to-end in order to identify quality of the used embedding
model. Generally the word embeddings are evaluated through a number of tests,
including analogy test. In this paper we propose a test framework for Persian
embedding models. Persian is a low resource language and there is no rich
semantic benchmark to evaluate word embedding models for this language. In this
paper we introduce an evaluation framework including a hand crafted Persian SAT
based analogy dataset, a colliquial test set (specific to Persian) and a
benchmark to study the impact of various parameters on the semantic evaluation
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmoudi_S/0/1/0/all/0/1"&gt;Seyyed Ehsan Mahmoudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1"&gt;Mehrnoush Shamsfard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information. (arXiv:2106.15991v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15991</id>
        <link href="http://arxiv.org/abs/2106.15991"/>
        <updated>2021-07-01T01:59:32.512Z</updated>
        <summary type="html"><![CDATA[This article presents a novel approach to incorporate visual cues from
video-data from a wide-angle stereo camera system mounted at an urban
intersection into the forecast of cyclist trajectories. We extract features
from image and optical flow (OF) sequences using 3D convolutional neural
networks (3D-ConvNet) and combine them with features extracted from the
cyclist's past trajectory to forecast future cyclist positions. By the use of
additional information, we are able to improve positional accuracy by about 7.5
% for our test dataset and by up to 22 % for specific motion types compared to
a method solely based on past trajectories. Furthermore, we compare the use of
image sequences to the use of OF sequences as additional information, showing
that OF alone leads to significant improvements in positional accuracy. By
training and testing our methods using a real-world dataset recorded at a
heavily frequented public intersection and evaluating the methods' runtimes, we
demonstrate the applicability in real traffic scenarios. Our code and parts of
our dataset are made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1"&gt;Stefan Zernetsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trupp_O/0/1/0/all/0/1"&gt;Oliver Trupp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1"&gt;Viktor Kress&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1"&gt;Konrad Doll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1"&gt;Bernhard Sick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Errors and Estimating Accuracy on Unlabeled Data with Self-training Ensembles. (arXiv:2106.15728v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15728</id>
        <link href="http://arxiv.org/abs/2106.15728"/>
        <updated>2021-07-01T01:59:32.505Z</updated>
        <summary type="html"><![CDATA[When a deep learning model is deployed in the wild, it can encounter test
data drawn from distributions different from the training data distribution and
suffer drop in performance. For safe deployment, it is essential to estimate
the accuracy of the pre-trained model on the test data. However, the labels for
the test inputs are usually not immediately available in practice, and
obtaining them can be expensive. This observation leads to two challenging
tasks: (1) unsupervised accuracy estimation, which aims to estimate the
accuracy of a pre-trained classifier on a set of unlabeled test inputs; (2)
error detection, which aims to identify mis-classified test inputs. In this
paper, we propose a principled and practically effective framework that
simultaneously addresses the two tasks. The proposed framework iteratively
learns an ensemble of models to identify mis-classified data points and
performs self-training to improve the ensemble with the identified points.
Theoretical analysis demonstrates that our framework enjoys provable guarantees
for both accuracy estimation and error detection under mild conditions readily
satisfied by practical deep learning models. Along with the framework, we
proposed and experimented with two instantiations and achieved state-of-the-art
results on 59 tasks. For example, on iWildCam, one instantiation reduces the
estimation error for unsupervised accuracy estimation by at least 70% and
improves the F1 score for error detection by at least 4.7% compared to existing
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Frederick Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1"&gt;Besim Avci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingyu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiagent Deep Reinforcement Learning: Challenges and Directions Towards Human-Like Approaches. (arXiv:2106.15691v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15691</id>
        <link href="http://arxiv.org/abs/2106.15691"/>
        <updated>2021-07-01T01:59:32.499Z</updated>
        <summary type="html"><![CDATA[This paper surveys the field of multiagent deep reinforcement learning. The
combination of deep neural networks with reinforcement learning has gained
increased traction in recent years and is slowly shifting the focus from
single-agent to multiagent environments. Dealing with multiple agents is
inherently more complex as (a) the future rewards depend on the joint actions
of multiple players and (b) the computational complexity of functions
increases. We present the most common multiagent problem representations and
their main challenges, and identify five research areas that address one or
more of these challenges: centralised training and decentralised execution,
opponent modelling, communication, efficient coordination, and reward shaping.
We find that many computational studies rely on unrealistic assumptions or are
not generalisable to other settings; they struggle to overcome the curse of
dimensionality or nonstationarity. Approaches from psychology and sociology
capture promising relevant behaviours such as communication and coordination.
We suggest that, for multiagent reinforcement learning to be successful, future
research addresses these challenges with an interdisciplinary approach to open
up new possibilities for more human-oriented solutions in multiagent
reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Annie Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1"&gt;Thomas B&amp;#xe4;ck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kononova_A/0/1/0/all/0/1"&gt;Anna V. Kononova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1"&gt;Aske Plaat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAT Based Analogy Evaluation Framework for Persian Word Embeddings. (arXiv:2106.15674v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15674</id>
        <link href="http://arxiv.org/abs/2106.15674"/>
        <updated>2021-07-01T01:59:32.482Z</updated>
        <summary type="html"><![CDATA[In recent years there has been a special interest in word embeddings as a new
approach to convert words to vectors. It has been a focal point to understand
how much of the semantics of the the words has been transferred into embedding
vectors. This is important as the embedding is going to be used as the basis
for downstream NLP applications and it will be costly to evaluate the
application end-to-end in order to identify quality of the used embedding
model. Generally the word embeddings are evaluated through a number of tests,
including analogy test. In this paper we propose a test framework for Persian
embedding models. Persian is a low resource language and there is no rich
semantic benchmark to evaluate word embedding models for this language. In this
paper we introduce an evaluation framework including a hand crafted Persian SAT
based analogy dataset, a colliquial test set (specific to Persian) and a
benchmark to study the impact of various parameters on the semantic evaluation
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmoudi_S/0/1/0/all/0/1"&gt;Seyyed Ehsan Mahmoudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1"&gt;Mehrnoush Shamsfard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Ask Conversational Questions by Optimizing Levenshtein Distance. (arXiv:2106.15903v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15903</id>
        <link href="http://arxiv.org/abs/2106.15903"/>
        <updated>2021-07-01T01:59:32.475Z</updated>
        <summary type="html"><![CDATA[Conversational Question Simplification (CQS) aims to simplify self-contained
questions into conversational ones by incorporating some conversational
characteristics, e.g., anaphora and ellipsis. Existing maximum likelihood
estimation (MLE) based methods often get trapped in easily learned tokens as
all tokens are treated equally during training. In this work, we introduce a
Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the
minimum Levenshtein distance (MLD) through explicit editing actions. RISE is
able to pay attention to tokens that are related to conversational
characteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)
algorithm with a Dynamic Programming based Sampling (DPS) process to improve
exploration. Experimental results on two benchmark datasets show that RISE
significantly outperforms state-of-the-art methods and generalizes well on
unseen data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhongkun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pengjie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhaochun Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Ming Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early Risk Detection of Pathological Gambling, Self-Harm and Depression Using BERT. (arXiv:2106.16175v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16175</id>
        <link href="http://arxiv.org/abs/2106.16175"/>
        <updated>2021-07-01T01:59:32.469Z</updated>
        <summary type="html"><![CDATA[Early risk detection of mental illnesses has a massive positive impact upon
the well-being of people. The eRisk workshop has been at the forefront of
enabling interdisciplinary research in developing computational methods to
automatically estimate early risk factors for mental issues such as depression,
self-harm, anorexia and pathological gambling. In this paper, we present the
contributions of the BLUE team in the 2021 edition of the workshop, in which we
tackle the problems of early detection of gambling addiction, self-harm and
estimating depression severity from social media posts. We employ pre-trained
BERT transformers and data crawled automatically from mental health subreddits
and obtain reasonable results on all three tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1"&gt;Ana-Maria Bucur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1"&gt;Adrian Cosma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1"&gt;Liviu P. Dinu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Map for Active Semantic Goal Navigation. (arXiv:2106.15648v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15648</id>
        <link href="http://arxiv.org/abs/2106.15648"/>
        <updated>2021-07-01T01:59:32.462Z</updated>
        <summary type="html"><![CDATA[We consider the problem of object goal navigation in unseen environments. In
our view, solving this problem requires learning of contextual semantic priors,
a challenging endeavour given the spatial and semantic variability of indoor
environments. Current methods learn to implicitly encode these priors through
goal-oriented navigation policy functions operating on spatial representations
that are limited to the agent's observable areas. In this work, we propose a
novel framework that actively learns to generate semantic maps outside the
field of view of the agent and leverages the uncertainty over the semantic
classes in the unobserved areas to decide on long term goals. We demonstrate
that through this spatial prediction strategy, we are able to learn semantic
priors in scenes that can be leveraged in unknown environments. Additionally,
we show how different objectives can be defined by balancing exploration with
exploitation during searching for semantic targets. Our method is validated in
the visually realistic environments offered by the Matterport3D dataset and
show state of the art results on the object goal navigation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1"&gt;Georgios Georgakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucher_B/0/1/0/all/0/1"&gt;Bernadette Bucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1"&gt;Karl Schmeckpeper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Siddharth Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1"&gt;Kostas Daniilidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers. (arXiv:2106.15772v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.15772</id>
        <link href="http://arxiv.org/abs/2106.15772"/>
        <updated>2021-07-01T01:59:32.440Z</updated>
        <summary type="html"><![CDATA[We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms
of both language patterns and problem types) English math word problem (MWP)
corpus for evaluating the capability of various MWP solvers. Existing MWP
corpora for studying AI progress remain limited either in language usage
patterns or in problem types. We thus present a new English MWP corpus with
2,305 MWPs that cover more text patterns and most problem types taught in
elementary school. Each MWP is annotated with its problem type and grade level
(for indicating the level of difficulty). Furthermore, we propose a metric to
measure the lexicon usage diversity of a given MWP corpus, and demonstrate that
ASDiv is more diverse than existing corpora. Experiments show that our proposed
corpus reflects the true capability of MWP solvers more faithfully.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_S/0/1/0/all/0/1"&gt;Shen-Yun Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chao-Chun Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1"&gt;Keh-Yih Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed Cross Entropy Loss for Neural Machine Translation. (arXiv:2106.15880v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15880</id>
        <link href="http://arxiv.org/abs/2106.15880"/>
        <updated>2021-07-01T01:59:32.425Z</updated>
        <summary type="html"><![CDATA[In neural machine translation, cross entropy (CE) is the standard loss
function in two training methods of auto-regressive models, i.e., teacher
forcing and scheduled sampling. In this paper, we propose mixed cross entropy
loss (mixed CE) as a substitute for CE in both training approaches. In teacher
forcing, the model trained with CE regards the translation problem as a
one-to-one mapping process, while in mixed CE this process can be relaxed to
one-to-many. In scheduled sampling, we show that mixed CE has the potential to
encourage the training and testing behaviours to be similar to each other, more
effectively mitigating the exposure bias problem. We demonstrate the
superiority of mixed CE over CE on several machine translation datasets, WMT'16
Ro-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled
sampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE
consistently outperforms CE on a multi-reference set as well as a challenging
paraphrased reference set. We also found the model trained with mixed CE is
able to provide a better probability distribution defined over the translation
output space. Our code is available at https://github.com/haorannlp/mix.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08208</id>
        <link href="http://arxiv.org/abs/2106.08208"/>
        <updated>2021-07-01T01:59:32.418Z</updated>
        <summary type="html"><![CDATA[Adaptive gradient methods have shown excellent performance for solving many
machine learning problems. Although multiple adaptive methods were recently
studied, they mainly focus on either empirical or theoretical aspects and also
only work for specific problems by using specific adaptive learning rates. It
is desired to design a universal framework for practical algorithms of adaptive
gradients with theoretical guarantee to solve general problems. To fill this
gap, we propose a faster and universal framework of adaptive gradients (i.e.,
SUPER-ADAM) by introducing a universal adaptive matrix that includes most
existing adaptive gradient forms. Moreover, our framework can flexibly
integrates the momentum and variance reduced techniques. In particular, our
novel framework provides the convergence analysis support for adaptive gradient
methods under the nonconvex setting. In theoretical analysis, we prove that our
new algorithm can achieve the best known complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of
nonconvex optimization, which matches the lower bound for stochastic smooth
nonconvex optimization. In numerical experiments, we employ various deep
learning tasks to validate that our algorithm consistently outperforms the
existing adaptive algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10587</id>
        <link href="http://arxiv.org/abs/2106.10587"/>
        <updated>2021-07-01T01:59:32.401Z</updated>
        <summary type="html"><![CDATA[Existing computer vision research in categorization struggles with
fine-grained attributes recognition due to the inherently high intra-class
variances and low inter-class variances. SOTA methods tackle this challenge by
locating the most informative image regions and rely on them to classify the
complete image. The most recent work, Vision Transformer (ViT), shows its
strong performance in both traditional and fine-grained classification tasks.
In this work, we propose a multi-stage ViT framework for fine-grained image
classification tasks, which localizes the informative image regions without
requiring architectural changes using the inherent multi-head self-attention
mechanism. We also introduce attention-guided augmentations for improving the
model's capabilities. We demonstrate the value of our approach by experimenting
with four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,
Stanford Dogs, and FGVC7 Plant Pathology. We also prove our model's
interpretability via qualitative results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1"&gt;Marcos V. Conde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1"&gt;Kerem Turgutlu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demographic Fairness in Face Identification: The Watchlist Imbalance Effect. (arXiv:2106.08049v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08049</id>
        <link href="http://arxiv.org/abs/2106.08049"/>
        <updated>2021-07-01T01:59:32.394Z</updated>
        <summary type="html"><![CDATA[Recently, different researchers have found that the gallery composition of a
face database can induce performance differentials to facial identification
systems in which a probe image is compared against up to all stored reference
images to reach a biometric decision. This negative effect is referred to as
"watchlist imbalance effect". In this work, we present a method to
theoretically estimate said effect for a biometric identification system given
its verification performance across demographic groups and the composition of
the used gallery. Further, we report results for identification experiments on
differently composed demographic subsets, i.e. females and males, of the public
academic MORPH database using the open-source ArcFace face recognition system.
It is shown that the database composition has a huge impact on performance
differentials in biometric identification systems, even if performance
differentials are less pronounced in the verification scenario. This study
represents the first detailed analysis of the watchlist imbalance effect which
is expected to be of high interest for future research in the field of facial
recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1"&gt;Pawel Drozdowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1"&gt;Christian Rathgeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1"&gt;Christoph Busch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10013</id>
        <link href="http://arxiv.org/abs/2105.10013"/>
        <updated>2021-07-01T01:59:32.387Z</updated>
        <summary type="html"><![CDATA[Image classification methods are usually trained to perform predictions
taking into account a predefined group of known classes. Real-world problems,
however, may not allow for a full knowledge of the input and label spaces,
making failures in recognition a hazard to deep visual learning. Open set
recognition methods are characterized by the ability to correctly identify
inputs of known and unknown classes. In this context, we propose GeMOS: simple
and plug-and-play open set recognition modules that can be attached to
pretrained Deep Neural Networks for visual recognition. The GeMOS framework
pairs pre-trained Convolutional Neural Networks with generative models for open
set recognition to extract open set scores for each sample, allowing for
failure recognition in object recognition tasks. We conduct a thorough
evaluation of the proposed method in comparison with state-of-the-art open set
algorithms, finding that GeMOS either outperforms or is statistically
indistinguishable from more complex and costly models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1"&gt;Marcos Vendramini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1"&gt;Alexei Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Operator-valued formulas for Riemannian Gradient and Hessian and families of tractable metrics. (arXiv:2009.10159v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10159</id>
        <link href="http://arxiv.org/abs/2009.10159"/>
        <updated>2021-07-01T01:59:32.375Z</updated>
        <summary type="html"><![CDATA[We provide an explicit formula for the Levi-Civita connection and Riemannian
Hessian for a Riemannian manifold that is a quotient of a manifold embedded in
an inner product space with a non-constant metric function. Together with a
classical formula for projection, this allows us to evaluate Riemannian
gradient and Hessian for several families of metrics on classical manifolds,
including a family of metrics on Stiefel manifolds connecting both the constant
and canonical ambient metrics with closed-form geodesics. Using these formulas,
we derive Riemannian optimization frameworks on quotients of Stiefel manifolds,
including flag manifolds, and a new family of complete quotient metrics on the
manifold of positive-semidefinite matrices of fixed rank, considered as a
quotient of a product of Stiefel and positive-definite matrix manifold with
affine-invariant metrics. The method is procedural, and in many instances, the
Riemannian gradient and Hessian formulas could be derived by symbolic calculus.
The method extends the list of potential metrics that could be used in manifold
optimization and machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Du Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAttANet: Global attention agreement for convolutional neural networks. (arXiv:2104.05575v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05575</id>
        <link href="http://arxiv.org/abs/2104.05575"/>
        <updated>2021-07-01T01:59:32.355Z</updated>
        <summary type="html"><![CDATA[Transformer attention architectures, similar to those developed for natural
language processing, have recently proved efficient also in vision, either in
conjunction with or as a replacement for convolutional layers. Typically,
visual attention is inserted in the network architecture as a (series of)
feedforward self-attention module(s), with mutual key-query agreement as the
main selection and routing operation. However efficient, this strategy is only
vaguely compatible with the way that attention is implemented in biological
brains: as a separate and unified network of attentional selection regions,
receiving inputs from and exerting modulatory influence on the entire hierarchy
of visual regions. Here, we report experiments with a simple such attention
system that can improve the performance of standard convolutional networks,
with relatively few additional parameters. Each spatial position in each layer
of the network produces a key-query vector pair; all queries are then pooled
into a global attention query. On the next iteration, the match between each
key and the global attention query modulates the network's activations --
emphasizing or silencing the locations that agree or disagree (respectively)
with the global attention system. We demonstrate the usefulness of this
brain-inspired Global Attention Agreement network (GAttANet) for various
convolutional backbones (from a simple 5-layer toy model to a standard ResNet50
architecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our
global attention system improves accuracy over the corresponding baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1"&gt;Rufin VanRullen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1"&gt;Andrea Alamia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of the Vision-based Approaches for Dietary Assessment. (arXiv:2106.11776v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11776</id>
        <link href="http://arxiv.org/abs/2106.11776"/>
        <updated>2021-07-01T01:59:32.345Z</updated>
        <summary type="html"><![CDATA[Last ten years have witnessed the growth of many computer vision applications
for food recognition. Dietary studies showed that dietary-related problem such
as obesity is associated with other chronic diseases like hypertension,
irregular blood sugar levels, and increased risk of heart attacks. The primary
cause of these problems is poor lifestyle choices and unhealthy dietary habits,
which are manageable by using interactive mHealth apps that use automatic
visual-based methods to assess dietary intake. This review discusses the most
performing methodologies that have been developed so far for automatic food
recognition. First, we will present the rationale of visual-based methods for
food recognition. The core of the paper is the presentation, discussion and
evaluation of these methods on popular food image databases. We also discussed
the mobile applications that are implementing these methods. The review ends
with a discussion of research gaps and future challenges in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1"&gt;Ghalib Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1"&gt;Chu Kiong Loo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Learning with Class Description Regularization. (arXiv:2106.16108v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16108</id>
        <link href="http://arxiv.org/abs/2106.16108"/>
        <updated>2021-07-01T01:59:32.310Z</updated>
        <summary type="html"><![CDATA[The purpose of generative Zero-shot learning (ZSL) is to learning from seen
classes, transfer the learned knowledge, and create samples of unseen classes
from the description of these unseen categories. To achieve better ZSL
accuracies, models need to better understand the descriptions of unseen
classes. We introduce a novel form of regularization that encourages generative
ZSL models to pay more attention to the description of each category. Our
empirical results demonstrate improvements over the performance of multiple
state-of-the-art models on the task of generalized zero-shot recognition and
classification when trained on textual description-based datasets like CUB and
NABirds and attribute-based datasets like AWA2, aPY and SUN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kousha_S/0/1/0/all/0/1"&gt;Shayan Kousha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1"&gt;Marcus A. Brubaker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Super-Resolution via Iterative Refinement. (arXiv:2104.07636v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07636</id>
        <link href="http://arxiv.org/abs/2104.07636"/>
        <updated>2021-07-01T01:59:32.295Z</updated>
        <summary type="html"><![CDATA[We present SR3, an approach to image Super-Resolution via Repeated
Refinement. SR3 adapts denoising diffusion probabilistic models to conditional
image generation and performs super-resolution through a stochastic denoising
process. Inference starts with pure Gaussian noise and iteratively refines the
noisy output using a U-Net model trained on denoising at various noise levels.
SR3 exhibits strong performance on super-resolution tasks at different
magnification factors, on faces and natural images. We conduct human evaluation
on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA
GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic
outputs, while GANs do not exceed a fool rate of 34%. We further show the
effectiveness of SR3 in cascaded image generation, where generative models are
chained with super-resolution models, yielding a competitive FID score of 11.3
on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saharia_C/0/1/0/all/0/1"&gt;Chitwan Saharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1"&gt;William Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salimans_T/0/1/0/all/0/1"&gt;Tim Salimans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pros and Cons of GAN Evaluation Measures: New Developments. (arXiv:2103.09396v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09396</id>
        <link href="http://arxiv.org/abs/2103.09396"/>
        <updated>2021-07-01T01:59:32.288Z</updated>
        <summary type="html"><![CDATA[This work is an update of a previous paper on the same topic published a few
years ago. With the dramatic progress in generative modeling, a suite of new
quantitative and qualitative techniques to evaluate models has emerged.
Although some measures such as Inception Score, Frechet Inception Distance,
Precision-Recall, and Perceptual Path Length are relatively more popular, GAN
evaluation is not a settled issue and there is still room for improvement.
Here, I describe new dimensions that are becoming important in assessing models
(e.g. bias and fairness) and discuss the connection between GAN evaluation and
deepfakes. These are important areas of concern in the machine learning
community today and progress in GAN evaluation can help mitigate them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1"&gt;Ali Borji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interventional Assays for the Latent Space of Autoencoders. (arXiv:2106.16091v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16091</id>
        <link href="http://arxiv.org/abs/2106.16091"/>
        <updated>2021-07-01T01:59:32.268Z</updated>
        <summary type="html"><![CDATA[The encoders and decoders of autoencoders effectively project the input onto
learned manifolds in the latent space and data space respectively. We propose a
framework, called latent responses, for probing the learned data manifold using
interventions in the latent space. Using this framework, we investigate "holes"
in the representation to quantitatively ascertain to what extent the latent
space of a trained VAE is consistent with the chosen prior. Furthermore, we use
the identified structure to improve interpolation between latent vectors. We
evaluate how our analyses improve the quality of the generated samples using
the VAE on a variety of benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1"&gt;Felix Leeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1"&gt;Stefan Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoder-side Cross Resolution Synthesis for Video Compression Enhancement. (arXiv:2012.00650v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00650</id>
        <link href="http://arxiv.org/abs/2012.00650"/>
        <updated>2021-07-01T01:59:32.263Z</updated>
        <summary type="html"><![CDATA[This paper proposes a decoder-side Cross Resolution Synthesis (CRS) module to
pursue better compression efficiency beyond the latest Versatile Video Coding
(VVC), where we encode intra frames at original high resolution (HR), compress
inter frames at a lower resolution (LR), and then super-resolve decoded LR
inter frames with the help from preceding HR intra and neighboring LR inter
frames. For a LR inter frame, a motion alignment and aggregation network (MAN)
is devised to produce temporally aggregated motion representation (AMR) for the
guarantee of temporal smoothness; Another texture compensation network (TCN)
inputs decoded HR intra frame, re-sampled HR intra frame, and this LR inter
frame to generate multiscale affinity map (MAM) and multiscale texture
representation (MTR) for better augmenting spatial details; Finally,
similarity-driven fusion synthesizes AMR, MTR, MAM to upscale LR inter frame
for the removal of compression and resolution re-sampling noises. We enhance
the VVC using proposed CRS, showing averaged 8.76% and 11.93% Bj{\o}ntegaard
Delta Rate (BD-Rate) gains against the latest VVC anchor in Random Access (RA)
and Low-delay P (LDP) settings respectively. In addition, experimental
comparisons to the state-of-the-art super-resolution (SR) based VVC enhancement
methods, and ablation studies are conducted to further report superior
efficiency and generalization of proposed algorithm. All materials will be made
to public at https://njuvision.github.io/CRS for reproducible research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1"&gt;Ming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_z/0/1/0/all/0/1"&gt;zhenyu Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1"&gt;Dandan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhan Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MissFormer: (In-)attention-based handling of missing observations for trajectory filtering and prediction. (arXiv:2106.16009v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16009</id>
        <link href="http://arxiv.org/abs/2106.16009"/>
        <updated>2021-07-01T01:59:32.256Z</updated>
        <summary type="html"><![CDATA[In applications such as object tracking, time-series data inevitably carry
missing observations. Following the success of deep learning-based models for
various sequence learning tasks, these models increasingly replace classic
approaches in object tracking applications for inferring the object motions
state. While traditional tracking approaches can deal with missing
observations, most of their deep counterparts are, by default, not suited for
this.

Towards this end, this paper introduces a transformer-based approach for
handling missing observations in variable input length trajectory data. The
model is formed indirectly by successively increasing the complexity of the
demanded inference tasks. Starting from reproducing noise-free trajectories,
the model then learns to infer trajectories from noisy inputs. By providing
missing tokens, binary-encoded missing events, the model learns to in-attend to
missing data and infers a complete trajectory conditioned on the remaining
inputs. In the case of a sequence of successive missing events, the model then
acts as a pure prediction model. The model's abilities are demonstrated on
synthetic data and real-world data reflecting prototypical object tracking
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1"&gt;Stefan Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1"&gt;Ronny Hug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1"&gt;Wolfgang H&amp;#xfc;bner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1"&gt;Michael Arens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1"&gt;Brendan T. Morris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual-GAN: Towards Unsupervised Cross-Weather Adaptation with Mutual Information Constraint. (arXiv:2106.16000v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16000</id>
        <link href="http://arxiv.org/abs/2106.16000"/>
        <updated>2021-07-01T01:59:32.248Z</updated>
        <summary type="html"><![CDATA[Convolutional neural network (CNN) have proven its success for semantic
segmentation, which is a core task of emerging industrial applications such as
autonomous driving. However, most progress in semantic segmentation of urban
scenes is reported on standard scenarios, i.e., daytime scenes with favorable
illumination conditions. In practical applications, the outdoor weather and
illumination are changeable, e.g., cloudy and nighttime, which results in a
significant drop of semantic segmentation accuracy of CNN only trained with
daytime data. In this paper, we propose a novel generative adversarial network
(namely Mutual-GAN) to alleviate the accuracy decline when daytime-trained
neural network is applied to videos captured under adverse weather conditions.
The proposed Mutual-GAN adopts mutual information constraint to preserve
image-objects during cross-weather adaptation, which is an unsolved problem for
most unsupervised image-to-image translation approaches (e.g., CycleGAN). The
proposed Mutual-GAN is evaluated on two publicly available driving video
datasets (i.e., CamVid and SYNTHIA). The experimental results demonstrate that
our Mutual-GAN can yield visually plausible translated images and significantly
improve the semantic segmentation accuracy of daytime-trained deep learning
network while processing videos under challenging weathers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuexiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast whole-slide cartography in colon cancer histology using superpixels and CNN classification. (arXiv:2106.15893v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.15893</id>
        <link href="http://arxiv.org/abs/2106.15893"/>
        <updated>2021-07-01T01:59:32.242Z</updated>
        <summary type="html"><![CDATA[Whole-slide-image cartography is the process of automatically detecting and
outlining different tissue types in digitized histological specimen. This
semantic segmentation provides a basis for many follow-up analyses and can
potentially guide subsequent medical decisions. Due to their large size,
whole-slide-images typically have to be divided into smaller patches which are
then analyzed individually using machine learning-based approaches. Thereby,
local dependencies of image regions get lost and since a whole-slide-image
comprises many thousands of such patches this process is inherently slow. We
propose to subdivide the image into coherent regions prior to classification by
grouping visually similar adjacent image pixels into larger segments, i.e.
superpixels. Afterwards, only a random subset of patches per superpixel is
classified and patch labels are combined into a single superpixel label. The
algorithm has been developed and validated on a dataset of 159 hand-annotated
whole-slide-images of colon resections and its performance has been compared to
a standard patch-based approach. The algorithm shows an average speed-up of 41%
on the test data and the overall accuracy is increased from 93.8% to 95.7%. We
additionally propose a metric for identifying superpixels with an uncertain
classification so they can be excluded from further analysis. Finally, we
evaluate two potential medical applications, namely tumor area estimation
including tumor invasive margin generation and tumor composition analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1"&gt;Frauke Wilm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benz_M/0/1/0/all/0/1"&gt;Michaela Benz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bruns_V/0/1/0/all/0/1"&gt;Volker Bruns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Baghdadlian_S/0/1/0/all/0/1"&gt;Serop Baghdadlian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dexl_J/0/1/0/all/0/1"&gt;Jakob Dexl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hartmann_D/0/1/0/all/0/1"&gt;David Hartmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuritcyn_P/0/1/0/all/0/1"&gt;Petr Kuritcyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weidenfeller_M/0/1/0/all/0/1"&gt;Martin Weidenfeller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wittenberg_T/0/1/0/all/0/1"&gt;Thomas Wittenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Merkel_S/0/1/0/all/0/1"&gt;Susanne Merkel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hartmann_A/0/1/0/all/0/1"&gt;Arndt Hartmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eckstein_M/0/1/0/all/0/1"&gt;Markus Eckstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Geppert_C/0/1/0/all/0/1"&gt;Carol I. Geppert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Structured Analysis of the Video Degradation Effects on the Performance of a Machine Learning-enabled Pedestrian Detector. (arXiv:2106.15889v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15889</id>
        <link href="http://arxiv.org/abs/2106.15889"/>
        <updated>2021-07-01T01:59:32.233Z</updated>
        <summary type="html"><![CDATA[ML-enabled software systems have been incorporated in many public
demonstrations for automated driving (AD) systems. Such solutions have also
been considered as a crucial approach to aim at SAE Level 5 systems, where the
passengers in such vehicles do not have to interact with the system at all
anymore. Already in 2016, Nvidia demonstrated a complete end-to-end approach
for training the complete software stack covering perception, planning and
decision making, and the actual vehicle control. While such approaches show the
great potential of such ML-enabled systems, there have also been demonstrations
where already changes to single pixels in a video frame can potentially lead to
completely different decisions with dangerous consequences. In this paper, a
structured analysis has been conducted to explore video degradation effects on
the performance of an ML-enabled pedestrian detector. Firstly, a baseline of
applying YOLO to 1,026 frames with pedestrian annotations in the KITTI Vision
Benchmark Suite has been established. Next, video degradation candidates for
each of these frames were generated using the leading video codecs libx264,
libx265, Nvidia HEVC, and AV1: 52 frames for the various compression presets
for color and gray-scale frames resulting in 104 degradation candidates per
original KITTI frame and 426,816 images in total. YOLO was applied to each
image to compute the intersection-over-union (IoU) metric to compare the
performance with the original baseline. While aggressively lossy compression
settings result in significant performance drops as expected, it was also
observed that some configurations actually result in slightly better IoU
results compared to the baseline. The findings show that carefully chosen lossy
video configurations preserve a decent performance of particular ML-enabled
systems while allowing for substantial savings when storing or transmitting
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1"&gt;Christian Berger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RICE: Refining Instance Masks in Cluttered Environments with Graph Neural Networks. (arXiv:2106.15711v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15711</id>
        <link href="http://arxiv.org/abs/2106.15711"/>
        <updated>2021-07-01T01:59:32.203Z</updated>
        <summary type="html"><![CDATA[Segmenting unseen object instances in cluttered environments is an important
capability that robots need when functioning in unstructured environments.
While previous methods have exhibited promising results, they still tend to
provide incorrect results in highly cluttered scenes. We postulate that a
network architecture that encodes relations between objects at a high-level can
be beneficial. Thus, in this work, we propose a novel framework that refines
the output of such methods by utilizing a graph-based representation of
instance masks. We train deep networks capable of sampling smart perturbations
to the segmentations, and a graph neural network, which can encode relations
between objects, to evaluate the perturbed segmentations. Our proposed method
is orthogonal to previous works and achieves state-of-the-art performance when
combined with them. We demonstrate an application that uses uncertainty
estimates generated by our method to guide a manipulator, leading to efficient
understanding of cluttered scenes. Code, models, and video can be found at
https://github.com/chrisdxie/rice .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Christopher Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1"&gt;Arsalan Mousavian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yu Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Contrastive Distribution Learning for Unsupervised Anomaly Detection and Localisation in Medical Images. (arXiv:2103.03423v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03423</id>
        <link href="http://arxiv.org/abs/2103.03423"/>
        <updated>2021-07-01T01:59:32.187Z</updated>
        <summary type="html"><![CDATA[Unsupervised anomaly detection (UAD) learns one-class classifiers exclusively
with normal (i.e., healthy) images to detect any abnormal (i.e., unhealthy)
samples that do not conform to the expected normal patterns. UAD has two main
advantages over its fully supervised counterpart. Firstly, it is able to
directly leverage large datasets available from health screening programs that
contain mostly normal image samples, avoiding the costly manual labelling of
abnormal samples and the subsequent issues involved in training with extremely
class-imbalanced data. Further, UAD approaches can potentially detect and
localise any type of lesions that deviate from the normal patterns. One
significant challenge faced by UAD methods is how to learn effective
low-dimensional image representations to detect and localise subtle
abnormalities, generally consisting of small lesions. To address this
challenge, we propose a novel self-supervised representation learning method,
called Constrained Contrastive Distribution learning for anomaly detection
(CCD), which learns fine-grained feature representations by simultaneously
predicting the distribution of augmented data and image contexts using
contrastive learning with pretext constraints. The learned representations can
be leveraged to train more anomaly-sensitive detection models. Extensive
experiment results show that our method outperforms current state-of-the-art
UAD approaches on three different colonoscopy and fundus screening datasets.
Our code is available at https://github.com/tianyu0207/CCD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1"&gt;Guansong Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fengbei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+chen_Y/0/1/0/all/0/1"&gt;Yuanhong chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1"&gt;Seon Ho Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1"&gt;Johan W. Verjans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rajvinder Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1"&gt;Gustavo Carneiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distill on the Go: Online knowledge distillation in self-supervised learning. (arXiv:2104.09866v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09866</id>
        <link href="http://arxiv.org/abs/2104.09866"/>
        <updated>2021-07-01T01:59:32.169Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning solves pretext prediction tasks that do not require
annotations to learn feature representations. For vision tasks, pretext tasks
such as predicting rotation, solving jigsaw are solely created from the input
data. Yet, predicting this known information helps in learning representations
useful for downstream tasks. However, recent works have shown that wider and
deeper models benefit more from self-supervised learning than smaller models.
To address the issue of self-supervised pre-training of smaller models, we
propose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using
single-stage online knowledge distillation to improve the representation
quality of the smaller models. We employ deep mutual learning strategy in which
two models collaboratively learn from each other to improve one another.
Specifically, each model is trained using self-supervised learning along with
distillation that aligns each model's softmax probabilities of similarity
scores with that of the peer model. We conduct extensive experiments on
multiple benchmark datasets, learning objectives, and architectures to
demonstrate the potential of our proposed method. Our results show significant
performance gain in the presence of noisy and limited labels and generalization
to out-of-distribution data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1"&gt;Prashant Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Small in-distribution changes in 3D perspective and lighting fool both CNNs and Transformers. (arXiv:2106.16198v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16198</id>
        <link href="http://arxiv.org/abs/2106.16198"/>
        <updated>2021-07-01T01:59:32.146Z</updated>
        <summary type="html"><![CDATA[Neural networks are susceptible to small transformations including 2D
rotations and shifts, image crops, and even changes in object colors. This is
often attributed to biases in the training dataset, and the lack of 2D
shift-invariance due to not respecting the sampling theorem. In this paper, we
challenge this hypothesis by training and testing on unbiased datasets, and
showing that networks are brittle to both small 3D perspective changes and
lighting variations which cannot be explained by dataset bias or lack of
shift-invariance. To find these in-distribution errors, we introduce an
evolution strategies (ES) based approach, which we call CMA-Search. Despite
training with a large-scale (0.5 million images), unbiased dataset of camera
and light variations, in over 71% cases CMA-Search can find camera parameters
in the vicinity of a correctly classified image which lead to in-distribution
misclassifications with < 3.6% change in parameters. With lighting changes,
CMA-Search finds misclassifications in 33% cases with < 11.6% change in
parameters. Finally, we extend this method to find misclassifications in the
vicinity of ImageNet images for both ResNet and OpenAI's CLIP model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1"&gt;Spandan Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tzu-Mao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1"&gt;Hanspeter Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05704</id>
        <link href="http://arxiv.org/abs/2104.05704"/>
        <updated>2021-07-01T01:59:32.140Z</updated>
        <summary type="html"><![CDATA[With the rise of Transformers as the standard for language processing, and
their advancements in computer vision, along with their unprecedented size and
amounts of training data, many have come to believe that they are not suitable
for small sets of data. This trend leads to great concerns, including but not
limited to: limited availability of data in certain scientific domains and the
exclusion of those with limited resource from research in the field. In this
paper, we dispel the myth that transformers are "data hungry" and therefore can
only be applied to large sets of data. We show for the first time that with the
right size and tokenization, transformers can perform head-to-head with
state-of-the-art CNNs on small datasets. Our model eliminates the requirement
for class token and positional embeddings through a novel sequence pooling
strategy and the use of convolutions. We show that compared to CNNs, our
compact transformers have fewer parameters and MACs, while obtaining similar
accuracies. Our method is flexible in terms of model size, and can have as
little as 0.28M parameters and achieve reasonable results. It can reach an
accuracy of 95.29 % when training from scratch on CIFAR-10, which is comparable
with modern CNN based approaches, and a significant improvement over previous
Transformer based models. Our simple and compact design democratizes
transformers by making them accessible to those equipped with basic computing
resources and/or dealing with important small datasets. Our method works on
larger datasets, such as ImageNet (80.28% accuracy with 29% parameters of ViT),
and NLP tasks as well. Our code and pre-trained models are publicly available
at https://github.com/SHI-Labs/Compact-Transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1"&gt;Ali Hassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1"&gt;Steven Walton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Nikhil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1"&gt;Abulikemu Abuduweili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Humphrey Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16245</id>
        <link href="http://arxiv.org/abs/2106.16245"/>
        <updated>2021-07-01T01:59:32.133Z</updated>
        <summary type="html"><![CDATA[Model-agnostic meta-learning (MAML) is arguably the most popular
meta-learning algorithm nowadays, given its flexibility to incorporate various
model architectures and to be applied to different problems. Nevertheless, its
performance on few-shot classification is far behind many recent algorithms
dedicated to the problem. In this paper, we point out several key facets of how
to train MAML to excel in few-shot classification. First, we find that a large
number of gradient steps are needed for the inner loop update, which
contradicts the common usage of MAML for few-shot classification. Second, we
find that MAML is sensitive to the permutation of class assignments in
meta-testing: for a few-shot task of $N$ classes, there are exponentially many
ways to assign the learned initialization of the $N$-way classifier to the $N$
classes, leading to an unavoidably huge variance. Third, we investigate several
ways for permutation invariance and find that learning a shared classifier
initialization for all the classes performs the best. On benchmark datasets
such as MiniImageNet and TieredImageNet, our approach, which we name
UNICORN-MAML, performs on a par with or even outperforms state-of-the-art
algorithms, while keeping the simplicity of MAML without adding any extra
sub-networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1"&gt;Wei-Lun Chao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Short Temporal Modeling for Efficient Action Recognition. (arXiv:2106.15787v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15787</id>
        <link href="http://arxiv.org/abs/2106.15787"/>
        <updated>2021-07-01T01:59:32.113Z</updated>
        <summary type="html"><![CDATA[Efficient long-short temporal modeling is key for enhancing the performance
of action recognition task. In this paper, we propose a new two-stream action
recognition network, termed as MENet, consisting of a Motion Enhancement (ME)
module and a Video-level Aggregation (VLA) module to achieve long-short
temporal modeling. Specifically, motion representations have been proved
effective in capturing short-term and high-frequency action. However, current
motion representations are calculated from adjacent frames, which may have poor
interpretation and bring useless information (noisy or blank). Thus, for
short-term motions, we design an efficient ME module to enhance the short-term
motions by mingling the motion saliency among neighboring segments. As for
long-term aggregations, VLA is adopted at the top of the appearance branch to
integrate the long-term dependencies across all segments. The two components of
MENet are complementary in temporal modeling. Extensive experiments are
conducted on UCF101 and HMDB51 benchmarks, which verify the effectiveness and
efficiency of our proposed MENet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Can Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Segmentation of Periocular Near-Infra-Red Eye Images Under Alcohol Effects. (arXiv:2106.15828v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15828</id>
        <link href="http://arxiv.org/abs/2106.15828"/>
        <updated>2021-07-01T01:59:32.106Z</updated>
        <summary type="html"><![CDATA[This paper proposes a new framework to detect, segment, and estimate the
localization of the eyes from a periocular Near-Infra-Red iris image under
alcohol consumption. The purpose of the system is to measure the fitness for
duty. Fitness systems allow us to determine whether a person is physically or
psychologically able to perform their tasks. Our framework is based on an
object detector trained from scratch to detect both eyes from a single image.
Then, two efficient networks were used for semantic segmentation; a Criss-Cross
attention network and DenseNet10, with only 122,514 and 210,732 parameters,
respectively. These networks can find the pupil, iris, and sclera. In the end,
the binary output eye mask is used for pupil and iris diameter estimation with
high precision. Five state-of-the-art algorithms were used for this purpose. A
mixed proposal reached the best results. A second contribution is establishing
an alcohol behavior curve to detect the alcohol presence utilizing a stream of
images captured from an iris instance. Also, a manually labeled database with
more than 20k images was created. Our best method obtains a mean
Intersection-over-Union of 94.54% with DenseNet10 with only 210,732 parameters
and an error of only 1-pixel on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1"&gt;Juan Tapia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droguett_E/0/1/0/all/0/1"&gt;Enrique Lopez Droguett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valenzuela_A/0/1/0/all/0/1"&gt;Andres Valenzuela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benalcazar_D/0/1/0/all/0/1"&gt;Daniel Benalcazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Causa_L/0/1/0/all/0/1"&gt;Leonardo Causa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1"&gt;Christoph Busch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOLO: A Simple Framework for Instance Segmentation. (arXiv:2106.15947v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15947</id>
        <link href="http://arxiv.org/abs/2106.15947"/>
        <updated>2021-07-01T01:59:32.089Z</updated>
        <summary type="html"><![CDATA[Compared to many other dense prediction tasks, e.g., semantic segmentation,
it is the arbitrary number of instances that has made instance segmentation
much more challenging. In order to predict a mask for each instance, mainstream
approaches either follow the 'detect-then-segment' strategy (e.g., Mask R-CNN),
or predict embedding vectors first then cluster pixels into individual
instances. In this paper, we view the task of instance segmentation from a
completely new perspective by introducing the notion of "instance categories",
which assigns categories to each pixel within an instance according to the
instance's location. With this notion, we propose segmenting objects by
locations (SOLO), a simple, direct, and fast framework for instance
segmentation with strong performance. We derive a few SOLO variants (e.g.,
Vanilla SOLO, Decoupled SOLO, Dynamic SOLO) following the basic principle. Our
method directly maps a raw input image to the desired object categories and
instance masks, eliminating the need for the grouping post-processing or the
bounding box detection. Our approach achieves state-of-the-art results for
instance segmentation in terms of both speed and accuracy, while being
considerably simpler than the existing methods. Besides instance segmentation,
our method yields state-of-the-art results in object detection (from our mask
byproduct) and panoptic segmentation. We further demonstrate the flexibility
and high-quality segmentation of SOLO by extending it to perform one-stage
instance-level image matting. Code is available at: https://git.io/AdelaiDet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinlong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rufeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1"&gt;Tao Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positive-unlabeled Learning for Cell Detection in Histopathology Images with Incomplete Annotations. (arXiv:2106.15918v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15918</id>
        <link href="http://arxiv.org/abs/2106.15918"/>
        <updated>2021-07-01T01:59:32.068Z</updated>
        <summary type="html"><![CDATA[Cell detection in histopathology images is of great value in clinical
practice. \textit{Convolutional neural networks} (CNNs) have been applied to
cell detection to improve the detection accuracy, where cell annotations are
required for network training. However, due to the variety and large number of
cells, complete annotations that include every cell of interest in the training
images can be challenging. Usually, incomplete annotations can be achieved,
where positive labeling results are carefully examined to ensure their
reliability but there can be other positive instances, i.e., cells of interest,
that are not included in the annotations. This annotation strategy leads to a
lack of knowledge about true negative samples. Most existing methods simply
treat instances that are not labeled as positive as truly negative during
network training, which can adversely affect the network performance. In this
work, to address the problem of incomplete annotations, we formulate the
training of detection networks as a positive-unlabeled learning problem.
Specifically, the classification loss in network training is revised to take
into account incomplete annotations, where the terms corresponding to negative
samples are approximated with the true positive samples and the other samples
of which the labels are unknown. To evaluate the proposed method, experiments
were performed on a publicly available dataset for mitosis detection in breast
cancer cells, and the experimental results show that our method improves the
performance of cell detection given incomplete annotations for training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zipei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_F/0/1/0/all/0/1"&gt;Fengqian Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1"&gt;Chuyang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape Completion via IMLE. (arXiv:2106.16237v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16237</id>
        <link href="http://arxiv.org/abs/2106.16237"/>
        <updated>2021-07-01T01:59:32.031Z</updated>
        <summary type="html"><![CDATA[Shape completion is the problem of completing partial input shapes such as
partial scans. This problem finds important applications in computer vision and
robotics due to issues such as occlusion or sparsity in real-world data.
However, most of the existing research related to shape completion has been
focused on completing shapes by learning a one-to-one mapping which limits the
diversity and creativity of the produced results. We propose a novel multimodal
shape completion technique that is effectively able to learn a one-to-many
mapping and generates diverse complete shapes. Our approach is based on the
conditional Implicit MaximumLikelihood Estimation (IMLE) technique wherein we
condition our inputs on partial 3D point clouds. We extensively evaluate our
approach by comparing it to various baselines both quantitatively and
qualitatively. We show that our method is superior to alternatives in terms of
completeness and diversity of shapes]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1"&gt;Himanshu Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Saurabh Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shichong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1"&gt;Ali Mahdavi-Amiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Analysis of the Recent Visibility of the SigDial Conference. (arXiv:2106.16196v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16196</id>
        <link href="http://arxiv.org/abs/2106.16196"/>
        <updated>2021-07-01T01:59:32.007Z</updated>
        <summary type="html"><![CDATA[Automated speech and text interfaces are continuing to improve, resulting in
increased research in the area of dialogue systems. Moreover, conferences and
workshops from various fields are focusing more on language through speech and
text mediums as candidates for interaction with applications such as search
interfaces and robots. In this paper, we explore how visible the SigDial
conference is to outside conferences by analysing papers from top Natural
Langauge Processing conferences since 2015 to determine the popularity of
certain SigDial-related topics, as well as analysing what SigDial papers are
being cited by others outside of SigDial. We find that despite a dramatic
increase in dialogue-related research, SigDial visibility has not increased. We
conclude by offering some suggestions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1"&gt;Casey Kennington&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steenson_M/0/1/0/all/0/1"&gt;McKenzie Steenson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms. (arXiv:2106.16174v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.16174</id>
        <link href="http://arxiv.org/abs/2106.16174"/>
        <updated>2021-07-01T01:59:31.999Z</updated>
        <summary type="html"><![CDATA[The cells and their spatial patterns in the tumor microenvironment (TME) play
a key role in tumor evolution, and yet remains an understudied topic in
computational pathology. This study, to the best of our knowledge, is among the
first to hybrid local and global graph methods to profile orchestration and
interaction of cellular components. To address the challenge in hematolymphoid
cancers where the cell classes in TME are unclear, we first implemented cell
level unsupervised learning and identified two new cell subtypes. Local cell
graphs or supercells were built for each image by considering the individual
cell's geospatial location and classes. Then, we applied supercell level
clustering and identified two new cell communities. In the end, we built global
graphs to abstract spatial interaction patterns and extract features for
disease diagnosis. We evaluate the proposed algorithm on H\&E slides of 60
hematolymphoid neoplasm patients and further compared it with three cell level
graph-based algorithms, including the global cell graph, cluster cell graph,
and FLocK. The proposed algorithm achieves a mean diagnosis accuracy of 0.703
with the repeated 5-fold cross-validation scheme. In conclusion, our algorithm
shows superior performance over the existing methods and can be potentially
applied to other cancer types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pingjun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Aminu_M/0/1/0/all/0/1"&gt;Muhammad Aminu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hussein_S/0/1/0/all/0/1"&gt;Siba El Hussein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Khoury_J/0/1/0/all/0/1"&gt;Joseph Khoury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jia Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ResViT: Residual vision transformers for multi-modal medical image synthesis. (arXiv:2106.16031v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.16031</id>
        <link href="http://arxiv.org/abs/2106.16031"/>
        <updated>2021-07-01T01:59:31.980Z</updated>
        <summary type="html"><![CDATA[Multi-modal imaging is a key healthcare technology in the diagnosis and
management of disease, but it is often underutilized due to costs associated
with multiple separate scans. This limitation yields the need for synthesis of
unacquired modalities from the subset of available modalities. In recent years,
generative adversarial network (GAN) models with superior depiction of
structural details have been established as state-of-the-art in numerous
medical image synthesis tasks. However, GANs are characteristically based on
convolutional neural network (CNN) backbones that perform local processing with
compact filters. This inductive bias, in turn, compromises learning of
long-range spatial dependencies. While attention maps incorporated in GANs can
multiplicatively modulate CNN features to emphasize critical image regions,
their capture of global context is mostly implicit. Here, we propose a novel
generative adversarial approach for medical image synthesis, ResViT, to combine
local precision of convolution operators with contextual sensitivity of vision
transformers. Based on an encoder-decoder architecture, ResViT employs a
central bottleneck comprising novel aggregated residual transformer (ART)
blocks that synergistically combine convolutional and transformer modules.
Comprehensive demonstrations are performed for synthesizing missing sequences
in multi-contrast MRI and CT images from MRI. Our results indicate the
superiority of ResViT against competing methods in terms of qualitative
observations and quantitative metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dalmaz_O/0/1/0/all/0/1"&gt;Onat Dalmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1"&gt;Mahmut Yurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1"&gt;Tolga &amp;#xc7;ukur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content-Aware Convolutional Neural Networks. (arXiv:2106.15797v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15797</id>
        <link href="http://arxiv.org/abs/2106.15797"/>
        <updated>2021-07-01T01:59:31.970Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) have achieved great success due to the
powerful feature learning ability of convolution layers. Specifically, the
standard convolution traverses the input images/features using a sliding window
scheme to extract features. However, not all the windows contribute equally to
the prediction results of CNNs. In practice, the convolutional operation on
some of the windows (e.g., smooth windows that contain very similar pixels) can
be very redundant and may introduce noises into the computation. Such
redundancy may not only deteriorate the performance but also incur the
unnecessary computational cost. Thus, it is important to reduce the
computational redundancy of convolution to improve the performance. To this
end, we propose a Content-aware Convolution (CAC) that automatically detects
the smooth windows and applies a 1x1 convolutional kernel to replace the
original large kernel. In this sense, we are able to effectively avoid the
redundant computation on similar pixels. By replacing the standard convolution
in CNNs with our CAC, the resultant models yield significantly better
performance and lower computational cost than the baseline models with the
standard convolution. More critically, we are able to dynamically allocate
suitable computation resources according to the data smoothness of different
images, making it possible for content-aware computation. Extensive experiments
on various computer vision tasks demonstrate the superiority of our method over
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yaofo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingdong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Data Are as Good as the Real for Association Knowledge Learning in Multi-object Tracking. (arXiv:2106.16100v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16100</id>
        <link href="http://arxiv.org/abs/2106.16100"/>
        <updated>2021-07-01T01:59:31.963Z</updated>
        <summary type="html"><![CDATA[Association, aiming to link bounding boxes of the same identity in a video
sequence, is a central component in multi-object tracking (MOT). To train
association modules, e.g., parametric networks, real video data are usually
used. However, annotating person tracks in consecutive video frames is
expensive, and such real data, due to its inflexibility, offer us limited
opportunities to evaluate the system performance w.r.t changing tracking
scenarios. In this paper, we study whether 3D synthetic data can replace
real-world videos for association training. Specifically, we introduce a
large-scale synthetic data engine named MOTX, where the motion characteristics
of cameras and objects are manually configured to be similar to those in
real-world datasets. We show that compared with real data, association
knowledge obtained from synthetic data can achieve very similar performance on
real-world test sets without domain adaption techniques. Our intriguing
observation is credited to two factors. First and foremost, 3D engines can well
simulate motion factors such as camera movement, camera view and object
movement, so that the simulated videos can provide association modules with
effective motion features. Second, experimental results show that the
appearance domain gap hardly harms the learning of association knowledge. In
addition, the strong customization ability of MOTX allows us to quantitatively
assess the impact of motion factors on MOT, which brings new insights to the
community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuchi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhongdao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiangxin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affective Image Content Analysis: Two Decades Review and New Perspectives. (arXiv:2106.16125v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16125</id>
        <link href="http://arxiv.org/abs/2106.16125"/>
        <updated>2021-07-01T01:59:31.957Z</updated>
        <summary type="html"><![CDATA[Images can convey rich semantics and induce various emotions in viewers.
Recently, with the rapid advancement of emotional intelligence and the
explosive growth of visual data, extensive research efforts have been dedicated
to affective image content analysis (AICA). In this survey, we will
comprehensively review the development of AICA in the recent two decades,
especially focusing on the state-of-the-art methods with respect to three main
challenges -- the affective gap, perception subjectivity, and label noise and
absence. We begin with an introduction to the key emotion representation models
that have been widely employed in AICA and description of available datasets
for performing evaluation with quantitative comparison of label noise and
dataset bias. We then summarize and compare the representative approaches on
(1) emotion feature extraction, including both handcrafted and deep features,
(2) learning methods on dominant emotion recognition, personalized emotion
prediction, emotion distribution learning, and learning from noisy data or few
labels, and (3) AICA based applications. Finally, we discuss some challenges
and promising research directions in the future, such as image content and
context understanding, group emotion clustering, and viewer-image interaction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sicheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xingxu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jufeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1"&gt;Guoli Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Guiguang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-Step Adversarial Training for Semantic Segmentation. (arXiv:2106.15998v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15998</id>
        <link href="http://arxiv.org/abs/2106.15998"/>
        <updated>2021-07-01T01:59:31.936Z</updated>
        <summary type="html"><![CDATA[Even though deep neural networks succeed on many different tasks including
semantic segmentation, they lack on robustness against adversarial examples. To
counteract this exploit, often adversarial training is used. However, it is
known that adversarial training with weak adversarial attacks (e.g. using the
Fast Gradient Method) does not improve the robustness against stronger attacks.
Recent research shows that it is possible to increase the robustness of such
single-step methods by choosing an appropriate step size during the training.
Finding such a step size, without increasing the computational effort of
single-step adversarial training, is still an open challenge. In this work we
address the computationally particularly demanding task of semantic
segmentation and propose a new step size control algorithm that increases the
robustness of single-step adversarial training. The proposed algorithm does not
increase the computational effort of single-step adversarial training
considerably and also simplifies training, because it is free of
meta-parameter. We show that the robustness of our approach can compete with
multi-step adversarial training on two popular benchmarks for semantic
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wiens_D/0/1/0/all/0/1"&gt;Daniel Wiens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1"&gt;Barbara Hammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Power of Saturated Transformers: A View from Circuit Complexity. (arXiv:2106.16213v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16213</id>
        <link href="http://arxiv.org/abs/2106.16213"/>
        <updated>2021-07-01T01:59:31.930Z</updated>
        <summary type="html"><![CDATA[Transformers have become a standard architecture for many NLP problems. This
has motivated theoretically analyzing their capabilities as models of language,
in order to understand what makes them successful, and what their potential
weaknesses might be. Recent work has shown that transformers with hard
attention are quite limited in capacity, and in fact can be simulated by
constant-depth circuits. However, hard attention is a restrictive assumption,
which may complicate the relevance of these results for practical transformers.
In this work, we analyze the circuit complexity of transformers with saturated
attention: a generalization of hard attention that more closely captures the
attention patterns learnable in practical transformers. We show that saturated
transformers transcend the limitations of hard-attention transformers. With
some minor assumptions, we prove that the number of bits needed to represent a
saturated transformer memory vector is $O(\log n)$, which implies saturated
transformers can be simulated by log-depth circuits. Thus, the jump from hard
to saturated attention can be understood as increasing the transformer's
effective circuit depth by a factor of $O(\log n)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1"&gt;William Merrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1"&gt;Roy Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Thematic Coherence in Microblogs. (arXiv:2106.15971v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15971</id>
        <link href="http://arxiv.org/abs/2106.15971"/>
        <updated>2021-07-01T01:59:31.923Z</updated>
        <summary type="html"><![CDATA[Collecting together microblogs representing opinions about the same topics
within the same timeframe is useful to a number of different tasks and
practitioners. A major question is how to evaluate the quality of such thematic
clusters. Here we create a corpus of microblog clusters from three different
domains and time windows and define the task of evaluating thematic coherence.
We provide annotation guidelines and human annotations of thematic coherence by
journalist experts. We subsequently investigate the efficacy of different
automated evaluation metrics for the task. We consider a range of metrics
including surface level metrics, ones for topic model coherence and text
generation metrics (TGMs). While surface level metrics perform well,
outperforming topic coherence metrics, they are not as consistent as TGMs. TGMs
are more reliable than all other metrics considered for capturing thematic
coherence in microblog clusters due to being less sensitive to the effect of
time windows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bilal_I/0/1/0/all/0/1"&gt;Iman Munire Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1"&gt;Maria Liakata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1"&gt;Rob Procter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1"&gt;Adam Tsakalidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BLNet: A Fast Deep Learning Framework for Low-Light Image Enhancement with Noise Removal and Color Restoration. (arXiv:2106.15953v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.15953</id>
        <link href="http://arxiv.org/abs/2106.15953"/>
        <updated>2021-07-01T01:59:31.910Z</updated>
        <summary type="html"><![CDATA[Images obtained in real-world low-light conditions are not only low in
brightness, but they also suffer from many other types of degradation, such as
color bias, unknown noise, detail loss and halo artifacts. In this paper, we
propose a very fast deep learning framework called Bringing the Lightness
(denoted as BLNet) that consists of two U-Nets with a series of well-designed
loss functions to tackle all of the above degradations. Based on Retinex
Theory, the decomposition net in our model can decompose low-light images into
reflectance and illumination and remove noise in the reflectance during the
decomposition phase. We propose a Noise and Color Bias Control module (NCBC
Module) that contains a convolutional neural network and two loss functions
(noise loss and color loss). This module is only used to calculate the loss
functions during the training phase, so our method is very fast during the test
phase. This module can smooth the reflectance to achieve the purpose of noise
removal while preserving details and edge information and controlling color
bias. We propose a network that can be trained to learn the mapping between
low-light and normal-light illumination and enhance the brightness of images
taken in low-light illumination. We train and evaluate the performance of our
proposed model over the real-world Low-Light (LOL) dataset), and we also test
our model over several other frequently used datasets (LIME, DICM and MEF
datasets). We conduct extensive experiments to demonstrate that our approach
achieves a promising effect with good rubustness and generalization and
outperforms many other state-of-the-art methods qualitatively and
quantitatively. Our method achieves high speed because we use loss functions
instead of introducing additional denoisers for noise removal and color
correction. The code and model are available at
https://github.com/weixinxu666/BLNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xinxu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xianshi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shisen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Cheng Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yanlin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kaifu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yongjie Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIMPL: Generating Synthetic Overhead Imagery to Address Zero-shot and Few-Shot Detection Problems. (arXiv:2106.15681v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15681</id>
        <link href="http://arxiv.org/abs/2106.15681"/>
        <updated>2021-07-01T01:59:31.903Z</updated>
        <summary type="html"><![CDATA[Recently deep neural networks (DNNs) have achieved tremendous success for
object detection in overhead (e.g., satellite) imagery. One ongoing challenge
however is the acquisition of training data, due to high costs of obtaining
satellite imagery and annotating objects in it. In this work we present a
simple approach - termed Synthetic object IMPLantation (SIMPL) - to easily and
rapidly generate large quantities of synthetic overhead training data for
custom target objects. We demonstrate the effectiveness of using SIMPL
synthetic imagery for training DNNs in zero-shot scenarios where no real
imagery is available; and few-shot learning scenarios, where limited real-world
imagery is available. We also conduct experiments to study the sensitivity of
SIMPL's effectiveness to some key design parameters, providing users for
insights when designing synthetic imagery for custom objects. We release a
software implementation of our SIMPL approach so that others can build upon it,
or use it for their own custom problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Bohao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bradbury_K/0/1/0/all/0/1"&gt;Kyle Bradbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malof_J/0/1/0/all/0/1"&gt;Jordan M. Malof&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Efficiency of Transformers for Resource-Constrained Devices. (arXiv:2106.16006v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.16006</id>
        <link href="http://arxiv.org/abs/2106.16006"/>
        <updated>2021-07-01T01:59:31.887Z</updated>
        <summary type="html"><![CDATA[Transformers provide promising accuracy and have become popular and used in
various domains such as natural language processing and computer vision.
However, due to their massive number of model parameters, memory and
computation requirements, they are not suitable for resource-constrained
low-power devices. Even with high-performance and specialized devices, the
memory bandwidth can become a performance-limiting bottleneck. In this paper,
we present a performance analysis of state-of-the-art vision transformers on
several devices. We propose to reduce the overall memory footprint and memory
transfers by clustering the model parameters. We show that by using only 64
clusters to represent model parameters, it is possible to reduce the data
transfer from the main memory by more than 4x, achieve up to 22% speedup and
39% energy savings on mobile devices with less than 0.1% accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tabani_H/0/1/0/all/0/1"&gt;Hamid Tabani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramaniam_A/0/1/0/all/0/1"&gt;Ajay Balasubramaniam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1"&gt;Shabbir Marzban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Graph Convolutional Neural Networks on 3D Meshes for 3D Object Segmentation and Classification. (arXiv:2106.15778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15778</id>
        <link href="http://arxiv.org/abs/2106.15778"/>
        <updated>2021-07-01T01:59:31.848Z</updated>
        <summary type="html"><![CDATA[This paper presents new designs of graph convolutional neural networks (GCNs)
on 3D meshes for 3D object segmentation and classification. We use the faces of
the mesh as basic processing units and represent a 3D mesh as a graph where
each node corresponds to a face. To enhance the descriptive power of the graph,
we introduce a 1-ring face neighbourhood structure to derive novel
multi-dimensional spatial and structure features to represent the graph nodes.
Based on this new graph representation, we then design a densely connected
graph convolutional block which aggregates local and regional features as the
key construction component to build effective and efficient practical GCN
models for 3D object classification and segmentation. We will present
experimental results to show that our new technique outperforms state of the
art where our models are shown to have the smallest number of parameters and
consietently achieve the highest accuracies across a number of benchmark
datasets. We will also present ablation studies to demonstrate the soundness of
our design principles and the effectiveness of our practical models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1"&gt;Wenming Tang Guoping Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. (arXiv:2106.15831v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15831</id>
        <link href="http://arxiv.org/abs/2106.15831"/>
        <updated>2021-07-01T01:59:31.828Z</updated>
        <summary type="html"><![CDATA[Although machine learning models typically experience a drop in performance
on out-of-distribution data, accuracies on in- versus out-of-distribution data
are widely observed to follow a single linear trend when evaluated across a
testbed of models. Models that are more accurate on the out-of-distribution
data relative to this baseline exhibit "effective robustness" and are
exceedingly rare. Identifying such models, and understanding their properties,
is key to improving out-of-distribution performance. We conduct a thorough
empirical investigation of effective robustness during fine-tuning and
surprisingly find that models pre-trained on larger datasets exhibit effective
robustness during training that vanishes at convergence. We study how
properties of the data influence effective robustness, and we show that it
increases with the larger size, more diversity, and higher example difficulty
of the dataset. We also find that models that display effective robustness are
able to correctly classify 10% of the examples that no other current testbed
model gets correct. Finally, we discuss several strategies for scaling
effective robustness to the high-accuracy regime to improve the
out-of-distribution accuracy of state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1"&gt;Anders Andreassen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_Y/0/1/0/all/0/1"&gt;Yasaman Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1"&gt;Behnam Neyshabur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1"&gt;Rebecca Roelofs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention Aware Wavelet-based Detection of Morphed Face Images. (arXiv:2106.15686v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15686</id>
        <link href="http://arxiv.org/abs/2106.15686"/>
        <updated>2021-07-01T01:59:31.806Z</updated>
        <summary type="html"><![CDATA[Morphed images have exploited loopholes in the face recognition checkpoints,
e.g., Credential Authentication Technology (CAT), used by Transportation
Security Administration (TSA), which is a non-trivial security concern. To
overcome the risks incurred due to morphed presentations, we propose a
wavelet-based morph detection methodology which adopts an end-to-end trainable
soft attention mechanism . Our attention-based deep neural network (DNN)
focuses on the salient Regions of Interest (ROI) which have the most spatial
support for morph detector decision function, i.e, morph class binary softmax
output. A retrospective of morph synthesizing procedure aids us to speculate
the ROI as regions around facial landmarks , particularly for the case of
landmark-based morphing techniques. Moreover, our attention-based DNN is
adapted to the wavelet space, where inputs of the network are coarse-to-fine
spectral representations, 48 stacked wavelet sub-bands to be exact. We evaluate
performance of the proposed framework using three datasets, VISAPP17, LMA, and
MorGAN. In addition, as attention maps can be a robust indicator whether a
probe image under investigation is genuine or counterfeit, we analyze the
estimated attention maps for both a bona fide image and its corresponding
morphed image. Finally, we present an ablation study on the efficacy of
utilizing attention mechanism for the sake of morph detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1"&gt;Poorya Aghdaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1"&gt;Baaria Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1"&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Advances in Fibrosis and Scar Segmentation from Cardiac MRI: A State-of-the-Art Review and Future Perspectives. (arXiv:2106.15707v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.15707</id>
        <link href="http://arxiv.org/abs/2106.15707"/>
        <updated>2021-07-01T01:59:31.800Z</updated>
        <summary type="html"><![CDATA[Segmentation of cardiac fibrosis and scar are essential for clinical
diagnosis and can provide invaluable guidance for the treatment of cardiac
diseases. Late Gadolinium enhancement (LGE) cardiovascular magnetic resonance
(CMR) has been successful for its efficacy in guiding the clinical diagnosis
and treatment reliably. For LGE CMR, many methods have demonstrated success in
accurately segmenting scarring regions. Co-registration with other
non-contrast-agent (non-CA) modalities, balanced steady-state free precession
(bSSFP) and cine magnetic resonance imaging (MRI) for example, can further
enhance the efficacy of automated segmentation of cardiac anatomies. Many
conventional methods have been proposed to provide automated or semi-automated
segmentation of scars. With the development of deep learning in recent years,
we can also see more advanced methods that are more efficient in providing more
accurate segmentations. This paper conducts a state-of-the-art review of
conventional and current state-of-the-art approaches utilising different
modalities for accurate cardiac fibrosis and scar segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yinzhe Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zeyu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Binghuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Firmin_D/0/1/0/all/0/1"&gt;David Firmin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Factual Consistency of Abstractive Summarization on Customer Feedback. (arXiv:2106.16188v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16188</id>
        <link href="http://arxiv.org/abs/2106.16188"/>
        <updated>2021-07-01T01:59:31.768Z</updated>
        <summary type="html"><![CDATA[E-commerce stores collect customer feedback to let sellers learn about
customer concerns and enhance customer order experience. Because customer
feedback often contains redundant information, a concise summary of the
feedback can be generated to help sellers better understand the issues causing
customer dissatisfaction. Previous state-of-the-art abstractive text
summarization models make two major types of factual errors when producing
summaries from customer feedback, which are wrong entity detection (WED) and
incorrect product-defect description (IPD). In this work, we introduce a set of
methods to enhance the factual consistency of abstractive summarization on
customer feedback. We augment the training data with artificially corrupted
summaries, and use them as counterparts of the target summaries. We add a
contrastive loss term into the training objective so that the model learns to
avoid certain factual errors. Evaluation results show that a large portion of
WED and IPD errors are alleviated for BART and T5. Furthermore, our approaches
do not depend on the structure of the summarization model and thus are
generalizable to any abstractive summarization systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yifei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1"&gt;Vincent Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiSubs: A Large-scale Multimodal and Multilingual Dataset. (arXiv:2103.01910v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01910</id>
        <link href="http://arxiv.org/abs/2103.01910"/>
        <updated>2021-07-01T01:59:31.758Z</updated>
        <summary type="html"><![CDATA[This paper introduces a large-scale multimodal and multilingual dataset that
aims to facilitate research on grounding words to images in their contextual
usage in language. The dataset consists of images selected to unambiguously
illustrate concepts expressed in sentences from movie subtitles. The dataset is
a valuable resource as (i) the images are aligned to text fragments rather than
whole sentences; (ii) multiple images are possible for a text fragment and a
sentence; (iii) the sentences are free-form and real-world like; (iv) the
parallel texts are multilingual. We set up a fill-in-the-blank game for humans
to evaluate the quality of the automatic image selection process of our
dataset. We show the utility of the dataset on two automatic tasks: (i)
fill-in-the blank; (ii) lexical translation. Results of the human evaluation
and automatic models demonstrate that images can be a useful complement to the
textual context. The dataset will benefit research on visual grounding of words
especially in the context of free-form sentences, and can be obtained from
https://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Josiah Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1"&gt;Pranava Madhyastha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Figueiredo_J/0/1/0/all/0/1"&gt;Josiel Figueiredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lala_C/0/1/0/all/0/1"&gt;Chiraag Lala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1"&gt;Lucia Specia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatically Select Emotion for Response via Personality-affected Emotion Transition. (arXiv:2106.15846v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15846</id>
        <link href="http://arxiv.org/abs/2106.15846"/>
        <updated>2021-07-01T01:59:31.750Z</updated>
        <summary type="html"><![CDATA[To provide consistent emotional interaction with users, dialog systems should
be capable to automatically select appropriate emotions for responses like
humans. However, most existing works focus on rendering specified emotions in
responses or empathetically respond to the emotion of users, yet the individual
difference in emotion expression is overlooked. This may lead to inconsistent
emotional expressions and disinterest users. To tackle this issue, we propose
to equip the dialog system with personality and enable it to automatically
select emotions in responses by simulating the emotion transition of humans in
conversation. In detail, the emotion of the dialog system is transitioned from
its preceding emotion in context. The transition is triggered by the preceding
dialog context and affected by the specified personality trait. To achieve
this, we first model the emotion transition in the dialog system as the
variation between the preceding emotion and the response emotion in the
Valence-Arousal-Dominance (VAD) emotion space. Then, we design neural networks
to encode the preceding dialog context and the specified personality traits to
compose the variation. Finally, the emotion for response is selected from the
sum of the preceding emotion and the variation. We construct a dialog dataset
with emotion and personality labels and conduct emotion prediction tasks for
evaluation. Experimental results validate the effectiveness of the
personality-affected emotion transition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhiyuan_W/0/1/0/all/0/1"&gt;Wen Zhiyuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiannong_C/0/1/0/all/0/1"&gt;Cao Jiannong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruosong_Y/0/1/0/all/0/1"&gt;Yang Ruosong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuaiqi_L/0/1/0/all/0/1"&gt;Liu Shuaiqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiaxing_S/0/1/0/all/0/1"&gt;Shen Jiaxing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Volctrans Neural Speech Translation System for IWSLT 2021. (arXiv:2105.07319v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07319</id>
        <link href="http://arxiv.org/abs/2105.07319"/>
        <updated>2021-07-01T01:59:31.741Z</updated>
        <summary type="html"><![CDATA[This paper describes the systems submitted to IWSLT 2021 by the Volctrans
team. We participate in the offline speech translation and text-to-text
simultaneous translation tracks. For offline speech translation, our best
end-to-end model achieves 8.1 BLEU improvements over the benchmark on the
MuST-C test set and is even approaching the results of a strong cascade
solution. For text-to-text simultaneous translation, we explore the best
practice to optimize the wait-k model. As a result, our final submitted systems
exceed the benchmark at around 7 BLEU on the same latency regime. We will
publish our code and model to facilitate both future research works and
industrial applications.

This paper describes the systems submitted to IWSLT 2021 by the Volctrans
team. We participate in the offline speech translation and text-to-text
simultaneous translation tracks. For offline speech translation, our best
end-to-end model achieves 7.9 BLEU improvements over the benchmark on the
MuST-C test set and is even approaching the results of a strong cascade
solution. For text-to-text simultaneous translation, we explore the best
practice to optimize the wait-k model. As a result, our final submitted systems
exceed the benchmark at around 7 BLEU on the same latency regime. We release
our code and model at
\url{https://github.com/bytedance/neurst/tree/master/examples/iwslt21} to
facilitate both future research works and industrial applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chengqi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_J/0/1/0/all/0/1"&gt;Jian Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1"&gt;Rong Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1"&gt;Qianqian Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?. (arXiv:2106.15818v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15818</id>
        <link href="http://arxiv.org/abs/2106.15818"/>
        <updated>2021-07-01T01:59:31.718Z</updated>
        <summary type="html"><![CDATA[Whereas existing literature on unsupervised machine translation (MT) focuses
on exploiting unsupervised techniques for low-resource language pairs where
bilingual training data is scare or unavailable, we investigate whether
unsupervised MT can also improve translation quality of high-resource language
pairs where sufficient bitext does exist. We compare the style of correct
translations generated by either supervised or unsupervised MT and find that
the unsupervised output is less monotonic and more natural than supervised
output. We demonstrate a way to combine the benefits of unsupervised and
supervised MT into a single system, resulting in better human evaluation of
quality and fluency. Our results open the door to discussions about the
potential contributions of unsupervised MT in high-resource settings, and how
supervised and unsupervised systems might be mutually-beneficial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1"&gt;Kelly Marchisio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1"&gt;Markus Freitag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1"&gt;David Grangier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Domain Knowledge for Extractive Summarization of Legal Case Documents. (arXiv:2106.15876v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15876</id>
        <link href="http://arxiv.org/abs/2106.15876"/>
        <updated>2021-07-01T01:59:31.712Z</updated>
        <summary type="html"><![CDATA[Automatic summarization of legal case documents is an important and practical
challenge. Apart from many domain-independent text summarization algorithms
that can be used for this purpose, several algorithms have been developed
specifically for summarizing legal case documents. However, most of the
existing algorithms do not systematically incorporate domain knowledge that
specifies what information should ideally be present in a legal case document
summary. To address this gap, we propose an unsupervised summarization
algorithm DELSumm which is designed to systematically incorporate guidelines
from legal experts into an optimization setup. We conduct detailed experiments
over case documents from the Indian Supreme Court. The experiments show that
our proposed unsupervised method outperforms several strong baselines in terms
of ROUGE scores, including both general summarization algorithms and
legal-specific ones. In fact, though our proposed algorithm is unsupervised, it
outperforms several supervised summarization models that are trained over
thousands of document-summary pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1"&gt;Paheli Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1"&gt;Soham Poddar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1"&gt;Koustav Rudra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1"&gt;Kripabandhu Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Saptarshi Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Estimation of Base Models' Weights in Ensemble of Machine Reading Comprehension Systems for Robust Generalization. (arXiv:2106.16013v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16013</id>
        <link href="http://arxiv.org/abs/2106.16013"/>
        <updated>2021-07-01T01:59:31.705Z</updated>
        <summary type="html"><![CDATA[One of the main challenges of the machine reading comprehension (MRC) models
is their fragile out-of-domain generalization, which makes these models not
properly applicable to real-world general-purpose question answering problems.
In this paper, we leverage a zero-shot weighted ensemble method for improving
the robustness of out-of-domain generalization in MRC models. In the proposed
method, a weight estimation module is used to estimate out-of-domain weights,
and an ensemble module aggregate several base models' predictions based on
their weights. The experiments indicate that the proposed method not only
improves the final accuracy, but also is robust against domain changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1"&gt;Razieh Baradaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1"&gt;Hossein Amirkhani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-lingual alignments of ELMo contextual embeddings. (arXiv:2106.15986v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15986</id>
        <link href="http://arxiv.org/abs/2106.15986"/>
        <updated>2021-07-01T01:59:31.698Z</updated>
        <summary type="html"><![CDATA[Building machine learning prediction models for a specific NLP task requires
sufficient training data, which can be difficult to obtain for low-resource
languages. Cross-lingual embeddings map word embeddings from a low-resource
language to a high-resource language so that a prediction model trained on data
from the high-resource language can also be used in the low-resource language.
To produce cross-lingual mappings of recent contextual embeddings, anchor
points between the embedding spaces have to be words in the same context. We
address this issue with a new method for creating datasets for cross-lingual
contextual alignments. Based on that, we propose novel cross-lingual mapping
methods for ELMo embeddings. Our linear mapping methods use existing vecmap and
MUSE alignments on contextual ELMo embeddings. Our new nonlinear ELMoGAN
mapping method is based on GANs and does not assume isomorphic embedding
spaces. We evaluate the proposed mapping methods on nine languages, using two
downstream tasks, NER and dependency parsing. The ELMoGAN method performs well
on the NER task, with low cross-lingual loss compared to direct training on
some languages. In the dependency parsing, linear alignment variants are more
successful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1"&gt;Matej Ul&amp;#x10d;ar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1"&gt;Marko Robnik-&amp;#x160;ikonja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction. (arXiv:2106.15838v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15838</id>
        <link href="http://arxiv.org/abs/2106.15838"/>
        <updated>2021-07-01T01:59:31.691Z</updated>
        <summary type="html"><![CDATA[Text-to-Graph extraction aims to automatically extract information graphs
consisting of mentions and types from natural language texts. Existing
approaches, such as table filling and pairwise scoring, have shown impressive
performance on various information extraction tasks, but they are difficult to
scale to datasets with longer input texts because of their second-order
space/time complexities with respect to the input length. In this work, we
propose a Hybrid Span Generator (HySPA) that invertibly maps the information
graph to an alternating sequence of nodes and edge types, and directly
generates such sequences via a hybrid span decoder which can decode both the
spans and the types recurrently in linear time and space complexities.
Extensive experiments on the ACE05 dataset show that our approach also
significantly outperforms state-of-the-art on the joint entity and relation
extraction task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1"&gt;Liliang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chenkai Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hockenmaier_J/0/1/0/all/0/1"&gt;Julia Hockenmaier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoding Time Lexical Domain Adaptation for Neural Machine Translation. (arXiv:2101.00421v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00421</id>
        <link href="http://arxiv.org/abs/2101.00421"/>
        <updated>2021-07-01T01:59:31.657Z</updated>
        <summary type="html"><![CDATA[Machine translation systems are vulnerable to domain mismatch, especially
when the task is low-resource. In this setting, out of domain translations are
often of poor quality and prone to hallucinations, due to the translation model
preferring to predict common words it has seen during training, as opposed to
the more uncommon ones from a different domain. We present two simple methods
for improving translation quality in this particular setting: First, we use
lexical shortlisting in order to restrict the neural network predictions by IBM
model computed alignments. Second, we perform $n$-best list reordering by
reranking all translations based on the amount they overlap with each other.
Our methods are computationally simpler and faster than alternative approaches,
and show a moderate success on low-resource settings with explicit out of
domain test sets. However, our methods lose their effectiveness when the domain
mismatch is too great, or in high resource setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1"&gt;Nikolay Bogoychev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pinzhen Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The MultiBERTs: BERT Reproductions for Robustness Analysis. (arXiv:2106.16163v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16163</id>
        <link href="http://arxiv.org/abs/2106.16163"/>
        <updated>2021-07-01T01:59:31.644Z</updated>
        <summary type="html"><![CDATA[Experiments with pretrained models such as BERT are often based on a single
checkpoint. While the conclusions drawn apply to the artifact (i.e., the
particular instance of the model), it is not always clear whether they hold for
the more general procedure (which includes the model architecture, training
data, initialization scheme, and loss function). Recent work has shown that
re-running pretraining can lead to substantially different conclusions about
performance, suggesting that alternative evaluations are needed to make
principled statements about procedures. To address this question, we introduce
MultiBERTs: a set of 25 BERT-base checkpoints, trained with similar
hyper-parameters as the original BERT model but differing in random
initialization and data shuffling. The aim is to enable researchers to draw
robust and statistically justified conclusions about pretraining procedures.
The full release includes 25 fully trained checkpoints, as well as statistical
guidelines and a code library implementing our recommended hypothesis testing
methods. Finally, for five of these models we release a set of 28 intermediate
checkpoints in order to support research on learning dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1"&gt;Thibault Sellam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1"&gt;Steve Yadlowsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1"&gt;Naomi Saphra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAmour_A/0/1/0/all/0/1"&gt;Alexander D&amp;#x27;Amour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1"&gt;Tal Linzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1"&gt;Jasmijn Bastings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1"&gt;Iulia Turc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1"&gt;Jacob Eisenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Dipanjan Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1"&gt;Ian Tenney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1"&gt;Ellie Pavlick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Literature Review of Critical Features and General Issues of Freely Available mHealth Apps For Dietary Assessment. (arXiv:2008.09883v3 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09883</id>
        <link href="http://arxiv.org/abs/2008.09883"/>
        <updated>2021-07-01T01:59:31.601Z</updated>
        <summary type="html"><![CDATA[Obesity is known to lower the quality of life substantially. It is often
associated with increased chances of non-communicable diseases such as
diabetes, cardiovascular problems, different types of cancers, etc. Evidence
suggests that diet-related mobile applications play a vital role in assisting
an individual in making healthier choices and keeping track of food intake.
However, due to an abundance of similar applications, it becomes pertinent to
evaluate each of them in terms of functionality, usability, and possible design
issues to truly determine state-of-the-art solutions for the future. Since
these applications involve implementing multiple user requirements and
recommendations from different dietitians, the evaluation becomes quite
complex. Therefore, this study aims to review existing dietary applications at
length to highlight key features and problems that enhance or undermine an
application's usability. For this purpose, we have examined the published
literature from various scientific databases of the CINAHL, Science Direct, and
PUBMED. Out of our findings, fifty-six primary studies met our inclusion
criteria after filtering out titles, abstracts, and full text. A total of 35
apps are analyzed from the selected studies. Our detailed analysis concluded
the comprehensiveness of freely available mHealth applications from users and
dietitians' frames of reference. Furthermore, we have also specified potential
future challenges and stated recommendations to help develop clinically
accurate diet-related applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1"&gt;Ghalib Ahmed Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1"&gt;Chu Kiong Loo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moy_F/0/1/0/all/0/1"&gt;Foong Ming Moy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_N/0/1/0/all/0/1"&gt;Nadine Kong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Reading of Hypotheses for Organizational Research Reviews and Pre-trained Models via R Shiny App for Non-Programmers. (arXiv:2106.16102v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.16102</id>
        <link href="http://arxiv.org/abs/2106.16102"/>
        <updated>2021-07-01T01:59:31.579Z</updated>
        <summary type="html"><![CDATA[The volume of scientific publications in organizational research becomes
exceedingly overwhelming for human researchers who seek to timely extract and
review knowledge. This paper introduces natural language processing (NLP)
models to accelerate the discovery, extraction, and organization of theoretical
developments (i.e., hypotheses) from social science publications. We illustrate
and evaluate NLP models in the context of a systematic review of stakeholder
value constructs and hypotheses. Specifically, we develop NLP models to
automatically 1) detect sentences in scholarly documents as hypotheses or not
(Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs)
and links (causal/associative relationships) (Relationship Deconstruction ),
and 3) classify the features of links in terms causality (versus association)
and direction (positive, negative, versus nonlinear) (Feature Classification).
Our models have reported high performance metrics for all three tasks. While
our models are built in Python, we have made the pre-trained models fully
accessible for non-programmers. We have provided instructions on installing and
using our pre-trained models via an R Shiny app graphic user interface (GUI).
Finally, we suggest the next paths to extend our methodology for
computer-assisted knowledge synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1"&gt;Victor Zitian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montano_Campos_F/0/1/0/all/0/1"&gt;Felipe Montano-Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadrozny_W/0/1/0/all/0/1"&gt;Wlodek Zadrozny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canfield_E/0/1/0/all/0/1"&gt;Evan Canfield&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Spoken Language Understanding using RNN-Transducer ASR. (arXiv:2106.15919v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15919</id>
        <link href="http://arxiv.org/abs/2106.15919"/>
        <updated>2021-07-01T01:59:31.560Z</updated>
        <summary type="html"><![CDATA[We propose an end-to-end trained spoken language understanding (SLU) system
that extracts transcripts, intents and slots from an input speech utterance. It
consists of a streaming recurrent neural network transducer (RNNT) based
automatic speech recognition (ASR) model connected to a neural natural language
understanding (NLU) model through a neural interface. This interface allows for
end-to-end training using multi-task RNNT and NLU losses. Additionally, we
introduce semantic sequence loss training for the joint RNNT-NLU system that
allows direct optimization of non-differentiable SLU metrics. This end-to-end
SLU model paradigm can leverage state-of-the-art advancements and pretrained
models in both ASR and NLU research communities, outperforming recently
proposed direct speech-to-semantics models, and conventional pipelined ASR and
NLU systems. We show that this method improves both ASR and NLU metrics on both
public SLU datasets and large proprietary datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1"&gt;Anirudh Raju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1"&gt;Gautam Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1"&gt;Milind Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1"&gt;Pranav Dheram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1"&gt;Bryan Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_B/0/1/0/all/0/1"&gt;Bach Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1"&gt;Ariya Rastrow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08903</id>
        <link href="http://arxiv.org/abs/2008.08903"/>
        <updated>2021-07-01T01:59:31.548Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have shown remarkable success in
producing realistic-looking images in the computer vision area. Recently,
GAN-based techniques are shown to be promising for spatio-temporal-based
applications such as trajectory prediction, events generation and time-series
data imputation. While several reviews for GANs in computer vision have been
presented, no one has considered addressing the practical applications and
challenges relevant to spatio-temporal data. In this paper, we have conducted a
comprehensive review of the recent developments of GANs for spatio-temporal
data. We summarise the application of popular GAN architectures for
spatio-temporal data and the common practices for evaluating the performance of
spatio-temporal applications with GANs. Finally, we point out future research
directions to benefit researchers in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1"&gt;Nan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1"&gt;Wei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sichen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1"&gt;Kyle Kai Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1"&gt;Arian Prabowo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1"&gt;Mohammad Saiedur Rahaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1"&gt;Flora D. Salim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Chorus Recognition for Improving Song Search. (arXiv:2106.16153v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.16153</id>
        <link href="http://arxiv.org/abs/2106.16153"/>
        <updated>2021-07-01T01:59:31.536Z</updated>
        <summary type="html"><![CDATA[We discuss a novel task, Chorus Recognition, which could potentially benefit
downstream tasks such as song search and music summarization. Different from
the existing tasks such as music summarization or lyrics summarization relying
on single-modal information, this paper models chorus recognition as a
multi-modal one by utilizing both the lyrics and the tune information of songs.
We propose a multi-modal Chorus Recognition model that considers diverse
features. Besides, we also create and publish the first Chorus Recognition
dataset containing 627 songs for public use. Our empirical study performed on
the dataset demonstrates that our approach outperforms several baselines in
chorus recognition. In addition, our approach also helps to improve the
accuracy of its downstream task - song search by more than 10.6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhixu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1"&gt;Binbin Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tingyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingsheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhigang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alzheimer's Dementia Recognition Using Acoustic, Lexical, Disfluency and Speech Pause Features Robust to Noisy Inputs. (arXiv:2106.15684v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15684</id>
        <link href="http://arxiv.org/abs/2106.15684"/>
        <updated>2021-07-01T01:59:31.528Z</updated>
        <summary type="html"><![CDATA[We present two multimodal fusion-based deep learning models that consume ASR
transcribed speech and acoustic data simultaneously to classify whether a
speaker in a structured diagnostic task has Alzheimer's Disease and to what
degree, evaluating the ADReSSo challenge 2021 data. Our best model, a BiLSTM
with highway layers using words, word probabilities, disfluency features, pause
information, and a variety of acoustic features, achieves an accuracy of 84%
and RSME error prediction of 4.26 on MMSE cognitive scores. While predicting
cognitive decline is more challenging, our models show improvement using the
multimodal approach and word probabilities, disfluency and pause information
over word-only models. We show considerable gains for AD classification using
multimodal fusion and gating, which can effectively deal with noisy inputs from
acoustic features and ASR hypotheses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rohanian_M/0/1/0/all/0/1"&gt;Morteza Rohanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hough_J/0/1/0/all/0/1"&gt;Julian Hough&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1"&gt;Matthew Purver&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Evaluation of Rating Systems in Team-based Battle Royale Games. (arXiv:2105.14069v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14069</id>
        <link href="http://arxiv.org/abs/2105.14069"/>
        <updated>2021-07-01T01:59:31.521Z</updated>
        <summary type="html"><![CDATA[Online competitive games have become a mainstream entertainment platform. To
create a fair and exciting experience, these games use rating systems to match
players with similar skills. While there has been an increasing amount of
research on improving the performance of these systems, less attention has been
paid to how their performance is evaluated. In this paper, we explore the
utility of several metrics for evaluating three popular rating systems on a
real-world dataset of over 25,000 team battle royale matches. Our results
suggest considerable differences in their evaluation patterns. Some metrics
were highly impacted by the inclusion of new players. Many could not capture
the real differences between certain groups of players. Among all metrics
studied, normalized discounted cumulative gain (NDCG) demonstrated more
reliable performance and more flexibility. It alleviated most of the challenges
faced by the other metrics while adding the freedom to adjust the focus of the
evaluations on different groups of players.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dehpanah_A/0/1/0/all/0/1"&gt;Arman Dehpanah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghori_M/0/1/0/all/0/1"&gt;Muheeb Faizan Ghori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemmell_J/0/1/0/all/0/1"&gt;Jonathan Gemmell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1"&gt;Bamshad Mobasher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Ask Conversational Questions by Optimizing Levenshtein Distance. (arXiv:2106.15903v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15903</id>
        <link href="http://arxiv.org/abs/2106.15903"/>
        <updated>2021-07-01T01:59:31.503Z</updated>
        <summary type="html"><![CDATA[Conversational Question Simplification (CQS) aims to simplify self-contained
questions into conversational ones by incorporating some conversational
characteristics, e.g., anaphora and ellipsis. Existing maximum likelihood
estimation (MLE) based methods often get trapped in easily learned tokens as
all tokens are treated equally during training. In this work, we introduce a
Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the
minimum Levenshtein distance (MLD) through explicit editing actions. RISE is
able to pay attention to tokens that are related to conversational
characteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)
algorithm with a Dynamic Programming based Sampling (DPS) process to improve
exploration. Experimental results on two benchmark datasets show that RISE
significantly outperforms state-of-the-art methods and generalizes well on
unseen data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhongkun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pengjie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhaochun Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Ming Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer. (arXiv:2106.16171v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16171</id>
        <link href="http://arxiv.org/abs/2106.16171"/>
        <updated>2021-07-01T01:59:31.491Z</updated>
        <summary type="html"><![CDATA[Despite their success, large pre-trained multilingual models have not
completely alleviated the need for labeled data, which is cumbersome to collect
for all target languages. Zero-shot cross-lingual transfer is emerging as a
practical solution: pre-trained models later fine-tuned on one transfer
language exhibit surprising performance when tested on many target languages.
English is the dominant source language for transfer, as reinforced by popular
zero-shot benchmarks. However, this default choice has not been systematically
vetted. In our study, we compare English against other transfer languages for
fine-tuning, on two pre-trained multilingual models (mBERT and mT5) and
multiple classification and question answering tasks. We find that other
high-resource languages such as German and Russian often transfer more
effectively, especially when the set of target languages is diverse or unknown
a priori. Unexpectedly, this can be true even when the training sets were
automatically translated from English. This finding can have immediate impact
on multilingual zero-shot systems, and should inform future benchmark designs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1"&gt;Iulia Turc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kenton Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1"&gt;Jacob Eisenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1"&gt;Ming-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1"&gt;Kristina Toutanova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Multi-modal Fusion Hashing via Hadamard Matrix. (arXiv:2009.12148v3 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12148</id>
        <link href="http://arxiv.org/abs/2009.12148"/>
        <updated>2021-07-01T01:59:31.469Z</updated>
        <summary type="html"><![CDATA[Hashing plays an important role in information retrieval, due to its low
storage and high speed of processing. Among the techniques available in the
literature, multi-modal hashing, which can encode heterogeneous multi-modal
features into compact hash codes, has received particular attention. Most of
the existing multi-modal hashing methods adopt the fixed weighting factors to
fuse multiple modalities for any query data, which cannot capture the variation
of different queries. Besides, many methods introduce hyper-parameters to
balance many regularization terms that make the optimization harder. Meanwhile,
it is time-consuming and labor-intensive to set proper parameter values. The
limitations may significantly hinder their promotion in real applications. In
this paper, we propose a simple, yet effective method that is inspired by the
Hadamard matrix. The proposed method captures the multi-modal feature
information in an adaptive manner and preserves the discriminative semantic
information in the hash codes. Our framework is flexible and involves a very
few hyper-parameters. Extensive experimental results show the method is
effective and achieves superior performance compared to state-of-the-art
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Donglin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1"&gt;Zhenqiu Shu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[News Article Retrieval in Context for Event-centric Narrative Creation. (arXiv:2106.16053v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.16053</id>
        <link href="http://arxiv.org/abs/2106.16053"/>
        <updated>2021-07-01T01:59:31.458Z</updated>
        <summary type="html"><![CDATA[Writers such as journalists often use automatic tools to find relevant
content to include in their narratives. In this paper, we focus on supporting
writers in the news domain to develop event-centric narratives. Given an
incomplete narrative that specifies a main event and a context, we aim to
retrieve news articles that discuss relevant events that would enable the
continuation of the narrative. We formally define this task and propose a
retrieval dataset construction procedure that relies on existing news articles
to simulate incomplete narratives and relevant articles. Experiments on two
datasets derived from this procedure show that state-of-the-art lexical and
semantic rankers are not sufficient for this task. We show that combining those
with a ranker that ranks articles by reverse chronological order outperforms
those rankers alone. We also perform an in-depth quantitative and qualitative
analysis of the results that sheds light on the characteristics of this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1"&gt;Nikos Voskarides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meij_E/0/1/0/all/0/1"&gt;Edgar Meij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sauer_S/0/1/0/all/0/1"&gt;Sabrina Sauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09199</id>
        <link href="http://arxiv.org/abs/2006.09199"/>
        <updated>2021-07-01T01:59:31.361Z</updated>
        <summary type="html"><![CDATA[Current methods for learning visually grounded language from videos often
rely on text annotation, such as human generated captions or machine generated
automatic speech recognition (ASR) transcripts. In this work, we introduce the
Audio-Video Language Network (AVLnet), a self-supervised network that learns a
shared audio-visual embedding space directly from raw video inputs. To
circumvent the need for text annotation, we learn audio-visual representations
from randomly segmented video clips and their raw audio waveforms. We train
AVLnet on HowTo100M, a large corpus of publicly available instructional videos,
and evaluate on image retrieval and video retrieval tasks, achieving
state-of-the-art performance. We perform analysis of AVLnet's learned
representations, showing our model utilizes speech and natural sounds to learn
audio-visual concepts. Further, we propose a tri-modal model that jointly
processes raw audio, video, and text captions from videos to learn a
multi-modal semantic embedding space useful for text-video retrieval. Our code,
data, and trained models will be released at avlnet.csail.mit.edu]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1"&gt;Andrew Rouditchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1"&gt;Angie Boggust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1"&gt;David Harwath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Brian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1"&gt;Dhiraj Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1"&gt;Samuel Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1"&gt;Kartik Audhkhasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1"&gt;Hilde Kuehne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1"&gt;Brian Kingsbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1"&gt;Michael Picheny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1"&gt;James Glass&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the contextual factors affecting multimodal emotion recognition in videos. (arXiv:2004.13274v5 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13274</id>
        <link href="http://arxiv.org/abs/2004.13274"/>
        <updated>2021-07-01T01:59:31.319Z</updated>
        <summary type="html"><![CDATA[Emotional expressions form a key part of user behavior on today's digital
platforms. While multimodal emotion recognition techniques are gaining research
attention, there is a lack of deeper understanding on how visual and non-visual
features can be used to better recognize emotions in certain contexts, but not
others. This study analyzes the interplay between the effects of multimodal
emotion features derived from facial expressions, tone and text in conjunction
with two key contextual factors: i) gender of the speaker, and ii) duration of
the emotional episode. Using a large public dataset of 2,176 manually annotated
YouTube videos, we found that while multimodal features consistently
outperformed bimodal and unimodal features, their performance varied
significantly across different emotions, gender and duration contexts.
Multimodal features performed particularly better for male speakers in
recognizing most emotions. Furthermore, multimodal features performed
particularly better for shorter than for longer videos in recognizing neutral
and happiness, but not sadness and anger. These findings offer new insights
towards the development of more context-aware emotion recognition and
empathetic systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1"&gt;Prasanta Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Raj Kumar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinping Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.16036</id>
        <link href="http://arxiv.org/abs/2106.16036"/>
        <updated>2021-07-01T01:59:31.307Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel way of doing audio synthesis at the waveform
level using Transformer architectures. We propose a deep neural network for
generating waveforms, similar to wavenet \cite{oord2016wavenet}. This is fully
probabilistic, auto-regressive, and causal, i.e. each sample generated depends
only on the previously observed samples. Our approach outperforms a widely used
wavenet architecture by up to 9\% on a similar dataset for predicting the next
step. Using the attention mechanism, we enable the architecture to learn which
audio samples are important for the prediction of the future sample. We show
how causal transformer generative models can be used for raw waveform
synthesis. We also show that this performance can be improved by another 2\% by
conditioning samples over a wider context. The flexibility of the current model
to synthesize audio from latent representations suggests a large number of
potential applications. The novel approach of using generative transformer
architectures for raw audio synthesis is, however, still far away from
generating any meaningful music, without using latent codes/meta-data to aid
the generation process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1"&gt;Prateek Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1"&gt;Chris Chafe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11866</id>
        <link href="http://arxiv.org/abs/2105.11866"/>
        <updated>2021-07-01T01:59:31.292Z</updated>
        <summary type="html"><![CDATA[Factorization machine (FM) is a prevalent approach to modeling pairwise
(second-order) feature interactions when dealing with high-dimensional sparse
data. However, on the one hand, FM fails to capture higher-order feature
interactions suffering from combinatorial expansion, on the other hand, taking
into account interaction between every pair of features may introduce noise and
degrade prediction accuracy. To solve the problems, we propose a novel approach
Graph Factorization Machine (GraphFM) by naturally representing features in the
graph structure. In particular, a novel mechanism is designed to select the
beneficial feature interactions and formulate them as edges between features.
Then our proposed model which integrates the interaction function of FM into
the feature aggregation strategy of Graph Neural Network (GNN), can model
arbitrary-order feature interactions on the graph-structured features by
stacking layers. Experimental results on several real-world datasets has
demonstrated the rationality and effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zeyu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word-level Sign Language Recognition with Multi-stream Neural Networks Focusing on Local Regions. (arXiv:2106.15989v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15989</id>
        <link href="http://arxiv.org/abs/2106.15989"/>
        <updated>2021-07-01T01:59:31.271Z</updated>
        <summary type="html"><![CDATA[In recent years, Word-level Sign Language Recognition (WSLR) research has
gained popularity in the computer vision community, and thus various approaches
have been proposed. Among these approaches, the method using I3D network
achieves the highest recognition accuracy on large public datasets for WSLR.
However, the method with I3D only utilizes appearance information of the upper
body of the signers to recognize sign language words. On the other hand, in
WSLR, the information of local regions, such as the hand shape and facial
expression, and the positional relationship among the body and both hands are
important. Thus in this work, we utilized local region images of both hands and
face, along with skeletal information to capture local information and the
positions of both hands relative to the body, respectively. In other words, we
propose a novel multi-stream WSLR framework, in which a stream with local
region images and a stream with skeletal information are introduced by
extending I3D network to improve the recognition accuracy of WSLR. From the
experimental results on WLASL dataset, it is evident that the proposed method
has achieved about 15% improvement in the Top-1 accuracy than the existing
conventional methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maruyama_M/0/1/0/all/0/1"&gt;Mizuki Maruyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Shuvozit Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1"&gt;Katsufumi Inoue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1"&gt;Partha Pratim Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwamura_M/0/1/0/all/0/1"&gt;Masakazu Iwamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1"&gt;Michifumi Yoshioka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affective Image Content Analysis: Two Decades Review and New Perspectives. (arXiv:2106.16125v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.16125</id>
        <link href="http://arxiv.org/abs/2106.16125"/>
        <updated>2021-07-01T01:59:31.259Z</updated>
        <summary type="html"><![CDATA[Images can convey rich semantics and induce various emotions in viewers.
Recently, with the rapid advancement of emotional intelligence and the
explosive growth of visual data, extensive research efforts have been dedicated
to affective image content analysis (AICA). In this survey, we will
comprehensively review the development of AICA in the recent two decades,
especially focusing on the state-of-the-art methods with respect to three main
challenges -- the affective gap, perception subjectivity, and label noise and
absence. We begin with an introduction to the key emotion representation models
that have been widely employed in AICA and description of available datasets
for performing evaluation with quantitative comparison of label noise and
dataset bias. We then summarize and compare the representative approaches on
(1) emotion feature extraction, including both handcrafted and deep features,
(2) learning methods on dominant emotion recognition, personalized emotion
prediction, emotion distribution learning, and learning from noisy data or few
labels, and (3) AICA based applications. Finally, we discuss some challenges
and promising research directions in the future, such as image content and
context understanding, group emotion clustering, and viewer-image interaction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sicheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xingxu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jufeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1"&gt;Guoli Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Guiguang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Aware Attention-Based Data Augmentation for POI Recommendation. (arXiv:2106.15984v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.15984</id>
        <link href="http://arxiv.org/abs/2106.15984"/>
        <updated>2021-07-01T01:59:31.223Z</updated>
        <summary type="html"><![CDATA[With the rapid growth of location-based social networks (LBSNs),
Point-Of-Interest (POI) recommendation has been broadly studied in this decade.
Recently, the next POI recommendation, a natural extension of POI
recommendation, has attracted much attention. It aims at suggesting the next
POI to a user in spatial and temporal context, which is a practical yet
challenging task in various applications. Existing approaches mainly model the
spatial and temporal information, and memorize historical patterns through
user's trajectories for recommendation. However, they suffer from the negative
impact of missing and irregular check-in data, which significantly influences
the model performance. In this paper, we propose an attention-based
sequence-to-sequence generative model, namely POI-Augmentation Seq2Seq
(PA-Seq2Seq), to address the sparsity of training set by making check-in
records to be evenly-spaced. Specifically, the encoder summarises each check-in
sequence and the decoder predicts the possible missing check-ins based on the
encoded information. In order to learn time-aware correlation among user
history, we employ local attention mechanism to help the decoder focus on a
specific range of context information when predicting a certain missing
check-in point. Extensive experiments have been conducted on two real-world
check-in datasets, Gowalla and Brightkite, for performance and effectiveness
evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yadan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadiq_S/0/1/0/all/0/1"&gt;Shazia W. Sadiq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1"&gt;Peng Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Domain Knowledge for Extractive Summarization of Legal Case Documents. (arXiv:2106.15876v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15876</id>
        <link href="http://arxiv.org/abs/2106.15876"/>
        <updated>2021-07-01T01:59:31.127Z</updated>
        <summary type="html"><![CDATA[Automatic summarization of legal case documents is an important and practical
challenge. Apart from many domain-independent text summarization algorithms
that can be used for this purpose, several algorithms have been developed
specifically for summarizing legal case documents. However, most of the
existing algorithms do not systematically incorporate domain knowledge that
specifies what information should ideally be present in a legal case document
summary. To address this gap, we propose an unsupervised summarization
algorithm DELSumm which is designed to systematically incorporate guidelines
from legal experts into an optimization setup. We conduct detailed experiments
over case documents from the Indian Supreme Court. The experiments show that
our proposed unsupervised method outperforms several strong baselines in terms
of ROUGE scores, including both general summarization algorithms and
legal-specific ones. In fact, though our proposed algorithm is unsupervised, it
outperforms several supervised summarization models that are trained over
thousands of document-summary pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1"&gt;Paheli Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1"&gt;Soham Poddar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1"&gt;Koustav Rudra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1"&gt;Kripabandhu Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Saptarshi Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Adversarial Variational Embedding for Robust Recommendation. (arXiv:2106.15779v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.15779</id>
        <link href="http://arxiv.org/abs/2106.15779"/>
        <updated>2021-07-01T01:59:31.084Z</updated>
        <summary type="html"><![CDATA[Robust recommendation aims at capturing true preference of users from noisy
data, for which there are two lines of methods have been proposed. One is based
on noise injection, and the other is to adopt the generative model Variational
Auto-encoder (VAE). However, the existing works still face two challenges.
First, the noise injection based methods often draw the noise from a fixed
noise distribution given in advance, while in real world, the noise
distributions of different users and items may differ from each other due to
personal behaviors and item usage patterns. Second, the VAE based models are
not expressive enough to capture the true preference since VAE often yields an
embedding space of a single modal, while in real world, user-item interactions
usually exhibit multi-modality on user preference distribution. In this paper,
we propose a novel model called Dual Adversarial Variational Embedding (DAVE)
for robust recommendation, which can provide personalized noise reduction for
different users and items, and capture the multi-modality of the embedding
space, by combining the advantages of VAE and adversarial training between the
introduced auxiliary discriminators and the variational inference networks. The
extensive experiments conducted on real datasets verify the effectiveness of
DAVE on robust recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1"&gt;Qiaomin Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1"&gt;Ning Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Collaborative Signals for Next POI Recommendation with Iterative Seq2Graph Augmentation. (arXiv:2106.15814v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.15814</id>
        <link href="http://arxiv.org/abs/2106.15814"/>
        <updated>2021-07-01T01:59:31.058Z</updated>
        <summary type="html"><![CDATA[Being an indispensable component in location-based social networks, next
point-of-interest (POI) recommendation recommends users unexplored POIs based
on their recent visiting histories. However, existing work mainly models
check-in data as isolated POI sequences, neglecting the crucial collaborative
signals from cross-sequence check-in information. Furthermore, the sparse
POI-POI transitions restrict the ability of a model to learn effective
sequential patterns for recommendation. In this paper, we propose
Sequence-to-Graph (Seq2Graph) augmentation for each POI sequence, allowing
collaborative signals to be propagated from correlated POIs belonging to other
sequences. We then devise a novel Sequence-to-Graph POI Recommender (SGRec),
which jointly learns POI embeddings and infers a user's temporal preferences
from the graph-augmented POI sequence. To overcome the sparsity of POI-level
interactions, we further infuse category-awareness into SGRec with a multi-task
learning scheme that captures the denser category-wise transitions. As such,
SGRec makes full use of the collaborative signals for learning expressive POI
representations, and also comprehensively uncovers multi-level sequential
patterns for user preference modelling. Extensive experiments on two real-world
datasets demonstrate the superiority of SGRec against state-of-the-art methods
in next POI recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Graph Learning on Visual and Kinematics Embeddings for Accurate Gesture Recognition in Robotic Surgery. (arXiv:2011.01619v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01619</id>
        <link href="http://arxiv.org/abs/2011.01619"/>
        <updated>2021-06-30T02:01:04.203Z</updated>
        <summary type="html"><![CDATA[Automatic surgical gesture recognition is fundamentally important to enable
intelligent cognitive assistance in robotic surgery. With recent advancement in
robot-assisted minimally invasive surgery, rich information including surgical
videos and robotic kinematics can be recorded, which provide complementary
knowledge for understanding surgical gestures. However, existing methods either
solely adopt uni-modal data or directly concatenate multi-modal
representations, which can not sufficiently exploit the informative
correlations inherent in visual and kinematics data to boost gesture
recognition accuracies. In this regard, we propose a novel online approach of
multi-modal relational graph network (i.e., MRG-Net) to dynamically integrate
visual and kinematics information through interactive message propagation in
the latent feature space. In specific, we first extract embeddings from video
and kinematics sequences with temporal convolutional networks and LSTM units.
Next, we identify multi-relations in these multi-modal embeddings and leverage
them through a hierarchical relational graph learning module. The effectiveness
of our method is demonstrated with state-of-the-art results on the public
JIGSAWS dataset, outperforming current uni-modal and multi-modal methods on
both suturing and knot typing tasks. Furthermore, we validated our method on
in-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK)
platforms in two centers, with consistent promising performance achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yonghao Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jie Ying Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1"&gt;Bo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yueming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1"&gt;Mathias Unberath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun-Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forest Fire Clustering: Iterative Label Propagation Clustering and Monte Carlo Validation For Single-cell Sequencing Analysis. (arXiv:2103.11802v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11802</id>
        <link href="http://arxiv.org/abs/2103.11802"/>
        <updated>2021-06-30T02:01:04.189Z</updated>
        <summary type="html"><![CDATA[With the rise of single-cell sequencing technologies, there is a growing need
for robust clustering algorithms to extract deeper insights from data. Here, we
introduce an intuitive and efficient clustering method, Forest Fire Clustering,
for discovering and validating cell types in single-cell sequencing analysis.
Compared to existing methods, our clustering algorithm makes minimum prior
assumptions about the data distribution and can provide a point-wise
significance value via Monte Carlo simulations for internal validation.
Additionally, point-wise label entropies can highlight novel transition cell
types \emph{de novo} along developmental pseudo-time manifolds. Lastly, our
inductive algorithm has the ability to make robust inferences in an
online-learning context. In this paper, we describe the method, provide a
summary of its performance against common clustering benchmarks, and
demonstrate that Forest Fire Clustering is uniquely suitable for single-cell
sequencing analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhanlin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldwasser_J/0/1/0/all/0/1"&gt;Jeremy Goldwasser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuckman_P/0/1/0/all/0/1"&gt;Philip Tuckman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1"&gt;Mark Gerstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetry meets AI. (arXiv:2103.06115v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06115</id>
        <link href="http://arxiv.org/abs/2103.06115"/>
        <updated>2021-06-30T02:01:04.183Z</updated>
        <summary type="html"><![CDATA[We explore whether Neural Networks (NNs) can {\it discover} the presence of
symmetries as they learn to perform a task. For this, we train hundreds of NNs
on a {\it decoy task} based on well-controlled Physics templates, where no
information on symmetry is provided. We use the output from the last hidden
layer of all these NNs, projected to fewer dimensions, as the input for a
symmetry classification task, and show that information on symmetry had indeed
been identified by the original NN without guidance. As an interdisciplinary
application of this procedure, we identify the presence and level of symmetry
in artistic paintings from different styles such as those of Picasso, Pollock
and Van Gogh.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barenboim_G/0/1/0/all/0/1"&gt;Gabriela Barenboim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirn_J/0/1/0/all/0/1"&gt;Johannes Hirn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanz_V/0/1/0/all/0/1"&gt;Veronica Sanz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepFaceLab: Integrated, flexible and extensible face-swapping framework. (arXiv:2005.05535v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05535</id>
        <link href="http://arxiv.org/abs/2005.05535"/>
        <updated>2021-06-30T02:01:04.171Z</updated>
        <summary type="html"><![CDATA[Deepfake defense not only requires the research of detection but also
requires the efforts of generation methods. However, current deepfake methods
suffer the effects of obscure workflow and poor performance. To solve this
problem, we present DeepFaceLab, the current dominant deepfake framework for
face-swapping. It provides the necessary tools as well as an easy-to-use way to
conduct high-quality face-swapping. It also offers a flexible and loose
coupling structure for people who need to strengthen their pipeline with other
features without writing complicated boilerplate code. We detail the principles
that drive the implementation of DeepFaceLab and introduce its pipeline,
through which every aspect of the pipeline can be modified painlessly by users
to achieve their customization purpose. It is noteworthy that DeepFaceLab could
achieve cinema-quality results with high fidelity. We demonstrate the advantage
of our system by comparing our approach with other face-swapping methods.For
more information, please visit:https://github.com/iperov/DeepFaceLab/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perov_I/0/1/0/all/0/1"&gt;Ivan Perov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1"&gt;Daiheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chervoniy_N/0/1/0/all/0/1"&gt;Nikolay Chervoniy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kunlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marangonda_S/0/1/0/all/0/1"&gt;Sugasa Marangonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ume_C/0/1/0/all/0/1"&gt;Chris Um&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dpfks_M/0/1/0/all/0/1"&gt;Mr. Dpfks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Facenheim_C/0/1/0/all/0/1"&gt;Carl Shift Facenheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+RP_L/0/1/0/all/0/1"&gt;Luis RP&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Pingyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wasserstein Adversarial Regularization (WAR) on label noise. (arXiv:1904.03936v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.03936</id>
        <link href="http://arxiv.org/abs/1904.03936"/>
        <updated>2021-06-30T02:01:04.147Z</updated>
        <summary type="html"><![CDATA[Noisy labels often occur in vision datasets, especially when they are
obtained from crowdsourcing or Web scraping. We propose a new regularization
method, which enables learning robust classifiers in presence of noisy data. To
achieve this goal, we propose a new adversarial regularization scheme based on
the Wasserstein distance. Using this distance allows taking into account
specific relations between classes by leveraging the geometric properties of
the labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a
selective regularization, which promotes smoothness of the classifier between
some classes, while preserving sufficient complexity of the decision boundary
between others. We first discuss how and why adversarial regularization can be
used in the context of label noise and then show the effectiveness of our
method on five datasets corrupted with noisy labels: in both benchmarks and
real datasets, WAR outperforms the state-of-the-art competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fatras_K/0/1/0/all/0/1"&gt;Kilian Fatras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1"&gt;Bharath Bhushan Damodaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobry_S/0/1/0/all/0/1"&gt;Sylvain Lobry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1"&gt;Devis Tuia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1"&gt;Nicolas Courty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Policy Gradients: Leveraging Structure for Efficient Learning in (Factored) MOMDPs. (arXiv:2102.10362v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10362</id>
        <link href="http://arxiv.org/abs/2102.10362"/>
        <updated>2021-06-30T02:01:04.141Z</updated>
        <summary type="html"><![CDATA[Policy gradient methods can solve complex tasks but often fail when the
dimensionality of the action-space or objective multiplicity grow very large.
This occurs, in part, because the variance on score-based gradient estimators
scales quadratically. In this paper, we address this problem through a causal
baseline which exploits independence structure encoded in a novel action-target
influence network. Causal policy gradients (CPGs), which follow, provide a
common framework for analysing key state-of-the-art algorithms, are shown to
generalise traditional policy gradients, and yield a principled way of
incorporating prior knowledge of a problem domain's generative processes. We
provide an analysis of the proposed estimator and identify the conditions under
which variance is reduced. The algorithmic aspects of CPGs are discussed,
including optimal policy factorisation, as characterised by minimum biclique
coverings, and the implications for the bias-variance trade-off of incorrectly
specifying the network. Finally, we demonstrate the performance advantages of
our algorithm on large-scale bandit and traffic intersection problems,
providing a novel contribution to the latter in the form of a spatio-causal
approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1"&gt;Thomas Spooner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vadori_N/0/1/0/all/0/1"&gt;Nelson Vadori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1"&gt;Sumitra Ganesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning of Implicit and Explicit Control Flow in Instructions. (arXiv:2102.13195v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13195</id>
        <link href="http://arxiv.org/abs/2102.13195"/>
        <updated>2021-06-30T02:01:04.136Z</updated>
        <summary type="html"><![CDATA[Learning to flexibly follow task instructions in dynamic environments poses
interesting challenges for reinforcement learning agents. We focus here on the
problem of learning control flow that deviates from a strict step-by-step
execution of instructions -- that is, control flow that may skip forward over
parts of the instructions or return backward to previously completed or skipped
steps. Demand for such flexible control arises in two fundamental ways:
explicitly when control is specified in the instructions themselves (such as
conditional branching and looping) and implicitly when stochastic environment
dynamics require re-completion of instructions whose effects have been
perturbed, or opportunistic skipping of instructions whose effects are already
present. We formulate an attention-based architecture that meets these
challenges by learning, from task reward only, to flexibly attend to and
condition behavior on an internal encoding of the instructions. We test the
architecture's ability to learn both explicit and implicit control in two
illustrative domains -- one inspired by Minecraft and the other by StarCraft --
and show that the architecture exhibits zero-shot generalization to novel
instructions of length greater than those in a training set, at a performance
level unmatched by two baseline recurrent architectures and one ablation
architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brooks_E/0/1/0/all/0/1"&gt;Ethan A. Brooks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1"&gt;Janarthanan Rajendran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_R/0/1/0/all/0/1"&gt;Richard L. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satinder Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-asymptotic Superlinear Convergence of Standard Quasi-Newton Methods. (arXiv:2003.13607v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.13607</id>
        <link href="http://arxiv.org/abs/2003.13607"/>
        <updated>2021-06-30T02:01:04.131Z</updated>
        <summary type="html"><![CDATA[In this paper, we study and prove the non-asymptotic superlinear convergence
rate of the Broyden class of quasi-Newton methods including
Davidon--Fletcher--Powell (DFP) method and Broyden--Fletcher--Goldfarb--Shanno
(BFGS) method. The asymptotic superlinear convergence rate of these
quasi-Newton methods has been extensively studied, but their explicit finite
time local convergence rate is not fully investigated. In this paper, we
provide a finite time (non-asymptotic) convergence analysis for BFGS and DFP
methods under the assumptions that the objective function is strongly convex,
its gradient is Lipschitz continuous, and its Hessian is Lipschitz continuous
only in the direction of the optimal solution. We show that in a local
neighborhood of the optimal solution, the iterates generated by both DFP and
BFGS converge to the optimal solution at a superlinear rate of $(1/k)^{k/2}$,
where $k$ is the number of iterations. We also prove the same local superlinear
convergence rate in the case that the objective function is self-concordant.
Numerical experiments on different objective functions confirm our explicit
convergence rates. Our theoretical guarantee is one of the first results that
provide a non-asymptotic superlinear convergence rate for DFP and BFGS
quasi-Newton methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qiujiang Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mokhtari_A/0/1/0/all/0/1"&gt;Aryan Mokhtari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning. (arXiv:2102.07868v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07868</id>
        <link href="http://arxiv.org/abs/2102.07868"/>
        <updated>2021-06-30T02:01:04.126Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) are non-parametric, flexible, models that work well
in many tasks. Combining GPs with deep learning methods via deep kernel
learning (DKL) is especially compelling due to the strong representational
power induced by the network. However, inference in GPs, whether with or
without DKL, can be computationally challenging on large datasets. Here, we
propose GP-Tree, a novel method for multi-class classification with Gaussian
processes and DKL. We develop a tree-based hierarchical model in which each
internal node of the tree fits a GP to the data using the P\'olya Gamma
augmentation scheme. As a result, our method scales well with both the number
of classes and data size. We demonstrate the effectiveness of our method
against other Gaussian process training baselines, and we show how our general
GP approach achieves improved accuracy on standard incremental few-shot
learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1"&gt;Idan Achituve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1"&gt;Aviv Navon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yemini_Y/0/1/0/all/0/1"&gt;Yochai Yemini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1"&gt;Ethan Fetaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias-Free Scalable Gaussian Processes via Randomized Truncations. (arXiv:2102.06695v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06695</id>
        <link href="http://arxiv.org/abs/2102.06695"/>
        <updated>2021-06-30T02:01:04.110Z</updated>
        <summary type="html"><![CDATA[Scalable Gaussian Process methods are computationally attractive, yet
introduce modeling biases that require rigorous study. This paper analyzes two
common techniques: early truncated conjugate gradients (CG) and random Fourier
features (RFF). We find that both methods introduce a systematic bias on the
learned hyperparameters: CG tends to underfit while RFF tends to overfit. We
address these issues using randomized truncation estimators that eliminate bias
in exchange for increased variance. In the case of RFF, we show that the
bias-to-variance conversion is indeed a trade-off: the additional variance
proves detrimental to optimization. However, in the case of CG, our unbiased
learning procedure meaningfully outperforms its biased counterpart with minimal
additional computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Potapczynski_A/0/1/0/all/0/1"&gt;Andres Potapczynski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Luhuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biderman_D/0/1/0/all/0/1"&gt;Dan Biderman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1"&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1"&gt;John P. Cunningham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm. (arXiv:2102.12238v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12238</id>
        <link href="http://arxiv.org/abs/2102.12238"/>
        <updated>2021-06-30T02:01:04.105Z</updated>
        <summary type="html"><![CDATA[We study the function space characterization of the inductive bias resulting
from controlling the $\ell_2$ norm of the weights in linear convolutional
networks. We view this in terms of an induced regularizer in the function space
given by the minimum norm of weights required to realize a linear function. For
two layer linear convolutional networks with $C$ output channels and kernel
size $K$, we show the following: (a) If the inputs to the network have a single
channel, the induced regularizer for any $K$ is a norm given by a semidefinite
program (SDP) that is independent of the number of output channels $C$. (b) In
contrast, for networks with multi-channel inputs, multiple output channels can
be necessary to merely realize all matrix-valued linear functions and thus the
inductive bias does depend on $C$. Further, for sufficiently large $C$, the
induced regularizer for $K=1$ and $K=D$ are the nuclear norm and the
$\ell_{2,1}$ group-sparse norm, respectively, of the Fourier coefficients. (c)
Complementing our theoretical results, we show through experiments on MNIST and
CIFAR-10 that our key findings extend to implicit biases from gradient descent
in overparameterized networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1"&gt;Meena Jagadeesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razenshteyn_I/0/1/0/all/0/1"&gt;Ilya Razenshteyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1"&gt;Suriya Gunasekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Approximation Theorems for Differentiable Geometric Deep Learning. (arXiv:2101.05390v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05390</id>
        <link href="http://arxiv.org/abs/2101.05390"/>
        <updated>2021-06-30T02:01:04.099Z</updated>
        <summary type="html"><![CDATA[This paper addresses the growing need to process non-Euclidean data, by
introducing a geometric deep learning (GDL) framework for building universal
feedforward-type models compatible with differentiable manifold geometries. We
show that our GDL models can approximate any continuous target function
uniformly on compacts of a controlled maximum diameter. We obtain curvature
dependant lower-bounds on this maximum diameter and upper-bounds on the depth
of our approximating GDL models. Conversely, we find that there is always a
continuous function between any two non-degenerate compact manifolds that any
"locally-defined" GDL model cannot uniformly approximate. Our last main result
identifies data-dependent conditions guaranteeing that the GDL model
implementing our approximation breaks "the curse of dimensionality." We find
that any "real-world" (i.e. finite) dataset always satisfies our condition and,
conversely, any dataset satisfies our requirement if the target function is
smooth. As applications, we confirm the universal approximation capabilities of
the following GDL models: Ganea et al. (2018)'s hyperbolic feedforward
networks, the architecture implementing Krishnan et al. (2015)'s deep
Kalman-Filter, and deep softmax classifiers. We build universal
extensions/variants of: the SPD-matrix regressor of Meyer et al. (2011), and
Fletcher et al. (2009)'s Procrustean regressor. In the Euclidean setting, our
results imply a quantitative version of Kidger and Lyons (2020)'s approximation
theorem and a data-dependent version of Yarotsky and Zhevnerchuk (2020)'s
uncursed approximation rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1"&gt;Anastasis Kratsios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papon_L/0/1/0/all/0/1"&gt;Leonie Papon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks. (arXiv:2102.11600v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11600</id>
        <link href="http://arxiv.org/abs/2102.11600"/>
        <updated>2021-06-30T02:01:04.094Z</updated>
        <summary type="html"><![CDATA[Recently, learning algorithms motivated from sharpness of loss surface as an
effective measure of generalization gap have shown state-of-the-art
performances. Nevertheless, sharpness defined in a rigid region with a fixed
radius, has a drawback in sensitivity to parameter re-scaling which leaves the
loss unaffected, leading to weakening of the connection between sharpness and
generalization gap. In this paper, we introduce the concept of adaptive
sharpness which is scale-invariant and propose the corresponding generalization
bound. We suggest a novel learning method, adaptive sharpness-aware
minimization (ASAM), utilizing the proposed generalization bound. Experimental
results in various benchmark datasets show that ASAM contributes to significant
improvement of model generalization performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1"&gt;Jungmin Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jeongseop Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyunseo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1"&gt;In Kwon Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Arabic Speech Recognition by End-to-End, Modular Systems and Human. (arXiv:2101.08454v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08454</id>
        <link href="http://arxiv.org/abs/2101.08454"/>
        <updated>2021-06-30T02:01:04.082Z</updated>
        <summary type="html"><![CDATA[Recent advances in automatic speech recognition (ASR) have achieved accuracy
levels comparable to human transcribers, which led researchers to debate if the
machine has reached human performance. Previous work focused on the English
language and modular hidden Markov model-deep neural network (HMM-DNN) systems.
In this paper, we perform a comprehensive benchmarking for end-to-end
transformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the
Arabic language and its dialects. For the HSR, we evaluate linguist performance
and lay-native speaker performance on a new dataset collected as a part of this
study. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new
performance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our
results suggest that human performance in the Arabic language is still
considerably better than the machine with an absolute WER gap of 3.5% on
average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hussein_A/0/1/0/all/0/1"&gt;Amir Hussein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1"&gt;Ahmed Ali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rapid parameter estimation of discrete decaying signals using autoencoder networks. (arXiv:2103.08663v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08663</id>
        <link href="http://arxiv.org/abs/2103.08663"/>
        <updated>2021-06-30T02:01:04.077Z</updated>
        <summary type="html"><![CDATA[In this work we demonstrate the use of neural networks for rapid extraction
of signal parameters of discretely sampled signals. In particular, we use dense
autoencoder networks to extract the parameters of interest from exponentially
decaying signals and decaying oscillations. By using a three-stage training
method and careful choice of the neural network size, we are able to retrieve
the relevant signal parameters directly from the latent space of the
autoencoder network at significantly improved rates compared to traditional
algorithmic signal-analysis approaches. We show that the achievable precision
and accuracy of this method of analysis is similar to conventional
algorithm-based signal analysis methods, by demonstrating that the extracted
signal parameters are approaching their fundamental parameter estimation limit
as provided by the Cram\'er-Rao bound. Furthermore, we demonstrate that
autoencoder networks are able to achieve signal analysis, and, hence, parameter
extraction, at rates of 75 kHz, orders-of-magnitude faster than conventional
techniques with similar precision. Finally, we explore the limitations of our
approach, demonstrating that analysis rates of $>$200 kHz are feasible with
further optimization of the transfer rate between the data-acquisition system
and data-analysis system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Visschers_J/0/1/0/all/0/1"&gt;Jim C. Visschers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Budker_D/0/1/0/all/0/1"&gt;Dmitry Budker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bougas_L/0/1/0/all/0/1"&gt;Lykourgos Bougas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Complexity of Stochastic Dual Dynamic Programming. (arXiv:1912.07702v6 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.07702</id>
        <link href="http://arxiv.org/abs/1912.07702"/>
        <updated>2021-06-30T02:01:04.061Z</updated>
        <summary type="html"><![CDATA[Stochastic dual dynamic programming is a cutting plane type algorithm for
multi-stage stochastic optimization originated about 30 years ago. In spite of
its popularity in practice, there does not exist any analysis on the
convergence rates of this method. In this paper, we first establish the number
of iterations, i.e., iteration complexity, required by a basic dynamic cutting
plane method for solving relatively simple multi-stage optimization problems,
by introducing novel mathematical tools including the saturation of search
points. We then refine these basic tools and establish the iteration complexity
for both deterministic and stochastic dual dynamic programming methods for
solving more general multi-stage stochastic optimization problems under the
standard stage-wise independence assumption. Our results indicate that the
complexity of these methods mildly increases with the number of stages $T$, in
fact linearly dependent on $T$ for discounted problems. Therefore, they are
efficient for strategic decision making which involves a large number of
stages, but with a relatively small number of decision variables in each stage.
Without explicitly discretizing the state and action spaces, these methods
might also be pertinent to the related reinforcement learning and stochastic
control areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lan_G/0/1/0/all/0/1"&gt;Guanghui Lan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing the Stability of Non-coplanar Circumbinary Planets using Machine Learning. (arXiv:2101.02316v2 [astro-ph.EP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02316</id>
        <link href="http://arxiv.org/abs/2101.02316"/>
        <updated>2021-06-30T02:01:04.056Z</updated>
        <summary type="html"><![CDATA[Exoplanet detection in the past decade by efforts including NASA's Kepler and
TESS missions has discovered many worlds that differ substantially from planets
in our own Solar system, including more than 400 exoplanets orbiting binary or
multi-star systems. This not only broadens our understanding of the diversity
of exoplanets, but also promotes our study of exoplanets in the complex binary
and multi-star systems and provides motivation to explore their habitability.
In this study, we analyze orbital stability of exoplanets in non-coplanar
circumbinary systems using a numerical simulation method, with which a large
number of circumbinary planet samples are generated in order to quantify the
effects of various orbital parameters on orbital stability. We also train a
machine learning model that can quickly determine the stability of the
circumbinary planetary systems. Our results indicate that larger inclinations
of the planet tend to increase the stability of its orbit, but change in the
planet's mass range between Earth and Jupiter has little effect on the
stability of the system. In addition, we find that Deep Neural Networks (DNNs)
have higher accuracy and precision than other machine learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhihui Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jonathan H. Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zong-Hong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Fahy_K/0/1/0/all/0/1"&gt;Kristen A. Fahy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Burn_R/0/1/0/all/0/1"&gt;Remo Burn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-Optimal Explainable $k$-Means for All Dimensions. (arXiv:2106.15566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15566</id>
        <link href="http://arxiv.org/abs/2106.15566"/>
        <updated>2021-06-30T02:01:04.050Z</updated>
        <summary type="html"><![CDATA[Many clustering algorithms are guided by certain cost functions such as the
widely-used $k$-means cost. These algorithms divide data points into clusters
with often complicated boundaries, creating difficulties in explaining the
clustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and
Rashtchian (ICML'20) introduced explainable clustering, where the cluster
boundaries are axis-parallel hyperplanes and the clustering is obtained by
applying a decision tree to the data. The central question here is: how much
does the explainability constraint increase the value of the cost function?

Given $d$-dimensional data points, we show an efficient algorithm that finds
an explainable clustering whose $k$-means cost is at most $k^{1 -
2/d}\mathrm{poly}(d\log k)$ times the minimum cost achievable by a clustering
without the explainability constraint, assuming $k,d\ge 2$. Combining this with
an independent work by Makarychev and Shan (ICML'21), we get an improved bound
of $k^{1 - 2/d}\mathrm{polylog}(k)$, which we show is optimal for every choice
of $k,d\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in
particular, we show an $O(\log k\log\log k)$ bound, improving exponentially
over the previous best bound of $\widetilde O(k)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Charikar_M/0/1/0/all/0/1"&gt;Moses Charikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lunjia Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Approximation Properties of Dictionaries and Applications to Neural Networks. (arXiv:2101.12365v6 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12365</id>
        <link href="http://arxiv.org/abs/2101.12365"/>
        <updated>2021-06-30T02:01:04.045Z</updated>
        <summary type="html"><![CDATA[This article addresses the problem of approximating a function in a Hilbert
space by an expansion over a dictionary $\mathbb{D}$. We introduce the notion
of a smoothly parameterized dictionary and give upper bounds on the
approximation rates, metric entropy and $n$-widths of the absolute convex hull,
which we denote $B_1(\mathbb{D})$, of such dictionaries. The upper bounds
depend upon the order of smoothness of the parameterization, and improve upon
existing results in many cases. The main applications of these results is to
the dictionaries $\mathbb{D} = \{\sigma(\omega\cdot x + b)\}\subset L^2$
corresponding to shallow neural networks with activation function $\sigma$, and
to the dictionary of decaying Fourier modes corresponding to the spectral
Barron space. This improves upon existing approximation rates for shallow
neural networks when $\sigma = \text{ReLU}^k$ for $k\geq 2$, sharpens bounds on
the metric entropy, and provides the first bounds on the Gelfand $n$-widths of
the Barron space and spectral Barron space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1"&gt;Jonathan W. Siegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinchao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Classification Learning with Neural Networks and Conceptors for Speech Recognition and Car Driving Maneuvers. (arXiv:2102.05588v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05588</id>
        <link href="http://arxiv.org/abs/2102.05588"/>
        <updated>2021-06-30T02:01:04.040Z</updated>
        <summary type="html"><![CDATA[Recurrent neural networks are a powerful means in diverse applications. We
show that, together with so-called conceptors, they also allow fast learning,
in contrast to other deep learning methods. In addition, a relatively small
number of examples suffices to train neural networks with high accuracy. We
demonstrate this with two applications, namely speech recognition and detecting
car driving maneuvers. We improve the state of the art by application-specific
preparation techniques: For speech recognition, we use mel frequency cepstral
coefficients leading to a compact representation of the frequency spectra, and
detecting car driving maneuvers can be done without the commonly used
polynomial interpolation, as our evaluation suggests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krause_S/0/1/0/all/0/1"&gt;Stefanie Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otto_O/0/1/0/all/0/1"&gt;Oliver Otto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolzenburg_F/0/1/0/all/0/1"&gt;Frieder Stolzenburg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Gradient Aggregation for Decentralized Learning from Non-IID data. (arXiv:2103.02051v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02051</id>
        <link href="http://arxiv.org/abs/2103.02051"/>
        <updated>2021-06-30T02:01:04.027Z</updated>
        <summary type="html"><![CDATA[Decentralized learning enables a group of collaborative agents to learn
models using a distributed dataset without the need for a central parameter
server. Recently, decentralized learning algorithms have demonstrated
state-of-the-art results on benchmark data sets, comparable with centralized
algorithms. However, the key assumption to achieve competitive performance is
that the data is independently and identically distributed (IID) among the
agents which, in real-life applications, is often not applicable. Inspired by
ideas from continual learning, we propose Cross-Gradient Aggregation (CGA), a
novel decentralized learning algorithm where (i) each agent aggregates
cross-gradient information, i.e., derivatives of its model with respect to its
neighbors' datasets, and (ii) updates its model using a projected gradient
based on quadratic programming (QP). We theoretically analyze the convergence
characteristics of CGA and demonstrate its efficiency on non-IID data
distributions sampled from the MNIST and CIFAR-10 datasets. Our empirical
comparisons show superior learning performance of CGA over existing
state-of-the-art decentralized learning algorithms, as well as maintaining the
improved performance under information compression to reduce peer-to-peer
communication overhead. The code is available here on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esfandiari_Y/0/1/0/all/0/1"&gt;Yasaman Esfandiari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Sin Yong Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhanhong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balu_A/0/1/0/all/0/1"&gt;Aditya Balu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herron_E/0/1/0/all/0/1"&gt;Ethan Herron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1"&gt;Chinmay Hegde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1"&gt;Soumik Sarkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Muddling Label Regularization: Deep Learning for Tabular Datasets. (arXiv:2106.04462v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04462</id>
        <link href="http://arxiv.org/abs/2106.04462"/>
        <updated>2021-06-30T02:01:04.021Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) is considered the state-of-the-art in computer vision,
speech recognition and natural language processing. Until recently, it was also
widely accepted that DL is irrelevant for learning tasks on tabular data,
especially in the small sample regime where ensemble methods are acknowledged
as the gold standard. We present a new end-to-end differentiable method to
train a standard FFNN. Our method, \textbf{Muddling labels for Regularization}
(\texttt{MLR}), penalizes memorization through the generation of uninformative
labels and the application of a differentiable close-form regularization scheme
on the last hidden layer during training. \texttt{MLR} outperforms classical NN
and the gold standard (GBDT, RF) for regression and classification tasks on
several datasets from the UCI database and Kaggle covering a large range of
sample sizes and feature to sample ratios. Researchers and practitioners can
use \texttt{MLR} on its own as an off-the-shelf \DL{} solution or integrate it
into the most advanced ML pipelines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lounici_K/0/1/0/all/0/1"&gt;Karim Lounici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meziani_K/0/1/0/all/0/1"&gt;Katia Meziani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riu_B/0/1/0/all/0/1"&gt;Benjamin Riu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Policy Risk Assessment in Contextual Bandits. (arXiv:2104.08977v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08977</id>
        <link href="http://arxiv.org/abs/2104.08977"/>
        <updated>2021-06-30T02:01:04.015Z</updated>
        <summary type="html"><![CDATA[Even when unable to run experiments, practitioners can evaluate prospective
policies, using previously logged data. However, while the bandits literature
has adopted a diverse set of objectives, most research on off-policy evaluation
to date focuses on the expected reward. In this paper, we introduce Lipschitz
risk functionals, a broad class of objectives that subsumes conditional
value-at-risk (CVaR), variance, mean-variance, many distorted risks, and CPT
risks, among others. We propose Off-Policy Risk Assessment (OPRA), a framework
that first estimates a target policy's CDF and then generates plugin estimates
for any collection of Lipschitz risks, providing finite sample guarantees that
hold simultaneously over the entire class. We instantiate OPRA with both
importance sampling and doubly robust estimators. Our primary theoretical
contributions are (i) the first uniform concentration inequalities for both CDF
estimators in contextual bandits and (ii) error bounds on our Lipschitz risk
estimates, which all converge at a rate of $O(1/\sqrt{n})$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1"&gt;Audrey Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leqi_L/0/1/0/all/0/1"&gt;Liu Leqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1"&gt;Kamyar Azizzadenesheli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild. (arXiv:2106.15610v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15610</id>
        <link href="http://arxiv.org/abs/2106.15610"/>
        <updated>2021-06-30T02:01:04.009Z</updated>
        <summary type="html"><![CDATA[Unsupervised disentanglement has been shown to be theoretically impossible
without inductive biases on the models and the data. As an alternative
approach, recent methods rely on limited supervision to disentangle the factors
of variation and allow their identifiability. While annotating the true
generative factors is only required for a limited number of observations, we
argue that it is infeasible to enumerate all the factors of variation that
describe a real-world image distribution. To this end, we propose a method for
disentangling a set of factors which are only partially labeled, as well as
separating the complementary set of residual factors that are never explicitly
specified. Our success in this challenging setting, demonstrated on synthetic
benchmarks, gives rise to leveraging off-the-shelf image descriptors to
partially annotate a subset of attributes in real image domains (e.g. of human
faces) with minimal manual effort. Specifically, we use a recent language-image
embedding model (CLIP) to annotate a set of attributes of interest in a
zero-shot manner and demonstrate state-of-the-art disentangled image
manipulation results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gabbay_A/0/1/0/all/0/1"&gt;Aviv Gabbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1"&gt;Niv Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1"&gt;Yedid Hoshen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised learning with Bayesian Confidence Propagation Neural Network. (arXiv:2106.15546v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15546</id>
        <link href="http://arxiv.org/abs/2106.15546"/>
        <updated>2021-06-30T02:01:04.003Z</updated>
        <summary type="html"><![CDATA[Learning internal representations from data using no or few labels is useful
for machine learning research, as it allows using massive amounts of unlabeled
data. In this work, we use the Bayesian Confidence Propagation Neural Network
(BCPNN) model developed as a biologically plausible model of the cortex. Recent
work has demonstrated that these networks can learn useful internal
representations from data using local Bayesian-Hebbian learning rules. In this
work, we show how such representations can be leveraged in a semi-supervised
setting by introducing and comparing different classifiers. We also evaluate
and compare such networks with other popular semi-supervised classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_N/0/1/0/all/0/1"&gt;Naresh Balaji Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lansner_A/0/1/0/all/0/1"&gt;Anders Lansner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herman_P/0/1/0/all/0/1"&gt;Pawel Herman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The U-Net based GLOW for Optical-Flow-free Video Interframe Generation. (arXiv:2103.09576v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09576</id>
        <link href="http://arxiv.org/abs/2103.09576"/>
        <updated>2021-06-30T02:01:03.991Z</updated>
        <summary type="html"><![CDATA[Video frame interpolation is the task of creating an interframe between two
adjacent frames along the time axis. So, instead of simply averaging two
adjacent frames to create an intermediate image, this operation should maintain
semantic continuity with the adjacent frames. Most conventional methods use
optical flow, and various tools such as occlusion handling and object smoothing
are indispensable. Since the use of these various tools leads to complex
problems, we tried to tackle the video interframe generation problem without
using problematic optical flow . To enable this , we have tried to use a deep
neural network with an invertible structure, and developed an U-Net based
Generative Flow which is a modified normalizing flow. In addition, we propose a
learning method with a new consistency loss in the latent space to maintain
semantic temporal consistency between frames. The resolution of the generated
image is guaranteed to be identical to that of the original images by using an
invertible network. Furthermore, as it is not a random image like the ones by
generative models, our network guarantees stable outputs without flicker.
Through experiments, we \sam {confirmed the feasibility of the proposed
algorithm and would like to suggest the U-Net based Generative Flow as a new
possibility for baseline in video frame interpolation. This paper is meaningful
in that it is the world's first attempt to use invertible networks instead of
optical flows for video interpolation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Saem Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Donghoon Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1"&gt;Nojun Kwak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDMI: High-order Deep Multiplex Infomax. (arXiv:2102.07810v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07810</id>
        <link href="http://arxiv.org/abs/2102.07810"/>
        <updated>2021-06-30T02:01:03.985Z</updated>
        <summary type="html"><![CDATA[Networks have been widely used to represent the relations between objects
such as academic networks and social networks, and learning embedding for
networks has thus garnered plenty of research attention. Self-supervised
network representation learning aims at extracting node embedding without
external supervision. Recently, maximizing the mutual information between the
local node embedding and the global summary (e.g. Deep Graph Infomax, or DGI
for short) has shown promising results on many downstream tasks such as node
classification. However, there are two major limitations of DGI. Firstly, DGI
merely considers the extrinsic supervision signal (i.e., the mutual information
between node embedding and global summary) while ignores the intrinsic signal
(i.e., the mutual dependence between node embedding and node attributes).
Secondly, nodes in a real-world network are usually connected by multiple edges
with different relations, while DGI does not fully explore the various
relations among nodes. To address the above-mentioned problems, we propose a
novel framework, called High-order Deep Multiplex Infomax (HDMI), for learning
node embedding on multiplex networks in a self-supervised way. To be more
specific, we first design a joint supervision signal containing both extrinsic
and intrinsic mutual information by high-order mutual information, and we
propose a High-order Deep Infomax (HDI) to optimize the proposed supervision
signal. Then we propose an attention based fusion module to combine node
embedding from different layers of the multiplex network. Finally, we evaluate
the proposed HDMI on various downstream tasks such as unsupervised clustering
and supervised classification. The experimental results show that HDMI achieves
state-of-the-art performance on these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1"&gt;Baoyu Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chanyoung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1"&gt;Hanghang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impossibility of Partial Recovery in the Graph Alignment Problem. (arXiv:2102.02685v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02685</id>
        <link href="http://arxiv.org/abs/2102.02685"/>
        <updated>2021-06-30T02:01:03.980Z</updated>
        <summary type="html"><![CDATA[Random graph alignment refers to recovering the underlying vertex
correspondence between two random graphs with correlated edges. This can be
viewed as an average-case and noisy version of the well-known graph isomorphism
problem. For the correlated Erd\"os-R\'enyi model, we prove an impossibility
result for partial recovery in the sparse regime, with constant average degree
and correlation, as well as a general bound on the maximal reachable overlap.
Our bound is tight in the noiseless case (the graph isomorphism problem) and we
conjecture that it is still tight with noise. Our proof technique relies on a
careful application of the probabilistic method to build automorphisms between
tree components of a subcritical Erd\"os-R\'enyi graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ganassali_L/0/1/0/all/0/1"&gt;Luca Ganassali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1"&gt;Laurent Massouli&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lelarge_M/0/1/0/all/0/1"&gt;Marc Lelarge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated classification of plasma regions using 3D particle energy distributions. (arXiv:1908.05715v3 [physics.space-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.05715</id>
        <link href="http://arxiv.org/abs/1908.05715"/>
        <updated>2021-06-30T02:01:03.975Z</updated>
        <summary type="html"><![CDATA[We investigate the properties of the ion sky maps produced by the Dual Ion
Spectrometers (DIS) from the Fast Plasma Investigation (FPI). We have trained a
convolutional neural network classifier to predict four regions crossed by the
MMS on the dayside magnetosphere: solar wind, ion foreshock, magnetosheath, and
magnetopause using solely DIS spectrograms. The accuracy of the classifier is
>98%. We use the classifier to detect mixed plasma regions, in particular to
find the bow shock regions. A similar approach can be used to identify the
magnetopause crossings and reveal regions prone to magnetic reconnection. Data
processing through the trained classifier is fast and efficient and thus can be
used for classification for the whole MMS database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Olshevsky_V/0/1/0/all/0/1"&gt;Vyacheslav Olshevsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Khotyaintsev_Y/0/1/0/all/0/1"&gt;Yuri V. Khotyaintsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lalti_A/0/1/0/all/0/1"&gt;Ahmad Lalti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Divin_A/0/1/0/all/0/1"&gt;Andrey Divin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Delzanno_G/0/1/0/all/0/1"&gt;Gian Luca Delzanno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Anderzen_S/0/1/0/all/0/1"&gt;Sven Anderzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Herman_P/0/1/0/all/0/1"&gt;Pawel Herman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chien_S/0/1/0/all/0/1"&gt;Steven W.D. Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Avanov_L/0/1/0/all/0/1"&gt;Levon Avanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Dimmock_A/0/1/0/all/0/1"&gt;Andrew P. Dimmock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Markidis_S/0/1/0/all/0/1"&gt;Stefano Markidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Interpretable Natural Language Understanding with Explanations as Latent Variables. (arXiv:2011.05268v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05268</id>
        <link href="http://arxiv.org/abs/2011.05268"/>
        <updated>2021-06-30T02:01:03.961Z</updated>
        <summary type="html"><![CDATA[Recently generating natural language explanations has shown very promising
results in not only offering interpretable explanations but also providing
additional information and supervision for prediction. However, existing
approaches usually require a large set of human annotated explanations for
training while collecting a large set of explanations is not only time
consuming but also expensive. In this paper, we develop a general framework for
interpretable natural language understanding that requires only a small set of
human annotated explanations for training. Our framework treats natural
language explanations as latent variables that model the underlying reasoning
process of a neural model. We develop a variational EM framework for
optimization where an explanation generation module and an
explanation-augmented prediction module are alternatively optimized and
mutually enhance each other. Moreover, we further propose an explanation-based
self-training method under this framework for semi-supervised learning. It
alternates between assigning pseudo-labels to unlabeled data and generating new
explanations to iteratively improve each other. Experiments on two natural
language understanding tasks demonstrate that our framework can not only make
effective predictions in both supervised and semi-supervised settings, but also
generate good natural language explanation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jinyi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanlin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A function approximation approach to the prediction of blood glucose levels. (arXiv:2105.05893v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05893</id>
        <link href="http://arxiv.org/abs/2105.05893"/>
        <updated>2021-06-30T02:01:03.956Z</updated>
        <summary type="html"><![CDATA[The problem of real time prediction of blood glucose (BG) levels based on the
readings from a continuous glucose monitoring (CGM) device is a problem of
great importance in diabetes care, and therefore, has attracted a lot of
research in recent years, especially based on machine learning. An accurate
prediction with a 30, 60, or 90 minute prediction horizon has the potential of
saving millions of dollars in emergency care costs. In this paper, we treat the
problem as one of function approximation, where the value of the BG level at
time $t+h$ (where $h$ the prediction horizon) is considered to be an unknown
function of $d$ readings prior to the time $t$. This unknown function may be
supported in particular on some unknown submanifold of the $d$-dimensional
Euclidean space. While manifold learning is classically done in a
semi-supervised setting, where the entire data has to be known in advance, we
use recent ideas to achieve an accurate function approximation in a supervised
setting; i.e., construct a model for the target function. We use the
state-of-the-art clinically relevant PRED-EGA grid to evaluate our results, and
demonstrate that for a real life dataset, our method performs better than a
standard deep network, especially in hypoglycemic and hyperglycemic regimes.
One noteworthy aspect of this work is that the training data and test data may
come from different distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mhaskar_H/0/1/0/all/0/1"&gt;H.N. Mhaskar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereverzyev_S/0/1/0/all/0/1"&gt;S.V. Pereverzyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walt_M/0/1/0/all/0/1"&gt;M.D. van der Walt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Approaches for Indoor Localization for Ambient Assisted Living in Smart Homes. (arXiv:2106.15606v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.15606</id>
        <link href="http://arxiv.org/abs/2106.15606"/>
        <updated>2021-06-30T02:01:03.951Z</updated>
        <summary type="html"><![CDATA[This work makes multiple scientific contributions to the field of Indoor
Localization for Ambient Assisted Living in Smart Homes. First, it presents a
Big-Data driven methodology that studies the multimodal components of user
interactions and analyzes the data from Bluetooth Low Energy (BLE) beacons and
BLE scanners to detect a user's indoor location in a specific activity-based
zone during Activities of Daily Living. Second, it introduces a context
independent approach that can interpret the accelerometer and gyroscope data
from diverse behavioral patterns to detect the zone-based indoor location of a
user in any Internet of Things (IoT)-based environment. These two approaches
achieved performance accuracies of 81.36% and 81.13%, respectively, when tested
on a dataset. Third, it presents a methodology to detect the spatial
coordinates of a user's indoor position that outperforms all similar works in
this field, as per the associated root mean squared error - one of the
performance evaluation metrics in ISO/IEC18305:2016- an international standard
for testing Localization and Tracking Systems. Finally, it presents a
comprehensive comparative study that includes Random Forest, Artificial Neural
Network, Decision Tree, Support Vector Machine, k-NN, Gradient Boosted Trees,
Deep Learning, and Linear Regression, to address the challenge of identifying
the optimal machine learning approach for Indoor Localization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Partitioning and Sparse Matrix Ordering using Reinforcement Learning and Graph Neural Networks. (arXiv:2104.03546v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03546</id>
        <link href="http://arxiv.org/abs/2104.03546"/>
        <updated>2021-06-30T02:01:03.940Z</updated>
        <summary type="html"><![CDATA[We present a novel method for graph partitioning, based on reinforcement
learning and graph convolutional neural networks. Our approach is to
recursively partition coarser representations of a given graph. The neural
network is implemented using SAGE graph convolution layers, and trained using
an advantage actor critic (A2C) agent. We present two variants, one for finding
an edge separator that minimizes the normalized cut or quotient cut, and one
that finds a small vertex separator. The vertex separators are then used to
construct a nested dissection ordering to permute a sparse matrix so that its
triangular factorization will incur less fill-in. The partitioning quality is
compared with partitions obtained using METIS and SCOTCH, and the nested
dissection ordering is evaluated in the sparse solver SuperLU. Our results show
that the proposed method achieves similar partitioning quality as METIS and
SCOTCH. Furthermore, the method generalizes across different classes of graphs,
and works well on a variety of graphs from the SuiteSparse sparse matrix
collection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gatti_A/0/1/0/all/0/1"&gt;Alice Gatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhixiong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smidt_T/0/1/0/all/0/1"&gt;Tess Smidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1"&gt;Esmond G. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghysels_P/0/1/0/all/0/1"&gt;Pieter Ghysels&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Graph Convolution for Resting-State fMRI Analysis. (arXiv:2003.10613v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10613</id>
        <link href="http://arxiv.org/abs/2003.10613"/>
        <updated>2021-06-30T02:01:03.925Z</updated>
        <summary type="html"><![CDATA[The Blood-Oxygen-Level-Dependent (BOLD) signal of resting-state fMRI
(rs-fMRI) records the temporal dynamics of intrinsic functional networks in the
brain. However, existing deep learning methods applied to rs-fMRI either
neglect the functional dependency between different brain regions in a network
or discard the information in the temporal dynamics of brain activity. To
overcome those shortcomings, we propose to formulate functional connectivity
networks within the context of spatio-temporal graphs. We train a
spatio-temporal graph convolutional network (ST-GCN) on short sub-sequences of
the BOLD time series to model the non-stationary nature of functional
connectivity. Simultaneously, the model learns the importance of graph edges
within ST-GCN to gain insight into the functional connectivities contributing
to the prediction. In analyzing the rs-fMRI of the Human Connectome Project
(HCP, N=1,091) and the National Consortium on Alcohol and Neurodevelopment in
Adolescence (NCANDA, N=773), ST-GCN is significantly more accurate than common
approaches in predicting gender and age based on BOLD signals. Furthermore, the
brain regions and functional connections significantly contributing to the
predictions of our model are important markers according to the neuroscience
literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gadgil_S/0/1/0/all/0/1"&gt;Soham Gadgil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1"&gt;Adolf Pfefferbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1"&gt;Edith V. Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from History for Byzantine Robust Optimization. (arXiv:2012.10333v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10333</id>
        <link href="http://arxiv.org/abs/2012.10333"/>
        <updated>2021-06-30T02:01:03.880Z</updated>
        <summary type="html"><![CDATA[Byzantine robustness has received significant attention recently given its
importance for distributed and federated learning. In spite of this, we
identify severe flaws in existing algorithms even when the data across the
participants is identically distributed. First, we show realistic examples
where current state of the art robust aggregation rules fail to converge even
in the absence of any Byzantine attackers. Secondly, we prove that even if the
aggregation rules may succeed in limiting the influence of the attackers in a
single round, the attackers can couple their attacks across time eventually
leading to divergence. To address these issues, we present two surprisingly
simple strategies: a new robust iterative clipping procedure, and incorporating
worker momentum to overcome time-coupled attacks. This is the first provably
robust method for the standard stochastic optimization setting. Our code is
open sourced at https://github.com/epfml/byzantine-robust-optimizer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1"&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lie He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near field Acoustic Holography on arbitrary shapes using Convolutional Neural Network. (arXiv:2103.16935v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16935</id>
        <link href="http://arxiv.org/abs/2103.16935"/>
        <updated>2021-06-30T02:01:03.872Z</updated>
        <summary type="html"><![CDATA[Near-field Acoustic Holography (NAH) is a well-known problem aimed at
estimating the vibrational velocity field of a structure by means of acoustic
measurements. In this paper, we propose a NAH technique based on Convolutional
Neural Network (CNN). The devised CNN predicts the vibrational field on the
surface of arbitrary shaped plates (violin plates) with orthotropic material
properties from a limited number of measurements. In particular, the
architecture, named Super Resolution CNN (SRCNN), is able to estimate the
vibrational field with a higher spatial resolution compared to the input
pressure. The pressure and velocity datasets have been generated through Finite
Element Method simulations. We validate the proposed method by comparing the
estimates with the synthesized ground truth and with a state-of-the-art
technique. Moreover, we evaluate the robustness of the devised network against
noisy input data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olivieri_M/0/1/0/all/0/1"&gt;Marco Olivieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pezzoli_M/0/1/0/all/0/1"&gt;Mirco Pezzoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antonacci_F/0/1/0/all/0/1"&gt;Fabio Antonacci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarti_A/0/1/0/all/0/1"&gt;Augusto Sarti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poisson CNN: Convolutional neural networks for the solution of the Poisson equation on a Cartesian mesh. (arXiv:1910.08613v3 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.08613</id>
        <link href="http://arxiv.org/abs/1910.08613"/>
        <updated>2021-06-30T02:01:03.780Z</updated>
        <summary type="html"><![CDATA[The Poisson equation is commonly encountered in engineering, for instance in
computational fluid dynamics (CFD) where it is needed to compute corrections to
the pressure field to ensure the incompressibility of the velocity field. In
the present work, we propose a novel fully convolutional neural network (CNN)
architecture to infer the solution of the Poisson equation on a 2D Cartesian
grid with different resolutions given the right hand side term, arbitrary
boundary conditions and grid parameters. It provides unprecedented versatility
for a CNN approach dealing with partial differential equations. The boundary
conditions are handled using a novel approach by decomposing the original
Poisson problem into a homogeneous Poisson problem plus four inhomogeneous
Laplace sub-problems. The model is trained using a novel loss function
approximating the continuous $L^p$ norm between the prediction and the target.
Even when predicting on grids denser than previously encountered, our model
demonstrates encouraging capacity to reproduce the correct solution profile.
The proposed model, which outperforms well-known neural network models, can be
included in a CFD solver to help with solving the Poisson equation. Analytical
test cases indicate that our CNN architecture is capable of predicting the
correct solution of a Poisson problem with mean percentage errors below 10%, an
improvement by comparison to the first step of conventional iterative methods.
Predictions from our model, used as the initial guess to iterative algorithms
like Multigrid, can reduce the RMS error after a single iteration by more than
90% compared to a zero initial guess.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Ozbay_A/0/1/0/all/0/1"&gt;Ali Girayhan &amp;#xd6;zbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hamzehloo_A/0/1/0/all/0/1"&gt;Arash Hamzehloo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Laizet_S/0/1/0/all/0/1"&gt;Sylvain Laizet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tzirakis_P/0/1/0/all/0/1"&gt;Panagiotis Tzirakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Rizos_G/0/1/0/all/0/1"&gt;Georgios Rizos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization using Causal Matching. (arXiv:2006.07500v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07500</id>
        <link href="http://arxiv.org/abs/2006.07500"/>
        <updated>2021-06-30T02:01:03.760Z</updated>
        <summary type="html"><![CDATA[In the domain generalization literature, a common objective is to learn
representations independent of the domain after conditioning on the class
label. We show that this objective is not sufficient: there exist
counter-examples where a model fails to generalize to unseen domains even after
satisfying class-conditional domain invariance. We formalize this observation
through a structural causal model and show the importance of modeling
within-class variations for generalization. Specifically, classes contain
objects that characterize specific causal features, and domains can be
interpreted as interventions on these objects that change non-causal features.
We highlight an alternative condition: inputs across domains should have the
same representation if they are derived from the same object. Based on this
objective, we propose matching-based algorithms when base objects are observed
(e.g., through data augmentation) and approximate the objective when objects
are not observed (MatchDG). Our simple matching-based algorithms are
competitive to prior work on out-of-domain accuracy for rotated MNIST,
Fashion-MNIST, PACS, and Chest-Xray datasets. Our method MatchDG also recovers
ground-truth object matches: on MNIST and Fashion-MNIST, top-10 matches from
MatchDG have over 50% overlap with ground-truth matches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1"&gt;Divyat Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1"&gt;Shruti Tople&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Amit Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Chain Rule and Bayes' Theorem to Compare Probability Distributions. (arXiv:2012.14100v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14100</id>
        <link href="http://arxiv.org/abs/2012.14100"/>
        <updated>2021-06-30T02:01:03.755Z</updated>
        <summary type="html"><![CDATA[To measure the difference between two probability distributions, referred to
as the source and target, respectively, we exploit both the chain rule and
Bayes' theorem to construct conditional transport (CT), which is constituted by
both a forward component and a backward one. The forward CT is the expected
cost of moving a source data point to a target one, with their joint
distribution defined by the product of the source probability density function
(PDF) and a source-dependent conditional distribution, which is related to the
target PDF via Bayes' theorem. The backward CT is defined by reversing the
direction. The CT cost can be approximated by replacing the source and target
PDFs with their discrete empirical distributions supported on mini-batches,
making it amenable to implicit distributions and stochastic gradient
descent-based optimization. When applied to train a generative model, CT is
shown to strike a good balance between mode-covering and mode-seeking behaviors
and strongly resist mode collapse. On a wide variety of benchmark datasets for
generative modeling, substituting the default statistical distance of an
existing generative adversarial network with CT is shown to consistently
improve the performance. PyTorch-style code is provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huangjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Contrastive Learning. (arXiv:2106.15499v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15499</id>
        <link href="http://arxiv.org/abs/2106.15499"/>
        <updated>2021-06-30T02:01:03.728Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel contrastive learning framework, coined as
Self-Contrastive (SelfCon) Learning, that self-contrasts within multiple
outputs from the different levels of a network. We confirmed that SelfCon loss
guarantees the lower bound of mutual information (MI) between the intermediate
and last representations. Besides, we empirically showed, via various MI
estimators, that SelfCon loss highly correlates to the increase of MI and
better classification performance. In our experiments, SelfCon surpasses
supervised contrastive (SupCon) learning without the need for a multi-viewed
batch and with the cheaper computational cost. Especially on ResNet-18, we
achieved top-1 classification accuracy of 76.45% for the CIFAR-100 dataset,
which is 2.87% and 4.36% higher than SupCon and cross-entropy loss,
respectively. We found that mitigating both vanishing gradient and overfitting
issue makes our method outperform the counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1"&gt;Sangmin Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungnyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1"&gt;Jongwoo Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gihun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_S/0/1/0/all/0/1"&gt;Seungjong Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Se-Young Yun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Size and Depth Separation in Approximating Benign Functions with Neural Networks. (arXiv:2102.00314v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00314</id>
        <link href="http://arxiv.org/abs/2102.00314"/>
        <updated>2021-06-30T02:01:03.723Z</updated>
        <summary type="html"><![CDATA[When studying the expressive power of neural networks, a main challenge is to
understand how the size and depth of the network affect its ability to
approximate real functions. However, not all functions are interesting from a
practical viewpoint: functions of interest usually have a polynomially-bounded
Lipschitz constant, and can be computed efficiently. We call functions that
satisfy these conditions "benign", and explore the benefits of size and depth
for approximation of benign functions with ReLU networks. As we show, this
problem is more challenging than the corresponding problem for non-benign
functions. We give barriers to showing depth-lower-bounds: Proving existence of
a benign function that cannot be approximated by polynomial-size networks of
depth $4$ would settle longstanding open problems in computational complexity.
It implies that beyond depth $4$ there is a barrier to showing depth-separation
for benign functions, even between networks of constant depth and networks of
nonconstant depth. We also study size-separation, namely, whether there are
benign functions that can be approximated with networks of size $O(s(d))$, but
not with networks of size $O(s'(d))$. We show a complexity-theoretic barrier to
proving such results beyond size $O(d\log^2(d))$, but also show an explicit
benign function, that can be approximated with networks of size $O(d)$ and not
with networks of size $o(d/\log d)$. For approximation in $L_\infty$ we achieve
such separation already between size $O(d)$ and size $o(d)$. Moreover, we show
superpolynomial size lower bounds and barriers to such lower bounds, depending
on the assumptions on the function. Our size-separation results rely on an
analysis of size lower bounds for Boolean functions, which is of independent
interest: We show linear size lower bounds for computing explicit Boolean
functions with neural networks and threshold circuits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1"&gt;Gal Vardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichman_D/0/1/0/all/0/1"&gt;Daniel Reichman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pitassi_T/0/1/0/all/0/1"&gt;Toniann Pitassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attack Transferability Characterization for Adversarially Robust Multi-label Classification. (arXiv:2106.15360v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15360</id>
        <link href="http://arxiv.org/abs/2106.15360"/>
        <updated>2021-06-30T02:01:03.717Z</updated>
        <summary type="html"><![CDATA[Despite of the pervasive existence of multi-label evasion attack, it is an
open yet essential problem to characterize the origin of the adversarial
vulnerability of a multi-label learning system and assess its attackability. In
this study, we focus on non-targeted evasion attack against multi-label
classifiers. The goal of the threat is to cause miss-classification with
respect to as many labels as possible, with the same input perturbation. Our
work gains in-depth understanding about the multi-label adversarial attack by
first characterizing the transferability of the attack based on the functional
properties of the multi-label classifier. We unveil how the transferability
level of the attack determines the attackability of the classifier via
establishing an information-theoretic analysis of the adversarial risk.
Furthermore, we propose a transferability-centered attackability assessment,
named Soft Attackability Estimator (SAE), to evaluate the intrinsic
vulnerability level of the targeted multi-label classifier. This estimator is
then integrated as a transferability-tuning regularization term into the
multi-label learning paradigm to achieve adversarially robust classification.
The experimental study on real-world data echos the theoretical analysis and
verify the validity of the transferability-regularized multi-label learning
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yufei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangliang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On exploring practical potentials of quantum auto-encoder with advantages. (arXiv:2106.15432v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.15432</id>
        <link href="http://arxiv.org/abs/2106.15432"/>
        <updated>2021-06-30T02:01:03.703Z</updated>
        <summary type="html"><![CDATA[Quantum auto-encoder (QAE) is a powerful tool to relieve the curse of
dimensionality encountered in quantum physics, celebrated by the ability to
extract low-dimensional patterns from quantum states living in the
high-dimensional space. Despite its attractive properties, little is known
about the practical applications of QAE with provable advantages. To address
these issues, here we prove that QAE can be used to efficiently calculate the
eigenvalues and prepare the corresponding eigenvectors of a high-dimensional
quantum state with the low-rank property. With this regard, we devise three
effective QAE-based learning protocols to solve the low-rank state fidelity
estimation, the quantum Gibbs state preparation, and the quantum metrology
tasks, respectively. Notably, all of these protocols are scalable and can be
readily executed on near-term quantum machines. Moreover, we prove that the
error bounds of the proposed QAE-based methods outperform those in previous
literature. Numerical simulations collaborate with our theoretical analysis.
Our work opens a new avenue of utilizing QAE to tackle various quantum physics
and quantum information processing problems in a scalable way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuxuan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Dimensionality Reduction for Comparative Analysis. (arXiv:2106.15481v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15481</id>
        <link href="http://arxiv.org/abs/2106.15481"/>
        <updated>2021-06-30T02:01:03.698Z</updated>
        <summary type="html"><![CDATA[Finding the similarities and differences between two or more groups of
datasets is a fundamental analysis task. For high-dimensional data,
dimensionality reduction (DR) methods are often used to find the
characteristics of each group. However, existing DR methods provide limited
capability and flexibility for such comparative analysis as each method is
designed only for a narrow analysis target, such as identifying factors that
most differentiate groups. In this work, we introduce an interactive DR
framework where we integrate our new DR method, called ULCA (unified linear
comparative analysis), with an interactive visual interface. ULCA unifies two
DR schemes, discriminant analysis and contrastive learning, to support various
comparative analysis tasks. To provide flexibility for comparative analysis, we
develop an optimization algorithm that enables analysts to interactively refine
ULCA results. Additionally, we provide an interactive visualization interface
to examine ULCA results with a rich set of analysis libraries. We evaluate ULCA
and the optimization algorithm to show their efficiency as well as present
multiple case studies using real-world datasets to demonstrate the usefulness
of our framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujiwara_T/0/1/0/all/0/1"&gt;Takanori Fujiwara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xinhai Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kwan-Liu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Gradient Methods for Convex Optimization with General Affine and Nonlinear Constraints. (arXiv:2007.00153v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00153</id>
        <link href="http://arxiv.org/abs/2007.00153"/>
        <updated>2021-06-30T02:01:03.693Z</updated>
        <summary type="html"><![CDATA[Conditional gradient methods have attracted much attention in both machine
learning and optimization communities recently. These simple methods can
guarantee the generation of sparse solutions. In addition, without the
computation of full gradients, they can handle huge-scale problems sometimes
even with an exponentially increasing number of decision variables. This paper
aims to significantly expand the application areas of these methods by
presenting new conditional gradient methods for solving convex optimization
problems with general affine and nonlinear constraints. More specifically, we
first present a new constraint extrapolated condition gradient (CoexCG) method
that can achieve an ${\cal O}(1/\epsilon^2)$ iteration complexity for both
smooth and structured nonsmooth function constrained convex optimization. We
further develop novel variants of CoexCG, namely constraint extrapolated and
dual regularized conditional gradient (CoexDurCG) methods, that can achieve
similar iteration complexity to CoexCG but allow adaptive selection for
algorithmic parameters. We illustrate the effectiveness of these methods for
solving an important class of radiation therapy treatment planning problems
arising from healthcare industry. To the best of our knowledge, all the
algorithmic schemes and their complexity results are new in the area of
projection-free methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lan_G/0/1/0/all/0/1"&gt;Guanghui Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Romeijn_E/0/1/0/all/0/1"&gt;Edwin Romeijn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhiqiang Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphAnoGAN: Detecting Anomalous Snapshots from Attributed Graphs. (arXiv:2106.15504v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15504</id>
        <link href="http://arxiv.org/abs/2106.15504"/>
        <updated>2021-06-30T02:01:03.688Z</updated>
        <summary type="html"><![CDATA[Finding anomalous snapshots from a graph has garnered huge attention
recently. Existing studies address the problem using shallow learning
mechanisms such as subspace selection, ego-network, or community analysis.
These models do not take into account the multifaceted interactions between the
structure and attributes in the network. In this paper, we propose GraphAnoGAN,
an anomalous snapshot ranking framework, which consists of two core components
-- generative and discriminative models. Specifically, the generative model
learns to approximate the distribution of anomalous samples from the candidate
set of graph snapshots, and the discriminative model detects whether the
sampled snapshot is from the ground-truth or not. Experiments on 4 real-world
networks show that GraphAnoGAN outperforms 6 baselines with a significant
margin (28.29% and 22.01% higher precision and recall, respectively compared to
the best baseline, averaged across all datasets).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1"&gt;Siddharth Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1"&gt;Bryan Hooi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Off-Policy with Online Planning. (arXiv:2008.10066v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10066</id>
        <link href="http://arxiv.org/abs/2008.10066"/>
        <updated>2021-06-30T02:01:03.684Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) in low-data and risk-sensitive domains requires
performant and flexible deployment policies that can readily incorporate
constraints during deployment. One such class of policies are the
semi-parametric H-step lookahead policies, which select actions using
trajectory optimization over a dynamics model for a fixed horizon with a
terminal value function. In this work, we investigate a novel instantiation of
H-step lookahead with a learned model and a terminal value function learned by
a model-free off-policy algorithm, named Learning Off-Policy with Online
Planning (LOOP). We provide a theoretical analysis of this method, suggesting a
tradeoff between model errors and value function errors and empirically
demonstrate this tradeoff to be beneficial in deep reinforcement learning.
Furthermore, we identify the "Actor Divergence" issue in this framework and
propose Actor Regularized Control (ARC), a modified trajectory optimization
procedure. We evaluate our method on a set of robotic tasks for Offline and
Online RL and demonstrate improved performance. We also show the flexibility of
LOOP to incorporate safety constraints during deployment with a set of
navigation environments. We demonstrate that LOOP is a desirable framework for
robotics applications based on its strong performance in various important RL
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sikchi_H/0/1/0/all/0/1"&gt;Harshit Sikchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wenxuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1"&gt;David Held&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bytecode-based Approach for Smart Contract Classification. (arXiv:2106.15497v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.15497</id>
        <link href="http://arxiv.org/abs/2106.15497"/>
        <updated>2021-06-30T02:01:03.678Z</updated>
        <summary type="html"><![CDATA[With the development of blockchain technologies, the number of smart
contracts deployed on blockchain platforms is growing exponentially, which
makes it difficult for users to find desired services by manual screening. The
automatic classification of smart contracts can provide blockchain users with
keyword-based contract searching and helps to manage smart contracts
effectively. Current research on smart contract classification focuses on
Natural Language Processing (NLP) solutions which are based on contract source
code. However, more than 94% of smart contracts are not open-source, so the
application scenarios of NLP methods are very limited. Meanwhile, NLP models
are vulnerable to adversarial attacks. This paper proposes a classification
model based on features from contract bytecode instead of source code to solve
these problems. We also use feature selection and ensemble learning to optimize
the model. Our experimental studies on over 3,300 real-world Ethereum smart
contracts show that our model can classify smart contracts without source code
and has better performance than baseline models. Our model also has good
resistance to adversarial attacks compared with NLP-based models. In addition,
our analysis reveals that account features used in many smart contract
classification models have little effect on classification and can be excluded.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chaochen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yong Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doss_R/0/1/0/all/0/1"&gt;Robin Ram Mohan Doss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jiangshan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sood_K/0/1/0/all/0/1"&gt;Keshav Sood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Longxiang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-dimensional separability for one- and few-shot learning. (arXiv:2106.15416v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15416</id>
        <link href="http://arxiv.org/abs/2106.15416"/>
        <updated>2021-06-30T02:01:03.673Z</updated>
        <summary type="html"><![CDATA[This work is driven by a practical question, corrections of Artificial
Intelligence (AI) errors. Systematic re-training of a large AI system is hardly
possible. To solve this problem, special external devices, correctors, are
developed. They should provide quick and non-iterative system fix without
modification of a legacy AI system. A common universal part of the AI corrector
is a classifier that should separate undesired and erroneous behavior from
normal operation. Training of such classifiers is a grand challenge at the
heart of the one- and few-shot learning methods. Effectiveness of one- and
few-short methods is based on either significant dimensionality reductions or
the blessing of dimensionality effects. Stochastic separability is a blessing
of dimensionality phenomenon that allows one-and few-shot error correction: in
high-dimensional datasets under broad assumptions each point can be separated
from the rest of the set by simple and robust linear discriminant. The
hierarchical structure of data universe is introduced where each data cluster
has a granular internal structure, etc. New stochastic separation theorems for
the data distributions with fine-grained structure are formulated and proved.
Separation theorems in infinite-dimensional limits are proven under assumptions
of compact embedding of patterns into data space. New multi-correctors of AI
systems are presented and illustrated with examples of predicting errors and
learning new classes of objects by a deep convolutional neural network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1"&gt;Alexander N. Gorban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grechuk_B/0/1/0/all/0/1"&gt;Bogdan Grechuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1"&gt;Evgeny M. Mirkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stasenko_S/0/1/0/all/0/1"&gt;Sergey V. Stasenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1"&gt;Ivan Y. Tyukin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Convergent and Efficient Deep Q Network Algorithm. (arXiv:2106.15419v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15419</id>
        <link href="http://arxiv.org/abs/2106.15419"/>
        <updated>2021-06-30T02:01:03.660Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of the deep Q network (DQN) reinforcement
learning algorithm and its variants, DQN is still not well understood and it
does not guarantee convergence. In this work, we show that DQN can diverge and
cease to operate in realistic settings. Although there exist gradient-based
convergent methods, we show that they actually have inherent problems in
learning behaviour and elucidate why they often fail in practice. To overcome
these problems, we propose a convergent DQN algorithm (C-DQN) by carefully
modifying DQN, and we show that the algorithm is convergent and can work with
large discount factors (0.9998). It learns robustly in difficult settings and
can learn several difficult games in the Atari 2600 benchmark where DQN fail,
within a moderate computational budget. Our codes have been publicly released
and can be used to reproduce our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhikang T. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masahito Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Mechanism for Producing Aligned Latent Spaces with Autoencoders. (arXiv:2106.15456v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15456</id>
        <link href="http://arxiv.org/abs/2106.15456"/>
        <updated>2021-06-30T02:01:03.654Z</updated>
        <summary type="html"><![CDATA[Aligned latent spaces, where meaningful semantic shifts in the input space
correspond to a translation in the embedding space, play an important role in
the success of downstream tasks such as unsupervised clustering and data
imputation. In this work, we prove that linear and nonlinear autoencoders
produce aligned latent spaces by stretching along the left singular vectors of
the data. We fully characterize the amount of stretching in linear autoencoders
and provide an initialization scheme to arbitrarily stretch along the top
directions using these networks. We also quantify the amount of stretching in
nonlinear autoencoders in a simplified setting. We use our theoretical results
to align drug signatures across cell types in gene expression space and
semantic shifts in word embedding spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saachi Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1"&gt;Adityanarayanan Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1"&gt;Caroline Uhler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sounds of COVID-19: exploring realistic performance of audio-based digital testing. (arXiv:2106.15523v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.15523</id>
        <link href="http://arxiv.org/abs/2106.15523"/>
        <updated>2021-06-30T02:01:03.650Z</updated>
        <summary type="html"><![CDATA[Researchers have been battling with the question of how we can identify
Coronavirus disease (COVID-19) cases efficiently, affordably and at scale.
Recent work has shown how audio based approaches, which collect respiratory
audio data (cough, breathing and voice) can be used for testing, however there
is a lack of exploration of how biases and methodological decisions impact
these tools' performance in practice. In this paper, we explore the realistic
performance of audio-based digital testing of COVID-19. To investigate this, we
collected a large crowdsourced respiratory audio dataset through a mobile app,
alongside recent COVID-19 test result and symptoms intended as a ground truth.
Within the collected dataset, we selected 5,240 samples from 2,478 participants
and split them into different participant-independent sets for model
development and validation. Among these, we controlled for potential
confounding factors (such as demographics and language). The unbiased model
takes features extracted from breathing, coughs, and voice signals as
predictors and yields an AUC-ROC of 0.71 (95\% CI: 0.65$-$0.77). We further
explore different unbalanced distributions to show how biases and participant
splits affect performance. Finally, we discuss how the realistic model
presented could be integrated in clinical practice to realize continuous,
ubiquitous, sustainable and affordable testing at population scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spathis_D/0/1/0/all/0/1"&gt;Dimitris Spathis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondareva_E/0/1/0/all/0/1"&gt;Erika Bondareva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_C/0/1/0/all/0/1"&gt;Chlo&amp;#xeb; Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1"&gt;Jagmohan Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1"&gt;Ting Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grammenos_A/0/1/0/all/0/1"&gt;Andreas Grammenos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasthanasombat_A/0/1/0/all/0/1"&gt;Apinan Hasthanasombat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Floto_A/0/1/0/all/0/1"&gt;Andres Floto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicuta_P/0/1/0/all/0/1"&gt;Pietro Cicuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1"&gt;Cecilia Mascolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining the Performance of Multi-label Classification Methods with Data Set Properties. (arXiv:2106.15411v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15411</id>
        <link href="http://arxiv.org/abs/2106.15411"/>
        <updated>2021-06-30T02:01:03.636Z</updated>
        <summary type="html"><![CDATA[Meta learning generalizes the empirical experience with different learning
tasks and holds promise for providing important empirical insight into the
behaviour of machine learning algorithms. In this paper, we present a
comprehensive meta-learning study of data sets and methods for multi-label
classification (MLC). MLC is a practically relevant machine learning task where
each example is labelled with multiple labels simultaneously. Here, we analyze
40 MLC data sets by using 50 meta features describing different properties of
the data. The main findings of this study are as follows. First, the most
prominent meta features that describe the space of MLC data sets are the ones
assessing different aspects of the label space. Second, the meta models show
that the most important meta features describe the label space, and, the meta
features describing the relationships among the labels tend to occur a bit more
often than the meta features describing the distributions between and within
the individual labels. Third, the optimization of the hyperparameters can
improve the predictive performance, however, quite often the extent of the
improvements does not always justify the resource utilization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bogatinovski_J/0/1/0/all/0/1"&gt;Jasmin Bogatinovski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Todorovski_L/0/1/0/all/0/1"&gt;Ljup&amp;#x10d;o Todorovski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dzeroski_S/0/1/0/all/0/1"&gt;Sa&amp;#x161;o D&amp;#x17e;eroski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kocev_D/0/1/0/all/0/1"&gt;Dragi Kocev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Survey of Incentive Mechanism for Federated Learning. (arXiv:2106.15406v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15406</id>
        <link href="http://arxiv.org/abs/2106.15406"/>
        <updated>2021-06-30T02:01:03.631Z</updated>
        <summary type="html"><![CDATA[Federated learning utilizes various resources provided by participants to
collaboratively train a global model, which potentially address the data
privacy issue of machine learning. In such promising paradigm, the performance
will be deteriorated without sufficient training data and other resources in
the learning process. Thus, it is quite crucial to inspire more participants to
contribute their valuable resources with some payments for federated learning.
In this paper, we present a comprehensive survey of incentive schemes for
federate learning. Specifically, we identify the incentive problem in federated
learning and then provide a taxonomy for various schemes. Subsequently, we
summarize the existing incentive mechanisms in terms of the main techniques,
such as Stackelberg game, auction, contract theory, Shapley value,
reinforcement learning, blockchain. By reviewing and comparing some impressive
results, we figure out three directions for the future study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_R/0/1/0/all/0/1"&gt;Rongfei Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1"&gt;Chao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xiaowen Chu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Short-Term Load Forecasting for Smart HomeAppliances with Sequence to Sequence Learning. (arXiv:2106.15348v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.15348</id>
        <link href="http://arxiv.org/abs/2106.15348"/>
        <updated>2021-06-30T02:01:03.626Z</updated>
        <summary type="html"><![CDATA[Appliance-level load forecasting plays a critical role in residential energy
management, besides having significant importance for ancillary services
performed by the utilities. In this paper, we propose to use an LSTM-based
sequence-to-sequence (seq2seq) learning model that can capture the load
profiles of appliances. We use a real dataset collected fromfour residential
buildings and compare our proposed schemewith three other techniques, namely
VARMA, Dilated One Dimensional Convolutional Neural Network, and an LSTM
model.The results show that the proposed LSTM-based seq2seq model outperforms
other techniques in terms of prediction error in most cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Razghandi_M/0/1/0/all/0/1"&gt;Mina Razghandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Erol_Kantarci_M/0/1/0/all/0/1"&gt;Melike Erol-Kantarci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turgut_D/0/1/0/all/0/1"&gt;Damla Turgut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-stage Optimization based Adversarial Training. (arXiv:2106.15357v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15357</id>
        <link href="http://arxiv.org/abs/2106.15357"/>
        <updated>2021-06-30T02:01:03.621Z</updated>
        <summary type="html"><![CDATA[In the field of adversarial robustness, there is a common practice that
adopts the single-step adversarial training for quickly developing
adversarially robust models. However, the single-step adversarial training is
most likely to cause catastrophic overfitting, as after a few training epochs
it will be hard to generate strong adversarial examples to continuously boost
the adversarial robustness. In this work, we aim to avoid the catastrophic
overfitting by introducing multi-step adversarial examples during the
single-step adversarial training. Then, to balance the large training overhead
of generating multi-step adversarial examples, we propose a Multi-stage
Optimization based Adversarial Training (MOAT) method that periodically trains
the model on mixed benign examples, single-step adversarial examples, and
multi-step adversarial examples stage by stage. In this way, the overall
training overhead is reduced significantly, meanwhile, the model could avoid
catastrophic overfitting. Extensive experiments on CIFAR-10 and CIFAR-100
datasets demonstrate that under similar amount of training overhead, the
proposed MOAT exhibits better robustness than either single-step or multi-step
adversarial training methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chuanbiao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"&gt;Kun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zoo-Tuning: Adaptive Transfer from a Zoo of Models. (arXiv:2106.15434v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15434</id>
        <link href="http://arxiv.org/abs/2106.15434"/>
        <updated>2021-06-30T02:01:03.616Z</updated>
        <summary type="html"><![CDATA[With the development of deep networks on various large-scale datasets, a
large zoo of pretrained models are available. When transferring from a model
zoo, applying classic single-model based transfer learning methods to each
source model suffers from high computational burden and cannot fully utilize
the rich knowledge in the zoo. We propose \emph{Zoo-Tuning} to address these
challenges, which learns to adaptively transfer the parameters of pretrained
models to the target task. With the learnable channel alignment layer and
adaptive aggregation layer, Zoo-Tuning \emph{adaptively aggregates channel
aligned pretrained parameters} to derive the target model, which promotes
knowledge transfer by simultaneously adapting multiple source models to
downstream tasks. The adaptive aggregation substantially reduces the
computation cost at both training and inference. We further propose lite
Zoo-Tuning with the temporal ensemble of batch average gating values to reduce
the storage cost at the inference time. We evaluate our approach on a variety
of tasks, including reinforcement learning, image classification, and facial
landmark detection. Experiment results demonstrate that the proposed adaptive
transfer learning approach can transfer knowledge from a zoo of models more
effectively and efficiently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1"&gt;Yang Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kou_Z/0/1/0/all/0/1"&gt;Zhi Kou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhangjie Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Graph Learning for Scalable Multi-view Clustering. (arXiv:2106.15382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15382</id>
        <link href="http://arxiv.org/abs/2106.15382"/>
        <updated>2021-06-30T02:01:03.611Z</updated>
        <summary type="html"><![CDATA[Graph-based multi-view clustering has become an active topic due to the
efficiency in characterizing both the complex structure and relationship
between multimedia data. However, existing methods have the following
shortcomings: (1) They are inefficient or even fail for graph learning in large
scale due to the graph construction and eigen-decomposition. (2) They cannot
well exploit both the complementary information and spatial structure embedded
in graphs of different views. To well exploit complementary information and
tackle the scalability issue plaguing graph-based multi-view clustering, we
propose an efficient multiple graph learning model via a small number of anchor
points and tensor Schatten p-norm minimization. Specifically, we construct a
hidden and tractable large graph by anchor graph for each view and well exploit
complementary information embedded in anchor graphs of different views by
tensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,
which scales linearly with the data size, to solve our proposed model.
Extensive experimental results on several datasets indicate that our proposed
method outperforms some state-of-the-art multi-view clustering algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tianyu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Quanxue Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Interaction Detection for Click-Through Rate Prediction. (arXiv:2106.15400v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15400</id>
        <link href="http://arxiv.org/abs/2106.15400"/>
        <updated>2021-06-30T02:01:03.606Z</updated>
        <summary type="html"><![CDATA[Click-Through Rate prediction aims to predict the ratio of clicks to
impressions of a specific link. This is a challenging task since (1) there are
usually categorical features, and the inputs will be extremely high-dimensional
if one-hot encoding is applied, (2) not only the original features but also
their interactions are important, (3) an effective prediction may rely on
different features and interactions in different time periods. To overcome
these difficulties, we propose a new interaction detection method, named Online
Random Intersection Chains. The method, which is based on the idea of frequent
itemset mining, detects informative interactions by observing the intersections
of randomly chosen samples. The discovered interactions enjoy high
interpretability as they can be comprehended as logical expressions. ORIC can
be updated every time new data is collected, without being retrained on
historical data. What's more, the importance of the historical and latest data
can be controlled by a tuning parameter. A framework is designed to deal with
the streaming interactions, so almost all existing models for CTR prediction
can be applied after interaction detection. Empirical results demonstrate the
efficiency and effectiveness of ORIC on three benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qiuqiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chuanhou Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning with Gaussian Processes. (arXiv:2106.15482v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15482</id>
        <link href="http://arxiv.org/abs/2106.15482"/>
        <updated>2021-06-30T02:01:03.602Z</updated>
        <summary type="html"><![CDATA[Federated learning aims to learn a global model that performs well on client
devices with limited cross-client communication. Personalized federated
learning (PFL) further extends this setup to handle data heterogeneity between
clients by learning personalized models. A key challenge in this setting is to
learn effectively across clients even though each client has unique data that
is often limited in size. Here we present pFedGP, a solution to PFL that is
based on Gaussian processes (GPs) with deep kernel learning. GPs are highly
expressive models that work well in the low data regime due to their Bayesian
nature. However, applying GPs to PFL raises multiple challenges. Mainly, GPs
performance depends heavily on access to a good kernel function, and learning a
kernel requires a large training set. Therefore, we propose learning a shared
kernel function across all clients, parameterized by a neural network, with a
personal GP classifier for each client. We further extend pFedGP to include
inducing points using two novel methods, the first helps to improve
generalization in the low data regime and the second reduces the computational
cost. We derive a PAC-Bayes generalization bound on novel clients and
empirically show that it gives non-vacuous guarantees. Extensive experiments on
standard PFL benchmarks with CIFAR-10, CIFAR-100, and CINIC-10, and on a new
setup of learning under input noise show that pFedGP achieves well-calibrated
predictions while significantly outperforming baseline methods, reaching up to
21% in accuracy gain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1"&gt;Idan Achituve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamsian_A/0/1/0/all/0/1"&gt;Aviv Shamsian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1"&gt;Aviv Navon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1"&gt;Ethan Fetaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attentive Neural Processes and Batch Bayesian Optimization for Scalable Calibration of Physics-Informed Digital Twins. (arXiv:2106.15502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15502</id>
        <link href="http://arxiv.org/abs/2106.15502"/>
        <updated>2021-06-30T02:01:03.583Z</updated>
        <summary type="html"><![CDATA[Physics-informed dynamical system models form critical components of digital
twins of the built environment. These digital twins enable the design of
energy-efficient infrastructure, but must be properly calibrated to accurately
reflect system behavior for downstream prediction and analysis. Dynamical
system models of modern buildings are typically described by a large number of
parameters and incur significant computational expenditure during simulations.
To handle large-scale calibration of digital twins without exorbitant
simulations, we propose ANP-BBO: a scalable and parallelizable batch-wise
Bayesian optimization (BBO) methodology that leverages attentive neural
processes (ANPs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarty_A/0/1/0/all/0/1"&gt;Ankush Chakrabarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wichern_G/0/1/0/all/0/1"&gt;Gordon Wichern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laughman_C/0/1/0/all/0/1"&gt;Christopher Laughman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Approximation of the Sliced-Wasserstein Distance Using Concentration of Random Projections. (arXiv:2106.15427v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15427</id>
        <link href="http://arxiv.org/abs/2106.15427"/>
        <updated>2021-06-30T02:01:03.568Z</updated>
        <summary type="html"><![CDATA[The Sliced-Wasserstein distance (SW) is being increasingly used in machine
learning applications as an alternative to the Wasserstein distance and offers
significant computational and statistical benefits. Since it is defined as an
expectation over random projections, SW is commonly approximated by Monte
Carlo. We adopt a new perspective to approximate SW by making use of the
concentration of measure phenomenon: under mild assumptions, one-dimensional
projections of a high-dimensional random vector are approximately Gaussian.
Based on this observation, we develop a simple deterministic approximation for
SW. Our method does not require sampling a number of random projections, and is
therefore both accurate and easy to use compared to the usual Monte Carlo
approximation. We derive nonasymptotical guarantees for our approach, and show
that the approximation error goes to zero as the dimension increases, under a
weak dependence condition on the data distribution. We validate our theoretical
findings on synthetic datasets, and illustrate the proposed approximation on a
generative modeling problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nadjahi_K/0/1/0/all/0/1"&gt;Kimia Nadjahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1"&gt;Alain Durmus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jacob_P/0/1/0/all/0/1"&gt;Pierre E. Jacob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Badeau_R/0/1/0/all/0/1"&gt;Roland Badeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1"&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Task Informed Abstraction. (arXiv:2106.15612v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15612</id>
        <link href="http://arxiv.org/abs/2106.15612"/>
        <updated>2021-06-30T02:01:03.556Z</updated>
        <summary type="html"><![CDATA[Current model-based reinforcement learning methods struggle when operating
from complex visual scenes due to their inability to prioritize task-relevant
features. To mitigate this problem, we propose learning Task Informed
Abstractions (TIA) that explicitly separates reward-correlated visual features
from distractors. For learning TIA, we introduce the formalism of Task Informed
MDP (TiMDP) that is realized by training two models that learn visual features
via cooperative reconstruction, but one model is adversarially dissociated from
the reward signal. Empirical evaluation shows that TIA leads to significant
performance gains over state-of-the-art methods on many visual control tasks
where natural and unconstrained visual distractions pose a formidable
challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Ge Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1"&gt;Pulkit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi Jaakkola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Framework for an Intelligent Affect Aware Smart Home Environment for Elderly People. (arXiv:2106.15599v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.15599</id>
        <link href="http://arxiv.org/abs/2106.15599"/>
        <updated>2021-06-30T02:01:03.540Z</updated>
        <summary type="html"><![CDATA[The population of elderly people has been increasing at a rapid rate over the
last few decades and their population is expected to further increase in the
upcoming future. Their increasing population is associated with their
increasing needs due to problems like physical disabilities, cognitive issues,
weakened memory and disorganized behavior, that elderly people face with
increasing age. To reduce their financial burden on the world economy and to
enhance their quality of life, it is essential to develop technology-based
solutions that are adaptive, assistive and intelligent in nature. Intelligent
Affect Aware Systems that can not only analyze but also predict the behavior of
elderly people in the context of their day to day interactions with technology
in an IoT-based environment, holds immense potential for serving as a long-term
solution for improving the user experience of elderly in smart homes. This work
therefore proposes the framework for an Intelligent Affect Aware environment
for elderly people that can not only analyze the affective components of their
interactions but also predict their likely user experience even before they
start engaging in any activity in the given smart home environment. This
forecasting of user experience would provide scope for enhancing the same,
thereby increasing the assistive and adaptive nature of such intelligent
systems. To uphold the efficacy of this proposed framework for improving the
quality of life of elderly people in smart homes, it has been tested on three
datasets and the results are presented and discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limited depth bandit-based strategy for Monte Carlo planning in continuous action spaces. (arXiv:2106.15594v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.15594</id>
        <link href="http://arxiv.org/abs/2106.15594"/>
        <updated>2021-06-30T02:01:03.535Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of optimal control using search trees. We
start by considering multi-armed bandit problems with continuous action spaces
and propose LD-HOO, a limited depth variant of the hierarchical optimistic
optimization (HOO) algorithm. We provide a regret analysis for LD-HOO and show
that, asymptotically, our algorithm exhibits the same cumulative regret as the
original HOO while being faster and more memory efficient. We then propose a
Monte Carlo tree search algorithm based on LD-HOO for optimal control problems
and illustrate the resulting approach's application in several optimal control
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Quinteiro_R/0/1/0/all/0/1"&gt;Ricardo Quinteiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Melo_F/0/1/0/all/0/1"&gt;Francisco S. Melo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Santos_P/0/1/0/all/0/1"&gt;Pedro A. Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Span-based Joint Entity and Relation Extraction with Transformer Pre-training. (arXiv:1909.07755v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.07755</id>
        <link href="http://arxiv.org/abs/1909.07755"/>
        <updated>2021-06-30T02:01:03.529Z</updated>
        <summary type="html"><![CDATA[We introduce SpERT, an attention model for span-based joint entity and
relation extraction. Our key contribution is a light-weight reasoning on BERT
embeddings, which features entity recognition and filtering, as well as
relation classification with a localized, marker-free context representation.
The model is trained using strong within-sentence negative samples, which are
efficiently extracted in a single BERT pass. These aspects facilitate a search
over all spans in the sentence.

In ablation studies, we demonstrate the benefits of pre-training, strong
negative sampling and localized context. Our model outperforms prior work by up
to 2.6% F1 score on several datasets for joint entity and relation extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eberts_M/0/1/0/all/0/1"&gt;Markus Eberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulges_A/0/1/0/all/0/1"&gt;Adrian Ulges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Latent Process Flows. (arXiv:2106.15580v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15580</id>
        <link href="http://arxiv.org/abs/2106.15580"/>
        <updated>2021-06-30T02:01:03.523Z</updated>
        <summary type="html"><![CDATA[Partial observations of continuous time-series dynamics at arbitrary time
stamps exist in many disciplines. Fitting this type of data using statistical
models with continuous dynamics is not only promising at an intuitive level but
also has practical benefits, including the ability to generate continuous
trajectories and to perform inference on previously unseen time stamps. Despite
exciting progress in this area, the existing models still face challenges in
terms of their representational power and the quality of their variational
approximations. We tackle these challenges with continuous latent process flows
(CLPF), a principled architecture decoding continuous latent processes into
continuous observable processes using a time-dependent normalizing flow driven
by a stochastic differential equation. To optimize our model using maximum
likelihood, we propose a novel piecewise construction of a variational
posterior process and derive the corresponding variational lower bound using
trajectory re-weighting. Our ablation studies demonstrate the effectiveness of
our contributions in various inference tasks on irregular time grids.
Comparisons to state-of-the-art baselines show our model's favourable
performance on both synthetic and real-world time-series data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1"&gt;Ruizhi Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1"&gt;Marcus A. Brubaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1"&gt;Greg Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehrmann_A/0/1/0/all/0/1"&gt;Andreas M. Lehrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Graph Neural Network Ensembles for Large-Scale Molecular Property Prediction. (arXiv:2106.15529v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15529</id>
        <link href="http://arxiv.org/abs/2106.15529"/>
        <updated>2021-06-30T02:01:03.518Z</updated>
        <summary type="html"><![CDATA[In order to advance large-scale graph machine learning, the Open Graph
Benchmark Large Scale Challenge (OGB-LSC) was proposed at the KDD Cup 2021. The
PCQM4M-LSC dataset defines a molecular HOMO-LUMO property prediction task on
about 3.8M graphs. In this short paper, we show our current work-in-progress
solution which builds an ensemble of three graph neural networks models based
on GIN, Bayesian Neural Networks and DiffPool. Our approach outperforms the
provided baseline by 7.6%. Moreover, using uncertainty in our ensemble's
prediction, we can identify molecules whose HOMO-LUMO gaps are harder to
predict (with Pearson's correlation of 0.5181). We anticipate that this will
facilitate active learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kosasih_E/0/1/0/all/0/1"&gt;Edward Elson Kosasih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabezas_J/0/1/0/all/0/1"&gt;Joaquin Cabezas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sumba_X/0/1/0/all/0/1"&gt;Xavier Sumba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bielak_P/0/1/0/all/0/1"&gt;Piotr Bielak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagowski_K/0/1/0/all/0/1"&gt;Kamil Tagowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Idanwekhai_K/0/1/0/all/0/1"&gt;Kelvin Idanwekhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tjandra_B/0/1/0/all/0/1"&gt;Benedict Aaron Tjandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamasb_A/0/1/0/all/0/1"&gt;Arian Rokkum Jamasb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement. (arXiv:2106.15575v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.15575</id>
        <link href="http://arxiv.org/abs/2106.15575"/>
        <updated>2021-06-30T02:01:03.503Z</updated>
        <summary type="html"><![CDATA[Deep neural networks for image quality enhancement typically need large
quantities of highly-curated training data comprising pairs of low-quality
images and their corresponding high-quality images. While high-quality image
acquisition is typically expensive and time-consuming, medium-quality images
are faster to acquire, at lower equipment costs, and available in larger
quantities. Thus, we propose a novel generative adversarial network (GAN) that
can leverage training data at multiple levels of quality (e.g., high and medium
quality) to improve performance while limiting costs of data curation. We apply
our mixed-supervision GAN to (i) super-resolve histopathology images and (ii)
enhance laparoscopy images by combining super-resolution and surgical smoke
removal. Results on large clinical and pre-clinical datasets show the benefits
of our mixed-supervision GAN over the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1"&gt;Uddeshya Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1"&gt;Suyash Awate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-aware Transformer for molecular property prediction. (arXiv:2106.15516v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15516</id>
        <link href="http://arxiv.org/abs/2106.15516"/>
        <updated>2021-06-30T02:01:03.472Z</updated>
        <summary type="html"><![CDATA[Recently, graph neural networks (GNNs) have achieved remarkable performances
for quantum mechanical problems. However, a graph convolution can only cover a
localized region, and cannot capture long-range interactions of atoms. This
behavior is contrary to theoretical interatomic potentials, which is a
fundamental limitation of the spatial based GNNs. In this work, we propose a
novel attention-based framework for molecular property prediction tasks. We
represent a molecular conformation as a discrete atomic sequence combined by
atom-atom distance attributes, named Geometry-aware Transformer (GeoT). In
particular, we adopt a Transformer architecture, which has been widely used for
sequential data. Our proposed model trains sequential representations of
molecular graphs based on globally constructed attentions, maintaining all
spatial arrangements of atom pairs. Our method does not suffer from cost
intensive computations, such as angle calculations. The experimental results on
several public benchmarks and visualization maps verified that keeping the
long-range interatomic attributes can significantly improve the model
predictability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_B/0/1/0/all/0/1"&gt;Bumju Kwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1"&gt;Jeonghee Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1"&gt;Byunghan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subgroup Generalization and Fairness of Graph Neural Networks. (arXiv:2106.15535v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15535</id>
        <link href="http://arxiv.org/abs/2106.15535"/>
        <updated>2021-06-30T02:01:03.467Z</updated>
        <summary type="html"><![CDATA[Despite enormous successful applications of graph neural networks (GNNs)
recently, theoretical understandings of their generalization ability,
especially for node-level tasks where data are not independent and
identically-distributed (IID), have been sparse. The theoretical investigation
of the generalization performance is beneficial for understanding fundamental
issues (such as fairness) of GNN models and designing better learning methods.
In this paper, we present a novel PAC-Bayesian analysis for GNNs under a
non-IID semi-supervised learning setup. Moreover, we analyze the generalization
performances on different subgroups of unlabeled nodes, which allows us to
further study an accuracy-(dis)parity-style (un)fairness of GNNs from a
theoretical perspective. Under reasonable assumptions, we demonstrate that the
distance between a test subgroup and the training set can be a key factor
affecting the GNN performance on that subgroup, which calls special attention
to the training node selection for fair learning. Experiments across multiple
GNN models and datasets support our theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Junwei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1"&gt;Qiaozhu Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Cattle and Elk in the Wild from Space. (arXiv:2106.15448v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15448</id>
        <link href="http://arxiv.org/abs/2106.15448"/>
        <updated>2021-06-30T02:01:03.458Z</updated>
        <summary type="html"><![CDATA[Localizing and counting large ungulates -- hoofed mammals like cows and elk
-- in very high-resolution satellite imagery is an important task for
supporting ecological studies. Prior work has shown that this is feasible with
deep learning based methods and sub-meter multi-spectral satellite imagery. We
extend this line of work by proposing a baseline method, CowNet, that
simultaneously estimates the number of animals in an image (counts), as well as
predicts their location at a pixel level (localizes). We also propose an
methodology for evaluating such models on counting and localization tasks
across large scenes that takes the uncertainty of noisy labels and the
information needed by stakeholders in ecological monitoring tasks into account.
Finally, we benchmark our baseline method with state of the art vision methods
for counting objects in scenes. We specifically test the temporal
generalization of the resulting models over a large landscape in Point Reyes
Seashore, CA. We find that the LC-FCN model performs the best and achieves an
average precision between 0.56 and 0.61 and an average recall between 0.78 and
0.92 over three held out test scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1"&gt;Caleb Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1"&gt;Anthony Ortiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughey_L/0/1/0/all/0/1"&gt;Lacey Hughey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stabach_J/0/1/0/all/0/1"&gt;Jared A. Stabach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan M. Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning latent causal graphs via mixture oracles. (arXiv:2106.15563v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15563</id>
        <link href="http://arxiv.org/abs/2106.15563"/>
        <updated>2021-06-30T02:01:03.449Z</updated>
        <summary type="html"><![CDATA[We study the problem of reconstructing a causal graphical model from data in
the presence of latent variables. The main problem of interest is recovering
the causal structure over the latent variables while allowing for general,
potentially nonlinear dependence between the variables. In many practical
problems, the dependence between raw observations (e.g. pixels in an image) is
much less relevant than the dependence between certain high-level, latent
features (e.g. concepts or objects), and this is the setting of interest. We
provide conditions under which both the latent representations and the
underlying latent causal model are identifiable by a reduction to a mixture
oracle. The proof is constructive, and leads to several algorithms for
explicitly reconstructing the full graphical model. We discuss efficient
algorithms and provide experiments illustrating the algorithms in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kivva_B/0/1/0/all/0/1"&gt;Bohdan Kivva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajendran_G/0/1/0/all/0/1"&gt;Goutham Rajendran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1"&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1"&gt;Bryon Aragam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Evolutionary Approach for the Design of Composite Machine Learning Pipelines. (arXiv:2106.15397v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15397</id>
        <link href="http://arxiv.org/abs/2106.15397"/>
        <updated>2021-06-30T02:01:03.435Z</updated>
        <summary type="html"><![CDATA[The effectiveness of the machine learning methods for real-world tasks
depends on the proper structure of the modeling pipeline. The proposed approach
is aimed to automate the design of composite machine learning pipelines, which
is equivalent to computation workflows that consist of models and data
operations. The approach combines key ideas of both automated machine learning
and workflow management systems. It designs the pipelines with a customizable
graph-based structure, analyzes the obtained results, and reproduces them. The
evolutionary approach is used for the flexible identification of pipeline
structure. The additional algorithms for sensitivity analysis, atomization, and
hyperparameter tuning are implemented to improve the effectiveness of the
approach. Also, the software implementation on this approach is presented as an
open-source framework. The set of experiments is conducted for the different
datasets and tasks (classification, regression, time series forecasting). The
obtained results confirm the correctness and effectiveness of the proposed
approach in the comparison with the state-of-the-art competitors and baseline
solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikitin_N/0/1/0/all/0/1"&gt;Nikolay O. Nikitin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vychuzhanin_P/0/1/0/all/0/1"&gt;Pavel Vychuzhanin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarafanov_M/0/1/0/all/0/1"&gt;Mikhail Sarafanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polonskaia_I/0/1/0/all/0/1"&gt;Iana S. Polonskaia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Revin_I/0/1/0/all/0/1"&gt;Ilia Revin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barabanova_I/0/1/0/all/0/1"&gt;Irina V. Barabanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maximov_G/0/1/0/all/0/1"&gt;Gleb Maximov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1"&gt;Anna V. Kalyuzhnaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boukhanovsky_A/0/1/0/all/0/1"&gt;Alexander Boukhanovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Realistic Data Generation Framework leveraging Deep Learning-based Human Digitization. (arXiv:2106.15409v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15409</id>
        <link href="http://arxiv.org/abs/2106.15409"/>
        <updated>2021-06-30T02:01:03.430Z</updated>
        <summary type="html"><![CDATA[The performance of supervised deep learning algorithms depends significantly
on the scale, quality and diversity of the data used for their training.
Collecting and manually annotating large amount of data can be both
time-consuming and costly tasks to perform. In the case of tasks related to
visual human-centric perception, the collection and distribution of such data
may also face restrictions due to legislation regarding privacy. In addition,
the design and testing of complex systems, e.g., robots, which often employ
deep learning-based perception models, may face severe difficulties as even
state-of-the-art methods trained on real and large-scale datasets cannot always
perform adequately as they have not adapted to the visual differences between
the virtual and the real world data. As an attempt to tackle and mitigate the
effect of these issues, we present a method that automatically generates
realistic synthetic data with annotations for a) person detection, b) face
recognition, and c) human pose estimation. The proposed method takes as input
real background images and populates them with human figures in various poses.
Instead of using hand-made 3D human models, we propose the use of models
generated through deep learning methods, further reducing the dataset creation
costs, while maintaining a high level of realism. In addition, we provide
open-source and easy to use tools that implement the proposed pipeline,
allowing for generating highly-realistic synthetic datasets for a variety of
tasks. A benchmarking and evaluation in the corresponding tasks shows that
synthetic data can be effectively used as a supplement to real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Symeonidis_C/0/1/0/all/0/1"&gt;C. Symeonidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nousi_P/0/1/0/all/0/1"&gt;P. Nousi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tosidis_P/0/1/0/all/0/1"&gt;P. Tosidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsampazis_K/0/1/0/all/0/1"&gt;K. Tsampazis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1"&gt;N. Passalis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1"&gt;A. Tefas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1"&gt;N. Nikolaidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble. (arXiv:2106.15412v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15412</id>
        <link href="http://arxiv.org/abs/2106.15412"/>
        <updated>2021-06-30T02:01:03.412Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization is a promising methodology for analog circuit
synthesis. However, the sequential nature of the Bayesian optimization
framework significantly limits its ability to fully utilize real-world
computational resources. In this paper, we propose an efficient parallelizable
Bayesian optimization algorithm via Multi-objective ACquisition function
Ensemble (MACE) to further accelerate the optimization procedure. By sampling
query points from the Pareto front of the probability of improvement (PI),
expected improvement (EI) and lower confidence bound (LCB), we combine the
benefits of state-of-the-art acquisition functions to achieve a delicate
tradeoff between exploration and exploitation for the unconstrained
optimization problem. Based on this batch design, we further adjust the
algorithm for the constrained optimization problem. By dividing the
optimization procedure into two stages and first focusing on finding an initial
feasible point, we manage to gain more information about the valid region and
can better avoid sampling around the infeasible area. After achieving the first
feasible point, we favor the feasible region by adopting a specially designed
penalization term to the acquisition function ensemble. The experimental
results quantitatively demonstrate that our proposed algorithm can reduce the
overall simulation time by up to 74 times compared to differential evolution
(DE) for the unconstrained optimization problem when the batch size is 15. For
the constrained optimization problem, our proposed algorithm can speed up the
optimization process by up to 15 times compared to the weighted expected
improvement based Bayesian optimization (WEIBO) approach, when the batch size
is 15.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuhan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Changhao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xuan Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MAML is a Noisy Contrastive Learner. (arXiv:2106.15367v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15367</id>
        <link href="http://arxiv.org/abs/2106.15367"/>
        <updated>2021-06-30T02:01:03.406Z</updated>
        <summary type="html"><![CDATA[Model-agnostic meta-learning (MAML) is one of the most popular and
widely-adopted meta-learning algorithms nowadays, which achieves remarkable
success in various learning problems. Yet, with the unique design of nested
inner-loop and outer-loop updates which respectively govern the task-specific
and meta-model-centric learning, the underlying learning objective of MAML
still remains implicit and thus impedes a more straightforward understanding of
it. In this paper, we provide a new perspective to the working mechanism of
MAML and discover that: MAML is analogous to a meta-learner using a supervised
contrastive objective function, where the query features are pulled towards the
support features of the same class and against those of different classes, in
which such contrastiveness is experimentally verified via an analysis based on
the cosine similarity. Moreover, our analysis reveals that the vanilla MAML
algorithm has an undesirable interference term originating from the random
initialization and the cross-task interaction. We therefore propose a simple
but effective technique, zeroing trick, to alleviate such interference, where
the extensive experiments are then conducted on both miniImagenet and Omniglot
datasets to demonstrate the consistent improvement brought by our proposed
technique thus well validating its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kao_C/0/1/0/all/0/1"&gt;Chia-Hsiang Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1"&gt;Wei-Chen Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curious Explorer: a provable exploration strategy in Policy Learning. (arXiv:2106.15503v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15503</id>
        <link href="http://arxiv.org/abs/2106.15503"/>
        <updated>2021-06-30T02:01:03.400Z</updated>
        <summary type="html"><![CDATA[Having access to an exploring restart distribution (the so-called wide
coverage assumption) is critical with policy gradient methods. This is due to
the fact that, while the objective function is insensitive to updates in
unlikely states, the agent may still need improvements in those states in order
to reach a nearly optimal payoff. For this reason, wide coverage is used in
some form when analyzing theoretical properties of practical policy gradient
methods. However, this assumption can be unfeasible in certain environments,
for instance when learning is online, or when restarts are possible only from a
fixed initial state. In these cases, classical policy gradient algorithms can
have very poor convergence properties and sample efficiency. In this paper, we
develop Curious Explorer, a novel and simple iterative state space exploration
strategy that can be used with any starting distribution $\rho$. Curious
Explorer starts from $\rho$, then using intrinsic rewards assigned to the set
of poorly visited states produces a sequence of policies, each one more
exploratory than the previous one in an informed way, and finally outputs a
restart model $\mu$ based on the state visitation distribution of the
exploratory policies. Curious Explorer is provable, in the sense that we
provide theoretical upper bounds on how often an optimal policy visits poorly
visited states. These bounds can be used to prove PAC convergence and sample
efficiency results when a PAC optimizer is plugged in Curious Explorer. This
allows to achieve global convergence and sample efficiency results without any
coverage assumption for REINFORCE, and potentially for any other policy
gradient method ensuring PAC convergence with wide coverage. Finally, we plug
(the output of) Curious Explorer into REINFORCE and TRPO, and show empirically
that it can improve performance in MDPs with challenging exploration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miani_M/0/1/0/all/0/1"&gt;Marco Miani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parton_M/0/1/0/all/0/1"&gt;Maurizio Parton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romito_M/0/1/0/all/0/1"&gt;Marco Romito&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes. (arXiv:2106.15380v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15380</id>
        <link href="http://arxiv.org/abs/2106.15380"/>
        <updated>2021-06-30T02:01:03.384Z</updated>
        <summary type="html"><![CDATA[In this work we present a novel approach to hierarchical reinforcement
learning for linearly-solvable Markov decision processes. Our approach assumes
that the state space is partitioned, and the subtasks consist in moving between
the partitions. We represent value functions on several levels of abstraction,
and use the compositionality of subtasks to estimate the optimal values of the
states in each partition. The policy is implicitly defined on these optimal
value estimates, rather than being decomposed among the subtasks. As a
consequence, our approach can learn the globally optimal policy, and does not
suffer from the non-stationarity of high-level decisions. If several partitions
have equivalent dynamics, the subtasks of those partitions can be shared. If
the set of boundary states is smaller than the entire state space, our approach
can have significantly smaller sample complexity than that of a flat learner,
and we validate this empirically in several experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Infante_G/0/1/0/all/0/1"&gt;Guillermo Infante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonsso_A/0/1/0/all/0/1"&gt;Anders Jonsso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1"&gt;Vicen&amp;#xe7; G&amp;#xf3;mez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SE-MD: A Single-encoder multiple-decoder deep network for point cloud generation from 2D images. (arXiv:2106.15325v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15325</id>
        <link href="http://arxiv.org/abs/2106.15325"/>
        <updated>2021-06-30T02:01:03.377Z</updated>
        <summary type="html"><![CDATA[3D model generation from single 2D RGB images is a challenging and actively
researched computer vision task. Various techniques using conventional network
architectures have been proposed for the same. However, the body of research
work is limited and there are various issues like using inefficient 3D
representation formats, weak 3D model generation backbones, inability to
generate dense point clouds, dependence of post-processing for generation of
dense point clouds, and dependence on silhouettes in RGB images. In this paper,
a novel 2D RGB image to point cloud conversion technique is proposed, which
improves the state of art in the field due to its efficient, robust and simple
model by using the concept of parallelization in network architecture. It not
only uses the efficient and rich 3D representation of point clouds, but also
uses a novel and robust point cloud generation backbone in order to address the
prevalent issues. This involves using a single-encoder multiple-decoder deep
network architecture wherein each decoder generates certain fixed viewpoints.
This is followed by fusing all the viewpoints to generate a dense point cloud.
Various experiments are conducted on the technique and its performance is
compared with those of other state of the art techniques and impressive gains
in performance are demonstrated. Code is available at
https://github.com/mueedhafiz1982/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hafiz_A/0/1/0/all/0/1"&gt;Abdul Mueed Hafiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1"&gt;Rouf Ul Alam Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parah_S/0/1/0/all/0/1"&gt;Shabir Ahmad Parah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassaballah_M/0/1/0/all/0/1"&gt;M. Hassaballah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Gaussian Processes for Data-Driven Design using Big Data with Categorical Factors. (arXiv:2106.15356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15356</id>
        <link href="http://arxiv.org/abs/2106.15356"/>
        <updated>2021-06-30T02:01:03.372Z</updated>
        <summary type="html"><![CDATA[Scientific and engineering problems often require the use of artificial
intelligence to aid understanding and the search for promising designs. While
Gaussian processes (GP) stand out as easy-to-use and interpretable learners,
they have difficulties in accommodating big datasets, categorical inputs, and
multiple responses, which has become a common challenge for a growing number of
data-driven design applications. In this paper, we propose a GP model that
utilizes latent variables and functions obtained through variational inference
to address the aforementioned challenges simultaneously. The method is built
upon the latent variable Gaussian process (LVGP) model where categorical
factors are mapped into a continuous latent space to enable GP modeling of
mixed-variable datasets. By extending variational inference to LVGP models, the
large training dataset is replaced by a small set of inducing points to address
the scalability issue. Output response vectors are represented by a linear
combination of independent latent functions, forming a flexible kernel
structure to handle multiple responses that might have distinct behaviors.
Comparative studies demonstrate that the proposed method scales well for large
datasets with over 10^4 data points, while outperforming state-of-the-art
machine learning methods without requiring much hyperparameter tuning. In
addition, an interpretable latent space is obtained to draw insights into the
effect of categorical factors, such as those associated with building blocks of
architectures and element choices in metamaterial and materials design. Our
approach is demonstrated for machine learning of ternary oxide materials and
topology optimization of a multiscale compliant mechanism with aperiodic
microstructures and multiple materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1"&gt;Akshay Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yerramilli_S/0/1/0/all/0/1"&gt;Suraj Yerramilli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apley_D/0/1/0/all/0/1"&gt;Daniel Apley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Ping Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Privacy for Credit Risk Model. (arXiv:2106.15343v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.15343</id>
        <link href="http://arxiv.org/abs/2106.15343"/>
        <updated>2021-06-30T02:01:03.366Z</updated>
        <summary type="html"><![CDATA[The use of machine learning algorithms to model user behavior and drive
business decisions has become increasingly commonplace, specifically providing
intelligent recommendations to automated decision making. This has led to an
increase in the use of customers personal data to analyze customer behavior and
predict their interests in a companys products. Increased use of this customer
personal data can lead to better models but also to the potential of customer
data being leaked, reverse engineered, and mishandled. In this paper, we assess
differential privacy as a solution to address these privacy problems by
building privacy protections into the data engineering and model training
stages of predictive model development. Our interest is a pragmatic
implementation in an operational environment, which necessitates a general
purpose differentially private modeling framework, and we evaluate one such
tool from LeapYear as applied to the Credit Risk modeling domain. Credit Risk
Model is a major modeling methodology in banking and finance where user data is
analyzed to determine the total Expected Loss to the bank. We examine the
application of differential privacy on the credit risk model and evaluate the
performance of a Differentially Private Model with a Non Differentially Private
Model. Credit Risk Model is a major modeling methodology in banking and finance
where users data is analyzed to determine the total Expected Loss to the bank.
In this paper, we explore the application of differential privacy on the credit
risk model and evaluate the performance of a Non Differentially Private Model
with Differentially Private Model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maniar_T/0/1/0/all/0/1"&gt;Tabish Maniar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akkinepally_A/0/1/0/all/0/1"&gt;Alekhya Akkinepally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Anantha Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Learning of Portrait Intrinsic Decomposition and Relighting. (arXiv:2106.15305v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15305</id>
        <link href="http://arxiv.org/abs/2106.15305"/>
        <updated>2021-06-30T02:01:03.361Z</updated>
        <summary type="html"><![CDATA[Inverse rendering is the problem of decomposing an image into its intrinsic
components, i.e. albedo, normal and lighting. To solve this ill-posed problem
from single image, state-of-the-art methods in shape from shading mostly resort
to supervised training on all the components on either synthetic or real
datasets. Here, we propose a new self-supervised training paradigm that 1)
reduces the need for full supervision on the decomposition task and 2) takes
into account the relighting task. We introduce new self-supervised loss terms
that leverage the consistencies between multi-lit images (images of the same
scene under different illuminations). Our approach is applicable to multi-lit
datasets. We apply our training approach in two settings: 1) train on a mixture
of synthetic and real data, 2) train on real datasets with limited supervision.
We show-case the effectiveness of our training paradigm on both intrinsic
decomposition and relighting and demonstrate how the model struggles in both
tasks without the self-supervised loss terms in limited supervision settings.
We provide results of comprehensive experiments on SfSNet, CelebA and Photoface
datasets and verify the performance of our approach on images in the wild.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zehni_M/0/1/0/all/0/1"&gt;Mona Zehni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Shaona Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridhar_K/0/1/0/all/0/1"&gt;Krishna Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1"&gt;Sethu Raman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpreadsheetCoder: Formula Prediction from Semi-structured Context. (arXiv:2106.15339v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.15339</id>
        <link href="http://arxiv.org/abs/2106.15339"/>
        <updated>2021-06-30T02:01:03.346Z</updated>
        <summary type="html"><![CDATA[Spreadsheet formula prediction has been an important program synthesis
problem with many real-world applications. Previous works typically utilize
input-output examples as the specification for spreadsheet formula synthesis,
where each input-output pair simulates a separate row in the spreadsheet.
However, this formulation does not fully capture the rich context in real-world
spreadsheets. First, spreadsheet data entries are organized as tables, thus
rows and columns are not necessarily independent from each other. In addition,
many spreadsheet tables include headers, which provide high-level descriptions
of the cell data. However, previous synthesis approaches do not consider
headers as part of the specification. In this work, we present the first
approach for synthesizing spreadsheet formulas from tabular context, which
includes both headers and semi-structured tabular data. In particular, we
propose SpreadsheetCoder, a BERT-based model architecture to represent the
tabular context in both row-based and column-based formats. We train our model
on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder
achieves top-1 prediction accuracy of 42.51%, which is a considerable
improvement over baselines that do not employ rich tabular context. Compared to
the rule-based system, SpreadsheetCoder assists 82% more users in composing
formulas on Google Sheets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinyun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maniatis_P/0/1/0/all/0/1"&gt;Petros Maniatis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rishabh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1"&gt;Charles Sutton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hanjun Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Max Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Denny Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning complex dependency structure of gene regulatory networks from high dimensional micro-array data with Gaussian Bayesian networks. (arXiv:2106.15365v1 [q-bio.MN])]]></title>
        <id>http://arxiv.org/abs/2106.15365</id>
        <link href="http://arxiv.org/abs/2106.15365"/>
        <updated>2021-06-30T02:01:03.340Z</updated>
        <summary type="html"><![CDATA[Gene expression datasets consist of thousand of genes with relatively small
samplesizes (i.e. are large-$p$-small-$n$). Moreover, dependencies of various
orders co-exist in the datasets. In the Undirected probabilistic Graphical
Model (UGM) framework the Glasso algorithm has been proposed to deal with high
dimensional micro-array datasets forcing sparsity. Also, modifications of the
default Glasso algorithm are developed to overcome the problem of complex
interaction structure. In this work we advocate the use of a simple score-based
Hill Climbing algorithm (HC) that learns Gaussian Bayesian Networks (BNs)
leaning on Directed Acyclic Graphs (DAGs). We compare HC with Glasso and its
modifications in the UGM framework on their capability to reconstruct GRNs from
micro-array data belonging to the Escherichia Coli genome. We benefit from the
analytical properties of the Joint Probability Density (JPD) function on which
both directed and undirected PGMs build to convert DAGs to UGMs.

We conclude that dependencies in complex data are learned best by the HC
algorithm, presenting them most accurately and efficiently, simultaneously
modelling strong local and weaker but significant global connections coexisting
in the gene expression dataset. The HC algorithm adapts intrinsically to the
complex dependency structure of the dataset, without forcing a specific
structure in advance. On the contrary, Glasso and modifications model
unnecessary dependencies at the expense of the probabilistic information in the
network and of a structural bias in the JPD function that can only be relieved
including many parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Graafland_C/0/1/0/all/0/1"&gt;Catharina Elisabeth Graafland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gutierrez_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Manuel Guti&amp;#xe9;rrez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRILL-- Deep Reinforcement Learning for Refinement Operators in $\mathcal{ALC}$. (arXiv:2106.15373v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.15373</id>
        <link href="http://arxiv.org/abs/2106.15373"/>
        <updated>2021-06-30T02:01:03.334Z</updated>
        <summary type="html"><![CDATA[Approaches based on refinement operators have been successfully applied to
class expression learning on RDF knowledge graphs. These approaches often need
to explore a large number of concepts to find adequate hypotheses. This need
arguably stems from current approaches relying on myopic heuristic functions to
guide their search through an infinite concept space. In turn, deep
reinforcement learning provides effective means to address myopia by estimating
how much discounted cumulated future reward states promise. In this work, we
leverage deep reinforcement learning to accelerate the learning of concepts in
$\mathcal{ALC}$ by proposing DRILL -- a novel class expression learning
approach that uses a convolutional deep Q-learning model to steer its search.
By virtue of its architecture, DRILL is able to compute the expected discounted
cumulated future reward of more than $10^3$ class expressions in a second on
standard hardware. We evaluate DRILL on four benchmark datasets against
state-of-the-art approaches. Our results suggest that DRILL converges to goal
states at least 2.7$\times$ faster than state-of-the-art models on all
benchmark datasets. We provide an open-source implementation of our approach,
including training and evaluation scripts as well as pre-trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1"&gt;Caglar Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1"&gt;Axel-Cyrille Ngonga Ngomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for Intrusion Detection in IoT Security: A Hybrid Ensemble Approach. (arXiv:2106.15349v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.15349</id>
        <link href="http://arxiv.org/abs/2106.15349"/>
        <updated>2021-06-30T02:01:03.329Z</updated>
        <summary type="html"><![CDATA[Critical role of Internet of Things (IoT) in various domains like smart city,
healthcare, supply chain and transportation has made them the target of
malicious attacks. Past works in this area focused on centralized Intrusion
Detection System (IDS), assuming the existence of a central entity to perform
data analysis and identify threats. However, such IDS may not always be
feasible, mainly due to spread of data across multiple sources and gathering at
central node can be costly. Also, the earlier works primarily focused on
improving True Positive Rate (TPR) and ignored the False Positive Rate (FPR),
which is also essential to avoid unnecessary downtime of the systems. In this
paper, we first present an architecture for IDS based on hybrid ensemble model,
named PHEC, which gives improved performance compared to state-of-the-art
architectures. We then adapt this model to a federated learning framework that
performs local training and aggregates only the model parameters. Next, we
propose Noise-Tolerant PHEC in centralized and federated settings to address
the label-noise problem. The proposed idea uses classifiers using weighted
convex surrogate loss functions. Natural robustness of KNN classifier towards
noisy data is also used in the proposed architecture. Experimental results on
four benchmark datasets drawn from various security attacks show that our model
achieves high TPR while keeping FPR low on noisy and clean data. Further, they
also demonstrate that the hybrid ensemble models achieve performance in
federated settings close to that of the centralized settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Sayan Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanawal_M/0/1/0/all/0/1"&gt;Manjesh K. Hanawal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where is the disease? Semi-supervised pseudo-normality synthesis from an abnormal image. (arXiv:2106.15345v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15345</id>
        <link href="http://arxiv.org/abs/2106.15345"/>
        <updated>2021-06-30T02:01:03.324Z</updated>
        <summary type="html"><![CDATA[Pseudo-normality synthesis, which computationally generates a pseudo-normal
image from an abnormal one (e.g., with lesions), is critical in many
perspectives, from lesion detection, data augmentation to clinical surgery
suggestion. However, it is challenging to generate high-quality pseudo-normal
images in the absence of the lesion information. Thus, expensive lesion
segmentation data have been introduced to provide lesion information for the
generative models and improve the quality of the synthetic images. In this
paper, we aim to alleviate the need of a large amount of lesion segmentation
data when generating pseudo-normal images. We propose a Semi-supervised Medical
Image generative LEarning network (SMILE) which not only utilizes limited
medical images with segmentation masks, but also leverages massive medical
images without segmentation masks to generate realistic pseudo-normal images.
Extensive experiments show that our model outperforms the best state-of-the-art
model by up to 6% for data augmentation task and 3% in generating high-quality
images. Moreover, the proposed semi-supervised learning achieves comparable
medical image synthesis quality with supervised learning model, using only 50
of segmentation data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuanqi Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1"&gt;Quan Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1"&gt;Hu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;S. Kevin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Inpainting Using Wasserstein Generative Adversarial Imputation Network. (arXiv:2106.15341v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15341</id>
        <link href="http://arxiv.org/abs/2106.15341"/>
        <updated>2021-06-30T02:01:03.309Z</updated>
        <summary type="html"><![CDATA[Image inpainting is one of the important tasks in computer vision which
focuses on the reconstruction of missing regions in an image. The aim of this
paper is to introduce an image inpainting model based on Wasserstein Generative
Adversarial Imputation Network. The generator network of the model uses
building blocks of convolutional layers with different dilation rates, together
with skip connections that help the model reproduce fine details of the output.
This combination yields a universal imputation model that is able to handle
various scenarios of missingness with sufficient quality. To show this
experimentally, the model is simultaneously trained to deal with three
scenarios given by missing pixels at random, missing various smaller square
regions, and one missing square placed in the center of the image. It turns out
that our model achieves high-quality inpainting results on all scenarios.
Performance is evaluated using peak signal-to-noise ratio and structural
similarity index on two real-world benchmark datasets, CelebA faces and Paris
StreetView. The results of our model are compared to biharmonic imputation and
to some of the other state-of-the-art image inpainting methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasata_D/0/1/0/all/0/1"&gt;Daniel Va&amp;#x161;ata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halama_T/0/1/0/all/0/1"&gt;Tom&amp;#xe1;&amp;#x161; Halama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedjungova_M/0/1/0/all/0/1"&gt;Magda Friedjungov&amp;#xe1;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Budget Scheduling. (arXiv:2106.15335v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.15335</id>
        <link href="http://arxiv.org/abs/2106.15335"/>
        <updated>2021-06-30T02:01:03.304Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) models trained on personal data have been shown to leak
information about users. Differential privacy (DP) enables model training with
a guaranteed bound on this leakage. Each new model trained with DP increases
the bound on data leakage and can be seen as consuming part of a global privacy
budget that should not be exceeded. This budget is a scarce resource that must
be carefully managed to maximize the number of successfully trained models.

We describe PrivateKube, an extension to the popular Kubernetes datacenter
orchestrator that adds privacy as a new type of resource to be managed
alongside other traditional compute resources, such as CPU, GPU, and memory.
The abstractions we design for the privacy resource mirror those defined by
Kubernetes for traditional resources, but there are also major differences. For
example, traditional compute resources are replenishable while privacy is not:
a CPU can be regained after a model finishes execution while privacy budget
cannot. This distinction forces a re-design of the scheduler. We present DPF
(Dominant Private Block Fairness) -- a variant of the popular Dominant Resource
Fairness (DRF) algorithm -- that is geared toward the non-replenishable privacy
resource but enjoys similar theoretical properties as DRF.

We evaluate PrivateKube and DPF on microbenchmarks and an ML workload on
Amazon Reviews data. Compared to existing baselines, DPF allows training more
models under the same global privacy guarantee. This is especially true for DPF
over R\'enyi DP, a highly composable form of DP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1"&gt;Mingen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tholoniat_P/0/1/0/all/0/1"&gt;Pierre Tholoniat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cidon_A/0/1/0/all/0/1"&gt;Asaf Cidon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geambasu_R/0/1/0/all/0/1"&gt;Roxana Geambasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lecuyer_M/0/1/0/all/0/1"&gt;Mathias L&amp;#xe9;cuyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Attention for Interactive Segmentation. (arXiv:2106.15338v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15338</id>
        <link href="http://arxiv.org/abs/2106.15338"/>
        <updated>2021-06-30T02:01:03.298Z</updated>
        <summary type="html"><![CDATA[We provide a probabilistic interpretation of attention and show that the
standard dot-product attention in transformers is a special case of Maximum A
Posteriori (MAP) inference. The proposed approach suggests the use of
Expectation Maximization algorithms for online adaptation of key and value
model parameters. This approach is useful for cases in which external agents,
e.g., annotators, provide inference-time information about the correct values
of some tokens, e.g, the semantic category of some pixels, and we need for this
new information to propagate to other tokens in a principled manner. We
illustrate the approach on an interactive semantic segmentation task in which
annotators and models collaborate online to improve annotation efficiency.
Using standard benchmarks, we observe that key adaptation boosts model
performance ($\sim10\%$ mIoU) in the low feedback regime and value propagation
improves model responsiveness in the high feedback regime. A PyTorch layer
implementation of our probabilistic attention model will be made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gabbur_P/0/1/0/all/0/1"&gt;Prasad Gabbur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilkhu_M/0/1/0/all/0/1"&gt;Manjot Bilkhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Movellan_J/0/1/0/all/0/1"&gt;Javier Movellan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Sample-Optimal Compressive Phase Retrieval with Sparse and Generative Priors. (arXiv:2106.15358v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15358</id>
        <link href="http://arxiv.org/abs/2106.15358"/>
        <updated>2021-06-30T02:01:03.291Z</updated>
        <summary type="html"><![CDATA[Compressive phase retrieval is a popular variant of the standard compressive
sensing problem, in which the measurements only contain magnitude information.
In this paper, motivated by recent advances in deep generative models, we
provide recovery guarantees with order-optimal sample complexity bounds for
phase retrieval with generative priors. We first show that when using i.i.d.
Gaussian measurements and an $L$-Lipschitz continuous generative model with
bounded $k$-dimensional inputs, roughly $O(k \log L)$ samples suffice to
guarantee that the signal is close to any vector that minimizes an
amplitude-based empirical loss function. Attaining this sample complexity with
a practical algorithm remains a difficult challenge, and a popular spectral
initialization method has been observed to pose a major bottleneck. To
partially address this, we further show that roughly $O(k \log L)$ samples
ensure sufficient closeness between the signal and any {\em globally optimal}
solution to an optimization problem designed for spectral initialization
(though finding such a solution may still be challenging). We adapt this result
to sparse phase retrieval, and show that $O(s \log n)$ samples are sufficient
for a similar guarantee when the underlying signal is $s$-sparse and
$n$-dimensional, matching an information-theoretic lower bound. While our
guarantees do not directly correspond to a practical algorithm, we propose a
practical spectral initialization method motivated by our findings, and
experimentally observe significant performance gains over various existing
spectral initialization methods of sparse phase retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhaoqiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Subhroshekhar Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scarlett_J/0/1/0/all/0/1"&gt;Jonathan Scarlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Framework for Spectral Dimensionality Reduction, Maximum Variance Unfolding, and Kernel Learning By Semidefinite Programming: Tutorial and Survey. (arXiv:2106.15379v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15379</id>
        <link href="http://arxiv.org/abs/2106.15379"/>
        <updated>2021-06-30T02:01:03.281Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper on unification of spectral dimensionality
reduction methods, kernel learning by Semidefinite Programming (SDP), Maximum
Variance Unfolding (MVU) or Semidefinite Embedding (SDE), and its variants. We
first explain how the spectral dimensionality reduction methods can be unified
as kernel Principal Component Analysis (PCA) with different kernels. This
unification can be interpreted as eigenfunction learning or representation of
kernel in terms of distance matrix. Then, since the spectral methods are
unified as kernel PCA, we say let us learn the best kernel for unfolding the
manifold of data to its maximum variance. We first briefly introduce kernel
learning by SDP for the transduction task. Then, we explain MVU in detail.
Various versions of supervised MVU using nearest neighbors graph, by class-wise
unfolding, by Fisher criterion, and by colored MVU are explained. We also
explain out-of-sample extension of MVU using eigenfunctions and kernel mapping.
Finally, we introduce other variants of MVU including action respecting
embedding, relaxed MVU, and landmark MVU for big data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15498</id>
        <link href="http://arxiv.org/abs/2106.15498"/>
        <updated>2021-06-30T02:01:03.265Z</updated>
        <summary type="html"><![CDATA[Social media offer plenty of information to perform market research in order
to meet the requirements of customers. One way how this research is conducted
is that a domain expert gathers and categorizes user-generated content into a
complex and fine-grained class structure. In many of such cases, little data
meets complex annotations. It is not yet fully understood how this can be
leveraged successfully for classification. We examine the classification
accuracy of expert labels when used with a) many fine-grained classes and b)
few abstract classes. For scenario b) we compare abstract class labels given by
the domain expert as baseline and by automatic hierarchical clustering. We
compare this to another baseline where the entire class structure is given by a
completely unsupervised clustering approach. By doing so, this work can serve
as an example of how complex expert annotations are potentially beneficial and
can be utilized in the most optimal way for opinion mining in highly specific
domains. By exploring across a range of techniques and experiments, we find
that automated class abstraction approaches in particular the unsupervised
approach performs remarkably well against domain expert baseline on text
classification tasks. This has the potential to inspire opinion mining
applications in order to support market researchers in practice and to inspire
fine-grained automated content analysis on a large scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1"&gt;Gerhard Hagerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1"&gt;Wenbin Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1"&gt;Hannah Danner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1"&gt;Georg Groh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security Analysis of Camera-LiDAR Semantic-Level Fusion Against Black-Box Attacks on Autonomous Vehicles. (arXiv:2106.07098v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07098</id>
        <link href="http://arxiv.org/abs/2106.07098"/>
        <updated>2021-06-30T02:01:02.870Z</updated>
        <summary type="html"><![CDATA[To enable safe and reliable decision-making, autonomous vehicles (AVs) feed
sensor data to perception algorithms to understand the environment. Sensor
fusion, and particularly semantic fusion, with multi-frame tracking is becoming
increasingly popular for detecting 3D objects. Recently, it was shown that
LiDAR-based perception built on deep neural networks is vulnerable to LiDAR
spoofing attacks. Thus, in this work, we perform the first analysis of
camera-LiDAR fusion under spoofing attacks and the first security analysis of
semantic fusion in any AV context. We find first that fusion is more successful
than existing defenses at guarding against naive spoofing. However, we then
define the frustum attack as a new class of attacks on AVs and find that
semantic camera-LiDAR fusion exhibits widespread vulnerability to frustum
attacks with between 70% and 90% success against target models. Importantly,
the attacker needs less than 20 random spoof points on average for successful
attacks - an order of magnitude less than established maximum capability.
Finally, we are the first to analyze the longitudinal impact of perception
attacks by showing the impact of multi-frame attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hallyburton_R/0/1/0/all/0/1"&gt;R. Spencer Hallyburton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yupei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1"&gt;Miroslav Pajic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Prediction and Network Estimation Using the Monotone Single Index Multi-variate Autoregressive Model. (arXiv:2106.14630v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14630</id>
        <link href="http://arxiv.org/abs/2106.14630"/>
        <updated>2021-06-30T02:01:02.865Z</updated>
        <summary type="html"><![CDATA[Network estimation from multi-variate point process or time series data is a
problem of fundamental importance. Prior work has focused on parametric
approaches that require a known parametric model, which makes estimation
procedures less robust to model mis-specification, non-linearities and
heterogeneities. In this paper, we develop a semi-parametric approach based on
the monotone single-index multi-variate autoregressive model (SIMAM) which
addresses these challenges. We provide theoretical guarantees for dependent
data and an alternating projected gradient descent algorithm. Significantly we
do not explicitly assume mixing conditions on the process (although we do
require conditions analogous to restricted strong convexity) and we achieve
rates of the form $O(T^{-\frac{1}{3}} \sqrt{s\log(TM)})$ (optimal in the
independent design case) where $s$ is the threshold for the maximum in-degree
of the network that indicates the sparsity level, $M$ is the number of actors
and $T$ is the number of time points. In addition, we demonstrate the superior
performance both on simulated data and two real data examples where our SIMAM
approach out-performs state-of-the-art parametric methods both in terms of
prediction and network estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Raskutti_G/0/1/0/all/0/1"&gt;Garvesh Raskutti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Look-Ahead Screening Rules for the Lasso. (arXiv:2105.05648v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05648</id>
        <link href="http://arxiv.org/abs/2105.05648"/>
        <updated>2021-06-30T02:01:02.858Z</updated>
        <summary type="html"><![CDATA[The lasso is a popular method to induce shrinkage and sparsity in the
solution vector (coefficients) of regression problems, particularly when there
are many predictors relative to the number of observations. Solving the lasso
in this high-dimensional setting can, however, be computationally demanding.
Fortunately, this demand can be alleviated via the use of screening rules that
discard predictors prior to fitting the model, leading to a reduced problem to
be solved. In this paper, we present a new screening strategy: look-ahead
screening. Our method uses safe screening rules to find a range of penalty
values for which a given predictor cannot enter the model, thereby screening
predictors along the remainder of the path. In experiments we show that these
look-ahead screening rules outperform the active warm-start version of the Gap
Safe rules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Larsson_J/0/1/0/all/0/1"&gt;Johan Larsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Learnability via Gradient Method for Two-Layer ReLU Neural Networks in Teacher-Student Setting. (arXiv:2106.06251v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06251</id>
        <link href="http://arxiv.org/abs/2106.06251"/>
        <updated>2021-06-30T02:01:02.808Z</updated>
        <summary type="html"><![CDATA[Deep learning empirically achieves high performance in many applications, but
its training dynamics has not been fully understood theoretically. In this
paper, we explore theoretical analysis on training two-layer ReLU neural
networks in a teacher-student regression model, in which a student network
learns an unknown teacher network through its outputs. We show that with a
specific regularization and sufficient over-parameterization, the student
network can identify the parameters of the teacher network with high
probability via gradient descent with a norm dependent stepsize even though the
objective function is highly non-convex. The key theoretical tool is the
measure representation of the neural networks and a novel application of a dual
certificate argument for sparse estimation on a measure space. We analyze the
global minima and global convergence property in the measure space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Akiyama_S/0/1/0/all/0/1"&gt;Shunta Akiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Finding Longer Proofs. (arXiv:1905.13100v2 [cs.LO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.13100</id>
        <link href="http://arxiv.org/abs/1905.13100"/>
        <updated>2021-06-30T02:01:02.717Z</updated>
        <summary type="html"><![CDATA[We present a reinforcement learning (RL) based guidance system for automated
theorem proving geared towards Finding Longer Proofs (FLoP). Unlike most
learning based approaches, we focus on generalising from very little training
data and achieving near complete confidence. We use several simple, structured
datasets with very long proofs to show that FLoP can successfully generalise a
single training proof to a large class of related problems. On these
benchmarks, FLoP is competitive with strong theorem provers despite using very
limited search, due to its ability to solve problems that are prohibitively
long for other systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zombori_Z/0/1/0/all/0/1"&gt;Zsolt Zombori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Csiszarik_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Csisz&amp;#xe1;rik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1"&gt;Henryk Michalewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1"&gt;Cezary Kaliszyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1"&gt;Josef Urban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Adaptive Swarm System (SASS). (arXiv:2106.04679v2 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04679</id>
        <link href="http://arxiv.org/abs/2106.04679"/>
        <updated>2021-06-30T02:01:02.712Z</updated>
        <summary type="html"><![CDATA[Distributed artificial intelligence (DAI) studies artificial intelligence
entities working together to reason, plan, solve problems, organize behaviors
and strategies, make collective decisions and learn. This Ph.D. research
proposes a principled Multi-Agent Systems (MAS) cooperation framework,
Self-Adaptive Swarm System (SASS), to bridge the fourth level automation gap
between perception, communication, planning, execution, decision-making, and
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization of Reinforcement Learning with Policy-Aware Adversarial Data Augmentation. (arXiv:2106.15587v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15587</id>
        <link href="http://arxiv.org/abs/2106.15587"/>
        <updated>2021-06-30T02:01:02.707Z</updated>
        <summary type="html"><![CDATA[The generalization gap in reinforcement learning (RL) has been a significant
obstacle that prevents the RL agent from learning general skills and adapting
to varying environments. Increasing the generalization capacity of the RL
systems can significantly improve their performance on real-world working
environments. In this work, we propose a novel policy-aware adversarial data
augmentation method to augment the standard policy learning method with
automatically generated trajectory data. Different from the commonly used
observation transformation based data augmentations, our proposed method
adversarially generates new trajectory data based on the policy gradient
objective and aims to more effectively increase the RL agent's generalization
ability with the policy-aware data augmentation. Moreover, we further deploy a
mixup step to integrate the original and generated data to enhance the
generalization capacity while mitigating the over-deviation of the adversarial
data. We conduct experiments on a number of RL tasks to investigate the
generalization performance of the proposed method by comparing it with the
standard baselines and the state-of-the-art mixreg approach. The results show
our method can generalize well with limited training diversity, and achieve the
state-of-the-art generalization test performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuhong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Neural Speech Synthesis. (arXiv:2106.15561v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.15561</id>
        <link href="http://arxiv.org/abs/2106.15561"/>
        <updated>2021-06-30T02:01:02.702Z</updated>
        <summary type="html"><![CDATA[Text to speech (TTS), or speech synthesis, which aims to synthesize
intelligible and natural speech given text, is a hot research topic in speech,
language, and machine learning communities and has broad applications in the
industry. As the development of deep learning and artificial intelligence,
neural network-based TTS has significantly improved the quality of synthesized
speech in recent years. In this paper, we conduct a comprehensive survey on
neural TTS, aiming to provide a good understanding of current research and
future trends. We focus on the key components in neural TTS, including text
analysis, acoustic models and vocoders, and several advanced topics, including
fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.
We further summarize resources related to TTS (e.g., datasets, opensource
implementations) and discuss future research directions. This survey can serve
both academic researchers and industry practitioners working on TTS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1"&gt;Frank Soong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Against Membership Inference Attack: Pruning is All You Need. (arXiv:2008.13578v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.13578</id>
        <link href="http://arxiv.org/abs/2008.13578"/>
        <updated>2021-06-30T02:01:02.697Z</updated>
        <summary type="html"><![CDATA[The large model size, high computational operations, and vulnerability
against membership inference attack (MIA) have impeded deep learning or deep
neural networks (DNNs) popularity, especially on mobile devices. To address the
challenge, we envision that the weight pruning technique will help DNNs against
MIA while reducing model storage and computational operation. In this work, we
propose a pruning algorithm, and we show that the proposed algorithm can find a
subnetwork that can prevent privacy leakage from MIA and achieves competitive
accuracy with the original DNNs. We also verify our theoretical insights with
experiments. Our experimental results illustrate that the attack accuracy using
model compression is up to 13.6% and 10% lower than that of the baseline and
Min-Max game, accordingly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yijue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenghong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zigeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shanglin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1"&gt;Jinbo Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Caiwen Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajasekaran_S/0/1/0/all/0/1"&gt;Sanguthevar Rajasekaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Body Region Classification of MRI and CT examinations. (arXiv:2104.13826v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13826</id>
        <link href="http://arxiv.org/abs/2104.13826"/>
        <updated>2021-06-30T02:01:02.682Z</updated>
        <summary type="html"><![CDATA[Standardized body region labelling of individual images provides data that
can improve human and computer use of medical images. A CNN-based classifier
was developed to identify body regions in CT and MRI. 17 CT (18 MRI) body
regions covering the entire human body were defined for the classification
task. Three retrospective databases were built for the AI model training,
validation, and testing, with a balanced distribution of studies per body
region. The test databases originated from a different healthcare network.
Accuracy, recall and precision of the classifier was evaluated for patient age,
patient gender, institution, scanner manufacturer, contrast, slice thickness,
MRI sequence, and CT kernel. The data included a retrospective cohort of 2,934
anonymized CT cases (training: 1,804 studies, validation: 602 studies, test:
528 studies) and 3,185 anonymized MRI cases (training: 1,911 studies,
validation: 636 studies, test: 638 studies). 27 institutions from primary care
hospitals, community hospitals and imaging centers contributed to the test
datasets. The data included cases of all genders in equal proportions and
subjects aged from a few months old to +90 years old. An image-level prediction
accuracy of 91.9% (90.2 - 92.1) for CT, and 94.2% (92.0 - 95.6) for MRI was
achieved. The classification results were robust across all body regions and
confounding factors. Due to limited data, performance results for subjects
under 10 years-old could not be reliably evaluated. We show that deep learning
models can classify CT and MRI images by body region including lower and upper
extremities with high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Raffy_P/0/1/0/all/0/1"&gt;Philippe Raffy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pambrun_J/0/1/0/all/0/1"&gt;Jean-Fran&amp;#xe7;ois Pambrun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ashish Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dubois_D/0/1/0/all/0/1"&gt;David Dubois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patti_J/0/1/0/all/0/1"&gt;Jay Waldron Patti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cairns_R/0/1/0/all/0/1"&gt;Robyn Alexandra Cairns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Young_R/0/1/0/all/0/1"&gt;Ryan Young&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Values Encoded in Machine Learning Research. (arXiv:2106.15590v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15590</id>
        <link href="http://arxiv.org/abs/2106.15590"/>
        <updated>2021-06-30T02:01:02.676Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) currently exerts an outsized influence on the world,
increasingly affecting communities and institutional practices. It is therefore
critical that we question vague conceptions of the field as value-neutral or
universally beneficial, and investigate what specific values the field is
advancing. In this paper, we present a rigorous examination of the values of
the field by quantitatively and qualitatively analyzing 100 highly cited ML
papers published at premier ML conferences, ICML and NeurIPS. We annotate key
features of papers which reveal their values: how they justify their choice of
project, which aspects they uplift, their consideration of potential negative
consequences, and their institutional affiliations and funding sources. We find
that societal needs are typically very loosely connected to the choice of
project, if mentioned at all, and that consideration of negative consequences
is extremely rare. We identify 67 values that are uplifted in machine learning
research, and, of these, we find that papers most frequently justify and assess
themselves based on performance, generalization, efficiency, researcher
understanding, novelty, and building on previous work. We present extensive
textual evidence and analysis of how these values are operationalized. Notably,
we find that each of these top values is currently being defined and applied
with assumptions and implications generally supporting the centralization of
power. Finally, we find increasingly close ties between these highly cited
papers and tech companies and elite universities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Birhane_A/0/1/0/all/0/1"&gt;Abeba Birhane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalluri_P/0/1/0/all/0/1"&gt;Pratyusha Kalluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1"&gt;Dallas Card&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agnew_W/0/1/0/all/0/1"&gt;William Agnew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dotan_R/0/1/0/all/0/1"&gt;Ravit Dotan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_M/0/1/0/all/0/1"&gt;Michelle Bao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Evaluation of Deep Active Learning on Image Classification Tasks. (arXiv:2106.15324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15324</id>
        <link href="http://arxiv.org/abs/2106.15324"/>
        <updated>2021-06-30T02:01:02.671Z</updated>
        <summary type="html"><![CDATA[With the goal of making deep learning more label-efficient, a growing number
of papers have been studying active learning (AL) for deep models. However,
there are a number of issues in the prevalent experimental settings, mainly
stemming from a lack of unified implementation and benchmarking. Issues in the
current literature include sometimes contradictory observations on the
performance of different AL algorithms, unintended exclusion of important
generalization approaches such as data augmentation and SGD for optimization, a
lack of study of evaluation facets like the labeling efficiency of AL, and
little or no clarity on the scenarios in which AL outperforms random sampling
(RS). In this work, we present a unified re-implementation of state-of-the-art
AL algorithms in the context of image classification, and we carefully study
these issues as facets of effective evaluation. On the positive side, we show
that AL techniques are 2x to 4x more label-efficient compared to RS with the
use of data augmentation. Surprisingly, when data augmentation is included,
there is no longer a consistent gain in using BADGE, a state-of-the-art
approach, over simple uncertainty sampling. We then do a careful analysis of
how existing approaches perform with varying amounts of redundancy and number
of examples per class. Finally, we provide several insights for AL
practitioners to consider in future work, such as the effect of the AL batch
size, the effect of initialization, the importance of retraining a new model at
every round, and other insights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1"&gt;Nathan Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1"&gt;Durga Sivasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dani_A/0/1/0/all/0/1"&gt;Apurva Dani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PPFL: Privacy-preserving Federated Learning with Trusted Execution Environments. (arXiv:2104.14380v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14380</id>
        <link href="http://arxiv.org/abs/2104.14380"/>
        <updated>2021-06-30T02:01:02.635Z</updated>
        <summary type="html"><![CDATA[We propose and implement a Privacy-preserving Federated Learning ($PPFL$)
framework for mobile systems to limit privacy leakages in federated learning.
Leveraging the widespread presence of Trusted Execution Environments (TEEs) in
high-end and mobile devices, we utilize TEEs on clients for local training, and
on servers for secure aggregation, so that model/gradient updates are hidden
from adversaries. Challenged by the limited memory size of current TEEs, we
leverage greedy layer-wise training to train each model's layer inside the
trusted area until its convergence. The performance evaluation of our
implementation shows that $PPFL$ can significantly improve privacy while
incurring small system overheads at the client-side. In particular, $PPFL$ can
successfully defend the trained model against data reconstruction, property
inference, and membership inference attacks. Furthermore, it can achieve
comparable model utility with fewer communication rounds (0.54$\times$) and a
similar amount of network traffic (1.002$\times$) compared to the standard
federated learning of a complete model. This is achieved while only introducing
up to ~15% CPU time, ~18% memory usage, and ~21% energy consumption overhead in
$PPFL$'s client-side.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1"&gt;Fan Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1"&gt;Hamed Haddadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katevas_K/0/1/0/all/0/1"&gt;Kleomenis Katevas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marin_E/0/1/0/all/0/1"&gt;Eduard Marin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perino_D/0/1/0/all/0/1"&gt;Diego Perino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kourtellis_N/0/1/0/all/0/1"&gt;Nicolas Kourtellis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expert Q-learning: Deep Q-learning With State Values From Expert Examples. (arXiv:2106.14642v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14642</id>
        <link href="http://arxiv.org/abs/2106.14642"/>
        <updated>2021-06-30T02:01:02.629Z</updated>
        <summary type="html"><![CDATA[We propose a novel algorithm named Expert Q-learning. Expert Q-learning was
inspired by Dueling Q-learning and aimed at incorporating the ideas from
semi-supervised learning into reinforcement learning through splitting Q-values
into state values and action advantages. Different from Generative Adversarial
Imitation Learning and Deep Q-Learning from Demonstrations, the offline expert
we have used only predicts the value of a state from {-1, 0, 1}, indicating
whether this is a bad, neutral or good state. An expert network was designed in
addition to the Q-network, which updates each time following the regular
offline minibatch update whenever the expert example buffer is not empty. The
Q-network plays the role of the advantage function only during the update. Our
algorithm also keeps asynchronous copies of the Q-network and expert network,
predicting the target values using the same manner as of Double Q-learning.

We compared on the game of Othello our algorithm with the state-of-the-art
Q-learning algorithm, which was a combination of Double Q-learning and Dueling
Q-learning. The results showed that Expert Q-learning was indeed useful and
more resistant to the overestimation bias of Q-learning. The baseline
Q-learning algorithm exhibited unstable and suboptimal behavior, especially
when playing against a stochastic player, whereas Expert Q-learning
demonstrated more robust performance with higher scores. Expert Q-learning
without using examples has also gained better results than the baseline
algorithm when trained and tested against a fixed player. On the other hand,
Expert Q-learning without examples cannot win against the baseline Q-learning
algorithm in direct game competitions despite the fact that it has also shown
the strength of reducing the overestimation bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Li Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yazidi_A/0/1/0/all/0/1"&gt;Anis Yazidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1"&gt;Morten Goodwin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engelstad_P/0/1/0/all/0/1"&gt;Paal Engelstad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doing good by fighting fraud: Ethical anti-fraud systems for mobile payments. (arXiv:2106.14861v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14861</id>
        <link href="http://arxiv.org/abs/2106.14861"/>
        <updated>2021-06-30T02:01:02.616Z</updated>
        <summary type="html"><![CDATA[App builders commonly use security challenges, a form of step-up
authentication, to add security to their apps. However, the ethical
implications of this type of architecture has not been studied previously. In
this paper, we present a large-scale measurement study of running an existing
anti-fraud security challenge, Boxer, in real apps running on mobile devices.
We find that although Boxer does work well overall, it is unable to scan
effectively on devices that run its machine learning models at less than one
frame per second (FPS), blocking users who use inexpensive devices. With the
insights from our study, we design Daredevil, anew anti-fraud system for
scanning payment cards that work swell across the broad range of performance
characteristics and hardware configurations found on modern mobile devices.
Daredevil reduces the number of devices that run at less than one FPS by an
order of magnitude compared to Boxer, providing a more equitable system for
fighting fraud. In total, we collect data from 5,085,444 real devices spread
across 496 real apps running production software and interacting with real
users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Din_Z/0/1/0/all/0/1"&gt;Zainul Abi Din&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopalan_H/0/1/0/all/0/1"&gt;Hari Venugopalan&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Henry Lin&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Wushensky_A/0/1/0/all/0/1"&gt;Adam Wushensky&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Steven Liu&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+King_S/0/1/0/all/0/1"&gt;Samuel T. King&lt;/a&gt; (1 and 2) ((1) University of California, Davis, (2) Bouncer Technologies)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Decentralized Adaptive Momentum Method for Solving a Class of Min-Max Optimization Problems. (arXiv:2106.06075v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06075</id>
        <link href="http://arxiv.org/abs/2106.06075"/>
        <updated>2021-06-30T02:01:02.611Z</updated>
        <summary type="html"><![CDATA[Min-max saddle point games have recently been intensely studied, due to their
wide range of applications, including training Generative Adversarial Networks
(GANs). However, most of the recent efforts for solving them are limited to
special regimes such as convex-concave games. Further, it is customarily
assumed that the underlying optimization problem is solved either by a single
machine or in the case of multiple machines connected in centralized fashion,
wherein each one communicates with a central node. The latter approach becomes
challenging, when the underlying communications network has low bandwidth. In
addition, privacy considerations may dictate that certain nodes can communicate
with a subset of other nodes. Hence, it is of interest to develop methods that
solve min-max games in a decentralized manner. To that end, we develop a
decentralized adaptive momentum (ADAM)-type algorithm for solving min-max
optimization problem under the condition that the objective function satisfies
a Minty Variational Inequality condition, which is a generalization to
convex-concave case. The proposed method overcomes shortcomings of recent
non-adaptive gradient-based decentralized algorithms for min-max optimization
problems that do not perform well in practice and require careful tuning. In
this paper, we obtain non-asymptotic rates of convergence of the proposed
algorithm (coined DADAM$^3$) for finding a (stochastic) first-order Nash
equilibrium point and subsequently evaluate its performance on training GANs.
The extensive empirical evaluation shows that DADAM$^3$ outperforms recently
developed methods, including decentralized optimistic stochastic gradient for
solving such min-max problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Barazandeh_B/0/1/0/all/0/1"&gt;Babak Barazandeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tianjian Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Michailidis_G/0/1/0/all/0/1"&gt;George Michailidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Guided Progressive GANs for Medical Image Translation. (arXiv:2106.15542v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15542</id>
        <link href="http://arxiv.org/abs/2106.15542"/>
        <updated>2021-06-30T02:01:02.599Z</updated>
        <summary type="html"><![CDATA[Image-to-image translation plays a vital role in tackling various medical
imaging tasks such as attenuation correction, motion correction, undersampled
reconstruction, and denoising. Generative adversarial networks have been shown
to achieve the state-of-the-art in generating high fidelity images for these
tasks. However, the state-of-the-art GAN-based frameworks do not estimate the
uncertainty in the predictions made by the network that is essential for making
informed medical decisions and subsequent revision by medical experts and has
recently been shown to improve the performance and interpretability of the
model. In this work, we propose an uncertainty-guided progressive learning
scheme for image-to-image translation. By incorporating aleatoric uncertainty
as attention maps for GANs trained in a progressive manner, we generate images
of increasing fidelity progressively. We demonstrate the efficacy of our model
on three challenging medical image translation tasks, including PET to CT
translation, undersampled MRI reconstruction, and MRI motion artefact
correction. Our model generalizes well in three different tasks and improves
performance over state of the art under full-supervision and weak-supervision
with limited data. Code is released here:
https://github.com/ExplainableML/UncerGuidedI2I]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1"&gt;Uddeshya Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanbei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hepp_T/0/1/0/all/0/1"&gt;Tobias Hepp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1"&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03746</id>
        <link href="http://arxiv.org/abs/2105.03746"/>
        <updated>2021-06-30T02:01:02.593Z</updated>
        <summary type="html"><![CDATA[Contrastive learning (CL) is effective in learning data representations
without label supervision, where the encoder needs to contrast each positive
sample over multiple negative samples via a one-vs-many softmax cross-entropy
loss. However, conventional CL is sensitive to how many negative samples are
included and how they are selected. Proposed in this paper is a doubly CL
strategy that contrasts positive samples and negative ones within themselves
separately. We realize this strategy with contrastive attraction and
contrastive repulsion (CACR) makes the query not only exert a greater force to
attract more distant positive samples but also do so to repel closer negative
samples. Theoretical analysis reveals the connection between CACR and CL from
the perspectives of both positive attraction and negative repulsion and shows
the benefits in both efficiency and robustness brought by separately
contrasting within the sampled positive and negative pairs. Extensive
large-scale experiments on standard vision tasks show that CACR not only
consistently outperforms existing CL methods on benchmark datasets in
representation learning, but also provides interpretable contrastive weights,
demonstrating the efficacy of the proposed doubly contrastive strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huangjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VolterraNet: A higher order convolutional network with group equivariance for homogeneous manifolds. (arXiv:2106.15301v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15301</id>
        <link href="http://arxiv.org/abs/2106.15301"/>
        <updated>2021-06-30T02:01:02.563Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks have been highly successful in image-based
learning tasks due to their translation equivariance property. Recent work has
generalized the traditional convolutional layer of a convolutional neural
network to non-Euclidean spaces and shown group equivariance of the generalized
convolution operation. In this paper, we present a novel higher order Volterra
convolutional neural network (VolterraNet) for data defined as samples of
functions on Riemannian homogeneous spaces. Analagous to the result for
traditional convolutions, we prove that the Volterra functional convolutions
are equivariant to the action of the isometry group admitted by the Riemannian
homogeneous spaces, and under some restrictions, any non-linear equivariant
function can be expressed as our homogeneous space Volterra convolution,
generalizing the non-linear shift equivariant characterization of Volterra
expansions in Euclidean space. We also prove that second order functional
convolution operations can be represented as cascaded convolutions which leads
to an efficient implementation. Beyond this, we also propose a dilated
VolterraNet model. These advances lead to large parameter reductions relative
to baseline non-Euclidean CNNs. To demonstrate the efficacy of the VolterraNet
performance, we present several real data experiments involving classification
tasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing
on diffusion MRI data. Performance comparisons to the state-of-the-art are also
presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_M/0/1/0/all/0/1"&gt;Monami Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1"&gt;Rudrasis Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouza_J/0/1/0/all/0/1"&gt;Jose Bouza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vemuri_B/0/1/0/all/0/1"&gt;Baba C. Vemuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Ambient Intelligence-Based Human Behavior Monitoring Framework for Ubiquitous Environments. (arXiv:2106.15609v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.15609</id>
        <link href="http://arxiv.org/abs/2106.15609"/>
        <updated>2021-06-30T02:01:02.549Z</updated>
        <summary type="html"><![CDATA[This framework for human behavior monitoring aims to take a holistic approach
to study, track, monitor, and analyze human behavior during activities of daily
living (ADLs). The framework consists of two novel functionalities. First, it
can perform the semantic analysis of user interactions on the diverse
contextual parameters during ADLs to identify a list of distinct behavioral
patterns associated with different complex activities. Second, it consists of
an intelligent decision-making algorithm that can analyze these behavioral
patterns and their relationships with the dynamic contextual and spatial
features of the environment to detect any anomalies in user behavior that could
constitute an emergency. These functionalities of this interdisciplinary
framework were developed by integrating the latest advancements and
technologies in human-computer interaction, machine learning, Internet of
Things, pattern recognition, and ubiquitous computing. The framework was
evaluated on a dataset of ADLs, and the performance accuracies of these two
functionalities were found to be 76.71% and 83.87%, respectively. The presented
and discussed results uphold the relevance and immense potential of this
framework to contribute towards improving the quality of life and assisted
living of the aging population in the future of Internet of Things (IoT)-based
ubiquitous living environments, e.g., smart homes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Representation Learning Perspective on the Importance of Train-Validation Splitting in Meta-Learning. (arXiv:2106.15615v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15615</id>
        <link href="http://arxiv.org/abs/2106.15615"/>
        <updated>2021-06-30T02:01:02.544Z</updated>
        <summary type="html"><![CDATA[An effective approach in meta-learning is to utilize multiple "train tasks"
to learn a good initialization for model parameters that can help solve unseen
"test tasks" with very few samples by fine-tuning from this initialization.
Although successful in practice, theoretical understanding of such methods is
limited. This work studies an important aspect of these methods: splitting the
data from each task into train (support) and validation (query) sets during
meta-training. Inspired by recent work (Raghu et al., 2020), we view such
meta-learning methods through the lens of representation learning and argue
that the train-validation split encourages the learned representation to be
low-rank without compromising on expressivity, as opposed to the non-splitting
variant that encourages high-rank representations. Since sample efficiency
benefits from low-rankness, the splitting strategy will require very few
samples to solve unseen test tasks. We present theoretical results that
formalize this idea for linear representation learning on a subspace
meta-learning instance, and experimentally verify this practical benefit of
splitting in simulations and on standard meta-learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saunshi_N/0/1/0/all/0/1"&gt;Nikunj Saunshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Arushi Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ScanBank: A Benchmark Dataset for Figure Extraction from Scanned Electronic Theses and Dissertations. (arXiv:2106.15320v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15320</id>
        <link href="http://arxiv.org/abs/2106.15320"/>
        <updated>2021-06-30T02:01:02.539Z</updated>
        <summary type="html"><![CDATA[We focus on electronic theses and dissertations (ETDs), aiming to improve
access and expand their utility, since more than 6 million are publicly
available, and they constitute an important corpus to aid research and
education across disciplines. The corpus is growing as new born-digital
documents are included, and since millions of older theses and dissertations
have been converted to digital form to be disseminated electronically in
institutional repositories. In ETDs, as with other scholarly works, figures and
tables can communicate a large amount of information in a concise way. Although
methods have been proposed for extracting figures and tables from born-digital
PDFs, they do not work well with scanned ETDs. Considering this problem, our
assessment of state-of-the-art figure extraction systems is that the reason
they do not function well on scanned PDFs is that they have only been trained
on born-digital documents. To address this limitation, we present ScanBank, a
new dataset containing 10 thousand scanned page images, manually labeled by
humans as to the presence of the 3.3 thousand figures or tables found therein.
We use this dataset to train a deep neural network model based on YOLOv5 to
accurately extract figures and tables from scanned ETDs. We pose and answer
important research questions aimed at finding better methods for figure
extraction from scanned documents. One of those concerns the value for
training, of data augmentation techniques applied to born-digital documents
which are used to train models better suited for figure extraction from scanned
documents. To the best of our knowledge, ScanBank is the first manually
annotated dataset for figure and table extraction for scanned ETDs. A
YOLOv5-based model, trained on ScanBank, outperforms existing comparable
open-source and freely available baseline methods by a considerable margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kahu_S/0/1/0/all/0/1"&gt;Sampanna Yashwant Kahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ingram_W/0/1/0/all/0/1"&gt;William A. Ingram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1"&gt;Edward A. Fox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jian Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft Attention: Does it Actually Help to Learn Social Interactions in Pedestrian Trajectory Prediction?. (arXiv:2106.15321v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15321</id>
        <link href="http://arxiv.org/abs/2106.15321"/>
        <updated>2021-06-30T02:01:02.478Z</updated>
        <summary type="html"><![CDATA[We consider the problem of predicting the future path of a pedestrian using
its motion history and the motion history of the surrounding pedestrians,
called social information. Since the seminal paper on Social-LSTM,
deep-learning has become the main tool used to model the impact of social
interactions on a pedestrian's motion. The demonstration that these models can
learn social interactions relies on an ablative study of these models. The
models are compared with and without their social interactions module on two
standard metrics, the Average Displacement Error and Final Displacement Error.
Yet, these complex models were recently outperformed by a simple
constant-velocity approach. This questions if they actually allow to model
social interactions as well as the validity of the proof. In this paper, we
focus on the deep-learning models with a soft-attention mechanism for social
interaction modeling and study whether they use social information at
prediction time. We conduct two experiments across four state-of-the-art
approaches on the ETH and UCY datasets, which were also used in previous work.
First, the models are trained by replacing the social information with random
noise and compared to model trained with actual social information. Second, we
use a gating mechanism along with a $L_0$ penalty, allowing models to shut down
their inner components. The models consistently learn to prune their
soft-attention mechanism. For both experiments, neither the course of the
convergence nor the prediction performance were altered. This demonstrates that
the soft-attention mechanism and therefore the social information are ignored
by the models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boucaud_L/0/1/0/all/0/1"&gt;Laurent Boucaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aloise_D/0/1/0/all/0/1"&gt;Daniel Aloise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1"&gt;Nicolas Saunier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in Minimally Invasive Interventional Treatment. (arXiv:2106.15306v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15306</id>
        <link href="http://arxiv.org/abs/2106.15306"/>
        <updated>2021-06-30T02:01:02.472Z</updated>
        <summary type="html"><![CDATA[Minimally invasive image guided treatment procedures often employ advanced
image processing algorithms. The recent developments of artificial intelligence
algorithms harbor potential to further enhance this domain. In this article we
explore several application areas within the minimally invasive treatment space
and discuss the deployment of artificial intelligence within these areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruijters_D/0/1/0/all/0/1"&gt;Daniel Ruijters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliable and Fast Recurrent Neural Network Architecture Optimization. (arXiv:2106.15295v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.15295</id>
        <link href="http://arxiv.org/abs/2106.15295"/>
        <updated>2021-06-30T02:01:02.458Z</updated>
        <summary type="html"><![CDATA[This article introduces Random Error Sampling-based Neuroevolution (RESN), a
novel automatic method to optimize recurrent neural network architectures. RESN
combines an evolutionary algorithm with a training-free evaluation approach.
The results show that RESN achieves state-of-the-art error performance while
reducing by half the computational time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Camero_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Camero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toutouh_J/0/1/0/all/0/1"&gt;Jamal Toutouh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alba_E/0/1/0/all/0/1"&gt;Enrique Alba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCASE 2021 Task 3: Spectrotemporally-aligned Features for Polyphonic Sound Event Localization and Detection. (arXiv:2106.15190v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.15190</id>
        <link href="http://arxiv.org/abs/2106.15190"/>
        <updated>2021-06-30T02:01:02.453Z</updated>
        <summary type="html"><![CDATA[Sound event localization and detection consists of two subtasks which are
sound event detection and direction-of-arrival estimation. While sound event
detection mainly relies on time-frequency patterns to distinguish different
sound classes, direction-of-arrival estimation uses magnitude or phase
differences between microphones to estimate source directions. Therefore, it is
often difficult to jointly train these two subtasks simultaneously. We propose
a novel feature called spatial cue-augmented log-spectrogram (SALSA) with exact
time-frequency mapping between the signal power and the source
direction-of-arrival. The feature includes multichannel log-spectrograms
stacked along with the estimated direct-to-reverberant ratio and a normalized
version of the principal eigenvector of the spatial covariance matrix at each
time-frequency bin on the spectrograms. Experimental results on the DCASE 2021
dataset for sound event localization and detection with directional
interference showed that the deep learning-based models trained on this new
feature outperformed the DCASE challenge baseline by a large margin. We
combined several models with slightly different architectures that were trained
on the new feature to further improve the system performances for the DCASE
sound event localization and detection challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thi Ngoc Tho Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1"&gt;Karn Watcharasupat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngoc Khanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jones_D/0/1/0/all/0/1"&gt;Douglas L. Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1"&gt;Woon Seng Gan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Sparse Coding Fast Approximation with Application to Seismic Reflectivity Estimation. (arXiv:2106.15296v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15296</id>
        <link href="http://arxiv.org/abs/2106.15296"/>
        <updated>2021-06-30T02:01:02.448Z</updated>
        <summary type="html"><![CDATA[In sparse coding, we attempt to extract features of input vectors, assuming
that the data is inherently structured as a sparse superposition of basic
building blocks. Similarly, neural networks perform a given task by learning
features of the training data set. Recently both data-driven and model-driven
feature extracting methods have become extremely popular and have achieved
remarkable results. Nevertheless, practical implementations are often too slow
to be employed in real-life scenarios, especially for real-time applications.
We propose a speed-up upgraded version of the classic iterative thresholding
algorithm, that produces a good approximation of the convolutional sparse code
within 2-5 iterations. The speed advantage is gained mostly from the
observation that most solvers are slowed down by inefficient global
thresholding. The main idea is to normalize each data point by the local
receptive field energy, before applying a threshold. This way, the natural
inclination towards strong feature expressions is suppressed, so that one can
rely on a global threshold that can be easily approximated, or learned during
training. The proposed algorithm can be employed with a known predetermined
dictionary, or with a trained dictionary. The trained version is implemented as
a neural net designed as the unfolding of the proposed solver. The performance
of the proposed solution is demonstrated via the seismic inversion problem in
both synthetic and real data scenarios. We also provide theoretical guarantees
for a stable support recovery. Namely, we prove that under certain conditions
the true support is perfectly recovered within the first iteration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereg_D/0/1/0/all/0/1"&gt;Deborah Pereg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Israel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vassiliou_A/0/1/0/all/0/1"&gt;Anthony A. Vassiliou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[US Fatal Police Shooting Analysis and Prediction. (arXiv:2106.15298v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2106.15298</id>
        <link href="http://arxiv.org/abs/2106.15298"/>
        <updated>2021-06-30T02:01:02.443Z</updated>
        <summary type="html"><![CDATA[We believe that "all men are created equal". With the rise of the police
shootings reported by media, more people in the U.S. think that police use
excessive force during law enforcement, especially to a specific group of
people. We want to apply multidimensional statistical analysis to reveal more
facts than the monotone mainstream media. Our paper has three parts. First, we
proposed a new method to quantify fatal police shooting news reporting
deviation of mainstream media, which includes CNN, FOX, ABC, and NBC. Second,
we analyzed the most comprehensive US fatal police shooting dataset from
Washington Post. We used FP-growth to reveal the frequent patterns and DBSCAN
clustering to find fatal shooting hotspots. We brought multi-attributes (social
economics, demographics, political tendency, education, gun ownership rate,
police training hours, etc.) to reveal connections under the iceberg. We found
that the police shooting rate of a state depends on many variables. The top
four most relevant attributes were state joined year, state land area, gun
ownership rate, and violent crime rate. Third, we proposed four regression
models to predict police shooting rates at the state level. The best model
Kstar could predict the fatal police shooting rate with about 88.53%
correlation coefficient. We also proposed classification models, including
Gradient Boosting Machine, Multi-class Classifier, Logistic Regression, and
Naive Bayes Classifier, to predict the race of fatal police shooting victims.
Our classification models show no significant evidence to conclude that racial
discrimination happened during fatal police shootings recorded by the WP
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yangxin Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection and Automated Labeling for Voter Registration File Changes. (arXiv:2106.15285v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.15285</id>
        <link href="http://arxiv.org/abs/2106.15285"/>
        <updated>2021-06-30T02:01:02.426Z</updated>
        <summary type="html"><![CDATA[Voter eligibility in United States elections is determined by a patchwork of
state databases containing information about which citizens are eligible to
vote. Administrators at the state and local level are faced with the
exceedingly difficult task of ensuring that each of their jurisdictions is
properly managed, while also monitoring for improper modifications to the
database. Monitoring changes to Voter Registration Files (VRFs) is crucial,
given that a malicious actor wishing to disrupt the democratic process in the
US would be well-advised to manipulate the contents of these files in order to
achieve their goals. In 2020, we saw election officials perform admirably when
faced with administering one of the most contentious elections in US history,
but much work remains to secure and monitor the election systems Americans rely
on. Using data created by comparing snapshots taken of VRFs over time, we
present a set of methods that make use of machine learning to ease the burden
on analysts and administrators in protecting voter rolls. We first evaluate the
effectiveness of multiple unsupervised anomaly detection methods in detecting
VRF modifications by modeling anomalous changes as sparse additive noise. In
this setting we determine that statistical models comparing administrative
districts within a short time span and non-negative matrix factorization are
most effective for surfacing anomalous events for review. These methods were
deployed during 2019-2020 in our organization's monitoring system and were used
in collaboration with the office of the Iowa Secretary of State. Additionally,
we propose a newly deployed model which uses historical and demographic
metadata to label the likely root cause of database modifications. We hope to
use this model to predict which modifications have known causes and therefore
better identify potentially anomalous modifications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Royston_S/0/1/0/all/0/1"&gt;Sam Royston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greenberg_B/0/1/0/all/0/1"&gt;Ben Greenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tavasoli_O/0/1/0/all/0/1"&gt;Omeed Tavasoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotton_C/0/1/0/all/0/1"&gt;Courtenay Cotton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Deep Neural Networks for Image Document Enhancement. (arXiv:2106.15286v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15286</id>
        <link href="http://arxiv.org/abs/2106.15286"/>
        <updated>2021-06-30T02:01:02.421Z</updated>
        <summary type="html"><![CDATA[This work evaluates six state-of-the-art deep neural network (DNN)
architectures applied to the problem of enhancing camera-captured document
images. The results from each network were evaluated both qualitatively and
quantitatively using Image Quality Assessment (IQA) metrics, and also compared
with an existing approach based on traditional computer vision techniques. The
best performing architectures generally produced good enhancement compared to
the existing algorithm, showing that it is possible to use DNNs for document
image enhancement. Furthermore, the best performing architectures could work as
a baseline for future investigations on document enhancement using deep
learning techniques. The main contributions of this paper are: a baseline of
deep learning techniques that can be further improved to provide better
results, and a evaluation methodology using IQA metrics for quantitatively
comparing the produced images from the neural networks to a ground truth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirsten_L/0/1/0/all/0/1"&gt;Lucas N. Kirsten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piccoli_R/0/1/0/all/0/1"&gt;Ricardo Piccoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribani_R/0/1/0/all/0/1"&gt;Ricardo Ribani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Solar Potential of Rooftops using Image Segmentation and Structured Data. (arXiv:2106.15268v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15268</id>
        <link href="http://arxiv.org/abs/2106.15268"/>
        <updated>2021-06-30T02:01:02.416Z</updated>
        <summary type="html"><![CDATA[Estimating the amount of electricity that can be produced by rooftop
photovoltaic systems is a time-consuming process that requires on-site
measurements, a difficult task to achieve on a large scale. In this paper, we
present an approach to estimate the solar potential of rooftops based on their
location and architectural characteristics, as well as the amount of solar
radiation they receive annually. Our technique uses computer vision to achieve
semantic segmentation of roof sections and roof objects on the one hand, and a
machine learning model based on structured building features to predict roof
pitch on the other hand. We then compute the azimuth and maximum number of
solar panels that can be installed on a rooftop with geometric approaches.
Finally, we compute precise shading masks and combine them with solar
irradiation data that enables us to estimate the yearly solar potential of a
rooftop.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soares_D/0/1/0/all/0/1"&gt;Daniel de Barros Soares&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Andrieux_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Andrieux&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Hell_B/0/1/0/all/0/1"&gt;Bastien Hell&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Lenhardt_J/0/1/0/all/0/1"&gt;Julien Lenhardt&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Badosa_J/0/1/0/all/0/1"&gt;Jordi Badosa&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Gavoille_S/0/1/0/all/0/1"&gt;Sylvain Gavoille&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Gaiffas_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Gaiffas&lt;/a&gt; (1, 4 and 5), &lt;a href="http://arxiv.org/find/cs/1/au:+Bacry_E/0/1/0/all/0/1"&gt;Emmanuel Bacry&lt;/a&gt; (1 and 6), ((1) namR, Paris, France, (2) ENSTA Paris, France, (3) LMD, Ecole polytechnique, IP Paris, Palaiseau, France, (4) LPSM, Universit&amp;#xe9; de Paris, France, (5) DMA, Ecole normale sup&amp;#xe9;rieure, Paris, France, (6) CEREMADE, Universit&amp;#xe9; Paris Dauphine, Paris, France)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open-Set Representation Learning through Combinatorial Embedding. (arXiv:2106.15278v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15278</id>
        <link href="http://arxiv.org/abs/2106.15278"/>
        <updated>2021-06-30T02:01:02.411Z</updated>
        <summary type="html"><![CDATA[Visual recognition tasks are often limited to dealing with a small subset of
classes simply because the labels for the remaining classes are unavailable. We
are interested in identifying novel concepts in a dataset through
representation learning based on the examples in both labeled and unlabeled
classes, and extending the horizon of recognition to both known and novel
classes. To address this challenging task, we propose a combinatorial learning
approach, which naturally clusters the examples in unseen classes using the
compositional knowledge given by multiple supervised meta-classifiers on
heterogeneous label spaces. We also introduce a metric learning strategy to
estimate pairwise pseudo-labels for improving representations of unlabeled
examples, which preserves semantic relations across known and novel classes
effectively. The proposed algorithm discovers novel concepts via a joint
optimization of enhancing the discrimitiveness of unseen classes as well as
learning the representations of known classes generalizable to novel ones. Our
extensive experiments demonstrate remarkable performance gains by the proposed
approach in multiple image retrieval and novel class discovery benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Geeho Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bohyung Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating the Graph Gestalt: Kernel-Regularized Graph Representation Learning. (arXiv:2106.15239v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15239</id>
        <link href="http://arxiv.org/abs/2106.15239"/>
        <updated>2021-06-30T02:01:02.405Z</updated>
        <summary type="html"><![CDATA[Recent work on graph generative models has made remarkable progress towards
generating increasingly realistic graphs, as measured by global graph features
such as degree distribution, density, and clustering coefficients. Deep
generative models have also made significant advances through better modelling
of the local correlations in the graph topology, which have been very useful
for predicting unobserved graph components, such as the existence of a link or
the class of a node, from nearby observed graph components. A complete
scientific understanding of graph data should address both global and local
structure. In this paper, we propose a joint model for both as complementary
objectives in a graph VAE framework. Global structure is captured by
incorporating graph kernels in a probabilistic model whose loss function is
closely related to the maximum mean discrepancy(MMD) between the global
structures of the reconstructed and the input graphs. The ELBO objective
derived from the model regularizes a standard local link reconstruction term
with an MMD term. Our experiments demonstrate a significant improvement in the
realism of the generated graph structures, typically by 1-2 orders of magnitude
of graph structure metrics, compared to leading graph VAEand GAN models. Local
link reconstruction improves as well in many cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zahirnia_K/0/1/0/all/0/1"&gt;Kiarash Zahirnia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakhuja_A/0/1/0/all/0/1"&gt;Ankita Sakhuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1"&gt;Oliver Schulte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadaf_P/0/1/0/all/0/1"&gt;Parmis Nadaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System. (arXiv:2106.15130v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.15130</id>
        <link href="http://arxiv.org/abs/2106.15130"/>
        <updated>2021-06-30T02:01:02.400Z</updated>
        <summary type="html"><![CDATA[The last-generation video conferencing software allows users to utilize a
virtual background to conceal their personal environment due to privacy
concerns, especially in official meetings with other employers. On the other
hand, users maybe want to fool people in the meeting by considering the virtual
background to conceal where they are. In this case, developing tools to
understand the virtual background utilize for fooling people in meeting plays
an important role. Besides, such detectors must prove robust against different
kinds of attacks since a malicious user can fool the detector by applying a set
of adversarial editing steps on the video to conceal any revealing footprint.
In this paper, we study the feasibility of an efficient tool to detect whether
a videoconferencing user background is real. In particular, we provide the
first tool which computes pixel co-occurrences matrices and uses them to search
for inconsistencies among spectral and spatial bands. Our experiments confirm
that cross co-occurrences matrices improve the robustness of the detector
against different kinds of attacks. This work's performance is especially
noteworthy with regard to color SPAM features. Moreover, the performance
especially is significant with regard to robustness versus post-processing,
like geometric transformations, filtering, contrast enhancement, and JPEG
compression with different quality factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1"&gt;Mauro Conti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1"&gt;Simone Milani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1"&gt;Ehsan Nowroozi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orazi_G/0/1/0/all/0/1"&gt;Gabriele Orazi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment. (arXiv:2106.14993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14993</id>
        <link href="http://arxiv.org/abs/2106.14993"/>
        <updated>2021-06-30T02:01:02.386Z</updated>
        <summary type="html"><![CDATA[Many transfer problems require re-using previously optimal decisions for
solving new tasks, which suggests the need for learning algorithms that can
modify the mechanisms for choosing certain actions independently of those for
choosing others. However, there is currently no formalism nor theory for how to
achieve this kind of modular credit assignment. To answer this question, we
define modular credit assignment as a constraint on minimizing the algorithmic
mutual information among feedback signals for different decisions. We introduce
what we call the modularity criterion for testing whether a learning algorithm
satisfies this constraint by performing causal analysis on the algorithm
itself. We generalize the recently proposed societal decision-making framework
as a more granular formalism than the Markov decision process to prove that for
decision sequences that do not contain cycles, certain single-step temporal
difference action-value methods meet this criterion while all policy-gradient
methods do not. Empirical evidence suggests that such action-value methods are
more sample efficient than policy-gradient methods on transfer problems that
require only sparse changes to a sequence of previously optimal decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1"&gt;Michael Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1"&gt;Sidhant Kaushik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1"&gt;Thomas L. Griffiths&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ElephantBook: A Semi-Automated Human-in-the-Loop System for Elephant Re-Identification. (arXiv:2106.15083v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15083</id>
        <link href="http://arxiv.org/abs/2106.15083"/>
        <updated>2021-06-30T02:01:02.381Z</updated>
        <summary type="html"><![CDATA[African elephants are vital to their ecosystems, but their populations are
threatened by a rise in human-elephant conflict and poaching. Monitoring
population dynamics is essential in conservation efforts; however, tracking
elephants is a difficult task, usually relying on the invasive and sometimes
dangerous placement of GPS collars. Although there have been many recent
successes in the use of computer vision techniques for automated identification
of other species, identification of elephants is extremely difficult and
typically requires expertise as well as familiarity with elephants in the
population. We have built and deployed a web-based platform and database for
human-in-the-loop re-identification of elephants combining manual attribute
labeling and state-of-the-art computer vision algorithms, known as
ElephantBook. Our system is currently in use at the Mara Elephant Project,
helping monitor the protected and at-risk population of elephants in the
Greater Maasai Mara ecosystem. ElephantBook makes elephant re-identification
usable by non-experts and scalable for use by multiple conservation NGOs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulits_P/0/1/0/all/0/1"&gt;Peter Kulits&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wall_J/0/1/0/all/0/1"&gt;Jake Wall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bedetti_A/0/1/0/all/0/1"&gt;Anka Bedetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henley_M/0/1/0/all/0/1"&gt;Michelle Henley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1"&gt;Sara Beery&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Hypercomplex Embeddings for Link Prediction. (arXiv:2106.15230v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15230</id>
        <link href="http://arxiv.org/abs/2106.15230"/>
        <updated>2021-06-30T02:01:02.376Z</updated>
        <summary type="html"><![CDATA[Knowledge graph embedding research has mainly focused on the two smallest
normed division algebras, $\mathbb{R}$ and $\mathbb{C}$. Recent results suggest
that trilinear products of quaternion-valued embeddings can be a more effective
means to tackle link prediction. In addition, models based on convolutions on
real-valued embeddings often yield state-of-the-art results for link
prediction. In this paper, we investigate a composition of convolution
operations with hypercomplex multiplications. We propose the four approaches
QMult, OMult, ConvQ and ConvO to tackle the link prediction problem. QMult and
OMult can be considered as quaternion and octonion extensions of previous
state-of-the-art approaches, including DistMult and ComplEx. ConvQ and ConvO
build upon QMult and OMult by including convolution operations in a way
inspired by the residual learning framework. We evaluated our approaches on
seven link prediction datasets including WN18RR, FB15K-237 and YAGO3-10.
Experimental results suggest that the benefits of learning hypercomplex-valued
vector representations become more apparent as the size and complexity of the
knowledge graph grows. ConvO outperforms state-of-the-art approaches on
FB15K-237 in MRR, Hit@1 and Hit@3, while QMult, OMult, ConvQ and ConvO
outperform state-of-the-approaches on YAGO3-10 in all metrics. Results also
suggest that link prediction performances can be further improved via
prediction averaging. To foster reproducible research, we provide an
open-source implementation of approaches, including training and evaluation
scripts as well as pretrained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1"&gt;Caglar Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moussallem_D/0/1/0/all/0/1"&gt;Diego Moussallem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heindorf_S/0/1/0/all/0/1"&gt;Stefan Heindorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1"&gt;Axel-Cyrille Ngonga Ngomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15292</id>
        <link href="http://arxiv.org/abs/2106.15292"/>
        <updated>2021-06-30T02:01:02.361Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have been shown to be susceptible to memorization
or overfitting in the presence of noisily labelled data. For the problem of
robust learning under such noisy data, several algorithms have been proposed. A
prominent class of algorithms rely on sample selection strategies, motivated by
curriculum learning. For example, many algorithms use the `small loss trick'
wherein a fraction of samples with loss values below a certain threshold are
selected for training. These algorithms are sensitive to such thresholds, and
it is difficult to fix or learn these thresholds. Often, these algorithms also
require information such as label noise rates which are typically unavailable
in practice. In this paper, we propose a data-dependent, adaptive sample
selection strategy that relies only on batch statistics of a given mini-batch
to provide robustness against label noise. The algorithm does not have any
additional hyperparameters for sample selection, does not need any information
on noise rates, and does not need access to separate data with clean labels. We
empirically demonstrate the effectiveness of our algorithm on benchmark
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1"&gt;P.S. Sastry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Rates for Random Order Online Optimization. (arXiv:2106.15207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15207</id>
        <link href="http://arxiv.org/abs/2106.15207"/>
        <updated>2021-06-30T02:01:02.356Z</updated>
        <summary type="html"><![CDATA[We study online convex optimization in the random order model, recently
proposed by \citet{garber2020online}, where the loss functions may be chosen by
an adversary, but are then presented to the online algorithm in a uniformly
random order. Focusing on the scenario where the cumulative loss function is
(strongly) convex, yet individual loss functions are smooth but might be
non-convex, we give algorithms that achieve the optimal bounds and
significantly outperform the results of \citet{garber2020online}, completely
removing the dimension dependence and improving their scaling with respect to
the strong convexity parameter. Our analysis relies on novel connections
between algorithmic stability and generalization for sampling
without-replacement analogous to those studied in the with-replacement
i.i.d.~setting, as well as on a refined average stability analysis of
stochastic gradient descent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sherman_U/0/1/0/all/0/1"&gt;Uri Sherman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Majorization-Minimization for Nonnegative Matrix Factorization with the $\beta$-divergence. (arXiv:2106.15214v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15214</id>
        <link href="http://arxiv.org/abs/2106.15214"/>
        <updated>2021-06-30T02:01:02.350Z</updated>
        <summary type="html"><![CDATA[This article proposes new multiplicative updates for nonnegative matrix
factorization (NMF) with the $\beta$-divergence objective function. Our new
updates are derived from a joint majorization-minimization (MM) scheme, in
which an auxiliary function (a tight upper bound of the objective function) is
built for the two factors jointly and minimized at each iteration. This is in
contrast with the classic approach in which the factors are optimized
alternately and a MM scheme is applied to each factor individually. Like the
classic approach, our joint MM algorithm also results in multiplicative updates
that are simple to implement. They however yield a significant drop of
computation time (for equally good solutions), in particular for some
$\beta$-divergences of important applicative interest, such as the squared
Euclidean distance and the Kullback-Leibler or Itakura-Saito divergences. We
report experimental results using diverse datasets: face images, audio
spectrograms, hyperspectral data and song play counts. Depending on the value
of $\beta$ and on the dataset, our joint MM approach yields a CPU time
reduction of about $10\%$ to $78\%$ in comparison to the classic alternating
scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marmin_A/0/1/0/all/0/1"&gt;Arthur Marmin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goulart_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Henrique de Morais Goulart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric F&amp;#xe9;votte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Static Models for Link Prediction in Temporal Knowledge Graphs. (arXiv:2106.15223v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15223</id>
        <link href="http://arxiv.org/abs/2106.15223"/>
        <updated>2021-06-30T02:01:02.345Z</updated>
        <summary type="html"><![CDATA[The inclusion of temporal scopes of facts in knowledge graph embedding (KGE)
presents significant opportunities for improving the resulting embeddings, and
consequently for increased performance in downstream applications. Yet, little
research effort has focussed on this area and much of the carried out research
reports only marginally improved results compared to models trained without
temporal scopes (static models). Furthermore, rather than leveraging existing
work on static models, they introduce new models specific to temporal knowledge
graphs. We propose a novel perspective that takes advantage of the power of
existing static embedding models by focussing effort on manipulating the data
instead. Our method, SpliMe, draws inspiration from the field of signal
processing and early work in graph embedding. We show that SpliMe competes with
or outperforms the current state of the art in temporal KGE. Additionally, we
uncover issues with the procedure currently used to assess the performance of
static models on temporal graphs and introduce two ways to counteract them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Radstok_W/0/1/0/all/0/1"&gt;Wessel Radstok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chekol_M/0/1/0/all/0/1"&gt;Mel Chekol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Reach Real-Time AI on Consumer Devices? Solutions for Programmable and Custom Architectures. (arXiv:2106.15021v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15021</id>
        <link href="http://arxiv.org/abs/2106.15021"/>
        <updated>2021-06-30T02:01:02.340Z</updated>
        <summary type="html"><![CDATA[The unprecedented performance of deep neural networks (DNNs) has led to large
strides in various Artificial Intelligence (AI) inference tasks, such as object
and speech recognition. Nevertheless, deploying such AI models across commodity
devices faces significant challenges: large computational cost, multiple
performance objectives, hardware heterogeneity and a common need for high
accuracy, together pose critical problems to the deployment of DNNs across the
various embedded and mobile devices in the wild. As such, we have yet to
witness the mainstream usage of state-of-the-art deep learning algorithms
across consumer devices. In this paper, we provide preliminary answers to this
potentially game-changing question by presenting an array of design techniques
for efficient AI systems. We start by examining the major roadblocks when
targeting both programmable processors and custom accelerators. Then, we
present diverse methods for achieving real-time performance following a
cross-stack approach. These span model-, system- and hardware-level techniques,
and their combination. Our findings provide illustrative examples of AI systems
that do not overburden mobile hardware, while also indicating how they can
improve inference accuracy. Moreover, we showcase how custom ASIC- and
FPGA-based accelerators can be an enabling factor for next-generation AI
applications, such as multi-DNN systems. Collectively, these results highlight
the critical need for further exploration as to how the various cross-stack
solutions can be best combined in order to bring the latest advances in deep
learning close to users, in a robust and efficient manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1"&gt;Stylianos I. Venieris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panopoulos_I/0/1/0/all/0/1"&gt;Ioannis Panopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1"&gt;Ilias Leontiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_I/0/1/0/all/0/1"&gt;Iakovos S. Venieris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[INN: A Method Identifying Clean-annotated Samples via Consistency Effect in Deep Neural Networks. (arXiv:2106.15185v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15185</id>
        <link href="http://arxiv.org/abs/2106.15185"/>
        <updated>2021-06-30T02:01:02.334Z</updated>
        <summary type="html"><![CDATA[In many classification problems, collecting massive clean-annotated data is
not easy, and thus a lot of researches have been done to handle data with noisy
labels. Most recent state-of-art solutions for noisy label problems are built
on the small-loss strategy which exploits the memorization effect. While it is
a powerful tool, the memorization effect has several drawbacks. The
performances are sensitive to the choice of a training epoch required for
utilizing the memorization effect. In addition, when the labels are heavily
contaminated or imbalanced, the memorization effect may not occur in which case
the methods based on the small-loss strategy fail to identify clean labeled
data. We introduce a new method called INN(Integration with the Nearest
Neighborhoods) to refine clean labeled data from training data with noisy
labels. The proposed method is based on a new discovery that a prediction
pattern at neighbor regions of clean labeled data is consistently different
from that of noisy labeled data regardless of training epochs. The INN method
requires more computation but is much stable and powerful than the small-loss
strategy. By carrying out various experiments, we demonstrate that the INN
method resolves the shortcomings in the memorization effect successfully and
thus is helpful to construct more accurate deep prediction models with training
data with noisy labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dongha Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yongchan Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kunwoong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yongdai Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Explanations for Arbitrary Regression Models. (arXiv:2106.15212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15212</id>
        <link href="http://arxiv.org/abs/2106.15212"/>
        <updated>2021-06-30T02:01:02.329Z</updated>
        <summary type="html"><![CDATA[We present a new method for counterfactual explanations (CFEs) based on
Bayesian optimisation that applies to both classification and regression
models. Our method is a globally convergent search algorithm with support for
arbitrary regression models and constraints like feature sparsity and
actionable recourse, and furthermore can answer multiple counterfactual
questions in parallel while learning from previous queries. We formulate CFE
search for regression models in a rigorous mathematical framework using
differentiable potentials, which resolves robustness issues in threshold-based
objectives. We prove that in this framework, (a) verifying the existence of
counterfactuals is NP-complete; and (b) that finding instances using such
potentials is CLS-complete. We describe a unified algorithm for CFEs using a
specialised acquisition function that composes both expected improvement and an
exponential-polynomial (EP) family with desirable properties. Our evaluation on
real-world benchmark domains demonstrate high sample-efficiency and precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1"&gt;Thomas Spooner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dervovic_D/0/1/0/all/0/1"&gt;Danial Dervovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1"&gt;Jason Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shepard_J/0/1/0/all/0/1"&gt;Jon Shepard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiahao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1"&gt;Daniele Magazzeni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Distributed Optimization With Randomly Corrupted Gradients. (arXiv:2106.14956v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.14956</id>
        <link href="http://arxiv.org/abs/2106.14956"/>
        <updated>2021-06-30T02:01:02.324Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a first-order distributed optimization algorithm
that is provably robust to Byzantine failures-arbitrary and potentially
adversarial behavior, where all the participating agents are prone to failure.
We model each agent's state over time as a two-state Markov chain that
indicates Byzantine or trustworthy behaviors at different time instants. We set
no restrictions on the maximum number of Byzantine agents at any given time. We
design our method based on three layers of defense: 1) Temporal gradient
averaging, 2) robust aggregation, and 3) gradient normalization. We study two
settings for stochastic optimization, namely Sample Average Approximation and
Stochastic Approximation, and prove that for strongly convex and smooth
non-convex cost functions, our algorithm achieves order-optimal statistical
error and convergence rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Turan_B/0/1/0/all/0/1"&gt;Berkay Turan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Uribe_C/0/1/0/all/0/1"&gt;Cesar A. Uribe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wai_H/0/1/0/all/0/1"&gt;Hoi-To Wai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Alizadeh_M/0/1/0/all/0/1"&gt;Mahnoosh Alizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning for plant microRNA prediction: A systematic review. (arXiv:2106.15159v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2106.15159</id>
        <link href="http://arxiv.org/abs/2106.15159"/>
        <updated>2021-06-30T02:01:02.302Z</updated>
        <summary type="html"><![CDATA[MicroRNAs (miRNAs) are endogenous small non-coding RNAs that play an
important role in post-transcriptional gene regulation. However, the
experimental determination of miRNA sequence and structure is both expensive
and time-consuming. Therefore, computational and machine learning-based
approaches have been adopted to predict novel microRNAs. With the involvement
of data science and machine learning in biology, multiple research studies have
been conducted to find microRNAs with different computational methods and
different miRNA features. Multiple approaches are discussed in detail
considering the learning algorithm/s used, features considered, dataset/s used
and the criteria used in evaluations. This systematic review focuses on the
machine learning methods developed for miRNA identification in plants. This
will help researchers to gain a detailed idea about past studies and identify
novel paths that solve drawbacks occurred in past studies. Our findings
highlight the need for plant-specific computational methods for miRNA
identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Jayasundara_S/0/1/0/all/0/1"&gt;Shyaman Jayasundara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lokuge_S/0/1/0/all/0/1"&gt;Sandali Lokuge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ihalagedara_P/0/1/0/all/0/1"&gt;Puwasuru Ihalagedara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Herath_D/0/1/0/all/0/1"&gt;Damayanthi Herath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding the Effectiveness of Attention Mechanism. (arXiv:2106.15067v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15067</id>
        <link href="http://arxiv.org/abs/2106.15067"/>
        <updated>2021-06-30T02:01:02.295Z</updated>
        <summary type="html"><![CDATA[Attention Mechanism is a widely used method for improving the performance of
convolutional neural networks (CNNs) on computer vision tasks. Despite its
pervasiveness, we have a poor understanding of what its effectiveness stems
from. It is popularly believed that its effectiveness stems from the visual
attention explanation, advocating focusing on the important part of input data
rather than ingesting the entire input. In this paper, we find that there is
only a weak consistency between the attention weights of features and their
importance. Instead, we verify the crucial role of feature map multiplication
in attention mechanism and uncover a fundamental impact of feature map
multiplication on the learned landscapes of CNNs: with the high order
non-linearity brought by the feature map multiplication, it played a
regularization role on CNNs, which made them learn smoother and more stable
landscapes near real samples compared to vanilla CNNs. This smoothness and
stability induce a more predictive and stable behavior in-between real samples,
and make CNNs generate better. Moreover, motivated by the proposed
effectiveness of feature map multiplication, we design feature map
multiplication network (FMMNet) by simply replacing the feature map addition in
ResNet with feature map multiplication. FMMNet outperforms ResNet on various
datasets, and this indicates that feature map multiplication plays a vital role
in improving the performance even without finely designed attention mechanism
in existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zihang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Heng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TUCaN: Progressively Teaching Colourisation to Capsules. (arXiv:2106.15176v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15176</id>
        <link href="http://arxiv.org/abs/2106.15176"/>
        <updated>2021-06-30T02:01:02.285Z</updated>
        <summary type="html"><![CDATA[Automatic image colourisation is the computer vision research path that
studies how to colourise greyscale images (for restoration). Deep learning
techniques improved image colourisation yielding astonishing results. These
differ by various factors, such as structural differences, input types, user
assistance, etc. Most of them, base the architectural structure on
convolutional layers with no emphasis on layers specialised in object features
extraction. We introduce a novel downsampling upsampling architecture named
TUCaN (Tiny UCapsNet) that exploits the collaboration of convolutional layers
and capsule layers to obtain a neat colourisation of entities present in every
single image. This is obtained by enforcing collaboration among such layers by
skip and residual connections. We pose the problem as a per pixel colour
classification task that identifies colours as a bin in a quantized space. To
train the network, in contrast with the standard end to end learning method, we
propose the progressive learning scheme to extract the context of objects by
only manipulating the learning process without changing the model. In this
scheme, the upsampling starts from the reconstruction of low resolution images
and progressively grows to high resolution images throughout the training
phase. Experimental results on three benchmark datasets show that our approach
with ImageNet10k dataset outperforms existing methods on standard quality
metrics and achieves state of the art performances on image colourisation. We
performed a user study to quantify the perceptual realism of the colourisation
results demonstrating: that progressive learning let the TUCaN achieve better
colours than the end to end scheme; and pointing out the limitations of the
existing evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pucci_R/0/1/0/all/0/1"&gt;Rita Pucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinel_N/0/1/0/all/0/1"&gt;Niki Martinel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-domain error minimization for unsupervised domain adaptation. (arXiv:2106.15057v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15057</id>
        <link href="http://arxiv.org/abs/2106.15057"/>
        <updated>2021-06-30T02:01:02.277Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation aims to transfer knowledge from a labeled
source domain to an unlabeled target domain. Previous methods focus on learning
domain-invariant features to decrease the discrepancy between the feature
distributions as well as minimizing the source error and have made remarkable
progress. However, a recently proposed theory reveals that such a strategy is
not sufficient for a successful domain adaptation. It shows that besides a
small source error, both the discrepancy between the feature distributions and
the discrepancy between the labeling functions should be small across domains.
The discrepancy between the labeling functions is essentially the cross-domain
errors which are ignored by existing methods. To overcome this issue, in this
paper, a novel method is proposed to integrate all the objectives into a
unified optimization framework. Moreover, the incorrect pseudo labels widely
used in previous methods can lead to error accumulation during learning. To
alleviate this problem, the pseudo labels are obtained by utilizing structural
information of the target domain besides source classifier and we propose a
curriculum learning based strategy to select the target samples with more
accurate pseudo-labels during training. Comprehensive experiments are
conducted, and the results validate that our approach outperforms
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuntao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_F/0/1/0/all/0/1"&gt;Fengli Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaowen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chongjun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastPitchFormant: Source-filter based Decomposed Modeling for Speech Synthesis. (arXiv:2106.15123v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.15123</id>
        <link href="http://arxiv.org/abs/2106.15123"/>
        <updated>2021-06-30T02:01:02.270Z</updated>
        <summary type="html"><![CDATA[Methods for modeling and controlling prosody with acoustic features have been
proposed for neural text-to-speech (TTS) models. Prosodic speech can be
generated by conditioning acoustic features. However, synthesized speech with a
large pitch-shift scale suffers from audio quality degradation, and speaker
characteristics deformation. To address this problem, we propose a feed-forward
Transformer based TTS model that is designed based on the source-filter theory.
This model, called FastPitchFormant, has a unique structure that handles text
and acoustic features in parallel. With modeling each feature separately, the
tendency that the model learns the relationship between two features can be
mitigated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bak_T/0/1/0/all/0/1"&gt;Taejun Bak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1"&gt;Jae-Sung Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bae_H/0/1/0/all/0/1"&gt;Hanbin Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Ik Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hoon-Young Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constructing Forest Biomass Prediction Maps from Radar Backscatter by Sequential Regression with a Conditional Generative Adversarial Network. (arXiv:2106.15020v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15020</id>
        <link href="http://arxiv.org/abs/2106.15020"/>
        <updated>2021-06-30T02:01:02.247Z</updated>
        <summary type="html"><![CDATA[This paper studies construction of above-ground biomass (AGB) prediction maps
from synthetic aperture radar (SAR) intensity images. The purpose is to improve
traditional regression models based on SAR intensity, trained with a limited
amount of AGB in situ measurements. Although it is costly to collect, data from
airborne laser scanning (ALS) sensors are highly correlated with AGB.
Therefore, we propose using AGB predictions based on ALS data as surrogate
response variables for SAR data in a sequential modelling fashion. This
increases the amount of training data dramatically. To model the regression
function between SAR intensity and ALS-predicted AGB we propose to utilise a
conditional generative adversarial network (cGAN), i.e. the Pix2Pix
convolutional neural network. This enables the recreation of existing ALS-based
AGB prediction maps. The generated synthesised ALS-based AGB predictions are
evaluated qualitatively and quantitatively against ALS-based AGB predictions
retrieved from a traditional non-sequential regression model trained in the
same area. Results show that the proposed architecture manages to capture
characteristics of the actual data. This suggests that the use of ALS-guided
generative models is a promising avenue for AGB prediction from SAR intensity.
Further research on this area has the potential of providing both large-scale
and low-cost predictions of AGB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bjork_S/0/1/0/all/0/1"&gt;Sara Bj&amp;#xf6;rk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anfinsen_S/0/1/0/all/0/1"&gt;Stian Normann Anfinsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naesset_E/0/1/0/all/0/1"&gt;Erik N&amp;#xe6;sset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gobakken_T/0/1/0/all/0/1"&gt;Terje Gobakken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zahabu_E/0/1/0/all/0/1"&gt;Eliakimu Zahabu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Generalisable Deep Inertial Tracking via Geometry-Aware Learning. (arXiv:2106.15178v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15178</id>
        <link href="http://arxiv.org/abs/2106.15178"/>
        <updated>2021-06-30T02:01:02.234Z</updated>
        <summary type="html"><![CDATA[Autonomous navigation in uninstrumented and unprepared environments is a
fundamental demand for next generation indoor and outdoor location-based
services. To bring about such ambition, a suite of collaborative sensing
modalities is required in order to sustain performance irrespective of
challenging dynamic conditions. Of the many modalities on offer, inertial
tracking plays a key role under momentary unfavourable operational conditions
owing to its independence of the surrounding environment. However, inertial
tracking has traditionally (i) suffered from excessive error growth and (ii)
required extensive and cumbersome tuning. Both of these issues have limited the
appeal and utility of inertial tracking. In this paper, we present DIT: a novel
Deep learning Inertial Tracking system that overcomes prior limitations;
namely, by (i) significantly reducing tracking drift and (ii) seamlessly
constructing robust and generalisable learned models. DIT describes two core
contributions: (i) DIT employs a robotic platform augmented with a mechanical
slider subsystem that automatically samples inertial signal variabilities
arising from different sensor mounting geometries. We use the platform to
curate in-house a 7.2 million sample dataset covering an aggregate distance of
21 kilometres split into 11 indexed sensor mounting geometries. (ii) DIT uses
deep learning, optimal transport, and domain adaptation (DA) to create a model
which is robust to variabilities in sensor mounting geometry. The overall
system synthesises high-performance and generalisable inertial navigation
models in an end-to-end, robotic-learning fashion. In our evaluation, DIT
outperforms an industrial-grade sensor fusion baseline by 10x (90th percentile)
and a state-of-the-art adversarial DA technique by > 2.5x in performance (90th
percentile) and >10x in training time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alloulah_M/0/1/0/all/0/1"&gt;Mohammed Alloulah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1"&gt;Maximilian Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isopoussu_A/0/1/0/all/0/1"&gt;Anton Isopoussu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test-Time Adaptation to Distribution Shift by Confidence Maximization and Input Transformation. (arXiv:2106.14999v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.14999</id>
        <link href="http://arxiv.org/abs/2106.14999"/>
        <updated>2021-06-30T02:01:02.221Z</updated>
        <summary type="html"><![CDATA[Deep neural networks often exhibit poor performance on data that is unlikely
under the train-time data distribution, for instance data affected by
corruptions. Previous works demonstrate that test-time adaptation to data
shift, for instance using entropy minimization, effectively improves
performance on such shifted distributions. This paper focuses on the fully
test-time adaptation setting, where only unlabeled data from the target
distribution is required. This allows adapting arbitrary pretrained networks.
Specifically, we propose a novel loss that improves test-time adaptation by
addressing both premature convergence and instability of entropy minimization.
This is achieved by replacing the entropy by a non-saturating surrogate and
adding a diversity regularizer based on batch-wise entropy maximization that
prevents convergence to trivial collapsed solutions. Moreover, we propose to
prepend an input transformation module to the network that can partially undo
test-time distribution shifts. Surprisingly, this preprocessing can be learned
solely using the fully test-time adaptation loss in an end-to-end fashion
without any target domain labels or source domain data. We show that our
approach outperforms previous work in improving the robustness of publicly
available pretrained image classifiers to common corruptions on such
challenging benchmarks as ImageNet-C.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mummadi_C/0/1/0/all/0/1"&gt;Chaithanya Kumar Mummadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hutmacher_R/0/1/0/all/0/1"&gt;Robin Hutmacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rambach_K/0/1/0/all/0/1"&gt;Kilian Rambach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Levinkov_E/0/1/0/all/0/1"&gt;Evgeny Levinkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brox_T/0/1/0/all/0/1"&gt;Thomas Brox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Metzen_J/0/1/0/all/0/1"&gt;Jan Hendrik Metzen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving Statistical Optimality of Federated Learning: Beyond Stationary Points. (arXiv:2106.15216v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15216</id>
        <link href="http://arxiv.org/abs/2106.15216"/>
        <updated>2021-06-30T02:01:02.215Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a promising framework that has great potentials in
privacy preservation and in lowering the computation load at the cloud. FedAvg
and FedProx are two widely adopted algorithms. However, recent work raised
concerns on these two methods: (1) their fixed points do not correspond to the
stationary points of the original optimization problem, and (2) the common
model found might not generalize well locally.

In this paper, we alleviate these concerns. Towards this, we adopt the
statistical learning perspective yet allow the distributions to be
heterogeneous and the local data to be unbalanced. We show, in the general
kernel regression setting, that both FedAvg and FedProx converge to the
minimax-optimal error rates. Moreover, when the kernel function has a finite
rank, the convergence is exponentially fast. Our results further analytically
quantify the impact of the model heterogeneity and characterize the federation
gain - the reduction of the estimation error for a worker to join the federated
learning compared to the best local estimator. To the best of our knowledge, we
are the first to show the achievability of minimax error rates under FedAvg and
FedProx, and the first to characterize the gains in joining FL. Numerical
experiments further corroborate our theoretical findings on the statistical
optimality of FedAvg and FedProx and the federation gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Su_L/0/1/0/all/0/1"&gt;Lili Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiaming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_P/0/1/0/all/0/1"&gt;Pengkun Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis. (arXiv:2106.15153v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.15153</id>
        <link href="http://arxiv.org/abs/2106.15153"/>
        <updated>2021-06-30T02:01:02.210Z</updated>
        <summary type="html"><![CDATA[Recent advances in neural multi-speaker text-to-speech (TTS) models have
enabled the generation of reasonably good speech quality with a single model
and made it possible to synthesize the speech of a speaker with limited
training data. Fine-tuning to the target speaker data with the multi-speaker
model can achieve better quality, however, there still exists a gap compared to
the real speech sample and the model depends on the speaker. In this work, we
propose GANSpeech, which is a high-fidelity multi-speaker TTS model that adopts
the adversarial training method to a non-autoregressive multi-speaker TTS
model. In addition, we propose simple but efficient automatic scaling methods
for feature matching loss used in adversarial training. In the subjective
listening tests, GANSpeech significantly outperformed the baseline
multi-speaker FastSpeech and FastSpeech2 models, and showed a better MOS score
than the speaker-specific fine-tuned FastSpeech2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinhyeok Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1"&gt;Jae-Sung Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bak_T/0/1/0/all/0/1"&gt;Taejun Bak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngik Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hoon-Young Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Image Restoration Network. (arXiv:2008.10796v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10796</id>
        <link href="http://arxiv.org/abs/2008.10796"/>
        <updated>2021-06-30T02:01:02.194Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have achieved significant success in image
restoration tasks by directly learning a powerful non-linear mapping from
corrupted images to their latent clean ones. However, there still exist two
major limitations for these deep learning (DL)-based methods. Firstly, the
noises contained in real corrupted images are very complex, usually neglected
and largely under-estimated in most current methods. Secondly, existing DL
methods are mostly trained on one pre-assumed degradation process for all of
the training image pairs, such as the widely used bicubic downsampling
assumption in the image super-resolution task, inevitably leading to poor
generalization performance when the true degradation does not match with such
assumed one. To address these issues, we propose a unified generative model for
the image restoration, which elaborately configures the degradation process
from the latent clean image to the observed corrupted one. Specifically,
different from most of current methods, the pixel-wisely non-i.i.d. Gaussian
distribution, being with more flexibility, is adopted in our method to fit the
complex real noises. Furthermore, the method is built on the general image
degradation process, making it capable of adapting diverse degradations under
one single model. Besides, we design a variational inference algorithm to learn
all parameters involved in the proposed model with explicit form of objective
loss. Specifically, beyond traditional variational methodology, two DNNs are
employed to parameterize the posteriori distributions, one to infer the
distribution of the latent clean image, and another to infer the distribution
of the image noise. Extensive experiments demonstrate the superiority of the
proposed method on three classical image restoration tasks, including image
denoising, image super-resolution and JPEG image deblocking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yue_Z/0/1/0/all/0/1"&gt;Zongsheng Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yong_H/0/1/0/all/0/1"&gt;Hongwei Yong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trinity: A No-Code AI platform for complex spatial datasets. (arXiv:2106.11756v4 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11756</id>
        <link href="http://arxiv.org/abs/2106.11756"/>
        <updated>2021-06-30T02:01:02.188Z</updated>
        <summary type="html"><![CDATA[We present a no-code Artificial Intelligence (AI) platform called Trinity
with the main design goal of enabling both machine learning researchers and
non-technical geospatial domain experts to experiment with domain-specific
signals and datasets for solving a variety of complex problems on their own.
This versatility to solve diverse problems is achieved by transforming complex
Spatio-temporal datasets to make them consumable by standard deep learning
models, in this case, Convolutional Neural Networks (CNNs), and giving the
ability to formulate disparate problems in a standard way, eg. semantic
segmentation. With an intuitive user interface, a feature store that hosts
derivatives of complex feature engineering, a deep learning kernel, and a
scalable data processing mechanism, Trinity provides a powerful platform for
domain experts to share the stage with scientists and engineers in solving
business-critical problems. It enables quick prototyping, rapid experimentation
and reduces the time to production by standardizing model building and
deployment. In this paper, we present our motivation behind Trinity and its
design along with showcasing sample applications to motivate the idea of
lowering the bar to using AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_C/0/1/0/all/0/1"&gt;C.V.Krishnakumar Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1"&gt;Feili Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Henry Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yonghong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1"&gt;Kay Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Swetava Ganguli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1"&gt;Vipul Pandey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent. (arXiv:2106.15023v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15023</id>
        <link href="http://arxiv.org/abs/2106.15023"/>
        <updated>2021-06-30T02:01:02.175Z</updated>
        <summary type="html"><![CDATA[Evading adversarial example detection defenses requires finding adversarial
examples that must simultaneously (a) be misclassified by the model and (b) be
detected as non-adversarial. We find that existing attacks that attempt to
satisfy multiple simultaneous constraints often over-optimize against one
constraint at the cost of satisfying another. We introduce Orthogonal Projected
Gradient Descent, an improved attack technique to generate adversarial examples
that avoids this problem by orthogonalizing the gradients when running standard
gradient-based attacks. We use our technique to evade four state-of-the-art
detection defenses, reducing their accuracy to 0% while maintaining a 0%
detection rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bryniarski_O/0/1/0/all/0/1"&gt;Oliver Bryniarski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hingun_N/0/1/0/all/0/1"&gt;Nabeel Hingun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pachuca_P/0/1/0/all/0/1"&gt;Pedro Pachuca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_V/0/1/0/all/0/1"&gt;Vincent Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Adder Neural Networks. (arXiv:2105.14202v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14202</id>
        <link href="http://arxiv.org/abs/2105.14202"/>
        <updated>2021-06-30T02:01:02.169Z</updated>
        <summary type="html"><![CDATA[Compared with cheap addition operation, multiplication operation is of much
higher computation complexity. The widely-used convolutions in deep neural
networks are exactly cross-correlation to measure the similarity between input
feature and convolution filters, which involves massive multiplications between
float values. In this paper, we present adder networks (AdderNets) to trade
these massive multiplications in deep neural networks, especially convolutional
neural networks (CNNs), for much cheaper additions to reduce computation costs.
In AdderNets, we take the $\ell_1$-norm distance between filters and input
feature as the output response. We first develop a theoretical foundation for
AdderNets, by showing that both the single hidden layer AdderNet and the
width-bounded deep AdderNet with ReLU activation functions are universal
function approximators. An approximation bound for AdderNets with a single
hidden layer is also presented. We further analyze the influence of this new
similarity measure on the optimization of neural network and develop a special
training scheme for AdderNets. Based on the gradient magnitude, an adaptive
learning rate strategy is proposed to enhance the training procedure of
AdderNets. AdderNets can achieve a 75.7% Top-1 accuracy and a 92.3% Top-5
accuracy using ResNet-50 on the ImageNet dataset without any multiplication in
the convolutional layer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early Mobility Recognition for Intensive Care Unit Patients Using Accelerometers. (arXiv:2106.15017v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15017</id>
        <link href="http://arxiv.org/abs/2106.15017"/>
        <updated>2021-06-30T02:01:02.164Z</updated>
        <summary type="html"><![CDATA[With the development of the Internet of Things(IoT) and Artificial
Intelligence(AI) technologies, human activity recognition has enabled various
applications, such as smart homes and assisted living. In this paper, we target
a new healthcare application of human activity recognition, early mobility
recognition for Intensive Care Unit(ICU) patients. Early mobility is essential
for ICU patients who suffer from long-time immobilization. Our system includes
accelerometer-based data collection from ICU patients and an AI model to
recognize patients' early mobility. To improve the model accuracy and
stability, we identify features that are insensitive to sensor orientations and
propose a segment voting process that leverages a majority voting strategy to
recognize each segment's activity. Our results show that our system improves
model accuracy from 77.78\% to 81.86\% and reduces the model instability
(standard deviation) from 16.69\% to 6.92\%, compared to the same AI model
without our feature engineering and segment voting process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Rex Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazio_S/0/1/0/all/0/1"&gt;Sarina A Fazio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanle Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramli_A/0/1/0/all/0/1"&gt;Albara Ah Ramli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1"&gt;Jason Yeates Adams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FallDeF5: A Fall Detection Framework Using 5G-based Deep Gated Recurrent Unit Networks. (arXiv:2106.15049v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15049</id>
        <link href="http://arxiv.org/abs/2106.15049"/>
        <updated>2021-06-30T02:01:02.149Z</updated>
        <summary type="html"><![CDATA[Fall prevalence is high among elderly people, which is challenging due to the
severe consequences of falling. This is why rapid assistance is a critical
task. Ambient assisted living (AAL) uses recent technologies such as 5G
networks and the internet of medical things (IoMT) to address this research
area. Edge computing can reduce the cost of cloud communication, including high
latency and bandwidth use, by moving conventional healthcare services and
applications closer to end-users. Artificial intelligence (AI) techniques such
as deep learning (DL) have been used recently for automatic fall detection, as
well as supporting healthcare services. However, DL requires a vast amount of
data and substantial processing power to improve its performance for the IoMT
linked to the traditional edge computing environment. This research proposes an
effective fall detection framework based on DL algorithms and mobile edge
computing (MEC) within 5G wireless networks, the aim being to empower
IoMT-based healthcare applications. We also propose the use of a deep gated
recurrent unit (DGRU) neural network to improve the accuracy of existing
DL-based fall detection methods. DGRU has the advantage of dealing with
time-series IoMT data, and it can reduce the number of parameters and avoid the
vanishing gradient problem. The experimental results on two public datasets
show that the DGRU model of the proposed framework achieves higher accuracy
rates compared to the current related works on the same datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Rakhami_M/0/1/0/all/0/1"&gt;Mabrook S. Al-Rakhami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gumaei1_A/0/1/0/all/0/1"&gt;Abdu Gumaei1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Altaf_M/0/1/0/all/0/1"&gt;Meteb Altaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1"&gt;Mohammad Mehedi Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alkhamees_B/0/1/0/all/0/1"&gt;Bader Fahad Alkhamees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muhammad_K/0/1/0/all/0/1"&gt;Khan Muhammad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortino_G/0/1/0/all/0/1"&gt;Giancarlo Fortino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterization of the Variation Spaces Corresponding to Shallow Neural Networks. (arXiv:2106.15002v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15002</id>
        <link href="http://arxiv.org/abs/2106.15002"/>
        <updated>2021-06-30T02:01:02.109Z</updated>
        <summary type="html"><![CDATA[We consider the variation space corresponding to a dictionary of functions in
$L^2(\Omega)$ and present the basic theory of approximation in these spaces.
Specifically, we compare the definition based on integral representations with
the definition in terms of convex hulls. We show that in many cases, including
the dictionaries corresponding to shallow ReLU$^k$ networks and a dictionary of
decaying Fourier modes, that the two definitions coincide. We also give a
partial characterization of the variation space for shallow ReLU$^k$ networks
and show that the variation space with respect to the dictionary of decaying
Fourier modes corresponds to the Barron spectral space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1"&gt;Jonathan W. Siegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinchao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Dynamic Spectrum Access. (arXiv:2106.14976v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.14976</id>
        <link href="http://arxiv.org/abs/2106.14976"/>
        <updated>2021-06-30T02:01:02.101Z</updated>
        <summary type="html"><![CDATA[Due to the growing volume of data traffic produced by the surge of Internet
of Things (IoT) devices, the demand for radio spectrum resources is approaching
their limitation defined by Federal Communications Commission (FCC). To this
end, Dynamic Spectrum Access (DSA) is considered as a promising technology to
handle this spectrum scarcity. However, standard DSA techniques often rely on
analytical modeling wireless networks, making its application intractable in
under-measured network environments. Therefore, utilizing neural networks to
approximate the network dynamics is an alternative approach. In this article,
we introduce a Federated Learning (FL) based framework for the task of DSA,
where FL is a distributive machine learning framework that can reserve the
privacy of network terminals under heterogeneous data distributions. We discuss
the opportunities, challenges, and opening problems of this framework. To
evaluate its feasibility, we implement a Multi-Agent Reinforcement Learning
(MARL)-based FL as a realization associated with its initial evaluation
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yifei Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hao-Hsuan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhou Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jere_S/0/1/0/all/0/1"&gt;Shashank Jere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjia Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Features for training Support Vector Machine. (arXiv:2104.03488v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03488</id>
        <link href="http://arxiv.org/abs/2104.03488"/>
        <updated>2021-06-30T02:01:02.086Z</updated>
        <summary type="html"><![CDATA[Features play a crucial role in computer vision. Initially designed to detect
salient elements by means of handcrafted algorithms, features are now often
learned by different layers in Convolutional Neural Networks (CNNs). This paper
develops a generic computer vision system based on features extracted from
trained CNNs. Multiple learned features are combined into a single structure to
work on different image classification tasks. The proposed system was
experimentally derived by testing several approaches for extracting features
from the inner layers of CNNs and using them as inputs to SVMs that are then
combined by sum rule. Dimensionality reduction techniques are used to reduce
the high dimensionality of inner layers. The resulting vision system is shown
to significantly boost the performance of standard CNNs across a large and
diverse collection of image data sets. An ensemble of different topologies
using the same approach obtains state-of-the-art results on a virus data set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nanni_L/0/1/0/all/0/1"&gt;Loris Nanni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghidoni_S/0/1/0/all/0/1"&gt;Stefano Ghidoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brahnam_S/0/1/0/all/0/1"&gt;Sheryl Brahnam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Training of Neural Lumigraph Representations using Meta Learning. (arXiv:2106.14942v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14942</id>
        <link href="http://arxiv.org/abs/2106.14942"/>
        <updated>2021-06-30T02:01:02.063Z</updated>
        <summary type="html"><![CDATA[Novel view synthesis is a long-standing problem in machine learning and
computer vision. Significant progress has recently been made in developing
neural scene representations and rendering techniques that synthesize
photorealistic images from arbitrary views. These representations, however, are
extremely slow to train and often also slow to render. Inspired by neural
variants of image-based rendering, we develop a new neural rendering approach
with the goal of quickly learning a high-quality representation which can also
be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a
unique combination of a neural shape representation and 2D CNN-based image
feature extraction, aggregation, and re-projection. To push representation
convergence times down to minutes, we leverage meta learning to learn neural
shape and image feature priors which accelerate training. The optimized shape
and image features can then be extracted using traditional graphics techniques
and rendered in real time. We show that MetaNLR++ achieves similar or better
novel view synthesis results in a fraction of the time that competing methods
require.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1"&gt;Alexander W. Bergman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kellnhofer_P/0/1/0/all/0/1"&gt;Petr Kellnhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1"&gt;Gordon Wetzstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ACN: Adversarial Co-training Network for Brain Tumor Segmentation with Missing Modalities. (arXiv:2106.14591v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14591</id>
        <link href="http://arxiv.org/abs/2106.14591"/>
        <updated>2021-06-30T02:01:02.045Z</updated>
        <summary type="html"><![CDATA[Accurate segmentation of brain tumors from magnetic resonance imaging (MRI)
is clinically relevant in diagnoses, prognoses and surgery treatment, which
requires multiple modalities to provide complementary morphological and
physiopathologic information. However, missing modality commonly occurs due to
image corruption, artifacts, different acquisition protocols or allergies to
certain contrast agents in clinical practice. Though existing efforts
demonstrate the possibility of a unified model for all missing situations, most
of them perform poorly when more than one modality is missing. In this paper,
we propose a novel Adversarial Co-training Network (ACN) to solve this issue,
in which a series of independent yet related models are trained dedicated to
each missing situation with significantly better results. Specifically, ACN
adopts a novel co-training network, which enables a coupled learning process
for both full modality and missing modality to supplement each other's domain
and feature representations, and more importantly, to recover the `missing'
information of absent modalities. Then, two unsupervised modules, i.e., entropy
and knowledge adversarial learning modules are proposed to minimize the domain
gap while enhancing prediction reliability and encouraging the alignment of
latent representations, respectively. We also adapt modality-mutual information
knowledge transfer learning to ACN to retain the rich mutual information among
modalities. Extensive experiments on BraTS2018 dataset show that our proposed
method significantly outperforms all state-of-the-art methods under any missing
situation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zihao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tian_J/0/1/0/all/0/1"&gt;Jiang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhong_C/0/1/0/all/0/1"&gt;Cheng Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhongchao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jianping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhiqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unmixing Convolutional Features for Crisp Edge Detection. (arXiv:2011.09808v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09808</id>
        <link href="http://arxiv.org/abs/2011.09808"/>
        <updated>2021-06-30T02:01:02.040Z</updated>
        <summary type="html"><![CDATA[This paper presents a context-aware tracing strategy (CATS) for crisp edge
detection with deep edge detectors, based on an observation that the
localization ambiguity of deep edge detectors is mainly caused by the mixing
phenomenon of convolutional neural networks: feature mixing in edge
classification and side mixing during fusing side predictions. The CATS
consists of two modules: a novel tracing loss that performs feature unmixing by
tracing boundaries for better side edge learning, and a context-aware fusion
block that tackles the side mixing by aggregating the complementary merits of
learned side edges. Experiments demonstrate that the proposed CATS can be
integrated into modern deep edge detectors to improve localization accuracy.
With the vanilla VGG16 backbone, in terms of BSDS500 dataset, our CATS improves
the F-measure (ODS) of the RCF and BDCN deep edge detectors by 12% and 6%
respectively when evaluating without using the morphological non-maximal
suppression scheme for edge detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huan_L/0/1/0/all/0/1"&gt;Linxi Huan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1"&gt;Nan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xianwei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1"&gt;Jianya Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1"&gt;Gui-Song Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepFaceLab: Integrated, flexible and extensible face-swapping framework. (arXiv:2005.05535v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05535</id>
        <link href="http://arxiv.org/abs/2005.05535"/>
        <updated>2021-06-30T02:01:02.035Z</updated>
        <summary type="html"><![CDATA[Deepfake defense not only requires the research of detection but also
requires the efforts of generation methods. However, current deepfake methods
suffer the effects of obscure workflow and poor performance. To solve this
problem, we present DeepFaceLab, the current dominant deepfake framework for
face-swapping. It provides the necessary tools as well as an easy-to-use way to
conduct high-quality face-swapping. It also offers a flexible and loose
coupling structure for people who need to strengthen their pipeline with other
features without writing complicated boilerplate code. We detail the principles
that drive the implementation of DeepFaceLab and introduce its pipeline,
through which every aspect of the pipeline can be modified painlessly by users
to achieve their customization purpose. It is noteworthy that DeepFaceLab could
achieve cinema-quality results with high fidelity. We demonstrate the advantage
of our system by comparing our approach with other face-swapping methods.For
more information, please visit:https://github.com/iperov/DeepFaceLab/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perov_I/0/1/0/all/0/1"&gt;Ivan Perov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1"&gt;Daiheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chervoniy_N/0/1/0/all/0/1"&gt;Nikolay Chervoniy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kunlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marangonda_S/0/1/0/all/0/1"&gt;Sugasa Marangonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ume_C/0/1/0/all/0/1"&gt;Chris Um&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dpfks_M/0/1/0/all/0/1"&gt;Mr. Dpfks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Facenheim_C/0/1/0/all/0/1"&gt;Carl Shift Facenheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+RP_L/0/1/0/all/0/1"&gt;Luis RP&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Pingyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction. (arXiv:2106.15013v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15013</id>
        <link href="http://arxiv.org/abs/2106.15013"/>
        <updated>2021-06-30T02:01:02.030Z</updated>
        <summary type="html"><![CDATA[Recently there has been significant theoretical progress on understanding the
convergence and generalization of gradient-based methods on nonconvex losses
with overparameterized models. Nevertheless, many aspects of optimization and
generalization and in particular the critical role of small random
initialization are not fully understood. In this paper, we take a step towards
demystifying this role by proving that small random initialization followed by
a few iterations of gradient descent behaves akin to popular spectral methods.
We also show that this implicit spectral bias from small random initialization,
which is provably more prominent for overparameterized models, also puts the
gradient descent iterations on a particular trajectory towards solutions that
are not only globally optimal but also generalize well. Concretely, we focus on
the problem of reconstructing a low-rank matrix from a few measurements via a
natural nonconvex formulation. In this setting, we show that the trajectory of
the gradient descent iterations from small random initialization can be
approximately decomposed into three phases: (I) a spectral or alignment phase
where we show that that the iterates have an implicit spectral bias akin to
spectral initialization allowing us to show that at the end of this phase the
column space of the iterates and the underlying low-rank matrix are
sufficiently aligned, (II) a saddle avoidance/refinement phase where we show
that the trajectory of the gradient iterates moves away from certain degenerate
saddle points, and (III) a local refinement phase where we show that after
avoiding the saddles the iterates converge quickly to the underlying low-rank
matrix. Underlying our analysis are insights for the analysis of
overparameterized nonconvex optimization schemes that may have implications for
computational problems beyond low-rank reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stoger_D/0/1/0/all/0/1"&gt;Dominik St&amp;#xf6;ger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1"&gt;Mahdi Soltanolkotabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03746</id>
        <link href="http://arxiv.org/abs/2105.03746"/>
        <updated>2021-06-30T02:01:02.024Z</updated>
        <summary type="html"><![CDATA[Contrastive learning (CL) is effective in learning data representations
without label supervision, where the encoder needs to contrast each positive
sample over multiple negative samples via a one-vs-many softmax cross-entropy
loss. However, conventional CL is sensitive to how many negative samples are
included and how they are selected. Proposed in this paper is a doubly CL
strategy that contrasts positive samples and negative ones within themselves
separately. We realize this strategy with contrastive attraction and
contrastive repulsion (CACR) makes the query not only exert a greater force to
attract more distant positive samples but also do so to repel closer negative
samples. Theoretical analysis reveals the connection between CACR and CL from
the perspectives of both positive attraction and negative repulsion and shows
the benefits in both efficiency and robustness brought by separately
contrasting within the sampled positive and negative pairs. Extensive
large-scale experiments on standard vision tasks show that CACR not only
consistently outperforms existing CL methods on benchmark datasets in
representation learning, but also provides interpretable contrastive weights,
demonstrating the efficacy of the proposed doubly contrastive strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huangjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Applications of Class-wise Robustness in Adversarial Training. (arXiv:2105.14240v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14240</id>
        <link href="http://arxiv.org/abs/2105.14240"/>
        <updated>2021-06-30T02:01:02.009Z</updated>
        <summary type="html"><![CDATA[Adversarial training is one of the most effective approaches to improve model
robustness against adversarial examples. However, previous works mainly focus
on the overall robustness of the model, and the in-depth analysis on the role
of each class involved in adversarial training is still missing. In this paper,
we propose to analyze the class-wise robustness in adversarial training. First,
we provide a detailed diagnosis of adversarial training on six benchmark
datasets, i.e., MNIST, CIFAR-10, CIFAR-100, SVHN, STL-10 and ImageNet.
Surprisingly, we find that there are remarkable robustness discrepancies among
classes, leading to unbalance/unfair class-wise robustness in the robust
models. Furthermore, we keep investigating the relations between classes and
find that the unbalanced class-wise robustness is pretty consistent among
different attack and defense methods. Moreover, we observe that the stronger
attack methods in adversarial learning achieve performance improvement mainly
from a more successful attack on the vulnerable classes (i.e., classes with
less robustness). Inspired by these interesting findings, we design a simple
but effective attack method based on the traditional PGD attack, named
Temperature-PGD attack, which proposes to enlarge the robustness disparity
among classes with a temperature factor on the confidence distribution of each
image. Experiments demonstrate our method can achieve a higher attack rate than
the PGD attack. Furthermore, from the defense perspective, we also make some
modifications in the training and inference phase to improve the robustness of
the most vulnerable class, so as to mitigate the large difference in class-wise
robustness. We believe our work can contribute to a more comprehensive
understanding of adversarial training as well as rethinking the class-wise
properties in robust models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1"&gt;Kun Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1"&gt;Kelu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yisen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Transferability of Adversarial Patches on Face Recognition with Generative Models. (arXiv:2106.15058v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15058</id>
        <link href="http://arxiv.org/abs/2106.15058"/>
        <updated>2021-06-30T02:01:01.998Z</updated>
        <summary type="html"><![CDATA[Face recognition is greatly improved by deep convolutional neural networks
(CNNs). Recently, these face recognition models have been used for identity
authentication in security sensitive applications. However, deep CNNs are
vulnerable to adversarial patches, which are physically realizable and
stealthy, raising new security concerns on the real-world applications of these
models. In this paper, we evaluate the robustness of face recognition models
using adversarial patches based on transferability, where the attacker has
limited accessibility to the target models. First, we extend the existing
transfer-based attack techniques to generate transferable adversarial patches.
However, we observe that the transferability is sensitive to initialization and
degrades when the perturbation magnitude is large, indicating the overfitting
to the substitute models. Second, we propose to regularize the adversarial
patches on the low dimensional data manifold. The manifold is represented by
generative models pre-trained on legitimate human face images. Using face-like
features as adversarial perturbations through optimization on the manifold, we
show that the gaps between the responses of substitute models and the target
models dramatically decrease, exhibiting a better transferability. Extensive
digital world experiments are conducted to demonstrate the superiority of the
proposed method in the black-box setting. We apply the proposed method in the
physical world as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zihao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Chilin Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaolu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetry meets AI. (arXiv:2103.06115v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06115</id>
        <link href="http://arxiv.org/abs/2103.06115"/>
        <updated>2021-06-30T02:01:01.980Z</updated>
        <summary type="html"><![CDATA[We explore whether Neural Networks (NNs) can {\it discover} the presence of
symmetries as they learn to perform a task. For this, we train hundreds of NNs
on a {\it decoy task} based on well-controlled Physics templates, where no
information on symmetry is provided. We use the output from the last hidden
layer of all these NNs, projected to fewer dimensions, as the input for a
symmetry classification task, and show that information on symmetry had indeed
been identified by the original NN without guidance. As an interdisciplinary
application of this procedure, we identify the presence and level of symmetry
in artistic paintings from different styles such as those of Picasso, Pollock
and Van Gogh.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barenboim_G/0/1/0/all/0/1"&gt;Gabriela Barenboim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirn_J/0/1/0/all/0/1"&gt;Johannes Hirn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanz_V/0/1/0/all/0/1"&gt;Veronica Sanz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attaining entropy production and dissipation maps from Brownian movies via neural networks. (arXiv:2106.15108v1 [cond-mat.stat-mech])]]></title>
        <id>http://arxiv.org/abs/2106.15108</id>
        <link href="http://arxiv.org/abs/2106.15108"/>
        <updated>2021-06-30T02:01:01.974Z</updated>
        <summary type="html"><![CDATA[Quantifying entropy production (EP) is essential to understand stochastic
systems at mesoscopic scales, such as living organisms or biological
assemblies. However, without tracking the relevant variables, it is challenging
to figure out where and to what extent EP occurs from recorded time-series
image data from experiments. Here, applying a convolutional neural network
(CNN), a powerful tool for image processing, we develop an estimation method
for EP through an unsupervised learning algorithm that calculates only from
movies. Together with an attention map of the CNN's last layer, our method can
not only quantify stochastic EP but also produce the spatiotemporal pattern of
the EP (dissipation map). We show that our method accurately measures the EP
and creates a dissipation map in two nonequilibrium systems, the bead-spring
model and a network of elastic filaments. We further confirm high performance
even with noisy, low spatial resolution data, and partially observed
situations. Our method will provide a practical way to obtain dissipation maps
and ultimately contribute to uncovering the nonequilibrium nature of complex
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Bae_Y/0/1/0/all/0/1"&gt;Youngkyoung Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dong-Kyum Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Jeong_H/0/1/0/all/0/1"&gt;Hawoong Jeong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Estimation and Coverage Control with Heterogeneous Sensing Information. (arXiv:2106.14984v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.14984</id>
        <link href="http://arxiv.org/abs/2106.14984"/>
        <updated>2021-06-30T02:01:01.969Z</updated>
        <summary type="html"><![CDATA[Heterogeneous multi-robot sensing systems are able to characterize physical
processes more comprehensively than homogeneous systems. Access to multiple
modalities of sensory data allow such systems to fuse information between
complementary sources and learn richer representations of a phenomenon of
interest. Often, these data are correlated but vary in fidelity, i.e., accuracy
(bias) and precision (noise). Low-fidelity data may be more plentiful, while
high-fidelity data may be more trustworthy. In this paper, we address the
problem of multi-robot online estimation and coverage control by combining low-
and high-fidelity data to learn and cover a sensory function of interest. We
propose two algorithms for this task of heterogeneous learning and coverage --
namely Stochastic Sequencing of Multi-fidelity Learning and Coverage (SMLC) and
Deterministic Sequencing of Multi-fidelity Learning and Coverage (DMLC) -- and
prove that they converge asymptotically. In addition, we demonstrate the
empirical efficacy of SMLC and DMLC through numerical simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_A/0/1/0/all/0/1"&gt;Andrew McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Lai Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vaibhav Srivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Diffusion for Dense Depth Estimation from Multi-view Images. (arXiv:2106.08917v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08917</id>
        <link href="http://arxiv.org/abs/2106.08917"/>
        <updated>2021-06-30T02:01:01.953Z</updated>
        <summary type="html"><![CDATA[We present a method to estimate dense depth by optimizing a sparse set of
points such that their diffusion into a depth map minimizes a multi-view
reprojection error from RGB supervision. We optimize point positions, depths,
and weights with respect to the loss by differential splatting that models
points as Gaussians with analytic transmittance. Further, we develop an
efficient optimization routine that can simultaneously optimize the 50k+ points
required for complex scene reconstruction. We validate our routine using ground
truth data and show high reconstruction quality. Then, we apply this to light
field and wider baseline images via self supervision, and show improvements in
both average and outlier error for depth maps diffused from inaccurate sparse
points. Finally, we compare qualitative and quantitative results to image
processing and deep learning methods. this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Numair Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Min H. Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1"&gt;James Tompkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On component interactions in two-stage recommender systems. (arXiv:2106.14979v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.14979</id>
        <link href="http://arxiv.org/abs/2106.14979"/>
        <updated>2021-06-30T02:01:01.947Z</updated>
        <summary type="html"><![CDATA[Thanks to their scalability, two-stage recommenders are used by many of
today's largest online platforms, including YouTube, LinkedIn, and Pinterest.
These systems produce recommendations in two steps: (i) multiple nominators --
tuned for low prediction latency -- preselect a small subset of candidates from
the whole item pool; (ii)~a slower but more accurate ranker further narrows
down the nominated items, and serves to the user. Despite their popularity, the
literature on two-stage recommenders is relatively scarce, and the algorithms
are often treated as the sum of their parts. Such treatment presupposes that
the two-stage performance is explained by the behavior of individual components
if they were deployed independently. This is not the case: using synthetic and
real-world data, we demonstrate that interactions between the ranker and the
nominators substantially affect the overall performance. Motivated by these
findings, we derive a generalization lower bound which shows that careful
choice of each nominator's training set is sometimes the only difference
between a poor and an optimal two-stage recommender. Since searching for a good
choice manually is difficult, we learn one instead. In particular, using a
Mixture-of-Experts approach, we train the nominators (experts) to specialize on
different subsets of the item pool. This significantly improves performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hron_J/0/1/0/all/0/1"&gt;Jiri Hron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1"&gt;Karl Krauth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1"&gt;Niki Kilbertus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks. (arXiv:2106.14997v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.14997</id>
        <link href="http://arxiv.org/abs/2106.14997"/>
        <updated>2021-06-30T02:01:01.941Z</updated>
        <summary type="html"><![CDATA[We consider the approximation rates of shallow neural networks with respect
to the variation norm. Upper bounds on these rates have been established for
sigmoidal and ReLU activation functions, but it has remained an important open
problem whether these rates are sharp. In this article, we provide a solution
to this problem by proving sharp lower bounds on the approximation rates for
shallow neural networks, which are obtained by lower bounding the $L^2$-metric
entropy of the convex hull of the neural network basis functions. In addition,
our methods also give sharp lower bounds on the Kolmogorov $n$-widths of this
convex hull, which show that the variation spaces corresponding to shallow
neural networks cannot be efficiently approximated by linear methods. These
lower bounds apply to both sigmoidal activation functions with bounded
variation and to activation functions which are a power of the ReLU. Our
results also quantify how much stronger the Barron spectral norm is than the
variation norm and, combined with previous results, give the asymptotics of the
$L^\infty$-metric entropy up to logarithmic factors in the case of the ReLU
activation function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1"&gt;Jonathan W. Siegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinchao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Fidelity 3D Digital Human Head Creation from RGB-D Selfies. (arXiv:2010.05562v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05562</id>
        <link href="http://arxiv.org/abs/2010.05562"/>
        <updated>2021-06-30T02:01:01.935Z</updated>
        <summary type="html"><![CDATA[We present a fully automatic system that can produce high-fidelity,
photo-realistic 3D digital human heads with a consumer RGB-D selfie camera. The
system only needs the user to take a short selfie RGB-D video while rotating
his/her head, and can produce a high quality head reconstruction in less than
30 seconds. Our main contribution is a new facial geometry modeling and
reflectance synthesis procedure that significantly improves the
state-of-the-art. Specifically, given the input video a two-stage frame
selection procedure is first employed to select a few high-quality frames for
reconstruction. Then a differentiable renderer based 3D Morphable Model (3DMM)
fitting algorithm is applied to recover facial geometries from multiview RGB-D
data, which takes advantages of a powerful 3DMM basis constructed with
extensive data generation and perturbation. Our 3DMM has much larger expressive
capacities than conventional 3DMM, allowing us to recover more accurate facial
geometry using merely linear basis. For reflectance synthesis, we present a
hybrid approach that combines parametric fitting and CNNs to synthesize
high-resolution albedo/normal maps with realistic hair/pore/wrinkle details.
Results show that our system can produce faithful 3D digital human faces with
extremely realistic details. The main code and the newly constructed 3DMM basis
is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1"&gt;Linchao Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xiangkai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yajing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoxian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1"&gt;Xuefei Zhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1"&gt;Di Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haozhi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xinwei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Dong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyou Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[As easy as APC: Leveraging self-supervised learning in the context of time series classification with varying levels of sparsity and severe class imbalance. (arXiv:2106.15577v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15577</id>
        <link href="http://arxiv.org/abs/2106.15577"/>
        <updated>2021-06-30T02:01:01.929Z</updated>
        <summary type="html"><![CDATA[High levels of sparsity and strong class imbalance are ubiquitous challenges
that are often presented simultaneously in real-world time series data. While
most methods tackle each problem separately, our proposed approach handles both
in conjunction, while imposing fewer assumptions on the data. In this work, we
propose leveraging a self-supervised learning method, specifically
Autoregressive Predictive Coding (APC), to learn relevant hidden
representations of time series data in the context of both missing data and
class imbalance. We apply APC using either a GRU or GRU-D encoder on two
real-world datasets, and show that applying one-step-ahead prediction with APC
improves the classification results in all settings. In fact, by applying GRU-D
- APC, we achieve state-of-the-art AUPRC results on the Physionet benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wever_F/0/1/0/all/0/1"&gt;Fiorella Wever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_T/0/1/0/all/0/1"&gt;T. Anderson Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_V/0/1/0/all/0/1"&gt;Victor Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Symul_L/0/1/0/all/0/1"&gt;Laura Symul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MemX: An Attention-Aware Smart Eyewear System for Personalized Moment Auto-capture. (arXiv:2105.00916v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00916</id>
        <link href="http://arxiv.org/abs/2105.00916"/>
        <updated>2021-06-30T02:01:01.913Z</updated>
        <summary type="html"><![CDATA[This work presents MemX: a biologically-inspired attention-aware eyewear
system developed with the goal of pursuing the long-awaited vision of a
personalized visual Memex. MemX captures human visual attention on the fly,
analyzes the salient visual content, and records moments of personal interest
in the form of compact video snippets. Accurate attentive scene detection and
analysis on resource-constrained platforms is challenging because these tasks
are computation and energy intensive. We propose a new temporal visual
attention network that unifies human visual attention tracking and salient
visual content analysis. Attention tracking focuses computation-intensive video
analysis on salient regions, while video analysis makes human attention
detection and tracking more accurate. Using the YouTube-VIS dataset and 30
participants, we experimentally show that MemX significantly improves the
attention tracking accuracy over the eye-tracking-alone method, while
maintaining high system energy efficiency. We have also conducted 11 in-field
pilot studies across a range of daily usage scenarios, which demonstrate the
feasibility and potential benefits of MemX.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yuhu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yingying Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1"&gt;Mingzhi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yutian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1"&gt;Qin Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1"&gt;Robert P. Dick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tun Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1"&gt;Ning Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1"&gt;Li Shang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild. (arXiv:2106.15610v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15610</id>
        <link href="http://arxiv.org/abs/2106.15610"/>
        <updated>2021-06-30T02:01:01.891Z</updated>
        <summary type="html"><![CDATA[Unsupervised disentanglement has been shown to be theoretically impossible
without inductive biases on the models and the data. As an alternative
approach, recent methods rely on limited supervision to disentangle the factors
of variation and allow their identifiability. While annotating the true
generative factors is only required for a limited number of observations, we
argue that it is infeasible to enumerate all the factors of variation that
describe a real-world image distribution. To this end, we propose a method for
disentangling a set of factors which are only partially labeled, as well as
separating the complementary set of residual factors that are never explicitly
specified. Our success in this challenging setting, demonstrated on synthetic
benchmarks, gives rise to leveraging off-the-shelf image descriptors to
partially annotate a subset of attributes in real image domains (e.g. of human
faces) with minimal manual effort. Specifically, we use a recent language-image
embedding model (CLIP) to annotate a set of attributes of interest in a
zero-shot manner and demonstrate state-of-the-art disentangled image
manipulation results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gabbay_A/0/1/0/all/0/1"&gt;Aviv Gabbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1"&gt;Niv Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1"&gt;Yedid Hoshen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Body Region Classification of MRI and CT examinations. (arXiv:2104.13826v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13826</id>
        <link href="http://arxiv.org/abs/2104.13826"/>
        <updated>2021-06-30T02:01:01.885Z</updated>
        <summary type="html"><![CDATA[Standardized body region labelling of individual images provides data that
can improve human and computer use of medical images. A CNN-based classifier
was developed to identify body regions in CT and MRI. 17 CT (18 MRI) body
regions covering the entire human body were defined for the classification
task. Three retrospective databases were built for the AI model training,
validation, and testing, with a balanced distribution of studies per body
region. The test databases originated from a different healthcare network.
Accuracy, recall and precision of the classifier was evaluated for patient age,
patient gender, institution, scanner manufacturer, contrast, slice thickness,
MRI sequence, and CT kernel. The data included a retrospective cohort of 2,934
anonymized CT cases (training: 1,804 studies, validation: 602 studies, test:
528 studies) and 3,185 anonymized MRI cases (training: 1,911 studies,
validation: 636 studies, test: 638 studies). 27 institutions from primary care
hospitals, community hospitals and imaging centers contributed to the test
datasets. The data included cases of all genders in equal proportions and
subjects aged from a few months old to +90 years old. An image-level prediction
accuracy of 91.9% (90.2 - 92.1) for CT, and 94.2% (92.0 - 95.6) for MRI was
achieved. The classification results were robust across all body regions and
confounding factors. Due to limited data, performance results for subjects
under 10 years-old could not be reliably evaluated. We show that deep learning
models can classify CT and MRI images by body region including lower and upper
extremities with high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Raffy_P/0/1/0/all/0/1"&gt;Philippe Raffy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pambrun_J/0/1/0/all/0/1"&gt;Jean-Fran&amp;#xe7;ois Pambrun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ashish Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dubois_D/0/1/0/all/0/1"&gt;David Dubois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patti_J/0/1/0/all/0/1"&gt;Jay Waldron Patti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cairns_R/0/1/0/all/0/1"&gt;Robyn Alexandra Cairns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Young_R/0/1/0/all/0/1"&gt;Ryan Young&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The U-Net based GLOW for Optical-Flow-free Video Interframe Generation. (arXiv:2103.09576v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09576</id>
        <link href="http://arxiv.org/abs/2103.09576"/>
        <updated>2021-06-30T02:01:01.879Z</updated>
        <summary type="html"><![CDATA[Video frame interpolation is the task of creating an interframe between two
adjacent frames along the time axis. So, instead of simply averaging two
adjacent frames to create an intermediate image, this operation should maintain
semantic continuity with the adjacent frames. Most conventional methods use
optical flow, and various tools such as occlusion handling and object smoothing
are indispensable. Since the use of these various tools leads to complex
problems, we tried to tackle the video interframe generation problem without
using problematic optical flow . To enable this , we have tried to use a deep
neural network with an invertible structure, and developed an U-Net based
Generative Flow which is a modified normalizing flow. In addition, we propose a
learning method with a new consistency loss in the latent space to maintain
semantic temporal consistency between frames. The resolution of the generated
image is guaranteed to be identical to that of the original images by using an
invertible network. Furthermore, as it is not a random image like the ones by
generative models, our network guarantees stable outputs without flicker.
Through experiments, we \sam {confirmed the feasibility of the proposed
algorithm and would like to suggest the U-Net based Generative Flow as a new
possibility for baseline in video frame interpolation. This paper is meaningful
in that it is the world's first attempt to use invertible networks instead of
optical flows for video interpolation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Saem Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Donghoon Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1"&gt;Nojun Kwak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Six-channel Image Representation for Cross-domain Object Detection. (arXiv:2101.00561v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00561</id>
        <link href="http://arxiv.org/abs/2101.00561"/>
        <updated>2021-06-30T02:01:01.863Z</updated>
        <summary type="html"><![CDATA[Most deep learning models are data-driven and the excellent performance is
highly dependent on the abundant and diverse datasets. However, it is very hard
to obtain and label the datasets of some specific scenes or applications. If we
train the detector using the data from one domain, it cannot perform well on
the data from another domain due to domain shift, which is one of the big
challenges of most object detection models. To address this issue, some
image-to-image translation techniques have been employed to generate some fake
data of some specific scenes to train the models. With the advent of Generative
Adversarial Networks (GANs), we could realize unsupervised image-to-image
translation in both directions from a source to a target domain and from the
target to the source domain. In this study, we report a new approach to making
use of the generated images. We propose to concatenate the original 3-channel
images and their corresponding GAN-generated fake images to form 6-channel
representations of the dataset, hoping to address the domain shift problem
while exploiting the success of available detection models. The idea of
augmented data representation may inspire further study on object detection and
other applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianxiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wenchi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanghui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity Embedding Networks for Robust Human Activity Recognition. (arXiv:2106.15283v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15283</id>
        <link href="http://arxiv.org/abs/2106.15283"/>
        <updated>2021-06-30T02:01:01.847Z</updated>
        <summary type="html"><![CDATA[Deep learning models for human activity recognition (HAR) based on sensor
data have been heavily studied recently. However, the generalization ability of
deep models on complex real-world HAR data is limited by the availability of
high-quality labeled activity data, which are hard to obtain. In this paper, we
design a similarity embedding neural network that maps input sensor signals
onto real vectors through carefully designed convolutional and LSTM layers. The
embedding network is trained with a pairwise similarity loss, encouraging the
clustering of samples from the same class in the embedded real space, and can
be effectively trained on a small dataset and even on a noisy dataset with
mislabeled samples. Based on the learned embeddings, we further propose both
nonparametric and parametric approaches for activity recognition. Extensive
evaluation based on two public datasets has shown that the proposed similarity
embedding network significantly outperforms state-of-the-art deep models on HAR
classification tasks, is robust to mislabeled samples in the training set, and
can also be used to effectively denoise a noisy dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1"&gt;Carrie Lu Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xiao Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Lei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jian Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianming Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09008</id>
        <link href="http://arxiv.org/abs/2105.09008"/>
        <updated>2021-06-30T02:01:01.841Z</updated>
        <summary type="html"><![CDATA[In the paper of ExquisiteNetV1, the ability of classification of
ExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and
better model ExquisiteNetV2. We conduct many experiments to evaluate its
performance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known
models on 15 credible datasets under the same condition. According to the
experimental results, ExquisiteNetV2 gets the highest classification accuracy
over half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts
of parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing
speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shyh_Yaw_J/0/1/0/all/0/1"&gt;Jou Shyh-Yaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_Yen_S/0/1/0/all/0/1"&gt;Su Chung-Yen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition. (arXiv:2101.11530v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11530</id>
        <link href="http://arxiv.org/abs/2101.11530"/>
        <updated>2021-06-30T02:01:01.808Z</updated>
        <summary type="html"><![CDATA[We introduce SynSE, a novel syntactically guided generative approach for
Zero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined
generative embedding spaces constrained within and across the involved
modalities (visual, language). The inter-modal constraints are defined between
action sequence embedding and embeddings of Parts of Speech (PoS) tagged words
in the corresponding action description. We deploy SynSE for the task of
skeleton-based action sequence recognition. Our design choices enable SynSE to
generalize compositionally, i.e., recognize sequences whose action descriptions
contain words not encountered during training. We also extend our approach to
the more challenging Generalized Zero-Shot Learning (GZSL) problem via a
confidence-based gating mechanism. We are the first to present zero-shot
skeleton action recognition results on the large-scale NTU-60 and NTU-120
skeleton action datasets with multiple splits. Our results demonstrate SynSE's
state of the art performance in both ZSL and GZSL settings compared to strong
baselines on the NTU-60 and NTU-120 datasets. The code and pretrained models
are available at https://github.com/skelemoa/synse-zsl]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Pranay Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Divyanshu Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Joint Low-light Enhancement and Noise Removal for Raw Images. (arXiv:2106.14844v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14844</id>
        <link href="http://arxiv.org/abs/2106.14844"/>
        <updated>2021-06-30T02:01:01.801Z</updated>
        <summary type="html"><![CDATA[Low-light imaging on mobile devices is typically challenging due to
insufficient incident light coming through the relatively small aperture,
resulting in a low signal-to-noise ratio. Most of the previous works on
low-light image processing focus either only on a single task such as
illumination adjustment, color enhancement, or noise removal; or on a joint
illumination adjustment and denoising task that heavily relies on short-long
exposure image pairs collected from specific camera models, and thus these
approaches are less practical and generalizable in real-world settings where
camera-specific joint enhancement and restoration is required. To tackle this
problem, in this paper, we propose a low-light image processing framework that
performs joint illumination adjustment, color enhancement, and denoising.
Considering the difficulty in model-specific data collection and the ultra-high
definition of the captured images, we design two branches: a coefficient
estimation branch as well as a joint enhancement and denoising branch. The
coefficient estimation branch works in a low-resolution space and predicts the
coefficients for enhancement via bilateral learning, whereas the joint
enhancement and denoising branch works in a full-resolution space and performs
joint enhancement and denoising in a progressive manner. In contrast to
existing methods, our framework does not need to recollect massive data when
being adapted to another camera model, which significantly reduces the efforts
required to fine-tune our approach for practical usage. Through extensive
experiments, we demonstrate its great potential in real-world low-light imaging
applications when compared with current state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yucheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1"&gt;Seung-Won Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information-Theoretic Segmentation by Inpainting Error Maximization. (arXiv:2012.07287v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07287</id>
        <link href="http://arxiv.org/abs/2012.07287"/>
        <updated>2021-06-30T02:01:01.783Z</updated>
        <summary type="html"><![CDATA[We study image segmentation from an information-theoretic perspective,
proposing a novel adversarial method that performs unsupervised segmentation by
partitioning images into maximally independent sets. More specifically, we
group image pixels into foreground and background, with the goal of minimizing
predictability of one set from the other. An easily computed loss drives a
greedy search process to maximize inpainting error over these partitions. Our
method does not involve training deep networks, is computationally cheap,
class-agnostic, and even applicable in isolation to a single unlabeled image.
Experiments demonstrate that it achieves a new state-of-the-art in unsupervised
segmentation quality, while being substantially faster and more general than
competing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_P/0/1/0/all/0/1"&gt;Pedro Savarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sunnie S. Y. Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maire_M/0/1/0/all/0/1"&gt;Michael Maire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1"&gt;Greg Shakhnarovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAllester_D/0/1/0/all/0/1"&gt;David McAllester&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Graph Learning on Visual and Kinematics Embeddings for Accurate Gesture Recognition in Robotic Surgery. (arXiv:2011.01619v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01619</id>
        <link href="http://arxiv.org/abs/2011.01619"/>
        <updated>2021-06-30T02:01:01.777Z</updated>
        <summary type="html"><![CDATA[Automatic surgical gesture recognition is fundamentally important to enable
intelligent cognitive assistance in robotic surgery. With recent advancement in
robot-assisted minimally invasive surgery, rich information including surgical
videos and robotic kinematics can be recorded, which provide complementary
knowledge for understanding surgical gestures. However, existing methods either
solely adopt uni-modal data or directly concatenate multi-modal
representations, which can not sufficiently exploit the informative
correlations inherent in visual and kinematics data to boost gesture
recognition accuracies. In this regard, we propose a novel online approach of
multi-modal relational graph network (i.e., MRG-Net) to dynamically integrate
visual and kinematics information through interactive message propagation in
the latent feature space. In specific, we first extract embeddings from video
and kinematics sequences with temporal convolutional networks and LSTM units.
Next, we identify multi-relations in these multi-modal embeddings and leverage
them through a hierarchical relational graph learning module. The effectiveness
of our method is demonstrated with state-of-the-art results on the public
JIGSAWS dataset, outperforming current uni-modal and multi-modal methods on
both suturing and knot typing tasks. Furthermore, we validated our method on
in-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK)
platforms in two centers, with consistent promising performance achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yonghao Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jie Ying Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1"&gt;Bo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yueming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1"&gt;Mathias Unberath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun-Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdderNet: Do We Really Need Multiplications in Deep Learning?. (arXiv:1912.13200v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.13200</id>
        <link href="http://arxiv.org/abs/1912.13200"/>
        <updated>2021-06-30T02:01:01.771Z</updated>
        <summary type="html"><![CDATA[Compared with cheap addition operation, multiplication operation is of much
higher computation complexity. The widely-used convolutions in deep neural
networks are exactly cross-correlation to measure the similarity between input
feature and convolution filters, which involves massive multiplications between
float values. In this paper, we present adder networks (AdderNets) to trade
these massive multiplications in deep neural networks, especially convolutional
neural networks (CNNs), for much cheaper additions to reduce computation costs.
In AdderNets, we take the $\ell_1$-norm distance between filters and input
feature as the output response. The influence of this new similarity measure on
the optimization of neural network have been thoroughly analyzed. To achieve a
better performance, we develop a special back-propagation approach for
AdderNets by investigating the full-precision gradient. We then propose an
adaptive learning rate strategy to enhance the training procedure of AdderNets
according to the magnitude of each neuron's gradient. As a result, the proposed
AdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50
on the ImageNet dataset without any multiplication in convolution layer. The
codes are publicly available at: https://github.com/huaweinoah/AdderNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Boxin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate speech detection using static BERT embeddings. (arXiv:2106.15537v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15537</id>
        <link href="http://arxiv.org/abs/2106.15537"/>
        <updated>2021-06-30T02:01:01.765Z</updated>
        <summary type="html"><![CDATA[With increasing popularity of social media platforms hate speech is emerging
as a major concern, where it expresses abusive speech that targets specific
group characteristics, such as gender, religion or ethnicity to spread
violence. Earlier people use to verbally deliver hate speeches but now with the
expansion of technology, some people are deliberately using social media
platforms to spread hate by posting, sharing, commenting, etc. Whether it is
Christchurch mosque shootings or hate crimes against Asians in west, it has
been observed that the convicts are very much influenced from hate text present
online. Even though AI systems are in place to flag such text but one of the
key challenges is to reduce the false positive rate (marking non hate as hate),
so that these systems can detect hate speech without undermining the freedom of
expression. In this paper, we use ETHOS hate speech detection dataset and
analyze the performance of hate speech detection classifier by replacing or
integrating the word embeddings (fastText (FT), GloVe (GV) or FT + GV) with
static BERT embeddings (BE). With the extensive experimental trails it is
observed that the neural network performed better with static BE compared to
using FT, GV or FT + GV as word embeddings. In comparison to fine-tuned BERT,
one metric that significantly improved is specificity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajput_G/0/1/0/all/0/1"&gt;Gaurav Rajput&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+punn_N/0/1/0/all/0/1"&gt;Narinder Singh punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Unsupervised Object Representations for Video Sequences. (arXiv:2006.07034v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07034</id>
        <link href="http://arxiv.org/abs/2006.07034"/>
        <updated>2021-06-30T02:01:01.759Z</updated>
        <summary type="html"><![CDATA[Perceiving the world in terms of objects and tracking them through time is a
crucial prerequisite for reasoning and scene understanding. Recently, several
methods have been proposed for unsupervised learning of object-centric
representations. However, since these models were evaluated on different
downstream tasks, it remains unclear how they compare in terms of basic
perceptual abilities such as detection, figure-ground segmentation and tracking
of objects. To close this gap, we design a benchmark with four data sets of
varying complexity and seven additional test sets featuring challenging
tracking scenarios relevant for natural videos. Using this benchmark, we
compare the perceptual abilities of four object-centric approaches: ViMON, a
video-extension of MONet, based on recurrent spatial attention, OP3, which
exploits clustering via spatial mixture models, as well as TBA and SCALOR,
which use explicit factorization via spatial transformers. Our results suggest
that the architectures with unconstrained latent representations learn more
powerful representations in terms of object detection, segmentation and
tracking than the spatial transformer based architectures. We also observe that
none of the methods are able to gracefully handle the most challenging tracking
scenarios despite their synthetic nature, suggesting that our benchmark may
provide fruitful guidance towards learning more robust object-centric video
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weis_M/0/1/0/all/0/1"&gt;Marissa A. Weis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1"&gt;Kashyap Chitta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1"&gt;Yash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Andreas Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ecker_A/0/1/0/all/0/1"&gt;Alexander S. Ecker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal End-to-End Learning for Autonomous Steering in Adverse Road and Weather Conditions. (arXiv:2010.14924v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14924</id>
        <link href="http://arxiv.org/abs/2010.14924"/>
        <updated>2021-06-30T02:01:01.743Z</updated>
        <summary type="html"><![CDATA[Autonomous driving is challenging in adverse road and weather conditions in
which there might not be lane lines, the road might be covered in snow and the
visibility might be poor. We extend the previous work on end-to-end learning
for autonomous steering to operate in these adverse real-life conditions with
multimodal data. We collected 28 hours of driving data in several road and
weather conditions and trained convolutional neural networks to predict the car
steering wheel angle from front-facing color camera images and lidar range and
reflectance data. We compared the CNN model performances based on the different
modalities and our results show that the lidar modality improves the
performances of different multimodal sensor-fusion models. We also performed
on-road tests with different models and they support this observation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maanpaa_J/0/1/0/all/0/1"&gt;Jyri Maanp&amp;#xe4;&amp;#xe4;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taher_J/0/1/0/all/0/1"&gt;Josef Taher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manninen_P/0/1/0/all/0/1"&gt;Petri Manninen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pakola_L/0/1/0/all/0/1"&gt;Leo Pakola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melekhov_I/0/1/0/all/0/1"&gt;Iaroslav Melekhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hyyppa_J/0/1/0/all/0/1"&gt;Juha Hyypp&amp;#xe4;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Framework for an Intelligent Affect Aware Smart Home Environment for Elderly People. (arXiv:2106.15599v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.15599</id>
        <link href="http://arxiv.org/abs/2106.15599"/>
        <updated>2021-06-30T02:01:01.737Z</updated>
        <summary type="html"><![CDATA[The population of elderly people has been increasing at a rapid rate over the
last few decades and their population is expected to further increase in the
upcoming future. Their increasing population is associated with their
increasing needs due to problems like physical disabilities, cognitive issues,
weakened memory and disorganized behavior, that elderly people face with
increasing age. To reduce their financial burden on the world economy and to
enhance their quality of life, it is essential to develop technology-based
solutions that are adaptive, assistive and intelligent in nature. Intelligent
Affect Aware Systems that can not only analyze but also predict the behavior of
elderly people in the context of their day to day interactions with technology
in an IoT-based environment, holds immense potential for serving as a long-term
solution for improving the user experience of elderly in smart homes. This work
therefore proposes the framework for an Intelligent Affect Aware environment
for elderly people that can not only analyze the affective components of their
interactions but also predict their likely user experience even before they
start engaging in any activity in the given smart home environment. This
forecasting of user experience would provide scope for enhancing the same,
thereby increasing the assistive and adaptive nature of such intelligent
systems. To uphold the efficacy of this proposed framework for improving the
quality of life of elderly people in smart homes, it has been tested on three
datasets and the results are presented and discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LB-CNN: An Open Source Framework for Fast Training of Light Binary Convolutional Neural Networks using Chainer and Cupy. (arXiv:2106.15350v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15350</id>
        <link href="http://arxiv.org/abs/2106.15350"/>
        <updated>2021-06-30T02:01:01.731Z</updated>
        <summary type="html"><![CDATA[Light binary convolutional neural networks (LB-CNN) are particularly useful
when implemented in low-energy computing platforms as required in many
industrial applications. Herein, a framework for optimizing compact LB-CNN is
introduced and its effectiveness is evaluated. The framework is freely
available and may run on free-access cloud platforms, thus requiring no major
investments. The optimized model is saved in the standardized .h5 format and
can be used as input to specialized tools for further deployment into specific
technologies, thus enabling the rapid development of various intelligent image
sensors. The main ingredient in accelerating the optimization of our model,
particularly the selection of binary convolution kernels, is the Chainer/Cupy
machine learning library offering significant speed-ups for training the output
layer as an extreme-learning machine. Additional training of the output layer
using Keras/Tensorflow is included, as it allows an increase in accuracy.
Results for widely used datasets including MNIST, GTSRB, ORL, VGG show very
good compromise between accuracy and complexity. Particularly, for face
recognition problems a carefully optimized LB-CNN model provides up to 100%
accuracies. Such TinyML solutions are well suited for industrial applications
requiring image recognition with low energy consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dogaru_R/0/1/0/all/0/1"&gt;Radu Dogaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dogaru_I/0/1/0/all/0/1"&gt;Ioana Dogaru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Driving Implementation in an Experimental Environment. (arXiv:2106.15274v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.15274</id>
        <link href="http://arxiv.org/abs/2106.15274"/>
        <updated>2021-06-30T02:01:01.706Z</updated>
        <summary type="html"><![CDATA[Autonomous systems require identifying the environment and it has a long way
to go before putting it safely into practice. In autonomous driving systems,
the detection of obstacles and traffic lights are of importance as well as lane
tracking. In this study, an autonomous driving system is developed and tested
in the experimental environment designed for this purpose. In this system, a
model vehicle having a camera is used to trace the lanes and avoid obstacles to
experimentally study autonomous driving behavior. Convolutional Neural Network
models were trained for Lane tracking. For the vehicle to avoid obstacles,
corner detection, optical flow, focus of expansion, time to collision, balance
calculation, and decision mechanism were created, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aliyev_N/0/1/0/all/0/1"&gt;Namig Aliyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sezer_O/0/1/0/all/0/1"&gt;Oguzhan Sezer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1"&gt;Mehmet Turan Guzel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepGD: A Deep Learning Framework for Graph Drawing Using GNN. (arXiv:2106.15347v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15347</id>
        <link href="http://arxiv.org/abs/2106.15347"/>
        <updated>2021-06-30T02:01:01.700Z</updated>
        <summary type="html"><![CDATA[In the past decades, many graph drawing techniques have been proposed for
generating aesthetically pleasing graph layouts. However, it remains a
challenging task since different layout methods tend to highlight different
characteristics of the graphs. Recently, studies on deep learning based graph
drawing algorithm have emerged but they are often not generalizable to
arbitrary graphs without re-training. In this paper, we propose a Convolutional
Graph Neural Network based deep learning framework, DeepGD, which can draw
arbitrary graphs once trained. It attempts to generate layouts by compromising
among multiple pre-specified aesthetics considering a good graph layout usually
complies with multiple aesthetics simultaneously. In order to balance the
trade-off, we propose two adaptive training strategies which adjust the weight
factor of each aesthetic dynamically during training. The quantitative and
qualitative assessment of DeepGD demonstrates that it is capable of drawing
arbitrary graphs effectively, while being flexible at accommodating different
aesthetic criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1"&gt;Kevin Yen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yifan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Han-Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15282</id>
        <link href="http://arxiv.org/abs/2106.15282"/>
        <updated>2021-06-30T02:01:01.686Z</updated>
        <summary type="html"><![CDATA[We show that cascaded diffusion models are capable of generating high
fidelity images on the class-conditional ImageNet generation challenge, without
any assistance from auxiliary image classifiers to boost sample quality. A
cascaded diffusion model comprises a pipeline of multiple diffusion models that
generate images of increasing resolution, beginning with a standard diffusion
model at the lowest resolution, followed by one or more super-resolution
diffusion models that successively upsample the image and add higher resolution
details. We find that the sample quality of a cascading pipeline relies
crucially on conditioning augmentation, our proposed method of data
augmentation of the lower resolution conditioning inputs to the
super-resolution models. Our experiments show that conditioning augmentation
prevents compounding error during sampling in a cascaded model, helping us to
train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at
128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1"&gt;Chitwan Saharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1"&gt;William Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1"&gt;Tim Salimans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15281</id>
        <link href="http://arxiv.org/abs/2106.15281"/>
        <updated>2021-06-30T02:01:01.671Z</updated>
        <summary type="html"><![CDATA[In recent years, the growth of Machine Learning algorithms in a variety of
different applications has raised numerous studies on the applicability of
these algorithms in real scenarios. Among all, one of the hardest scenarios,
due to its physical requirements, is the aerospace one. In this context, the
authors of this work aim to propose a first prototype and a study of
feasibility for an AI model to be 'loaded' on board. As a case study, the
authors decided to investigate the detection of volcanic eruptions as a method
to swiftly produce alerts. Two Convolutional Neural Networks have been proposed
and created, also showing how to correctly implement them on real hardware and
how the complexity of a CNN can be adapted to fit computational requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1"&gt;Maria Pia Del Rosso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1"&gt;Alessandro Sebastianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1"&gt;Dario Spiller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1"&gt;Pierre Philippe Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1"&gt;Silvia Liberata Ullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Depth from Semantic Segmentation using Game Engine Dataset. (arXiv:2106.15257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15257</id>
        <link href="http://arxiv.org/abs/2106.15257"/>
        <updated>2021-06-30T02:01:01.665Z</updated>
        <summary type="html"><![CDATA[Depth perception is fundamental for robots to understand the surrounding
environment. As the view of cognitive neuroscience, visual depth perception
methods are divided into three categories, namely binocular, active, and
pictorial. The first two categories have been studied for decades in detail.
However, research for the exploration of the third category is still in its
infancy and has got momentum by the advent of deep learning methods in recent
years. In cognitive neuroscience, it is known that pictorial depth perception
mechanisms are dependent on the perception of seen objects. Inspired by this
fact, in this thesis, we investigated the relation of perception of objects and
depth estimation convolutional neural networks. For this purpose, we developed
new network structures based on a simple depth estimation network that only
used a single image at its input. Our proposed structures use both an image and
a semantic label of the image as their input. We used semantic labels as the
output of object perception. The obtained results of performance comparison
between the developed network and original network showed that our novel
structures can improve the performance of depth estimation by 52\% of relative
error of distance in the examined cases. Most of the experimental studies were
carried out on synthetic datasets that were generated by game engines to
isolate the performance comparison from the effect of inaccurate depth and
semantic labels of non-synthetic datasets. It is shown that particular
synthetic datasets may be used for training of depth networks in cases that an
appropriate dataset is not available. Furthermore, we showed that in these
cases, usage of semantic labels improves the robustness of the network against
domain shift from synthetic training data to non-synthetic test data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kashi_M/0/1/0/all/0/1"&gt;Mohammad Amin Kashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MuViS: Online MU-MIMO Grouping for Multi-User Applications Over Commodity WiFi. (arXiv:2106.15262v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.15262</id>
        <link href="http://arxiv.org/abs/2106.15262"/>
        <updated>2021-06-30T02:01:01.641Z</updated>
        <summary type="html"><![CDATA[Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency
have promised to increase data throughput by allowing concurrent communication
between one Access Point and multiple users. However, we are still a long way
from enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry
applications such as video streaming in practical WiFi network settings due to
heterogeneous channel conditions and devices, unreliable transmissions, and
lack of useful feedback exchange among the lower and upper layers'
requirements. This paper introduces MuViS, a novel dual-phase optimization
framework that proposes a Quality of Experience (QoE) aware MU-MIMO
optimization for multi-user video streaming over IEEE 802.11ac. MuViS first
employs reinforcement learning to optimize the MU-MIMO user group and mode
selection for users based on their PHY/MAC layer characteristics. The video
bitrate is then optimized based on the user's mode (Multi-User (MU) or
Single-User (SU)). We present our design and its evaluation on smartphones and
laptops using 802.11ac WiFi. Our experimental results in various indoor
environments and configurations show a scalable framework that can support a
large number of users with streaming at high video rates and satisfying QoE
requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1"&gt;Hannaneh Barahouei Pasandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1"&gt;Tamer Nadeem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1"&gt;Hadi Amirpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption. (arXiv:2106.15147v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15147</id>
        <link href="http://arxiv.org/abs/2106.15147"/>
        <updated>2021-06-30T02:01:01.606Z</updated>
        <summary type="html"><![CDATA[Self-supervised contrastive representation learning has proved incredibly
successful in the vision and natural language domains, enabling
state-of-the-art performance with orders of magnitude less labeled data.
However, such methods are domain-specific and little has been done to leverage
this technique on real-world tabular datasets. We propose SCARF, a simple,
widely-applicable technique for contrastive learning, where views are formed by
corrupting a random subset of features. When applied to pre-train deep neural
networks on the 69 real-world, tabular classification datasets from the
OpenML-CC18 benchmark, SCARF not only improves classification accuracy in the
fully-supervised setting but does so also in the presence of label noise and in
the semi-supervised setting where only a fraction of the available training
data is labeled. We show that SCARF complements existing strategies and
outperforms alternatives like autoencoders. We conduct comprehensive ablations,
detailing the importance of a range of factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Heinrich Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certifiable Machine Unlearning for Linear Models. (arXiv:2106.15093v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15093</id>
        <link href="http://arxiv.org/abs/2106.15093"/>
        <updated>2021-06-30T02:01:01.591Z</updated>
        <summary type="html"><![CDATA[Machine unlearning is the task of updating machine learning (ML) models after
a subset of the training data they were trained on is deleted. Methods for the
task are desired to combine effectiveness and efficiency, i.e., they should
effectively "unlearn" deleted data, but in a way that does not require
excessive computation effort (e.g., a full retraining) for a small amount of
deletions. Such a combination is typically achieved by tolerating some amount
of approximation in the unlearning. In addition, laws and regulations in the
spirit of "the right to be forgotten" have given rise to requirements for
certifiability, i.e., the ability to demonstrate that the deleted data has
indeed been unlearned by the ML model.

In this paper, we present an experimental study of the three state-of-the-art
approximate unlearning methods for linear models and demonstrate the trade-offs
between efficiency, effectiveness and certifiability offered by each method. In
implementing the study, we extend some of the existing works and describe a
common ML pipeline to compare and evaluate the unlearning methods on six
real-world datasets and a variety of settings. We provide insights into the
effect of the quantity and distribution of the deleted data on ML models and
the performance of each unlearning method in different settings. We also
propose a practical online strategy to determine when the accumulated error
from approximate unlearning is large enough to warrant a full retrain of the ML
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1"&gt;Ananth Mahadevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathioudakis_M/0/1/0/all/0/1"&gt;Michael Mathioudakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Realistic Data Generation Framework leveraging Deep Learning-based Human Digitization. (arXiv:2106.15409v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15409</id>
        <link href="http://arxiv.org/abs/2106.15409"/>
        <updated>2021-06-30T02:01:01.585Z</updated>
        <summary type="html"><![CDATA[The performance of supervised deep learning algorithms depends significantly
on the scale, quality and diversity of the data used for their training.
Collecting and manually annotating large amount of data can be both
time-consuming and costly tasks to perform. In the case of tasks related to
visual human-centric perception, the collection and distribution of such data
may also face restrictions due to legislation regarding privacy. In addition,
the design and testing of complex systems, e.g., robots, which often employ
deep learning-based perception models, may face severe difficulties as even
state-of-the-art methods trained on real and large-scale datasets cannot always
perform adequately as they have not adapted to the visual differences between
the virtual and the real world data. As an attempt to tackle and mitigate the
effect of these issues, we present a method that automatically generates
realistic synthetic data with annotations for a) person detection, b) face
recognition, and c) human pose estimation. The proposed method takes as input
real background images and populates them with human figures in various poses.
Instead of using hand-made 3D human models, we propose the use of models
generated through deep learning methods, further reducing the dataset creation
costs, while maintaining a high level of realism. In addition, we provide
open-source and easy to use tools that implement the proposed pipeline,
allowing for generating highly-realistic synthetic datasets for a variety of
tasks. A benchmarking and evaluation in the corresponding tasks shows that
synthetic data can be effectively used as a supplement to real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Symeonidis_C/0/1/0/all/0/1"&gt;C. Symeonidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nousi_P/0/1/0/all/0/1"&gt;P. Nousi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tosidis_P/0/1/0/all/0/1"&gt;P. Tosidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsampazis_K/0/1/0/all/0/1"&gt;K. Tsampazis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1"&gt;N. Passalis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1"&gt;A. Tefas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1"&gt;N. Nikolaidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Automated Image Descriptions for Visually Impaired Students. (arXiv:2106.15553v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.15553</id>
        <link href="http://arxiv.org/abs/2106.15553"/>
        <updated>2021-06-30T02:01:01.578Z</updated>
        <summary type="html"><![CDATA[Illustrations are widely used in education, and sometimes, alternatives are
not available for visually impaired students. Therefore, those students would
benefit greatly from an automatic illustration description system, but only if
those descriptions were complete, correct, and easily understandable using a
screenreader. In this paper, we report on a study for the assessment of
automated image descriptions. We interviewed experts to establish evaluation
criteria, which we then used to create an evaluation questionnaire for sighted
non-expert raters, and description templates. We used this questionnaire to
evaluate the quality of descriptions which could be generated with a
template-based automatic image describer. We present evidence that these
templates have the potential to generate useful descriptions, and that the
questionnaire identifies problems with description templates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoppe_A/0/1/0/all/0/1"&gt;Anett Hoppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1"&gt;David Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness of Streaming Algorithms through Importance Sampling. (arXiv:2106.14952v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14952</id>
        <link href="http://arxiv.org/abs/2106.14952"/>
        <updated>2021-06-30T02:01:01.573Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce adversarially robust streaming algorithms for
central machine learning and algorithmic tasks, such as regression and
clustering, as well as their more general counterparts, subspace embedding,
low-rank approximation, and coreset construction. For regression and other
numerical linear algebra related tasks, we consider the row arrival streaming
model. Our results are based on a simple, but powerful, observation that many
importance sampling-based algorithms give rise to adversarial robustness which
is in contrast to sketching based algorithms, which are very prevalent in the
streaming literature but suffer from adversarial attacks. In addition, we show
that the well-known merge and reduce paradigm in streaming is adversarially
robust. Since the merge and reduce paradigm allows coreset constructions in the
streaming setting, we thus obtain robust algorithms for $k$-means, $k$-median,
$k$-center, Bregman clustering, projective clustering, principal component
analysis (PCA) and non-negative matrix factorization. To the best of our
knowledge, these are the first adversarially robust results for these problems
yet require no new algorithmic implementations. Finally, we empirically confirm
the robustness of our algorithms on various adversarial attacks and demonstrate
that by contrast, some common existing algorithms are not robust.

(Abstract shortened to meet arXiv limits)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1"&gt;Vladimir Braverman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1"&gt;Avinatan Hassidim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1"&gt;Yossi Matias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schain_M/0/1/0/all/0/1"&gt;Mariano Schain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1"&gt;Sandeep Silwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Samson Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphPiece: Efficiently Generating High-Quality Molecular Graph with Substructures. (arXiv:2106.15098v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15098</id>
        <link href="http://arxiv.org/abs/2106.15098"/>
        <updated>2021-06-30T02:01:01.567Z</updated>
        <summary type="html"><![CDATA[Molecular graph generation is a fundamental but challenging task in various
applications such as drug discovery and material science, which requires
generating valid molecules with desired properties. Auto-regressive models,
which usually construct graphs following sequential actions of adding nodes and
edges at the atom-level, have made rapid progress in recent years. However,
these atom-level models ignore high-frequency subgraphs that not only capture
the regularities of atomic combination in molecules but also are often related
to desired chemical properties. In this paper, we propose a method to
automatically discover such common substructures, which we call {\em graph
pieces}, from given molecular graphs. Based on graph pieces, we leverage a
variational autoencoder to generate molecules in two phases: piece-level graph
generation followed by bond completion. Experiments show that our graph piece
variational autoencoder achieves better performance over state-of-the-art
baselines on property optimization and constrained property optimization tasks
with higher computational efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xiangzhe Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1"&gt;Zhixing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving-Graph Gaussian Processes. (arXiv:2106.15127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15127</id>
        <link href="http://arxiv.org/abs/2106.15127"/>
        <updated>2021-06-30T02:01:01.552Z</updated>
        <summary type="html"><![CDATA[Graph Gaussian Processes (GGPs) provide a data-efficient solution on graph
structured domains. Existing approaches have focused on static structures,
whereas many real graph data represent a dynamic structure, limiting the
applications of GGPs. To overcome this we propose evolving-Graph Gaussian
Processes (e-GGPs). The proposed method is capable of learning the transition
function of graph vertices over time with a neighbourhood kernel to model the
connectivity and interaction changes between vertices. We assess the
performance of our method on time-series regression problems where graphs
evolve over time. We demonstrate the benefits of e-GGPs over static graph
Gaussian Process approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blanco_Mulero_D/0/1/0/all/0/1"&gt;David Blanco-Mulero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1"&gt;Markus Heinonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1"&gt;Ville Kyrki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logic could be learned from images. (arXiv:1908.01931v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.01931</id>
        <link href="http://arxiv.org/abs/1908.01931"/>
        <updated>2021-06-30T02:01:01.546Z</updated>
        <summary type="html"><![CDATA[Logic reasoning is a significant ability of human intelligence and also an
important task in artificial intelligence. The existing logic reasoning
methods, quite often, need to design some reasoning patterns beforehand. This
has led to an interesting question: can logic reasoning patterns be directly
learned from given data? The problem is termed as a data concept logic. In this
study, a learning logic task from images, called a LiLi task, first is
proposed. This task is to learn and reason the logic relation from images,
without presetting any reasoning patterns. As a preliminary exploration, we
design six LiLi data sets (Bitwise And, Bitwise Or, Bitwise Xor, Addition,
Subtraction and Multiplication), in which each image is embedded with a n-digit
number. It is worth noting that a learning model beforehand does not know the
meaning of the n-digit numbers embedded in images and the relation between the
input images and the output image. In order to tackle the task, in this work we
use many typical neural network models and produce fruitful results. However,
these models have the poor performances on the difficult logic task. For
furthermore addressing this task, a novel network framework called a divide and
conquer model by adding some label information is designed, achieving a high
testing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qian Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yuhua Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xinyan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_Y/0/1/0/all/0/1"&gt;Yanhong She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Deyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jiye Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data augmentation for deep learning based accelerated MRI reconstruction with limited data. (arXiv:2106.14947v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14947</id>
        <link href="http://arxiv.org/abs/2106.14947"/>
        <updated>2021-06-30T02:01:01.540Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have emerged as very successful tools for image
restoration and reconstruction tasks. These networks are often trained
end-to-end to directly reconstruct an image from a noisy or corrupted
measurement of that image. To achieve state-of-the-art performance, training on
large and diverse sets of images is considered critical. However, it is often
difficult and/or expensive to collect large amounts of training images.
Inspired by the success of Data Augmentation (DA) for classification problems,
in this paper, we propose a pipeline for data augmentation for accelerated MRI
reconstruction and study its effectiveness at reducing the required training
data in a variety of settings. Our DA pipeline, MRAugment, is specifically
designed to utilize the invariances present in medical imaging measurements as
naive DA strategies that neglect the physics of the problem fail. Through
extensive studies on multiple datasets we demonstrate that in the low-data
regime DA prevents overfitting and can match or even surpass the state of the
art while using significantly fewer training data, whereas in the high-data
regime it has diminishing returns. Furthermore, our findings show that DA can
improve the robustness of the model against various shifts in the test
distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fabian_Z/0/1/0/all/0/1"&gt;Zalan Fabian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heckel_R/0/1/0/all/0/1"&gt;Reinhard Heckel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soltanolkotabi_M/0/1/0/all/0/1"&gt;Mahdi Soltanolkotabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularized OFU: an Efficient UCB Estimator forNon-linear Contextual Bandit. (arXiv:2106.15128v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15128</id>
        <link href="http://arxiv.org/abs/2106.15128"/>
        <updated>2021-06-30T02:01:01.532Z</updated>
        <summary type="html"><![CDATA[Balancing exploration and exploitation (EE) is a fundamental problem in
contex-tual bandit. One powerful principle for EE trade-off isOptimism in Face
of Uncer-tainty(OFU), in which the agent takes the action according to an upper
confidencebound (UCB) of reward. OFU has achieved (near-)optimal regret bound
for lin-ear/kernel contextual bandits. However, it is in general unknown how to
deriveefficient and effective EE trade-off methods for non-linearcomplex tasks,
suchas contextual bandit with deep neural network as the reward function. In
thispaper, we propose a novel OFU algorithm namedregularized OFU(ROFU). InROFU,
we measure the uncertainty of the reward by a differentiable function
andcompute the upper confidence bound by solving a regularized optimization
prob-lem. We prove that, for multi-armed bandit, kernel contextual bandit and
neuraltangent kernel bandit, ROFU achieves (near-)optimal regret bounds with
certainuncertainty measure, which theoretically justifies its effectiveness on
EE trade-off.Importantly, ROFU admits a very efficient implementation with
gradient-basedoptimizer, which easily extends to general deep neural network
models beyondneural tangent kernel, in sharp contrast with previous OFU
methods. The em-pirical evaluation demonstrates that ROFU works extremelywell
for contextualbandits under various settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yichi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shihong Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-learning for Matrix Factorization without Shared Rows or Columns. (arXiv:2106.15133v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15133</id>
        <link href="http://arxiv.org/abs/2106.15133"/>
        <updated>2021-06-30T02:01:01.527Z</updated>
        <summary type="html"><![CDATA[We propose a method that meta-learns a knowledge on matrix factorization from
various matrices, and uses the knowledge for factorizing unseen matrices. The
proposed method uses a neural network that takes a matrix as input, and
generates prior distributions of factorized matrices of the given matrix. The
neural network is meta-learned such that the expected imputation error is
minimized when the factorized matrices are adapted to each matrix by a maximum
a posteriori (MAP) estimation. We use a gradient descent method for the MAP
estimation, which enables us to backpropagate the expected imputation error
through the gradient descent steps for updating neural network parameters since
each gradient descent step is written in a closed form and is differentiable.
The proposed method can meta-learn from matrices even when their rows and
columns are not shared, and their sizes are different from each other. In our
experiments with three user-item rating datasets, we demonstrate that our
proposed method can impute the missing values from a limited number of
observations in unseen matrices after being trained with different matrices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Iwata_T/0/1/0/all/0/1"&gt;Tomoharu Iwata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Cluster Matching for Change Detection of Structures from Satellite Imagery. (arXiv:2103.09787v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09787</id>
        <link href="http://arxiv.org/abs/2103.09787"/>
        <updated>2021-06-30T02:01:01.521Z</updated>
        <summary type="html"><![CDATA[Longitudinal studies are vital to understanding dynamic changes of the
planet, but labels (e.g., buildings, facilities, roads) are often available
only for a single point in time. We propose a general model, Temporal Cluster
Matching (TCM), for detecting building changes in time series of remotely
sensed imagery when footprint labels are observed only once. The intuition
behind the model is that the relationship between spectral values inside and
outside of building's footprint will change when a building is constructed (or
demolished). For instance, in rural settings, the pre-construction area may
look similar to the surrounding environment until the building is constructed.
Similarly, in urban settings, the pre-construction areas will look different
from the surrounding environment until construction. We further propose a
heuristic method for selecting the parameters of our model which allows it to
be applied in novel settings without requiring data labeling efforts (to fit
the parameters). We apply our model over a dataset of poultry barns from
2016/2017 high-resolution aerial imagery in the Delmarva Peninsula and a
dataset of solar farms from a 2020 mosaic of Sentinel 2 imagery in India. Our
results show that our model performs as well when fit using the proposed
heuristic as it does when fit with labeled data, and further, that supervised
versions of our model perform the best among all the baselines we test against.
Finally, we show that our proposed approach can act as an effective data
augmentation strategy -- it enables researchers to augment existing structure
footprint labels along the time dimension and thus use imagery from multiple
points in time to train deep learning models. We show that this improves the
spatial generalization of such models when evaluated on the same change
detection task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1"&gt;Caleb Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1"&gt;Anthony Ortiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan M. Lavista Ferres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1"&gt;Brandon Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1"&gt;Daniel E. Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Multiple Annotators by Incorporating Instance Features. (arXiv:2106.15146v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15146</id>
        <link href="http://arxiv.org/abs/2106.15146"/>
        <updated>2021-06-30T02:01:01.505Z</updated>
        <summary type="html"><![CDATA[Learning from multiple annotators aims to induce a high-quality classifier
from training instances, where each of them is associated with a set of
possibly noisy labels provided by multiple annotators under the influence of
their varying abilities and own biases. In modeling the probability transition
process from latent true labels to observed labels, most existing methods adopt
class-level confusion matrices of annotators that observed labels do not depend
on the instance features, just determined by the true labels. It may limit the
performance that the classifier can achieve. In this work, we propose the noise
transition matrix, which incorporates the influence of instance features on
annotators' performance based on confusion matrices. Furthermore, we propose a
simple yet effective learning framework, which consists of a classifier module
and a noise transition matrix module in a unified neural network architecture.
Experimental results demonstrate the superiority of our method in comparison
with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingzheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hailong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhijun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1"&gt;Renshuai Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yufei Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Multi-View Stereo via Plane Sweep: A Survey. (arXiv:2106.15328v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15328</id>
        <link href="http://arxiv.org/abs/2106.15328"/>
        <updated>2021-06-30T02:01:01.499Z</updated>
        <summary type="html"><![CDATA[3D reconstruction has lately attracted increasing attention due to its wide
application in many areas, such as autonomous driving, robotics and virtual
reality. As a dominant technique in artificial intelligence, deep learning has
been successfully adopted to solve various computer vision problems. However,
deep learning for 3D reconstruction is still at its infancy due to its unique
challenges and varying pipelines. To stimulate future research, this paper
presents a review of recent progress in deep learning methods for Multi-view
Stereo (MVS), which is considered as a crucial task of image-based 3D
reconstruction. It also presents comparative results on several publicly
available datasets, with insightful observations and inspiring future research
directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qingtian Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Waveform Learning Through Joint Optimization of Pulse and Constellation Shaping. (arXiv:2106.15158v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.15158</id>
        <link href="http://arxiv.org/abs/2106.15158"/>
        <updated>2021-06-30T02:01:01.493Z</updated>
        <summary type="html"><![CDATA[As communication systems are foreseen to enable new services such as joint
communication and sensing and utilize parts of the sub-THz spectrum, the design
of novel waveforms that can support these emerging applications becomes
increasingly challenging. We present in this work an end-to-end learning
approach to design waveforms through joint learning of pulse shaping and
constellation geometry, together with a neural network (NN)-based receiver.
Optimization is performed to maximize an achievable information rate, while
satisfying constraints on out-of-band emission and power envelope. Our results
show that the proposed approach enables up to orders of magnitude smaller
adjacent channel leakage ratios (ACLRs) with peak-to-average power ratios
(PAPRs) competitive with traditional filters, without significant loss of
information rate on an additive white Gaussian noise (AWGN) channel, and no
additional complexity at the transmitter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1"&gt;Fay&amp;#xe7;al Ait Aoudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1"&gt;Jakob Hoydis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Evaluation of Domain Adaptation in Facial Expression Recognition. (arXiv:2106.15453v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15453</id>
        <link href="http://arxiv.org/abs/2106.15453"/>
        <updated>2021-06-30T02:01:01.475Z</updated>
        <summary type="html"><![CDATA[Facial Expression Recognition is a commercially important application, but
one common limitation is that applications often require making predictions on
out-of-sample distributions, where target images may have very different
properties from the images that the model was trained on. How well, or badly,
do these models do on unseen target domains? In this paper, we provide a
systematic evaluation of domain adaptation in facial expression recognition.
Using state-of-the-art transfer learning techniques and six commonly-used
facial expression datasets (three collected in the lab and three
"in-the-wild"), we conduct extensive round-robin experiments to examine the
classification accuracies for a state-of-the-art CNN model. We also perform
multi-source experiments where we examine a model's ability to transfer from
multiple source datasets, including (i) within-setting (e.g., lab to lab), (ii)
cross-setting (e.g., in-the-wild to lab), (iii) mixed-setting (e.g., lab and
wild to lab) transfer learning experiments. We find sobering results that the
accuracy of transfer learning is not high, and varies idiosyncratically with
the target dataset, and to a lesser extent the source dataset. Generally, the
best settings for transfer include fine-tuning the weights of a pre-trained
model, and we find that training with more datasets, regardless of setting,
improves transfer performance. We end with a discussion of the need for more --
and regular -- systematic investigations into the generalizability of FER
models, especially for deployed applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1"&gt;Yan San Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_V/0/1/0/all/0/1"&gt;Varsha Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soh_J/0/1/0/all/0/1"&gt;Jonathan Soh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1"&gt;Desmond C. Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Padding in CNNs for Quantitative Susceptibility Mapping. (arXiv:2106.15331v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15331</id>
        <link href="http://arxiv.org/abs/2106.15331"/>
        <updated>2021-06-30T02:01:01.470Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning methods have been proposed for quantitative
susceptibility mapping (QSM) data processing: background field removal,
field-to-source inversion, and single-step QSM reconstruction. However, the
conventional padding mechanism used in convolutional neural networks (CNNs) can
introduce spatial artifacts, especially in QSM background field removal and
single-step QSM which requires inference from total fields with extreme large
values at the edge boundaries of volume of interest. To address this issue, we
propose an improved padding technique which utilizes the neighboring valid
voxels to estimate the invalid voxels of feature maps at volume boundaries in
the neural networks. Studies using simulated and in-vivo data show that the
proposed padding greatly improves estimation accuracy and reduces artifacts in
the results in the tasks of background field removal, field-to-source
inversion, and single-step QSM reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Juan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation with Multiple Acceptable Annotations: A Case Study of Myocardial Segmentation in Contrast Echocardiography. (arXiv:2106.15597v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15597</id>
        <link href="http://arxiv.org/abs/2106.15597"/>
        <updated>2021-06-30T02:01:01.464Z</updated>
        <summary type="html"><![CDATA[Most existing deep learning-based frameworks for image segmentation assume
that a unique ground truth is known and can be used for performance evaluation.
This is true for many applications, but not all. Myocardial segmentation of
Myocardial Contrast Echocardiography (MCE), a critical task in automatic
myocardial perfusion analysis, is an example. Due to the low resolution and
serious artifacts in MCE data, annotations from different cardiologists can
vary significantly, and it is hard to tell which one is the best. In this case,
how can we find a good way to evaluate segmentation performance and how do we
train the neural network? In this paper, we address the first problem by
proposing a new extended Dice to effectively evaluate the segmentation
performance when multiple accepted ground truth is available. Then based on our
proposed metric, we solve the second problem by further incorporating the new
metric into a loss function that enables neural networks to flexibly learn
general features of myocardium. Experiment results on our clinical MCE data set
demonstrate that the neural network trained with the proposed loss function
outperforms those existing ones that try to obtain a unique ground truth from
multiple annotations, both quantitatively and qualitatively. Finally, our
grading study shows that using extended Dice as an evaluation metric can better
identify segmentation results that need manual correction compared with using
Dice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1"&gt;Dewen Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mingqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yukun Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaowei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1"&gt;Qiu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruixue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1"&gt;Hongwen Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Meiping Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jian Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yiyu Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Behavior-aware Graph Convolution Network Model for Video Recommendation. (arXiv:2106.15402v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15402</id>
        <link href="http://arxiv.org/abs/2106.15402"/>
        <updated>2021-06-30T02:01:01.459Z</updated>
        <summary type="html"><![CDATA[Interactions between users and videos are the major data source of performing
video recommendation. Despite lots of existing recommendation methods, user
behaviors on videos, which imply the complex relations between users and
videos, are still far from being fully explored. In the paper, we present a
model named Sagittarius. Sagittarius adopts a graph convolutional neural
network to capture the influence between users and videos. In particular,
Sagittarius differentiates between different user behaviors by weighting and
fuses the semantics of user behaviors into the embeddings of users and videos.
Moreover, Sagittarius combines multiple optimization objectives to learn user
and video embeddings and then achieves the video recommendation by the learned
user and video embeddings. The experimental results on multiple datasets show
that Sagittarius outperforms several state-of-the-art models in terms of
recall, unique recall and NDCG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1"&gt;Wei Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kunchi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1"&gt;Taofeng Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1"&gt;Beihong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Beibei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xinzhou Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;He Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Wenhai Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuejian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuo Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Cattle and Elk in the Wild from Space. (arXiv:2106.15448v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15448</id>
        <link href="http://arxiv.org/abs/2106.15448"/>
        <updated>2021-06-30T02:01:01.454Z</updated>
        <summary type="html"><![CDATA[Localizing and counting large ungulates -- hoofed mammals like cows and elk
-- in very high-resolution satellite imagery is an important task for
supporting ecological studies. Prior work has shown that this is feasible with
deep learning based methods and sub-meter multi-spectral satellite imagery. We
extend this line of work by proposing a baseline method, CowNet, that
simultaneously estimates the number of animals in an image (counts), as well as
predicts their location at a pixel level (localizes). We also propose an
methodology for evaluating such models on counting and localization tasks
across large scenes that takes the uncertainty of noisy labels and the
information needed by stakeholders in ecological monitoring tasks into account.
Finally, we benchmark our baseline method with state of the art vision methods
for counting objects in scenes. We specifically test the temporal
generalization of the resulting models over a large landscape in Point Reyes
Seashore, CA. We find that the LC-FCN model performs the best and achieves an
average precision between 0.56 and 0.61 and an average recall between 0.78 and
0.92 over three held out test scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1"&gt;Caleb Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1"&gt;Anthony Ortiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughey_L/0/1/0/all/0/1"&gt;Lacey Hughey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stabach_J/0/1/0/all/0/1"&gt;Jared A. Stabach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan M. Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement. (arXiv:2106.15413v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15413</id>
        <link href="http://arxiv.org/abs/2106.15413"/>
        <updated>2021-06-30T02:01:01.449Z</updated>
        <summary type="html"><![CDATA[3D semantic scene completion and 2D semantic segmentation are two tightly
correlated tasks that are both essential for indoor scene understanding,
because they predict the same semantic classes, using positively correlated
high-level features. Current methods use 2D features extracted from early-fused
RGB-D images for 2D segmentation to improve 3D scene completion. We argue that
this sequential scheme does not ensure these two tasks fully benefit each
other, and present an Iterative Mutual Enhancement Network (IMENet) to solve
them jointly, which interactively refines the two tasks at the late prediction
stage. Specifically, two refinement modules are developed under a unified
framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP)
module, which receives the projection from the current 3D predictions to refine
the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is
proposed to leverage the reprojected results from 2D predictions to update the
coarse 3D predictions. This iterative fusion happens to the stable high-level
features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD
datasets verify the effectiveness of the proposed iterative late fusion scheme,
and our approach outperforms the state of the art on both 3D semantic scene
completion and 2D semantic segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Laiyan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1"&gt;Rui Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue. (arXiv:2106.15550v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15550</id>
        <link href="http://arxiv.org/abs/2106.15550"/>
        <updated>2021-06-30T02:01:01.444Z</updated>
        <summary type="html"><![CDATA[Building an interactive artificial intelligence that can ask questions about
the real world is one of the biggest challenges for vision and language
problems. In particular, goal-oriented visual dialogue, where the aim of the
agent is to seek information by asking questions during a turn-taking dialogue,
has been gaining scholarly attention recently. While several existing models
based on the GuessWhat?! dataset have been proposed, the Questioner typically
asks simple category-based questions or absolute spatial questions. This might
be problematic for complex scenes where the objects share attributes or in
cases where descriptive questions are required to distinguish objects. In this
paper, we propose a novel Questioner architecture, called Unified Questioner
Transformer (UniQer), for descriptive question generation with referring
expressions. In addition, we build a goal-oriented visual dialogue task called
CLEVR Ask. It synthesizes complex scenes that require the Questioner to
generate descriptive questions. We train our model with two variants of CLEVR
Ask datasets. The results of the quantitative and qualitative evaluations show
that UniQer outperforms the baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matsumori_S/0/1/0/all/0/1"&gt;Shoya Matsumori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shingyouchi_K/0/1/0/all/0/1"&gt;Kosuke Shingyouchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abe_Y/0/1/0/all/0/1"&gt;Yuki Abe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fukuchi_Y/0/1/0/all/0/1"&gt;Yosuke Fukuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1"&gt;Komei Sugiura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imai_M/0/1/0/all/0/1"&gt;Michita Imai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature selection for intrusion detection systems. (arXiv:2106.14941v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.14941</id>
        <link href="http://arxiv.org/abs/2106.14941"/>
        <updated>2021-06-30T02:01:01.439Z</updated>
        <summary type="html"><![CDATA[In this paper, we analyze existing feature selection methods to identify the
key elements of network traffic data that allow intrusion detection. In
addition, we propose a new feature selection method that addresses the
challenge of considering continuous input features and discrete target values.
We show that the proposed method performs well against the benchmark selection
methods. We use our findings to develop a highly effective machine
learning-based detection systems that achieves 99.9% accuracy in distinguishing
between DDoS and benign signals. We believe that our results can be useful to
experts who are interested in designing and building automated intrusion
detection systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamalov_F/0/1/0/all/0/1"&gt;Firuz Kamalov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moussa_S/0/1/0/all/0/1"&gt;Sherif Moussa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zgheib_R/0/1/0/all/0/1"&gt;Rita Zgheib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mashaal_O/0/1/0/all/0/1"&gt;Omar Mashaal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Accurate Road Crack Detection Based on Adaptive Cost-Sensitive Loss Function. (arXiv:2106.15510v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15510</id>
        <link href="http://arxiv.org/abs/2106.15510"/>
        <updated>2021-06-30T02:01:01.426Z</updated>
        <summary type="html"><![CDATA[Numerous detection problems in computer vision, including road crack
detection, suffer from exceedingly foreground-background imbalance.
Fortunately, modification of loss function appears to solve this puzzle once
and for all. In this paper, we propose a pixel-based adaptive weighted
cross-entropy loss in conjunction with Jaccard distance to facilitate
high-quality pixel-level road crack detection. Our work profoundly demonstrates
the influence of loss functions on detection outcomes, and sheds light on the
sophisticated consecutive improvements in the realm of crack detection.
Specifically, to verify the effectiveness of the proposed loss, we conduct
extensive experiments on four public databases, i.e., CrackForest, AigleRN,
Crack360, and BJN260. Compared with the vanilla weighted cross-entropy, the
proposed loss significantly speeds up the training process while retaining the
test accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yingjie Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhiquan Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?. (arXiv:2106.15475v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15475</id>
        <link href="http://arxiv.org/abs/2106.15475"/>
        <updated>2021-06-30T02:01:01.407Z</updated>
        <summary type="html"><![CDATA[Incorrectly labeled examples, or label noise, is common in real-world
computer vision datasets. While the impact of label noise on learning in deep
neural networks has been studied in prior work, these studies have exclusively
focused on homogeneous label noise, i.e., the degree of label noise is the same
across all categories. However, in the real-world, label noise is often
heterogeneous, with some categories being affected to a greater extent than
others. Here, we address this gap in the literature. We hypothesized that
heterogeneous label noise would only affect the classes that had label noise
unless there was transfer from those classes to the classes without label
noise. To test this hypothesis, we designed a series of computer vision studies
using MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous
label noise during the training of multi-class, multi-task, and multi-label
systems. Our results provide evidence in support of our hypothesis: label noise
only affects the class affected by it unless there is transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanal_B/0/1/0/all/0/1"&gt;Bidur Khanal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1"&gt;Christopher Kanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel Medical Imaging for Intelligent Medical Image Analysis: Concepts, Methods, and Applications. (arXiv:1903.04855v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.04855</id>
        <link href="http://arxiv.org/abs/1903.04855"/>
        <updated>2021-06-30T02:01:01.396Z</updated>
        <summary type="html"><![CDATA[There has been much progress in data-driven artificial intelligence
technology for medical image analysis in the last decades. However, it still
remains challenging due to its distinctive complexity of acquiring and
annotating image data, extracting medical domain knowledge, and explaining the
diagnostic decision for medical image analysis. In this paper, we propose a
data-knowledge-driven framework termed as Parallel Medical Imaging (PMI) for
intelligent medical image analysis based on the methodology of interactive
ACP-based parallel intelligence. In the PMI framework, computational
experiments with predictive learning in a data-driven way are conducted to
extract medical knowledge for diagnostic decision support. Artificial imaging
systems are introduced to select and prescriptively generate medical image data
in a knowledge-driven way to utilize medical domain knowledge. Through the
closed-loop optimization based on parallel execution, our proposed PMI
framework can boost the generalization ability and alleviate the limitation of
medical interpretation for diagnostic decisions. Furthermore, we illustrate the
preliminary implementation of PMI method through the case studies of mammogram
analysis and skin lesion image analysis. Experimental results on several public
medical image datasets demonstrate the effectiveness of proposed PMI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gou_C/0/1/0/all/0/1"&gt;Chao Gou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1"&gt;Tianyu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenbo Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Huadan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hui Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1"&gt;Qiang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhengyu Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei-Yue Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement. (arXiv:2106.15575v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.15575</id>
        <link href="http://arxiv.org/abs/2106.15575"/>
        <updated>2021-06-30T02:01:01.380Z</updated>
        <summary type="html"><![CDATA[Deep neural networks for image quality enhancement typically need large
quantities of highly-curated training data comprising pairs of low-quality
images and their corresponding high-quality images. While high-quality image
acquisition is typically expensive and time-consuming, medium-quality images
are faster to acquire, at lower equipment costs, and available in larger
quantities. Thus, we propose a novel generative adversarial network (GAN) that
can leverage training data at multiple levels of quality (e.g., high and medium
quality) to improve performance while limiting costs of data curation. We apply
our mixed-supervision GAN to (i) super-resolve histopathology images and (ii)
enhance laparoscopy images by combining super-resolution and surgical smoke
removal. Results on large clinical and pre-clinical datasets show the benefits
of our mixed-supervision GAN over the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1"&gt;Uddeshya Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1"&gt;Suyash Awate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Framework for Spectral Dimensionality Reduction, Maximum Variance Unfolding, and Kernel Learning By Semidefinite Programming: Tutorial and Survey. (arXiv:2106.15379v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.15379</id>
        <link href="http://arxiv.org/abs/2106.15379"/>
        <updated>2021-06-30T02:01:01.367Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper on unification of spectral dimensionality
reduction methods, kernel learning by Semidefinite Programming (SDP), Maximum
Variance Unfolding (MVU) or Semidefinite Embedding (SDE), and its variants. We
first explain how the spectral dimensionality reduction methods can be unified
as kernel Principal Component Analysis (PCA) with different kernels. This
unification can be interpreted as eigenfunction learning or representation of
kernel in terms of distance matrix. Then, since the spectral methods are
unified as kernel PCA, we say let us learn the best kernel for unfolding the
manifold of data to its maximum variance. We first briefly introduce kernel
learning by SDP for the transduction task. Then, we explain MVU in detail.
Various versions of supervised MVU using nearest neighbors graph, by class-wise
unfolding, by Fisher criterion, and by colored MVU are explained. We also
explain out-of-sample extension of MVU using eigenfunctions and kernel mapping.
Finally, we introduce other variants of MVU including action respecting
embedding, relaxed MVU, and landmark MVU for big data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Guided Progressive GANs for Medical Image Translation. (arXiv:2106.15542v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15542</id>
        <link href="http://arxiv.org/abs/2106.15542"/>
        <updated>2021-06-30T02:01:01.361Z</updated>
        <summary type="html"><![CDATA[Image-to-image translation plays a vital role in tackling various medical
imaging tasks such as attenuation correction, motion correction, undersampled
reconstruction, and denoising. Generative adversarial networks have been shown
to achieve the state-of-the-art in generating high fidelity images for these
tasks. However, the state-of-the-art GAN-based frameworks do not estimate the
uncertainty in the predictions made by the network that is essential for making
informed medical decisions and subsequent revision by medical experts and has
recently been shown to improve the performance and interpretability of the
model. In this work, we propose an uncertainty-guided progressive learning
scheme for image-to-image translation. By incorporating aleatoric uncertainty
as attention maps for GANs trained in a progressive manner, we generate images
of increasing fidelity progressively. We demonstrate the efficacy of our model
on three challenging medical image translation tasks, including PET to CT
translation, undersampled MRI reconstruction, and MRI motion artefact
correction. Our model generalizes well in three different tasks and improves
performance over state of the art under full-supervision and weak-supervision
with limited data. Code is released here:
https://github.com/ExplainableML/UncerGuidedI2I]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1"&gt;Uddeshya Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanbei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hepp_T/0/1/0/all/0/1"&gt;Tobias Hepp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1"&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1"&gt;Zeynep Akata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wasserstein Adversarial Regularization (WAR) on label noise. (arXiv:1904.03936v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.03936</id>
        <link href="http://arxiv.org/abs/1904.03936"/>
        <updated>2021-06-30T02:01:01.347Z</updated>
        <summary type="html"><![CDATA[Noisy labels often occur in vision datasets, especially when they are
obtained from crowdsourcing or Web scraping. We propose a new regularization
method, which enables learning robust classifiers in presence of noisy data. To
achieve this goal, we propose a new adversarial regularization scheme based on
the Wasserstein distance. Using this distance allows taking into account
specific relations between classes by leveraging the geometric properties of
the labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a
selective regularization, which promotes smoothness of the classifier between
some classes, while preserving sufficient complexity of the decision boundary
between others. We first discuss how and why adversarial regularization can be
used in the context of label noise and then show the effectiveness of our
method on five datasets corrupted with noisy labels: in both benchmarks and
real datasets, WAR outperforms the state-of-the-art competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fatras_K/0/1/0/all/0/1"&gt;Kilian Fatras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1"&gt;Bharath Bhushan Damodaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobry_S/0/1/0/all/0/1"&gt;Sylvain Lobry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1"&gt;Devis Tuia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1"&gt;Nicolas Courty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Winner Team Mia at TextVQA Challenge 2021: Vision-and-Language Representation Learning with Pre-trained Sequence-to-Sequence Model. (arXiv:2106.15332v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15332</id>
        <link href="http://arxiv.org/abs/2106.15332"/>
        <updated>2021-06-30T02:01:01.341Z</updated>
        <summary type="html"><![CDATA[TextVQA requires models to read and reason about text in images to answer
questions about them. Specifically, models need to incorporate a new modality
of text present in the images and reason over it to answer TextVQA questions.
In this challenge, we use generative model T5 for TextVQA task. Based on
pre-trained checkpoint T5-3B from HuggingFace repository, two other
pre-training tasks including masked language modeling(MLM) and relative
position prediction(RPP) are designed to better align object feature and scene
text. In the stage of pre-training, encoder is dedicate to handle the fusion
among multiple modalities: question text, object text labels, scene text
labels, object visual features, scene visual features. After that decoder
generates the text sequence step-by-step, cross entropy loss is required by
default. We use a large-scale scene text dataset in pre-training and then
fine-tune the T5-3B with the TextVQA dataset only.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yixuan Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yihao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xianbin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xianbiao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1"&gt;Guotong Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-Stage Self-Supervised Cycle-Consistency Network for Reconstruction of Thin-Slice MR Images. (arXiv:2106.15395v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.15395</id>
        <link href="http://arxiv.org/abs/2106.15395"/>
        <updated>2021-06-30T02:01:01.336Z</updated>
        <summary type="html"><![CDATA[The thick-slice magnetic resonance (MR) images are often structurally blurred
in coronal and sagittal views, which causes harm to diagnosis and image
post-processing. Deep learning (DL) has shown great potential to re-construct
the high-resolution (HR) thin-slice MR images from those low-resolution (LR)
cases, which we refer to as the slice interpolation task in this work. However,
since it is generally difficult to sample abundant paired LR-HR MR images, the
classical fully supervised DL-based models cannot be effectively trained to get
robust performance. To this end, we propose a novel Two-stage Self-supervised
Cycle-consistency Network (TSCNet) for MR slice interpolation, in which a
two-stage self-supervised learning (SSL) strategy is developed for unsupervised
DL network training. The paired LR-HR images are synthesized along the sagittal
and coronal directions of input LR images for network pretraining in the
first-stage SSL, and then a cyclic in-terpolation procedure based on triplet
axial slices is designed in the second-stage SSL for further refinement. More
training samples with rich contexts along all directions are exploited as
guidance to guarantee the improved in-terpolation performance. Moreover, a new
cycle-consistency constraint is proposed to supervise this cyclic procedure,
which encourages the network to reconstruct more realistic HR images. The
experimental results on a real MRI dataset indicate that TSCNet achieves
superior performance over the conventional and other SSL-based algorithms, and
obtains competitive quali-tative and quantitative results compared with the
fully supervised algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiyang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+shi_J/0/1/0/all/0/1"&gt;Jun shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1"&gt;Dinggang Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Graph Learning for Scalable Multi-view Clustering. (arXiv:2106.15382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15382</id>
        <link href="http://arxiv.org/abs/2106.15382"/>
        <updated>2021-06-30T02:01:01.328Z</updated>
        <summary type="html"><![CDATA[Graph-based multi-view clustering has become an active topic due to the
efficiency in characterizing both the complex structure and relationship
between multimedia data. However, existing methods have the following
shortcomings: (1) They are inefficient or even fail for graph learning in large
scale due to the graph construction and eigen-decomposition. (2) They cannot
well exploit both the complementary information and spatial structure embedded
in graphs of different views. To well exploit complementary information and
tackle the scalability issue plaguing graph-based multi-view clustering, we
propose an efficient multiple graph learning model via a small number of anchor
points and tensor Schatten p-norm minimization. Specifically, we construct a
hidden and tractable large graph by anchor graph for each view and well exploit
complementary information embedded in anchor graphs of different views by
tensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,
which scales linearly with the data size, to solve our proposed model.
Extensive experimental results on several datasets indicate that our proposed
method outperforms some state-of-the-art multi-view clustering algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tianyu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Quanxue Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TANet++: Triple Attention Network with Filtered Pointcloud on 3D Detection. (arXiv:2106.15366v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15366</id>
        <link href="http://arxiv.org/abs/2106.15366"/>
        <updated>2021-06-30T02:01:01.313Z</updated>
        <summary type="html"><![CDATA[TANet is one of state-of-the-art 3D object detection method on KITTI and JRDB
benchmark, the network contains a Triple Attention module and Coarse-to-Fine
Regression module to improve the robustness and accuracy of 3D Detection.
However, since the original input data (point clouds) contains a lot of noise
during collecting the data, which will further affect the training of the
model. For example, the object is far from the robot, the sensor is difficult
to obtain enough pointcloud. If the objects only contains few point clouds, and
the samples are fed into model with the normal samples together during
training, the detector will be difficult to distinguish the individual with few
pointcloud belong to object or background. In this paper, we propose TANet++ to
improve the performance on 3D Detection, which adopt a novel training strategy
on training the TANet. In order to reduce the negative impact by the weak
samples, the training strategy previously filtered the training data, and then
the TANet++ is trained by the rest of data. The experimental results shows that
AP score of TANet++ is 8.98\% higher than TANet on JRDB benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Cong Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where is the disease? Semi-supervised pseudo-normality synthesis from an abnormal image. (arXiv:2106.15345v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15345</id>
        <link href="http://arxiv.org/abs/2106.15345"/>
        <updated>2021-06-30T02:01:01.307Z</updated>
        <summary type="html"><![CDATA[Pseudo-normality synthesis, which computationally generates a pseudo-normal
image from an abnormal one (e.g., with lesions), is critical in many
perspectives, from lesion detection, data augmentation to clinical surgery
suggestion. However, it is challenging to generate high-quality pseudo-normal
images in the absence of the lesion information. Thus, expensive lesion
segmentation data have been introduced to provide lesion information for the
generative models and improve the quality of the synthetic images. In this
paper, we aim to alleviate the need of a large amount of lesion segmentation
data when generating pseudo-normal images. We propose a Semi-supervised Medical
Image generative LEarning network (SMILE) which not only utilizes limited
medical images with segmentation masks, but also leverages massive medical
images without segmentation masks to generate realistic pseudo-normal images.
Extensive experiments show that our model outperforms the best state-of-the-art
model by up to 6% for data augmentation task and 3% in generating high-quality
images. Moreover, the proposed semi-supervised learning achieves comparable
medical image synthesis quality with supervised learning model, using only 50
of segmentation data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuanqi Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1"&gt;Quan Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1"&gt;Hu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;S. Kevin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spiking-GAN: A Spiking Generative Adversarial Network Using Time-To-First-Spike Coding. (arXiv:2106.15420v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.15420</id>
        <link href="http://arxiv.org/abs/2106.15420"/>
        <updated>2021-06-30T02:01:01.302Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have shown great potential in solving deep
learning problems in an energy-efficient manner. However, they are still
limited to simple classification tasks. In this paper, we propose Spiking-GAN,
the first spike-based Generative Adversarial Network (GAN). It employs a kind
of temporal coding scheme called time-to-first-spike coding. We train it using
approximate backpropagation in the temporal domain. We use simple
integrate-and-fire (IF) neurons with very high refractory period for our
network which ensures a maximum of one spike per neuron. This makes the model
much sparser than a spike rate-based system. Our modified temporal loss
function called 'Aggressive TTFS' improves the inference time of the network by
over 33% and reduces the number of spikes in the network by more than 11%
compared to previous works. Our experiments show that on training the network
on the MNIST dataset using this approach, we can generate high quality samples.
Thereby demonstrating the potential of this framework for solving such problems
in the spiking domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kotariya_V/0/1/0/all/0/1"&gt;Vineet Kotariya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguly_U/0/1/0/all/0/1"&gt;Udayan Ganguly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-stage Optimization based Adversarial Training. (arXiv:2106.15357v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15357</id>
        <link href="http://arxiv.org/abs/2106.15357"/>
        <updated>2021-06-30T02:01:01.286Z</updated>
        <summary type="html"><![CDATA[In the field of adversarial robustness, there is a common practice that
adopts the single-step adversarial training for quickly developing
adversarially robust models. However, the single-step adversarial training is
most likely to cause catastrophic overfitting, as after a few training epochs
it will be hard to generate strong adversarial examples to continuously boost
the adversarial robustness. In this work, we aim to avoid the catastrophic
overfitting by introducing multi-step adversarial examples during the
single-step adversarial training. Then, to balance the large training overhead
of generating multi-step adversarial examples, we propose a Multi-stage
Optimization based Adversarial Training (MOAT) method that periodically trains
the model on mixed benign examples, single-step adversarial examples, and
multi-step adversarial examples stage by stage. In this way, the overall
training overhead is reduced significantly, meanwhile, the model could avoid
catastrophic overfitting. Extensive experiments on CIFAR-10 and CIFAR-100
datasets demonstrate that under similar amount of training overhead, the
proposed MOAT exhibits better robustness than either single-step or multi-step
adversarial training methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chuanbiao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"&gt;Kun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LB-CNN: An Open Source Framework for Fast Training of Light Binary Convolutional Neural Networks using Chainer and Cupy. (arXiv:2106.15350v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15350</id>
        <link href="http://arxiv.org/abs/2106.15350"/>
        <updated>2021-06-30T02:01:01.276Z</updated>
        <summary type="html"><![CDATA[Light binary convolutional neural networks (LB-CNN) are particularly useful
when implemented in low-energy computing platforms as required in many
industrial applications. Herein, a framework for optimizing compact LB-CNN is
introduced and its effectiveness is evaluated. The framework is freely
available and may run on free-access cloud platforms, thus requiring no major
investments. The optimized model is saved in the standardized .h5 format and
can be used as input to specialized tools for further deployment into specific
technologies, thus enabling the rapid development of various intelligent image
sensors. The main ingredient in accelerating the optimization of our model,
particularly the selection of binary convolution kernels, is the Chainer/Cupy
machine learning library offering significant speed-ups for training the output
layer as an extreme-learning machine. Additional training of the output layer
using Keras/Tensorflow is included, as it allows an increase in accuracy.
Results for widely used datasets including MNIST, GTSRB, ORL, VGG show very
good compromise between accuracy and complexity. Particularly, for face
recognition problems a carefully optimized LB-CNN model provides up to 100%
accuracies. Such TinyML solutions are well suited for industrial applications
requiring image recognition with low energy consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dogaru_R/0/1/0/all/0/1"&gt;Radu Dogaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dogaru_I/0/1/0/all/0/1"&gt;Ioana Dogaru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Attention for Interactive Segmentation. (arXiv:2106.15338v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15338</id>
        <link href="http://arxiv.org/abs/2106.15338"/>
        <updated>2021-06-30T02:01:01.260Z</updated>
        <summary type="html"><![CDATA[We provide a probabilistic interpretation of attention and show that the
standard dot-product attention in transformers is a special case of Maximum A
Posteriori (MAP) inference. The proposed approach suggests the use of
Expectation Maximization algorithms for online adaptation of key and value
model parameters. This approach is useful for cases in which external agents,
e.g., annotators, provide inference-time information about the correct values
of some tokens, e.g, the semantic category of some pixels, and we need for this
new information to propagate to other tokens in a principled manner. We
illustrate the approach on an interactive semantic segmentation task in which
annotators and models collaborate online to improve annotation efficiency.
Using standard benchmarks, we observe that key adaptation boosts model
performance ($\sim10\%$ mIoU) in the low feedback regime and value propagation
improves model responsiveness in the high feedback regime. A PyTorch layer
implementation of our probabilistic attention model will be made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gabbur_P/0/1/0/all/0/1"&gt;Prasad Gabbur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilkhu_M/0/1/0/all/0/1"&gt;Manjot Bilkhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Movellan_J/0/1/0/all/0/1"&gt;Javier Movellan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Study of visual processing techniques for dynamic speckles: a comparative analysis. (arXiv:2106.15507v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15507</id>
        <link href="http://arxiv.org/abs/2106.15507"/>
        <updated>2021-06-30T02:01:01.254Z</updated>
        <summary type="html"><![CDATA[Main visual techniques used to obtain information from speckle patterns are
Fujii method, generalized difference, weighted generalized difference, mean
windowed difference, structural function (SF), modified SF, etc. In this work,
a comparative analysis of major visual techniques for natural gum sample is
carried out. Obtained results conclusively establish SF based method as an
optimum tool for visual inspection of dynamic speckle data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1"&gt;Amit Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhanotiya_J/0/1/0/all/0/1"&gt;Jitendra Dhanotiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_V/0/1/0/all/0/1"&gt;Vimal Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1"&gt;Shashi Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Prior Guided Scene Text Image Super-resolution. (arXiv:2106.15368v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15368</id>
        <link href="http://arxiv.org/abs/2106.15368"/>
        <updated>2021-06-30T02:01:01.232Z</updated>
        <summary type="html"><![CDATA[Scene text image super-resolution (STISR) aims to improve the resolution and
visual quality of low-resolution (LR) scene text images, and consequently boost
the performance of text recognition. However, most of existing STISR methods
regard text images as natural scene images, ignoring the categorical
information of text. In this paper, we make an inspiring attempt to embed
categorical text prior into STISR model training. Specifically, we adopt the
character probability sequence as the text prior, which can be obtained
conveniently from a text recognition model. The text prior provides categorical
guidance to recover high-resolution (HR) text images. On the other hand, the
reconstructed HR image can refine the text prior in return. Finally, we present
a multi-stage text prior guided super-resolution (TPGSR) framework for STISR.
Our experiments on the benchmark TextZoom dataset show that TPGSR can not only
effectively improve the visual quality of scene text images, but also
significantly improve the text recognition accuracy over existing STISR
methods. Our model trained on TextZoom also demonstrates certain generalization
capability to the LR images in other datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianqi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying urban streetscapes with deep learning: focus on aesthetic evaluation. (arXiv:2106.15361v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15361</id>
        <link href="http://arxiv.org/abs/2106.15361"/>
        <updated>2021-06-30T02:01:01.226Z</updated>
        <summary type="html"><![CDATA[The disorder of urban streetscapes would negatively affect people's
perception of their aesthetic quality. The presence of billboards on building
facades has been regarded as an important factor of the disorder, but its
quantification methodology has not yet been developed in a scalable manner. To
fill the gap, this paper reports the performance of our deep learning model on
a unique data set prepared in Tokyo to recognize the areas covered by facades
and billboards in streetscapes, respectively. The model achieved 63.17 % of
accuracy, measured by Intersection-over-Union (IoU), thus enabling researchers
and practitioners to obtain insights on urban streetscape design by combining
data of people's preferences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumakoshi_Y/0/1/0/all/0/1"&gt;Yusuke Kumakoshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onoda_S/0/1/0/all/0/1"&gt;Shigeaki Onoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_T/0/1/0/all/0/1"&gt;Tetsuya Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshimura_Y/0/1/0/all/0/1"&gt;Yuji Yoshimura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Inpainting Using Wasserstein Generative Adversarial Imputation Network. (arXiv:2106.15341v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15341</id>
        <link href="http://arxiv.org/abs/2106.15341"/>
        <updated>2021-06-30T02:01:01.195Z</updated>
        <summary type="html"><![CDATA[Image inpainting is one of the important tasks in computer vision which
focuses on the reconstruction of missing regions in an image. The aim of this
paper is to introduce an image inpainting model based on Wasserstein Generative
Adversarial Imputation Network. The generator network of the model uses
building blocks of convolutional layers with different dilation rates, together
with skip connections that help the model reproduce fine details of the output.
This combination yields a universal imputation model that is able to handle
various scenarios of missingness with sufficient quality. To show this
experimentally, the model is simultaneously trained to deal with three
scenarios given by missing pixels at random, missing various smaller square
regions, and one missing square placed in the center of the image. It turns out
that our model achieves high-quality inpainting results on all scenarios.
Performance is evaluated using peak signal-to-noise ratio and structural
similarity index on two real-world benchmark datasets, CelebA faces and Paris
StreetView. The results of our model are compared to biharmonic imputation and
to some of the other state-of-the-art image inpainting methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasata_D/0/1/0/all/0/1"&gt;Daniel Va&amp;#x161;ata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halama_T/0/1/0/all/0/1"&gt;Tom&amp;#xe1;&amp;#x161; Halama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedjungova_M/0/1/0/all/0/1"&gt;Magda Friedjungov&amp;#xe1;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source-free Domain Adaptation via Avatar Prototype Generation and Adaptation. (arXiv:2106.15326v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15326</id>
        <link href="http://arxiv.org/abs/2106.15326"/>
        <updated>2021-06-30T02:01:01.179Z</updated>
        <summary type="html"><![CDATA[We study a practical domain adaptation task, called source-free unsupervised
domain adaptation (UDA) problem, in which we cannot access source domain data
due to data privacy issues but only a pre-trained source model and unlabeled
target data are available. This task, however, is very difficult due to one key
challenge: the lack of source data and target domain labels makes model
adaptation very challenging. To address this, we propose to mine the hidden
knowledge in the source model and exploit it to generate source avatar
prototypes (i.e., representative features for each source class) as well as
target pseudo labels for domain alignment. To this end, we propose a
Contrastive Prototype Generation and Adaptation (CPGA) method. Specifically,
CPGA consists of two stages: (1) prototype generation: by exploring the
classification boundary information of the source model, we train a prototype
generator to generate avatar prototypes via contrastive learning. (2) prototype
adaptation: based on the generated source prototypes and target pseudo labels,
we develop a new robust contrastive prototype adaptation strategy to align each
pseudo-labeled target data to the corresponding source prototypes. Extensive
experiments on three UDA benchmark datasets demonstrate the effectiveness and
superiority of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1"&gt;Zhen Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hongbin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1"&gt;Shuaicheng Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanxia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1"&gt;Qing Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Evaluation of Deep Active Learning on Image Classification Tasks. (arXiv:2106.15324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15324</id>
        <link href="http://arxiv.org/abs/2106.15324"/>
        <updated>2021-06-30T02:01:01.173Z</updated>
        <summary type="html"><![CDATA[With the goal of making deep learning more label-efficient, a growing number
of papers have been studying active learning (AL) for deep models. However,
there are a number of issues in the prevalent experimental settings, mainly
stemming from a lack of unified implementation and benchmarking. Issues in the
current literature include sometimes contradictory observations on the
performance of different AL algorithms, unintended exclusion of important
generalization approaches such as data augmentation and SGD for optimization, a
lack of study of evaluation facets like the labeling efficiency of AL, and
little or no clarity on the scenarios in which AL outperforms random sampling
(RS). In this work, we present a unified re-implementation of state-of-the-art
AL algorithms in the context of image classification, and we carefully study
these issues as facets of effective evaluation. On the positive side, we show
that AL techniques are 2x to 4x more label-efficient compared to RS with the
use of data augmentation. Surprisingly, when data augmentation is included,
there is no longer a consistent gain in using BADGE, a state-of-the-art
approach, over simple uncertainty sampling. We then do a careful analysis of
how existing approaches perform with varying amounts of redundancy and number
of examples per class. Finally, we provide several insights for AL
practitioners to consider in future work, such as the effect of the AL batch
size, the effect of initialization, the importance of retraining a new model at
every round, and other insights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1"&gt;Nathan Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1"&gt;Durga Sivasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dani_A/0/1/0/all/0/1"&gt;Apurva Dani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch-Based Image Restoration using Expectation Propagation. (arXiv:2106.15327v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15327</id>
        <link href="http://arxiv.org/abs/2106.15327"/>
        <updated>2021-06-30T02:01:01.168Z</updated>
        <summary type="html"><![CDATA[This paper presents a new Expectation Propagation (EP) framework for image
restoration using patch-based prior distributions. While Monte Carlo techniques
are classically used to sample from intractable posterior distributions, they
can suffer from scalability issues in high-dimensional inference problems such
as image restoration. To address this issue, EP is used here to approximate the
posterior distributions using products of multivariate Gaussian densities.
Moreover, imposing structural constraints on the covariance matrices of these
densities allows for greater scalability and distributed computation. While the
method is naturally suited to handle additive Gaussian observation noise, it
can also be extended to non-Gaussian noise. Experiments conducted for
denoising, inpainting and deconvolution problems with Gaussian and Poisson
noise illustrate the potential benefits of such flexible approximate Bayesian
method for uncertainty quantification in imaging problems, at a reduced
computational cost compared to sampling techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1"&gt;Dan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McLaughlin_S/0/1/0/all/0/1"&gt;Stephen McLaughlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Altmann_Y/0/1/0/all/0/1"&gt;Yoann Altmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cloud based Scalable Object Recognition from Video Streams using Orientation Fusion and Convolutional Neural Networks. (arXiv:2106.15329v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15329</id>
        <link href="http://arxiv.org/abs/2106.15329"/>
        <updated>2021-06-30T02:01:01.163Z</updated>
        <summary type="html"><![CDATA[Object recognition from live video streams comes with numerous challenges
such as the variation in illumination conditions and poses. Convolutional
neural networks (CNNs) have been widely used to perform intelligent visual
object recognition. Yet, CNNs still suffer from severe accuracy degradation,
particularly on illumination-variant datasets. To address this problem, we
propose a new CNN method based on orientation fusion for visual object
recognition. The proposed cloud-based video analytics system pioneers the use
of bi-dimensional empirical mode decomposition to split a video frame into
intrinsic mode functions (IMFs). We further propose these IMFs to endure Reisz
transform to produce monogenic object components, which are in turn used for
the training of CNNs. Past works have demonstrated how the object orientation
component may be used to pursue accuracy levels as high as 93\%. Herein we
demonstrate how a feature-fusion strategy of the orientation components leads
to further improving visual recognition accuracy to 97\%. We also assess the
scalability of our method, looking at both the number and the size of the video
streams under scrutiny. We carry out extensive experimentation on the publicly
available Yale dataset, including also a self generated video datasets, finding
significant improvements (both in accuracy and scale), in comparison to
AlexNet, LeNet and SE-ResNeXt, which are the three most commonly used deep
learning models for visual object recognition and classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yaseen_M/0/1/0/all/0/1"&gt;Muhammad Usman Yaseen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anjum_A/0/1/0/all/0/1"&gt;Ashiq Anjum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortino_G/0/1/0/all/0/1"&gt;Giancarlo Fortino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liotta_A/0/1/0/all/0/1"&gt;Antonio Liotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1"&gt;Amir Hussain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TNCR: Table Net Detection and Classification Dataset. (arXiv:2106.15322v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15322</id>
        <link href="http://arxiv.org/abs/2106.15322"/>
        <updated>2021-06-30T02:01:01.156Z</updated>
        <summary type="html"><![CDATA[We present TNCR, a new table dataset with varying image quality collected
from free websites. The TNCR dataset can be used for table detection in scanned
document images and their classification into 5 different classes. TNCR
contains 9428 high-quality labeled images. In this paper, we have implemented
state-of-the-art deep learning-based methods for table detection to create
several strong baselines. Cascade Mask R-CNN with ResNeXt-101-64x4d Backbone
Network achieves the highest performance compared to other methods with a
precision of 79.7%, recall of 89.8%, and f1 score of 84.4% on the TNCR dataset.
We have made TNCR open source in the hope of encouraging more deep learning
approaches to table detection, classification, and structure recognition. The
dataset and trained model checkpoints are available at
https://github.com/abdoelsayed2016/TNCR_Dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1"&gt;Abdelrahman Abdallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berendeyev_A/0/1/0/all/0/1"&gt;Alexander Berendeyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nuradin_I/0/1/0/all/0/1"&gt;Islam Nuradin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurseitov_D/0/1/0/all/0/1"&gt;Daniyar Nurseitov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SE-MD: A Single-encoder multiple-decoder deep network for point cloud generation from 2D images. (arXiv:2106.15325v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15325</id>
        <link href="http://arxiv.org/abs/2106.15325"/>
        <updated>2021-06-30T02:01:01.138Z</updated>
        <summary type="html"><![CDATA[3D model generation from single 2D RGB images is a challenging and actively
researched computer vision task. Various techniques using conventional network
architectures have been proposed for the same. However, the body of research
work is limited and there are various issues like using inefficient 3D
representation formats, weak 3D model generation backbones, inability to
generate dense point clouds, dependence of post-processing for generation of
dense point clouds, and dependence on silhouettes in RGB images. In this paper,
a novel 2D RGB image to point cloud conversion technique is proposed, which
improves the state of art in the field due to its efficient, robust and simple
model by using the concept of parallelization in network architecture. It not
only uses the efficient and rich 3D representation of point clouds, but also
uses a novel and robust point cloud generation backbone in order to address the
prevalent issues. This involves using a single-encoder multiple-decoder deep
network architecture wherein each decoder generates certain fixed viewpoints.
This is followed by fusing all the viewpoints to generate a dense point cloud.
Various experiments are conducted on the technique and its performance is
compared with those of other state of the art techniques and impressive gains
in performance are demonstrated. Code is available at
https://github.com/mueedhafiz1982/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hafiz_A/0/1/0/all/0/1"&gt;Abdul Mueed Hafiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1"&gt;Rouf Ul Alam Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parah_S/0/1/0/all/0/1"&gt;Shabir Ahmad Parah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassaballah_M/0/1/0/all/0/1"&gt;M. Hassaballah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Serial-EMD: Fast Empirical Mode Decomposition Method for Multi-dimensional Signals Based on Serialization. (arXiv:2106.15319v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15319</id>
        <link href="http://arxiv.org/abs/2106.15319"/>
        <updated>2021-06-30T02:01:01.133Z</updated>
        <summary type="html"><![CDATA[Empirical mode decomposition (EMD) has developed into a prominent tool for
adaptive, scale-based signal analysis in various fields like robotics, security
and biomedical engineering. Since the dramatic increase in amount of data puts
forward higher requirements for the capability of real-time signal analysis, it
is difficult for existing EMD and its variants to trade off the growth of data
dimension and the speed of signal analysis. In order to decompose
multi-dimensional signals at a faster speed, we present a novel
signal-serialization method (serial-EMD), which concatenates multi-variate or
multi-dimensional signals into a one-dimensional signal and uses various
one-dimensional EMD algorithms to decompose it. To verify the effects of the
proposed method, synthetic multi-variate time series, artificial 2D images with
various textures and real-world facial images are tested. Compared with
existing multi-EMD algorithms, the decomposition time becomes significantly
reduced. In addition, the results of facial recognition with Intrinsic Mode
Functions (IMFs) extracted using our method can achieve a higher accuracy than
those obtained by existing multi-EMD algorithms, which demonstrates the
superior performance of our method in terms of the quality of IMFs.
Furthermore, this method can provide a new perspective to optimize the existing
EMD algorithms, that is, transforming the structure of the input signal rather
than being constrained by developing envelope computation techniques or signal
decomposition methods. In summary, the study suggests that the serial-EMD
technique is a highly competitive and fast alternative for multi-dimensional
signal analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marti_Puig_P/0/1/0/all/0/1"&gt;Pere Marti-Puig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caiafa_C/0/1/0/all/0/1"&gt;Cesar F. Caiafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhe Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_F/0/1/0/all/0/1"&gt;Feng Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sole_Casals_J/0/1/0/all/0/1"&gt;Jordi Sol&amp;#xe9;-Casals&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in Minimally Invasive Interventional Treatment. (arXiv:2106.15306v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15306</id>
        <link href="http://arxiv.org/abs/2106.15306"/>
        <updated>2021-06-30T02:01:01.128Z</updated>
        <summary type="html"><![CDATA[Minimally invasive image guided treatment procedures often employ advanced
image processing algorithms. The recent developments of artificial intelligence
algorithms harbor potential to further enhance this domain. In this article we
explore several application areas within the minimally invasive treatment space
and discuss the deployment of artificial intelligence within these areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruijters_D/0/1/0/all/0/1"&gt;Daniel Ruijters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face Identification Proficiency Test Designed Using Item Response Theory. (arXiv:2106.15323v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15323</id>
        <link href="http://arxiv.org/abs/2106.15323"/>
        <updated>2021-06-30T02:01:01.122Z</updated>
        <summary type="html"><![CDATA[Measures of face identification proficiency are essential to ensure accurate
and consistent performance by professional forensic face examiners and others
who perform face identification tasks in applied scenarios. Current proficiency
tests rely on static sets of stimulus items, and so, cannot be administered
validly to the same individual multiple times. To create a proficiency test, a
large number of items of "known" difficulty must be assembled. Multiple tests
of equal difficulty can be constructed then using subsets of items. Here, we
introduce a proficiency test, the Triad Identity Matching (TIM) test, based on
stimulus difficulty measures based on Item Response Theory (IRT). Participants
view face-image "triads" (N=225) (two images of one identity and one image of a
different identity) and select the different identity. In Experiment 1,
university students (N=197) showed wide-ranging accuracy on the TIM test.
Furthermore, IRT modeling demonstrated that the TIM test produces items of
various difficulty levels. In Experiment 2, IRT-based item difficulty measures
were used to partition the TIM test into three equally "easy" and three equally
"difficult" subsets. Simulation results indicated that the full set, as well as
curated subsets, of the TIM items yielded reliable estimates of subject
ability. In summary, the TIM test can provide a starting point for developing a
framework that is flexible, calibrated, and adaptive to measure proficiency
across various ability levels (e.g., professionals or populations with face
processing deficits)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeckeln_G/0/1/0/all/0/1"&gt;G&amp;#xe9;raldine Jeckeln&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Ying Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavazos_J/0/1/0/all/0/1"&gt;Jacqueline G. Cavazos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1"&gt;Amy N. Yates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_C/0/1/0/all/0/1"&gt;Carina A. Hahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1"&gt;Larry Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1"&gt;Jonathon Phillips&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OToole_A/0/1/0/all/0/1"&gt;Alice J. O&amp;#x27;Toole&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boggart: Accelerating Retrospective Video Analytics via Model-Agnostic Ingest Processing. (arXiv:2106.15315v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15315</id>
        <link href="http://arxiv.org/abs/2106.15315"/>
        <updated>2021-06-30T02:01:01.117Z</updated>
        <summary type="html"><![CDATA[Delivering fast responses to retrospective queries on video datasets is
difficult due to the large number of frames to consider and the high costs of
running convolutional neural networks (CNNs) on each one. A natural solution is
to perform a subset of the necessary computations ahead of time, as video is
ingested. However, existing ingest-time systems require knowledge of the
specific CNN that will be used in future queries -- a challenging requisite
given the evergrowing space of CNN architectures and training
datasets/methodologies.

This paper presents Boggart, a retrospective video analytics system that
delivers ingest-time speedups in a model-agnostic manner. Our underlying
insight is that traditional computer vision (CV) algorithms are capable of
performing computations that can be used to accelerate diverse queries with
wide-ranging CNNs. Building on this, at ingest-time, Boggart carefully employs
a variety of motion tracking algorithms to identify potential objects and their
trajectories across frames. Then, at query-time, Boggart uses several novel
techniques to collect the smallest sample of CNN results required to meet the
target accuracy: (1) a clustering strategy to efficiently unearth the
inevitable discrepancies between CV- and CNN-generated outputs, and (2) a set
of accuracy-preserving propagation techniques to safely extend sampled results
along each trajectory. Across many videos, CNNs, and queries Boggart
consistently meets accuracy targets while using CNNs sparingly (on 3-54% of
frames).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1"&gt;Neil Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1"&gt;Ravi Netravali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity Embedding Networks for Robust Human Activity Recognition. (arXiv:2106.15283v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15283</id>
        <link href="http://arxiv.org/abs/2106.15283"/>
        <updated>2021-06-30T02:01:01.111Z</updated>
        <summary type="html"><![CDATA[Deep learning models for human activity recognition (HAR) based on sensor
data have been heavily studied recently. However, the generalization ability of
deep models on complex real-world HAR data is limited by the availability of
high-quality labeled activity data, which are hard to obtain. In this paper, we
design a similarity embedding neural network that maps input sensor signals
onto real vectors through carefully designed convolutional and LSTM layers. The
embedding network is trained with a pairwise similarity loss, encouraging the
clustering of samples from the same class in the embedded real space, and can
be effectively trained on a small dataset and even on a noisy dataset with
mislabeled samples. Based on the learned embeddings, we further propose both
nonparametric and parametric approaches for activity recognition. Extensive
evaluation based on two public datasets has shown that the proposed similarity
embedding network significantly outperforms state-of-the-art deep models on HAR
classification tasks, is robust to mislabeled samples in the training set, and
can also be used to effectively denoise a noisy dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1"&gt;Carrie Lu Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xiao Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Lei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jian Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianming Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MFR 2021: Masked Face Recognition Competition. (arXiv:2106.15288v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15288</id>
        <link href="http://arxiv.org/abs/2106.15288"/>
        <updated>2021-06-30T02:01:01.095Z</updated>
        <summary type="html"><![CDATA[This paper presents a summary of the Masked Face Recognition Competitions
(MFR) held within the 2021 International Joint Conference on Biometrics (IJCB
2021). The competition attracted a total of 10 participating teams with valid
submissions. The affiliations of these teams are diverse and associated with
academia and industry in nine different countries. These teams successfully
submitted 18 valid solutions. The competition is designed to motivate solutions
aiming at enhancing the face recognition accuracy of masked faces. Moreover,
the competition considered the deployability of the proposed solutions by
taking the compactness of the face recognition models into account. A private
dataset representing a collaborative, multi-session, real masked, capture
scenario is used to evaluate the submitted solutions. In comparison to one of
the top-performing academic face recognition solutions, 10 out of the 18
submitted solutions did score higher masked face verification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1"&gt;Fadi Boutros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1"&gt;Naser Damer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolf_J/0/1/0/all/0/1"&gt;Jan Niklas Kolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1"&gt;Kiran Raja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1"&gt;Florian Kirchbuchner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1"&gt;Raghavendra Ramachandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1"&gt;Arjan Kuijper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1"&gt;Pengcheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montero_D/0/1/0/all/0/1"&gt;David Montero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aginako_N/0/1/0/all/0/1"&gt;Naiara Aginako&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sierra_B/0/1/0/all/0/1"&gt;Basilio Sierra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1"&gt;Marcos Nieto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erakin_M/0/1/0/all/0/1"&gt;Mustafa Ekrem Erakin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1"&gt;Ugur Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kemal_H/0/1/0/all/0/1"&gt;Hazim Kemal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekenel/0/1/0/all/0/1"&gt;Ekenel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kataoka_A/0/1/0/all/0/1"&gt;Asaki Kataoka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ichikawa_K/0/1/0/all/0/1"&gt;Kohei Ichikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kubo_S/0/1/0/all/0/1"&gt;Shizuma Kubo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1"&gt;Mingjie He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grm_K/0/1/0/all/0/1"&gt;Klemen Grm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1"&gt;Vitomir &amp;#x160;truc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1"&gt;Sachith Seneviratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasthuriarachchi_N/0/1/0/all/0/1"&gt;Nuran Kasthuriarachchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasnayaka_S/0/1/0/all/0/1"&gt;Sanka Rasnayaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neto_P/0/1/0/all/0/1"&gt;Pedro C. Neto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sequeira_A/0/1/0/all/0/1"&gt;Ana F. Sequeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1"&gt;Joao Ribeiro Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saffari_M/0/1/0/all/0/1"&gt;Mohsen Saffari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1"&gt;Jaime S. Cardoso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ScanBank: A Benchmark Dataset for Figure Extraction from Scanned Electronic Theses and Dissertations. (arXiv:2106.15320v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15320</id>
        <link href="http://arxiv.org/abs/2106.15320"/>
        <updated>2021-06-30T02:01:01.089Z</updated>
        <summary type="html"><![CDATA[We focus on electronic theses and dissertations (ETDs), aiming to improve
access and expand their utility, since more than 6 million are publicly
available, and they constitute an important corpus to aid research and
education across disciplines. The corpus is growing as new born-digital
documents are included, and since millions of older theses and dissertations
have been converted to digital form to be disseminated electronically in
institutional repositories. In ETDs, as with other scholarly works, figures and
tables can communicate a large amount of information in a concise way. Although
methods have been proposed for extracting figures and tables from born-digital
PDFs, they do not work well with scanned ETDs. Considering this problem, our
assessment of state-of-the-art figure extraction systems is that the reason
they do not function well on scanned PDFs is that they have only been trained
on born-digital documents. To address this limitation, we present ScanBank, a
new dataset containing 10 thousand scanned page images, manually labeled by
humans as to the presence of the 3.3 thousand figures or tables found therein.
We use this dataset to train a deep neural network model based on YOLOv5 to
accurately extract figures and tables from scanned ETDs. We pose and answer
important research questions aimed at finding better methods for figure
extraction from scanned documents. One of those concerns the value for
training, of data augmentation techniques applied to born-digital documents
which are used to train models better suited for figure extraction from scanned
documents. To the best of our knowledge, ScanBank is the first manually
annotated dataset for figure and table extraction for scanned ETDs. A
YOLOv5-based model, trained on ScanBank, outperforms existing comparable
open-source and freely available baseline methods by a considerable margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kahu_S/0/1/0/all/0/1"&gt;Sampanna Yashwant Kahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ingram_W/0/1/0/all/0/1"&gt;William A. Ingram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1"&gt;Edward A. Fox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jian Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling Catastrophic Forgetting and Background Shift in Continual Semantic Segmentation. (arXiv:2106.15287v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15287</id>
        <link href="http://arxiv.org/abs/2106.15287"/>
        <updated>2021-06-30T02:01:01.083Z</updated>
        <summary type="html"><![CDATA[Deep learning approaches are nowadays ubiquitously used to tackle computer
vision tasks such as semantic segmentation, requiring large datasets and
substantial computational power. Continual learning for semantic segmentation
(CSS) is an emerging trend that consists in updating an old model by
sequentially adding new classes. However, continual learning methods are
usually prone to catastrophic forgetting. This issue is further aggravated in
CSS where, at each step, old classes from previous iterations are collapsed
into the background. In this paper, we propose Local POD, a multi-scale pooling
distillation scheme that preserves long- and short-range spatial relationships
at feature level. Furthermore, we design an entropy-based pseudo-labelling of
the background w.r.t. classes predicted by the old model to deal with
background shift and avoid catastrophic forgetting of the old classes. Finally,
we introduce a novel rehearsal method that is particularly suited for
segmentation. Our approach, called PLOP, significantly outperforms
state-of-the-art methods in existing CSS scenarios, as well as in newly
proposed challenging benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1"&gt;Arthur Douillard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yifu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1"&gt;Arnaud Dapogny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1"&gt;Matthieu Cord&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft Attention: Does it Actually Help to Learn Social Interactions in Pedestrian Trajectory Prediction?. (arXiv:2106.15321v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15321</id>
        <link href="http://arxiv.org/abs/2106.15321"/>
        <updated>2021-06-30T02:01:01.078Z</updated>
        <summary type="html"><![CDATA[We consider the problem of predicting the future path of a pedestrian using
its motion history and the motion history of the surrounding pedestrians,
called social information. Since the seminal paper on Social-LSTM,
deep-learning has become the main tool used to model the impact of social
interactions on a pedestrian's motion. The demonstration that these models can
learn social interactions relies on an ablative study of these models. The
models are compared with and without their social interactions module on two
standard metrics, the Average Displacement Error and Final Displacement Error.
Yet, these complex models were recently outperformed by a simple
constant-velocity approach. This questions if they actually allow to model
social interactions as well as the validity of the proof. In this paper, we
focus on the deep-learning models with a soft-attention mechanism for social
interaction modeling and study whether they use social information at
prediction time. We conduct two experiments across four state-of-the-art
approaches on the ETH and UCY datasets, which were also used in previous work.
First, the models are trained by replacing the social information with random
noise and compared to model trained with actual social information. Second, we
use a gating mechanism along with a $L_0$ penalty, allowing models to shut down
their inner components. The models consistently learn to prune their
soft-attention mechanism. For both experiments, neither the course of the
convergence nor the prediction performance were altered. This demonstrates that
the soft-attention mechanism and therefore the social information are ignored
by the models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boucaud_L/0/1/0/all/0/1"&gt;Laurent Boucaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aloise_D/0/1/0/all/0/1"&gt;Daniel Aloise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1"&gt;Nicolas Saunier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Semantic Similarity Learning for Image Captioning Evaluation with Intrinsic Auto-encoder. (arXiv:2106.15312v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15312</id>
        <link href="http://arxiv.org/abs/2106.15312"/>
        <updated>2021-06-30T02:01:01.071Z</updated>
        <summary type="html"><![CDATA[Automatically evaluating the quality of image captions can be very
challenging since human language is quite flexible that there can be various
expressions for the same meaning. Most of the current captioning metrics rely
on token level matching between candidate caption and the ground truth label
sentences. It usually neglects the sentence-level information. Motivated by the
auto-encoder mechanism and contrastive representation learning advances, we
propose a learning-based metric for image captioning, which we call Intrinsic
Image Captioning Evaluation($I^2CE$). We develop three progressive model
structures to learn the sentence level representations--single branch model,
dual branches model, and triple branches model. Our empirical tests show that
$I^2CE$ trained with dual branches structure achieves better consistency with
human judgments to contemporary image captioning evaluation metrics.
Furthermore, We select several state-of-the-art image captioning models and
test their performances on the MS COCO dataset concerning both contemporary
metrics and the proposed $I^2CE$. Experiment results show that our proposed
method can align well with the scores generated from other contemporary
metrics. On this concern, the proposed metric could serve as a novel indicator
of the intrinsic information between captions, which may be complementary to
the existing ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1"&gt;Chao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiesong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1"&gt;Sam Kwong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Semantic Scene Graphs for Holistic Modeling of Surgical Procedures. (arXiv:2106.15309v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15309</id>
        <link href="http://arxiv.org/abs/2106.15309"/>
        <updated>2021-06-30T02:01:01.056Z</updated>
        <summary type="html"><![CDATA[From a computer science viewpoint, a surgical domain model needs to be a
conceptual one incorporating both behavior and data. It should therefore model
actors, devices, tools, their complex interactions and data flow. To capture
and model these, we take advantage of the latest computer vision methodologies
for generating 3D scene graphs from camera views. We then introduce the
Multimodal Semantic Scene Graph (MSSG) which aims at providing a unified
symbolic, spatiotemporal and semantic representation of surgical procedures.
This methodology aims at modeling the relationship between different components
in surgical domain including medical staff, imaging systems, and surgical
devices, opening the path towards holistic understanding and modeling of
surgical procedures. We then use MSSG to introduce a dynamically generated
graphical user interface tool for surgical procedure analysis which could be
used for many applications including process optimization, OR design and
automatic report generation. We finally demonstrate that the proposed MSSGs
could also be used for synchronizing different complex surgical procedures.
While the system still needs to be integrated into real operating rooms before
getting validated, this conference paper aims mainly at providing the community
with the basic principles of this novel concept through a first prototypal
partial realization based on MVOR dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1"&gt;Ege &amp;#xd6;zsoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ornek_E/0/1/0/all/0/1"&gt;Evin P&amp;#x131;nar &amp;#xd6;rnek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eck_U/0/1/0/all/0/1"&gt;Ulrich Eck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Fast and Accurate Multi-Person Pose Estimation on Mobile Devices. (arXiv:2106.15304v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15304</id>
        <link href="http://arxiv.org/abs/2106.15304"/>
        <updated>2021-06-30T02:01:01.051Z</updated>
        <summary type="html"><![CDATA[The rapid development of autonomous driving, abnormal behavior detection, and
behavior recognition makes an increasing demand for multi-person pose
estimation-based applications, especially on mobile platforms. However, to
achieve high accuracy, state-of-the-art methods tend to have a large model size
and complex post-processing algorithm, which costs intense computation and long
end-to-end latency. To solve this problem, we propose an architecture
optimization and weight pruning framework to accelerate inference of
multi-person pose estimation on mobile devices. With our optimization
framework, we achieve up to 2.51x faster model inference speed with higher
accuracy compared to representative lightweight multi-person pose estimator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xuan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1"&gt;Wei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1"&gt;Jiexiong Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysing Affective Behavior in the second ABAW2 Competition. (arXiv:2106.15318v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15318</id>
        <link href="http://arxiv.org/abs/2106.15318"/>
        <updated>2021-06-30T02:01:01.045Z</updated>
        <summary type="html"><![CDATA[The Affective Behavior Analysis in-the-wild (ABAW2) 2021 Competition is the
second -- following the first very successful ABAW Competition held in
conjunction with IEEE FG 2020- Competition that aims at automatically analyzing
affect. ABAW2 is split into three Challenges, each one addressing one of the
three main behavior tasks of valence-arousal estimation, basic expression
classification and action unit detection. All three Challenges are based on a
common benchmark database, Aff-Wild2, which is a large scale in-the-wild
database and the first one to be annotated for all these three tasks. In this
paper, we describe this Competition, to be held in conjunction with ICCV 2021.
We present the three Challenges, with the utilized Competition corpora. We
outline the evaluation metrics and present the baseline system with its
results. More information regarding the Competition is provided in the
Competition site: https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kollias_D/0/1/0/all/0/1"&gt;Dimitrios Kollias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotsia_I/0/1/0/all/0/1"&gt;Irene Kotsia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajiyev_E/0/1/0/all/0/1"&gt;Elnar Hajiyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1"&gt;Stefanos Zafeiriou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Class Correlation Decomposition for Generalizable Person Re-Identification. (arXiv:2106.15206v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15206</id>
        <link href="http://arxiv.org/abs/2106.15206"/>
        <updated>2021-06-30T02:01:01.040Z</updated>
        <summary type="html"><![CDATA[Domain generalization in person re-identification is a highly important
meaningful and practical task in which a model trained with data from several
source domains is expected to generalize well to unseen target domains. Domain
adversarial learning is a promising domain generalization method that aims to
remove domain information in the latent representation through adversarial
training. However, in person re-identification, the domain and class are
correlated, and we theoretically show that domain adversarial learning will
lose certain information about class due to this domain-class correlation.
Inspired by casual inference, we propose to perform interventions to the domain
factor $d$, aiming to decompose the domain-class correlation. To achieve this
goal, we proposed estimating the resulting representation $z^{*}$ caused by the
intervention through first- and second-order statistical characteristic
matching. Specifically, we build a memory bank to restore the statistical
characteristics of each domain. Then, we use the newly generated samples
$\{z^{*},y,d^{*}\}$ to compute the loss function. These samples are
domain-class correlation decomposed; thus, we can learn a domain-invariant
representation that can capture more class-related features. Extensive
experiments show that our model outperforms the state-of-the-art methods on the
large-scale domain generalization Re-ID benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kaiwen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1"&gt;Xinmei Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Roof Damage Assessment from Automated 3D Building Models. (arXiv:2106.15294v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15294</id>
        <link href="http://arxiv.org/abs/2106.15294"/>
        <updated>2021-06-30T02:01:01.035Z</updated>
        <summary type="html"><![CDATA[The 3D building modelling is important in urban planning and related domains
that draw upon the content of 3D models of urban scenes. Such 3D models can be
used to visualize city images at multiple scales from individual buildings to
entire cities prior to and after a change has occurred. This ability is of
great importance in day-to-day work and special projects undertaken by
planners, geo-designers, and architects. In this research, we implemented a
novel approach to 3D building models for such matter, which included the
integration of geographic information systems (GIS) and 3D Computer Graphics
(3DCG) components that generate 3D house models from building footprints
(polygons), and the automated generation of simple and complex roof geometries
for rapid roof area damage reporting. These polygons (footprints) are usually
orthogonal. A complicated orthogonal polygon can be partitioned into a set of
rectangles. The proposed GIS and 3DCG integrated system partitions orthogonal
building polygons into a set of rectangles and places rectangular roofs and
box-shaped building bodies on these rectangles. Since technicians are drawing
these polygons manually with digitizers, depending on aerial photos, not all
building polygons are precisely orthogonal. But, when placing a set of boxes as
building bodies for creating the buildings, there may be gaps or overlaps
between these boxes if building polygons are not precisely orthogonal. In our
proposal, after approximately orthogonal building polygons are partitioned and
rectified into a set of mutually orthogonal rectangles, each rectangle knows
which rectangle is adjacent to and which edge of the rectangle is adjacent to,
which will avoid unwanted intersection of windows and doors when building
bodies combined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sugihara_K/0/1/0/all/0/1"&gt;Kenichi Sugihara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_M/0/1/0/all/0/1"&gt;Martin Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kongwen/0/1/0/all/0/1"&gt;Kongwen&lt;/a&gt; (Frank) &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1"&gt;Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khmelevsky_Y/0/1/0/all/0/1"&gt;Youry Khmelevsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15292</id>
        <link href="http://arxiv.org/abs/2106.15292"/>
        <updated>2021-06-30T02:01:01.020Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have been shown to be susceptible to memorization
or overfitting in the presence of noisily labelled data. For the problem of
robust learning under such noisy data, several algorithms have been proposed. A
prominent class of algorithms rely on sample selection strategies, motivated by
curriculum learning. For example, many algorithms use the `small loss trick'
wherein a fraction of samples with loss values below a certain threshold are
selected for training. These algorithms are sensitive to such thresholds, and
it is difficult to fix or learn these thresholds. Often, these algorithms also
require information such as label noise rates which are typically unavailable
in practice. In this paper, we propose a data-dependent, adaptive sample
selection strategy that relies only on batch statistics of a given mini-batch
to provide robustness against label noise. The algorithm does not have any
additional hyperparameters for sample selection, does not need any information
on noise rates, and does not need access to separate data with clean labels. We
empirically demonstrate the effectiveness of our algorithm on benchmark
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1"&gt;P.S. Sastry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic 2D-3D Registration without Contrast Agent during Neurovascular Interventions. (arXiv:2106.15308v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15308</id>
        <link href="http://arxiv.org/abs/2106.15308"/>
        <updated>2021-06-30T02:01:01.014Z</updated>
        <summary type="html"><![CDATA[Fusing live fluoroscopy images with a 3D rotational reconstruction of the
vasculature allows to navigate endovascular devices in minimally invasive
neuro-vascular treatment, while reducing the usage of harmful iodine contrast
medium. The alignment of the fluoroscopy images and the 3D reconstruction is
initialized using the sensor information of the X-ray C-arm geometry. Patient
motion is then corrected by an image-based registration algorithm, based on a
gradient difference similarity measure using digital reconstructed radiographs
of the 3D reconstruction. This algorithm does not require the vessels in the
fluoroscopy image to be filled with iodine contrast agent, but rather relies on
gradients in the image (bone structures, sinuses) as landmark features. This
paper investigates the accuracy, robustness and computation time aspects of the
image-based registration algorithm. Using phantom experiments 97% of the
registration attempts passed the success criterion of a residual registration
error of less than 1 mm translation and 3{\deg} rotation. The paper establishes
a new method for validation of 2D-3D registration without requiring changes to
the clinical workflow, such as attaching fiducial markers. As a consequence,
this method can be retrospectively applied to pre-existing clinical data. For
clinical data experiments, 87% of the registration attempts passed the
criterion of a residual translational error of < 1 mm, and 84% possessed a
rotational error of < 3{\deg}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Homan_R/0/1/0/all/0/1"&gt;Robert Homan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijsselt_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; van Rijsselt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruijters_D/0/1/0/all/0/1"&gt;Daniel Ruijters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Learning of Portrait Intrinsic Decomposition and Relighting. (arXiv:2106.15305v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15305</id>
        <link href="http://arxiv.org/abs/2106.15305"/>
        <updated>2021-06-30T02:01:01.009Z</updated>
        <summary type="html"><![CDATA[Inverse rendering is the problem of decomposing an image into its intrinsic
components, i.e. albedo, normal and lighting. To solve this ill-posed problem
from single image, state-of-the-art methods in shape from shading mostly resort
to supervised training on all the components on either synthetic or real
datasets. Here, we propose a new self-supervised training paradigm that 1)
reduces the need for full supervision on the decomposition task and 2) takes
into account the relighting task. We introduce new self-supervised loss terms
that leverage the consistencies between multi-lit images (images of the same
scene under different illuminations). Our approach is applicable to multi-lit
datasets. We apply our training approach in two settings: 1) train on a mixture
of synthetic and real data, 2) train on real datasets with limited supervision.
We show-case the effectiveness of our training paradigm on both intrinsic
decomposition and relighting and demonstrate how the model struggles in both
tasks without the self-supervised loss terms in limited supervision settings.
We provide results of comprehensive experiments on SfSNet, CelebA and Photoface
datasets and verify the performance of our approach on images in the wild.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zehni_M/0/1/0/all/0/1"&gt;Mona Zehni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Shaona Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridhar_K/0/1/0/all/0/1"&gt;Krishna Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1"&gt;Sethu Raman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VolterraNet: A higher order convolutional network with group equivariance for homogeneous manifolds. (arXiv:2106.15301v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15301</id>
        <link href="http://arxiv.org/abs/2106.15301"/>
        <updated>2021-06-30T02:01:01.003Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks have been highly successful in image-based
learning tasks due to their translation equivariance property. Recent work has
generalized the traditional convolutional layer of a convolutional neural
network to non-Euclidean spaces and shown group equivariance of the generalized
convolution operation. In this paper, we present a novel higher order Volterra
convolutional neural network (VolterraNet) for data defined as samples of
functions on Riemannian homogeneous spaces. Analagous to the result for
traditional convolutions, we prove that the Volterra functional convolutions
are equivariant to the action of the isometry group admitted by the Riemannian
homogeneous spaces, and under some restrictions, any non-linear equivariant
function can be expressed as our homogeneous space Volterra convolution,
generalizing the non-linear shift equivariant characterization of Volterra
expansions in Euclidean space. We also prove that second order functional
convolution operations can be represented as cascaded convolutions which leads
to an efficient implementation. Beyond this, we also propose a dilated
VolterraNet model. These advances lead to large parameter reductions relative
to baseline non-Euclidean CNNs. To demonstrate the efficacy of the VolterraNet
performance, we present several real data experiments involving classification
tasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing
on diffusion MRI data. Performance comparisons to the state-of-the-art are also
presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_M/0/1/0/all/0/1"&gt;Monami Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1"&gt;Rudrasis Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouza_J/0/1/0/all/0/1"&gt;Jose Bouza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vemuri_B/0/1/0/all/0/1"&gt;Baba C. Vemuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Sparse Coding Fast Approximation with Application to Seismic Reflectivity Estimation. (arXiv:2106.15296v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15296</id>
        <link href="http://arxiv.org/abs/2106.15296"/>
        <updated>2021-06-30T02:01:00.997Z</updated>
        <summary type="html"><![CDATA[In sparse coding, we attempt to extract features of input vectors, assuming
that the data is inherently structured as a sparse superposition of basic
building blocks. Similarly, neural networks perform a given task by learning
features of the training data set. Recently both data-driven and model-driven
feature extracting methods have become extremely popular and have achieved
remarkable results. Nevertheless, practical implementations are often too slow
to be employed in real-life scenarios, especially for real-time applications.
We propose a speed-up upgraded version of the classic iterative thresholding
algorithm, that produces a good approximation of the convolutional sparse code
within 2-5 iterations. The speed advantage is gained mostly from the
observation that most solvers are slowed down by inefficient global
thresholding. The main idea is to normalize each data point by the local
receptive field energy, before applying a threshold. This way, the natural
inclination towards strong feature expressions is suppressed, so that one can
rely on a global threshold that can be easily approximated, or learned during
training. The proposed algorithm can be employed with a known predetermined
dictionary, or with a trained dictionary. The trained version is implemented as
a neural net designed as the unfolding of the proposed solver. The performance
of the proposed solution is demonstrated via the seismic inversion problem in
both synthetic and real data scenarios. We also provide theoretical guarantees
for a stable support recovery. Namely, we prove that under certain conditions
the true support is perfectly recovered within the first iteration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pereg_D/0/1/0/all/0/1"&gt;Deborah Pereg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Israel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vassiliou_A/0/1/0/all/0/1"&gt;Anthony A. Vassiliou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Solar Potential of Rooftops using Image Segmentation and Structured Data. (arXiv:2106.15268v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15268</id>
        <link href="http://arxiv.org/abs/2106.15268"/>
        <updated>2021-06-30T02:01:00.982Z</updated>
        <summary type="html"><![CDATA[Estimating the amount of electricity that can be produced by rooftop
photovoltaic systems is a time-consuming process that requires on-site
measurements, a difficult task to achieve on a large scale. In this paper, we
present an approach to estimate the solar potential of rooftops based on their
location and architectural characteristics, as well as the amount of solar
radiation they receive annually. Our technique uses computer vision to achieve
semantic segmentation of roof sections and roof objects on the one hand, and a
machine learning model based on structured building features to predict roof
pitch on the other hand. We then compute the azimuth and maximum number of
solar panels that can be installed on a rooftop with geometric approaches.
Finally, we compute precise shading masks and combine them with solar
irradiation data that enables us to estimate the yearly solar potential of a
rooftop.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soares_D/0/1/0/all/0/1"&gt;Daniel de Barros Soares&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Andrieux_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Andrieux&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Hell_B/0/1/0/all/0/1"&gt;Bastien Hell&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Lenhardt_J/0/1/0/all/0/1"&gt;Julien Lenhardt&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Badosa_J/0/1/0/all/0/1"&gt;Jordi Badosa&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Gavoille_S/0/1/0/all/0/1"&gt;Sylvain Gavoille&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Gaiffas_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Gaiffas&lt;/a&gt; (1, 4 and 5), &lt;a href="http://arxiv.org/find/cs/1/au:+Bacry_E/0/1/0/all/0/1"&gt;Emmanuel Bacry&lt;/a&gt; (1 and 6), ((1) namR, Paris, France, (2) ENSTA Paris, France, (3) LMD, Ecole polytechnique, IP Paris, Palaiseau, France, (4) LPSM, Universit&amp;#xe9; de Paris, France, (5) DMA, Ecole normale sup&amp;#xe9;rieure, Paris, France, (6) CEREMADE, Universit&amp;#xe9; Paris Dauphine, Paris, France)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wrong Colored Vermeer: Color-Symmetric Image Distortion. (arXiv:2106.15179v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15179</id>
        <link href="http://arxiv.org/abs/2106.15179"/>
        <updated>2021-06-30T02:01:00.966Z</updated>
        <summary type="html"><![CDATA[Color symmetry implies that the colors of geometrical objects are assigned
according to their symmetry properties. It is defined by associating the
elements of the symmetry group with a color permutation. I use this concept for
generative art and apply symmetry-consistent color distortions to images of
paintings by Johannes Vermeer. The color permutations are realized as mappings
of the HSV color space onto itself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Richter_H/0/1/0/all/0/1"&gt;Hendrik Richter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cells are Actors: Social Network Analysis with Classical ML for SOTA Histology Image Classification. (arXiv:2106.15299v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15299</id>
        <link href="http://arxiv.org/abs/2106.15299"/>
        <updated>2021-06-30T02:01:00.937Z</updated>
        <summary type="html"><![CDATA[Digitization of histology images and the advent of new computational methods,
like deep learning, have helped the automatic grading of colorectal
adenocarcinoma cancer (CRA). Present automated CRA grading methods, however,
usually use tiny image patches and thus fail to integrate the entire tissue
micro-architecture for grading purposes. To tackle these challenges, we propose
to use a statistical network analysis method to describe the complex structure
of the tissue micro-environment by modelling nuclei and their connections as a
network. We show that by analyzing only the interactions between the cells in a
network, we can extract highly discriminative statistical features for CRA
grading. Unlike other deep learning or convolutional graph-based approaches,
our method is highly scalable (can be used for cell networks consist of
millions of nodes), completely explainable, and computationally inexpensive. We
create cell networks on a broad CRC histology image dataset, experiment with
our method, and report state-of-the-art performance for the prediction of
three-class CRA grading.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zamanitajeddin_N/0/1/0/all/0/1"&gt;Neda Zamanitajeddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1"&gt;Mostafa Jahanifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1"&gt;Nasir Rajpoot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ElephantBook: A Semi-Automated Human-in-the-Loop System for Elephant Re-Identification. (arXiv:2106.15083v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15083</id>
        <link href="http://arxiv.org/abs/2106.15083"/>
        <updated>2021-06-30T02:01:00.930Z</updated>
        <summary type="html"><![CDATA[African elephants are vital to their ecosystems, but their populations are
threatened by a rise in human-elephant conflict and poaching. Monitoring
population dynamics is essential in conservation efforts; however, tracking
elephants is a difficult task, usually relying on the invasive and sometimes
dangerous placement of GPS collars. Although there have been many recent
successes in the use of computer vision techniques for automated identification
of other species, identification of elephants is extremely difficult and
typically requires expertise as well as familiarity with elephants in the
population. We have built and deployed a web-based platform and database for
human-in-the-loop re-identification of elephants combining manual attribute
labeling and state-of-the-art computer vision algorithms, known as
ElephantBook. Our system is currently in use at the Mara Elephant Project,
helping monitor the protected and at-risk population of elephants in the
Greater Maasai Mara ecosystem. ElephantBook makes elephant re-identification
usable by non-experts and scalable for use by multiple conservation NGOs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulits_P/0/1/0/all/0/1"&gt;Peter Kulits&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wall_J/0/1/0/all/0/1"&gt;Jake Wall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bedetti_A/0/1/0/all/0/1"&gt;Anka Bedetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henley_M/0/1/0/all/0/1"&gt;Michelle Henley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1"&gt;Sara Beery&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face Sketch Synthesis via Semantic-Driven Generative Adversarial Network. (arXiv:2106.15121v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15121</id>
        <link href="http://arxiv.org/abs/2106.15121"/>
        <updated>2021-06-30T02:01:00.900Z</updated>
        <summary type="html"><![CDATA[Face sketch synthesis has made significant progress with the development of
deep neural networks in these years. The delicate depiction of sketch portraits
facilitates a wide range of applications like digital entertainment and law
enforcement. However, accurate and realistic face sketch generation is still a
challenging task due to the illumination variations and complex backgrounds in
the real scenes. To tackle these challenges, we propose a novel Semantic-Driven
Generative Adversarial Network (SDGAN) which embeds global structure-level
style injection and local class-level knowledge re-weighting. Specifically, we
conduct facial saliency detection on the input face photos to provide overall
facial texture structure, which could be used as a global type of prior
information. In addition, we exploit face parsing layouts as the semantic-level
spatial prior to enforce globally structural style injection in the generator
of SDGAN. Furthermore, to enhance the realistic effect of the details, we
propose a novel Adaptive Re-weighting Loss (ARLoss) which dedicates to balance
the contributions of different semantic classes. Experimentally, our extensive
experiments on CUFS and CUFSF datasets show that our proposed algorithm
achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xingqun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Muyi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weining Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaoxiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1"&gt;Caifeng Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System. (arXiv:2106.15130v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.15130</id>
        <link href="http://arxiv.org/abs/2106.15130"/>
        <updated>2021-06-30T02:01:00.884Z</updated>
        <summary type="html"><![CDATA[The last-generation video conferencing software allows users to utilize a
virtual background to conceal their personal environment due to privacy
concerns, especially in official meetings with other employers. On the other
hand, users maybe want to fool people in the meeting by considering the virtual
background to conceal where they are. In this case, developing tools to
understand the virtual background utilize for fooling people in meeting plays
an important role. Besides, such detectors must prove robust against different
kinds of attacks since a malicious user can fool the detector by applying a set
of adversarial editing steps on the video to conceal any revealing footprint.
In this paper, we study the feasibility of an efficient tool to detect whether
a videoconferencing user background is real. In particular, we provide the
first tool which computes pixel co-occurrences matrices and uses them to search
for inconsistencies among spectral and spatial bands. Our experiments confirm
that cross co-occurrences matrices improve the robustness of the detector
against different kinds of attacks. This work's performance is especially
noteworthy with regard to color SPAM features. Moreover, the performance
especially is significant with regard to robustness versus post-processing,
like geometric transformations, filtering, contrast enhancement, and JPEG
compression with different quality factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1"&gt;Mauro Conti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1"&gt;Simone Milani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1"&gt;Ehsan Nowroozi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orazi_G/0/1/0/all/0/1"&gt;Gabriele Orazi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Deep Neural Networks for Image Document Enhancement. (arXiv:2106.15286v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15286</id>
        <link href="http://arxiv.org/abs/2106.15286"/>
        <updated>2021-06-30T02:01:00.879Z</updated>
        <summary type="html"><![CDATA[This work evaluates six state-of-the-art deep neural network (DNN)
architectures applied to the problem of enhancing camera-captured document
images. The results from each network were evaluated both qualitatively and
quantitatively using Image Quality Assessment (IQA) metrics, and also compared
with an existing approach based on traditional computer vision techniques. The
best performing architectures generally produced good enhancement compared to
the existing algorithm, showing that it is possible to use DNNs for document
image enhancement. Furthermore, the best performing architectures could work as
a baseline for future investigations on document enhancement using deep
learning techniques. The main contributions of this paper are: a baseline of
deep learning techniques that can be further improved to provide better
results, and a evaluation methodology using IQA metrics for quantitatively
comparing the produced images from the neural networks to a ground truth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirsten_L/0/1/0/all/0/1"&gt;Lucas N. Kirsten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piccoli_R/0/1/0/all/0/1"&gt;Ricardo Piccoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribani_R/0/1/0/all/0/1"&gt;Ricardo Ribani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SRF-Net: Selective Receptive Field Network for Anchor-Free Temporal Action Detection. (arXiv:2106.15258v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15258</id>
        <link href="http://arxiv.org/abs/2106.15258"/>
        <updated>2021-06-30T02:01:00.862Z</updated>
        <summary type="html"><![CDATA[Temporal action detection (TAD) is a challenging task which aims to
temporally localize and recognize the human action in untrimmed videos. Current
mainstream one-stage TAD approaches localize and classify action proposals
relying on pre-defined anchors, where the location and scale for action
instances are set by designers. Obviously, such an anchor-based TAD method
limits its generalization capability and will lead to performance degradation
when videos contain rich action variation. In this study, we explore to remove
the requirement of pre-defined anchors for TAD methods. A novel TAD model
termed as Selective Receptive Field Network (SRF-Net) is developed, in which
the location offsets and classification scores at each temporal location can be
directly estimated in the feature map and SRF-Net is trained in an end-to-end
manner. Innovatively, a building block called Selective Receptive Field
Convolution (SRFC) is dedicatedly designed which is able to adaptively adjust
its receptive field size according to multiple scales of input information at
each temporal location in the feature map. Extensive experiments are conducted
on the THUMOS14 dataset, and superior results are reported comparing to
state-of-the-art TAD approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ning_R/0/1/0/all/0/1"&gt;Ranyu Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Can Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15282</id>
        <link href="http://arxiv.org/abs/2106.15282"/>
        <updated>2021-06-30T02:01:00.857Z</updated>
        <summary type="html"><![CDATA[We show that cascaded diffusion models are capable of generating high
fidelity images on the class-conditional ImageNet generation challenge, without
any assistance from auxiliary image classifiers to boost sample quality. A
cascaded diffusion model comprises a pipeline of multiple diffusion models that
generate images of increasing resolution, beginning with a standard diffusion
model at the lowest resolution, followed by one or more super-resolution
diffusion models that successively upsample the image and add higher resolution
details. We find that the sample quality of a cascading pipeline relies
crucially on conditioning augmentation, our proposed method of data
augmentation of the lower resolution conditioning inputs to the
super-resolution models. Our experiments show that conditioning augmentation
prevents compounding error during sampling in a cascaded model, helping us to
train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at
128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1"&gt;Chitwan Saharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1"&gt;William Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1"&gt;Tim Salimans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perception-aware Multi-sensor Fusion for 3D LiDAR Semantic Segmentation. (arXiv:2106.15277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15277</id>
        <link href="http://arxiv.org/abs/2106.15277"/>
        <updated>2021-06-30T02:01:00.849Z</updated>
        <summary type="html"><![CDATA[3D LiDAR (light detection and ranging) based semantic segmentation is
important in scene understanding for many applications, such as auto-driving
and robotics. For example, for autonomous cars equipped with RGB cameras and
LiDAR, it is crucial to fuse complementary information from different sensors
for robust and accurate segmentation. Existing fusion-based methods, however,
may not achieve promising performance due to the vast difference between two
modalities. In this work, we investigate a collaborative fusion scheme called
perception-aware multi-sensor fusion (PMF) to exploit perceptual information
from two modalities, namely, appearance information from RGB images and
spatio-depth information from point clouds. To this end, we first project point
clouds to the camera coordinates to provide spatio-depth information for RGB
images. Then, we propose a two-stream network to extract features from the two
modalities, separately, and fuse the features by effective residual-based
fusion modules. Moreover, we propose additional perception-aware losses to
measure the great perceptual difference between the two modalities. Extensive
experiments on two benchmark data sets show the superiority of our method. For
example, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8%
in mIoU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1"&gt;Zhuangwei Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanqing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Xihe: A 3D Vision-based Lighting Estimation Framework for Mobile Augmented Reality. (arXiv:2106.15280v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15280</id>
        <link href="http://arxiv.org/abs/2106.15280"/>
        <updated>2021-06-30T02:01:00.832Z</updated>
        <summary type="html"><![CDATA[Omnidirectional lighting provides the foundation for achieving
spatially-variant photorealistic 3D rendering, a desirable property for mobile
augmented reality applications. However, in practice, estimating
omnidirectional lighting can be challenging due to limitations such as partial
panoramas of the rendering positions, and the inherent environment lighting and
mobile user dynamics. A new opportunity arises recently with the advancements
in mobile 3D vision, including built-in high-accuracy depth sensors and deep
learning-powered algorithms, which provide the means to better sense and
understand the physical surroundings. Centering the key idea of 3D vision, in
this work, we design an edge-assisted framework called Xihe to provide mobile
AR applications the ability to obtain accurate omnidirectional lighting
estimation in real time. Specifically, we develop a novel sampling technique
that efficiently compresses the raw point cloud input generated at the mobile
device. This technique is derived based on our empirical analysis of a recent
3D indoor dataset and plays a key role in our 3D vision-based lighting
estimator pipeline design. To achieve the real-time goal, we develop a tailored
GPU pipeline for on-device point cloud processing and use an encoding technique
that reduces network transmitted bytes. Finally, we present an adaptive
triggering strategy that allows Xihe to skip unnecessary lighting estimations
and a practical way to provide temporal coherent rendering integration with the
mobile AR ecosystem. We evaluate both the lighting estimation accuracy and time
of Xihe using a reference mobile application developed with Xihe's APIs. Our
results show that Xihe takes as fast as 20.67ms per lighting estimation and
achieves 9.4% better estimation accuracy than a state-of-the-art neural
network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiqin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tian Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Depth from Semantic Segmentation using Game Engine Dataset. (arXiv:2106.15257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15257</id>
        <link href="http://arxiv.org/abs/2106.15257"/>
        <updated>2021-06-30T02:01:00.826Z</updated>
        <summary type="html"><![CDATA[Depth perception is fundamental for robots to understand the surrounding
environment. As the view of cognitive neuroscience, visual depth perception
methods are divided into three categories, namely binocular, active, and
pictorial. The first two categories have been studied for decades in detail.
However, research for the exploration of the third category is still in its
infancy and has got momentum by the advent of deep learning methods in recent
years. In cognitive neuroscience, it is known that pictorial depth perception
mechanisms are dependent on the perception of seen objects. Inspired by this
fact, in this thesis, we investigated the relation of perception of objects and
depth estimation convolutional neural networks. For this purpose, we developed
new network structures based on a simple depth estimation network that only
used a single image at its input. Our proposed structures use both an image and
a semantic label of the image as their input. We used semantic labels as the
output of object perception. The obtained results of performance comparison
between the developed network and original network showed that our novel
structures can improve the performance of depth estimation by 52\% of relative
error of distance in the examined cases. Most of the experimental studies were
carried out on synthetic datasets that were generated by game engines to
isolate the performance comparison from the effect of inaccurate depth and
semantic labels of non-synthetic datasets. It is shown that particular
synthetic datasets may be used for training of depth networks in cases that an
appropriate dataset is not available. Furthermore, we showed that in these
cases, usage of semantic labels improves the robustness of the network against
domain shift from synthetic training data to non-synthetic test data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kashi_M/0/1/0/all/0/1"&gt;Mohammad Amin Kashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Cervical Whole Slide Image Analysis Framework Based on Multi-scale Semantic and Spatial Features using Deep Learning. (arXiv:2106.15113v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15113</id>
        <link href="http://arxiv.org/abs/2106.15113"/>
        <updated>2021-06-30T02:01:00.820Z</updated>
        <summary type="html"><![CDATA[Digital gigapixel whole slide image (WSI) is widely used in clinical
diagnosis, and automated WSI analysis is key for computer-aided diagnosis.
Currently, analyzing the integrated descriptor of probabilities or feature maps
from massive local patches encoded by ResNet classifier is the main manner for
WSI-level prediction. Feature representations of the sparse and tiny lesion
cells in cervical slides, however, are still challengeable for the
under-promoted upstream encoders, while the unused spatial representations of
cervical cells are the available features to supply the semantics analysis. As
well as patches sampling with overlap and repetitive processing incur the
inefficiency and the unpredictable side effect. This study designs a novel
inline connection network (InCNet) by enriching the multi-scale connectivity to
build the lightweight model named You Only Look Cytopathology Once (YOLCO) with
the additional supervision of spatial information. The proposed model allows
the input size enlarged to megapixel that can stitch the WSI without any
overlap by the average repeats decreased from $10^3\sim10^4$ to $10^1\sim10^2$
for collecting features and predictions at two scales. Based on Transformer for
classifying the integrated multi-scale multi-task features, the experimental
results appear $0.872$ AUC score better and $2.51\times$ faster than the best
conventional method in WSI classification on multicohort datasets of 2,019
slides from four scanning devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Ziquan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Shenghua Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiuli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shaoqun Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via Implicit Neural Representation. (arXiv:2106.15097v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.15097</id>
        <link href="http://arxiv.org/abs/2106.15097"/>
        <updated>2021-06-30T02:01:00.815Z</updated>
        <summary type="html"><![CDATA[For collecting high-quality high-resolution (HR) MR image, we propose a novel
image reconstruction network named IREM, which is trained on multiple
low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR
image reconstruction. In this work, we suppose the desired HR image as an
implicit continuous function of the 3D image spatial coordinate and the
thick-slice LR images as several sparse discrete samplings of this function.
Then the super-resolution (SR) task is to learn the continuous volumetric
function from a limited observations using an fully-connected neural network
combined with Fourier feature positional encoding. By simply minimizing the
error between the network prediction and the acquired LR image intensity across
each imaging plane, IREM is trained to represent a continuous model of the
observed tissue anatomy. Experimental results indicate that IREM succeeds in
representing high frequency image feature, and in real scene data collection,
IREM reduces scan time and achieves high-quality high-resolution MR imaging in
terms of SNR and local image detail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qing Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ruiming Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1"&gt;Hongjiang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_B/0/1/0/all/0/1"&gt;Boliang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuyao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TUCaN: Progressively Teaching Colourisation to Capsules. (arXiv:2106.15176v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15176</id>
        <link href="http://arxiv.org/abs/2106.15176"/>
        <updated>2021-06-30T02:01:00.808Z</updated>
        <summary type="html"><![CDATA[Automatic image colourisation is the computer vision research path that
studies how to colourise greyscale images (for restoration). Deep learning
techniques improved image colourisation yielding astonishing results. These
differ by various factors, such as structural differences, input types, user
assistance, etc. Most of them, base the architectural structure on
convolutional layers with no emphasis on layers specialised in object features
extraction. We introduce a novel downsampling upsampling architecture named
TUCaN (Tiny UCapsNet) that exploits the collaboration of convolutional layers
and capsule layers to obtain a neat colourisation of entities present in every
single image. This is obtained by enforcing collaboration among such layers by
skip and residual connections. We pose the problem as a per pixel colour
classification task that identifies colours as a bin in a quantized space. To
train the network, in contrast with the standard end to end learning method, we
propose the progressive learning scheme to extract the context of objects by
only manipulating the learning process without changing the model. In this
scheme, the upsampling starts from the reconstruction of low resolution images
and progressively grows to high resolution images throughout the training
phase. Experimental results on three benchmark datasets show that our approach
with ImageNet10k dataset outperforms existing methods on standard quality
metrics and achieves state of the art performances on image colourisation. We
performed a user study to quantify the perceptual realism of the colourisation
results demonstrating: that progressive learning let the TUCaN achieve better
colours than the end to end scheme; and pointing out the limitations of the
existing evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pucci_R/0/1/0/all/0/1"&gt;Rita Pucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinel_N/0/1/0/all/0/1"&gt;Niki Martinel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SDL: New data generation tools for full-level annotated document layout. (arXiv:2106.15117v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15117</id>
        <link href="http://arxiv.org/abs/2106.15117"/>
        <updated>2021-06-30T02:01:00.793Z</updated>
        <summary type="html"><![CDATA[We present a novel data generation tool for document processing. The tool
focuses on providing a maximal level of visual information in a normal type
document, ranging from character position to paragraph-level position. It also
enables working with a large dataset on low-resource languages as well as
providing a mean of processing thorough full-level information of the
documented text. The data generation tools come with a dataset of 320000
Vietnamese synthetic document images and an instruction to generate a dataset
of similar size in other languages. The repository can be found at:
https://github.com/tson1997/SDL-Document-Image-Generation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1"&gt;Son Nguyen Truong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GuidedMix-Net: Learning to Improve Pseudo Masks Using Labeled Images as Reference. (arXiv:2106.15064v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15064</id>
        <link href="http://arxiv.org/abs/2106.15064"/>
        <updated>2021-06-30T02:01:00.788Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning is a challenging problem which aims to construct a
model by learning from a limited number of labeled examples. Numerous methods
have been proposed to tackle this problem, with most focusing on utilizing the
predictions of unlabeled instances consistency alone to regularize networks.
However, treating labeled and unlabeled data separately often leads to the
discarding of mass prior knowledge learned from the labeled examples, and
failure to mine the feature interaction between the labeled and unlabeled image
pairs. In this paper, we propose a novel method for semi-supervised semantic
segmentation named GuidedMix-Net, by leveraging labeled information to guide
the learning of unlabeled instances. Specifically, we first introduce a feature
alignment objective between labeled and unlabeled data to capture potentially
similar image pairs and then generate mixed inputs from them. The proposed
mutual information transfer (MITrans), based on the cluster assumption, is
shown to be a powerful knowledge module for further progressive refining
features of unlabeled data in the mixed data space. To take advantage of the
labeled examples and guide unlabeled data learning, we further propose a mask
generation module to generate high-quality pseudo masks for the unlabeled data.
Along with supervised learning for labeled data, the prediction of unlabeled
data is jointly learned with the generated pseudo masks from the mixed data.
Extensive experiments on PASCAL VOC 2012, PASCAL-Context and Cityscapes
demonstrate the effectiveness of our GuidedMix-Net, which achieves competitive
segmentation accuracy and significantly improves the mIoU by +7$\%$ compared to
previous state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1"&gt;Peng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yawen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition. (arXiv:2106.15125v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15125</id>
        <link href="http://arxiv.org/abs/2106.15125"/>
        <updated>2021-06-30T02:01:00.783Z</updated>
        <summary type="html"><![CDATA[One essential problem in skeleton-based action recognition is how to extract
discriminative features over all skeleton joints. However, the complexity of
the recent State-Of-The-Art (SOTA) models for this task tends to be exceedingly
sophisticated and over-parameterized. The low efficiency in model training and
inference has increased the validation costs of model architectures in
large-scale datasets. To address the above issue, recent advanced separable
convolutional layers are embedded into an early fused Multiple Input Branches
(MIB) network, constructing an efficient Graph Convolutional Network (GCN)
baseline for skeleton-based action recognition. In addition, based on such the
baseline, we design a compound scaling strategy to expand the model's width and
depth synchronously, and eventually obtain a family of efficient GCN baselines
with high accuracies and small amounts of trainable parameters, termed
EfficientGCN-Bx, where ''x'' denotes the scaling coefficient. On two
large-scale datasets, i.e., NTU RGB+D 60 and 120, the proposed EfficientGCN-B4
baseline outperforms other SOTA methods, e.g., achieving 91.7% accuracy on the
cross-subject benchmark of NTU 60 dataset, while being 3.15x smaller and 3.21x
faster than MS-G3D, which is one of the best SOTA methods. The source code in
PyTorch version and the pretrained models are available at
https://github.com/yfsong0709/EfficientGCNv1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yi-Fan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1"&gt;Caifeng Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open-Set Representation Learning through Combinatorial Embedding. (arXiv:2106.15278v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15278</id>
        <link href="http://arxiv.org/abs/2106.15278"/>
        <updated>2021-06-30T02:01:00.777Z</updated>
        <summary type="html"><![CDATA[Visual recognition tasks are often limited to dealing with a small subset of
classes simply because the labels for the remaining classes are unavailable. We
are interested in identifying novel concepts in a dataset through
representation learning based on the examples in both labeled and unlabeled
classes, and extending the horizon of recognition to both known and novel
classes. To address this challenging task, we propose a combinatorial learning
approach, which naturally clusters the examples in unseen classes using the
compositional knowledge given by multiple supervised meta-classifiers on
heterogeneous label spaces. We also introduce a metric learning strategy to
estimate pairwise pseudo-labels for improving representations of unlabeled
examples, which preserves semantic relations across known and novel classes
effectively. The proposed algorithm discovers novel concepts via a joint
optimization of enhancing the discrimitiveness of unseen classes as well as
learning the representations of known classes generalizable to novel ones. Our
extensive experiments demonstrate remarkable performance gains by the proposed
approach in multiple image retrieval and novel class discovery benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Geeho Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bohyung Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning. (arXiv:2106.15087v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15087</id>
        <link href="http://arxiv.org/abs/2106.15087"/>
        <updated>2021-06-30T02:01:00.772Z</updated>
        <summary type="html"><![CDATA[Contrary to the vast literature in modeling, perceiving, and understanding
agent-object (e.g., human-object, hand-object, robot-object) interaction in
computer vision and robotics, very few past works have studied the task of
object-object interaction, which also plays an important role in robotic
manipulation and planning tasks. There is a rich space of object-object
interaction scenarios in our daily life, such as placing an object on a messy
tabletop, fitting an object inside a drawer, pushing an object using a tool,
etc. In this paper, we propose a unified affordance learning framework to learn
object-object interaction for various tasks. By constructing four object-object
interaction task environments using physical simulation (SAPIEN) and thousands
of ShapeNet models with rich geometric diversity, we are able to conduct
large-scale object-object affordance learning without the need for human
annotations or demonstrations. At the core of technical contribution, we
propose an object-kernel point convolution network to reason about detailed
interaction between two objects. Experiments on large-scale synthetic data and
real-world data prove the effectiveness of the proposed approach. Please refer
to the project webpage for code, data, video, and more materials:
https://cs.stanford.edu/~kaichun/o2oafford]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1"&gt;Kaichun Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yuzhe Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1"&gt;Fanbo Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inconspicuous Adversarial Patches for Fooling Image Recognition Systems on Mobile Devices. (arXiv:2106.15202v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15202</id>
        <link href="http://arxiv.org/abs/2106.15202"/>
        <updated>2021-06-30T02:01:00.756Z</updated>
        <summary type="html"><![CDATA[Deep learning based image recognition systems have been widely deployed on
mobile devices in today's world. In recent studies, however, deep learning
models are shown vulnerable to adversarial examples. One variant of adversarial
examples, called adversarial patch, draws researchers' attention due to its
strong attack abilities. Though adversarial patches achieve high attack success
rates, they are easily being detected because of the visual inconsistency
between the patches and the original images. Besides, it usually requires a
large amount of data for adversarial patch generation in the literature, which
is computationally expensive and time-consuming. To tackle these challenges, we
propose an approach to generate inconspicuous adversarial patches with one
single image. In our approach, we first decide the patch locations basing on
the perceptual sensitivity of victim models, then produce adversarial patches
in a coarse-to-fine way by utilizing multiple-scale generators and
discriminators. The patches are encouraged to be consistent with the background
images with adversarial training while preserving strong attack abilities. Our
approach shows the strong attack abilities in white-box settings and the
excellent transferability in black-box settings through extensive experiments
on various models with different architectures and training methods. Compared
to other adversarial patches, our adversarial patches hold the most negligible
risks to be detected and can evade human observations, which is supported by
the illustrations of saliency maps and results of user evaluations. Lastly, we
show that our adversarial patches can be applied in the physical world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1"&gt;Tao Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jinqi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jun Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoNovel: Automatically Discovering and Learning Novel Visual Categories. (arXiv:2106.15252v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15252</id>
        <link href="http://arxiv.org/abs/2106.15252"/>
        <updated>2021-06-30T02:01:00.750Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of discovering novel classes in an image collection
given labelled examples of other classes. We present a new approach called
AutoNovel to address this problem by combining three ideas: (1) we suggest that
the common approach of bootstrapping an image representation using the labelled
data only introduces an unwanted bias, and that this can be avoided by using
self-supervised learning to train the representation from scratch on the union
of labelled and unlabelled data; (2) we use ranking statistics to transfer the
model's knowledge of the labelled classes to the problem of clustering the
unlabelled images; and, (3) we train the data representation by optimizing a
joint objective function on the labelled and unlabelled subsets of the data,
improving both the supervised classification of the labelled data, and the
clustering of the unlabelled data. Moreover, we propose a method to estimate
the number of classes for the case where the number of new categories is not
known a priori. We evaluate AutoNovel on standard classification benchmarks and
substantially outperform current methods for novel category discovery. In
addition, we also show that AutoNovel can be used for fully unsupervised image
clustering, achieving promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rebuffi_S/0/1/0/all/0/1"&gt;Sylvestre-Alvise Rebuffi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehrhardt_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ehrhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15281</id>
        <link href="http://arxiv.org/abs/2106.15281"/>
        <updated>2021-06-30T02:01:00.745Z</updated>
        <summary type="html"><![CDATA[In recent years, the growth of Machine Learning algorithms in a variety of
different applications has raised numerous studies on the applicability of
these algorithms in real scenarios. Among all, one of the hardest scenarios,
due to its physical requirements, is the aerospace one. In this context, the
authors of this work aim to propose a first prototype and a study of
feasibility for an AI model to be 'loaded' on board. As a case study, the
authors decided to investigate the detection of volcanic eruptions as a method
to swiftly produce alerts. Two Convolutional Neural Networks have been proposed
and created, also showing how to correctly implement them on real hardware and
how the complexity of a CNN can be adapted to fit computational requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1"&gt;Maria Pia Del Rosso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1"&gt;Alessandro Sebastianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1"&gt;Dario Spiller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1"&gt;Pierre Philippe Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1"&gt;Silvia Liberata Ullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Exit Vision Transformer for Dynamic Inference. (arXiv:2106.15183v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15183</id>
        <link href="http://arxiv.org/abs/2106.15183"/>
        <updated>2021-06-30T02:01:00.739Z</updated>
        <summary type="html"><![CDATA[Deep neural networks can be converted to multi-exit architectures by
inserting early exit branches after some of their intermediate layers. This
allows their inference process to become dynamic, which is useful for time
critical IoT applications with stringent latency requirements, but with
time-variant communication and computation resources. In particular, in edge
computing systems and IoT networks where the exact computation time budget is
variable and not known beforehand. Vision Transformer is a recently proposed
architecture which has since found many applications across various domains of
computer vision. In this work, we propose seven different architectures for
early exit branches that can be used for dynamic inference in Vision
Transformer backbones. Through extensive experiments involving both
classification and regression problems, we show that each one of our proposed
architectures could prove useful in the trade-off between accuracy and speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bakhtiarnia_A/0/1/0/all/0/1"&gt;Arian Bakhtiarnia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Context for Action Detection. (arXiv:2106.15171v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15171</id>
        <link href="http://arxiv.org/abs/2106.15171"/>
        <updated>2021-06-30T02:01:00.734Z</updated>
        <summary type="html"><![CDATA[Research in action detection has grown in the recentyears, as it plays a key
role in video understanding. Modelling the interactions (either spatial or
temporal) between actors and their context has proven to be essential for this
task. While recent works use spatial features with aggregated temporal
information, this work proposes to use non-aggregated temporal information.
This is done by adding an attention based method that leverages spatio-temporal
interactions between elements in the scene along the clip.The main contribution
of this work is the introduction of two cross attention blocks to effectively
model the spatial relations and capture short range temporal
interactions.Experiments on the AVA dataset show the advantages of the proposed
approach that models spatio-temporal relations between relevant elements in the
scene, outperforming other methods that model actor interactions with their
context by +0.31 mAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caldero_M/0/1/0/all/0/1"&gt;Manuel Sarmiento Calder&amp;#xf3;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varas_D/0/1/0/all/0/1"&gt;David Varas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bou_Balust_E/0/1/0/all/0/1"&gt;Elisenda Bou-Balust&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Driving Implementation in an Experimental Environment. (arXiv:2106.15274v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.15274</id>
        <link href="http://arxiv.org/abs/2106.15274"/>
        <updated>2021-06-30T02:01:00.687Z</updated>
        <summary type="html"><![CDATA[Autonomous systems require identifying the environment and it has a long way
to go before putting it safely into practice. In autonomous driving systems,
the detection of obstacles and traffic lights are of importance as well as lane
tracking. In this study, an autonomous driving system is developed and tested
in the experimental environment designed for this purpose. In this system, a
model vehicle having a camera is used to trace the lanes and avoid obstacles to
experimentally study autonomous driving behavior. Convolutional Neural Network
models were trained for Lane tracking. For the vehicle to avoid obstacles,
corner detection, optical flow, focus of expansion, time to collision, balance
calculation, and decision mechanism were created, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aliyev_N/0/1/0/all/0/1"&gt;Namig Aliyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sezer_O/0/1/0/all/0/1"&gt;Oguzhan Sezer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1"&gt;Mehmet Turan Guzel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Robust Regression to Find Font Usage Trends. (arXiv:2106.15232v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15232</id>
        <link href="http://arxiv.org/abs/2106.15232"/>
        <updated>2021-06-30T02:01:00.682Z</updated>
        <summary type="html"><![CDATA[Fonts have had trends throughout their history, not only in when they were
invented but also in their usage and popularity. In this paper, we attempt to
specifically find the trends in font usage using robust regression on a large
collection of text images. We utilize movie posters as the source of fonts for
this task because movie posters can represent time periods by using their
release date. In addition, movie posters are documents that are carefully
designed and represent a wide range of fonts. To understand the relationship
between the fonts of movie posters and time, we use a regression Convolutional
Neural Network (CNN) to estimate the release year of a movie using an isolated
title text image. Due to the difficulty of the task, we propose to use of a
hybrid training regimen that uses a combination of Mean Squared Error (MSE) and
Tukey's biweight loss. Furthermore, we perform a thorough analysis on the
trends of fonts through time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsuji_K/0/1/0/all/0/1"&gt;Kaigen Tsuji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haraguchi_D/0/1/0/all/0/1"&gt;Daichi Haraguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1"&gt;Brian Kenji Iwana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding the Effectiveness of Attention Mechanism. (arXiv:2106.15067v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15067</id>
        <link href="http://arxiv.org/abs/2106.15067"/>
        <updated>2021-06-30T02:01:00.585Z</updated>
        <summary type="html"><![CDATA[Attention Mechanism is a widely used method for improving the performance of
convolutional neural networks (CNNs) on computer vision tasks. Despite its
pervasiveness, we have a poor understanding of what its effectiveness stems
from. It is popularly believed that its effectiveness stems from the visual
attention explanation, advocating focusing on the important part of input data
rather than ingesting the entire input. In this paper, we find that there is
only a weak consistency between the attention weights of features and their
importance. Instead, we verify the crucial role of feature map multiplication
in attention mechanism and uncover a fundamental impact of feature map
multiplication on the learned landscapes of CNNs: with the high order
non-linearity brought by the feature map multiplication, it played a
regularization role on CNNs, which made them learn smoother and more stable
landscapes near real samples compared to vanilla CNNs. This smoothness and
stability induce a more predictive and stable behavior in-between real samples,
and make CNNs generate better. Moreover, motivated by the proposed
effectiveness of feature map multiplication, we design feature map
multiplication network (FMMNet) by simply replacing the feature map addition in
ResNet with feature map multiplication. FMMNet outperforms ResNet on various
datasets, and this indicates that feature map multiplication plays a vital role
in improving the performance even without finely designed attention mechanism
in existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zihang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Heng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals. (arXiv:2106.15004v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15004</id>
        <link href="http://arxiv.org/abs/2106.15004"/>
        <updated>2021-06-30T02:01:00.579Z</updated>
        <summary type="html"><![CDATA[Accurately predicting the future motion of surrounding vehicles requires
reasoning about the inherent uncertainty in goals and driving behavior. This
uncertainty can be loosely decoupled into lateral (e.g., keeping lane, turning)
and longitudinal (e.g., accelerating, braking). We present a novel method that
combines learned discrete policy rollouts with a focused decoder on subsets of
the lane graph. The policy rollouts explore different goals given our current
observations, ensuring that the model captures lateral variability. The
longitudinal variability is captured by our novel latent variable model decoder
that is conditioned on various subsets of the lane graph. Our model achieves
state-of-the-art performance on the nuScenes motion prediction dataset, and
qualitatively demonstrates excellent scene compliance. Detailed ablations
highlight the importance of both the policy rollouts and the decoder
architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1"&gt;Nachiket Deo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1"&gt;Eric M. Wolff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1"&gt;Oscar Beijbom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are conditional GANs explicitly conditional?. (arXiv:2106.15011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15011</id>
        <link href="http://arxiv.org/abs/2106.15011"/>
        <updated>2021-06-30T02:01:00.563Z</updated>
        <summary type="html"><![CDATA[This paper proposes two important contributions for conditional Generative
Adversarial Networks (cGANs) to improve the wide variety of applications that
exploit this architecture. The first main contribution is an analysis of cGANs
to show that they are not explicitly conditional. In particular, it will be
shown that the discriminator and subsequently the cGAN does not automatically
learn the conditionality between inputs. The second contribution is a new
method, called acontrario, that explicitly models conditionality for both parts
of the adversarial architecture via a novel acontrario loss that involves
training the discriminator to learn unconditional (adverse) examples. This
leads to a novel type of data augmentation approach for GANs (acontrario
learning) which allows to restrict the search space of the generator to
conditional outputs using adverse examples. Extensive experimentation is
carried out to evaluate the conditionality of the discriminator by proposing a
probability distribution analysis. Comparisons with the cGAN architecture for
different applications show significant improvements in performance on well
known datasets including, semantic image synthesis, image segmentation and
monocular depth prediction using different metrics including Fr\'echet
Inception Distance(FID), mean Intersection over Union (mIoU), Root Mean Square
Error log (RMSE log) and Number of statistically-Different Bins (NDB)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boulahbal_H/0/1/0/all/0/1"&gt;Houssem-eddine Boulahbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voicila_A/0/1/0/all/0/1"&gt;Adrian Voicila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comport_A/0/1/0/all/0/1"&gt;Andrew Comport&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Transferability of Adversarial Patches on Face Recognition with Generative Models. (arXiv:2106.15058v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15058</id>
        <link href="http://arxiv.org/abs/2106.15058"/>
        <updated>2021-06-30T02:01:00.558Z</updated>
        <summary type="html"><![CDATA[Face recognition is greatly improved by deep convolutional neural networks
(CNNs). Recently, these face recognition models have been used for identity
authentication in security sensitive applications. However, deep CNNs are
vulnerable to adversarial patches, which are physically realizable and
stealthy, raising new security concerns on the real-world applications of these
models. In this paper, we evaluate the robustness of face recognition models
using adversarial patches based on transferability, where the attacker has
limited accessibility to the target models. First, we extend the existing
transfer-based attack techniques to generate transferable adversarial patches.
However, we observe that the transferability is sensitive to initialization and
degrades when the perturbation magnitude is large, indicating the overfitting
to the substitute models. Second, we propose to regularize the adversarial
patches on the low dimensional data manifold. The manifold is represented by
generative models pre-trained on legitimate human face images. Using face-like
features as adversarial perturbations through optimization on the manifold, we
show that the gaps between the responses of substitute models and the target
models dramatically decrease, exhibiting a better transferability. Extensive
digital world experiments are conducted to demonstrate the superiority of the
proposed method in the black-box setting. We apply the proposed method in the
physical world as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zihao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Chilin Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaolu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constructing Forest Biomass Prediction Maps from Radar Backscatter by Sequential Regression with a Conditional Generative Adversarial Network. (arXiv:2106.15020v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15020</id>
        <link href="http://arxiv.org/abs/2106.15020"/>
        <updated>2021-06-30T02:01:00.551Z</updated>
        <summary type="html"><![CDATA[This paper studies construction of above-ground biomass (AGB) prediction maps
from synthetic aperture radar (SAR) intensity images. The purpose is to improve
traditional regression models based on SAR intensity, trained with a limited
amount of AGB in situ measurements. Although it is costly to collect, data from
airborne laser scanning (ALS) sensors are highly correlated with AGB.
Therefore, we propose using AGB predictions based on ALS data as surrogate
response variables for SAR data in a sequential modelling fashion. This
increases the amount of training data dramatically. To model the regression
function between SAR intensity and ALS-predicted AGB we propose to utilise a
conditional generative adversarial network (cGAN), i.e. the Pix2Pix
convolutional neural network. This enables the recreation of existing ALS-based
AGB prediction maps. The generated synthesised ALS-based AGB predictions are
evaluated qualitatively and quantitatively against ALS-based AGB predictions
retrieved from a traditional non-sequential regression model trained in the
same area. Results show that the proposed architecture manages to capture
characteristics of the actual data. This suggests that the use of ALS-guided
generative models is a promising avenue for AGB prediction from SAR intensity.
Further research on this area has the potential of providing both large-scale
and low-cost predictions of AGB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bjork_S/0/1/0/all/0/1"&gt;Sara Bj&amp;#xf6;rk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anfinsen_S/0/1/0/all/0/1"&gt;Stian Normann Anfinsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naesset_E/0/1/0/all/0/1"&gt;Erik N&amp;#xe6;sset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gobakken_T/0/1/0/all/0/1"&gt;Terje Gobakken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zahabu_E/0/1/0/all/0/1"&gt;Eliakimu Zahabu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Reach Real-Time AI on Consumer Devices? Solutions for Programmable and Custom Architectures. (arXiv:2106.15021v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15021</id>
        <link href="http://arxiv.org/abs/2106.15021"/>
        <updated>2021-06-30T02:01:00.544Z</updated>
        <summary type="html"><![CDATA[The unprecedented performance of deep neural networks (DNNs) has led to large
strides in various Artificial Intelligence (AI) inference tasks, such as object
and speech recognition. Nevertheless, deploying such AI models across commodity
devices faces significant challenges: large computational cost, multiple
performance objectives, hardware heterogeneity and a common need for high
accuracy, together pose critical problems to the deployment of DNNs across the
various embedded and mobile devices in the wild. As such, we have yet to
witness the mainstream usage of state-of-the-art deep learning algorithms
across consumer devices. In this paper, we provide preliminary answers to this
potentially game-changing question by presenting an array of design techniques
for efficient AI systems. We start by examining the major roadblocks when
targeting both programmable processors and custom accelerators. Then, we
present diverse methods for achieving real-time performance following a
cross-stack approach. These span model-, system- and hardware-level techniques,
and their combination. Our findings provide illustrative examples of AI systems
that do not overburden mobile hardware, while also indicating how they can
improve inference accuracy. Moreover, we showcase how custom ASIC- and
FPGA-based accelerators can be an enabling factor for next-generation AI
applications, such as multi-DNN systems. Collectively, these results highlight
the critical need for further exploration as to how the various cross-stack
solutions can be best combined in order to bring the latest advances in deep
learning close to users, in a robust and efficient manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1"&gt;Stylianos I. Venieris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panopoulos_I/0/1/0/all/0/1"&gt;Ioannis Panopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1"&gt;Ilias Leontiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_I/0/1/0/all/0/1"&gt;Iakovos S. Venieris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Detection Based Handwriting Localization. (arXiv:2106.14989v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14989</id>
        <link href="http://arxiv.org/abs/2106.14989"/>
        <updated>2021-06-30T02:01:00.538Z</updated>
        <summary type="html"><![CDATA[We present an object detection based approach to localize handwritten regions
from documents, which initially aims to enhance the anonymization during the
data transmission. The concatenated fusion of original and preprocessed images
containing both printed texts and handwritten notes or signatures are fed into
the convolutional neural network, where the bounding boxes are learned to
detect the handwriting. Afterwards, the handwritten regions can be processed
(e.g. replaced with redacted signatures) to conceal the personally identifiable
information (PII). This processing pipeline based on the deep learning network
Cascade R-CNN works at 10 fps on a GPU during the inference, which ensures the
enhanced anonymization with minimal computational overheads. Furthermore, the
impressive generalizability has been empirically showcased: the trained model
based on the English-dominant dataset works well on the fictitious unseen
invoices, even in Chinese. The proposed approach is also expected to facilitate
other tasks such as handwriting recognition and signature verification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuli Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yucheng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_S/0/1/0/all/0/1"&gt;Suting Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following. (arXiv:2106.15045v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15045</id>
        <link href="http://arxiv.org/abs/2106.15045"/>
        <updated>2021-06-30T02:01:00.532Z</updated>
        <summary type="html"><![CDATA[The rapid rise of accessibility of unmanned aerial vehicles or drones pose a
threat to general security and confidentiality. Most of the commercially
available or custom-built drones are multi-rotors and are comprised of multiple
propellers. Since these propellers rotate at a high-speed, they are generally
the fastest moving parts of an image and cannot be directly "seen" by a
classical camera without severe motion blur. We utilize a class of sensors that
are particularly suitable for such scenarios called event cameras, which have a
high temporal resolution, low-latency, and high dynamic range.

In this paper, we model the geometry of a propeller and use it to generate
simulated events which are used to train a deep neural network called EVPropNet
to detect propellers from the data of an event camera. EVPropNet directly
transfers to the real world without any fine-tuning or retraining. We present
two applications of our network: (a) tracking and following an unmarked drone
and (b) landing on a near-hover drone. We successfully evaluate and demonstrate
the proposed approach in many real-world experiments with different propeller
shapes and sizes. Our network can detect propellers at a rate of 85.1% even
when 60% of the propeller is occluded and can run at upto 35Hz on a 2W power
budget. To our knowledge, this is the first deep learning-based solution for
detecting propellers (to detect drones). Finally, our applications also show an
impressive success rate of 92% and 90% for the tracking and landing tasks
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanket_N/0/1/0/all/0/1"&gt;Nitin J. Sanket&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1"&gt;Chahat Deep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parameshwara_C/0/1/0/all/0/1"&gt;Chethan M. Parameshwara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1"&gt;Cornelia Ferm&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;Guido C.H.E. de Croon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1"&gt;Yiannis Aloimonos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End Autofocus Camera for Iris on the Move. (arXiv:2106.15069v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15069</id>
        <link href="http://arxiv.org/abs/2106.15069"/>
        <updated>2021-06-30T02:01:00.509Z</updated>
        <summary type="html"><![CDATA[For distant iris recognition, a long focal length lens is generally used to
ensure the resolution ofiris images, which reduces the depth of field and leads
to potential defocus blur. To accommodate users at different distances, it is
necessary to control focus quickly and accurately. While for users in motion,
it is expected to maintain the correct focus on the iris area continuously. In
this paper, we introduced a novel rapid autofocus camera for active refocusing
ofthe iris area ofthe moving objects using a focus-tunable lens. Our end-to-end
computational algorithm can predict the best focus position from one single
blurred image and generate a lens diopter control signal automatically. This
scene-based active manipulation method enables real-time focus tracking of the
iris area ofa moving object. We built a testing bench to collect real-world
focal stacks for evaluation of the autofocus methods. Our camera has reached an
autofocus speed ofover 50 fps. The results demonstrate the advantages of our
proposed camera for biometric perception in static and dynamic scenes. The code
is available at https://github.com/Debatrix/AquulaCam.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Leyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kunbo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunlong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenan Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Recalibrated Aggregation Network for Medical Code Prediction. (arXiv:2104.00952v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00952</id>
        <link href="http://arxiv.org/abs/2104.00952"/>
        <updated>2021-06-30T02:01:00.504Z</updated>
        <summary type="html"><![CDATA[Medical coding translates professionally written medical reports into
standardized codes, which is an essential part of medical information systems
and health insurance reimbursement. Manual coding by trained human coders is
time-consuming and error-prone. Thus, automated coding algorithms have been
developed, building especially on the recent advances in machine learning and
deep neural networks. To solve the challenges of encoding lengthy and noisy
clinical documents and capturing code associations, we propose a multitask
recalibrated aggregation network. In particular, multitask learning shares
information across different coding schemes and captures the dependencies
between different medical codes. Feature recalibration and aggregation in
shared modules enhance representation learning for lengthy notes. Experiments
with a real-world MIMIC-III dataset show significantly improved predictive
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shaoxiong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1"&gt;Pekka Marttinen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Uncertainty Estimation Framework for Probabilistic Object Detection. (arXiv:2106.15007v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15007</id>
        <link href="http://arxiv.org/abs/2106.15007"/>
        <updated>2021-06-30T02:01:00.498Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a new technique that combines two popular methods
to estimate uncertainty in object detection. Quantifying uncertainty is
critical in real-world robotic applications. Traditional detection models can
be ambiguous even when they provide a high-probability output. Robot actions
based on high-confidence, yet unreliable predictions, may result in serious
repercussions. Our framework employs deep ensembles and Monte Carlo dropout for
approximating predictive uncertainty, and it improves upon the uncertainty
estimation quality of the baseline method. The proposed approach is evaluated
on publicly available synthetic image datasets captured from sequences of
video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1"&gt;Zongyao Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_N/0/1/0/all/0/1"&gt;Nolan B. Gutierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beksi_W/0/1/0/all/0/1"&gt;William J. Beksi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cosmic-CoNN: A Cosmic Ray Detection Deep-Learning Framework, Dataset, and Toolkit. (arXiv:2106.14922v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2106.14922</id>
        <link href="http://arxiv.org/abs/2106.14922"/>
        <updated>2021-06-30T02:01:00.485Z</updated>
        <summary type="html"><![CDATA[Rejecting cosmic rays (CRs) is essential for scientific interpretation of
CCD-captured data, but detecting CRs in single-exposure images has remained
challenging. Conventional CR-detection algorithms require tuning multiple
parameters experimentally making it hard to automate across different
instruments or observation requests. Recent work using deep learning to train
CR-detection models has demonstrated promising results. However,
instrument-specific models suffer from performance loss on images from
ground-based facilities not included in the training data. In this work, we
present Cosmic-CoNN, a deep-learning framework designed to produce generic
CR-detection models. We build a large, diverse ground-based CR dataset
leveraging thousands of images from the Las Cumbres Observatory global
telescope network to produce a generic CR-detection model which achieves a
99.91% true-positive detection rate and maintains over 96.40% true-positive
rates on unseen data from Gemini GMOS-N/S, with a false-positive rate of 0.01%.
Apart from the open-source framework and dataset, we also build a suite of
tools including console commands, a web-based application, and Python APIs to
make automatic, robust CR detection widely accessible by the community of
astronomers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chengyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+McCully_C/0/1/0/all/0/1"&gt;Curtis McCully&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Dong_B/0/1/0/all/0/1"&gt;Boning Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Howell_D/0/1/0/all/0/1"&gt;D. Andrew Howell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sen_P/0/1/0/all/0/1"&gt;Pradeep Sen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Striking the Right Balance: Recall Loss for Semantic Segmentation. (arXiv:2106.14917v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14917</id>
        <link href="http://arxiv.org/abs/2106.14917"/>
        <updated>2021-06-30T02:01:00.479Z</updated>
        <summary type="html"><![CDATA[Class imbalance is a fundamental problem in computer vision applications such
as semantic segmentation. Specifically, uneven class distributions in a
training dataset often result in unsatisfactory performance on
under-represented classes. Many works have proposed to weight the standard
cross entropy loss function with pre-computed weights based on class
statistics, such as the number of samples and class margins. There are two
major drawbacks to these methods: 1) constantly up-weighting minority classes
can introduce excessive false positives in semantic segmentation; 2) a minority
class is not necessarily a hard class. The consequence is low precision due to
excessive false positives. In this regard, we propose a hard-class mining loss
by reshaping the vanilla cross entropy loss such that it weights the loss for
each class dynamically based on instantaneous recall performance. We show that
the novel recall loss changes gradually between the standard cross entropy loss
and the inverse frequency weighted loss. Recall loss also leads to improved
mean accuracy while offering competitive mean Intersection over Union (IoU)
performance. On Synthia dataset, recall loss achieves 9% relative improvement
on mean accuracy with competitive mean IoU using DeepLab-ResNet18 compared to
the cross entropy loss. Code available at
https://github.com/PotatoTian/recall-semseg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1"&gt;Junjiao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1"&gt;Niluthpol Mithun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seymour_Z/0/1/0/all/0/1"&gt;Zach Seymour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1"&gt;Han-Pang Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1"&gt;Zsolt Kira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Face Anti-Spoofing: A Survey. (arXiv:2106.14948v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14948</id>
        <link href="http://arxiv.org/abs/2106.14948"/>
        <updated>2021-06-30T02:01:00.474Z</updated>
        <summary type="html"><![CDATA[Face anti-spoofing (FAS) has lately attracted increasing attention due to its
vital role in securing face recognition systems from presentation attacks
(PAs). As more and more realistic PAs with novel types spring up, traditional
FAS methods based on handcrafted features become unreliable due to their
limited representation capacity. With the emergence of large-scale academic
datasets in the recent decade, deep learning based FAS achieves remarkable
performance and dominates this area. However, existing reviews in this field
mainly focus on the handcrafted features, which are outdated and uninspiring
for the progress of FAS community. In this paper, to stimulate future research,
we present the first comprehensive review of recent advances in deep learning
based FAS. It covers several novel and insightful components: 1) besides
supervision with binary label (e.g., '0' for bonafide vs. '1' for PAs), we also
investigate recent methods with pixel-wise supervision (e.g., pseudo depth
map); 2) in addition to traditional intra-dataset evaluation, we collect and
analyze the latest methods specially designed for domain generalization and
open-set FAS; and 3) besides commercial RGB camera, we summarize the deep
learning applications under multi-modal (e.g., depth and infrared) or
specialized (e.g., light field and flash) sensors. We conclude this survey by
emphasizing current open issues and highlighting potential prospects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zitong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yunxiao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaobai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chenxu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1"&gt;Zhen Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guoying Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Cognitive Fatigue from fMRI Scans with Self-supervised Learning. (arXiv:2106.15009v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.15009</id>
        <link href="http://arxiv.org/abs/2106.15009"/>
        <updated>2021-06-30T02:01:00.469Z</updated>
        <summary type="html"><![CDATA[Functional magnetic resonance imaging (fMRI) is a neuroimaging technique that
records neural activations in the brain by capturing the blood oxygen level in
different regions based on the task performed by a subject. Given fMRI data,
the problem of predicting the state of cognitive fatigue in a person has not
been investigated to its full extent. This paper proposes tackling this issue
as a multi-class classification problem by dividing the state of cognitive
fatigue into six different levels, ranging from no-fatigue to extreme fatigue
conditions. We built a spatio-temporal model that uses convolutional neural
networks (CNN) for spatial feature extraction and a long short-term memory
(LSTM) network for temporal modeling of 4D fMRI scans. We also applied a
self-supervised method called MoCo to pre-train our model on a public dataset
BOLD5000 and fine-tuned it on our labeled dataset to classify cognitive
fatigue. Our novel dataset contains fMRI scans from Traumatic Brain Injury
(TBI) patients and healthy controls (HCs) while performing a series of
cognitive tasks. This method establishes a state-of-the-art technique to
analyze cognitive fatigue from fMRI data and beats previous approaches to solve
this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1"&gt;Ashish Jaiswal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1"&gt;Ashwin Ramesh Babu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadeh_M/0/1/0/all/0/1"&gt;Mohammad Zaki Zadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makedon_F/0/1/0/all/0/1"&gt;Fillia Makedon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wylie_G/0/1/0/all/0/1"&gt;Glenn Wylie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Span-based Joint Entity and Relation Extraction with Transformer Pre-training. (arXiv:1909.07755v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.07755</id>
        <link href="http://arxiv.org/abs/1909.07755"/>
        <updated>2021-06-30T02:01:00.463Z</updated>
        <summary type="html"><![CDATA[We introduce SpERT, an attention model for span-based joint entity and
relation extraction. Our key contribution is a light-weight reasoning on BERT
embeddings, which features entity recognition and filtering, as well as
relation classification with a localized, marker-free context representation.
The model is trained using strong within-sentence negative samples, which are
efficiently extracted in a single BERT pass. These aspects facilitate a search
over all spans in the sentence.

In ablation studies, we demonstrate the benefits of pre-training, strong
negative sampling and localized context. Our model outperforms prior work by up
to 2.6% F1 score on several datasets for joint entity and relation extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eberts_M/0/1/0/all/0/1"&gt;Markus Eberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulges_A/0/1/0/all/0/1"&gt;Adrian Ulges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving Real-Time Object Detection on MobileDevices with Neural Pruning Search. (arXiv:2106.14943v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14943</id>
        <link href="http://arxiv.org/abs/2106.14943"/>
        <updated>2021-06-30T02:01:00.457Z</updated>
        <summary type="html"><![CDATA[Object detection plays an important role in self-driving cars for security
development. However, mobile systems on self-driving cars with limited
computation resources lead to difficulties for object detection. To facilitate
this, we propose a compiler-aware neural pruning search framework to achieve
high-speed inference on autonomous vehicles for 2D and 3D object detection. The
framework automatically searches the pruning scheme and rate for each layer to
find a best-suited pruning for optimizing detection accuracy and speed
performance under compiler optimization. Our experiments demonstrate that for
the first time, the proposed method achieves (close-to) real-time, 55ms and
99ms inference times for YOLOv4 based 2D object detection and PointPillars
based 3D detection, respectively, on an off-the-shelf mobile phone with minor
(or no) accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1"&gt;Wei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yuxuan Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data augmentation for deep learning based accelerated MRI reconstruction with limited data. (arXiv:2106.14947v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14947</id>
        <link href="http://arxiv.org/abs/2106.14947"/>
        <updated>2021-06-30T02:01:00.452Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have emerged as very successful tools for image
restoration and reconstruction tasks. These networks are often trained
end-to-end to directly reconstruct an image from a noisy or corrupted
measurement of that image. To achieve state-of-the-art performance, training on
large and diverse sets of images is considered critical. However, it is often
difficult and/or expensive to collect large amounts of training images.
Inspired by the success of Data Augmentation (DA) for classification problems,
in this paper, we propose a pipeline for data augmentation for accelerated MRI
reconstruction and study its effectiveness at reducing the required training
data in a variety of settings. Our DA pipeline, MRAugment, is specifically
designed to utilize the invariances present in medical imaging measurements as
naive DA strategies that neglect the physics of the problem fail. Through
extensive studies on multiple datasets we demonstrate that in the low-data
regime DA prevents overfitting and can match or even surpass the state of the
art while using significantly fewer training data, whereas in the high-data
regime it has diminishing returns. Furthermore, our findings show that DA can
improve the robustness of the model against various shifts in the test
distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fabian_Z/0/1/0/all/0/1"&gt;Zalan Fabian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heckel_R/0/1/0/all/0/1"&gt;Reinhard Heckel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soltanolkotabi_M/0/1/0/all/0/1"&gt;Mahdi Soltanolkotabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning to Compositionally Generalize. (arXiv:2106.04252v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04252</id>
        <link href="http://arxiv.org/abs/2106.04252"/>
        <updated>2021-06-30T02:01:00.446Z</updated>
        <summary type="html"><![CDATA[Natural language is compositional; the meaning of a sentence is a function of
the meaning of its parts. This property allows humans to create and interpret
novel sentences, generalizing robustly outside their prior experience. Neural
networks have been shown to struggle with this kind of generalization, in
particular performing poorly on tasks designed to assess compositional
generalization (i.e. where training and testing distributions differ in ways
that would be trivial for a compositional strategy to resolve). Their poor
performance on these tasks may in part be due to the nature of supervised
learning which assumes training and testing data to be drawn from the same
distribution. We implement a meta-learning augmented version of supervised
learning whose objective directly optimizes for out-of-distribution
generalization. We construct pairs of tasks for meta-learning by sub-sampling
existing training data. Each pair of tasks is constructed to contain relevant
examples, as determined by a similarity metric, in an effort to inhibit models
from memorizing their input. Experimental results on the COGS and SCAN datasets
show that our similarity-driven meta-learning can improve generalization
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conklin_H/0/1/0/all/0/1"&gt;Henry Conklin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bailin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1"&gt;Kenny Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1"&gt;Ivan Titov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Interpretable Natural Language Understanding with Explanations as Latent Variables. (arXiv:2011.05268v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05268</id>
        <link href="http://arxiv.org/abs/2011.05268"/>
        <updated>2021-06-30T02:01:00.441Z</updated>
        <summary type="html"><![CDATA[Recently generating natural language explanations has shown very promising
results in not only offering interpretable explanations but also providing
additional information and supervision for prediction. However, existing
approaches usually require a large set of human annotated explanations for
training while collecting a large set of explanations is not only time
consuming but also expensive. In this paper, we develop a general framework for
interpretable natural language understanding that requires only a small set of
human annotated explanations for training. Our framework treats natural
language explanations as latent variables that model the underlying reasoning
process of a neural model. We develop a variational EM framework for
optimization where an explanation generation module and an
explanation-augmented prediction module are alternatively optimized and
mutually enhance each other. Moreover, we further propose an explanation-based
self-training method under this framework for semi-supervised learning. It
alternates between assigning pseudo-labels to unlabeled data and generating new
explanations to iteratively improve each other. Experiments on two natural
language understanding tasks demonstrate that our framework can not only make
effective predictions in both supervised and semi-supervised settings, but also
generate good natural language explanation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jinyi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanlin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Arabic Speech Recognition by End-to-End, Modular Systems and Human. (arXiv:2101.08454v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08454</id>
        <link href="http://arxiv.org/abs/2101.08454"/>
        <updated>2021-06-30T02:01:00.419Z</updated>
        <summary type="html"><![CDATA[Recent advances in automatic speech recognition (ASR) have achieved accuracy
levels comparable to human transcribers, which led researchers to debate if the
machine has reached human performance. Previous work focused on the English
language and modular hidden Markov model-deep neural network (HMM-DNN) systems.
In this paper, we perform a comprehensive benchmarking for end-to-end
transformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the
Arabic language and its dialects. For the HSR, we evaluate linguist performance
and lay-native speaker performance on a new dataset collected as a part of this
study. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new
performance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our
results suggest that human performance in the Arabic language is still
considerably better than the machine with an absolute WER gap of 3.5% on
average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hussein_A/0/1/0/all/0/1"&gt;Amir Hussein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1"&gt;Ahmed Ali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Training of Neural Lumigraph Representations using Meta Learning. (arXiv:2106.14942v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14942</id>
        <link href="http://arxiv.org/abs/2106.14942"/>
        <updated>2021-06-30T02:01:00.413Z</updated>
        <summary type="html"><![CDATA[Novel view synthesis is a long-standing problem in machine learning and
computer vision. Significant progress has recently been made in developing
neural scene representations and rendering techniques that synthesize
photorealistic images from arbitrary views. These representations, however, are
extremely slow to train and often also slow to render. Inspired by neural
variants of image-based rendering, we develop a new neural rendering approach
with the goal of quickly learning a high-quality representation which can also
be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a
unique combination of a neural shape representation and 2D CNN-based image
feature extraction, aggregation, and re-projection. To push representation
convergence times down to minutes, we leverage meta learning to learn neural
shape and image feature priors which accelerate training. The optimized shape
and image features can then be extracted using traditional graphics techniques
and rendered in real time. We show that MetaNLR++ achieves similar or better
novel view synthesis results in a fraction of the time that competing methods
require.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1"&gt;Alexander W. Bergman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kellnhofer_P/0/1/0/all/0/1"&gt;Petr Kellnhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1"&gt;Gordon Wetzstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate speech detection using static BERT embeddings. (arXiv:2106.15537v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15537</id>
        <link href="http://arxiv.org/abs/2106.15537"/>
        <updated>2021-06-30T02:01:00.389Z</updated>
        <summary type="html"><![CDATA[With increasing popularity of social media platforms hate speech is emerging
as a major concern, where it expresses abusive speech that targets specific
group characteristics, such as gender, religion or ethnicity to spread
violence. Earlier people use to verbally deliver hate speeches but now with the
expansion of technology, some people are deliberately using social media
platforms to spread hate by posting, sharing, commenting, etc. Whether it is
Christchurch mosque shootings or hate crimes against Asians in west, it has
been observed that the convicts are very much influenced from hate text present
online. Even though AI systems are in place to flag such text but one of the
key challenges is to reduce the false positive rate (marking non hate as hate),
so that these systems can detect hate speech without undermining the freedom of
expression. In this paper, we use ETHOS hate speech detection dataset and
analyze the performance of hate speech detection classifier by replacing or
integrating the word embeddings (fastText (FT), GloVe (GV) or FT + GV) with
static BERT embeddings (BE). With the extensive experimental trails it is
observed that the neural network performed better with static BE compared to
using FT, GV or FT + GV as word embeddings. In comparison to fine-tuned BERT,
one metric that significantly improved is specificity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajput_G/0/1/0/all/0/1"&gt;Gaurav Rajput&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+punn_N/0/1/0/all/0/1"&gt;Narinder Singh punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Electronic Health Record Coding through Graph Contrastive Learning. (arXiv:2106.15467v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.15467</id>
        <link href="http://arxiv.org/abs/2106.15467"/>
        <updated>2021-06-30T02:01:00.382Z</updated>
        <summary type="html"><![CDATA[Electronic health record (EHR) coding is the task of assigning ICD codes to
each EHR. Most previous studies either only focus on the frequent ICD codes or
treat rare and frequent ICD codes in the same way. These methods perform well
on frequent ICD codes but due to the extremely unbalanced distribution of ICD
codes, the performance on rare ones is far from satisfactory. We seek to
improve the performance for both frequent and rare ICD codes by using a
contrastive graph-based EHR coding framework, CoGraph, which re-casts EHR
coding as a few-shot learning task. First, we construct a heterogeneous EHR
word-entity (HEWE) graph for each EHR, where the words and entities extracted
from an EHR serve as nodes and the relations between them serve as edges. Then,
CoGraph learns similarities and dissimilarities between HEWE graphs from
different ICD codes so that information can be transferred among them. In a
few-shot learning scenario, the model only has access to frequent ICD codes
during training, which might force it to encode features that are useful for
frequent ICD codes only. To mitigate this risk, CoGraph devises two graph
contrastive learning schemes, GSCL and GECL, that exploit the HEWE graph
structures so as to encode transferable features. GSCL utilizes the
intra-correlation of different sub-graphs sampled from HEWE graphs while GECL
exploits the inter-correlation among HEWE graphs at different clinical stages.
Experiments on the MIMIC-III benchmark dataset show that CoGraph significantly
outperforms state-of-the-art methods on EHR coding, not only on frequent ICD
codes, but also on rare codes, in terms of several evaluation indicators. On
frequent ICD codes, GSCL and GECL improve the classification accuracy and F1 by
1.31% and 0.61%, respectively, and on rare ICD codes CoGraph has more obvious
improvements by 2.12% and 2.95%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shanshan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pengjie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhaochun Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Huasheng Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1"&gt;Qiang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1"&gt;Evangelos Kanoulas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Neural Speech Synthesis. (arXiv:2106.15561v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.15561</id>
        <link href="http://arxiv.org/abs/2106.15561"/>
        <updated>2021-06-30T02:01:00.372Z</updated>
        <summary type="html"><![CDATA[Text to speech (TTS), or speech synthesis, which aims to synthesize
intelligible and natural speech given text, is a hot research topic in speech,
language, and machine learning communities and has broad applications in the
industry. As the development of deep learning and artificial intelligence,
neural network-based TTS has significantly improved the quality of synthesized
speech in recent years. In this paper, we conduct a comprehensive survey on
neural TTS, aiming to provide a good understanding of current research and
future trends. We focus on the key components in neural TTS, including text
analysis, acoustic models and vocoders, and several advanced topics, including
fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.
We further summarize resources related to TTS (e.g., datasets, opensource
implementations) and discuss future research directions. This survey can serve
both academic researchers and industry practitioners working on TTS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1"&gt;Frank Soong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Privacy for Credit Risk Model. (arXiv:2106.15343v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.15343</id>
        <link href="http://arxiv.org/abs/2106.15343"/>
        <updated>2021-06-30T02:01:00.362Z</updated>
        <summary type="html"><![CDATA[The use of machine learning algorithms to model user behavior and drive
business decisions has become increasingly commonplace, specifically providing
intelligent recommendations to automated decision making. This has led to an
increase in the use of customers personal data to analyze customer behavior and
predict their interests in a companys products. Increased use of this customer
personal data can lead to better models but also to the potential of customer
data being leaked, reverse engineered, and mishandled. In this paper, we assess
differential privacy as a solution to address these privacy problems by
building privacy protections into the data engineering and model training
stages of predictive model development. Our interest is a pragmatic
implementation in an operational environment, which necessitates a general
purpose differentially private modeling framework, and we evaluate one such
tool from LeapYear as applied to the Credit Risk modeling domain. Credit Risk
Model is a major modeling methodology in banking and finance where user data is
analyzed to determine the total Expected Loss to the bank. We examine the
application of differential privacy on the credit risk model and evaluate the
performance of a Differentially Private Model with a Non Differentially Private
Model. Credit Risk Model is a major modeling methodology in banking and finance
where users data is analyzed to determine the total Expected Loss to the bank.
In this paper, we explore the application of differential privacy on the credit
risk model and evaluate the performance of a Non Differentially Private Model
with Differentially Private Model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maniar_T/0/1/0/all/0/1"&gt;Tabish Maniar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akkinepally_A/0/1/0/all/0/1"&gt;Alekhya Akkinepally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Anantha Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers. (arXiv:2106.15195v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15195</id>
        <link href="http://arxiv.org/abs/2106.15195"/>
        <updated>2021-06-30T02:01:00.339Z</updated>
        <summary type="html"><![CDATA[This paper presents the first large-scale meta-evaluation of machine
translation (MT). We annotated MT evaluations conducted in 769 research papers
published from 2010 to 2020. Our study shows that practices for automatic MT
evaluation have dramatically changed during the past decade and follow
concerning trends. An increasing number of MT evaluations exclusively rely on
differences between BLEU scores to draw conclusions, without performing any
kind of statistical significance testing nor human evaluation, while at least
108 metrics claiming to be better than BLEU have been proposed. MT evaluations
in recent papers tend to copy and compare automatic metric scores from previous
work to claim the superiority of a method or an algorithm without confirming
neither exactly the same training, validating, and testing data have been used
nor the metric scores are comparable. Furthermore, tools for reporting
standardized metric scores are still far from being widely adopted by the MT
community. After showing how the accumulation of these pitfalls leads to
dubious evaluation, we propose a guideline to encourage better automatic MT
evaluation along with a simple meta-evaluation scoring method to assess its
credibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marie_B/0/1/0/all/0/1"&gt;Benjamin Marie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1"&gt;Atsushi Fujita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubino_R/0/1/0/all/0/1"&gt;Raphael Rubino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Static Models for Link Prediction in Temporal Knowledge Graphs. (arXiv:2106.15223v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15223</id>
        <link href="http://arxiv.org/abs/2106.15223"/>
        <updated>2021-06-30T02:01:00.320Z</updated>
        <summary type="html"><![CDATA[The inclusion of temporal scopes of facts in knowledge graph embedding (KGE)
presents significant opportunities for improving the resulting embeddings, and
consequently for increased performance in downstream applications. Yet, little
research effort has focussed on this area and much of the carried out research
reports only marginally improved results compared to models trained without
temporal scopes (static models). Furthermore, rather than leveraging existing
work on static models, they introduce new models specific to temporal knowledge
graphs. We propose a novel perspective that takes advantage of the power of
existing static embedding models by focussing effort on manipulating the data
instead. Our method, SpliMe, draws inspiration from the field of signal
processing and early work in graph embedding. We show that SpliMe competes with
or outperforms the current state of the art in temporal KGE. Additionally, we
uncover issues with the procedure currently used to assess the performance of
static models on temporal graphs and introduce two ways to counteract them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Radstok_W/0/1/0/all/0/1"&gt;Wessel Radstok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chekol_M/0/1/0/all/0/1"&gt;Mel Chekol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15498</id>
        <link href="http://arxiv.org/abs/2106.15498"/>
        <updated>2021-06-30T02:01:00.311Z</updated>
        <summary type="html"><![CDATA[Social media offer plenty of information to perform market research in order
to meet the requirements of customers. One way how this research is conducted
is that a domain expert gathers and categorizes user-generated content into a
complex and fine-grained class structure. In many of such cases, little data
meets complex annotations. It is not yet fully understood how this can be
leveraged successfully for classification. We examine the classification
accuracy of expert labels when used with a) many fine-grained classes and b)
few abstract classes. For scenario b) we compare abstract class labels given by
the domain expert as baseline and by automatic hierarchical clustering. We
compare this to another baseline where the entire class structure is given by a
completely unsupervised clustering approach. By doing so, this work can serve
as an example of how complex expert annotations are potentially beneficial and
can be utilized in the most optimal way for opinion mining in highly specific
domains. By exploring across a range of techniques and experiments, we find
that automated class abstraction approaches in particular the unsupervised
approach performs remarkably well against domain expert baseline on text
classification tasks. This has the potential to inspire opinion mining
applications in order to support market researchers in practice and to inspire
fine-grained automated content analysis on a large scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1"&gt;Gerhard Hagerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1"&gt;Wenbin Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1"&gt;Hannah Danner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1"&gt;Georg Groh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topic Modeling Based Extractive Text Summarization. (arXiv:2106.15313v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15313</id>
        <link href="http://arxiv.org/abs/2106.15313"/>
        <updated>2021-06-30T02:01:00.304Z</updated>
        <summary type="html"><![CDATA[Text summarization is an approach for identifying important information
present within text documents. This computational technique aims to generate
shorter versions of the source text, by including only the relevant and salient
information present within the source text. In this paper, we propose a novel
method to summarize a text document by clustering its contents based on latent
topics produced using topic modeling techniques and by generating extractive
summaries for each of the identified text clusters. All extractive
sub-summaries are later combined to generate a summary for any given source
document. We utilize the lesser used and challenging WikiHow dataset in our
approach to text summarization. This dataset is unlike the commonly used news
datasets which are available for text summarization. The well-known news
datasets present their most important information in the first few lines of
their source texts, which make their summarization a lesser challenging task
when compared to summarizing the WikiHow dataset. Contrary to these news
datasets, the documents in the WikiHow dataset are written using a generalized
approach and have lesser abstractedness and higher compression ratio, thus
proposing a greater challenge to generate summaries. A lot of the current
state-of-the-art text summarization techniques tend to eliminate important
information present in source documents in the favor of brevity. Our proposed
technique aims to capture all the varied information present in source
documents. Although the dataset proved challenging, after performing extensive
tests within our experimental setup, we have discovered that our model produces
encouraging ROUGE results and summaries when compared to the other published
extractive and abstractive text summarization models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Issam_K/0/1/0/all/0/1"&gt;Kalliath Abdul Rasheed Issam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1"&gt;Shivam Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1"&gt;Subalalitha C. N&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis. (arXiv:2106.15231v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15231</id>
        <link href="http://arxiv.org/abs/2106.15231"/>
        <updated>2021-06-30T02:01:00.295Z</updated>
        <summary type="html"><![CDATA[While state-of-the-art NLP models have been achieving the excellent
performance of a wide range of tasks in recent years, important questions are
being raised about their robustness and their underlying sensitivity to
systematic biases that may exist in their training and test data. Such issues
come to be manifest in performance problems when faced with out-of-distribution
data in the field. One recent solution has been to use counterfactually
augmented datasets in order to reduce any reliance on spurious patterns that
may exist in the original data. Producing high-quality augmented data can be
costly and time-consuming as it usually needs to involve human feedback and
crowdsourcing efforts. In this work, we propose an alternative by describing
and evaluating an approach to automatically generating counterfactual data for
data augmentation and explanation. A comprehensive evaluation on several
different datasets and using a variety of state-of-the-art benchmarks
demonstrate how our approach can achieve significant improvements in model
performance when compared to models training on the original data and even when
compared to models trained with the benefit of human-generated augmented data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Linyi_Y/0/1/0/all/0/1"&gt;Yang Linyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiazheng_L/0/1/0/all/0/1"&gt;Li Jiazheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padraig_C/0/1/0/all/0/1"&gt;Cunningham P&amp;#xe1;draig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1"&gt;Zhang Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barry_S/0/1/0/all/0/1"&gt;Smyth Barry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruihai_D/0/1/0/all/0/1"&gt;Dong Ruihai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Interaction of Belief Bias and Explanations. (arXiv:2106.15355v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15355</id>
        <link href="http://arxiv.org/abs/2106.15355"/>
        <updated>2021-06-30T02:01:00.279Z</updated>
        <summary type="html"><![CDATA[A myriad of explainability methods have been proposed in recent years, but
there is little consensus on how to evaluate them. While automatic metrics
allow for quick benchmarking, it isn't clear how such metrics reflect human
interaction with explanations. Human evaluation is of paramount importance, but
previous protocols fail to account for belief biases affecting human
performance, which may lead to misleading conclusions. We provide an overview
of belief bias, its role in human evaluation, and ideas for NLP practitioners
on how to account for it. For two experimental paradigms, we present a case
study of gradient-based explainability introducing simple ways to account for
humans' prior beliefs: models of varying quality and adversarial examples. We
show that conclusions about the highest performing methods change when
introducing such controls, pointing to the importance of accounting for belief
bias in evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1"&gt;Ana Valeria Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1"&gt;Anna Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1"&gt;Anders S&amp;#xf8;gaard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis. (arXiv:2106.15153v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.15153</id>
        <link href="http://arxiv.org/abs/2106.15153"/>
        <updated>2021-06-30T02:01:00.260Z</updated>
        <summary type="html"><![CDATA[Recent advances in neural multi-speaker text-to-speech (TTS) models have
enabled the generation of reasonably good speech quality with a single model
and made it possible to synthesize the speech of a speaker with limited
training data. Fine-tuning to the target speaker data with the multi-speaker
model can achieve better quality, however, there still exists a gap compared to
the real speech sample and the model depends on the speaker. In this work, we
propose GANSpeech, which is a high-fidelity multi-speaker TTS model that adopts
the adversarial training method to a non-autoregressive multi-speaker TTS
model. In addition, we propose simple but efficient automatic scaling methods
for feature matching loss used in adversarial training. In the subjective
listening tests, GANSpeech significantly outperformed the baseline
multi-speaker FastSpeech and FastSpeech2 models, and showed a better MOS score
than the speaker-specific fine-tuned FastSpeech2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinhyeok Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1"&gt;Jae-Sung Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bak_T/0/1/0/all/0/1"&gt;Taejun Bak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngik Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hoon-Young Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Technique To Conversational Machine Reading. (arXiv:2106.15247v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15247</id>
        <link href="http://arxiv.org/abs/2106.15247"/>
        <updated>2021-06-30T02:01:00.252Z</updated>
        <summary type="html"><![CDATA[Conversational machine reading (CMR) tools have seen a rapid progress in the
recent past. The current existing tools rely on the supervised learning
technique which require labeled dataset for their training. The supervised
technique necessitates that for every new rule text, a manually labeled dataset
must be created. This is tedious and error prone. This paper introduces and
demonstrates how unsupervised learning technique can be applied in the
development of CMR. Specifically, we demonstrate how unsupervised learning can
be used in rule extraction and entailment modules of CMR. Compared to the
current best CMR tool, our developed framework reports 3.3% improvement in
micro averaged accuracy and 1.4 % improvement in macro averaged accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ochieng_P/0/1/0/all/0/1"&gt;Peter Ochieng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mugambi_D/0/1/0/all/0/1"&gt;Dennis Mugambi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Arabic Medical Dataset for Diseases Classification. (arXiv:2106.15236v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15236</id>
        <link href="http://arxiv.org/abs/2106.15236"/>
        <updated>2021-06-30T02:01:00.239Z</updated>
        <summary type="html"><![CDATA[The Arabic language suffers from a great shortage of datasets suitable for
training deep learning models, and the existing ones include general
non-specialized classifications. In this work, we introduce a new Arab medical
dataset, which includes two thousand medical documents collected from several
Arabic medical websites, in addition to the Arab Medical Encyclopedia. The
dataset was built for the task of classifying texts and includes 10 classes
(Blood, Bone, Cardiovascular, Ear, Endocrine, Eye, Gastrointestinal, Immune,
Liver and Nephrological) diseases. Experiments on the dataset were performed by
fine-tuning three pre-trained models: BERT from Google, Arabert that based on
BERT with large Arabic corpus, and AraBioNER that based on Arabert with Arabic
medical corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hammoud_J/0/1/0/all/0/1"&gt;Jaafar Hammoud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vatian_A/0/1/0/all/0/1"&gt;Aleksandra Vatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobrenko_N/0/1/0/all/0/1"&gt;Natalia Dobrenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedernikov_N/0/1/0/all/0/1"&gt;Nikolai Vedernikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalyto_A/0/1/0/all/0/1"&gt;Anatoly Shalyto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gusarova_N/0/1/0/all/0/1"&gt;Natalia Gusarova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition. (arXiv:2106.15167v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15167</id>
        <link href="http://arxiv.org/abs/2106.15167"/>
        <updated>2021-06-30T02:01:00.224Z</updated>
        <summary type="html"><![CDATA[Few-shot Named Entity Recognition (NER) exploits only a handful of
annotations to identify and classify named entity mentions. Prototypical
network shows superior performance on few-shot NER. However, existing
prototypical methods fail to differentiate rich semantics in other-class words,
which will aggravate overfitting under few shot scenario. To address the issue,
we propose a novel model, Mining Undefined Classes from Other-class (MUCO),
that can automatically induce different undefined classes from the other class
to improve few-shot NER. With these extra-labeled undefined classes, our method
will improve the discriminative ability of NER classifier and enhance the
understanding of predefined classes with stand-by semantic knowledge.
Experimental results demonstrate that our model outperforms five
state-of-the-art models in both 1-shot and 5-shots settings on four NER
benchmarks. We will release the code upon acceptance. The source code is
released on https: //github.com/shuaiwa16/OtherClassNER.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1"&gt;Meihan Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Bin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yixin Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minghui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Lei Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Evaluation of Neural Machine Translation. (arXiv:2106.15217v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15217</id>
        <link href="http://arxiv.org/abs/2106.15217"/>
        <updated>2021-06-30T02:01:00.215Z</updated>
        <summary type="html"><![CDATA[The evaluation of neural machine translation systems is usually built upon
generated translation of a certain decoding method (e.g., beam search) with
evaluation metrics over the generated translation (e.g., BLEU). However, this
evaluation framework suffers from high search errors brought by heuristic
search algorithms and is limited by its nature of evaluation over one best
candidate. In this paper, we propose a novel evaluation protocol, which not
only avoids the effect of search errors but provides a system-level evaluation
in the perspective of model ranking. In particular, our method is based on our
newly proposed exact top-$k$ decoding instead of beam search. Our approach
evaluates model errors by the distance between the candidate spaces scored by
the references and the model respectively. Extensive experiments on WMT'14
English-German demonstrate that bad ranking ability is connected to the
well-known beam search curse, and state-of-the-art Transformer models are
facing serious ranking errors. By evaluating various model architectures and
techniques, we provide several interesting findings. Finally, to effectively
approximate the exact search algorithm with same time cost as original beam
search, we present a minimum heap augmented beam search algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jianhao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation based meta-learning for few-shot spoken intent recognition. (arXiv:2106.15238v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15238</id>
        <link href="http://arxiv.org/abs/2106.15238"/>
        <updated>2021-06-30T02:01:00.164Z</updated>
        <summary type="html"><![CDATA[Spoken intent detection has become a popular approach to interface with
various smart devices with ease. However, such systems are limited to the
preset list of intents-terms or commands, which restricts the quick
customization of personal devices to new intents. This paper presents a
few-shot spoken intent classification approach with task-agnostic
representations via meta-learning paradigm. Specifically, we leverage the
popular representation-based meta-learning learning to build a task-agnostic
representation of utterances, that then use a linear classifier for prediction.
We evaluate three such approaches on our novel experimental protocol developed
on two popular spoken intent classification datasets: Google Commands and the
Fluent Speech Commands dataset. For a 5-shot (1-shot) classification of novel
classes, the proposed framework provides an average classification accuracy of
88.6% (76.3%) on the Google Commands dataset, and 78.5% (64.2%) on the Fluent
Speech Commands dataset. The performance is comparable to traditionally
supervised classification models with abundant training samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Ashish Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1"&gt;Samarth Bharadwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1"&gt;Shreya Khare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1"&gt;Saneem Chemmengath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1"&gt;Karthik Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1"&gt;Brian Kingsbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple and Efficient Probabilistic Language model for Code-Mixed Text. (arXiv:2106.15102v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15102</id>
        <link href="http://arxiv.org/abs/2106.15102"/>
        <updated>2021-06-30T02:01:00.130Z</updated>
        <summary type="html"><![CDATA[The conventional natural language processing approaches are not accustomed to
the social media text due to colloquial discourse and non-homogeneous
characteristics. Significantly, the language identification in a multilingual
document is ascertained to be a preceding subtask in several information
extraction applications such as information retrieval, named entity
recognition, relation extraction, etc. The problem is often more challenging in
code-mixed documents wherein foreign languages words are drawn into base
language while framing the text. The word embeddings are powerful language
modeling tools for representation of text documents useful in obtaining
similarity between words or documents. We present a simple probabilistic
approach for building efficient word embedding for code-mixed text and
exemplifying it over language identification of Hindi-English short test
messages scrapped from Twitter. We examine its efficacy for the classification
task using bidirectional LSTMs and SVMs and observe its improved scores over
various existing code-mixed embeddings]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1"&gt;M Zeeshan Ansari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1"&gt;Tanvir Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1"&gt;M M Sufyan Beg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikram_A/0/1/0/all/0/1"&gt;Asma Ikram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Machine Translation for Low-Resource Languages: A Survey. (arXiv:2106.15115v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15115</id>
        <link href="http://arxiv.org/abs/2106.15115"/>
        <updated>2021-06-30T02:01:00.108Z</updated>
        <summary type="html"><![CDATA[Neural Machine Translation (NMT) has seen a tremendous spurt of growth in
less than ten years, and has already entered a mature phase. While considered
as the most widely used solution for Machine Translation, its performance on
low-resource language pairs still remains sub-optimal compared to the
high-resource counterparts, due to the unavailability of large parallel
corpora. Therefore, the implementation of NMT techniques for low-resource
language pairs has been receiving the spotlight in the recent NMT research
arena, thus leading to a substantial amount of research reported on this topic.
This paper presents a detailed survey of research advancements in low-resource
language NMT (LRL-NMT), along with a quantitative analysis aimed at identifying
the most popular solutions. Based on our findings from reviewing previous work,
this survey paper provides a set of guidelines to select the possible NMT
technique for a given LRL data setting. It also presents a holistic view of the
LRL-NMT research landscape and provides a list of recommendations to further
enhance the research efforts on LRL-NMT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1"&gt;Surangika Ranathunga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1"&gt;En-Shiun Annie Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skenduli_M/0/1/0/all/0/1"&gt;Marjana Prifti Skenduli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shekhar_R/0/1/0/all/0/1"&gt;Ravi Shekhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1"&gt;Mehreen Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1"&gt;Rishemjit Kaur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15078</id>
        <link href="http://arxiv.org/abs/2106.15078"/>
        <updated>2021-06-30T02:01:00.100Z</updated>
        <summary type="html"><![CDATA[Neural text generation models are typically trained by maximizing
log-likelihood with the sequence cross entropy loss, which encourages an exact
token-by-token match between a target sequence with a generated sequence. Such
training objective is sub-optimal when the target sequence not perfect, e.g.,
when the target sequence is corrupted with noises, or when only weak sequence
supervision is available. To address this challenge, we propose a novel
Edit-Invariant Sequence Loss (EISL), which computes the matching loss of a
target n-gram with all n-grams in the generated sequence. EISL draws
inspirations from convolutional networks (ConvNets) which are shift-invariant
to images, hence is robust to the shift of n-grams to tolerate edits in the
target sequences. Moreover, the computation of EISL is essentially a
convolution operation with target n-grams as kernels, which is easy to
implement with existing libraries. To demonstrate the effectiveness of EISL, we
conduct experiments on three tasks: machine translation with noisy target
sequences, unsupervised text style transfer, and non-autoregressive machine
translation. Experimental results show our method significantly outperforms
cross entropy loss on these three tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guangyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zichao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1"&gt;Tianhua Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bowen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiting Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding. (arXiv:2106.15065v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15065</id>
        <link href="http://arxiv.org/abs/2106.15065"/>
        <updated>2021-06-30T02:01:00.084Z</updated>
        <summary type="html"><![CDATA[Decomposable tasks are complex and comprise of a hierarchy of sub-tasks.
Spoken intent prediction, for example, combines automatic speech recognition
and natural language understanding. Existing benchmarks, however, typically
hold out examples for only the surface-level sub-task. As a result, models with
similar performance on these benchmarks may have unobserved performance
differences on the other sub-tasks. To allow insightful comparisons between
competitive end-to-end architectures, we propose a framework to construct
robust test sets using coordinate ascent over sub-task specific utility
functions. Given a dataset for a decomposable task, our method optimally
creates a test set for each sub-task to individually assess sub-components of
the end-to-end model. Using spoken language understanding as a case study, we
generate new splits for the Fluent Speech Commands and Snips SmartLights
datasets. Each split has two test sets: one with held-out utterances assessing
natural language understanding abilities, and one with held-out speakers to
test speech processing skills. Our splits identify performance gaps up to 10%
between end-to-end systems that were within 1% of each other on the original
test sets. These performance gaps allow more realistic and actionable
comparisons between different architectures, driving future model development.
We release our splits and tools for the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1"&gt;Siddhant Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostapenko_A/0/1/0/all/0/1"&gt;Alissa Ostapenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1"&gt;Vijay Viswanathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1"&gt;Siddharth Dalmia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1"&gt;Florian Metze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1"&gt;Alan W Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topic Modeling Based Extractive Text Summarization. (arXiv:2106.15313v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15313</id>
        <link href="http://arxiv.org/abs/2106.15313"/>
        <updated>2021-06-30T02:01:00.059Z</updated>
        <summary type="html"><![CDATA[Text summarization is an approach for identifying important information
present within text documents. This computational technique aims to generate
shorter versions of the source text, by including only the relevant and salient
information present within the source text. In this paper, we propose a novel
method to summarize a text document by clustering its contents based on latent
topics produced using topic modeling techniques and by generating extractive
summaries for each of the identified text clusters. All extractive
sub-summaries are later combined to generate a summary for any given source
document. We utilize the lesser used and challenging WikiHow dataset in our
approach to text summarization. This dataset is unlike the commonly used news
datasets which are available for text summarization. The well-known news
datasets present their most important information in the first few lines of
their source texts, which make their summarization a lesser challenging task
when compared to summarizing the WikiHow dataset. Contrary to these news
datasets, the documents in the WikiHow dataset are written using a generalized
approach and have lesser abstractedness and higher compression ratio, thus
proposing a greater challenge to generate summaries. A lot of the current
state-of-the-art text summarization techniques tend to eliminate important
information present in source documents in the favor of brevity. Our proposed
technique aims to capture all the varied information present in source
documents. Although the dataset proved challenging, after performing extensive
tests within our experimental setup, we have discovered that our model produces
encouraging ROUGE results and summaries when compared to the other published
extractive and abstractive text summarization models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Issam_K/0/1/0/all/0/1"&gt;Kalliath Abdul Rasheed Issam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1"&gt;Shivam Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1"&gt;Subalalitha C. N&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-aware Heterogeneous Graph Attention Network for User Behavior Prediction in Local Consumer Service Platform. (arXiv:2106.14652v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14652</id>
        <link href="http://arxiv.org/abs/2106.14652"/>
        <updated>2021-06-30T02:01:00.050Z</updated>
        <summary type="html"><![CDATA[As a new type of e-commerce platform developed in recent years, local
consumer service platform provides users with software to consume service to
the nearby store or to the home, such as Groupon and Koubei. Different from
other common e-commerce platforms, the behavior of users on the local consumer
service platform is closely related to their real-time local context
information. Therefore, building a context-aware user behavior prediction
system is able to provide both merchants and users better service in local
consumer service platforms. However, most of the previous work just treats the
contextual information as an ordinary feature into the prediction model to
obtain the prediction list under a specific context, which ignores the fact
that the interest of a user in different contexts is often significantly
different. Hence, in this paper, we propose a context-aware heterogeneous graph
attention network (CHGAT) to dynamically generate the representation of the
user and to estimate the probability for future behavior. Specifically, we
first construct the meta-path based heterogeneous graphs with the historical
behaviors from multiple sources and comprehend heterogeneous vertices in the
graph with a novel unified knowledge representing approach. Next, a multi-level
attention mechanism is introduced for context-aware aggregation with graph
vertices, which contains the vertex-level attention network and the path-level
attention network. Both of them aim to capture the semantic correlation between
information contained in the graph and the outside real-time contextual
information in the search system. Then the model proposed in this paper
aggregates specific graphs with their corresponding context features and
obtains the representation of user interest under a specific context and input
it into the prediction network to finally obtain the predicted probability of
user behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Peiyuan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaofeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_Z/0/1/0/all/0/1"&gt;Zisen Sang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1"&gt;Aiquan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1"&gt;Guodong Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Lexicons for Hindi-English Multilingual Text Processing. (arXiv:2106.15105v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15105</id>
        <link href="http://arxiv.org/abs/2106.15105"/>
        <updated>2021-06-30T02:01:00.044Z</updated>
        <summary type="html"><![CDATA[Language Identification in textual documents is the process of automatically
detecting the language contained in a document based on its content. The
present Language Identification techniques presume that a document contains
text in one of the fixed set of languages, however, this presumption is
incorrect when dealing with multilingual document which includes content in
more than one possible language. Due to the unavailability of large standard
corpora for Hindi-English mixed lingual language processing tasks we propose
the language lexicons, a novel kind of lexical database that supports several
multilingual language processing tasks. These lexicons are built by learning
classifiers over transliterated Hindi and English vocabulary. The designed
lexicons possess richer quantitative characteristic than its primary source of
collection which is revealed using the visualization techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1"&gt;Mohd Zeeshan Ansari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1"&gt;Tanvir Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bari_N/0/1/0/all/0/1"&gt;Noaima Bari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overview of BioASQ 2021: The ninth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering. (arXiv:2106.14885v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14885</id>
        <link href="http://arxiv.org/abs/2106.14885"/>
        <updated>2021-06-30T02:01:00.037Z</updated>
        <summary type="html"><![CDATA[Advancing the state-of-the-art in large-scale biomedical semantic indexing
and question answering is the main focus of the BioASQ challenge. BioASQ
organizes respective tasks where different teams develop systems that are
evaluated on the same benchmark datasets that represent the real information
needs of experts in the biomedical domain. This paper presents an overview of
the ninth edition of the BioASQ challenge in the context of the Conference and
Labs of the Evaluation Forum (CLEF) 2021. In this year, a new question
answering task, named Synergy, is introduced to support researchers studying
the COVID-19 disease and measure the ability of the participating teams to
discern information while the problem is still developing. In total, 42 teams
with more than 170 systems were registered to participate in the four tasks of
the challenge. The evaluation results, similarly to previous years, show a
performance gain against the baselines which indicates the continuous
improvement of the state-of-the-art in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1"&gt;Anastasios Nentidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katsimpras_G/0/1/0/all/0/1"&gt;Georgios Katsimpras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vandorou_E/0/1/0/all/0/1"&gt;Eirini Vandorou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1"&gt;Anastasia Krithara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasco_L/0/1/0/all/0/1"&gt;Luis Gasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krallinger_M/0/1/0/all/0/1"&gt;Martin Krallinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1"&gt;Georgios Paliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15498</id>
        <link href="http://arxiv.org/abs/2106.15498"/>
        <updated>2021-06-30T02:01:00.011Z</updated>
        <summary type="html"><![CDATA[Social media offer plenty of information to perform market research in order
to meet the requirements of customers. One way how this research is conducted
is that a domain expert gathers and categorizes user-generated content into a
complex and fine-grained class structure. In many of such cases, little data
meets complex annotations. It is not yet fully understood how this can be
leveraged successfully for classification. We examine the classification
accuracy of expert labels when used with a) many fine-grained classes and b)
few abstract classes. For scenario b) we compare abstract class labels given by
the domain expert as baseline and by automatic hierarchical clustering. We
compare this to another baseline where the entire class structure is given by a
completely unsupervised clustering approach. By doing so, this work can serve
as an example of how complex expert annotations are potentially beneficial and
can be utilized in the most optimal way for opinion mining in highly specific
domains. By exploring across a range of techniques and experiments, we find
that automated class abstraction approaches in particular the unsupervised
approach performs remarkably well against domain expert baseline on text
classification tasks. This has the potential to inspire opinion mining
applications in order to support market researchers in practice and to inspire
fine-grained automated content analysis on a large scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1"&gt;Gerhard Hagerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1"&gt;Wenbin Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1"&gt;Hannah Danner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1"&gt;Georg Groh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bytecode-based Approach for Smart Contract Classification. (arXiv:2106.15497v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.15497</id>
        <link href="http://arxiv.org/abs/2106.15497"/>
        <updated>2021-06-30T02:00:59.992Z</updated>
        <summary type="html"><![CDATA[With the development of blockchain technologies, the number of smart
contracts deployed on blockchain platforms is growing exponentially, which
makes it difficult for users to find desired services by manual screening. The
automatic classification of smart contracts can provide blockchain users with
keyword-based contract searching and helps to manage smart contracts
effectively. Current research on smart contract classification focuses on
Natural Language Processing (NLP) solutions which are based on contract source
code. However, more than 94% of smart contracts are not open-source, so the
application scenarios of NLP methods are very limited. Meanwhile, NLP models
are vulnerable to adversarial attacks. This paper proposes a classification
model based on features from contract bytecode instead of source code to solve
these problems. We also use feature selection and ensemble learning to optimize
the model. Our experimental studies on over 3,300 real-world Ethereum smart
contracts show that our model can classify smart contracts without source code
and has better performance than baseline models. Our model also has good
resistance to adversarial attacks compared with NLP-based models. In addition,
our analysis reveals that account features used in many smart contract
classification models have little effect on classification and can be excluded.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chaochen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yong Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doss_R/0/1/0/all/0/1"&gt;Robin Ram Mohan Doss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jiangshan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sood_K/0/1/0/all/0/1"&gt;Keshav Sood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Longxiang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A scalable solution to the nearest neighbor search problem through local-search methods on neighbor graphs. (arXiv:1705.10351v4 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1705.10351</id>
        <link href="http://arxiv.org/abs/1705.10351"/>
        <updated>2021-06-30T02:00:59.984Z</updated>
        <summary type="html"><![CDATA[Near neighbor search (NNS) is a powerful abstraction for data access;
however, data indexing is troublesome even for approximate indexes. For
intrinsically high-dimensional data, high-quality fast searches demand either
indexes with impractically large memory usage or preprocessing time.

In this paper, we introduce an algorithm to solve a nearest-neighbor query
$q$ by minimizing a kernel function defined by the distance from $q$ to each
object in the database. The minimization is performed using metaheuristics to
solve the problem rapidly; even when some methods in the literature use this
strategy behind the scenes, our approach is the first one using it explicitly.
We also provide two approaches to select edges in the graph's construction
stage that limit memory footprint and reduce the number of free parameters
simultaneously.

We carry out a thorough experimental comparison with state-of-the-art indexes
through synthetic and real-world datasets; we found out that our contributions
achieve competitive performances regarding speed, accuracy, and memory in
almost any of our benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tellez_E/0/1/0/all/0/1"&gt;Eric S. Tellez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_G/0/1/0/all/0/1"&gt;Guillermo Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chavez_E/0/1/0/all/0/1"&gt;Edgar Chavez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graff_M/0/1/0/all/0/1"&gt;Mario Graff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When standard network measures fail to rank journals: A theoretical and empirical analysis. (arXiv:2106.15541v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.15541</id>
        <link href="http://arxiv.org/abs/2106.15541"/>
        <updated>2021-06-30T02:00:59.978Z</updated>
        <summary type="html"><![CDATA[Journal rankings are widely used and are often based on citation data in
combination with a network perspective. We argue that some of these
network-based rankings can produce misleading results. From a theoretical point
of view, we show that the standard network modelling approach of citation data
at the journal level (i.e., the projection of paper citations onto journals)
introduces fictitious relations among journals. To overcome this problem, we
propose a citation path perspective, and empirically show that rankings based
on the network and the citation path perspective are very different. Based on
our theoretical and empirical analysis, we highlight the limitations of
standard network metrics, and propose a method to overcome these limitations
and compute journal rankings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaccario_G/0/1/0/all/0/1"&gt;Giacomo Vaccario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verginer_L/0/1/0/all/0/1"&gt;Luca Verginer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TWAG: A Topic-Guided Wikipedia Abstract Generator. (arXiv:2106.15135v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15135</id>
        <link href="http://arxiv.org/abs/2106.15135"/>
        <updated>2021-06-30T02:00:59.970Z</updated>
        <summary type="html"><![CDATA[Wikipedia abstract generation aims to distill a Wikipedia abstract from web
sources and has met significant success by adopting multi-document
summarization techniques. However, previous works generally view the abstract
as plain text, ignoring the fact that it is a description of a certain entity
and can be decomposed into different topics. In this paper, we propose a
two-stage model TWAG that guides the abstract generation with topical
information. First, we detect the topic of each input paragraph with a
classifier trained on existing Wikipedia articles to divide input documents
into different topics. Then, we predict the topic distribution of each abstract
sentence, and decode the sentence from topic-aware representations with a
Pointer-Generator network. We evaluate our model on the WikiCatSum dataset, and
the results show that \modelnames outperforms various existing baselines and is
capable of generating comprehensive abstracts. Our code and dataset can be
accessed at \url{https://github.com/THU-KEG/TWAG}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fangwei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1"&gt;Shangqing Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Lei Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1"&gt;Tong Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sexism in the Judiciary. (arXiv:2106.15103v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15103</id>
        <link href="http://arxiv.org/abs/2106.15103"/>
        <updated>2021-06-30T02:00:59.960Z</updated>
        <summary type="html"><![CDATA[We analyze 6.7 million case law documents to determine the presence of gender
bias within our judicial system. We find that current bias detectino methods in
NLP are insufficient to determine gender bias in our case law database and
propose an alternative approach. We show that existing algorithms' inconsistent
results are consequences of prior research's definition of biases themselves.
Bias detection algorithms rely on groups of words to represent bias (e.g.,
'salary,' 'job,' and 'boss' to represent employment as a potentially biased
theme against women in text). However, the methods to build these groups of
words have several weaknesses, primarily that the word lists are based on the
researchers' own intuitions. We suggest two new methods of automating the
creation of word lists to represent biases. We find that our methods outperform
current NLP bias detection methods. Our research improves the capabilities of
NLP technology to detect bias and highlights gender biases present in
influential case law. In order test our NLP bias detection method's
performance, we regress our results of bias in case law against U.S census data
of women's participation in the workforce in the last 100 years.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gillis_N/0/1/0/all/0/1"&gt;Noa Baker Gillis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Capacity of Quantum Private Information Retrieval from MDS-Coded and Colluding Servers. (arXiv:2106.14719v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14719</id>
        <link href="http://arxiv.org/abs/2106.14719"/>
        <updated>2021-06-30T02:00:59.939Z</updated>
        <summary type="html"><![CDATA[In quantum private information retrieval (QPIR), a user retrieves a classical
file from multiple servers by downloading quantum systems without revealing the
identity of the file. The QPIR capacity is the maximal achievable ratio of the
retrieved file size to the total download size. In this paper, the capacity of
QPIR from MDS-coded and colluding servers is studied. Two classes of QPIR,
called stabilizer QPIR and dimension squared QPIR induced from classical
strongly linear PIR are defined, and the related QPIR capacities are derived.
For the non-colluding case, the general QPIR capacity is derived when the
number of files goes to infinity. The capacities of symmetric and non-symmetric
QPIR with coded and colluding servers are proved to coincide, being double to
their classical counterparts. A general statement on the converse bound for
QPIR with coded and colluding servers is derived showing that the capacities of
stabilizer QPIR and dimension squared QPIR induced from any class of PIR are
upper bounded by twice the classical capacity of the respective PIR class. The
proposed capacity-achieving scheme combines the star-product scheme by
Freij-Hollanti et al. and the stabilizer QPIR scheme by Song et al. by
employing (weakly) self-dual Reed--Solomon codes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allaix_M/0/1/0/all/0/1"&gt;Matteo Allaix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Seunghoan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holzbaur_L/0/1/0/all/0/1"&gt;Lukas Holzbaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pllaha_T/0/1/0/all/0/1"&gt;Tefjol Pllaha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayashi_M/0/1/0/all/0/1"&gt;Masahito Hayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hollanti_C/0/1/0/all/0/1"&gt;Camilla Hollanti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time-Aware Language Models as Temporal Knowledge Bases. (arXiv:2106.15110v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15110</id>
        <link href="http://arxiv.org/abs/2106.15110"/>
        <updated>2021-06-30T02:00:59.929Z</updated>
        <summary type="html"><![CDATA[Many facts come with an expiration date, from the name of the President to
the basketball team Lebron James plays for. But language models (LMs) are
trained on snapshots of data collected at a specific moment in time, and this
can limit their utility, especially in the closed-book setting where the
pretraining corpus must contain the facts the model should memorize. We
introduce a diagnostic dataset aimed at probing LMs for factual knowledge that
changes over time and highlight problems with LMs at either end of the spectrum
-- those trained on specific slices of temporal data, as well as those trained
on a wide range of temporal data. To mitigate these problems, we propose a
simple technique for jointly modeling text with its timestamp. This improves
memorization of seen facts from the training time period, as well as
calibration on predictions about unseen facts from future time periods. We also
show that models trained with temporal context can be efficiently ``refreshed''
as new data arrives, without the need for retraining from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1"&gt;Jeremy R. Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1"&gt;Julian Martin Eisenschlos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gillick_D/0/1/0/all/0/1"&gt;Daniel Gillick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1"&gt;Jacob Eisenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topic-to-Essay Generation with Comprehensive Knowledge Enhancement. (arXiv:2106.15142v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15142</id>
        <link href="http://arxiv.org/abs/2106.15142"/>
        <updated>2021-06-30T02:00:59.920Z</updated>
        <summary type="html"><![CDATA[Generating high-quality and diverse essays with a set of topics is a
challenging task in natural language generation. Since several given topics
only provide limited source information, utilizing various topic-related
knowledge is essential for improving essay generation performance. However,
previous works cannot sufficiently use that knowledge to facilitate the
generation procedure. This paper aims to improve essay generation by extracting
information from both internal and external knowledge. Thus, a topic-to-essay
generation model with comprehensive knowledge enhancement, named TEGKE, is
proposed. For internal knowledge enhancement, both topics and related essays
are fed to a teacher network as source information. Then, informative features
would be obtained from the teacher network and transferred to a student network
which only takes topics as input but provides comparable information compared
with the teacher network. For external knowledge enhancement, a topic knowledge
graph encoder is proposed. Unlike the previous works only using the nearest
neighbors of topics in the commonsense base, our topic knowledge graph encoder
could exploit more structural and semantic information of the commonsense
knowledge graph to facilitate essay generation. Moreover, the adversarial
training based on the Wasserstein distance is proposed to improve generation
quality. Experimental results demonstrate that TEGKE could achieve
state-of-the-art performance on both automatic and human evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyue Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiahai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenghong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Construction of Enterprise Knowledge Base. (arXiv:2106.15085v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.15085</id>
        <link href="http://arxiv.org/abs/2106.15085"/>
        <updated>2021-06-30T02:00:59.898Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an automatic knowledge base construction system
from large scale enterprise documents with minimal efforts of human
intervention. In the design and deployment of such a knowledge mining system
for enterprise, we faced several challenges including data distributional
shift, performance evaluation, compliance requirements and other practical
issues. We leveraged state-of-the-art deep learning models to extract
information (named entities and definitions) at per document level, then
further applied classical machine learning techniques to process global
statistical information to improve the knowledge base. Experimental results are
reported on actual enterprise documents. This system is currently serving as
part of a Microsoft 365 service.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1"&gt;Junyi Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yujie He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1"&gt;Homa Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parveen_D/0/1/0/all/0/1"&gt;Daraksha Parveen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondapally_R/0/1/0/all/0/1"&gt;Ranganath Kondapally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenjin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Neural Speech Synthesis. (arXiv:2106.15561v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.15561</id>
        <link href="http://arxiv.org/abs/2106.15561"/>
        <updated>2021-06-30T02:00:59.384Z</updated>
        <summary type="html"><![CDATA[Text to speech (TTS), or speech synthesis, which aims to synthesize
intelligible and natural speech given text, is a hot research topic in speech,
language, and machine learning communities and has broad applications in the
industry. As the development of deep learning and artificial intelligence,
neural network-based TTS has significantly improved the quality of synthesized
speech in recent years. In this paper, we conduct a comprehensive survey on
neural TTS, aiming to provide a good understanding of current research and
future trends. We focus on the key components in neural TTS, including text
analysis, acoustic models and vocoders, and several advanced topics, including
fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.
We further summarize resources related to TTS (e.g., datasets, opensource
implementations) and discuss future research directions. This survey can serve
both academic researchers and industry practitioners working on TTS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1"&gt;Frank Soong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition. (arXiv:2101.11530v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11530</id>
        <link href="http://arxiv.org/abs/2101.11530"/>
        <updated>2021-06-30T02:00:59.373Z</updated>
        <summary type="html"><![CDATA[We introduce SynSE, a novel syntactically guided generative approach for
Zero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined
generative embedding spaces constrained within and across the involved
modalities (visual, language). The inter-modal constraints are defined between
action sequence embedding and embeddings of Parts of Speech (PoS) tagged words
in the corresponding action description. We deploy SynSE for the task of
skeleton-based action sequence recognition. Our design choices enable SynSE to
generalize compositionally, i.e., recognize sequences whose action descriptions
contain words not encountered during training. We also extend our approach to
the more challenging Generalized Zero-Shot Learning (GZSL) problem via a
confidence-based gating mechanism. We are the first to present zero-shot
skeleton action recognition results on the large-scale NTU-60 and NTU-120
skeleton action datasets with multiple splits. Our results demonstrate SynSE's
state of the art performance in both ZSL and GZSL settings compared to strong
baselines on the NTU-60 and NTU-120 datasets. The code and pretrained models
are available at https://github.com/skelemoa/synse-zsl]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Pranay Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Divyanshu Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overview of BioASQ 2021: The ninth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering. (arXiv:2106.14885v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14885</id>
        <link href="http://arxiv.org/abs/2106.14885"/>
        <updated>2021-06-30T02:00:59.350Z</updated>
        <summary type="html"><![CDATA[Advancing the state-of-the-art in large-scale biomedical semantic indexing
and question answering is the main focus of the BioASQ challenge. BioASQ
organizes respective tasks where different teams develop systems that are
evaluated on the same benchmark datasets that represent the real information
needs of experts in the biomedical domain. This paper presents an overview of
the ninth edition of the BioASQ challenge in the context of the Conference and
Labs of the Evaluation Forum (CLEF) 2021. In this year, a new question
answering task, named Synergy, is introduced to support researchers studying
the COVID-19 disease and measure the ability of the participating teams to
discern information while the problem is still developing. In total, 42 teams
with more than 170 systems were registered to participate in the four tasks of
the challenge. The evaluation results, similarly to previous years, show a
performance gain against the baselines which indicates the continuous
improvement of the state-of-the-art in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1"&gt;Anastasios Nentidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katsimpras_G/0/1/0/all/0/1"&gt;Georgios Katsimpras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vandorou_E/0/1/0/all/0/1"&gt;Eirini Vandorou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1"&gt;Anastasia Krithara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasco_L/0/1/0/all/0/1"&gt;Luis Gasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krallinger_M/0/1/0/all/0/1"&gt;Martin Krallinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1"&gt;Georgios Paliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On component interactions in two-stage recommender systems. (arXiv:2106.14979v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.14979</id>
        <link href="http://arxiv.org/abs/2106.14979"/>
        <updated>2021-06-30T02:00:59.314Z</updated>
        <summary type="html"><![CDATA[Thanks to their scalability, two-stage recommenders are used by many of
today's largest online platforms, including YouTube, LinkedIn, and Pinterest.
These systems produce recommendations in two steps: (i) multiple nominators --
tuned for low prediction latency -- preselect a small subset of candidates from
the whole item pool; (ii)~a slower but more accurate ranker further narrows
down the nominated items, and serves to the user. Despite their popularity, the
literature on two-stage recommenders is relatively scarce, and the algorithms
are often treated as the sum of their parts. Such treatment presupposes that
the two-stage performance is explained by the behavior of individual components
if they were deployed independently. This is not the case: using synthetic and
real-world data, we demonstrate that interactions between the ranker and the
nominators substantially affect the overall performance. Motivated by these
findings, we derive a generalization lower bound which shows that careful
choice of each nominator's training set is sometimes the only difference
between a poor and an optimal two-stage recommender. Since searching for a good
choice manually is difficult, we learn one instead. In particular, using a
Mixture-of-Experts approach, we train the nominators (experts) to specialize on
different subsets of the item pool. This significantly improves performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hron_J/0/1/0/all/0/1"&gt;Jiri Hron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1"&gt;Karl Krauth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1"&gt;Niki Kilbertus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Hypercomplex Embeddings for Link Prediction. (arXiv:2106.15230v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.15230</id>
        <link href="http://arxiv.org/abs/2106.15230"/>
        <updated>2021-06-30T02:00:59.290Z</updated>
        <summary type="html"><![CDATA[Knowledge graph embedding research has mainly focused on the two smallest
normed division algebras, $\mathbb{R}$ and $\mathbb{C}$. Recent results suggest
that trilinear products of quaternion-valued embeddings can be a more effective
means to tackle link prediction. In addition, models based on convolutions on
real-valued embeddings often yield state-of-the-art results for link
prediction. In this paper, we investigate a composition of convolution
operations with hypercomplex multiplications. We propose the four approaches
QMult, OMult, ConvQ and ConvO to tackle the link prediction problem. QMult and
OMult can be considered as quaternion and octonion extensions of previous
state-of-the-art approaches, including DistMult and ComplEx. ConvQ and ConvO
build upon QMult and OMult by including convolution operations in a way
inspired by the residual learning framework. We evaluated our approaches on
seven link prediction datasets including WN18RR, FB15K-237 and YAGO3-10.
Experimental results suggest that the benefits of learning hypercomplex-valued
vector representations become more apparent as the size and complexity of the
knowledge graph grows. ConvO outperforms state-of-the-art approaches on
FB15K-237 in MRR, Hit@1 and Hit@3, while QMult, OMult, ConvQ and ConvO
outperform state-of-the-approaches on YAGO3-10 in all metrics. Results also
suggest that link prediction performances can be further improved via
prediction averaging. To foster reproducible research, we provide an
open-source implementation of approaches, including training and evaluation
scripts as well as pretrained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1"&gt;Caglar Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moussallem_D/0/1/0/all/0/1"&gt;Diego Moussallem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heindorf_S/0/1/0/all/0/1"&gt;Stefan Heindorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1"&gt;Axel-Cyrille Ngonga Ngomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepFaceLab: Integrated, flexible and extensible face-swapping framework. (arXiv:2005.05535v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05535</id>
        <link href="http://arxiv.org/abs/2005.05535"/>
        <updated>2021-06-30T02:00:59.142Z</updated>
        <summary type="html"><![CDATA[Deepfake defense not only requires the research of detection but also
requires the efforts of generation methods. However, current deepfake methods
suffer the effects of obscure workflow and poor performance. To solve this
problem, we present DeepFaceLab, the current dominant deepfake framework for
face-swapping. It provides the necessary tools as well as an easy-to-use way to
conduct high-quality face-swapping. It also offers a flexible and loose
coupling structure for people who need to strengthen their pipeline with other
features without writing complicated boilerplate code. We detail the principles
that drive the implementation of DeepFaceLab and introduce its pipeline,
through which every aspect of the pipeline can be modified painlessly by users
to achieve their customization purpose. It is noteworthy that DeepFaceLab could
achieve cinema-quality results with high fidelity. We demonstrate the advantage
of our system by comparing our approach with other face-swapping methods.For
more information, please visit:https://github.com/iperov/DeepFaceLab/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perov_I/0/1/0/all/0/1"&gt;Ivan Perov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1"&gt;Daiheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chervoniy_N/0/1/0/all/0/1"&gt;Nikolay Chervoniy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kunlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marangonda_S/0/1/0/all/0/1"&gt;Sugasa Marangonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ume_C/0/1/0/all/0/1"&gt;Chris Um&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dpfks_M/0/1/0/all/0/1"&gt;Mr. Dpfks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Facenheim_C/0/1/0/all/0/1"&gt;Carl Shift Facenheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+RP_L/0/1/0/all/0/1"&gt;Luis RP&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Pingyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MuViS: Online MU-MIMO Grouping for Multi-User Applications Over Commodity WiFi. (arXiv:2106.15262v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.15262</id>
        <link href="http://arxiv.org/abs/2106.15262"/>
        <updated>2021-06-30T02:00:59.126Z</updated>
        <summary type="html"><![CDATA[Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency
have promised to increase data throughput by allowing concurrent communication
between one Access Point and multiple users. However, we are still a long way
from enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry
applications such as video streaming in practical WiFi network settings due to
heterogeneous channel conditions and devices, unreliable transmissions, and
lack of useful feedback exchange among the lower and upper layers'
requirements. This paper introduces MuViS, a novel dual-phase optimization
framework that proposes a Quality of Experience (QoE) aware MU-MIMO
optimization for multi-user video streaming over IEEE 802.11ac. MuViS first
employs reinforcement learning to optimize the MU-MIMO user group and mode
selection for users based on their PHY/MAC layer characteristics. The video
bitrate is then optimized based on the user's mode (Multi-User (MU) or
Single-User (SU)). We present our design and its evaluation on smartphones and
laptops using 802.11ac WiFi. Our experimental results in various indoor
environments and configurations show a scalable framework that can support a
large number of users with streaming at high video rates and satisfying QoE
requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1"&gt;Hannaneh Barahouei Pasandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1"&gt;Tamer Nadeem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1"&gt;Hadi Amirpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System. (arXiv:2106.15130v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.15130</id>
        <link href="http://arxiv.org/abs/2106.15130"/>
        <updated>2021-06-30T02:00:59.106Z</updated>
        <summary type="html"><![CDATA[The last-generation video conferencing software allows users to utilize a
virtual background to conceal their personal environment due to privacy
concerns, especially in official meetings with other employers. On the other
hand, users maybe want to fool people in the meeting by considering the virtual
background to conceal where they are. In this case, developing tools to
understand the virtual background utilize for fooling people in meeting plays
an important role. Besides, such detectors must prove robust against different
kinds of attacks since a malicious user can fool the detector by applying a set
of adversarial editing steps on the video to conceal any revealing footprint.
In this paper, we study the feasibility of an efficient tool to detect whether
a videoconferencing user background is real. In particular, we provide the
first tool which computes pixel co-occurrences matrices and uses them to search
for inconsistencies among spectral and spatial bands. Our experiments confirm
that cross co-occurrences matrices improve the robustness of the detector
against different kinds of attacks. This work's performance is especially
noteworthy with regard to color SPAM features. Moreover, the performance
especially is significant with regard to robustness versus post-processing,
like geometric transformations, filtering, contrast enhancement, and JPEG
compression with different quality factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1"&gt;Mauro Conti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1"&gt;Simone Milani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1"&gt;Ehsan Nowroozi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orazi_G/0/1/0/all/0/1"&gt;Gabriele Orazi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decaying Clipping Range in Proximal Policy Optimization. (arXiv:2102.10456v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10456</id>
        <link href="http://arxiv.org/abs/2102.10456"/>
        <updated>2021-06-29T01:55:20.694Z</updated>
        <summary type="html"><![CDATA[Proximal Policy Optimization (PPO) is among the most widely used algorithms
in reinforcement learning, which achieves state-of-the-art performance in many
challenging problems. The keys to its success are the reliable policy updates
through the clipping mechanism and the multiple epochs of minibatch updates.
The aim of this research is to give new simple but effective alternatives to
the former. For this, we propose linearly and exponentially decaying clipping
range approaches throughout the training. With these, we would like to provide
higher exploration at the beginning and stronger restrictions at the end of the
learning phase. We investigate their performance in several classical control
and locomotive robotic environments. During the analysis, we found that they
influence the achieved rewards and are effective alternatives to the constant
clipping method in many reinforcement learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farsang_M/0/1/0/all/0/1"&gt;M&amp;#xf3;nika Farsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szegletes_L/0/1/0/all/0/1"&gt;Luca Szegletes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Importance of Environment Design in Reinforcement Learning: A Study of a Robotic Environment. (arXiv:2102.10447v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10447</id>
        <link href="http://arxiv.org/abs/2102.10447"/>
        <updated>2021-06-29T01:55:20.689Z</updated>
        <summary type="html"><![CDATA[An in-depth understanding of the particular environment is crucial in
reinforcement learning (RL). To address this challenge, the decision-making
process of a mobile collaborative robotic assistant modeled by the Markov
decision process (MDP) framework is studied in this paper. The optimal
state-action combinations of the MDP are calculated with the non-linear Bellman
optimality equations. This system of equations can be solved with relative ease
by the computational power of Wolfram Mathematica, where the obtained optimal
action-values point to the optimal policy. Unlike other RL algorithms, this
methodology does not approximate the optimal behavior, it gives the exact,
explicit solution, which provides a strong foundation for our study. With this,
we offer new insights into understanding the action selection mechanisms in RL
by presenting various small modifications on the very same schema that lead to
different optimal policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farsang_M/0/1/0/all/0/1"&gt;M&amp;#xf3;nika Farsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szegletes_L/0/1/0/all/0/1"&gt;Luca Szegletes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Query Complexity of Least Absolute Deviation Regression via Robust Uniform Convergence. (arXiv:2102.02322v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02322</id>
        <link href="http://arxiv.org/abs/2102.02322"/>
        <updated>2021-06-29T01:55:20.671Z</updated>
        <summary type="html"><![CDATA[Consider a regression problem where the learner is given a large collection
of $d$-dimensional data points, but can only query a small subset of the
real-valued labels. How many queries are needed to obtain a $1+\epsilon$
relative error approximation of the optimum? While this problem has been
extensively studied for least squares regression, little is known for other
losses. An important example is least absolute deviation regression ($\ell_1$
regression) which enjoys superior robustness to outliers compared to least
squares. We develop a new framework for analyzing importance sampling methods
in regression problems, which enables us to show that the query complexity of
least absolute deviation regression is $\Theta(d/\epsilon^2)$ up to logarithmic
factors. We further extend our techniques to show the first bounds on the query
complexity for any $\ell_p$ loss with $p\in(1,2)$. As a key novelty in our
analysis, we introduce the notion of robust uniform convergence, which is a new
approximation guarantee for the empirical loss. While it is inspired by uniform
convergence in statistical learning, our approach additionally incorporates a
correction term to avoid unnecessary variance due to outliers. This can be
viewed as a new connection between statistical learning theory and variance
reduction techniques in stochastic optimization, which should be of independent
interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xue Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Derezi&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Longitudinal Self-Supervised Learning. (arXiv:2006.06930v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06930</id>
        <link href="http://arxiv.org/abs/2006.06930"/>
        <updated>2021-06-29T01:55:20.665Z</updated>
        <summary type="html"><![CDATA[Machine learning analysis of longitudinal neuroimaging data is typically
based on supervised learning, which requires a large number of ground-truth
labels to be informative. As ground-truth labels are often missing or expensive
to obtain in neuroscience, we avoid them in our analysis by combing factor
disentanglement with self-supervised learning to identify changes and
consistencies across the multiple MRIs acquired of each individual over time.
Specifically, we propose a new definition of disentanglement by formulating a
multivariate mapping between factors (e.g., brain age) associated with an MRI
and a latent image representation. Then, factors that evolve across
acquisitions of longitudinal sequences are disentangled from that mapping by
self-supervised learning in such a way that changes in a single factor induce
change along one direction in the representation space. We implement this
model, named Longitudinal Self-Supervised Learning (LSSL), via a standard
autoencoding structure with a cosine loss to disentangle brain age from the
image representation. We apply LSSL to two longitudinal neuroimaging studies to
highlight its strength in extracting the brain-age information from MRI and
revealing informative characteristics associated with neurodegenerative and
neuropsychological disorders. Moreover, the representations learned by LSSL
facilitate supervised classification by recording faster convergence and higher
(or similar) prediction accuracy compared to several other representation
learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zixuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Power Laws in Deep Ensembles. (arXiv:2007.08483v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08483</id>
        <link href="http://arxiv.org/abs/2007.08483"/>
        <updated>2021-06-29T01:55:20.659Z</updated>
        <summary type="html"><![CDATA[Ensembles of deep neural networks are known to achieve state-of-the-art
performance in uncertainty estimation and lead to accuracy improvement. In this
work, we focus on a classification problem and investigate the behavior of both
non-calibrated and calibrated negative log-likelihood (CNLL) of a deep ensemble
as a function of the ensemble size and the member network size. We indicate the
conditions under which CNLL follows a power law w.r.t. ensemble size or member
network size, and analyze the dynamics of the parameters of the discovered
power laws. Our important practical finding is that one large network may
perform worse than an ensemble of several medium-size networks with the same
total number of parameters (we call this ensemble a memory split). Using the
detected power law-like dependencies, we can predict (1) the possible gain from
the ensembling of networks with given structure, (2) the optimal memory split
given a memory budget, based on a relatively small number of trained networks.

We describe the memory split advantage effect in more details in
arXiv:2005.07292]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1"&gt;Ekaterina Lobacheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1"&gt;Nadezhda Chirkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1"&gt;Maxim Kodryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Deep Learning. (arXiv:2012.08405v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08405</id>
        <link href="http://arxiv.org/abs/2012.08405"/>
        <updated>2021-06-29T01:55:20.653Z</updated>
        <summary type="html"><![CDATA[Signal processing, communications, and control have traditionally relied on
classical statistical modeling techniques. Such model-based methods utilize
mathematical formulations that represent the underlying physics, prior
information and additional domain knowledge. Simple classical models are useful
but sensitive to inaccuracies and may lead to poor performance when real
systems display complex or dynamic behavior. On the other hand, purely
data-driven approaches that are model-agnostic are becoming increasingly
popular as datasets become abundant and the power of modern deep learning
pipelines increases. Deep neural networks (DNNs) use generic architectures
which learn to operate from data, and demonstrate excellent performance,
especially for supervised problems. However, DNNs typically require massive
amounts of data and immense computational resources, limiting their
applicability for some signal processing scenarios. We are interested in hybrid
techniques that combine principled mathematical models with data-driven systems
to benefit from the advantages of both approaches. Such model-based deep
learning methods exploit both partial domain knowledge, via mathematical
structures designed for specific problems, as well as learning from limited
data. In this article we survey the leading approaches for studying and
designing model-based deep learning systems. We divide hybrid
model-based/data-driven systems into categories based on their inference
mechanism. We provide a comprehensive review of the leading approaches for
combining model-based algorithms with deep learning in a systematic manner,
along with concrete guidelines and detailed signal processing oriented examples
from recent literature. Our aim is to facilitate the design and study of future
systems on the intersection of signal processing and machine learning that
incorporate the advantages of both domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shlezinger_N/0/1/0/all/0/1"&gt;Nir Shlezinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Whang_J/0/1/0/all/0/1"&gt;Jay Whang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accurate Learning of Graph Representations with Graph Multiset Pooling. (arXiv:2102.11533v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11533</id>
        <link href="http://arxiv.org/abs/2102.11533"/>
        <updated>2021-06-29T01:55:20.646Z</updated>
        <summary type="html"><![CDATA[Graph neural networks have been widely used on modeling graph data, achieving
impressive results on node classification and link prediction tasks. Yet,
obtaining an accurate representation for a graph further requires a pooling
function that maps a set of node representations into a compact form. A simple
sum or average over all node representations considers all node features
equally without consideration of their task relevance, and any structural
dependencies among them. Recently proposed hierarchical graph pooling methods,
on the other hand, may yield the same representation for two different graphs
that are distinguished by the Weisfeiler-Lehman test, as they suboptimally
preserve information from the node features. To tackle these limitations of
existing graph pooling methods, we first formulate the graph pooling problem as
a multiset encoding problem with auxiliary information about the graph
structure, and propose a Graph Multiset Transformer (GMT) which is a multi-head
attention based global pooling layer that captures the interaction between
nodes according to their structural dependencies. We show that GMT satisfies
both injectiveness and permutation invariance, such that it is at most as
powerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods
can be easily extended to the previous node clustering approaches for
hierarchical graph pooling. Our experimental results show that GMT
significantly outperforms state-of-the-art graph pooling methods on graph
classification benchmarks with high memory and time efficiency, and obtains
even larger performance gain on graph reconstruction and generation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1"&gt;Jinheon Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Minki Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-optimality in optimal value estimation: Adaptivity via variance-reduced Q-learning. (arXiv:2106.14352v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.14352</id>
        <link href="http://arxiv.org/abs/2106.14352"/>
        <updated>2021-06-29T01:55:20.627Z</updated>
        <summary type="html"><![CDATA[Various algorithms in reinforcement learning exhibit dramatic variability in
their convergence rates and ultimate accuracy as a function of the problem
structure. Such instance-specific behavior is not captured by existing global
minimax bounds, which are worst-case in nature. We analyze the problem of
estimating optimal $Q$-value functions for a discounted Markov decision process
with discrete states and actions and identify an instance-dependent functional
that controls the difficulty of estimation in the $\ell_\infty$-norm. Using a
local minimax framework, we show that this functional arises in lower bounds on
the accuracy on any estimation procedure. In the other direction, we establish
the sharpness of our lower bounds, up to factors logarithmic in the state and
action spaces, by analyzing a variance-reduced version of $Q$-learning. Our
theory provides a precise way of distinguishing "easy" problems from "hard"
ones in the context of $Q$-learning, as illustrated by an ensemble with a
continuum of difficulty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Khamaru_K/0/1/0/all/0/1"&gt;Koulik Khamaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xia_E/0/1/0/all/0/1"&gt;Eric Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1"&gt;Martin J. Wainwright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Large-scale Graph Similarity Computation via Graph Coarsening and Matching. (arXiv:2005.07115v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07115</id>
        <link href="http://arxiv.org/abs/2005.07115"/>
        <updated>2021-06-29T01:55:20.620Z</updated>
        <summary type="html"><![CDATA[In this work, we focus on large graph similarity computation problem and
propose a novel "embedding-coarsening-matching" learning framework, which
outperforms state-of-the-art methods in this task and has significant
improvement in time efficiency. Graph similarity computation for metrics such
as Graph Edit Distance (GED) is typically NP-hard, and existing
heuristics-based algorithms usually achieves a unsatisfactory trade-off between
accuracy and efficiency. Recently the development of deep learning techniques
provides a promising solution for this problem by a data-driven approach which
trains a network to encode graphs to their own feature vectors and computes
similarity based on feature vectors. These deep-learning methods can be
classified to two categories, embedding models and matching models. Embedding
models such as GCN-Mean and GCN-Max, which directly map graphs to respective
feature vectors, run faster but the performance is usually poor due to the lack
of interactions across graphs. Matching models such as GMN, whose encoding
process involves interaction across the two graphs, are more accurate but
interaction between whole graphs brings a significant increase in time
consumption (at least quadratic time complexity over number of nodes). Inspired
by large biological molecular identification where the whole molecular is first
mapped to functional groups and then identified based on these functional
groups, our "embedding-coarsening-matching" learning framework first embeds and
coarsens large graphs to coarsened graphs with denser local topology and then
matching mechanism is deployed on the coarsened graphs for the final similarity
scores. Detailed experiments have been conducted and the results demonstrate
the efficiency and effectiveness of our proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haoyan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Runjian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yunsheng Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1"&gt;Ziheng Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jie Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yizhou Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning of Context-dependent Processing in Neural Networks. (arXiv:1810.01256v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.01256</id>
        <link href="http://arxiv.org/abs/1810.01256"/>
        <updated>2021-06-29T01:55:20.614Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are powerful tools in learning sophisticated but
fixed mapping rules between inputs and outputs, thereby limiting their
application in more complex and dynamic situations in which the mapping rules
are not kept the same but changing according to different contexts. To lift
such limits, we developed a novel approach involving a learning algorithm,
called orthogonal weights modification (OWM), with the addition of a
context-dependent processing (CDP) module. We demonstrated that with OWM to
overcome the problem of catastrophic forgetting, and the CDP module to learn
how to reuse a feature representation and a classifier for different contexts,
a single network can acquire numerous context-dependent mapping rules in an
online and continual manner, with as few as $\sim$10 samples to learn each.
This should enable highly compact systems to gradually learn myriad
regularities of the real world and eventually behave appropriately within it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1"&gt;Guanxiong Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bo Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shan Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Recourse in the Wild: Understanding the Impact of Data and Model Shifts. (arXiv:2012.11788v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11788</id>
        <link href="http://arxiv.org/abs/2012.11788"/>
        <updated>2021-06-29T01:55:20.599Z</updated>
        <summary type="html"><![CDATA[As predictive models are increasingly being deployed to make a variety of
consequential decisions, there is a growing emphasis on designing algorithms
that can provide recourse to affected individuals. Existing recourse algorithms
function under the assumption that the underlying predictive model does not
change. However, models are regularly updated in practice for several reasons
including data distribution shifts. In this work, we make the first attempt at
understanding how model updates resulting from data distribution shifts impact
the algorithmic recourses generated by state-of-the-art algorithms. We carry
out a rigorous theoretical and empirical analysis to address the above
question. Our theoretical results establish a lower bound on the probability of
recourse invalidation due to model shifts, and show the existence of a tradeoff
between this invalidation probability and typical notions of "cost" minimized
by modern recourse generation algorithms. We experiment with multiple synthetic
and real world datasets, capturing different kinds of distribution shifts
including temporal shifts, geospatial shifts, and shifts due to data
correction. These experiments demonstrate that model updation due to all the
aforementioned distribution shifts can potentially invalidate recourses
generated by state-of-the-art algorithms. Our findings thus not only expose
previously unknown flaws in the current recourse generation paradigm, but also
pave the way for fundamentally rethinking the design and development of
recourse generation algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rawal_K/0/1/0/all/0/1"&gt;Kaivalya Rawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1"&gt;Ece Kamar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Private k-Means Clustering. (arXiv:1907.02513v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.02513</id>
        <link href="http://arxiv.org/abs/1907.02513"/>
        <updated>2021-06-29T01:55:20.593Z</updated>
        <summary type="html"><![CDATA[We design a new algorithm for the Euclidean $k$-means problem that operates
in the local model of differential privacy. Unlike in the non-private
literature, differentially private algorithms for the $k$-means objective incur
both additive and multiplicative errors. Our algorithm significantly reduces
the additive error while keeping the multiplicative error the same as in
previous state-of-the-art results. Specifically, on a database of size $n$, our
algorithm guarantees $O(1)$ multiplicative error and $\approx n^{1/2+a}$
additive error for an arbitrarily small constant $a>0$. All previous algorithms
in the local model had additive error $\approx n^{2/3+a}$. Our techniques
extend to $k$-median clustering.

We show that the additive error we obtain is almost optimal in terms of its
dependency on the database size $n$. Specifically, we give a simple lower bound
showing that every locally-private algorithm for the $k$-means objective must
have additive error at least $\approx\sqrt{n}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1"&gt;Uri Stemmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning. (arXiv:1906.02944v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.02944</id>
        <link href="http://arxiv.org/abs/1906.02944"/>
        <updated>2021-06-29T01:55:20.579Z</updated>
        <summary type="html"><![CDATA[Object recognition in the real-world requires handling long-tailed or even
open-ended data. An ideal visual system needs to recognize the populated head
visual concepts reliably and meanwhile efficiently learn about emerging new
tail categories with a few training instances. Class-balanced many-shot
learning and few-shot learning tackle one side of this problem, by either
learning strong classifiers for head or learning to learn few-shot classifiers
for the tail. In this paper, we investigate the problem of generalized few-shot
learning (GFSL) -- a model during the deployment is required to learn about
tail categories with few shots and simultaneously classify the head classes. We
propose the ClAssifier SynThesis LEarning (CASTLE), a learning framework that
learns how to synthesize calibrated few-shot classifiers in addition to the
multi-class classifiers of head classes with a shared neural dictionary,
shedding light upon the inductive GFSL. Furthermore, we propose an adaptive
version of CASTLE (ACASTLE) that adapts the head classifiers conditioned on the
incoming tail training examples, yielding a framework that allows effective
backward knowledge transfer. As a consequence, ACASTLE can handle GFSL with
classes from heterogeneous domains effectively. CASTLE and ACASTLE demonstrate
superior performances than existing GFSL algorithms and strong baselines on
MiniImageNet as well as TieredImageNet datasets. More interestingly, they
outperform previous state-of-the-art methods when evaluated with standard
few-shot learning criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hexiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning transferable and discriminative features for unsupervised domain adaptation. (arXiv:2003.11723v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.11723</id>
        <link href="http://arxiv.org/abs/2003.11723"/>
        <updated>2021-06-29T01:55:20.573Z</updated>
        <summary type="html"><![CDATA[Although achieving remarkable progress, it is very difficult to induce a
supervised classifier without any labeled data. Unsupervised domain adaptation
is able to overcome this challenge by transferring knowledge from a labeled
source domain to an unlabeled target domain. Transferability and
discriminability are two key criteria for characterizing the superiority of
feature representations to enable successful domain adaptation. In this paper,
a novel method called \textit{learning TransFerable and Discriminative Features
for unsupervised domain adaptation} (TFDF) is proposed to optimize these two
objectives simultaneously. On the one hand, distribution alignment is performed
to reduce domain discrepancy and learn more transferable representations.
Instead of adopting \textit{Maximum Mean Discrepancy} (MMD) which only captures
the first-order statistical information to measure distribution discrepancy, we
adopt a recently proposed statistic called \textit{Maximum Mean and Covariance
Discrepancy} (MMCD), which can not only capture the first-order statistical
information but also capture the second-order statistical information in the
reproducing kernel Hilbert space (RKHS). On the other hand, we propose to
explore both local discriminative information via manifold regularization and
global discriminative information via minimizing the proposed \textit{class
confusion} objective to learn more discriminative features, respectively. We
integrate these two objectives into the \textit{Structural Risk Minimization}
(RSM) framework and learn a domain-invariant classifier. Comprehensive
experiments are conducted on five real-world datasets and the results verify
the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuntao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruiting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaowen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yirong Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Hengyang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chongjun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSAF: Multimodal Split Attention Fusion. (arXiv:2012.07175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07175</id>
        <link href="http://arxiv.org/abs/2012.07175"/>
        <updated>2021-06-29T01:55:20.566Z</updated>
        <summary type="html"><![CDATA[Multimodal learning mimics the reasoning process of the human multi-sensory
system, which is used to perceive the surrounding world. While making a
prediction, the human brain tends to relate crucial cues from multiple sources
of information. In this work, we propose a novel multimodal fusion module that
learns to emphasize more contributive features across all modalities.
Specifically, the proposed Multimodal Split Attention Fusion (MSAF) module
splits each modality into channel-wise equal feature blocks and creates a joint
representation that is used to generate soft attention for each channel across
the feature blocks. Further, the MSAF module is designed to be compatible with
features of various spatial dimensions and sequence lengths, suitable for both
CNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal
networks and utilize existing pretrained unimodal model weights. To demonstrate
the effectiveness of our fusion module, we design three multimodal networks
with MSAF for emotion recognition, sentiment analysis, and action recognition
tasks. Our approach achieves competitive results in each task and outperforms
other application-specific networks and multimodal fusion benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Lang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chuqing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guofa Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1"&gt;Dongpu Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Gaussian Networks. (arXiv:1302.6808v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1302.6808</id>
        <link href="http://arxiv.org/abs/1302.6808"/>
        <updated>2021-06-29T01:55:20.560Z</updated>
        <summary type="html"><![CDATA[We describe algorithms for learning Bayesian networks from a combination of
user knowledge and statistical data. The algorithms have two components: a
scoring metric and a search procedure. The scoring metric takes a network
structure, statistical data, and a user's prior knowledge, and returns a score
proportional to the posterior probability of the network structure given the
data. The search procedure generates networks for evaluation by the scoring
metric. Previous work has concentrated on metrics for domains containing only
discrete variables, under the assumption that data represents a multinomial
sample. In this paper, we extend this work, developing scoring metrics for
domains containing all continuous variables or a mixture of discrete and
continuous variables, under the assumption that continuous data is sampled from
a multivariate normal distribution. Our work extends traditional statistical
approaches for identifying vanishing regression coefficients in that we
identify two important assumptions, called event equivalence and parameter
modularity, that when combined allow the construction of prior distributions
for multivariate normal parameters from a single prior Bayesian network
specified by a user.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1"&gt;Dan Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1"&gt;David Heckerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Convergence of Gradient Descent for Asymmetric Low-Rank Matrix Factorization. (arXiv:2106.14289v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.14289</id>
        <link href="http://arxiv.org/abs/2106.14289"/>
        <updated>2021-06-29T01:55:20.554Z</updated>
        <summary type="html"><![CDATA[We study the asymmetric low-rank factorization problem: \[\min_{\mathbf{U}
\in \mathbb{R}^{m \times d}, \mathbf{V} \in \mathbb{R}^{n \times d}}
\frac{1}{2}\|\mathbf{U}\mathbf{V}^\top -\mathbf{\Sigma}\|_F^2\] where
$\mathbf{\Sigma}$ is a given matrix of size $m \times n$ and rank $d$. This is
a canonical problem that admits two difficulties in optimization: 1)
non-convexity and 2) non-smoothness (due to unbalancedness of $\mathbf{U}$ and
$\mathbf{V}$). This is also a prototype for more complex problems such as
asymmetric matrix sensing and matrix completion. Despite being non-convex and
non-smooth, it has been observed empirically that the randomly initialized
gradient descent algorithm can solve this problem in polynomial time. Existing
theories to explain this phenomenon all require artificial modifications of the
algorithm, such as adding noise in each iteration and adding a balancing
regularizer to balance the $\mathbf{U}$ and $\mathbf{V}$.

This paper presents the first proof that shows randomly initialized gradient
descent converges to a global minimum of the asymmetric low-rank factorization
problem with a polynomial rate. For the proof, we develop 1) a new
symmetrization technique to capture the magnitudes of the symmetry and
asymmetry, and 2) a quantitative perturbation analysis to approximate matrix
derivatives. We believe both are useful for other related non-convex problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ye_T/0/1/0/all/0/1"&gt;Tian Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Three-dimensional Radial Visualization. (arXiv:1904.06366v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.06366</id>
        <link href="http://arxiv.org/abs/1904.06366"/>
        <updated>2021-06-29T01:55:20.539Z</updated>
        <summary type="html"><![CDATA[We develop methodology for three-dimensional (3D) radial visualization
(RadViz) of multidimensional datasets. Our tool is called RadViz3D and extends
the classical two-dimensional (2D) RadViz that visualizes multivariate data in
the 2D plane by mapping every observation to a point inside the unit circle. We
show that distributing anchor points uniformly on the 3D unit sphere provides
the best visualization with minimal artificial visual correlation for data with
uncorrelated variables. However, anchor points can be placed exactly
equi-distant from each other only for the five Platonic solids. We provide
equi-distant anchor points for these five settings, and approximately
equi-distant anchor points via a Fibonacci grid for the other cases. Our
methodology, implemented in the R package $radviz3d$, makes fully 3D RadViz
possible and is shown to improve clarity of this nonlinear display technique on
simulated and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yifan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dai_F/0/1/0/all/0/1"&gt;Fan Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1"&gt;Ranjan Maitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solar Irradiation Forecasting using Genetic Algorithms. (arXiv:2106.13956v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13956</id>
        <link href="http://arxiv.org/abs/2106.13956"/>
        <updated>2021-06-29T01:55:20.533Z</updated>
        <summary type="html"><![CDATA[Renewable energy forecasting is attaining greater importance due to its
constant increase in contribution to the electrical power grids. Solar energy
is one of the most significant contributors to renewable energy and is
dependent on solar irradiation. For the effective management of electrical
power grids, forecasting models that predict solar irradiation, with high
accuracy, are needed. In the current study, Machine Learning techniques such as
Linear Regression, Extreme Gradient Boosting and Genetic Algorithm Optimization
are used to forecast solar irradiation. The data used for training and
validation is recorded from across three different geographical stations in the
United States that are part of the SURFRAD network. A Global Horizontal Index
(GHI) is predicted for the models built and compared. Genetic Algorithm
Optimization is applied to XGB to further improve the accuracy of solar
irradiation prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gunasekaran_V/0/1/0/all/0/1"&gt;V. Gunasekaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovi_K/0/1/0/all/0/1"&gt;K.K. Kovi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arja_S/0/1/0/all/0/1"&gt;S. Arja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chimata_R/0/1/0/all/0/1"&gt;R. Chimata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poisoning the Search Space in Neural Architecture Search. (arXiv:2106.14406v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14406</id>
        <link href="http://arxiv.org/abs/2106.14406"/>
        <updated>2021-06-29T01:55:20.498Z</updated>
        <summary type="html"><![CDATA[Deep learning has proven to be a highly effective problem-solving tool for
object detection and image segmentation across various domains such as
healthcare and autonomous driving. At the heart of this performance lies neural
architecture design which relies heavily on domain knowledge and prior
experience on the researchers' behalf. More recently, this process of finding
the most optimal architectures, given an initial search space of possible
operations, was automated by Neural Architecture Search (NAS). In this paper,
we evaluate the robustness of one such algorithm known as Efficient NAS (ENAS)
against data agnostic poisoning attacks on the original search space with
carefully designed ineffective operations. By evaluating algorithm performance
on the CIFAR-10 dataset, we empirically demonstrate how our novel search space
poisoning (SSP) approach and multiple-instance poisoning attacks exploit design
flaws in the ENAS controller to result in inflated prediction error rates for
child networks. Our results provide insights into the challenges to surmount in
using NAS for more adversarially robust architecture search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Robert Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saxena_N/0/1/0/all/0/1"&gt;Nayan Saxena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1"&gt;Rohan Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capturing Dynamics of Time-Varying Data via Topology. (arXiv:2010.05780v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05780</id>
        <link href="http://arxiv.org/abs/2010.05780"/>
        <updated>2021-06-29T01:55:18.547Z</updated>
        <summary type="html"><![CDATA[One approach to understanding complex data is to study its shape through the
lens of algebraic topology. While the early development of topological data
analysis focused primarily on static data, in recent years, theoretical and
applied studies have turned to data that varies in time. A time-varying
collection of metric spaces as formed, for example, by a moving school of fish
or flock of birds, can contain a vast amount of information. There is often a
need to simplify or summarize the dynamic behavior. We provide an introduction
to topological summaries of time-varying metric spaces including vineyards
[19], crocker plots [56], and multiparameter rank functions [37]. We then
introduce a new tool to summarize time-varying metric spaces: a crocker stack.
Crocker stacks are convenient for visualization, amenable to machine learning,
and satisfy a desirable continuity property which we prove. We demonstrate the
utility of crocker stacks for a parameter identification task involving an
influential model of biological aggregations [58]. Altogether, we aim to bring
the broader applied mathematics community up-to-date on topological summaries
of time-varying metric spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xian_L/0/1/0/all/0/1"&gt;Lu Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adams_H/0/1/0/all/0/1"&gt;Henry Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topaz_C/0/1/0/all/0/1"&gt;Chad M. Topaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziegelmeier_L/0/1/0/all/0/1"&gt;Lori Ziegelmeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic. (arXiv:2106.14190v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14190</id>
        <link href="http://arxiv.org/abs/2106.14190"/>
        <updated>2021-06-29T01:55:18.531Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and
ResNeXt-56 are severely over-parameterized, necessitating a consequent increase
in the computational resources required for model training which scales
exponentially for increments in model depth. In this paper, we propose an
Entropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust
and simple, yet effective in resolving the problem of over-parameterization
with regards to network depth of CNN model. The EBCLE heuristic employs a
priori knowledge of the entropic data distribution of input datasets to
determine an upper bound for convolutional network depth, beyond which identity
transformations are prevalent offering insignificant contributions for
enhancing model performance. Restricting depth redundancies by forcing feature
compression and abstraction restricts over-parameterization while decreasing
training time by 24.99% - 78.59% without degradation in model performance. We
present empirical evidence to emphasize the relative effectiveness of broader,
yet shallower models trained using the EBCLE heuristic, which maintains or
outperforms baseline classification accuracies of narrower yet deeper models.
The EBCLE heuristic is architecturally agnostic and EBCLE based CNN models
restrict depth redundancies resulting in enhanced utilization of the available
computational resources. The proposed EBCLE heuristic is a compelling technique
for researchers to analytically justify their HyperParameter (HP) choices for
CNNs. Empirical validation of the EBCLE heuristic in training CNN models was
established on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10,
MNIST) and four network architectures (DenseNet, ResNet, ResNeXt and
EfficientNet B0-B2) with appropriate statistical tests employed to infer any
conclusive claims presented in this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gowdra_N/0/1/0/all/0/1"&gt;Nidhi Gowdra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1"&gt;Roopak Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MacDonell_S/0/1/0/all/0/1"&gt;Stephen MacDonell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1"&gt;Wei Qi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quasiconformal model with CNN features for large deformation image registration. (arXiv:2011.00731v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00731</id>
        <link href="http://arxiv.org/abs/2011.00731"/>
        <updated>2021-06-29T01:55:18.517Z</updated>
        <summary type="html"><![CDATA[Image registration has been widely studied over the past several decades,
with numerous applications in science, engineering and medicine. Most of the
conventional mathematical models for large deformation image registration rely
on prescribed landmarks, which usually require tedious manual labeling and are
prone to error. In recent years, there has been a surge of interest in the use
of machine learning for image registration. In this paper, we develop a novel
method for large deformation image registration by a fusion of quasiconformal
theory and convolutional neural network (CNN). More specifically, we propose a
quasiconformal energy model with a novel fidelity term that incorporates the
features extracted using a pre-trained CNN, thereby allowing us to obtain
meaningful registration results without any guidance of prescribed landmarks.
Moreover, unlike many prior image registration methods, the bijectivity of our
method is guaranteed by quasiconformal theory. Experimental results are
presented to demonstrate the effectiveness of the proposed method. More
broadly, our work sheds light on how rigorous mathematical theories and
practical machine learning approaches can be integrated for developing
computational methods with improved performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Law_H/0/1/0/all/0/1"&gt;Ho Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1"&gt;Gary P. T. Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Ka Chun Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1"&gt;Lok Ming Lui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building population models for large-scale neural recordings: opportunities and pitfalls. (arXiv:2102.01807v3 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01807</id>
        <link href="http://arxiv.org/abs/2102.01807"/>
        <updated>2021-06-29T01:55:18.510Z</updated>
        <summary type="html"><![CDATA[Modern recording technologies now enable simultaneous recording from large
numbers of neurons. This has driven the development of new statistical models
for analyzing and interpreting neural population activity. Here we provide a
broad overview of recent developments in this area. We compare and contrast
different approaches, highlight strengths and limitations, and discuss
biological and mechanistic insights that these methods provide.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Hurwitz_C/0/1/0/all/0/1"&gt;Cole Hurwitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kudryashova_N/0/1/0/all/0/1"&gt;Nina Kudryashova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Onken_A/0/1/0/all/0/1"&gt;Arno Onken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hennig_M/0/1/0/all/0/1"&gt;Matthias H. Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reward-Based 1-bit Compressed Federated Distillation on Blockchain. (arXiv:2106.14265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14265</id>
        <link href="http://arxiv.org/abs/2106.14265"/>
        <updated>2021-06-29T01:55:18.492Z</updated>
        <summary type="html"><![CDATA[The recent advent of various forms of Federated Knowledge Distillation (FD)
paves the way for a new generation of robust and communication-efficient
Federated Learning (FL), where mere soft-labels are aggregated, rather than
whole gradients of Deep Neural Networks (DNN) as done in previous FL schemes.
This security-per-design approach in combination with increasingly performant
Internet of Things (IoT) and mobile devices opens up a new realm of
possibilities to utilize private data from industries as well as from
individuals as input for artificial intelligence model training. Yet in
previous FL systems, lack of trust due to the imbalance of power between
workers and a central authority, the assumption of altruistic worker
participation and the inability to correctly measure and compare contributions
of workers hinder this technology from scaling beyond small groups of already
entrusted entities towards mass adoption. This work aims to mitigate the
aforementioned issues by introducing a novel decentralized federated learning
framework where heavily compressed 1-bit soft-labels, resembling 1-hot label
predictions, are aggregated on a smart contract. In a context where workers'
contributions are now easily comparable, we modify the Peer Truth Serum for
Crowdsourcing mechanism (PTSC) for FD to reward honest participation based on
peer consistency in an incentive compatible fashion. Due to heavy reductions of
both computational complexity and storage, our framework is a fully
on-blockchain FL system that is feasible on simple smart contracts and
therefore blockchain agnostic. We experimentally test our new framework and
validate its theoretical properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Witt_L/0/1/0/all/0/1"&gt;Leon Witt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafar_U/0/1/0/all/0/1"&gt;Usama Zafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1"&gt;KuoYeh Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sattler_F/0/1/0/all/0/1"&gt;Felix Sattler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use of Variational Inference in Music Emotion Recognition. (arXiv:2106.14323v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.14323</id>
        <link href="http://arxiv.org/abs/2106.14323"/>
        <updated>2021-06-29T01:55:18.485Z</updated>
        <summary type="html"><![CDATA[This work was developed aiming to employ Statistical techniques to the field
of Music Emotion Recognition, a well-recognized area within the Signal
Processing world, but hardly explored from the statistical point of view. Here,
we opened several possibilities within the field, applying modern Bayesian
Statistics techniques and developing efficient algorithms, focusing on the
applicability of the results obtained. Although the motivation for this project
was the development of a emotion-based music recommendation system, its main
contribution is a highly adaptable multivariate model that can be useful
interpreting any database where there is an interest in applying regularization
in an efficient manner. Broadly speaking, we will explore what role a sound
theoretical statistical analysis can play in the modeling of an algorithm that
is able to understand a well-known database and what can be gained with this
kind of approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Deziderio_N/0/1/0/all/0/1"&gt;Nathalie Deziderio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carvalho_H/0/1/0/all/0/1"&gt;Hugo Tremonte de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to solve geometric construction problems from images. (arXiv:2106.14195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14195</id>
        <link href="http://arxiv.org/abs/2106.14195"/>
        <updated>2021-06-29T01:55:18.473Z</updated>
        <summary type="html"><![CDATA[We describe a purely image-based method for finding geometric constructions
with a ruler and compass in the Euclidea geometric game. The method is based on
adapting the Mask R-CNN state-of-the-art image processing neural architecture
and adding a tree-based search procedure to it. In a supervised setting, the
method learns to solve all 68 kinds of geometric construction problems from the
first six level packs of Euclidea with an average 92% accuracy. When evaluated
on new kinds of problems, the method can solve 31 of the 68 kinds of Euclidea
problems. We believe that this is the first time that a purely image-based
learning has been trained to solve geometric construction problems of this
difficulty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1"&gt;J. Macke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sedlar_J/0/1/0/all/0/1"&gt;J. Sedlar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1"&gt;M. Olsak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1"&gt;J. Urban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1"&gt;J. Sivic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Skill Discovery with Bottleneck Option Learning. (arXiv:2106.14305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14305</id>
        <link href="http://arxiv.org/abs/2106.14305"/>
        <updated>2021-06-29T01:55:18.467Z</updated>
        <summary type="html"><![CDATA[Having the ability to acquire inherent skills from environments without any
external rewards or supervision like humans is an important problem. We propose
a novel unsupervised skill discovery method named Information Bottleneck Option
Learning (IBOL). On top of the linearization of environments that promotes more
various and distant state transitions, IBOL enables the discovery of diverse
skills. It provides the abstraction of the skills learned with the information
bottleneck framework for the options with improved stability and encouraged
disentanglement. We empirically demonstrate that IBOL outperforms multiple
state-of-the-art unsupervised skill discovery methods on the
information-theoretic evaluations and downstream tasks in MuJoCo environments,
including Ant, HalfCheetah, Hopper and D'Kitty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jaekyeom Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seohong Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gunhee Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co$^2$L: Contrastive Continual Learning. (arXiv:2106.14413v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14413</id>
        <link href="http://arxiv.org/abs/2106.14413"/>
        <updated>2021-06-29T01:55:18.454Z</updated>
        <summary type="html"><![CDATA[Recent breakthroughs in self-supervised learning show that such algorithms
learn visual representations that can be transferred better to unseen tasks
than joint-training methods relying on task-specific supervision. In this
paper, we found that the similar holds in the continual learning con-text:
contrastively learned representations are more robust against the catastrophic
forgetting than jointly trained representations. Based on this novel
observation, we propose a rehearsal-based continual learning algorithm that
focuses on continually learning and maintaining transferable representations.
More specifically, the proposed scheme (1) learns representations using the
contrastive learning objective, and (2) preserves learned representations using
a self-supervised distillation step. We conduct extensive experimental
validations under popular benchmark image classification datasets, where our
method sets the new state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_H/0/1/0/all/0/1"&gt;Hyuntak Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jaeho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How many moments does MMD compare?. (arXiv:2106.14277v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14277</id>
        <link href="http://arxiv.org/abs/2106.14277"/>
        <updated>2021-06-29T01:55:18.447Z</updated>
        <summary type="html"><![CDATA[We present a new way of study of Mercer kernels, by corresponding to a
special kernel $K$ a pseudo-differential operator $p({\mathbf x}, D)$ such that
$\mathcal{F} p({\mathbf x}, D)^\dag p({\mathbf x}, D) \mathcal{F}^{-1}$ acts on
smooth functions in the same way as an integral operator associated with $K$
(where $\mathcal{F}$ is the Fourier transform). We show that kernels defined by
pseudo-differential operators are able to approximate uniformly any continuous
Mercer kernel on a compact set.

The symbol $p({\mathbf x}, {\mathbf y})$ encapsulates a lot of useful
information about the structure of the Maximum Mean Discrepancy distance
defined by the kernel $K$. We approximate $p({\mathbf x}, {\mathbf y})$ with
the sum of the first $r$ terms of the Singular Value Decomposition of $p$,
denoted by $p_r({\mathbf x}, {\mathbf y})$. If ordered singular values of the
integral operator associated with $p({\mathbf x}, {\mathbf y})$ die down
rapidly, the MMD distance defined by the new symbol $p_r$ differs from the
initial one only slightly. Moreover, the new MMD distance can be interpreted as
an aggregated result of comparing $r$ local moments of two probability
distributions.

The latter results holds under the condition that right singular vectors of
the integral operator associated with $p$ are uniformly bounded. But even if
this is not satisfied we can still hold that the Hilbert-Schmidt distance
between $p$ and $p_r$ vanishes. Thus, we report an interesting phenomenon: the
MMD distance measures the difference of two probability distributions with
respect to a certain number of local moments, $r^\ast$, and this number
$r^\ast$ depends on the speed with which singular values of $p$ die down.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takhanov_R/0/1/0/all/0/1"&gt;Rustem Takhanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error bound of critical points and KL property of exponent $1/2$ for squared F-norm regularized factorization. (arXiv:1911.04293v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.04293</id>
        <link href="http://arxiv.org/abs/1911.04293"/>
        <updated>2021-06-29T01:55:18.442Z</updated>
        <summary type="html"><![CDATA[This paper is concerned with the squared F(robenius)-norm regularized
factorization form for noisy low-rank matrix recovery problems. Under a
suitable assumption on the restricted condition number of the Hessian for the
loss function, we derive an error bound to the true matrix for the non-strict
critical points with rank not more than that of the true matrix. Then, for the
squared F-norm regularized factorized least squares loss function, under the
noisy and full sample setting we establish its KL property of exponent $1/2$ on
its global minimizer set, and under the noisy and partial sample setting
achieve this property for a class of critical points. These theoretical
findings are also confirmed by solving the squared F-norm regularized
factorization problem with an accelerated alternating minimization method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Tao_T/0/1/0/all/0/1"&gt;Ting Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shaohua Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bi_S/0/1/0/all/0/1"&gt;Shujun Bi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LRG at SemEval-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting. (arXiv:2102.12255v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12255</id>
        <link href="http://arxiv.org/abs/2102.12255"/>
        <updated>2021-06-29T01:55:18.392Z</updated>
        <summary type="html"><![CDATA[In this article, we present our methodologies for SemEval-2021 Task-4:
Reading Comprehension of Abstract Meaning. Given a fill-in-the-blank-type
question and a corresponding context, the task is to predict the most suitable
word from a list of 5 options. There are three sub-tasks within this task:
Imperceptibility (subtask-I), Non-Specificity (subtask-II), and Intersection
(subtask-III). We use encoders of transformers-based models pre-trained on the
masked language modelling (MLM) task to build our Fill-in-the-blank (FitB)
models. Moreover, to model imperceptibility, we define certain linguistic
features, and to model non-specificity, we leverage information from hypernyms
and hyponyms provided by a lexical database. Specifically, for non-specificity,
we try out augmentation techniques, and other statistical techniques. We also
propose variants, namely Chunk Voting and Max Context, to take care of input
length restrictions for BERT, etc. Additionally, we perform a thorough ablation
study, and use Integrated Gradients to explain our predictions on a few
samples. Our best submissions achieve accuracies of 75.31% and 77.84%, on the
test sets for subtask-I and subtask-II, respectively. For subtask-III, we
achieve accuracies of 65.64% and 62.27%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1"&gt;Harshit Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1"&gt;Gunjan Chhablani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1"&gt;Yash Bhartia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1"&gt;Tirtharaj Dash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails. (arXiv:2106.14343v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14343</id>
        <link href="http://arxiv.org/abs/2106.14343"/>
        <updated>2021-06-29T01:55:18.379Z</updated>
        <summary type="html"><![CDATA[We consider non-convex stochastic optimization using first-order algorithms
for which the gradient estimates may have heavy tails. We show that a
combination of gradient clipping, momentum, and normalized gradient descent
yields convergence to critical points in high-probability with best-known rates
for smooth losses when the gradients only have bounded $\mathfrak{p}$th moments
for some $\mathfrak{p}\in(1,2]$. We then consider the case of second-order
smooth losses, which to our knowledge have not been studied in this setting,
and again obtain high-probability bounds for any $\mathfrak{p}$. Moreover, our
results hold for arbitrary smooth norms, in contrast to the typical SGD
analysis which requires a Hilbert space norm. Further, we show that after a
suitable "burn-in" period, the objective value will monotonically decrease for
every iteration until a critical point is identified, which provides intuition
behind the popular practice of learning rate "warm-up" and also yields a
last-iterate guarantee.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1"&gt;Ashok Cutkosky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1"&gt;Harsh Mehta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Nash Equilibria in Zero-Sum Stochastic Games via Entropy-Regularized Policy Approximation. (arXiv:2009.00162v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00162</id>
        <link href="http://arxiv.org/abs/2009.00162"/>
        <updated>2021-06-29T01:55:18.373Z</updated>
        <summary type="html"><![CDATA[We explore the use of policy approximations to reduce the computational cost
of learning Nash equilibria in zero-sum stochastic games. We propose a new
Q-learning type algorithm that uses a sequence of entropy-regularized soft
policies to approximate the Nash policy during the Q-function updates. We prove
that under certain conditions, by updating the regularized Q-function, the
algorithm converges to a Nash equilibrium. We also demonstrate the proposed
algorithm's ability to transfer previous training experiences, enabling the
agents to adapt quickly to new environments. We provide a dynamic
hyper-parameter scheduling scheme to further expedite convergence. Empirical
results applied to a number of stochastic games verify that the proposed
algorithm converges to the Nash equilibrium, while exhibiting a major speed-up
over existing algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yue Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qifan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsiotras_P/0/1/0/all/0/1"&gt;Panagiotis Tsiotras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASK: Adversarial Soft k-Nearest Neighbor Attack and Defense. (arXiv:2106.14300v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14300</id>
        <link href="http://arxiv.org/abs/2106.14300"/>
        <updated>2021-06-29T01:55:18.367Z</updated>
        <summary type="html"><![CDATA[K-Nearest Neighbor (kNN)-based deep learning methods have been applied to
many applications due to their simplicity and geometric interpretability.
However, the robustness of kNN-based classification models has not been
thoroughly explored and kNN attack strategies are underdeveloped. In this
paper, we propose an Adversarial Soft kNN (ASK) loss to both design more
effective kNN attack strategies and to develop better defenses against them.
Our ASK loss approach has two advantages. First, ASK loss can better
approximate the kNN's probability of classification error than objectives
proposed in previous works. Second, the ASK loss is interpretable: it preserves
the mutual information between the perturbed input and the kNN of the
unperturbed input. We use the ASK loss to generate a novel attack method called
the ASK-Attack (ASK-Atk), which shows superior attack efficiency and accuracy
degradation relative to previous kNN attacks. Based on the ASK-Atk, we then
derive an ASK-Defense (ASK-Def) method that optimizes the worst-case training
loss induced by ASK-Atk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ren Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_P/0/1/0/all/0/1"&gt;Philip Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1"&gt;Indika Rajapakse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1"&gt;Alfred Hero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confounder-Aware Visualization of ConvNets. (arXiv:1907.12727v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.12727</id>
        <link href="http://arxiv.org/abs/1907.12727"/>
        <updated>2021-06-29T01:55:18.353Z</updated>
        <summary type="html"><![CDATA[With recent advances in deep learning, neuroimaging studies increasingly rely
on convolutional networks (ConvNets) to predict diagnosis based on MR images.
To gain a better understanding of how a disease impacts the brain, the studies
visualize the salience maps of the ConvNet highlighting voxels within the brain
majorly contributing to the prediction. However, these salience maps are
generally confounded, i.e., some salient regions are more predictive of
confounding variables (such as age) than the diagnosis. To avoid such
misinterpretation, we propose in this paper an approach that aims to visualize
confounder-free saliency maps that only highlight voxels predictive of the
diagnosis. The approach incorporates univariate statistical tests to identify
confounding effects within the intermediate features learned by ConvNet. The
influence from the subset of confounded features is then removed by a novel
partial back-propagation procedure. We use this two-step approach to visualize
confounder-free saliency maps extracted from synthetic and two real datasets.
These experiments reveal the potential of our visualization in producing
unbiased model-interpretation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1"&gt;Adolf Pfefferbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1"&gt;Edith V. Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Image Recognition for Non-images. (arXiv:2106.14350v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14350</id>
        <link href="http://arxiv.org/abs/2106.14350"/>
        <updated>2021-06-29T01:55:18.348Z</updated>
        <summary type="html"><![CDATA[Powerful deep learning algorithms open an opportunity for solving non-image
Machine Learning (ML) problems by transforming these problems to into the image
recognition problems. The CPC-R algorithm presented in this chapter converts
non-image data into images by visualizing non-image data. Then deep learning
CNN algorithms solve the learning problems on these images. The design of the
CPC-R algorithm allows preserving all high-dimensional information in 2-D
images. The use of pair values mapping instead of single value mapping used in
the alternative approaches allows encoding each n-D point with 2 times fewer
visual elements. The attributes of an n-D point are divided into pairs of its
values and each pair is visualized as 2-D points in the same 2-D Cartesian
coordinates. Next, grey scale or color intensity values are assigned to each
pair to encode the order of pairs. This is resulted in the heatmap image. The
computational experiments with CPC-R are conducted for different CNN
architectures, and methods to optimize the CPC-R images showing that the
combined CPC-R and deep learning CNN algorithms are able to solve non-image ML
problems reaching high accuracy on the benchmark datasets. This chapter expands
our prior work by adding more experiments to test accuracy of classification,
exploring saliency and informativeness of discovered features to test their
interpretability, and generalizing the approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1"&gt;Boris Kovalerchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalla_D/0/1/0/all/0/1"&gt;Divya Chandrika Kalla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1"&gt;Bedant Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Reinforcement Learning Approach for Sequential Spatial Transformer Networks. (arXiv:2106.14295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14295</id>
        <link href="http://arxiv.org/abs/2106.14295"/>
        <updated>2021-06-29T01:55:18.338Z</updated>
        <summary type="html"><![CDATA[Spatial Transformer Networks (STN) can generate geometric transformations
which modify input images to improve the classifier's performance. In this
work, we combine the idea of STN with Reinforcement Learning (RL). To this end,
we break the affine transformation down into a sequence of simple and discrete
transformations. We formulate the task as a Markovian Decision Process (MDP)
and use RL to solve this sequential decision-making problem. STN architectures
learn the transformation parameters by minimizing the classification error and
backpropagating the gradients through a sub-differentiable sampling module. In
our method, we are not bound to the differentiability of the sampling modules.
Moreover, we have freedom in designing the objective rather than only
minimizing the error; e.g., we can directly set the target as maximizing the
accuracy. We design multiple experiments to verify the effectiveness of our
method using cluttered MNIST and Fashion-MNIST datasets and show that our
method outperforms STN with a proper definition of MDP components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azimi_F/0/1/0/all/0/1"&gt;Fatemeh Azimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1"&gt;Federico Raue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1"&gt;Joern Hees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse recovery by reduced variance stochastic approximation. (arXiv:2006.06365v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06365</id>
        <link href="http://arxiv.org/abs/2006.06365"/>
        <updated>2021-06-29T01:55:18.333Z</updated>
        <summary type="html"><![CDATA[In this paper, we discuss application of iterative Stochastic Optimization
routines to the problem of sparse signal recovery from noisy observation. Using
Stochastic Mirror Descent algorithm as a building block, we develop a
multistage procedure for recovery of sparse solutions to Stochastic
Optimization problem under assumption of smoothness and quadratic minoration on
the expected objective. An interesting feature of the proposed algorithm is
linear convergence of the approximate solution during the preliminary phase of
the routine when the component of stochastic error in the gradient observation
which is due to bad initial approximation of the optimal solution is larger
than the "ideal" asymptotic error component owing to observation noise "at the
optimal solution." We also show how one can straightforwardly enhance
reliability of the corresponding solution by using Median-of-Means like
techniques.

We illustrate the performance of the proposed algorithms in application to
classical problems of recovery of sparse and low rank signals in linear
regression framework. We show, under rather weak assumption on the regressor
and noise distributions, how they lead to parameter estimates which obey (up to
factors which are logarithmic in problem dimension and confidence level) the
best known to us accuracy bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Juditsky_A/0/1/0/all/0/1"&gt;Anatoli Juditsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kulunchakov_A/0/1/0/all/0/1"&gt;Andrei Kulunchakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tsyntseus_H/0/1/0/all/0/1"&gt;Hlib Tsyntseus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-stage framework for short-term wind power forecasting using different feature-learning models. (arXiv:2006.00413v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.00413</id>
        <link href="http://arxiv.org/abs/2006.00413"/>
        <updated>2021-06-29T01:55:18.318Z</updated>
        <summary type="html"><![CDATA[Two-stage ensemble-based forecasting methods have been studied extensively in
the wind power forecasting field. However, deep learning-based wind power
forecasting studies have not investigated two aspects. In the first stage,
different learning structures considering multiple inputs and multiple outputs
have not been discussed. In the second stage, the model extrapolation issue has
not been investigated. Therefore, we develop four deep neural networks for the
first stage to learn data features considering the input-and-output structure.
We then explore the model extrapolation issue in the second stage using
different modeling methods. Considering the overfitting issue, we propose a new
moving window-based algorithm using a validation set in the first stage to
update the training data in both stages with two different moving window
processes.Experiments were conducted at three wind farms, and the results
demonstrate that the model with single input multiple output structure obtains
better forecasting accuracy compared to existing models. In addition, the ridge
regression method results in a better ensemble model that can further improve
forecasting accuracy compared to existing machine learning methods. Finally,
the proposed two-stage forecasting algorithm can generate more accurate and
stable results than existing algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jiancheng Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hua Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models. (arXiv:2102.08239v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08239</id>
        <link href="http://arxiv.org/abs/2102.08239"/>
        <updated>2021-06-29T01:55:18.305Z</updated>
        <summary type="html"><![CDATA[Interpretability is a critical factor in applying complex deep learning
models to advance the understanding of brain disorders in neuroimaging studies.
To interpret the decision process of a trained classifier, existing techniques
typically rely on saliency maps to quantify the voxel-wise or feature-level
importance for classification through partial derivatives. Despite providing
some level of localization, these maps are not human-understandable from the
neuroscience perspective as they do not inform the specific meaning of the
alteration linked to the brain disorder. Inspired by the image-to-image
translation scheme, we propose to train simulator networks that can warp a
given image to inject or remove patterns of the disease. These networks are
trained such that the classifier produces consistently increased or decreased
prediction logits for the simulated images. Moreover, we propose to couple all
the simulators into a unified model based on conditional convolution. We
applied our approach to interpreting classifiers trained on a synthetic dataset
and two neuroimaging datasets to visualize the effect of the Alzheimer's
disease and alcohol use disorder. Compared to the saliency maps generated by
baseline approaches, our simulations and visualizations based on the Jacobian
determinants of the warping field reveal meaningful and understandable patterns
related to the diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zixuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent. (arXiv:2009.06107v3 [cs.CC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06107</id>
        <link href="http://arxiv.org/abs/2009.06107"/>
        <updated>2021-06-29T01:55:18.298Z</updated>
        <summary type="html"><![CDATA[Researchers currently use a number of approaches to predict and substantiate
information-computation gaps in high-dimensional statistical estimation
problems. A prominent approach is to characterize the limits of restricted
models of computation, which on the one hand yields strong computational lower
bounds for powerful classes of algorithms and on the other hand helps guide the
development of efficient algorithms. In this paper, we study two of the most
popular restricted computational models, the statistical query framework and
low-degree polynomials, in the context of high-dimensional hypothesis testing.
Our main result is that under mild conditions on the testing problem, the two
classes of algorithms are essentially equivalent in power. As corollaries, we
obtain new statistical query lower bounds for sparse PCA, tensor PCA and
several variants of the planted clique problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brennan_M/0/1/0/all/0/1"&gt;Matthew Brennan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bresler_G/0/1/0/all/0/1"&gt;Guy Bresler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hopkins_S/0/1/0/all/0/1"&gt;Samuel B. Hopkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schramm_T/0/1/0/all/0/1"&gt;Tselil Schramm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret Analysis in Deterministic Reinforcement Learning. (arXiv:2106.14338v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14338</id>
        <link href="http://arxiv.org/abs/2106.14338"/>
        <updated>2021-06-29T01:55:18.023Z</updated>
        <summary type="html"><![CDATA[We consider Markov Decision Processes (MDPs) with deterministic transitions
and study the problem of regret minimization, which is central to the analysis
and design of optimal learning algorithms. We present logarithmic
problem-specific regret lower bounds that explicitly depend on the system
parameter (in contrast to previous minimax approaches) and thus, truly quantify
the fundamental limit of performance achievable by any learning algorithm.
Deterministic MDPs can be interpreted as graphs and analyzed in terms of their
cycles, a fact which we leverage in order to identify a class of deterministic
MDPs whose regret lower bound can be determined numerically. We further
exemplify this result on a deterministic line search problem, and a
deterministic MDP with state-dependent rewards, whose regret lower bounds we
can state explicitly. These bounds share similarities with the known
problem-specific bound of the multi-armed bandit problem and suggest that
navigation on a deterministic MDP need not have an effect on the performance of
a learning algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tranos_D/0/1/0/all/0/1"&gt;Damianos Tranos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Proutiere_A/0/1/0/all/0/1"&gt;Alexandre Proutiere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Precision Training in Logarithmic Number System using Multiplicative Weight Update. (arXiv:2106.13914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13914</id>
        <link href="http://arxiv.org/abs/2106.13914"/>
        <updated>2021-06-29T01:55:18.010Z</updated>
        <summary type="html"><![CDATA[Training large-scale deep neural networks (DNNs) currently requires a
significant amount of energy, leading to serious environmental impacts. One
promising approach to reduce the energy costs is representing DNNs with
low-precision numbers. While it is common to train DNNs with forward and
backward propagation in low-precision, training directly over low-precision
weights, without keeping a copy of weights in high-precision, still remains to
be an unsolved problem. This is due to complex interactions between learning
algorithms and low-precision number systems. To address this, we jointly design
a low-precision training framework involving a logarithmic number system (LNS)
and a multiplicative weight update training method, termed LNS-Madam. LNS has a
high dynamic range even in a low-bitwidth setting, leading to high energy
efficiency and making it relevant for on-board training in energy-constrained
edge devices. We design LNS to have the flexibility of choosing different bases
for weights and gradients, as they usually require different quantization gaps
and dynamic ranges during training. By drawing the connection between LNS and
multiplicative update, LNS-Madam ensures low quantization error during weight
update, leading to a stable convergence even if the bitwidth is limited.
Compared to using a fixed-point or floating-point number system and training
with popular learning algorithms such as SGD and Adam, our joint design with
LNS and LNS-Madam optimizer achieves better accuracy while requiring smaller
bitwidth. Notably, with only 5-bit for gradients, the proposed training
framework achieves accuracy comparable to full-precision state-of-the-art
models such as ResNet-50 and BERT. After conducting energy estimations by
analyzing the math datapath units during training, the results show that our
design achieves over 60x energy reduction compared to FP32 on BERT models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jiawei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1"&gt;Steve Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesan_R/0/1/0/all/0/1"&gt;Rangharajan Venkatesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming-Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1"&gt;Brucek Khailany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dally_B/0/1/0/all/0/1"&gt;Bill Dally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last-iterate Convergence in Extensive-Form Games. (arXiv:2106.14326v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14326</id>
        <link href="http://arxiv.org/abs/2106.14326"/>
        <updated>2021-06-29T01:55:17.976Z</updated>
        <summary type="html"><![CDATA[Regret-based algorithms are highly efficient at finding approximate Nash
equilibria in sequential games such as poker games. However, most regret-based
algorithms, including counterfactual regret minimization (CFR) and its
variants, rely on iterate averaging to achieve convergence. Inspired by recent
advances on last-iterate convergence of optimistic algorithms in zero-sum
normal-form games, we study this phenomenon in sequential games, and provide a
comprehensive study of last-iterate convergence for zero-sum extensive-form
games with perfect recall (EFGs), using various optimistic regret-minimization
algorithms over treeplexes. This includes algorithms using the vanilla entropy
or squared Euclidean norm regularizers, as well as their dilated versions which
admit more efficient implementation. In contrast to CFR, we show that all of
these algorithms enjoy last-iterate convergence, with some of them even
converging exponentially fast. We also provide experiments to further support
our theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chung-Wei Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kroer_C/0/1/0/all/0/1"&gt;Christian Kroer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Generation of Continuous Images. (arXiv:2011.12026v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12026</id>
        <link href="http://arxiv.org/abs/2011.12026"/>
        <updated>2021-06-29T01:55:17.967Z</updated>
        <summary type="html"><![CDATA[In most existing learning systems, images are typically viewed as 2D pixel
arrays. However, in another paradigm gaining popularity, a 2D image is
represented as an implicit neural representation (INR) - an MLP that predicts
an RGB pixel value given its (x,y) coordinate. In this paper, we propose two
novel architectural techniques for building INR-based image decoders:
factorized multiplicative modulation and multi-scale INRs, and use them to
build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs
for image generation were limited to MNIST-like datasets and do not scale to
complex real-world data. Our proposed INR-GAN architecture improves the
performance of continuous image generators by several times, greatly reducing
the gap between continuous image GANs and pixel-based ones. Apart from that, we
explore several exciting properties of the INR-based decoders, like
out-of-the-box superresolution, meaningful image-space interpolation,
accelerated inference of low-resolution images, an ability to extrapolate
outside of image boundaries, and strong geometric prior. The project page is
located at https://universome.github.io/inr-gan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1"&gt;Ivan Skorokhodov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ignatyev_S/0/1/0/all/0/1"&gt;Savva Ignatyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1"&gt;Mohamed Elhoseiny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On a novel training algorithm for sequence-to-sequence predictive recurrent networks. (arXiv:2106.14120v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14120</id>
        <link href="http://arxiv.org/abs/2106.14120"/>
        <updated>2021-06-29T01:55:17.961Z</updated>
        <summary type="html"><![CDATA[Neural networks mapping sequences to sequences (seq2seq) lead to significant
progress in machine translation and speech recognition. Their traditional
architecture includes two recurrent networks (RNs) followed by a linear
predictor. In this manuscript we perform analysis of a corresponding algorithm
and show that the parameters of the RNs of the well trained predictive network
are not independent of each other. Their dependence can be used to
significantly improve the network effectiveness. The traditional seq2seq
algorithms require short term memory of a size proportional to the predicted
sequence length. This requirement is quite difficult to implement in a
neuroscience context. We present a novel memoryless algorithm for seq2seq
predictive networks and compare it to the traditional one in the context of
time series prediction. We show that the new algorithm is more robust and makes
predictions with higher accuracy than the traditional one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Boris Rubinstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating topic modeling and word embedding to characterize violent deaths. (arXiv:2106.14365v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14365</id>
        <link href="http://arxiv.org/abs/2106.14365"/>
        <updated>2021-06-29T01:55:17.956Z</updated>
        <summary type="html"><![CDATA[There is an escalating need for methods to identify latent patterns in text
data from many domains. We introduce a new method to identify topics in a
corpus and represent documents as topic sequences. Discourse Atom Topic
Modeling draws on advances in theoretical machine learning to integrate topic
modeling and word embedding, capitalizing on the distinct capabilities of each.
We first identify a set of vectors ("discourse atoms") that provide a sparse
representation of an embedding space. Atom vectors can be interpreted as latent
topics: Through a generative model, atoms map onto distributions over words;
one can also infer the topic that generated a sequence of words. We illustrate
our method with a prominent example of underutilized text: the U.S. National
Violent Death Reporting System (NVDRS). The NVDRS summarizes violent death
incidents with structured variables and unstructured narratives. We identify
225 latent topics in the narratives (e.g., preparation for death and physical
aggression); many of these topics are not captured by existing structured
variables. Motivated by known patterns in suicide and homicide by gender, and
recent research on gender biases in semantic space, we identify the gender bias
of our topics (e.g., a topic about pain medication is feminine). We then
compare the gender bias of topics to their prevalence in narratives of female
versus male victims. Results provide a detailed quantitative picture of
reporting about lethal violence and its gendered nature. Our method offers a
flexible and broadly applicable approach to model topics in text data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1"&gt;Alina Arseniev-Koehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cochran_S/0/1/0/all/0/1"&gt;Susan D. Cochran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mays_V/0/1/0/all/0/1"&gt;Vickie M. Mays&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1"&gt;Jacob Gates Foster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representational aspects of depth and conditioning in normalizing flows. (arXiv:2010.01155v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01155</id>
        <link href="http://arxiv.org/abs/2010.01155"/>
        <updated>2021-06-29T01:55:17.950Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are among the most popular paradigms in generative
modeling, especially for images, primarily because we can efficiently evaluate
the likelihood of a data point. This is desirable both for evaluating the fit
of a model, and for ease of training, as maximizing the likelihood can be done
by gradient descent. However, training normalizing flows comes with
difficulties as well: models which produce good samples typically need to be
extremely deep -- which comes with accompanying vanishing/exploding gradient
problems. A very related problem is that they are often poorly conditioned:
since they are parametrized as invertible maps from $\mathbb{R}^d \to
\mathbb{R}^d$, and typical training data like images intuitively is
lower-dimensional, the learned maps often have Jacobians that are close to
being singular.

In our paper, we tackle representational aspects around depth and
conditioning of normalizing flows: both for general invertible architectures,
and for a particular common architecture, affine couplings. We prove that
$\Theta(1)$ affine coupling layers suffice to exactly represent a permutation
or $1 \times 1$ convolution, as used in GLOW, showing that representationally
the choice of partition is not a bottleneck for depth. We also show that
shallow affine coupling networks are universal approximators in Wasserstein
distance if ill-conditioning is allowed, and experimentally investigate related
phenomena involving padding. Finally, we show a depth lower bound for general
flow architectures with few neurons per layer and bounded Lipschitz constant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1"&gt;Frederic Koehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1"&gt;Viraj Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1"&gt;Andrej Risteski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection for Aggregated Data Using Multi-Graph Autoencoder. (arXiv:2101.04053v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04053</id>
        <link href="http://arxiv.org/abs/2101.04053"/>
        <updated>2021-06-29T01:55:17.935Z</updated>
        <summary type="html"><![CDATA[In data systems, activities or events are continuously collected in the field
to trace their proper executions. Logging, which means recording sequences of
events, can be used for analyzing system failures and malfunctions, and
identifying the causes and locations of such issues. In our research we focus
on creating an Anomaly detection models for system logs. The task of anomaly
detection is identifying unexpected events in dataset, which differ from the
normal behavior. Anomaly detection models also assist in data systems analysis
tasks.

Modern systems may produce such a large amount of events monitoring every
individual event is not feasible. In such cases, the events are often
aggregated over a fixed period of time, reporting the number of times every
event has occurred in that time period. This aggregation facilitates scaling,
but requires a different approach for anomaly detection. In this research, we
present a thorough analysis of the aggregated data and the relationships
between aggregated events. Based on the initial phase of our research we
present graphs representations of our aggregated dataset, which represent the
different relationships between aggregated instances in the same context.

Using the graph representation, we propose Multiple-graphs autoencoder MGAE,
a novel convolutional graphs-autoencoder model which exploits the relationships
of the aggregated instances in our unique dataset. MGAE outperforms standard
graph-autoencoder models and the different experiments. With our novel MGAE we
present 60% decrease in reconstruction error in comparison to standard graph
autoencoder, which is expressed in reconstructing high-degree relationships.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meirman_T/0/1/0/all/0/1"&gt;Tomer Meirman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stern_R/0/1/0/all/0/1"&gt;Roni Stern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1"&gt;Gilad Katz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The mbsts package: Multivariate Bayesian Structural Time Series Models in R. (arXiv:2106.14045v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.14045</id>
        <link href="http://arxiv.org/abs/2106.14045"/>
        <updated>2021-06-29T01:55:17.930Z</updated>
        <summary type="html"><![CDATA[The multivariate Bayesian structural time series (MBSTS) model
\citep{qiu2018multivariate,Jammalamadaka2019Predicting} as a generalized
version of many structural time series models, deals with inference and
prediction for multiple correlated time series, where one also has the choice
of using a different candidate pool of contemporaneous predictors for each
target series. The MBSTS model has wide applications and is ideal for feature
selection, time series forecasting, nowcasting, inferring causal impact, and
others. This paper demonstrates how to use the R package \pkg{mbsts} for MBSTS
modeling, establishing a bridge between user-friendly and developer-friendly
functions in package and the corresponding methodology. A simulated dataset and
object-oriented functions in the \pkg{mbsts} package are explained in the way
that enables users to flexibly add or deduct some components, as well as to
simplify or complicate some settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ning_N/0/1/0/all/0/1"&gt;Ning Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jinwen Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-target normal behaviour models for wind farm condition monitoring. (arXiv:2012.03074v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03074</id>
        <link href="http://arxiv.org/abs/2012.03074"/>
        <updated>2021-06-29T01:55:17.925Z</updated>
        <summary type="html"><![CDATA[The trend towards larger wind turbines and remote locations of wind farms
fuels the demand for automated condition monitoring strategies that can reduce
the operating cost and avoid unplanned downtime. Normal behaviour modelling has
been introduced to detect anomalous deviations from normal operation based on
the turbine's SCADA data. A growing number of machine learning models of the
normal behaviour of turbine subsystems are being developed by wind farm
managers to this end. However, these models need to be kept track of, be
maintained and require frequent updates. This research explores multi-target
models as a new approach to capturing a wind turbine's normal behaviour. We
present an overview of multi-target regression methods, motivate their
application and benefits in wind turbine condition monitoring, and assess their
performance in a wind farm case study. We find that multi-target models are
advantageous in comparison to single-target modelling in that they can reduce
the cost and effort of practical condition monitoring without compromising on
the accuracy. We also outline some areas of future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1"&gt;Angela Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Should at Least Be Able to Design Molecules That Dock Well. (arXiv:2006.16955v4 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16955</id>
        <link href="http://arxiv.org/abs/2006.16955"/>
        <updated>2021-06-29T01:55:17.919Z</updated>
        <summary type="html"><![CDATA[Designing compounds with desired properties is a key element of the drug
discovery process. However, measuring progress in the field has been
challenging due to the lack of realistic retrospective benchmarks, and the
large cost of prospective validation. To close this gap, we propose a benchmark
based on docking, a popular computational method for assessing molecule binding
to a protein. Concretely, the goal is to generate drug-like molecules that are
scored highly by SMINA, a popular docking software. We observe that popular
graph-based generative models fail to generate molecules with a high docking
score when trained using a realistically sized training set. This suggests a
limitation of the current incarnation of models for de novo drug design.
Finally, we propose a simplified version of the benchmark based on a simpler
scoring function, and show that the tested models are able to partially solve
it. We release the benchmark as an easy to use package available at
https://github.com/cieplinski-tobiasz/smina-docking-benchmark. We hope that our
benchmark will serve as a stepping stone towards the goal of automatically
generating promising drug candidates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Cieplinski_T/0/1/0/all/0/1"&gt;Tobiasz Cieplinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Danel_T/0/1/0/all/0/1"&gt;Tomasz Danel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Podlewska_S/0/1/0/all/0/1"&gt;Sabina Podlewska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Jastrzebski_S/0/1/0/all/0/1"&gt;Stanislaw Jastrzebski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transflower: probabilistic autoregressive dance generation with multimodal attention. (arXiv:2106.13871v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13871</id>
        <link href="http://arxiv.org/abs/2106.13871"/>
        <updated>2021-06-29T01:55:17.913Z</updated>
        <summary type="html"><![CDATA[Dance requires skillful composition of complex movements that follow
rhythmic, tonal and timbral features of music. Formally, generating dance
conditioned on a piece of music can be expressed as a problem of modelling a
high-dimensional continuous motion signal, conditioned on an audio signal. In
this work we make two contributions to tackle this problem. First, we present a
novel probabilistic autoregressive architecture that models the distribution
over future poses with a normalizing flow conditioned on previous poses as well
as music context, using a multimodal transformer encoder. Second, we introduce
the currently largest 3D dance-motion dataset, obtained with a variety of
motion-capture technologies, and including both professional and casual
dancers. Using this dataset, we compare our new model against two baselines,
via objective metrics and a user study, and show that both the ability to model
a probability distribution, as well as being able to attend over a large motion
and music context are necessary to produce interesting, diverse, and realistic
dance that matches the music.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valle_Perez_G/0/1/0/all/0/1"&gt;Guillermo Valle-P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1"&gt;Gustav Eje Henter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beskow_J/0/1/0/all/0/1"&gt;Jonas Beskow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holzapfel_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Holzapfel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1"&gt;Pierre-Yves Oudeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexanderson_S/0/1/0/all/0/1"&gt;Simon Alexanderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advanced Stationary and Non-Stationary Kernel Designs for Domain-Aware Gaussian Processes. (arXiv:2102.03432v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03432</id>
        <link href="http://arxiv.org/abs/2102.03432"/>
        <updated>2021-06-29T01:55:17.898Z</updated>
        <summary type="html"><![CDATA[Gaussian process regression is a widely-applied method for function
approximation and uncertainty quantification. The technique has gained
popularity recently in the machine learning community due to its robustness and
interpretability. The mathematical methods we discuss in this paper are an
extension of the Gaussian-process framework. We are proposing advanced kernel
designs that only allow for functions with certain desirable characteristics to
be elements of the reproducing kernel Hilbert space (RKHS) that underlies all
kernel methods and serves as the sample space for Gaussian process regression.
These desirable characteristics reflect the underlying physics; two obvious
examples are symmetry and periodicity constraints. In addition, non-stationary
kernel designs can be defined in the same framework to yield flexible
multi-task Gaussian processes. We will show the impact of advanced kernel
designs on Gaussian processes using several synthetic and two scientific data
sets. The results show that including domain knowledge, communicated through
advanced kernel designs, has a significant impact on the accuracy and relevance
of the function approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Noack_M/0/1/0/all/0/1"&gt;Marcus M. Noack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sethian_J/0/1/0/all/0/1"&gt;James A. Sethian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smart Choices and the Selection Monad. (arXiv:2007.08926v5 [cs.LO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08926</id>
        <link href="http://arxiv.org/abs/2007.08926"/>
        <updated>2021-06-29T01:55:17.892Z</updated>
        <summary type="html"><![CDATA[Describing systems in terms of choices and their resulting costs and rewards
offers the promise of freeing algorithm designers and programmers from
specifying how those choices should be made; in implementations, the choices
can be realized by optimization techniques and,increasingly, by
machine-learning methods. We study this approach from a programming-language
perspective. We define two small languages that support decision-making
abstractions: one with choices and rewards, and the other additionally with
probabilities. We give both operational and denotational semantics.

In the case of the second language we consider three denotational semantics,
with varying degrees of correlation between possible program values and
expected rewards. The operational semantics combine the usual semantics of
standard constructs with optimization over spaces of possible execution
strategies. The denotational semantics, which are compositional rely on the
selection monad, to handle choice, augmented with an auxiliary monad to handle
other effects, such as rewards or probability.

We establish adequacy theorems that the two semantics coincide in all cases.
We also prove full abstraction at base types, with varying notions of
observation in the probabilistic case corresponding to the various degrees of
correlation. We present axioms for choice combined with rewards and
probability, establishing completeness at base types for the case of rewards
without probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abadi_M/0/1/0/all/0/1"&gt;Martin Abadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plotkin_G/0/1/0/all/0/1"&gt;Gordon Plotkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Planning in Average-Reward Markov Decision Processes. (arXiv:2006.16318v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16318</id>
        <link href="http://arxiv.org/abs/2006.16318"/>
        <updated>2021-06-29T01:55:17.886Z</updated>
        <summary type="html"><![CDATA[We introduce learning and planning algorithms for average-reward MDPs,
including 1) the first general proven-convergent off-policy model-free control
algorithm without reference states, 2) the first proven-convergent off-policy
model-free prediction algorithm, and 3) the first off-policy learning algorithm
that converges to the actual value function rather than to the value function
plus an offset. All of our algorithms are based on using the
temporal-difference error rather than the conventional error when updating the
estimate of the average reward. Our proof techniques are a slight
generalization of those by Abounadi, Bertsekas, and Borkar (2001). In
experiments with an Access-Control Queuing Task, we show some of the
difficulties that can arise when using methods that rely on reference states
and argue that our new algorithms can be significantly easier to use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1"&gt;Yi Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1"&gt;Abhishek Naik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1"&gt;Richard S. Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private SGD with Non-Smooth Losses. (arXiv:2101.08925v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08925</id>
        <link href="http://arxiv.org/abs/2101.08925"/>
        <updated>2021-06-29T01:55:17.881Z</updated>
        <summary type="html"><![CDATA[In this paper, we are concerned with differentially private {stochastic
gradient descent (SGD)} algorithms in the setting of stochastic convex
optimization (SCO). Most of the existing work requires the loss to be Lipschitz
continuous and strongly smooth, and the model parameter to be uniformly
bounded. However, these assumptions are restrictive as many popular losses
violate these conditions including the hinge loss for SVM, the absolute loss in
robust regression, and even the least square loss in an unbounded domain. We
significantly relax these restrictive assumptions and establish privacy and
generalization (utility) guarantees for private SGD algorithms using output and
gradient perturbations associated with non-smooth convex losses. Specifically,
the loss function is relaxed to have an $\alpha$-H\"{o}lder continuous gradient
(referred to as $\alpha$-H\"{o}lder smoothness) which instantiates the
Lipschitz continuity ($\alpha=0$) and the strong smoothness ($\alpha=1$). We
prove that noisy SGD with $\alpha$-H\"older smooth losses using gradient
perturbation can guarantee $(\epsilon,\delta)$-differential privacy (DP) and
attain optimal excess population risk
$\mathcal{O}\Big(\frac{\sqrt{d\log(1/\delta)}}{n\epsilon}+\frac{1}{\sqrt{n}}\Big)$,
up to logarithmic terms, with the gradient complexity $ \mathcal{O}(
n^{2-\alpha\over 1+\alpha}+ n).$ This shows an important trade-off between
$\alpha$-H\"older smoothness of the loss and the computational complexity for
private SGD with statistically optimal performance. In particular, our results
indicate that $\alpha$-H\"older smoothness with $\alpha\ge {1/2}$ is sufficient
to guarantee $(\epsilon,\delta)$-DP of noisy SGD algorithms while achieving
optimal excess risk with the linear gradient complexity $\mathcal{O}(n).$]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_P/0/1/0/all/0/1"&gt;Puyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yunwen Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ying_Y/0/1/0/all/0/1"&gt;Yiming Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hai Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Individual Privacy Accounting via a Renyi Filter. (arXiv:2008.11193v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11193</id>
        <link href="http://arxiv.org/abs/2008.11193"/>
        <updated>2021-06-29T01:55:17.873Z</updated>
        <summary type="html"><![CDATA[We consider a sequential setting in which a single dataset of individuals is
used to perform adaptively-chosen analyses, while ensuring that the
differential privacy loss of each participant does not exceed a pre-specified
privacy budget. The standard approach to this problem relies on bounding a
worst-case estimate of the privacy loss over all individuals and all possible
values of their data, for every single analysis. Yet, in many scenarios this
approach is overly conservative, especially for "typical" data points which
incur little privacy loss by participation in most of the analyses. In this
work, we give a method for tighter privacy loss accounting based on the value
of a personalized privacy loss estimate for each individual in each analysis.
To implement the accounting method we design a filter for R\'enyi differential
privacy. A filter is a tool that ensures that the privacy parameter of a
composed sequence of algorithms with adaptively-chosen privacy parameters does
not exceed a pre-specified budget. Our filter is simpler and tighter than the
known filter for $(\epsilon,\delta)$-differential privacy by Rogers et al. We
apply our results to the analysis of noisy gradient descent and show that
personalized accounting can be practical, easy to implement, and can only make
the privacy-utility tradeoff tighter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1"&gt;Vitaly Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1"&gt;Tijana Zrnic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Data Compression and Quantum Cross Entropy. (arXiv:2106.13823v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.13823</id>
        <link href="http://arxiv.org/abs/2106.13823"/>
        <updated>2021-06-29T01:55:17.858Z</updated>
        <summary type="html"><![CDATA[Quantum machine learning is an emerging field at the intersection of machine
learning and quantum computing. A central quantity for the theoretical
foundation of quantum machine learning is the quantum cross entropy. In this
paper, we present one operational interpretation of this quantity, that the
quantum cross entropy is the compression rate for sub-optimal quantum source
coding. To do so, we give a simple, universal quantum data compression
protocol, which is developed based on quantum generalization of variable-length
coding, as well as quantum strong typicality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Shangnan_Z/0/1/0/all/0/1"&gt;Zhou Shangnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14269</id>
        <link href="http://arxiv.org/abs/2106.14269"/>
        <updated>2021-06-29T01:55:17.851Z</updated>
        <summary type="html"><![CDATA[In large technology companies, the requirements for managing and organizing
technical documents created by engineers and managers in supporting relevant
decision making have increased dramatically in recent years, which has led to a
higher demand for more scalable, accurate, and automated document
classification. Prior studies have primarily focused on processing text for
classification and small-scale databases. This paper describes a novel
multimodal deep learning architecture, called TechDoc, for technical document
classification, which utilizes both natural language and descriptive images to
train hierarchical classifiers. The architecture synthesizes convolutional
neural networks and recurrent neural networks through an integrated training
process. We applied the architecture to a large multimodal technical document
database and trained the model for classifying documents based on the
hierarchical International Patent Classification system. Our results show that
the trained neural network presents a greater classification accuracy than
those using a single modality and several earlier text classification methods.
The trained model can potentially be scaled to millions of real-world technical
documents with both text and figures, which is useful for data and knowledge
management in large technology companies and organizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shuo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jianxi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1"&gt;Christopher L. Magee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seismic Facies Analysis: A Deep Domain Adaptation Approach. (arXiv:2011.10510v2 [physics.geo-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10510</id>
        <link href="http://arxiv.org/abs/2011.10510"/>
        <updated>2021-06-29T01:55:17.845Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) can learn accurately from large quantities of
labeled input data, but DNNs sometimes fail to generalize to test data sampled
from different input distributions. Unsupervised Deep Domain Adaptation (DDA)
proves useful when no input labels are available, and distribution shifts are
observed in the target domain (TD). Experiments are performed on seismic images
of the F3 block 3D dataset from offshore Netherlands (source domain; SD) and
Penobscot 3D survey data from Canada (target domain; TD). Three geological
classes from SD and TD that have similar reflection patterns are considered. In
the present study, an improved deep neural network architecture named
EarthAdaptNet (EAN) is proposed to semantically segment the seismic images. We
specifically use a transposed residual unit to replace the traditional dilated
convolution in the decoder block. The EAN achieved a pixel-level accuracy >84%
and an accuracy of ~70% for the minority classes, showing improved performance
compared to existing architectures. In addition, we introduced the CORAL
(Correlation Alignment) method to the EAN to create an unsupervised deep domain
adaptation network (EAN-DDA) for the classification of seismic reflections
fromF3 and Penobscot. Maximum class accuracy achieved was ~99% for class 2 of
Penobscot with >50% overall accuracy. Taken together, EAN-DDA has the potential
to classify target domain seismic facies classes with high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Nasim_M/0/1/0/all/0/1"&gt;M Quamer Nasim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Maiti_T/0/1/0/all/0/1"&gt;Tannistha Maiti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Ayush Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Singh_T/0/1/0/all/0/1"&gt;Tarry Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mei_J/0/1/0/all/0/1"&gt;Jie Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels. (arXiv:2008.10271v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10271</id>
        <link href="http://arxiv.org/abs/2008.10271"/>
        <updated>2021-06-29T01:55:17.839Z</updated>
        <summary type="html"><![CDATA[We present a novel multi-view training framework and CNN architecture for
combining information from multiple overlapping satellite images and noisy
training labels derived from OpenStreetMap (OSM) to semantically label
buildings and roads across large geographic regions (100 km$^2$). Our approach
to multi-view semantic segmentation yields a 4-7% improvement in the per-class
IoU scores compared to the traditional approaches that use the views
independently of one another. A unique (and, perhaps, surprising) property of
our system is that modifications that are added to the tail-end of the CNN for
learning from the multi-view data can be discarded at the time of inference
with a relatively small penalty in the overall performance. This implies that
the benefits of training using multiple views are absorbed by all the layers of
the network. Additionally, our approach only adds a small overhead in terms of
the GPU-memory consumption even when training with as many as 32 views per
scene. The system we present is end-to-end automated, which facilitates
comparing the classifiers trained directly on true orthophotos vis-a-vis first
training them on the off-nadir images and subsequently translating the
predicted labels to geographical coordinates. With no human supervision, our
IoU scores for the buildings and roads classes are 0.8 and 0.64 respectively
which are better than state-of-the-art approaches that use OSM labels and that
are not completely automated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Comandur_B/0/1/0/all/0/1"&gt;Bharath Comandur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1"&gt;Avinash C. Kak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stabilizing Equilibrium Models by Jacobian Regularization. (arXiv:2106.14342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14342</id>
        <link href="http://arxiv.org/abs/2106.14342"/>
        <updated>2021-06-29T01:55:17.829Z</updated>
        <summary type="html"><![CDATA[Deep equilibrium networks (DEQs) are a new class of models that eschews
traditional depth in favor of finding the fixed point of a single nonlinear
layer. These models have been shown to achieve performance competitive with the
state-of-the-art deep networks while using significantly less memory. Yet they
are also slower, brittle to architectural choices, and introduce potential
instability to the model. In this paper, we propose a regularization scheme for
DEQ models that explicitly regularizes the Jacobian of the fixed-point update
equations to stabilize the learning of equilibrium models. We show that this
regularization adds only minimal computational cost, significantly stabilizes
the fixed-point convergence in both forward and backward passes, and scales
well to high-dimensional, realistic domains (e.g., WikiText-103 language
modeling and ImageNet classification). Using this method, we demonstrate, for
the first time, an implicit-depth model that runs with approximately the same
speed and level of performance as popular conventional deep networks such as
ResNet-101, while still maintaining the constant memory footprint and
architectural simplicity of DEQs. Code is available at
https://github.com/locuslab/deq .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1"&gt;Shaojie Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1"&gt;Vladlen Koltun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1"&gt;J. Zico Kolter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating adversarial robustness in simulated cerebellum. (arXiv:2012.02976v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02976</id>
        <link href="http://arxiv.org/abs/2012.02976"/>
        <updated>2021-06-29T01:55:17.807Z</updated>
        <summary type="html"><![CDATA[It is well known that artificial neural networks are vulnerable to
adversarial examples, in which great efforts have been made to improve the
robustness. However, such examples are usually imperceptible to humans, and
thus their effect on biological neural circuits is largely unknown. This paper
will investigate the adversarial robustness in a simulated cerebellum, a
well-studied supervised learning system in computational neuroscience.
Specifically, we propose to study three unique characteristics revealed in the
cerebellum: (i) network width; (ii) long-term depression on the parallel
fiber-Purkinje cell synapses; (iii) sparse connectivity in the granule layer,
and hypothesize that they will be beneficial for improving robustness. To the
best of our knowledge, this is the first attempt to examine the adversarial
robustness in simulated cerebellum models.

The results are negative in the experimental phase -- no significant
improvements in robustness are discovered from the proposed three mechanisms.
Consequently, the cerebellum is expected to be vulnerable to adversarial
examples as the deep neural networks under batch training. Neuroscientists are
encouraged to fool the biological system in experiments with adversarial
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuezhang_L/0/1/0/all/0/1"&gt;Liu Yuezhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Deep Quality Monitoring in Streaming Environments. (arXiv:2106.13955v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13955</id>
        <link href="http://arxiv.org/abs/2106.13955"/>
        <updated>2021-06-29T01:55:17.801Z</updated>
        <summary type="html"><![CDATA[The common practice of quality monitoring in industry relies on manual
inspection well-known to be slow, error-prone and operator-dependent. This
issue raises strong demand for automated real-time quality monitoring developed
from data-driven approaches thus alleviating from operator dependence and
adapting to various process uncertainties. Nonetheless, current approaches do
not take into account the streaming nature of sensory information while relying
heavily on hand-crafted features making them application-specific. This paper
proposes the online quality monitoring methodology developed from recently
developed deep learning algorithms for data streams, Neural Networks with
Dynamically Evolved Capacity (NADINE), namely NADINE++. It features the
integration of 1-D and 2-D convolutional layers to extract natural features of
time-series and visual data streams captured from sensors and cameras of the
injection molding machines from our own project. Real-time experiments have
been conducted where the online quality monitoring task is simulated on the fly
under the prequential test-then-train fashion - the prominent data stream
evaluation protocol. Comparison with the state-of-the-art techniques clearly
exhibits the advantage of NADINE++ with 4.68\% improvement on average for the
quality monitoring task in streaming environments. To support the reproducible
research initiative, codes, results of NADINE++ along with supplementary
materials and injection molding dataset are made available in
\url{https://github.com/ContinualAL/NADINE-IJCNN2021}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashfahani_A/0/1/0/all/0/1"&gt;Andri Ashfahani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1"&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lughofer_E/0/1/0/all/0/1"&gt;Edwin Lughofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yee_E/0/1/0/all/0/1"&gt;Edward Yapp Kien Yee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Approximation Algorithms for Individually Fair Clustering. (arXiv:2106.14043v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.14043</id>
        <link href="http://arxiv.org/abs/2106.14043"/>
        <updated>2021-06-29T01:55:17.792Z</updated>
        <summary type="html"><![CDATA[We consider the $k$-clustering problem with $\ell_p$-norm cost, which
includes $k$-median, $k$-means and $k$-center cost functions, under an
individual notion of fairness proposed by Jung et al. [2020]: given a set of
points $P$ of size $n$, a set of $k$ centers induces a fair clustering if for
every point $v\in P$, $v$ can find a center among its $n/k$ closest neighbors.
Recently, Mahabadi and Vakilian [2020] showed how to get a
$(p^{O(p)},7)$-bicriteria approximation for the problem of fair $k$-clustering
with $\ell_p$-norm cost: every point finds a center within distance at most $7$
times its distance to its $(n/k)$-th closest neighbor and the $\ell_p$-norm
cost of the solution is at most $p^{O(p)}$ times the cost of an optimal fair
solution. In this work, for any $\varepsilon>0$, we present an improved $(16^p
+\varepsilon,3)$-bicriteria approximation for the fair $k$-clustering with
$\ell_p$-norm cost. To achieve our guarantees, we extend the framework of
[Charikar et al., 2002, Swamy, 2016] and devise a $16^p$-approximation
algorithm for the facility location with $\ell_p$-norm cost under matroid
constraint which might be of an independent interest. Besides, our approach
suggests a reduction from our individually fair clustering to a clustering with
a group fairness requirement proposed by Kleindessner et al. [2019], which is
essentially the median matroid problem [Krishnaswamy et al., 2011].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1"&gt;Ali Vakilian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yalciner_M/0/1/0/all/0/1"&gt;Mustafa Yal&amp;#xe7;&amp;#x131;ner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Recurrent Neural Networks for Gravitational Wave Experiments. (arXiv:2106.14089v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14089</id>
        <link href="http://arxiv.org/abs/2106.14089"/>
        <updated>2021-06-29T01:55:17.786Z</updated>
        <summary type="html"><![CDATA[This paper presents novel reconfigurable architectures for reducing the
latency of recurrent neural networks (RNNs) that are used for detecting
gravitational waves. Gravitational interferometers such as the LIGO detectors
capture cosmic events such as black hole mergers which happen at unknown times
and of varying durations, producing time-series data. We have developed a new
architecture capable of accelerating RNN inference for analyzing time-series
data from LIGO detectors. This architecture is based on optimizing the
initiation intervals (II) in a multi-layer LSTM (Long Short-Term Memory)
network, by identifying appropriate reuse factors for each layer. A
customizable template for this architecture has been designed, which enables
the generation of low-latency FPGA designs with efficient resource utilization
using high-level synthesis tools. The proposed approach has been evaluated
based on two LSTM models, targeting a ZYNQ 7045 FPGA and a U250 FPGA.
Experimental results show that with balanced II, the number of DSPs can be
reduced up to 42% while achieving the same IIs. When compared to other
FPGA-based LSTM designs, our design can achieve about 4.92 to 12.4 times lower
latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Que_Z/0/1/0/all/0/1"&gt;Zhiqiang Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Erwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marikar_U/0/1/0/all/0/1"&gt;Umar Marikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_E/0/1/0/all/0/1"&gt;Eric Moreno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngadiuba_J/0/1/0/all/0/1"&gt;Jennifer Ngadiuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_H/0/1/0/all/0/1"&gt;Hamza Javed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borzyszkowski_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej Borzyszkowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aarrestad_T/0/1/0/all/0/1"&gt;Thea Aarrestad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loncar_V/0/1/0/all/0/1"&gt;Vladimir Loncar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Summers_S/0/1/0/all/0/1"&gt;Sioni Summers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pierini_M/0/1/0/all/0/1"&gt;Maurizio Pierini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_P/0/1/0/all/0/1"&gt;Peter Y Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luk_W/0/1/0/all/0/1"&gt;Wayne Luk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Conditional Predictors for Domain Adaptation. (arXiv:2106.13899v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13899</id>
        <link href="http://arxiv.org/abs/2106.13899"/>
        <updated>2021-06-29T01:55:17.780Z</updated>
        <summary type="html"><![CDATA[Learning guarantees often rely on assumptions of i.i.d. data, which will
likely be violated in practice once predictors are deployed to perform
real-world tasks. Domain adaptation approaches thus appeared as a useful
framework yielding extra flexibility in that distinct train and test data
distributions are supported, provided that other assumptions are satisfied such
as covariate shift, which expects the conditional distributions over labels to
be independent of the underlying data distribution. Several approaches were
introduced in order to induce generalization across varying train and test data
sources, and those often rely on the general idea of domain-invariance, in such
a way that the data-generating distributions are to be disregarded by the
prediction model. In this contribution, we tackle the problem of generalizing
across data sources by approaching it from the opposite direction: we consider
a conditional modeling approach in which predictions, in addition to being
dependent on the input data, use information relative to the underlying
data-generating distribution. For instance, the model has an explicit mechanism
to adapt to changing environments and/or new data sources. We argue that such
an approach is more generally applicable than current domain adaptation methods
since it does not require extra assumptions such as covariate shift and further
yields simpler training algorithms that avoid a common source of training
instabilities caused by minimax formulations, often employed in
domain-invariant methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1"&gt;Joao Monteiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibert_X/0/1/0/all/0/1"&gt;Xavier Gibert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jianqiao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1"&gt;Vincent Dumoulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dar-Shyang Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrinsically Motivated Self-supervised Learning in Reinforcement Learning. (arXiv:2106.13970v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13970</id>
        <link href="http://arxiv.org/abs/2106.13970"/>
        <updated>2021-06-29T01:55:17.762Z</updated>
        <summary type="html"><![CDATA[In vision-based reinforcement learning (RL) tasks, it is prevalent to assign
the auxiliary task with a surrogate self-supervised loss so as to obtain more
semantic representations and improve sample efficiency. However, abundant
information in self-supervised auxiliary tasks has been disregarded, since the
representation learning part and the decision-making part are separated. To
sufficiently utilize information in the auxiliary task, we present a simple yet
effective idea to employ self-supervised loss as an intrinsic reward, called
Intrinsically Motivated Self-Supervised learning in Reinforcement learning
(IM-SSR). We formally show that the self-supervised loss can be decomposed as
exploration for novel states and robustness improvement from nuisance
elimination. IM-SSR can be effortlessly plugged into any reinforcement learning
with self-supervised auxiliary objectives with nearly no additional cost.
Combined with IM-SSR, the previous underlying algorithms achieve salient
improvements on both sample efficiency and generalization in various
vision-based robotics tasks from the DeepMind Control Suite, especially when
the reward signal is sparse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yue Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1"&gt;Chenzhuang Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tiejun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving Spatiotemporal PDEs. (arXiv:2106.14103v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14103</id>
        <link href="http://arxiv.org/abs/2106.14103"/>
        <updated>2021-06-29T01:55:17.755Z</updated>
        <summary type="html"><![CDATA[Partial differential equations (PDEs) play a fundamental role in modeling and
simulating problems across a wide range of disciplines. Recent advances in deep
learning have shown the great potential of physics-informed neural networks
(PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis.
However, the majority of existing PINN methods, based on fully-connected NNs,
pose intrinsic limitations to low-dimensional spatiotemporal parameterizations.
Moreover, since the initial/boundary conditions (I/BCs) are softly imposed via
penalty, the solution quality heavily relies on hyperparameter tuning. To this
end, we propose the novel physics-informed convolutional-recurrent learning
architectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled
data. Specifically, an encoder-decoder convolutional long short-term memory
network is proposed for low-dimensional spatial feature extraction and temporal
evolution learning. The loss function is defined as the aggregated discretized
PDE residuals, while the I/BCs are hard-encoded in the network to ensure
forcible satisfaction (e.g., periodic boundary padding). The networks are
further enhanced by autoregressive and residual connections that explicitly
simulate time marching. The performance of our proposed methods has been
assessed by solving three nonlinear PDEs (e.g., 2D Burgers' equations, the
$\lambda$-$\omega$ and FitzHugh Nagumo reaction-diffusion equations), and
compared against the start-of-the-art baseline algorithms. The numerical
results demonstrate the superiority of our proposed methodology in the context
of solution accuracy, extrapolability and generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_C/0/1/0/all/0/1"&gt;Chengping Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianxun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrently Predicting Hypergraphs. (arXiv:2106.13919v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13919</id>
        <link href="http://arxiv.org/abs/2106.13919"/>
        <updated>2021-06-29T01:55:17.749Z</updated>
        <summary type="html"><![CDATA[This work considers predicting the relational structure of a hypergraph for a
given set of vertices, as common for applications in particle physics,
biological systems and other complex combinatorial problems. A problem arises
from the number of possible multi-way relationships, or hyperedges, scaling in
$\mathcal{O}(2^n)$ for a set of $n$ elements. Simply storing an indicator
tensor for all relationships is already intractable for moderately sized $n$,
prompting previous approaches to restrict the number of vertices a hyperedge
connects. Instead, we propose a recurrent hypergraph neural network that
predicts the incidence matrix by iteratively refining an initial guess of the
solution. We leverage the property that most hypergraphs of interest are
sparsely connected and reduce the memory requirement to $\mathcal{O}(nk)$,
where $k$ is the maximum number of positive edges, i.e., edges that actually
exist. In order to counteract the linearly growing memory cost from training a
lengthening sequence of refinement steps, we further propose an algorithm that
applies backpropagation through time on randomly sampled subsequences. We
empirically show that our method can match an increase in the intrinsic
complexity without a performance decrease and demonstrate superior performance
compared to state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;David W. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1"&gt;Gertjan J. Burghouts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G. M. Snoek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13884</id>
        <link href="http://arxiv.org/abs/2106.13884"/>
        <updated>2021-06-29T01:55:17.742Z</updated>
        <summary type="html"><![CDATA[When trained at sufficient scale, auto-regressive language models exhibit the
notable ability to learn a new language task after being prompted with just a
few examples. Here, we present a simple, yet effective, approach for
transferring this few-shot learning ability to a multimodal setting (vision and
language). Using aligned image and caption data, we train a vision encoder to
represent each image as a sequence of continuous embeddings, such that a
pre-trained, frozen language model prompted with this prefix generates the
appropriate caption. The resulting system is a multimodal few-shot learner,
with the surprising ability to learn a variety of new tasks when conditioned on
examples, represented as a sequence of multiple interleaved image and text
embeddings. We demonstrate that it can rapidly learn words for new objects and
novel visual categories, do visual question-answering with only a handful of
examples, and make use of outside knowledge, by measuring a single model on a
variety of established and new benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1"&gt;Maria Tsimpoukelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1"&gt;Jacob Menick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1"&gt;Serkan Cabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1"&gt;S.M. Ali Eslami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1"&gt;Felix Hill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rationale-Inspired Natural Language Explanations with Commonsense. (arXiv:2106.13876v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13876</id>
        <link href="http://arxiv.org/abs/2106.13876"/>
        <updated>2021-06-29T01:55:17.726Z</updated>
        <summary type="html"><![CDATA[Explainable machine learning models primarily justify predicted labels using
either extractive rationales (i.e., subsets of input features) or free-text
natural language explanations (NLEs) as abstractive justifications. While NLEs
can be more comprehensive than extractive rationales, machine-generated NLEs
have been shown to sometimes lack commonsense knowledge. Here, we show that
commonsense knowledge can act as a bridge between extractive rationales and
NLEs, rendering both types of explanations better. More precisely, we introduce
a unified framework, called RExC (Rationale-Inspired Explanations with
Commonsense), that (1) extracts rationales as a set of features responsible for
machine predictions, (2) expands the extractive rationales using available
commonsense resources, and (3) uses the expanded knowledge to generate natural
language explanations. Our framework surpasses by a large margin the previous
state-of-the-art in generating NLEs across five tasks in both natural language
processing and vision-language understanding, with human annotators
consistently rating the explanations generated by RExC to be more
comprehensive, grounded in commonsense, and overall preferred compared to
previous state-of-the-art models. Moreover, our work shows that
commonsense-grounded explanations can enhance both task performance and
rationales extraction capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1"&gt;Bodhisattwa Prasad Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1"&gt;Oana-Maria Camburu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperbolic Busemann Learning with Ideal Prototypes. (arXiv:2106.14472v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14472</id>
        <link href="http://arxiv.org/abs/2106.14472"/>
        <updated>2021-06-29T01:55:17.720Z</updated>
        <summary type="html"><![CDATA[Hyperbolic space has become a popular choice of manifold for representation
learning of arbitrary data, from tree-like structures and text to graphs.
Building on the success of deep learning with prototypes in Euclidean and
hyperspherical spaces, a few recent works have proposed hyperbolic prototypes
for classification. Such approaches enable effective learning in
low-dimensional output spaces and can exploit hierarchical relations amongst
classes, but require privileged information about class labels to position the
hyperbolic prototypes. In this work, we propose Hyperbolic Busemann Learning.
The main idea behind our approach is to position prototypes on the ideal
boundary of the Poincare ball, which does not require prior label knowledge. To
be able to compute proximities to ideal prototypes, we introduce the penalised
Busemann loss. We provide theory supporting the use of ideal prototypes and the
proposed loss by proving its equivalence to logistic regression in the
one-dimensional case. Empirically, we show that our approach provides a natural
interpretation of classification confidence, while outperforming recent
hyperspherical and hyperbolic prototype approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atigh_M/0/1/0/all/0/1"&gt;Mina Ghadimi Atigh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_Ressel_M/0/1/0/all/0/1"&gt;Martin Keller-Ressel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1"&gt;Pascal Mettes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-assisted Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control. (arXiv:2106.14144v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.14144</id>
        <link href="http://arxiv.org/abs/2106.14144"/>
        <updated>2021-06-29T01:55:17.687Z</updated>
        <summary type="html"><![CDATA[As people spend up to 87% of their time indoors, intelligent Heating,
Ventilation, and Air Conditioning (HVAC) systems in buildings are essential for
maintaining occupant comfort and reducing energy consumption. Those HVAC
systems in modern smart buildings rely on real-time sensor readings, which in
practice often suffer from various faults and could also be vulnerable to
malicious attacks. Such faulty sensor inputs may lead to the violation of
indoor environment requirements (e.g., temperature, humidity, etc.) and the
increase of energy consumption. While many model-based approaches have been
proposed in the literature for building HVAC control, it is costly to develop
accurate physical models for ensuring their performance and even more
challenging to address the impact of sensor faults. In this work, we present a
novel learning-based framework for sensor fault-tolerant HVAC control, which
includes three deep learning based components for 1) generating temperature
proposals with the consideration of possible sensor faults, 2) selecting one of
the proposals based on the assessment of their accuracy, and 3) applying
reinforcement learning with the selected temperature proposal. Moreover, to
address the challenge of training data insufficiency in building-related tasks,
we propose a model-assisted learning method leveraging an abstract model of
building physical dynamics. Through extensive numerical experiments, we
demonstrate that the proposed fault-tolerant HVAC control framework can
significantly reduce building temperature violations under a variety of sensor
fault patterns while maintaining energy efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shichao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yangyang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+ONeill_Z/0/1/0/all/0/1"&gt;Zheng O&amp;#x27;Neill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task Over-the-Air Federated Learning: A Non-Orthogonal Transmission Approach. (arXiv:2106.14229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14229</id>
        <link href="http://arxiv.org/abs/2106.14229"/>
        <updated>2021-06-29T01:55:17.679Z</updated>
        <summary type="html"><![CDATA[In this letter, we propose a multi-task over-theair federated learning
(MOAFL) framework, where multiple learning tasks share edge devices for data
collection and learning models under the coordination of a edge server (ES).
Specially, the model updates for all the tasks are transmitted and
superpositioned concurrently over a non-orthogonal uplink channel via
over-the-air computation, and the aggregation results of all the tasks are
reconstructed at the ES through an extended version of the turbo compressed
sensing algorithm. Both the convergence analysis and numerical results
demonstrate that the MOAFL framework can significantly reduce the uplink
bandwidth consumption of multiple tasks without causing substantial learning
performance degradation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Haoming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaojun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Dian Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Maximum Halfspace Discrepancy. (arXiv:2106.13851v1 [cs.CG])]]></title>
        <id>http://arxiv.org/abs/2106.13851</id>
        <link href="http://arxiv.org/abs/2106.13851"/>
        <updated>2021-06-29T01:55:17.672Z</updated>
        <summary type="html"><![CDATA[Consider the geometric range space $(X, \mathcal{H}_d)$ where $X \subset
\mathbb{R}^d$ and $\mathcal{H}_d$ is the set of ranges defined by
$d$-dimensional halfspaces. In this setting we consider that $X$ is the
disjoint union of a red and blue set. For each halfspace $h \in \mathcal{H}_d$
define a function $\Phi(h)$ that measures the "difference" between the fraction
of red and fraction of blue points which fall in the range $h$. In this context
the maximum discrepancy problem is to find the $h^* = \arg \max_{h \in (X,
\mathcal{H}_d)} \Phi(h)$. We aim to instead find an $\hat{h}$ such that
$\Phi(h^*) - \Phi(\hat{h}) \le \varepsilon$. This is the central problem in
linear classification for machine learning, in spatial scan statistics for
spatial anomaly detection, and shows up in many other areas. We provide a
solution for this problem in $O(|X| + (1/\varepsilon^d) \log^4
(1/\varepsilon))$ time, which improves polynomially over the previous best
solutions. For $d=2$ we show that this is nearly tight through conditional
lower bounds. For different classes of $\Phi$ we can either provide a
$\Omega(|X|^{3/2 - o(1)})$ time lower bound for the exact solution with a
reduction to APSP, or an $\Omega(|X| + 1/\varepsilon^{2-o(1)})$ lower bound for
the approximate solution with a reduction to 3SUM.

A key technical result is a $\varepsilon$-approximate halfspace range
counting data structure of size $O(1/\varepsilon^d)$ with $O(\log
(1/\varepsilon))$ query time, which we can build in $O(|X| + (1/\varepsilon^d)
\log^4 (1/\varepsilon))$ time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matheny_M/0/1/0/all/0/1"&gt;Michael Matheny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1"&gt;Jeff M. Phillips&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Infused Policy Gradients with Upper Confidence Bound for Relational Bandits. (arXiv:2106.13895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13895</id>
        <link href="http://arxiv.org/abs/2106.13895"/>
        <updated>2021-06-29T01:55:17.649Z</updated>
        <summary type="html"><![CDATA[Contextual Bandits find important use cases in various real-life scenarios
such as online advertising, recommendation systems, healthcare, etc. However,
most of the algorithms use flat feature vectors to represent context whereas,
in the real world, there is a varying number of objects and relations among
them to model in the context. For example, in a music recommendation system,
the user context contains what music they listen to, which artists create this
music, the artist albums, etc. Adding richer relational context representations
also introduces a much larger context space making exploration-exploitation
harder. To improve the efficiency of exploration-exploitation knowledge about
the context can be infused to guide the exploration-exploitation strategy.
Relational context representations allow a natural way for humans to specify
knowledge owing to their descriptive nature. We propose an adaptation of
Knowledge Infused Policy Gradients to the Contextual Bandit setting and a novel
Knowledge Infused Policy Gradients Upper Confidence Bound algorithm and perform
an experimental analysis of a simulated music recommendation dataset and
various real-life datasets where expert knowledge can drastically reduce the
total regret and where it cannot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1"&gt;Kaushik Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1"&gt;Manas Gaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1"&gt;Amit Sheth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Dose CT Denoising Using a Structure-Preserving Kernel Prediction Network. (arXiv:2105.14758v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14758</id>
        <link href="http://arxiv.org/abs/2105.14758"/>
        <updated>2021-06-29T01:55:17.641Z</updated>
        <summary type="html"><![CDATA[Low-dose CT has been a key diagnostic imaging modality to reduce the
potential risk of radiation overdose to patient health. Despite recent
advances, CNN-based approaches typically apply filters in a spatially invariant
way and adopt similar pixel-level losses, which treat all regions of the CT
image equally and can be inefficient when fine-grained structures coexist with
non-uniformly distributed noises. To address this issue, we propose a
Structure-preserving Kernel Prediction Network (StructKPN) that combines the
kernel prediction network with a structure-aware loss function that utilizes
the pixel gradient statistics and guides the model towards spatially-variant
filters that enhance noise removal, prevent over-smoothing and preserve
detailed structures for different regions in CT imaging. Extensive experiments
demonstrated that our approach achieved superior performance on both synthetic
and non-synthetic datasets, and better preserves structures that are highly
desired in clinical screening and low-dose protocol optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1"&gt;Daoye Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jimmy Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jingwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Zhaoxiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Habitat 2.0: Training Home Assistants to Rearrange their Habitat. (arXiv:2106.14405v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14405</id>
        <link href="http://arxiv.org/abs/2106.14405"/>
        <updated>2021-06-29T01:55:17.630Z</updated>
        <summary type="html"><![CDATA[We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual
robots in interactive 3D environments and complex physics-enabled scenarios. We
make comprehensive contributions to all levels of the embodied AI stack - data,
simulation, and benchmark tasks. Specifically, we present: (i) ReplicaCAD: an
artist-authored, annotated, reconfigurable 3D dataset of apartments (matching
real spaces) with articulated objects (e.g. cabinets and drawers that can
open/close); (ii) H2.0: a high-performance physics-enabled 3D simulator with
speeds exceeding 25,000 simulation steps per second (850x real-time) on an
8-GPU node, representing 100x speed-ups over prior work; and, (iii) Home
Assistant Benchmark (HAB): a suite of common tasks for assistive robots (tidy
the house, prepare groceries, set the table) that test a range of mobile
manipulation capabilities. These large-scale engineering contributions allow us
to systematically compare deep reinforcement learning (RL) at scale and
classical sense-plan-act (SPA) pipelines in long-horizon structured tasks, with
an emphasis on generalization to new objects, receptacles, and layouts. We find
that (1) flat RL policies struggle on HAB compared to hierarchical ones; (2) a
hierarchy with independent skills suffers from 'hand-off problems', and (3) SPA
pipelines are more brittle than RL policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Szot_A/0/1/0/all/0/1"&gt;Andrew Szot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1"&gt;Alex Clegg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Undersander_E/0/1/0/all/0/1"&gt;Eric Undersander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1"&gt;Erik Wijmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yili Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1"&gt;John Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maestre_N/0/1/0/all/0/1"&gt;Noah Maestre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1"&gt;Mustafa Mukadam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1"&gt;Devendra Chaplot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1"&gt;Oleksandr Maksymets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1"&gt;Aaron Gokaslan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vondrus_V/0/1/0/all/0/1"&gt;Vladimir Vondrus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dharur_S/0/1/0/all/0/1"&gt;Sameer Dharur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meier_F/0/1/0/all/0/1"&gt;Franziska Meier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1"&gt;Wojciech Galuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1"&gt;Angel Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1"&gt;Zsolt Kira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1"&gt;Vladlen Koltun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1"&gt;Jitendra Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1"&gt;Manolis Savva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1"&gt;Dhruv Batra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seismic Facies Analysis: A Deep Domain Adaptation Approach. (arXiv:2011.10510v2 [physics.geo-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10510</id>
        <link href="http://arxiv.org/abs/2011.10510"/>
        <updated>2021-06-29T01:55:17.568Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) can learn accurately from large quantities of
labeled input data, but DNNs sometimes fail to generalize to test data sampled
from different input distributions. Unsupervised Deep Domain Adaptation (DDA)
proves useful when no input labels are available, and distribution shifts are
observed in the target domain (TD). Experiments are performed on seismic images
of the F3 block 3D dataset from offshore Netherlands (source domain; SD) and
Penobscot 3D survey data from Canada (target domain; TD). Three geological
classes from SD and TD that have similar reflection patterns are considered. In
the present study, an improved deep neural network architecture named
EarthAdaptNet (EAN) is proposed to semantically segment the seismic images. We
specifically use a transposed residual unit to replace the traditional dilated
convolution in the decoder block. The EAN achieved a pixel-level accuracy >84%
and an accuracy of ~70% for the minority classes, showing improved performance
compared to existing architectures. In addition, we introduced the CORAL
(Correlation Alignment) method to the EAN to create an unsupervised deep domain
adaptation network (EAN-DDA) for the classification of seismic reflections
fromF3 and Penobscot. Maximum class accuracy achieved was ~99% for class 2 of
Penobscot with >50% overall accuracy. Taken together, EAN-DDA has the potential
to classify target domain seismic facies classes with high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Nasim_M/0/1/0/all/0/1"&gt;M Quamer Nasim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Maiti_T/0/1/0/all/0/1"&gt;Tannistha Maiti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Ayush Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Singh_T/0/1/0/all/0/1"&gt;Tarry Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mei_J/0/1/0/all/0/1"&gt;Jie Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00250</id>
        <link href="http://arxiv.org/abs/2106.00250"/>
        <updated>2021-06-29T01:55:17.561Z</updated>
        <summary type="html"><![CDATA[Multimodal Machine Translation (MMT) enriches the source text with visual
information for translation. It has gained popularity in recent years, and
several pipelines have been proposed in the same direction. Yet, the task lacks
quality datasets to illustrate the contribution of visual modality in the
translation systems. In this paper, we propose our system under the team name
Volta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We
also participate in the textual-only subtask of the same language pair for
which we use mBART, a pretrained multilingual sequence-to-sequence model. For
multimodal translation, we propose to enhance the textual input by bringing the
visual information to a textual domain by extracting object tags from the
image. We also explore the robustness of our system by systematically degrading
the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test
set and challenge set of the multimodal task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1"&gt;Radhika Mamidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Convolutional Memory for Deep Reinforcement Learning. (arXiv:2106.14117v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14117</id>
        <link href="http://arxiv.org/abs/2106.14117"/>
        <updated>2021-06-29T01:55:17.545Z</updated>
        <summary type="html"><![CDATA[Solving partially-observable Markov decision processes (POMDPs) is critical
when applying deep reinforcement learning (DRL) to real-world robotics
problems, where agents have an incomplete view of the world. We present graph
convolutional memory (GCM) for solving POMDPs using deep reinforcement
learning. Unlike recurrent neural networks (RNNs) or transformers, GCM embeds
domain-specific priors into the memory recall process via a knowledge graph. By
encapsulating priors in the graph, GCM adapts to specific tasks but remains
applicable to any DRL task. Using graph convolutions, GCM extracts hierarchical
graph features, analogous to image features in a convolutional neural network
(CNN). We show GCM outperforms long short-term memory (LSTM), gated
transformers for reinforcement learning (GTrXL), and differentiable neural
computers (DNCs) on control, long-term non-sequential recall, and 3D navigation
tasks while using significantly fewer parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morad_S/0/1/0/all/0/1"&gt;Steven D. Morad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liwicki_S/0/1/0/all/0/1"&gt;Stephan Liwicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1"&gt;Amanda Prorok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07566</id>
        <link href="http://arxiv.org/abs/2104.07566"/>
        <updated>2021-06-29T01:55:17.539Z</updated>
        <summary type="html"><![CDATA[Attention mechanism has shown enormous potential for single image
super-resolution (SISR). However, existing works only proposed some attention
mechanism for a specific network. A universal attention mechanism for SISR,
which could further improve the performance of networks without attention and
provide a baseline for networks with attention, is still lacking. To fit this
gap, we propose a lightweight and efficient Balanced Attention Mechanism (BAM),
which consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial
Attention Module (MSAM) in parallel. The information extraction mechanism of
ACAM and MSAM effectively filters redundant information, making the overall
structure of BAM very lightweight. Owing to the parallel structure, during the
gradient backpropagation process of BAM, ACAM and MSAM not only conduct
self-optimization, but also mutual optimization so as to generate more balanced
attention information. To verify the effectiveness and robustness of BAM, we
applied it to 12 state-ofthe-art SISR networks. The results on 4 benchmark
datasets demonstrate that BAM can efficiently improve the networks'
performance, and for those with attention, the substitution with BAM further
reduces the amount of parameters and increase the inference speed. Moreover,
ablation experiments were conducted to prove the minimalism of BAM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fanyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haotian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1"&gt;Cheng Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v8 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10762</id>
        <link href="http://arxiv.org/abs/2104.10762"/>
        <updated>2021-06-29T01:55:17.533Z</updated>
        <summary type="html"><![CDATA[Random field and random cluster theory are used to prove certain mathematical
results concerning the probability distribution of image pixel intensities
characterized as generic $2D$ integer arrays. The size of the smallest bounded
region within an image is estimated for segmenting an image, from which, the
equilibrium distribution of intensities can be recovered. From the estimated
bounded regions, properties of the sub-optimal and equilibrium distributions of
intensities are derived, which leads to an image compression methodology
whereby only slightly more than half of all pixels are required for a
worst-case reconstruction of the original image. An example in unsupervised
object detection illustrates the mathematical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1"&gt;Robert A. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible Attention. (arXiv:2106.09003v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09003</id>
        <link href="http://arxiv.org/abs/2106.09003"/>
        <updated>2021-06-29T01:55:17.527Z</updated>
        <summary type="html"><![CDATA[Attention has been proved to be an efficient mechanism to capture long-range
dependencies. However, so far it has not been deployed in invertible networks.
This is due to the fact that in order to make a network invertible, every
component within the network needs to be a bijective transformation, but a
normal attention block is not. In this paper, we propose invertible attention
that can be plugged into existing invertible models. We mathematically and
experimentally prove that the invertibility of an attention model can be
achieved by carefully constraining its Lipschitz constant. We validate the
invertibility of our invertible attention on image reconstruction task with 3
popular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible
attention achieves similar performance in comparison with normal non-invertible
attention on dense prediction tasks. The code is available at
https://github.com/Schwartz-Zha/InvertibleAttention]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1"&gt;Jiajun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yiran Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1"&gt;Richard Hartley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Meta-model Quantifying for Medical Visual Question Answering. (arXiv:2105.08913v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08913</id>
        <link href="http://arxiv.org/abs/2105.08913"/>
        <updated>2021-06-29T01:55:17.521Z</updated>
        <summary type="html"><![CDATA[Transfer learning is an important step to extract meaningful features and
overcome the data limitation in the medical Visual Question Answering (VQA)
task. However, most of the existing medical VQA methods rely on external data
for transfer learning, while the meta-data within the dataset is not fully
utilized. In this paper, we present a new multiple meta-model quantifying
method that effectively learns meta-annotation and leverages meaningful
features to the medical VQA task. Our proposed method is designed to increase
meta-data by auto-annotation, deal with noisy labels, and output meta-models
which provide robust features for medical VQA tasks. Extensively experimental
results on two public medical VQA datasets show that our approach achieves
superior accuracy in comparison with other state-of-the-art methods, while does
not require external data to train meta-models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1"&gt;Tuong Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh X. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tjiputra_E/0/1/0/all/0/1"&gt;Erman Tjiputra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1"&gt;Quang D. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ladder Polynomial Neural Networks. (arXiv:2106.13834v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13834</id>
        <link href="http://arxiv.org/abs/2106.13834"/>
        <updated>2021-06-29T01:55:17.504Z</updated>
        <summary type="html"><![CDATA[Polynomial functions have plenty of useful analytical properties, but they
are rarely used as learning models because their function class is considered
to be restricted. This work shows that when trained properly polynomial
functions can be strong learning models. Particularly this work constructs
polynomial feedforward neural networks using the product activation, a new
activation function constructed from multiplications. The new neural network is
a polynomial function and provides accurate control of its polynomial order. It
can be trained by standard training techniques such as batch normalization and
dropout. This new feedforward network covers several previous polynomial models
as special cases. Compared with common feedforward neural networks, the
polynomial feedforward network has closed-form calculations of a few
interesting quantities, which are very useful in Bayesian learning. In a series
of regression and classification tasks in the empirical study, the proposed
model outperforms previous polynomial models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Li-Ping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1"&gt;Ruiyuan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaozhe Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Inverse Optimization: Offline and Online Learning. (arXiv:2106.14015v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14015</id>
        <link href="http://arxiv.org/abs/2106.14015"/>
        <updated>2021-06-29T01:55:17.498Z</updated>
        <summary type="html"><![CDATA[We study the problems of offline and online contextual optimization with
feedback information, where instead of observing the loss, we observe,
after-the-fact, the optimal action an oracle with full knowledge of the
objective function would have taken. We aim to minimize regret, which is
defined as the difference between our losses and the ones incurred by an
all-knowing oracle. In the offline setting, the decision-maker has information
available from past periods and needs to make one decision, while in the online
setting, the decision-maker optimizes decisions dynamically over time based a
new set of feasible actions and contextual functions in each period. For the
offline setting, we characterize the optimal minimax policy, establishing the
performance that can be achieved as a function of the underlying geometry of
the information induced by the data. In the online setting, we leverage this
geometric characterization to optimize the cumulative regret. We develop an
algorithm that yields the first regret bound for this problem that is
logarithmic in the time horizon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Besbes_O/0/1/0/all/0/1"&gt;Omar Besbes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_Y/0/1/0/all/0/1"&gt;Yuri Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobel_I/0/1/0/all/0/1"&gt;Ilan Lobel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OSKDet: Towards Orientation-sensitive Keypoint Localization for Rotated Object Detection. (arXiv:2104.08697v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08697</id>
        <link href="http://arxiv.org/abs/2104.08697"/>
        <updated>2021-06-29T01:55:17.488Z</updated>
        <summary type="html"><![CDATA[Rotated object detection is a challenging issue of computer vision field.
Loss of spatial information and confusion of parametric order have been the
bottleneck for rotated detection accuracy. In this paper, we propose an
orientation-sensitive keypoint based rotated detector OSKDet. We adopt a set of
keypoints to characterize the target and predict the keypoint heatmap on ROI to
form a rotated target. By proposing the orientation-sensitive heatmap, OSKDet
could learn the shape and direction of rotated target implicitly and has
stronger modeling capabilities for target representation, which improves the
localization accuracy and acquires high quality detection results. To extract
highly effective features at border areas, we design a rotation-aware
deformable convolution module. Furthermore, we explore a new keypoint reorder
algorithm and feature fusion module based on the angle distribution to
eliminate the confusion of keypoint order. Experimental results on several
public benchmarks show the state-of-the-art performance of OSKDet.
Specifically, we achieve an AP of 77.81% on DOTA, 89.91% on HRSC2016, and
97.18% on UCAS-AOD, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;Dongchen Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06963</id>
        <link href="http://arxiv.org/abs/2106.06963"/>
        <updated>2021-06-29T01:55:17.464Z</updated>
        <summary type="html"><![CDATA[Automatically generating radiology reports can improve current clinical
practice in diagnostic radiology. On one hand, it can relieve radiologists from
the heavy burden of report writing; On the other hand, it can remind
radiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.
Yet, this task remains a challenging job for data-driven neural networks, due
to the serious visual and textual data biases. To this end, we propose a
Posterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to
imitate the working patterns of radiologists, who will first examine the
abnormal regions and assign the disease topic tags to the abnormal regions, and
then rely on the years of prior medical knowledge and prior working experience
accumulations to write reports. Thus, the PPKED includes three modules:
Posterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and
Multi-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior
knowledge, which provides explicit abnormal visual regions to alleviate visual
data bias; PrKE explores the prior knowledge from the prior medical knowledge
graph (medical knowledge) and prior radiology reports (working experience) to
alleviate textual data bias. The explored knowledge is distilled by the MKD to
generate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our
method is able to outperform previous state-of-the-art models on these two
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1"&gt;Wei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Driving-Signal Aware Full-Body Avatars. (arXiv:2105.10441v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10441</id>
        <link href="http://arxiv.org/abs/2105.10441"/>
        <updated>2021-06-29T01:55:17.458Z</updated>
        <summary type="html"><![CDATA[We present a learning-based method for building driving-signal aware
full-body avatars. Our model is a conditional variational autoencoder that can
be animated with incomplete driving signals, such as human pose and facial
keypoints, and produces a high-quality representation of human geometry and
view-dependent appearance. The core intuition behind our method is that better
drivability and generalization can be achieved by disentangling the driving
signals and remaining generative factors, which are not available during
animation. To this end, we explicitly account for information deficiency in the
driving signal by introducing a latent space that exclusively captures the
remaining information, thus enabling the imputation of the missing factors
required during full-body animation, while remaining faithful to the driving
signal. We also propose a learnable localized compression for the driving
signal which promotes better generalization, and helps minimize the influence
of global chance-correlations often found in real datasets. For a given driving
signal, the resulting variational model produces a compact space of uncertainty
for missing factors that allows for an imputation strategy best suited to a
particular application. We demonstrate the efficacy of our approach on the
challenging problem of full-body animation for virtual telepresence with
driving signals acquired from minimal sensors placed in the environment and
mounted on a VR-headset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1"&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenglei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1"&gt;Tomas Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1"&gt;Fabian Prada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1"&gt;Takaaki Shiratori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1"&gt;Shih-En Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weipeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1"&gt;Yaser Sheikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1"&gt;Jason Saragih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer Transforms Salient Object Detection and Camouflaged Object Detection. (arXiv:2104.10127v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10127</id>
        <link href="http://arxiv.org/abs/2104.10127"/>
        <updated>2021-06-29T01:55:17.443Z</updated>
        <summary type="html"><![CDATA[The transformer networks are particularly good at modeling long-range
dependencies within a long sequence. In this paper, we conduct research on
applying the transformer networks for salient object detection (SOD). We adopt
the dense transformer backbone for fully supervised RGB image based SOD, RGB-D
image pair based SOD, and weakly supervised SOD within a unified framework
based on the observation that the transformer backbone can provide accurate
structure modeling, which makes it powerful in learning from weak labels with
less structure information. Further, we find that the vision transformer
architectures do not offer direct spatial supervision, instead encoding
position as a feature. Therefore, we investigate the contributions of two
strategies to provide stronger spatial supervision through the transformer
layers within our unified framework, namely deep supervision and
difficulty-aware learning. We find that deep supervision can get gradients back
into the higher level features, thus leads to uniform activation within the
same semantic object. Difficulty-aware learning on the other hand is capable of
identifying the hard pixels for effective hard negative mining. We also
visualize features of conventional backbone and transformer backbone before and
after fine-tuning them for SOD, and find that transformer backbone encodes more
accurate object structure information and more distinct semantic information
within the lower and higher level features respectively. We also apply our
model to camouflaged object detection (COD) and achieve similar observations as
the above three SOD tasks. Extensive experimental results on various SOD and
COD tasks illustrate that transformer networks can transform SOD and COD,
leading to new benchmarks for each related task. The source code and
experimental results are available via our project page:
https://github.com/fupiao1998/TrasformerSOD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yuxin Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1"&gt;Zhexiong Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yuchao Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Aixuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1"&gt;Yunqiu Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1"&gt;Xinyu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1"&gt;Nick Barnes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT based sentiment analysis: A software engineering perspective. (arXiv:2106.02581v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02581</id>
        <link href="http://arxiv.org/abs/2106.02581"/>
        <updated>2021-06-29T01:55:17.436Z</updated>
        <summary type="html"><![CDATA[Sentiment analysis can provide a suitable lead for the tools used in software
engineering along with the API recommendation systems and relevant libraries to
be used. In this context, the existing tools like SentiCR, SentiStrength-SE,
etc. exhibited low f1-scores that completely defeats the purpose of deployment
of such strategies, thereby there is enough scope for performance improvement.
Recent advancements show that transformer based pre-trained models (e.g., BERT,
RoBERTa, ALBERT, etc.) have displayed better results in the text classification
task. Following this context, the present research explores different
BERT-based models to analyze the sentences in GitHub comments, Jira comments,
and Stack Overflow posts. The paper presents three different strategies to
analyse BERT based model for sentiment analysis, where in the first strategy
the BERT based pre-trained models are fine-tuned; in the second strategy an
ensemble model is developed from BERT variants, and in the third strategy a
compressed model (Distil BERT) is used. The experimental results show that the
BERT based ensemble approach and the compressed BERT model attain improvements
by 6-12% over prevailing tools for the F1 measure on all three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batra_H/0/1/0/all/0/1"&gt;Himanshu Batra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Single Stage Weakly Supervised Semantic Segmentation. (arXiv:2106.10309v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10309</id>
        <link href="http://arxiv.org/abs/2106.10309"/>
        <updated>2021-06-29T01:55:17.398Z</updated>
        <summary type="html"><![CDATA[The costly process of obtaining semantic segmentation labels has driven
research towards weakly supervised semantic segmentation (WSSS) methods, using
only image-level, point, or box labels. The lack of dense scene representation
requires methods to increase complexity to obtain additional semantic
information about the scene, often done through multiple stages of training and
refinement. Current state-of-the-art (SOTA) models leverage image-level labels
to produce class activation maps (CAMs) which go through multiple stages of
refinement before they are thresholded to make pseudo-masks for supervision.
The multi-stage approach is computationally expensive, and dependency on
image-level labels for CAMs generation lacks generalizability to more complex
scenes. In contrary, our method offers a single-stage approach generalizable to
arbitrary dataset, that is trainable from scratch, without any dependency on
pre-trained backbones, classification, or separate refinement tasks. We utilize
point annotations to generate reliable, on-the-fly pseudo-masks through refined
and filtered features. While our method requires point annotations that are
only slightly more expensive than image-level annotations, we are to
demonstrate SOTA performance on benchmark datasets (PascalVOC 2012), as well as
significantly outperform other SOTA WSSS methods on recent real-world datasets
(CRAID, CityPersons, IAD).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akiva_P/0/1/0/all/0/1"&gt;Peri Akiva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dana_K/0/1/0/all/0/1"&gt;Kristin Dana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Attentive Ensemble Transformer: Representing Ensemble Interactions in Neural Networks for Earth System Models. (arXiv:2106.13924v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13924</id>
        <link href="http://arxiv.org/abs/2106.13924"/>
        <updated>2021-06-29T01:55:17.378Z</updated>
        <summary type="html"><![CDATA[Ensemble data from Earth system models has to be calibrated and
post-processed. I propose a novel member-by-member post-processing approach
with neural networks. I bridge ideas from ensemble data assimilation with
self-attention, resulting into the self-attentive ensemble transformer. Here,
interactions between ensemble members are represented as additive and dynamic
self-attentive part. As proof-of-concept, global ECMWF ensemble forecasts are
regressed to 2-metre-temperature fields from the ERA5 reanalysis. I demonstrate
that the ensemble transformer can calibrate the ensemble spread and extract
additional information from the ensemble. Furthermore, the ensemble transformer
directly outputs multivariate and spatially-coherent ensemble members.
Therefore, self-attention and the transformer technique can be a missing piece
for a member-by-member post-processing of ensemble data with neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1"&gt;Tobias Sebastian Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptation Broad Learning System Based on Locally Linear Embedding. (arXiv:2106.14367v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14367</id>
        <link href="http://arxiv.org/abs/2106.14367"/>
        <updated>2021-06-29T01:55:17.311Z</updated>
        <summary type="html"><![CDATA[Broad learning system (BLS) has been proposed for a few years. It
demonstrates an effective learning capability for many classification and
regression problems. However, BLS and its improved versions are mainly used to
deal with unsupervised, supervised and semi-supervised learning problems in a
single domain. As far as we know, a little attention is paid to the
cross-domain learning ability of BLS. Therefore, we introduce BLS into the
field of transfer learning and propose a novel algorithm called domain
adaptation broad learning system based on locally linear embedding (DABLS-LLE).
The proposed algorithm can learn a robust classification model by using a small
part of labeled data from the target domain and all labeled data from the
source domain. The proposed algorithm inherits the computational efficiency and
learning capability of BLS. Experiments on benchmark dataset
(Office-Caltech-10) verify the effectiveness of our approach. The results show
that our approach can get better classification accuracy with less running time
than many existing transfer learning approaches. It shows that our approach can
bring a new superiority for BLS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1"&gt;Chao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1"&gt;Chang-E Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoPipeline: Synthesize Data Pipelines By-Target Using Reinforcement Learning and Search. (arXiv:2106.13861v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.13861</id>
        <link href="http://arxiv.org/abs/2106.13861"/>
        <updated>2021-06-29T01:55:17.305Z</updated>
        <summary type="html"><![CDATA[Recent work has made significant progress in helping users to automate single
data preparation steps, such as string-transformations and table-manipulation
operators (e.g., Join, GroupBy, Pivot, etc.). We in this work propose to
automate multiple such steps end-to-end, by synthesizing complex data pipelines
with both string transformations and table-manipulation operators. We propose a
novel "by-target" paradigm that allows users to easily specify the desired
pipeline, which is a significant departure from the traditional by-example
paradigm. Using by-target, users would provide input tables (e.g., csv or json
files), and point us to a "target table" (e.g., an existing database table or
BI dashboard) to demonstrate how the output from the desired pipeline would
schematically "look like". While the problem is seemingly underspecified, our
unique insight is that implicit table constraints such as FDs and keys can be
exploited to significantly constrain the space to make the problem tractable.
We develop an Auto-Pipeline system that learns to synthesize pipelines using
reinforcement learning and search. Experiments on large numbers of real
pipelines crawled from GitHub suggest that Auto-Pipeline can successfully
synthesize 60-70% of these complex pipelines (up to 10 steps) in 10-20 seconds
on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junwen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yeye He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Surajit Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Channel Pruning in a White Box for Efficient Image Classification. (arXiv:2104.11883v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11883</id>
        <link href="http://arxiv.org/abs/2104.11883"/>
        <updated>2021-06-29T01:55:17.096Z</updated>
        <summary type="html"><![CDATA[Channel Pruning has been long studied to compress CNNs for efficient image
classification. Prior works implement channel pruning in an unexplainable
manner, which tends to reduce the final classification errors while failing to
consider the internal influence of each channel. In this paper, we conduct
channel pruning in a white box. Through deep visualization of feature maps
activated by different channels, we observe that different channels have a
varying contribution to different categories in image classification. Inspired
by this, we choose to preserve channels contributing to most categories.
Specifically, to model the contribution of each channel to differentiating
categories, we develop a class-wise mask for each channel, implemented in a
dynamic training manner w.r.t. the input image's category. On the basis of the
learned class-wise mask, we perform a global voting mechanism to remove
channels with less category discrimination. Lastly, a fine-tuning process is
conducted to recover the performance of the pruned model. To our best
knowledge, it is the first time that CNN interpretability theory is considered
to guide channel pruning. Extensive experiments on representative image
classification tasks demonstrate the superiority of our White-Box over many
state-of-the-arts. For instance, on CIFAR-10, it reduces 65.23% FLOPs with even
0.62% accuracy improvement for ResNet-110. On ILSVRC-2012, White-Box achieves a
45.6% FLOPs reduction with only a small loss of 0.83% in the top-1 accuracy for
ResNet-50.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chia-Wen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongjian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3rd Place Solution for Short-video Face Parsing Challenge. (arXiv:2106.07409v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07409</id>
        <link href="http://arxiv.org/abs/2106.07409"/>
        <updated>2021-06-29T01:55:17.090Z</updated>
        <summary type="html"><![CDATA[This is a short technical report introducing the solution of Team Rat for
Short-video Parsing Face Parsing Track of The 3rd Person in Context (PIC)
Workshop and Challenge at CVPR 2021.

In this report, we propose an Edge-Aware Network (EANet) that uses edge
information to refine the segmentation edge. To further obtain the finer edge
results, we introduce edge attention loss that only compute cross entropy on
the edges, it can effectively reduce the classification error around edge and
get more smooth boundary. Benefiting from the edge information and edge
attention loss, the proposed EANet achieves 86.16\% accuracy in the Short-video
Face Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge,
ranked the third place.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_X/0/1/0/all/0/1"&gt;Xiaofei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiangtao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts. (arXiv:2012.09165v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09165</id>
        <link href="http://arxiv.org/abs/2012.09165"/>
        <updated>2021-06-29T01:55:17.067Z</updated>
        <summary type="html"><![CDATA[The rapid progress in 3D scene understanding has come with growing demand for
data; however, collecting and annotating 3D scenes (e.g. point clouds) are
notoriously hard. For example, the number of scenes (e.g. indoor rooms) that
can be accessed and scanned might be limited; even given sufficient data,
acquiring 3D labels (e.g. instance masks) requires intensive human labor. In
this paper, we explore data-efficient learning for 3D point cloud. As a first
step towards this direction, we propose Contrastive Scene Contexts, a 3D
pre-training method that makes use of both point-level correspondences and
spatial contexts in a scene. Our method achieves state-of-the-art results on a
suite of benchmarks where training data or labels are scarce. Our study reveals
that exhaustive labelling of 3D point clouds might be unnecessary; and
remarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89%
(instance segmentation) and 96% (semantic segmentation) of the baseline
performance that uses full annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Ji Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1"&gt;Benjamin Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1"&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Saining Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Classification and Segmentation on High Resolution Aerial Images. (arXiv:2105.08655v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08655</id>
        <link href="http://arxiv.org/abs/2105.08655"/>
        <updated>2021-06-29T01:55:17.060Z</updated>
        <summary type="html"><![CDATA[FloodNet is a high-resolution image dataset acquired by a small UAV platform,
DJI Mavic Pro quadcopters, after Hurricane Harvey. The dataset presents a
unique challenge of advancing the damage assessment process for post-disaster
scenarios using unlabeled and limited labeled dataset. We propose a solution to
address their classification and semantic segmentation challenge. We approach
this problem by generating pseudo labels for both classification and
segmentation during training and slowly incrementing the amount by which the
pseudo label loss affects the final loss. Using this semi-supervised method of
training helped us improve our baseline supervised loss by a huge margin for
classification, allowing the model to generalize and perform better on the
validation and test splits of the dataset. In this paper, we compare and
contrast the various methods and models for image classification and semantic
segmentation on the FloodNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1"&gt;Sahil Khose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1"&gt;Abhiraj Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Ankita Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Evaluation of Self-Supervised Pre-Training for Skin-Lesion Analysis. (arXiv:2106.09229v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09229</id>
        <link href="http://arxiv.org/abs/2106.09229"/>
        <updated>2021-06-29T01:55:17.041Z</updated>
        <summary type="html"><![CDATA[Self-supervised pre-training appears as an advantageous alternative to
supervised pre-trained for transfer learning. By synthesizing annotations on
pretext tasks, self-supervision allows to pre-train models on large amounts of
pseudo-labels before fine-tuning them on the target task. In this work, we
assess self-supervision for the diagnosis of skin lesions, comparing three
self-supervised pipelines to a challenging supervised baseline, on five test
datasets comprising in- and out-of-distribution samples. Our results show that
self-supervision is competitive both in improving accuracies and in reducing
the variability of outcomes. Self-supervision proves particularly useful for
low training data scenarios ($<1\,500$ and $<150$ samples), where its ability
to stabilize the outcomes is essential to provide sound results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaves_L/0/1/0/all/0/1"&gt;Levy Chaves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bissoto_A/0/1/0/all/0/1"&gt;Alceu Bissoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1"&gt;Eduardo Valle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1"&gt;Sandra Avila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NTIRE 2021 Challenge on Perceptual Image Quality Assessment. (arXiv:2105.03072v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03072</id>
        <link href="http://arxiv.org/abs/2105.03072"/>
        <updated>2021-06-29T01:55:17.004Z</updated>
        <summary type="html"><![CDATA[This paper reports on the NTIRE 2021 challenge on perceptual image quality
assessment (IQA), held in conjunction with the New Trends in Image Restoration
and Enhancement workshop (NTIRE) workshop at CVPR 2021. As a new type of image
processing technology, perceptual image processing algorithms based on
Generative Adversarial Networks (GAN) have produced images with more realistic
textures. These output images have completely different characteristics from
traditional distortions, thus pose a new challenge for IQA methods to evaluate
their visual quality. In comparison with previous IQA challenges, the training
and testing datasets in this challenge include the outputs of perceptual image
processing algorithms and the corresponding subjective scores. Thus they can be
used to develop and evaluate IQA methods on GAN-based distortions. The
challenge has 270 registered participants in total. In the final testing stage,
13 participating teams submitted their models and fact sheets. Almost all of
them have achieved much better results than existing IQA methods, while the
winning method can demonstrate state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jinjin Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1"&gt;Haoming Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jimmy S. Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shuhang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheon_M/0/1/0/all/0/1"&gt;Manri Cheon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungjun Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kang_B/0/1/0/all/0/1"&gt;Byungyeon Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junwoo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1"&gt;Haiyang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bin_Y/0/1/0/all/0/1"&gt;Yi Bin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yuqing Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hengliang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jingyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zirui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bai_Q/0/1/0/all/0/1"&gt;Qingyan Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuwei Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1"&gt;Weihao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_M/0/1/0/all/0/1"&gt;Mingdeng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiahao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yifan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yujiu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_L/0/1/0/all/0/1"&gt;Longtao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yiting Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Junlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thong_W/0/1/0/all/0/1"&gt;William Thong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pereira_J/0/1/0/all/0/1"&gt;Jose Costa Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ales Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McDonagh_S/0/1/0/all/0/1"&gt;Steven McDonagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kele Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lehan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hengxing Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_P/0/1/0/all/0/1"&gt;Pengfei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ayyoubzadeh_S/0/1/0/all/0/1"&gt;Seyed Mehdi Ayyoubzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Royat_A/0/1/0/all/0/1"&gt;Ali Royat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fezza_S/0/1/0/all/0/1"&gt;Sid Ahmed Fezza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hammou_D/0/1/0/all/0/1"&gt;Dounia Hammou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sewoong Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoon_G/0/1/0/all/0/1"&gt;Gwangjin Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsubota_K/0/1/0/all/0/1"&gt;Koki Tsubota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Akutsu_H/0/1/0/all/0/1"&gt;Hiroaki Akutsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aizawa_K/0/1/0/all/0/1"&gt;Kiyoharu Aizawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent neural network transducer for Japanese and Chinese offline handwritten text recognition. (arXiv:2106.14459v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14459</id>
        <link href="http://arxiv.org/abs/2106.14459"/>
        <updated>2021-06-29T01:55:16.982Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an RNN-Transducer model for recognizing Japanese
and Chinese offline handwritten text line images. As far as we know, it is the
first approach that adopts the RNN-Transducer model for offline handwritten
text recognition. The proposed model consists of three main components: a
visual feature encoder that extracts visual features from an input image by CNN
and then encodes the visual features by BLSTM; a linguistic context encoder
that extracts and encodes linguistic features from the input image by embedded
layers and LSTM; and a joint decoder that combines and then decodes the visual
features and the linguistic features into the final label sequence by fully
connected and softmax layers. The proposed model takes advantage of both visual
and linguistic information from the input image. In the experiments, we
evaluated the performance of the proposed model on the two datasets: Kuzushiji
and SCUT-EPT. Experimental results show that the proposed model achieves
state-of-the-art performance on all datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1"&gt;Trung Tan Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hung Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ly_N/0/1/0/all/0/1"&gt;Nam Tuan Ly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1"&gt;Masaki Nakagawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Reinforcement Learning from Logical Specifications. (arXiv:2106.13906v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13906</id>
        <link href="http://arxiv.org/abs/2106.13906"/>
        <updated>2021-06-29T01:55:16.958Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning control policies for complex tasks given by
logical specifications. Recent approaches automatically generate a reward
function from a given specification and use a suitable reinforcement learning
algorithm to learn a policy that maximizes the expected reward. These
approaches, however, scale poorly to complex tasks that require high-level
planning. In this work, we develop a compositional learning approach, called
DiRL, that interleaves high-level planning and reinforcement learning. First,
DiRL encodes the specification as an abstract graph; intuitively, vertices and
edges of the graph correspond to regions of the state space and simpler
sub-tasks, respectively. Our approach then incorporates reinforcement learning
to learn neural network policies for each edge (sub-task) within a
Dijkstra-style planning algorithm to compute a high-level plan in the graph. An
evaluation of the proposed approach on a set of challenging control benchmarks
with continuous state and action spaces demonstrates that it outperforms
state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jothimurugan_K/0/1/0/all/0/1"&gt;Kishor Jothimurugan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1"&gt;Suguman Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1"&gt;Osbert Bastani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alur_R/0/1/0/all/0/1"&gt;Rajeev Alur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition. (arXiv:2106.10598v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10598</id>
        <link href="http://arxiv.org/abs/2106.10598"/>
        <updated>2021-06-29T01:55:16.953Z</updated>
        <summary type="html"><![CDATA[A table arranging data in rows and columns is a very effective data
structure, which has been widely used in business and scientific research.
Considering large-scale tabular data in online and offline documents, automatic
table recognition has attracted increasing attention from the document analysis
community. Though human can easily understand the structure of tables, it
remains a challenge for machines to understand that, especially due to a
variety of different table layouts and styles. Existing methods usually model a
table as either the markup sequence or the adjacency matrix between different
table cells, failing to address the importance of the logical location of table
cells, e.g., a cell is located in the first row and the second column of the
table. In this paper, we reformulate the problem of table structure recognition
as the table graph reconstruction, and propose an end-to-end trainable table
graph reconstruction network (TGRNet) for table structure recognition.
Specifically, the proposed method has two main branches, a cell detection
branch and a cell logical location branch, to jointly predict the spatial
location and the logical location of different cells. Experimental results on
three popular table recognition datasets and a new dataset with table graph
annotations (TableGraph-350K) demonstrate the effectiveness of the proposed
TGRNet for table structure recognition. Code and annotations will be made
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1"&gt;Wenyuan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Baosheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingyong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021: Team M3EM Technical Report. (arXiv:2106.10026v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10026</id>
        <link href="http://arxiv.org/abs/2106.10026"/>
        <updated>2021-06-29T01:55:16.946Z</updated>
        <summary type="html"><![CDATA[In this report, we describe the technical details of our submission to the
2021 EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action
Recognition. Leveraging multiple modalities has been proved to benefit the
Unsupervised Domain Adaptation (UDA) task. In this work, we present Multi-Modal
Mutual Enhancement Module (M3EM), a deep module for jointly considering
information from multiple modalities to find the most transferable
representations across domains. We achieve this by implementing two sub-modules
for enhancing each modality using the context of other modalities. The first
sub-module exchanges information across modalities through the semantic space,
while the second sub-module finds the most transferable spatial region based on
the consensus of all modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lijin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yifei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1"&gt;Yusuke Sugano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1"&gt;Yoichi Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functional Classwise Principal Component Analysis: A Novel Classification Framework. (arXiv:2106.13959v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13959</id>
        <link href="http://arxiv.org/abs/2106.13959"/>
        <updated>2021-06-29T01:55:16.935Z</updated>
        <summary type="html"><![CDATA[In recent times, functional data analysis (FDA) has been successfully applied
in the field of high dimensional data classification. In this paper, we present
a novel classification framework using functional data and classwise Principal
Component Analysis (PCA). Our proposed method can be used in high dimensional
time series data which typically suffers from small sample size problem. Our
method extracts a piece wise linear functional feature space and is
particularly suitable for hard classification problems.The proposed framework
converts time series data into functional data and uses classwise functional
PCA for feature extraction followed by classification using a Bayesian linear
classifier. We demonstrate the efficacy of our proposed method by applying it
to both synthetic data sets and real time series data from diverse fields
including but not limited to neuroscience, food science, medical sciences and
chemometrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chatterjee_A/0/1/0/all/0/1"&gt;Avishek Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mazumder_S/0/1/0/all/0/1"&gt;Satyaki Mazumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Das_K/0/1/0/all/0/1"&gt;Koel Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSAF: Multimodal Split Attention Fusion. (arXiv:2012.07175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07175</id>
        <link href="http://arxiv.org/abs/2012.07175"/>
        <updated>2021-06-29T01:55:16.924Z</updated>
        <summary type="html"><![CDATA[Multimodal learning mimics the reasoning process of the human multi-sensory
system, which is used to perceive the surrounding world. While making a
prediction, the human brain tends to relate crucial cues from multiple sources
of information. In this work, we propose a novel multimodal fusion module that
learns to emphasize more contributive features across all modalities.
Specifically, the proposed Multimodal Split Attention Fusion (MSAF) module
splits each modality into channel-wise equal feature blocks and creates a joint
representation that is used to generate soft attention for each channel across
the feature blocks. Further, the MSAF module is designed to be compatible with
features of various spatial dimensions and sequence lengths, suitable for both
CNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal
networks and utilize existing pretrained unimodal model weights. To demonstrate
the effectiveness of our fusion module, we design three multimodal networks
with MSAF for emotion recognition, sentiment analysis, and action recognition
tasks. Our approach achieves competitive results in each task and outperforms
other application-specific networks and multimodal fusion benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Lang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chuqing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guofa Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1"&gt;Dongpu Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior Flow Variational Autoencoder: A density estimation model for Non-Intrusive Load Monitoring. (arXiv:2011.14870v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14870</id>
        <link href="http://arxiv.org/abs/2011.14870"/>
        <updated>2021-06-29T01:55:16.900Z</updated>
        <summary type="html"><![CDATA[Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate
the power loads' appliance-by-appliance from the whole consumption measured by
a single meter. In this paper, we propose a conditional density estimation
model, based on deep neural networks, that joins a Conditional Variational
Autoencoder with a Conditional Invertible Normalizing Flow model to estimate
the individual appliance's power demand. The resulting model is called Prior
Flow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having
one model per appliance, the resulting model is responsible for estimating the
power demand, appliance-by-appliance, at once. We train and evaluate our
proposed model in a publicly available dataset composed of power demand
measures from a poultry feed factory located in Brazil. The proposed model's
quality is evaluated by comparing the obtained normalized disaggregation error
(NDE) and signal aggregated error (SAE) with the previous work values on the
same dataset. Our proposal achieves highly competitive results, and for six of
the eight machines belonging to the dataset, we observe consistent improvements
that go from 28% up to 81% in NDE and from 27% up to 86% in SAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1"&gt;Luis Felipe M.O. Henriques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_E/0/1/0/all/0/1"&gt;Eduardo Morgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1"&gt;Sergio Colcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1"&gt;Ruy Luiz Milidi&amp;#xfa;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Feasibility and Inevitability of Stealth Attacks. (arXiv:2106.13997v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.13997</id>
        <link href="http://arxiv.org/abs/2106.13997"/>
        <updated>2021-06-29T01:55:16.889Z</updated>
        <summary type="html"><![CDATA[We develop and study new adversarial perturbations that enable an attacker to
gain control over decisions in generic Artificial Intelligence (AI) systems
including deep learning neural networks. In contrast to adversarial data
modification, the attack mechanism we consider here involves alterations to the
AI system itself. Such a stealth attack could be conducted by a mischievous,
corrupt or disgruntled member of a software development team. It could also be
made by those wishing to exploit a "democratization of AI" agenda, where
network architectures and trained parameter sets are shared publicly. Building
on work by [Tyukin et al., International Joint Conference on Neural Networks,
2020], we develop a range of new implementable attack strategies with
accompanying analysis, showing that with high probability a stealth attack can
be made transparent, in the sense that system performance is unchanged on a
fixed validation set which is unknown to the attacker, while evoking any
desired output on a trigger input of interest. The attacker only needs to have
estimates of the size of the validation set and the spread of the AI's relevant
latent space. In the case of deep learning neural networks, we show that a one
neuron attack is possible - a modification to the weights and bias associated
with a single neuron - revealing a vulnerability arising from
over-parameterization. We illustrate these concepts in a realistic setting.
Guided by the theory and computational results, we also propose strategies to
guard against stealth attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1"&gt;Ivan Y. Tyukin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Higham_D/0/1/0/all/0/1"&gt;Desmond J. Higham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woldegeorgis_E/0/1/0/all/0/1"&gt;Eliyas Woldegeorgis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1"&gt;Alexander N. Gorban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring to establish an appropriate model for image aesthetic assessment via CNN-based RSRL: An empirical study. (arXiv:2106.03316v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03316</id>
        <link href="http://arxiv.org/abs/2106.03316"/>
        <updated>2021-06-29T01:55:16.876Z</updated>
        <summary type="html"><![CDATA[To establish an appropriate model for photo aesthetic assessment, in this
paper, a D-measure which reflects the disentanglement degree of the final layer
FC nodes of CNN is introduced. By combining F-measure with D-measure to obtain
a FD measure, an algorithm of determining the optimal model from the multiple
photo score prediction models generated by CNN-based repetitively self-revised
learning(RSRL) is proposed. Furthermore, the first fixation perspective(FFP)
and the assessment interest region(AIR) of the models are defined and
calculated. The experimental results show that the FD measure is effective for
establishing the appropriate model from the multiple score prediction models
with different CNN structures. Moreover, the FD-determined optimal models with
the comparatively high FD always have the FFP an AIR which are close to the
human's aesthetic perception when enjoying photos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Ying Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02800</id>
        <link href="http://arxiv.org/abs/2106.02800"/>
        <updated>2021-06-29T01:55:16.861Z</updated>
        <summary type="html"><![CDATA[Microaneurysms (MAs) are one of the earliest signs of diabetic retinopathy
(DR), a frequent complication of diabetes that can lead to visual impairment
and blindness. Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides
real-time retinal images with resolution down to 2 $\mu m$ and thus allows
detection of the morphologies of individual MAs, a potential marker that might
dictate MA pathology and affect the progression of DR. In contrast to the
numerous automatic models developed for assessing the number of MAs on fundus
photographs, currently there is no high throughput image protocol available for
automatic analysis of AOSLO photographs. To address this urgency, we introduce
AOSLO-net, a deep neural network framework with customized training policies to
automatically segment MAs from AOSLO images. We evaluate the performance of
AOSLO-net using 87 DR AOSLO images and our results demonstrate that the
proposed model outperforms the state-of-the-art segmentation model both in
accuracy and cost and enables correct MA morphological classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1"&gt;Konstantina Sampani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shengze Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yixiang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;He Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer K. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A multi-stage machine learning model on diagnosis of esophageal manometry. (arXiv:2106.13869v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13869</id>
        <link href="http://arxiv.org/abs/2106.13869"/>
        <updated>2021-06-29T01:55:16.802Z</updated>
        <summary type="html"><![CDATA[High-resolution manometry (HRM) is the primary procedure used to diagnose
esophageal motility disorders. Its interpretation and classification includes
an initial evaluation of swallow-level outcomes and then derivation of a
study-level diagnosis based on Chicago Classification (CC), using a tree-like
algorithm. This diagnostic approach on motility disordered using HRM was
mirrored using a multi-stage modeling framework developed using a combination
of various machine learning approaches. Specifically, the framework includes
deep-learning models at the swallow-level stage and feature-based machine
learning models at the study-level stage. In the swallow-level stage, three
models based on convolutional neural networks (CNNs) were developed to predict
swallow type, swallow pressurization, and integrated relaxation pressure (IRP).
At the study-level stage, model selection from families of the
expert-knowledge-based rule models, xgboost models and artificial neural
network(ANN) models were conducted, with the latter two model designed and
augmented with motivation from the export knowledge. A simple model-agnostic
strategy of model balancing motivated by Bayesian principles was utilized,
which gave rise to model averaging weighted by precision scores. The averaged
(blended) models and individual models were compared and evaluated, of which
the best performance on test dataset is 0.81 in top-1 prediction, 0.92 in top-2
predictions. This is the first artificial-intelligence-style model to
automatically predict CC diagnosis of HRM study from raw multi-swallow data.
Moreover, the proposed modeling framework could be easily extended to
multi-modal tasks, such as diagnosis of esophageal patients based on clinical
data from both HRM and functional luminal imaging probe panometry (FLIP).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kou_W/0/1/0/all/0/1"&gt;Wenjun Kou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlson_D/0/1/0/all/0/1"&gt;Dustin A. Carlson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumann_A/0/1/0/all/0/1"&gt;Alexandra J. Baumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donnan_E/0/1/0/all/0/1"&gt;Erica N. Donnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schauer_J/0/1/0/all/0/1"&gt;Jacob M. Schauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etemadi_M/0/1/0/all/0/1"&gt;Mozziyar Etemadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandolfino_J/0/1/0/all/0/1"&gt;John E. Pandolfino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[POLAR: A Polynomial Arithmetic Framework for Verifying Neural-Network Controlled Systems. (arXiv:2106.13867v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.13867</id>
        <link href="http://arxiv.org/abs/2106.13867"/>
        <updated>2021-06-29T01:55:16.795Z</updated>
        <summary type="html"><![CDATA[We propose POLAR, a \textbf{pol}ynomial \textbf{ar}ithmetic framework that
leverages polynomial overapproximations with interval remainders for
bounded-time reachability analysis of neural network-controlled systems
(NNCSs). Compared with existing arithmetic approaches that use standard Taylor
models, our framework uses a novel approach to iteratively overapproximate the
neuron output ranges layer-by-layer with a combination of Bernstein polynomial
interpolation for continuous activation functions and Taylor model arithmetic
for the other operations. This approach can overcome the main drawback in the
standard Taylor model arithmetic, i.e. its inability to handle functions that
cannot be well approximated by Taylor polynomials, and significantly improve
the accuracy and efficiency of reachable states computation for NNCSs. To
further tighten the overapproximation, our method keeps the Taylor model
remainders symbolic under the linear mappings when estimating the output range
of a neural network. We show that POLAR can be seamlessly integrated with
existing Taylor model flowpipe construction techniques, and demonstrate that
POLAR significantly outperforms the current state-of-the-art techniques on a
suite of benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiameng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Model-informed Precision Dosing with Expert-in-the-loop Machine Learning. (arXiv:2106.14384v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2106.14384</id>
        <link href="http://arxiv.org/abs/2106.14384"/>
        <updated>2021-06-29T01:55:16.783Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) and its applications have been transforming our lives
but it is also creating issues related to the development of fair, accountable,
transparent, and ethical Artificial Intelligence. As the ML models are not
fully comprehensible yet, it is obvious that we still need humans to be part of
algorithmic decision-making processes. In this paper, we consider a ML
framework that may accelerate model learning and improve its interpretability
by incorporating human experts into the model learning loop. We propose a novel
human-in-the-loop ML framework aimed at dealing with learning problems that the
cost of data annotation is high and the lack of appropriate data to model the
association between the target tasks and the input features. With an
application to precision dosing, our experimental results show that the
approach can learn interpretable rules from data and may potentially lower
experts' workload by replacing data annotation with rule representation
editing. The approach may also help remove algorithmic bias by introducing
experts' feedback into the iterative model learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yihuang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chiu_Y/0/1/0/all/0/1"&gt;Yi-Wen Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lin_M/0/1/0/all/0/1"&gt;Ming-Yen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Su_F/0/1/0/all/0/1"&gt;Fang-yi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Huang_S/0/1/0/all/0/1"&gt;Sheng-Tai Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Adversarial Supports in Few-shot Classifiers Using Self-Similarity and Filtering. (arXiv:2012.06330v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06330</id>
        <link href="http://arxiv.org/abs/2012.06330"/>
        <updated>2021-06-29T01:55:16.777Z</updated>
        <summary type="html"><![CDATA[Few-shot classifiers excel under limited training samples, making them useful
in applications with sparsely user-provided labels. Their unique relative
prediction setup offers opportunities for novel attacks, such as targeting
support sets required to categorise unseen test samples, which are not
available in other machine learning setups. In this work, we propose a
detection strategy to identify adversarial support sets, aimed at destroying
the understanding of a few-shot classifier for a certain class. We achieve this
by introducing the concept of self-similarity of a support set and by employing
filtering of supports. Our method is attack-agnostic, and we are the first to
explore adversarial detection for support sets of few-shot classifiers to the
best of our knowledge. Our evaluation of the miniImagenet (MI) and CUB datasets
exhibits good attack detection performance despite conceptual simplicity,
showing high AUROC scores. We show that self-similarity and filtering for
adversarial detection can be paired with other filtering functions,
constituting a generalisable concept.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yi Xiang Marcus Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chong_P/0/1/0/all/0/1"&gt;Penny Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1"&gt;Ngai-Man Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1"&gt;Yuval Elovici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Advantage Optimization for Model-Based Reinforcement Learning. (arXiv:2106.14080v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14080</id>
        <link href="http://arxiv.org/abs/2106.14080"/>
        <updated>2021-06-29T01:55:16.755Z</updated>
        <summary type="html"><![CDATA[Model-based Reinforcement Learning (MBRL) algorithms have been traditionally
designed with the goal of learning accurate dynamics of the environment. This
introduces a mismatch between the objectives of model-learning and the overall
learning problem of finding an optimal policy. Value-aware model learning, an
alternative model-learning paradigm to maximum likelihood, proposes to inform
model-learning through the value function of the learnt policy. While this
paradigm is theoretically sound, it does not scale beyond toy settings. In this
work, we propose a novel value-aware objective that is an upper bound on the
absolute performance difference of a policy across two models. Further, we
propose a general purpose algorithm that modifies the standard MBRL pipeline --
enabling learning with value aware objectives. Our proposed objective, in
conjunction with this algorithm, is the first successful instantiation of
value-aware MBRL on challenging continuous control environments, outperforming
previous value-aware objectives and with competitive performance w.r.t.
MLE-based MBRL approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Modhe_N/0/1/0/all/0/1"&gt;Nirbhay Modhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamath_H/0/1/0/all/0/1"&gt;Harish Kamath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1"&gt;Dhruv Batra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1"&gt;Ashwin Kalyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransBTS: Multimodal Brain Tumor Segmentation Using Transformer. (arXiv:2103.04430v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04430</id>
        <link href="http://arxiv.org/abs/2103.04430"/>
        <updated>2021-06-29T01:55:16.749Z</updated>
        <summary type="html"><![CDATA[Transformer, which can benefit from global (long-range) information modeling
using self-attention mechanisms, has been successful in natural language
processing and 2D image classification recently. However, both local and global
features are crucial for dense prediction tasks, especially for 3D medical
image segmentation. In this paper, we for the first time exploit Transformer in
3D CNN for MRI Brain Tumor Segmentation and propose a novel network named
TransBTS based on the encoder-decoder structure. To capture the local 3D
context information, the encoder first utilizes 3D CNN to extract the
volumetric spatial feature maps. Meanwhile, the feature maps are reformed
elaborately for tokens that are fed into Transformer for global feature
modeling. The decoder leverages the features embedded by Transformer and
performs progressive upsampling to predict the detailed segmentation map.
Extensive experimental results on both BraTS 2019 and 2020 datasets show that
TransBTS achieves comparable or higher results than previous state-of-the-art
3D methods for brain tumor segmentation on 3D MRI scans. The source code is
available at https://github.com/Wenxuan-1119/TransBTS]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Meng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiangyun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1"&gt;Sen Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matching Point Sets with Quantum Circuit Learning. (arXiv:2102.06697v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06697</id>
        <link href="http://arxiv.org/abs/2102.06697"/>
        <updated>2021-06-29T01:55:16.741Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a parameterised quantum circuit learning approach to
point set matching problem. In contrast to previous annealing-based methods, we
propose a quantum circuit-based framework whose parameters are optimised via
descending the gradients w.r.t a kernel-based loss function. We formulate the
shape matching problem into a distribution learning task; that is, to learn the
distribution of the optimal transformation parameters. We show that this
framework is able to find multiple optimal solutions for symmetric shapes and
is more accurate, scalable and robust than the previous annealing-based method.
Code, data and pre-trained weights are available at the project page:
\href{https://hansen7.github.io/qKC}{https://hansen7.github.io/qKC}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noormandipour_M/0/1/0/all/0/1"&gt;Mohammadreza Noormandipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hanchen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Saturation in Layerwise Quantum Approximate Optimisation. (arXiv:2106.13814v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.13814</id>
        <link href="http://arxiv.org/abs/2106.13814"/>
        <updated>2021-06-29T01:55:16.734Z</updated>
        <summary type="html"><![CDATA[Quantum Approximate Optimisation (QAOA) is the most studied gate based
variational quantum algorithm today. We train QAOA one layer at a time to
maximize overlap with an $n$ qubit target state. Doing so we discovered that
such training always saturates -- called \textit{training saturation} -- at
some depth $p^*$, meaning that past a certain depth, overlap can not be
improved by adding subsequent layers. We formulate necessary conditions for
saturation. Numerically, we find layerwise QAOA reaches its maximum overlap at
depth $p^*=n$. The addition of coherent dephasing errors to training removes
saturation, recovering robustness to layerwise training. This study sheds new
light on the performance limitations and prospects of QAOA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Campos_E/0/1/0/all/0/1"&gt;E. Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Rabinovich_D/0/1/0/all/0/1"&gt;D. Rabinovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Akshay_V/0/1/0/all/0/1"&gt;V. Akshay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Biamonte_J/0/1/0/all/0/1"&gt;J. Biamonte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonparametric estimation of continuous DPPs with kernel methods. (arXiv:2106.14210v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14210</id>
        <link href="http://arxiv.org/abs/2106.14210"/>
        <updated>2021-06-29T01:55:16.725Z</updated>
        <summary type="html"><![CDATA[Determinantal Point Process (DPPs) are statistical models for repulsive point
patterns. Both sampling and inference are tractable for DPPs, a rare feature
among models with negative dependence that explains their popularity in machine
learning and spatial statistics. Parametric and nonparametric inference methods
have been proposed in the finite case, i.e. when the point patterns live in a
finite ground set. In the continuous case, only parametric methods have been
investigated, while nonparametric maximum likelihood for DPPs -- an
optimization problem over trace-class operators -- has remained an open
question. In this paper, we show that a restricted version of this maximum
likelihood (MLE) problem falls within the scope of a recent representer theorem
for nonnegative functions in an RKHS. This leads to a finite-dimensional
problem, with strong statistical ties to the original MLE. Moreover, we
propose, analyze, and demonstrate a fixed point algorithm to solve this
finite-dimensional problem. Finally, we also provide a controlled estimate of
the correlation kernel of the DPP, thus providing more interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fanuel_M/0/1/0/all/0/1"&gt;Micha&amp;#xeb;l Fanuel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bardenet_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Bardenet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning via Inter-Task Synaptic Mapping. (arXiv:2106.13954v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13954</id>
        <link href="http://arxiv.org/abs/2106.13954"/>
        <updated>2021-06-29T01:55:16.709Z</updated>
        <summary type="html"><![CDATA[Learning from streaming tasks leads a model to catastrophically erase unique
experiences it absorbs from previous episodes. While regularization techniques
such as LWF, SI, EWC have proven themselves as an effective avenue to overcome
this issue by constraining important parameters of old tasks from changing when
accepting new concepts, these approaches do not exploit common information of
each task which can be shared to existing neurons. As a result, they do not
scale well to large-scale problems since the parameter importance variables
quickly explode. An Inter-Task Synaptic Mapping (ISYANA) is proposed here to
underpin knowledge retention for continual learning. ISYANA combines
task-to-neuron relationship as well as concept-to-concept relationship such
that it prevents a neuron to embrace distinct concepts while merely accepting
relevant concept. Numerical study in the benchmark continual learning problems
has been carried out followed by comparison against prominent continual
learning algorithms. ISYANA exhibits competitive performance compared to state
of the arts. Codes of ISYANA is made available in
\url{https://github.com/ContinualAL/ISYANAKBS}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fubing_M/0/1/0/all/0/1"&gt;Mao Fubing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiwei_W/0/1/0/all/0/1"&gt;Weng Weiwei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1"&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yee_E/0/1/0/all/0/1"&gt;Edward Yapp Kien Yee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revelio: ML-Generated Debugging Queries for Distributed Systems. (arXiv:2106.14347v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.14347</id>
        <link href="http://arxiv.org/abs/2106.14347"/>
        <updated>2021-06-29T01:55:16.703Z</updated>
        <summary type="html"><![CDATA[A major difficulty in debugging distributed systems lies in manually
determining which of the many available debugging tools to use and how to query
its logs. Our own study of a production debugging workflow confirms the
magnitude of this burden. This paper explores whether a machine-learning model
can assist developers in distributed systems debugging. We present Revelio, a
debugging assistant which takes user reports and system logs as input, and
outputs debugging queries that developers can use to find a bug's root cause.
The key challenges lie in (1) combining inputs of different types (e.g.,
natural language reports and quantitative logs) and (2) generalizing to unseen
faults. Revelio addresses these by employing deep neural networks to uniformly
embed diverse input sources and potential queries into a high-dimensional
vector space. In addition, it exploits observations from production systems to
factorize query generation into two computationally and statistically simpler
learning tasks. To evaluate Revelio, we built a testbed with multiple
distributed applications and debugging tools. By injecting faults and training
on logs and reports from 800 Mechanical Turkers, we show that Revelio includes
the most helpful query in its predicted list of top-3 relevant queries 96% of
the time. Our developer study confirms the utility of Revelio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dogga_P/0/1/0/all/0/1"&gt;Pradeep Dogga&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Sivaraman_A/0/1/0/all/0/1"&gt;Anirudh Sivaraman&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Saini_S/0/1/0/all/0/1"&gt;Shiv Kumar Saini&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Varghese_G/0/1/0/all/0/1"&gt;George Varghese&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1"&gt;Ravi Netravali&lt;/a&gt; (2) ((1) UCLA, (2) Princeton University, (3) NYU, (4) Adobe Research, India)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Longitudinal Self-Supervised Learning. (arXiv:2006.06930v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06930</id>
        <link href="http://arxiv.org/abs/2006.06930"/>
        <updated>2021-06-29T01:55:16.698Z</updated>
        <summary type="html"><![CDATA[Machine learning analysis of longitudinal neuroimaging data is typically
based on supervised learning, which requires a large number of ground-truth
labels to be informative. As ground-truth labels are often missing or expensive
to obtain in neuroscience, we avoid them in our analysis by combing factor
disentanglement with self-supervised learning to identify changes and
consistencies across the multiple MRIs acquired of each individual over time.
Specifically, we propose a new definition of disentanglement by formulating a
multivariate mapping between factors (e.g., brain age) associated with an MRI
and a latent image representation. Then, factors that evolve across
acquisitions of longitudinal sequences are disentangled from that mapping by
self-supervised learning in such a way that changes in a single factor induce
change along one direction in the representation space. We implement this
model, named Longitudinal Self-Supervised Learning (LSSL), via a standard
autoencoding structure with a cosine loss to disentangle brain age from the
image representation. We apply LSSL to two longitudinal neuroimaging studies to
highlight its strength in extracting the brain-age information from MRI and
revealing informative characteristics associated with neurodegenerative and
neuropsychological disorders. Moreover, the representations learned by LSSL
facilitate supervised classification by recording faster convergence and higher
(or similar) prediction accuracy compared to several other representation
learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zixuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers. (arXiv:2106.13870v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13870</id>
        <link href="http://arxiv.org/abs/2106.13870"/>
        <updated>2021-06-29T01:55:16.691Z</updated>
        <summary type="html"><![CDATA[We propose a method to estimate the uncertainty of the outcome of an image
classifier on a given input datum. Deep neural networks commonly used for image
classification are deterministic maps from an input image to an output class.
As such, their outcome on a given datum involves no uncertainty, so we must
specify what variability we are referring to when defining, measuring and
interpreting "confidence." To this end, we introduce the Wellington Posterior,
which is the distribution of outcomes that would have been obtained in response
to data that could have been generated by the same scene that produced the
given image. Since there are infinitely many scenes that could have generated
the given image, the Wellington Posterior requires induction from scenes other
than the one portrayed. We explore alternate methods using data augmentation,
ensembling, and model linearization. Additional alternatives include generative
adversarial networks, conditional prior networks, and supervised single-view
reconstruction. We test these alternatives against the empirical posterior
obtained by inferring the class of temporally adjacent frames in a video. These
developments are only a small step towards assessing the reliability of deep
network classifiers in a manner that is compatible with safety-critical
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1"&gt;Stephanie Tsuei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1"&gt;Aditya Golatkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Legendre Deep Neural Network (LDNN) and its application for approximation of nonlinear Volterra Fredholm Hammerstein integral equations. (arXiv:2106.14320v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.14320</id>
        <link href="http://arxiv.org/abs/2106.14320"/>
        <updated>2021-06-29T01:55:16.685Z</updated>
        <summary type="html"><![CDATA[Various phenomena in biology, physics, and engineering are modeled by
differential equations. These differential equations including partial
differential equations and ordinary differential equations can be converted and
represented as integral equations. In particular, Volterra Fredholm Hammerstein
integral equations are the main type of these integral equations and
researchers are interested in investigating and solving these equations. In
this paper, we propose Legendre Deep Neural Network (LDNN) for solving
nonlinear Volterra Fredholm Hammerstein integral equations (VFHIEs). LDNN
utilizes Legendre orthogonal polynomials as activation functions of the Deep
structure. We present how LDNN can be used to solve nonlinear VFHIEs. We show
using the Gaussian quadrature collocation method in combination with LDNN
results in a novel numerical solution for nonlinear VFHIEs. Several examples
are given to verify the performance and accuracy of LDNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hajimohammadi_Z/0/1/0/all/0/1"&gt;Zeinab Hajimohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Parand_K/0/1/0/all/0/1"&gt;Kourosh Parand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04830</id>
        <link href="http://arxiv.org/abs/2012.04830"/>
        <updated>2021-06-29T01:55:16.671Z</updated>
        <summary type="html"><![CDATA[Cataract is one of the leading causes of reversible visual impairment and
blindness globally. Over the years, researchers have achieved significant
progress in developing state-of-the-art artificial intelligence techniques for
automatic cataract classification and grading, helping clinicians prevent and
treat cataract in time. This paper provides a comprehensive survey of recent
advances in machine learning for cataract classification and grading based on
ophthalmic images. We summarize existing literature from two research
directions: conventional machine learning techniques and deep learning
techniques. This paper also provides insights into existing works of both
merits and limitations. In addition, we discuss several challenges of automatic
cataract classification and grading based on machine learning techniques and
present possible solutions to these challenges for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiansheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zunjie Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1"&gt;Risa Higashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Truncated Gaussian-Mixture Variational AutoEncoder. (arXiv:1902.03717v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.03717</id>
        <link href="http://arxiv.org/abs/1902.03717"/>
        <updated>2021-06-29T01:55:16.665Z</updated>
        <summary type="html"><![CDATA[Variation Autoencoder (VAE) has become a powerful tool in modeling the
non-linear generative process of data from a low-dimensional latent space.
Recently, several studies have proposed to use VAE for unsupervised clustering
by using mixture models to capture the multi-modal structure of latent
representations. This strategy, however, is ineffective when there are outlier
data samples whose latent representations are meaningless, yet contaminating
the estimation of key major clusters in the latent space. This exact problem
arises in the context of resting-state fMRI (rs-fMRI) analysis, where
clustering major functional connectivity patterns is often hindered by heavy
noise of rs-fMRI and many minor clusters (rare connectivity patterns) of no
interest to analysis. In this paper we propose a novel generative process, in
which we use a Gaussian-mixture to model a few major clusters in the data, and
use a non-informative uniform distribution to capture the remaining data. We
embed this truncated Gaussian-Mixture model in a Variational AutoEncoder
framework to obtain a general joint clustering and outlier detection approach,
called tGM-VAE. We demonstrated the applicability of tGM-VAE on the MNIST
dataset and further validated it in the context of rs-fMRI connectivity
analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honnorat_N/0/1/0/all/0/1"&gt;Nicolas Honnorat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Good and Bad Optimization Models: Insights from Rockafellians. (arXiv:2105.06073v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06073</id>
        <link href="http://arxiv.org/abs/2105.06073"/>
        <updated>2021-06-29T01:55:16.660Z</updated>
        <summary type="html"><![CDATA[A basic requirement for a mathematical model is often that its solution
(output) shouldn't change much if the model's parameters (input) are perturbed.
This is important because the exact values of parameters may not be known and
one would like to avoid being mislead by an output obtained using incorrect
values. Thus, it's rarely enough to address an application by formulating a
model, solving the resulting optimization problem and presenting the solution
as the answer. One would need to confirm that the model is suitable, i.e.,
"good," and this can, at least in part, be achieved by considering a family of
optimization problems constructed by perturbing parameters of concern. The
resulting sensitivity analysis uncovers troubling situations with unstable
solutions, which we referred to as "bad" models, and indicates better model
formulations. Embedding an actual problem of interest within a family of
problems is also a primary path to optimality conditions as well as
computationally attractive, alternative problems, which under ideal
circumstances, and when properly tuned, may even furnish the minimum value of
the actual problem. The tuning of these alternative problems turns out to be
intimately tied to finding multipliers in optimality conditions and thus
emerges as a main component of several optimization algorithms. In fact, the
tuning amounts to solving certain dual optimization problems. In this tutorial,
we'll discuss the opportunities and insights afforded by this broad
perspective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1"&gt;Johannes O. Royset&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus Spike Proteins. (arXiv:2105.00351v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00351</id>
        <link href="http://arxiv.org/abs/2105.00351"/>
        <updated>2021-06-29T01:55:16.652Z</updated>
        <summary type="html"><![CDATA[Topological data analysis, including persistent homology, has undergone
significant development in recent years. However, one outstanding challenge is
to build a coherent statistical inference procedure on persistent diagrams. The
paired dependent data structure, which are the births and deaths in persistent
diagrams, adds complexity to statistical inference. In this paper, we present a
new lattice path representation for persistent diagrams. A new exact
statistical inference procedure is developed for lattice paths via
combinatorial enumerations. The proposed lattice path method is applied to
study the topological characterization of the protein structures of the
COVID-19 virus. We demonstrate that there are topological changes during the
conformational change of spike proteins, a necessary step in infecting host
cells.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1"&gt;Moo K. Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1"&gt;Hernando Ombao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Generation of Continuous Images. (arXiv:2011.12026v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12026</id>
        <link href="http://arxiv.org/abs/2011.12026"/>
        <updated>2021-06-29T01:55:16.644Z</updated>
        <summary type="html"><![CDATA[In most existing learning systems, images are typically viewed as 2D pixel
arrays. However, in another paradigm gaining popularity, a 2D image is
represented as an implicit neural representation (INR) - an MLP that predicts
an RGB pixel value given its (x,y) coordinate. In this paper, we propose two
novel architectural techniques for building INR-based image decoders:
factorized multiplicative modulation and multi-scale INRs, and use them to
build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs
for image generation were limited to MNIST-like datasets and do not scale to
complex real-world data. Our proposed INR-GAN architecture improves the
performance of continuous image generators by several times, greatly reducing
the gap between continuous image GANs and pixel-based ones. Apart from that, we
explore several exciting properties of the INR-based decoders, like
out-of-the-box superresolution, meaningful image-space interpolation,
accelerated inference of low-resolution images, an ability to extrapolate
outside of image boundaries, and strong geometric prior. The project page is
located at https://universome.github.io/inr-gan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1"&gt;Ivan Skorokhodov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ignatyev_S/0/1/0/all/0/1"&gt;Savva Ignatyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1"&gt;Mohamed Elhoseiny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Learning for Human Mobility. (arXiv:2012.02825v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02825</id>
        <link href="http://arxiv.org/abs/2012.02825"/>
        <updated>2021-06-29T01:55:16.623Z</updated>
        <summary type="html"><![CDATA[The study of human mobility is crucial due to its impact on several aspects
of our society, such as disease spreading, urban planning, well-being,
pollution, and more. The proliferation of digital mobility data, such as phone
records, GPS traces, and social media posts, combined with the predictive power
of artificial intelligence, triggered the application of deep learning to human
mobility. Existing surveys focus on single tasks, data sources, mechanistic or
traditional machine learning approaches, while a comprehensive description of
deep learning solutions is missing. This survey provides a taxonomy of mobility
tasks, a discussion on the challenges related to each task and how deep
learning may overcome the limitations of traditional models, a description of
the most relevant solutions to the mobility tasks described above and the
relevant challenges for the future. Our survey is a guide to the leading deep
learning solutions to next-location prediction, crowd flow prediction,
trajectory generation, and flow generation. At the same time, it helps deep
learning scientists and practitioners understand the fundamental concepts and
the open challenges of the study of human mobility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luca_M/0/1/0/all/0/1"&gt;Massimiliano Luca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barlacchi_G/0/1/0/all/0/1"&gt;Gianni Barlacchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1"&gt;Bruno Lepri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappalardo_L/0/1/0/all/0/1"&gt;Luca Pappalardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaptCL: Efficient Collaborative Learning with Dynamic and Adaptive Pruning. (arXiv:2106.14126v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14126</id>
        <link href="http://arxiv.org/abs/2106.14126"/>
        <updated>2021-06-29T01:55:16.616Z</updated>
        <summary type="html"><![CDATA[In multi-party collaborative learning, the parameter server sends a global
model to each data holder for local training and then aggregates committed
models globally to achieve privacy protection. However, both the dragger issue
of synchronous collaborative learning and the staleness issue of asynchronous
collaborative learning make collaborative learning inefficient in real-world
heterogeneous environments. We propose a novel and efficient collaborative
learning framework named AdaptCL, which generates an adaptive sub-model
dynamically from the global base model for each data holder, without any prior
information about worker capability. All workers (data holders) achieve
approximately identical update time as the fastest worker by equipping them
with capability-adapted pruned models. Thus the training process can be
dramatically accelerated. Besides, we tailor the efficient pruned rate learning
algorithm and pruning approach for AdaptCL. Meanwhile, AdaptCL provides a
mechanism for handling the trade-off between accuracy and time overhead and can
be combined with other techniques to accelerate training further. Empirical
results show that AdaptCL introduces little computing and communication
overhead. AdaptCL achieves time savings of more than 41\% on average and
improves accuracy in a low heterogeneous environment. In a highly heterogeneous
environment, AdaptCL achieves a training speedup of 6.2x with a slight loss of
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Guangmeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yi Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Regression via Model Based Methods. (arXiv:2106.10759v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10759</id>
        <link href="http://arxiv.org/abs/2106.10759"/>
        <updated>2021-06-29T01:55:16.608Z</updated>
        <summary type="html"><![CDATA[The mean squared error loss is widely used in many applications, including
auto-encoders, multi-target regression, and matrix factorization, to name a
few. Despite computational advantages due to its differentiability, it is not
robust to outliers. In contrast, l_p norms are known to be robust, but cannot
be optimized via, e.g., stochastic gradient descent, as they are
non-differentiable. We propose an algorithm inspired by so-called model-based
optimization (MBO) [35, 36], which replaces a non-convex objective with a
convex model function and alternates between optimizing the model function and
updating the solution. We apply this to robust regression, proposing SADM, a
stochastic variant of the Online Alternating Direction Method of Multipliers
(OADM) [50] to solve the inner optimization in MBO. We show that SADM converges
with the rate O(log T/T). Finally, we demonstrate experimentally (a) the
robustness of l_p norms to outliers and (b) the efficiency of our proposed
model-based algorithms in comparison with gradient methods on autoencoders and
multi-target regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moharrer_A/0/1/0/all/0/1"&gt;Armin Moharrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_K/0/1/0/all/0/1"&gt;Khashayar Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_E/0/1/0/all/0/1"&gt;Edmund Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1"&gt;Stratis Ioannidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error analysis for physics informed neural networks (PINNs) approximating Kolmogorov PDEs. (arXiv:2106.14473v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.14473</id>
        <link href="http://arxiv.org/abs/2106.14473"/>
        <updated>2021-06-29T01:55:16.603Z</updated>
        <summary type="html"><![CDATA[Physics informed neural networks approximate solutions of PDEs by minimizing
pointwise residuals. We derive rigorous bounds on the error, incurred by PINNs
in approximating the solutions of a large class of linear parabolic PDEs,
namely Kolmogorov equations that include the heat equation and Black-Scholes
equation of option pricing, as examples. We construct neural networks, whose
PINN residual (generalization error) can be made as small as desired. We also
prove that the total $L^2$-error can be bounded by the generalization error,
which in turn is bounded in terms of the training error, provided that a
sufficient number of randomly chosen training (collocation) points is used.
Moreover, we prove that the size of the PINNs and the number of training
samples only grow polynomially with the underlying dimension, enabling PINNs to
overcome the curse of dimensionality in this context. These results enable us
to provide a comprehensive error analysis for PINNs in approximating Kolmogorov
PDEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ryck_T/0/1/0/all/0/1"&gt;Tim De Ryck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Siddhartha Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing numerical precision preserves classification accuracy in Mondrian Forests. (arXiv:2106.14340v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14340</id>
        <link href="http://arxiv.org/abs/2106.14340"/>
        <updated>2021-06-29T01:55:16.597Z</updated>
        <summary type="html"><![CDATA[Mondrian Forests are a powerful data stream classification method, but their
large memory footprint makes them ill-suited for low-resource platforms such as
connected objects. We explored using reduced-precision floating-point
representations to lower memory consumption and evaluated its effect on
classification performance. We applied the Mondrian Forest implementation
provided by OrpailleCC, a C++ collection of data stream algorithms, to two
canonical datasets in human activity recognition: Recofit and Banos \emph{et
al}. Results show that the precision of floating-point values used by tree
nodes can be reduced from 64 bits to 8 bits with no significant difference in
F1 score. In some cases, reduced precision was shown to improve classification
performance, presumably due to its regularization effect. We conclude that
numerical precision is a relevant hyperparameter in the Mondrian Forest, and
that commonly-used double precision values may not be necessary for optimal
performance. Future work will evaluate the generalizability of these findings
to other data stream classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vicuna_M/0/1/0/all/0/1"&gt;Marc Vicuna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khannouz_M/0/1/0/all/0/1"&gt;Martin Khannouz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiar_G/0/1/0/all/0/1"&gt;Gregory Kiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatelain_Y/0/1/0/all/0/1"&gt;Yohan Chatelain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glatard_T/0/1/0/all/0/1"&gt;Tristan Glatard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images. (arXiv:2106.14207v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14207</id>
        <link href="http://arxiv.org/abs/2106.14207"/>
        <updated>2021-06-29T01:55:16.567Z</updated>
        <summary type="html"><![CDATA[Diabetes foot ulceration (DFU) and amputation are a cause of significant
morbidity. The prevention of DFU may be achieved by the identification of
patients at risk of DFU and the institution of preventative measures through
education and offloading. Several studies have reported that thermogram images
may help to detect an increase in plantar temperature prior to DFU. However,
the distribution of plantar temperature may be heterogeneous, making it
difficult to quantify and utilize to predict outcomes. We have compared a
machine learning-based scoring technique with feature selection and
optimization techniques and learning classifiers to several state-of-the-art
Convolutional Neural Networks (CNNs) on foot thermogram images and propose a
robust solution to identify the diabetic foot. A comparatively shallow CNN
model, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram
image-based classification and the AdaBoost Classifier used 10 features and
achieved an F1 score of 97 %. A comparison of the inference time for the
best-performing networks confirmed that the proposed algorithm can be deployed
as a smartphone application to allow the user to monitor the progression of the
DFU in a home setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1"&gt;Amith Khandakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Reaz_M/0/1/0/all/0/1"&gt;Mamun Bin Ibne Reaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sawal Hamid Md Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Md Anwarul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tawsifur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alfkey_R/0/1/0/all/0/1"&gt;Rashad Alfkey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bakar_A/0/1/0/all/0/1"&gt;Ahmad Ashrif A. Bakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Malik_R/0/1/0/all/0/1"&gt;Rayaz A. Malik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Exhaustive Learning Using Gaussian Mixture Generative Adversarial Networks. (arXiv:2106.14344v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14344</id>
        <link href="http://arxiv.org/abs/2106.14344"/>
        <updated>2021-06-29T01:55:16.551Z</updated>
        <summary type="html"><![CDATA[Supervised learning, while deployed in real-life scenarios, often encounters
instances of unknown classes. Conventional algorithms for training a supervised
learning model do not provide an option to detect such instances, so they
miss-classify such instances with 100% probability. Open Set Recognition (OSR)
and Non-Exhaustive Learning (NEL) are potential solutions to overcome this
problem. Most existing methods of OSR first classify members of existing
classes and then identify instances of new classes. However, many of the
existing methods of OSR only makes a binary decision, i.e., they only identify
the existence of the unknown class. Hence, such methods cannot distinguish test
instances belonging to incremental unseen classes. On the other hand, the
majority of NEL methods often make a parametric assumption over the data
distribution, which either fail to return good results, due to the reason that
real-life complex datasets may not follow a well-known data distribution. In
this paper, we propose a new online non-exhaustive learning model, namely,
Non-Exhaustive Gaussian Mixture Generative Adversarial Networks (NE-GM-GAN) to
address these issues. Our proposed model synthesizes Gaussian mixture based
latent representation over a deep generative model, such as GAN, for
incremental detection of instances of emerging classes in the test data.
Extensive experimental results on several benchmark datasets show that
NE-GM-GAN significantly outperforms the state-of-the-art methods in detecting
instances of novel classes in streaming data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jun Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Mohammad Al Hasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Detection of Abnormalities from an EEG Recording of Epilepsy Patients With a Compact Convolutional Neural Network. (arXiv:2105.10358v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10358</id>
        <link href="http://arxiv.org/abs/2105.10358"/>
        <updated>2021-06-29T01:55:16.545Z</updated>
        <summary type="html"><![CDATA[Electroencephalography (EEG) is essential for the diagnosis of epilepsy, but
it requires expertise and experience to identify abnormalities. It is thus
crucial to develop automated models for the detection of abnormalities in EEGs
related to epilepsy. This paper describes the development of a novel class of
compact convolutional neural networks (CNNs) for detecting abnormal patterns
and electrodes in EEGs for epilepsy. The designed model is inspired by a CNN
developed for brain-computer interfacing called multichannel EEGNet (mEEGNet).
Unlike the EEGNet, the proposed model, mEEGNet, has the same number of
electrode inputs and outputs to detect abnormal patterns. The mEEGNet was
evaluated with a clinical dataset consisting of 29 cases of juvenile and
childhood absence epilepsy labeled by a clinical expert. The labels were given
to paroxysmal discharges visually observed in both ictal (seizure) and
interictal (nonseizure) durations. Results showed that the mEEGNet detected
abnormalities with the area under the curve, F1-values, and sensitivity
equivalent to or higher than those of existing CNNs. Moreover, the number of
parameters is much smaller than other CNN models. To our knowledge, the dataset
of absence epilepsy validated with machine learning through this research is
the largest in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shoji_T/0/1/0/all/0/1"&gt;Taku Shoji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoshida_N/0/1/0/all/0/1"&gt;Noboru Yoshida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tanaka_T/0/1/0/all/0/1"&gt;Toshihisa Tanaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An On-Device Federated Learning Approach for Cooperative Model Update between Edge Devices. (arXiv:2002.12301v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.12301</id>
        <link href="http://arxiv.org/abs/2002.12301"/>
        <updated>2021-06-29T01:55:16.531Z</updated>
        <summary type="html"><![CDATA[Most edge AI focuses on prediction tasks on resource-limited edge devices
while the training is done at server machines. However, retraining or
customizing a model is required at edge devices as the model is becoming
outdated due to environmental changes over time. To follow such a concept
drift, a neural-network based on-device learning approach is recently proposed,
so that edge devices train incoming data at runtime to update their model. In
this case, since a training is done at distributed edge devices, the issue is
that only a limited amount of training data can be used for each edge device.
To address this issue, one approach is a cooperative learning or federated
learning, where edge devices exchange their trained results and update their
model by using those collected from the other devices. In this paper, as an
on-device learning algorithm, we focus on OS-ELM (Online Sequential Extreme
Learning Machine) to sequentially train a model based on recent samples and
combine it with autoencoder for anomaly detection. We extend it for an
on-device federated learning so that edge devices can exchange their trained
results and update their model by using those collected from the other edge
devices. This cooperative model update is one-shot while it can be repeatedly
applied to synchronize their model. Our approach is evaluated with anomaly
detection tasks generated from a driving dataset of cars, a human activity
dataset, and MNIST dataset. The results demonstrate that the proposed on-device
federated learning can produce a merged model by integrating trained results
from multiple edge devices as accurately as traditional backpropagation based
neural networks and a traditional federated learning approach with lower
computation or communication cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ito_R/0/1/0/all/0/1"&gt;Rei Ito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsukada_M/0/1/0/all/0/1"&gt;Mineto Tsukada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1"&gt;Hiroki Matsutani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DGL-LifeSci: An Open-Source Toolkit for Deep Learning on Graphs in Life Science. (arXiv:2106.14232v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14232</id>
        <link href="http://arxiv.org/abs/2106.14232"/>
        <updated>2021-06-29T01:55:16.516Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) constitute a class of deep learning methods for
graph data. They have wide applications in chemistry and biology, such as
molecular property prediction, reaction prediction and drug-target interaction
prediction. Despite the interest, GNN-based modeling is challenging as it
requires graph data pre-processing and modeling in addition to programming and
deep learning. Here we present DGL-LifeSci, an open-source package for deep
learning on graphs in life science. DGL-LifeSci is a python toolkit based on
RDKit, PyTorch and Deep Graph Library (DGL). DGL-LifeSci allows GNN-based
modeling on custom datasets for molecular property prediction, reaction
prediction and molecule generation. With its command-line interfaces, users can
perform modeling without any background in programming and deep learning. We
test the command-line interfaces using standard benchmarks MoleculeNet, USPTO,
and ZINC. Compared with previous implementations, DGL-LifeSci achieves a speed
up by up to 6x. For modeling flexibility, DGL-LifeSci provides well-optimized
modules for various stages of the modeling pipeline. In addition, DGL-LifeSci
provides pre-trained models for reproducing the test experiment results and
applying models without training. The code is distributed under an Apache-2.0
License and is freely accessible at https://github.com/awslabs/dgl-lifesci.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mufei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinjing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jiajing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1"&gt;Wenxuan Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yangkang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yaxin Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1"&gt;George Karypis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lambda Learner: Fast Incremental Learning on Data Streams. (arXiv:2010.05154v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05154</id>
        <link href="http://arxiv.org/abs/2010.05154"/>
        <updated>2021-06-29T01:55:16.476Z</updated>
        <summary type="html"><![CDATA[One of the most well-established applications of machine learning is in
deciding what content to show website visitors. When observation data comes
from high-velocity, user-generated data streams, machine learning methods
perform a balancing act between model complexity, training time, and
computational costs. Furthermore, when model freshness is critical, the
training of models becomes time-constrained. Parallelized batch offline
training, although horizontally scalable, is often not time-considerate or
cost-effective. In this paper, we propose Lambda Learner, a new framework for
training models by incremental updates in response to mini-batches from data
streams. We show that the resulting model of our framework closely estimates a
periodically updated model trained on offline data and outperforms it when
model updates are time-sensitive. We provide theoretical proof that the
incremental learning updates improve the loss-function over a stale batch
model. We present a large-scale deployment on the sponsored content platform
for a large social network, serving hundreds of millions of users across
different channels (e.g., desktop, mobile). We address challenges and
complexities from both algorithms and infrastructure perspectives, and
illustrate the system details for computation, storage, and streaming
production of training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1"&gt;Rohan Ramanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salomatin_K/0/1/0/all/0/1"&gt;Konstantin Salomatin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1"&gt;Jeffrey D. Gee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talanine_K/0/1/0/all/0/1"&gt;Kirill Talanine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalal_O/0/1/0/all/0/1"&gt;Onkar Dalal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polatkan_G/0/1/0/all/0/1"&gt;Gungor Polatkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smoot_S/0/1/0/all/0/1"&gt;Sara Smoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1"&gt;Deepak Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[R-Drop: Regularized Dropout for Neural Networks. (arXiv:2106.14448v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14448</id>
        <link href="http://arxiv.org/abs/2106.14448"/>
        <updated>2021-06-29T01:55:16.470Z</updated>
        <summary type="html"><![CDATA[Dropout is a powerful and widely used technique to regularize the training of
deep neural networks. In this paper, we introduce a simple regularization
strategy upon dropout in model training, namely R-Drop, which forces the output
distributions of different sub models generated by dropout to be consistent
with each other. Specifically, for each training sample, R-Drop minimizes the
bidirectional KL-divergence between the output distributions of two sub models
sampled by dropout. Theoretical analysis reveals that R-Drop reduces the
freedom of the model parameters and complements dropout. Experiments on
$\bf{5}$ widely used deep learning tasks ($\bf{18}$ datasets in total),
including neural machine translation, abstractive summarization, language
understanding, language modeling, and image classification, show that R-Drop is
universally effective. In particular, it yields substantial improvements when
applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large,
and BART, and achieves state-of-the-art (SOTA) performances with the vanilla
Transformer model on WMT14 English$\to$German translation ($\bf{30.91}$ BLEU)
and WMT14 English$\to$French translation ($\bf{43.95}$ BLEU), even surpassing
models trained with extra large-scale data and expert-designed advanced
variants of Transformer models. Our code is available at
GitHub{\url{https://github.com/dropreg/R-Drop}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaobo Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lijun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juntao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Center Smoothing: Provable Robustness for Functions with Metric-Space Outputs. (arXiv:2102.09701v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09701</id>
        <link href="http://arxiv.org/abs/2102.09701"/>
        <updated>2021-06-29T01:55:16.459Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing has been successfully applied to classification tasks on
high-dimensional inputs, such as images, to obtain models that are provably
robust against adversarial perturbations of the input. We extend this technique
to produce provable robustness for functions that map inputs into an arbitrary
metric space rather than discrete classes. Such functions are used in many
machine learning problems like image reconstruction, dimensionality reduction,
facial recognition, etc. Our robustness certificates guarantee that the change
in the output of the smoothed model as measured by the distance metric remains
small for any norm-bounded perturbation of the input. We can certify robustness
under a variety of different output metrics, such as total variation distance,
Jaccard distance, perceptual metrics, etc. In our experiments, we apply our
procedure to create certifiably robust models with disparate output spaces --
from sets to images -- and show that it yields meaningful certificates without
significantly degrading the performance of the base model. The code for our
experiments is available at: https://github.com/aounon/center-smoothing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aounon Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sanity Simulations for Saliency Methods. (arXiv:2105.06506v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06506</id>
        <link href="http://arxiv.org/abs/2105.06506"/>
        <updated>2021-06-29T01:55:16.381Z</updated>
        <summary type="html"><![CDATA[Saliency methods are a popular class of feature attribution tools that aim to
capture a model's predictive reasoning by identifying "important" pixels in an
input image. However, the development and adoption of saliency methods are
currently hindered by the lack of access to underlying model reasoning, which
prevents accurate method evaluation. In this work, we design a synthetic
evaluation framework, SMERF, that allows us to perform ground-truth-based
evaluation of saliency methods while controlling the underlying complexity of
model reasoning. Experimental evaluations via SMERF reveal significant
limitations in existing saliency methods, especially given the relative
simplicity of SMERF's synthetic evaluation tasks. Moreover, the SMERF
benchmarking suite represents a useful tool in the development of new saliency
methods to potentially overcome these limitations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Joon Sik Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1"&gt;Gregory Plumb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PGST: a Polyglot Gender Style Transfer method. (arXiv:2009.01040v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.01040</id>
        <link href="http://arxiv.org/abs/2009.01040"/>
        <updated>2021-06-29T01:55:16.365Z</updated>
        <summary type="html"><![CDATA[Recent developments in Text Style Transfer have led this field to be more
highlighted than ever. The task of transferring an input's style to another is
accompanied by plenty of challenges (e.g., fluency and content preservation)
that need to be taken care of. In this research, we introduce PGST, a novel
polyglot text style transfer approach in the gender domain, composed of
different constitutive elements. In contrast to prior studies, it is feasible
to apply a style transfer method in multiple languages by fulfilling our
method's predefined elements. We have proceeded with a pre-trained word
embedding for token replacement purposes, a character-based token classifier
for gender exchange purposes, and a beam search algorithm for extracting the
most fluent combination. Since different approaches are introduced in our
research, we determine a trade-off value for evaluating different models'
success in faking our gender identification model with transferred text. To
demonstrate our method's multilingual applicability, we applied our method on
both English and Persian corpora and ended up defeating our proposed gender
identification model by 45.6% and 39.2%, respectively. While this research's
focus is not limited to a specific language, our obtained evaluation results
are highly competitive in an analogy among English state of the art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1"&gt;Reza Khanmohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1"&gt;Seyed Abolghasem Mirroshandel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TimeSHAP: Explaining Recurrent Models through Sequence Perturbations. (arXiv:2012.00073v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00073</id>
        <link href="http://arxiv.org/abs/2012.00073"/>
        <updated>2021-06-29T01:55:16.359Z</updated>
        <summary type="html"><![CDATA[Although recurrent neural networks (RNNs) are state-of-the-art in numerous
sequential decision-making tasks, there has been little research on explaining
their predictions. In this work, we present TimeSHAP, a model-agnostic
recurrent explainer that builds upon KernelSHAP and extends it to the
sequential domain. TimeSHAP computes feature-, timestep-, and cell-level
attributions. As sequences may be arbitrarily long, we further propose a
pruning method that is shown to dramatically decrease both its computational
cost and the variance of its attributions. We use TimeSHAP to explain the
predictions of a real-world bank account takeover fraud detection RNN model,
and draw key insights from its explanations: i) the model identifies important
features and events aligned with what fraud analysts consider cues for account
takeover; ii) positive predicted sequences can be pruned to only 10% of the
original length, as older events have residual attribution values; iii) the
most recent input event of positive predictions only contributes on average to
41% of the model's score; iv) notably high attribution to client's age,
suggesting a potential discriminatory reasoning, later confirmed as higher
false positive rates for older clients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bento_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Bento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saleiro_P/0/1/0/all/0/1"&gt;Pedro Saleiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1"&gt;M&amp;#xe1;rio A.T. Figueiredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1"&gt;Pedro Bizarro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Reinforcement Learning using Observational and Interventional Data. (arXiv:2106.14421v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14421</id>
        <link href="http://arxiv.org/abs/2106.14421"/>
        <updated>2021-06-29T01:55:16.353Z</updated>
        <summary type="html"><![CDATA[Learning efficiently a causal model of the environment is a key challenge of
model-based RL agents operating in POMDPs. We consider here a scenario where
the learning agent has the ability to collect online experiences through direct
interactions with the environment (interventional data), but has also access to
a large collection of offline experiences, obtained by observing another agent
interacting with the environment (observational data). A key ingredient, that
makes this situation non-trivial, is that we allow the observed agent to
interact with the environment based on hidden information, which is not
observed by the learning agent. We then ask the following questions: can the
online and offline experiences be safely combined for learning a causal model ?
And can we expect the offline experiences to improve the agent's performances ?
To answer these questions, we import ideas from the well-established causal
framework of do-calculus, and we express model-based reinforcement learning as
a causal inference problem. Then, we propose a general yet simple methodology
for leveraging offline data during learning. In a nutshell, the method relies
on learning a latent-based causal transition model that explains both the
interventional and observational regimes, and then using the recovered latent
variable to infer the standard POMDP transition model via deconfounding. We
prove our method is correct and efficient in the sense that it attains better
generalization guarantees due to the offline data (in the asymptotic case), and
we illustrate its effectiveness empirically on synthetic toy problems. Our
contribution aims at bridging the gap between the fields of reinforcement
learning and causality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gasse_M/0/1/0/all/0/1"&gt;Maxime Gasse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grasset_D/0/1/0/all/0/1"&gt;Damien Grasset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaudron_G/0/1/0/all/0/1"&gt;Guillaume Gaudron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1"&gt;Pierre-Yves Oudeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Sequential Recommendation Consistency with Self-Supervised Imitation. (arXiv:2106.14031v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.14031</id>
        <link href="http://arxiv.org/abs/2106.14031"/>
        <updated>2021-06-29T01:55:16.347Z</updated>
        <summary type="html"><![CDATA[Most sequential recommendation models capture the features of consecutive
items in a user-item interaction history. Though effective, their
representation expressiveness is still hindered by the sparse learning signals.
As a result, the sequential recommender is prone to make inconsistent
predictions. In this paper, we propose a model, \textbf{SSI}, to improve
sequential recommendation consistency with Self-Supervised Imitation.
Precisely, we extract the consistency knowledge by utilizing three
self-supervised pre-training tasks, where temporal consistency and persona
consistency capture user-interaction dynamics in terms of the chronological
order and persona sensitivities, respectively. Furthermore, to provide the
model with a global perspective, global session consistency is introduced by
maximizing the mutual information among global and local interaction sequences.
Finally, to comprehensively take advantage of all three independent aspects of
consistency-enhanced knowledge, we establish an integrated imitation learning
framework. The consistency knowledge is effectively internalized and
transferred to the student model by imitating the conventional prediction logit
as well as the consistency-enhanced item representations. In addition, the
flexible self-supervised imitation framework can also benefit other student
recommenders. Experiments on four real-world datasets show that SSI effectively
outperforms the state-of-the-art sequential recommendation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongshen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yonghao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiaofang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhuoye Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1"&gt;Bo Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use of Machine Learning Technique to maximize the signal over background for $H \rightarrow \tau \tau$. (arXiv:2106.14257v1 [physics.data-an])]]></title>
        <id>http://arxiv.org/abs/2106.14257</id>
        <link href="http://arxiv.org/abs/2106.14257"/>
        <updated>2021-06-29T01:55:16.341Z</updated>
        <summary type="html"><![CDATA[In recent years, artificial neural networks (ANNs) have won numerous contests
in pattern recognition and machine learning. ANNS have been applied to problems
ranging from speech recognition to prediction of protein secondary structure,
classification of cancers, and gene prediction. Here, we intend to maximize the
chances of finding the Higgs boson decays to two $\tau$ leptons in the pseudo
dataset using a Machine Learning technique to classify the recorded events as
signal or background.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kanhaiya Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Gradient Alignment in Distributed and Federated Learning. (arXiv:2106.13897v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13897</id>
        <link href="http://arxiv.org/abs/2106.13897"/>
        <updated>2021-06-29T01:55:16.327Z</updated>
        <summary type="html"><![CDATA[A major obstacle to achieving global convergence in distributed and federated
learning is the misalignment of gradients across clients, or mini-batches due
to heterogeneity and stochasticity of the distributed data. One way to
alleviate this problem is to encourage the alignment of gradients across
different clients throughout training. Our analysis reveals that this goal can
be accomplished by utilizing the right optimization method that replicates the
implicit regularization effect of SGD, leading to gradient alignment as well as
improvements in test accuracies. Since the existence of this regularization in
SGD completely relies on the sequential use of different mini-batches during
training, it is inherently absent when training with large mini-batches. To
obtain the generalization benefits of this regularization while increasing
parallelism, we propose a novel GradAlign algorithm that induces the same
implicit regularization while allowing the use of arbitrarily large batches in
each update. We experimentally validate the benefit of our algorithm in
different distributed and federated learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1"&gt;Yatin Dandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barba_L/0/1/0/all/0/1"&gt;Luis Barba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The KL-Divergence between a Graph Model and its Fair I-Projection as a Fairness Regularizer. (arXiv:2103.01846v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01846</id>
        <link href="http://arxiv.org/abs/2103.01846"/>
        <updated>2021-06-29T01:55:16.321Z</updated>
        <summary type="html"><![CDATA[Learning and reasoning over graphs is increasingly done by means of
probabilistic models, e.g. exponential random graph models, graph embedding
models, and graph neural networks. When graphs are modeling relations between
people, however, they will inevitably reflect biases, prejudices, and other
forms of inequity and inequality. An important challenge is thus to design
accurate graph modeling approaches while guaranteeing fairness according to the
specific notion of fairness that the problem requires. Yet, past work on the
topic remains scarce, is limited to debiasing specific graph modeling methods,
and often aims to ensure fairness in an indirect manner.

We propose a generic approach applicable to most probabilistic graph modeling
approaches. Specifically, we first define the class of fair graph models
corresponding to a chosen set of fairness criteria. Given this, we propose a
fairness regularizer defined as the KL-divergence between the graph model and
its I-projection onto the set of fair models. We demonstrate that using this
fairness regularizer in combination with existing graph modeling approaches
efficiently trades-off fairness with accuracy, whereas the state-of-the-art
models can only make this trade-off for the fairness criterion that they were
specifically designed for.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buyl_M/0/1/0/all/0/1"&gt;Maarten Buyl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1"&gt;Tijl De Bie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Mesh Representations via Binary Space Partitioning Tree Networks. (arXiv:2106.14274v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14274</id>
        <link href="http://arxiv.org/abs/2106.14274"/>
        <updated>2021-06-29T01:55:16.313Z</updated>
        <summary type="html"><![CDATA[Polygonal meshes are ubiquitous, but have only played a relatively minor role
in the deep learning revolution. State-of-the-art neural generative models for
3D shapes learn implicit functions and generate meshes via expensive
iso-surfacing. We overcome these challenges by employing a classical spatial
data structure from computer graphics, Binary Space Partitioning (BSP), to
facilitate 3D learning. The core operation of BSP involves recursive
subdivision of 3D space to obtain convex sets. By exploiting this property, we
devise BSP-Net, a network that learns to represent a 3D shape via convex
decomposition without supervision. The network is trained to reconstruct a
shape using a set of convexes obtained from a BSP-tree built over a set of
planes, where the planes and convexes are both defined by learned network
weights. BSP-Net directly outputs polygonal meshes from the inferred convexes.
The generated meshes are watertight, compact (i.e., low-poly), and well suited
to represent sharp geometry. We show that the reconstruction quality by BSP-Net
is competitive with those from state-of-the-art methods while using much fewer
primitives. We also explore variations to BSP-Net including using a more
generic decoder for reconstruction, more general primitives than planes, as
well as training a generative model with variational auto-encoders. Code is
available at https://github.com/czq142857/BSP-NET-original.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiqin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1"&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02800</id>
        <link href="http://arxiv.org/abs/2106.02800"/>
        <updated>2021-06-29T01:55:16.308Z</updated>
        <summary type="html"><![CDATA[Microaneurysms (MAs) are one of the earliest signs of diabetic retinopathy
(DR), a frequent complication of diabetes that can lead to visual impairment
and blindness. Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides
real-time retinal images with resolution down to 2 $\mu m$ and thus allows
detection of the morphologies of individual MAs, a potential marker that might
dictate MA pathology and affect the progression of DR. In contrast to the
numerous automatic models developed for assessing the number of MAs on fundus
photographs, currently there is no high throughput image protocol available for
automatic analysis of AOSLO photographs. To address this urgency, we introduce
AOSLO-net, a deep neural network framework with customized training policies to
automatically segment MAs from AOSLO images. We evaluate the performance of
AOSLO-net using 87 DR AOSLO images and our results demonstrate that the
proposed model outperforms the state-of-the-art segmentation model both in
accuracy and cost and enables correct MA morphological classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1"&gt;Konstantina Sampani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shengze Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yixiang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;He Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer K. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QuickBrowser: A Unified Model to Detect and Read Simple Object in Real-time. (arXiv:2102.07354v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07354</id>
        <link href="http://arxiv.org/abs/2102.07354"/>
        <updated>2021-06-29T01:55:16.303Z</updated>
        <summary type="html"><![CDATA[There are many real-life use cases such as barcode scanning or billboard
reading where people need to detect objects and read the object contents.
Commonly existing methods are first trying to localize object regions, then
determine layout and lastly classify content units. However, for simple fixed
structured objects like license plates, this approach becomes overkill and
lengthy to run. This work aims to solve this detect-and-read problem in a
lightweight way by integrating multi-digit recognition into a one-stage object
detection model. Our unified method not only eliminates the duplication in
feature extraction (one for localizing, one again for classifying) but also
provides useful contextual information around object regions for
classification. Additionally, our choice of backbones and modifications in
architecture, loss function, data augmentation and training make the method
robust, efficient and speedy. Secondly, we made a public benchmark dataset of
diverse real-life 1D barcodes for a reliable evaluation, which we collected,
annotated and checked carefully. Eventually, experimental results prove the
method's efficiency on the barcode problem by outperforming industrial tools in
both detecting and decoding rates with a real-time fps at a VGA-similar
resolution. It also did a great job expectedly on the license-plate recognition
task (on the AOLP dataset) by outperforming the current state-of-the-art method
significantly in terms of recognition rate and inference time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1"&gt;Thao Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daeyoung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLPerf Tiny Benchmark. (arXiv:2106.07597v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07597</id>
        <link href="http://arxiv.org/abs/2106.07597"/>
        <updated>2021-06-29T01:55:16.297Z</updated>
        <summary type="html"><![CDATA[Advancements in ultra-low-power tiny machine learning (TinyML) systems
promise to unlock an entirely new class of smart applications. However,
continued progress is limited by the lack of a widely accepted and easily
reproducible benchmark for these systems. To meet this need, we present MLPerf
Tiny, the first industry-standard benchmark suite for ultra-low-power tiny
machine learning systems. The benchmark suite is the collaborative effort of
more than 50 organizations from industry and academia and reflects the needs of
the community. MLPerf Tiny measures the accuracy, latency, and energy of
machine learning inference to properly evaluate the tradeoffs between systems.
Additionally, MLPerf Tiny implements a modular design that enables benchmark
submitters to show the benefits of their product, regardless of where it falls
on the ML deployment stack, in a fair and reproducible manner. The suite
features four benchmarks: keyword spotting, visual wake words, image
classification, and anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1"&gt;Colby Banbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1"&gt;Vijay Janapa Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torelli_P/0/1/0/all/0/1"&gt;Peter Torelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holleman_J/0/1/0/all/0/1"&gt;Jeremy Holleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeffries_N/0/1/0/all/0/1"&gt;Nat Jeffries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiraly_C/0/1/0/all/0/1"&gt;Csaba Kiraly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montino_P/0/1/0/all/0/1"&gt;Pietro Montino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanter_D/0/1/0/all/0/1"&gt;David Kanter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sebastian Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pau_D/0/1/0/all/0/1"&gt;Danilo Pau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1"&gt;Urmish Thakker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torrini_A/0/1/0/all/0/1"&gt;Antonio Torrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1"&gt;Peter Warden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cordaro_J/0/1/0/all/0/1"&gt;Jay Cordaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guglielmo_G/0/1/0/all/0/1"&gt;Giuseppe Di Guglielmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1"&gt;Javier Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibellini_S/0/1/0/all/0/1"&gt;Stephen Gibellini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1"&gt;Videet Parekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Honson Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1"&gt;Nhan Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenxu_N/0/1/0/all/0/1"&gt;Niu Wenxu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xuesong_X/0/1/0/all/0/1"&gt;Xu Xuesong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introduction to Multi-Armed Bandits. (arXiv:1904.07272v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.07272</id>
        <link href="http://arxiv.org/abs/1904.07272"/>
        <updated>2021-06-29T01:55:16.281Z</updated>
        <summary type="html"><![CDATA[Multi-armed bandits a simple but very powerful framework for algorithms that
make decisions over time under uncertainty. An enormous body of work has
accumulated over the years, covered in several books and surveys. This book
provides a more introductory, textbook-like treatment of the subject. Each
chapter tackles a particular line of work, providing a self-contained,
teachable technical introduction and a brief review of the further
developments; many of the chapters conclude with exercises.

The book is structured as follows. The first four chapters are on IID
rewards, from the basic model to impossibility results to Bayesian priors to
Lipschitz rewards. The next three chapters cover adversarial rewards, from the
full-feedback version to adversarial bandits to extensions with linear rewards
and combinatorially structured actions. Chapter 8 is on contextual bandits, a
middle ground between IID and adversarial bandits in which the change in reward
distributions is completely explained by observable contexts. The last three
chapters cover connections to economics, from learning in repeated games to
bandits with supply/budget constraints to exploration in the presence of
incentives. The appendix provides sufficient background on concentration and
KL-divergence.

The chapters on "bandits with similarity information", "bandits with
knapsacks" and "bandits and agents" can also be consumed as standalone surveys
on the respective topics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1"&gt;Aleksandrs Slivkins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An XAI Approach to Deep Learning Models in the Detection of Ductal Carcinoma in Situ. (arXiv:2106.14186v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14186</id>
        <link href="http://arxiv.org/abs/2106.14186"/>
        <updated>2021-06-29T01:55:16.275Z</updated>
        <summary type="html"><![CDATA[During the last decade or so, there has been an insurgence in the deep
learning community to solve health-related issues, particularly breast cancer.
Following the Camelyon-16 challenge in 2016, several researchers have dedicated
their time to build Convolutional Neural Networks (CNNs) to help radiologists
and other clinicians diagnose breast cancer. In particular, there has been an
emphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage
breast cancer. Large companies have given their fair share of research into
this subject, among these Google Deepmind who developed a model in 2020 that
has proven to be better than radiologists themselves to diagnose breast cancer
correctly.

We found that among the issues which exist, there is a need for an
explanatory system that goes through the hidden layers of a CNN to highlight
those pixels that contributed to the classification of a mammogram. We then
chose an open-source, reasonably successful project developed by Prof. Shen,
using the CBIS-DDSM image database to run our experiments on. It was later
improved using the Resnet-50 and VGG-16 patch-classifiers, analytically
comparing the outcome of both. The results showed that the Resnet-50 one
converged earlier in the experiments.

Following the research by Montavon and Binder, we used the DeepTaylor
Layer-wise Relevance Propagation (LRP) model to highlight those pixels and
regions within a mammogram which contribute most to its classification. This is
represented as a map of those pixels in the original image, which contribute to
the diagnosis and the extent to which they contribute to the final
classification. The most significant advantage of this algorithm is that it
performs exceptionally well with the Resnet-50 patch classifier architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ferla_M/0/1/0/all/0/1"&gt;Michele La Ferla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Montebello_M/0/1/0/all/0/1"&gt;Matthew Montebello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seychell_D/0/1/0/all/0/1"&gt;Dylan Seychell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to solve geometric construction problems from images. (arXiv:2106.14195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14195</id>
        <link href="http://arxiv.org/abs/2106.14195"/>
        <updated>2021-06-29T01:55:16.269Z</updated>
        <summary type="html"><![CDATA[We describe a purely image-based method for finding geometric constructions
with a ruler and compass in the Euclidea geometric game. The method is based on
adapting the Mask R-CNN state-of-the-art image processing neural architecture
and adding a tree-based search procedure to it. In a supervised setting, the
method learns to solve all 68 kinds of geometric construction problems from the
first six level packs of Euclidea with an average 92% accuracy. When evaluated
on new kinds of problems, the method can solve 31 of the 68 kinds of Euclidea
problems. We believe that this is the first time that a purely image-based
learning has been trained to solve geometric construction problems of this
difficulty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1"&gt;J. Macke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sedlar_J/0/1/0/all/0/1"&gt;J. Sedlar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1"&gt;M. Olsak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1"&gt;J. Urban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1"&gt;J. Sivic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based Token Classification and Span Prediction Techniques. (arXiv:2102.12254v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12254</id>
        <link href="http://arxiv.org/abs/2102.12254"/>
        <updated>2021-06-29T01:55:16.264Z</updated>
        <summary type="html"><![CDATA[Toxicity detection of text has been a popular NLP task in the recent years.
In SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic
spans within passages. Most state-of-the-art span detection approaches employ
various techniques, each of which can be broadly classified into Token
Classification or Span Prediction approaches. In our paper, we explore simple
versions of both of these approaches and their performance on the task.
Specifically, we use BERT-based models -- BERT, RoBERTa, and SpanBERT for both
approaches. We also combine these approaches and modify them to bring
improvements for Toxic Spans prediction. To this end, we investigate results on
four hybrid approaches -- Multi-Span, Span+Token, LSTM-CRF, and a combination
of predicted offsets using union/intersection. Additionally, we perform a
thorough ablative analysis and analyze our observed results. Our best
submission -- a combination of SpanBERT Span Predictor and RoBERTa Token
Classifier predictions -- achieves an F1 score of 0.6753 on the test set. Our
best post-eval F1 score is 0.6895 on intersection of predicted offsets from
top-3 RoBERTa Token Classification checkpoints. These approaches improve the
performance by 3% on average than those of the shared baseline models -- RNNSL
and SpaCy NER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1"&gt;Gunjan Chhablani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1"&gt;Harshit Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1"&gt;Yash Bhartia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suthaharan_S/0/1/0/all/0/1"&gt;Shan Suthaharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Automated Machine Learning Tools for SMS Spam Message Filtering. (arXiv:2106.08671v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08671</id>
        <link href="http://arxiv.org/abs/2106.08671"/>
        <updated>2021-06-29T01:55:16.250Z</updated>
        <summary type="html"><![CDATA[Short Message Service (SMS) is a very popular service used for communication
by mobile users. However, this popular service can be abused by executing
illegal activities and influencing security risks. Nowadays, many automatic
machine learning (AutoML) tools exist which can help domain experts and lay
users to build high-quality ML models with little or no machine learning
knowledge. In this work, a classification performance comparison was conducted
between three automatic ML tools for SMS spam message filtering. These tools
are mljar-supervised AutoML, H2O AutoML, and Tree-based Pipeline Optimization
Tool (TPOT) AutoML. Experimental results showed that ensemble models achieved
the best classification performance. The Stacked Ensemble model, which was
built using H2O AutoML, achieved the best performance in terms of Log Loss
(0.8370), true positive (1088/1116), and true negative (281/287) metrics. There
is a 19.05\% improvement in Log Loss with respect to TPOT AutoML and 5.56\%
improvement with respect to mljar-supervised AutoML. The satisfactory filtering
performance achieved with AutoML tools provides a potential application for
AutoML tools to automatically determine the best ML model that can perform best
for SMS spam message filtering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saeed_W/0/1/0/all/0/1"&gt;Waddah Saeed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07566</id>
        <link href="http://arxiv.org/abs/2104.07566"/>
        <updated>2021-06-29T01:55:16.244Z</updated>
        <summary type="html"><![CDATA[Attention mechanism has shown enormous potential for single image
super-resolution (SISR). However, existing works only proposed some attention
mechanism for a specific network. A universal attention mechanism for SISR,
which could further improve the performance of networks without attention and
provide a baseline for networks with attention, is still lacking. To fit this
gap, we propose a lightweight and efficient Balanced Attention Mechanism (BAM),
which consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial
Attention Module (MSAM) in parallel. The information extraction mechanism of
ACAM and MSAM effectively filters redundant information, making the overall
structure of BAM very lightweight. Owing to the parallel structure, during the
gradient backpropagation process of BAM, ACAM and MSAM not only conduct
self-optimization, but also mutual optimization so as to generate more balanced
attention information. To verify the effectiveness and robustness of BAM, we
applied it to 12 state-ofthe-art SISR networks. The results on 4 benchmark
datasets demonstrate that BAM can efficiently improve the networks'
performance, and for those with attention, the substitution with BAM further
reduces the amount of parameters and increase the inference speed. Moreover,
ablation experiments were conducted to prove the minimalism of BAM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fanyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haotian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1"&gt;Cheng Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior Flow Variational Autoencoder: A density estimation model for Non-Intrusive Load Monitoring. (arXiv:2011.14870v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14870</id>
        <link href="http://arxiv.org/abs/2011.14870"/>
        <updated>2021-06-29T01:55:16.238Z</updated>
        <summary type="html"><![CDATA[Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate
the power loads' appliance-by-appliance from the whole consumption measured by
a single meter. In this paper, we propose a conditional density estimation
model, based on deep neural networks, that joins a Conditional Variational
Autoencoder with a Conditional Invertible Normalizing Flow model to estimate
the individual appliance's power demand. The resulting model is called Prior
Flow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having
one model per appliance, the resulting model is responsible for estimating the
power demand, appliance-by-appliance, at once. We train and evaluate our
proposed model in a publicly available dataset composed of power demand
measures from a poultry feed factory located in Brazil. The proposed model's
quality is evaluated by comparing the obtained normalized disaggregation error
(NDE) and signal aggregated error (SAE) with the previous work values on the
same dataset. Our proposal achieves highly competitive results, and for six of
the eight machines belonging to the dataset, we observe consistent improvements
that go from 28% up to 81% in NDE and from 27% up to 86% in SAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1"&gt;Luis Felipe M.O. Henriques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_E/0/1/0/all/0/1"&gt;Eduardo Morgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1"&gt;Sergio Colcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1"&gt;Ruy Luiz Milidi&amp;#xfa;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13948</id>
        <link href="http://arxiv.org/abs/2106.13948"/>
        <updated>2021-06-29T01:55:16.232Z</updated>
        <summary type="html"><![CDATA[Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1"&gt;Jonathan Francis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1"&gt;Nariaki Kitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1"&gt;Felix Labelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1"&gt;Ingrid Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concentration of Contractive Stochastic Approximation and Reinforcement Learning. (arXiv:2106.14308v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14308</id>
        <link href="http://arxiv.org/abs/2106.14308"/>
        <updated>2021-06-29T01:55:16.227Z</updated>
        <summary type="html"><![CDATA[Using a martingale concentration inequality, concentration bounds `from time
$n_0$ on' are derived for stochastic approximation algorithms with contractive
maps and both martingale difference and Markov noises. These are applied to
reinforcement learning algorithms, in particular to asynchronous Q-learning and
TD(0).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandak_S/0/1/0/all/0/1"&gt;Siddharth Chandak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1"&gt;Vivek S. Borkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Note on Optimizing Distributions using Kernel Mean Embeddings. (arXiv:2106.09994v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09994</id>
        <link href="http://arxiv.org/abs/2106.09994"/>
        <updated>2021-06-29T01:55:16.221Z</updated>
        <summary type="html"><![CDATA[Kernel mean embeddings are a popular tool that consists in representing
probability measures by their infinite-dimensional mean embeddings in a
reproducing kernel Hilbert space. When the kernel is characteristic, mean
embeddings can be used to define a distance between probability measures, known
as the maximum mean discrepancy (MMD). A well-known advantage of mean
embeddings and MMD is their low computational cost and low sample complexity.
However, kernel mean embeddings have had limited applications to problems that
consist in optimizing distributions, due to the difficulty of characterizing
which Hilbert space vectors correspond to a probability distribution. In this
note, we propose to leverage the kernel sums-of-squares parameterization of
positive functions of Marteau-Ferey et al. [2020] to fit distributions in the
MMD geometry. First, we show that when the kernel is characteristic,
distributions with a kernel sum-of-squares density are dense. Then, we provide
algorithms to optimize such distributions in the finite-sample setting, which
we illustrate in a density fitting numerical experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muzellec_B/0/1/0/all/0/1"&gt;Boris Muzellec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning. (arXiv:1906.02944v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.02944</id>
        <link href="http://arxiv.org/abs/1906.02944"/>
        <updated>2021-06-29T01:55:16.216Z</updated>
        <summary type="html"><![CDATA[Object recognition in the real-world requires handling long-tailed or even
open-ended data. An ideal visual system needs to recognize the populated head
visual concepts reliably and meanwhile efficiently learn about emerging new
tail categories with a few training instances. Class-balanced many-shot
learning and few-shot learning tackle one side of this problem, by either
learning strong classifiers for head or learning to learn few-shot classifiers
for the tail. In this paper, we investigate the problem of generalized few-shot
learning (GFSL) -- a model during the deployment is required to learn about
tail categories with few shots and simultaneously classify the head classes. We
propose the ClAssifier SynThesis LEarning (CASTLE), a learning framework that
learns how to synthesize calibrated few-shot classifiers in addition to the
multi-class classifiers of head classes with a shared neural dictionary,
shedding light upon the inductive GFSL. Furthermore, we propose an adaptive
version of CASTLE (ACASTLE) that adapts the head classifiers conditioned on the
incoming tail training examples, yielding a framework that allows effective
backward knowledge transfer. As a consequence, ACASTLE can handle GFSL with
classes from heterogeneous domains effectively. CASTLE and ACASTLE demonstrate
superior performances than existing GFSL algorithms and strong baselines on
MiniImageNet as well as TieredImageNet datasets. More interestingly, they
outperform previous state-of-the-art methods when evaluated with standard
few-shot learning criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hexiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural tensor contractions and the expressive power of deep neural quantum states. (arXiv:2103.10293v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10293</id>
        <link href="http://arxiv.org/abs/2103.10293"/>
        <updated>2021-06-29T01:55:16.210Z</updated>
        <summary type="html"><![CDATA[We establish a direct connection between general tensor networks and deep
feed-forward artificial neural networks. The core of our results is the
construction of neural-network layers that efficiently perform tensor
contractions, and that use commonly adopted non-linear activation functions.
The resulting deep networks feature a number of edges that closely matches the
contraction complexity of the tensor networks to be approximated. In the
context of many-body quantum states, this result establishes that
neural-network states have strictly the same or higher expressive power than
practically usable variational tensor networks. As an example, we show that all
matrix product states can be efficiently written as neural-network states with
a number of edges polynomial in the bond dimension and depth logarithmic in the
system size. The opposite instead does not hold true, and our results imply
that there exist quantum states that are not efficiently expressible in terms
of matrix product states or practically usable PEPS, but that are instead
efficiently expressible with neural network states.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sharir_O/0/1/0/all/0/1"&gt;Or Sharir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Shashua_A/0/1/0/all/0/1"&gt;Amnon Shashua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1"&gt;Giuseppe Carleo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Amend Facial Expression Representation via De-albino and Affinity. (arXiv:2103.10189v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10189</id>
        <link href="http://arxiv.org/abs/2103.10189"/>
        <updated>2021-06-29T01:55:16.181Z</updated>
        <summary type="html"><![CDATA[Facial Expression Recognition (FER) is a classification task that points to
face variants. Hence, there are certain affinity features between facial
expressions, receiving little attention in the FER literature. Convolution
padding, despite helping capture the edge information, causes erosion of the
feature map simultaneously. After multi-layer filling convolution, the output
feature map named albino feature definitely weakens the representation of the
expression. To tackle these challenges, we propose a novel architecture named
Amending Representation Module (ARM). ARM is a substitute for the pooling
layer. Theoretically, it can be embedded in the back end of any network to deal
with the Padding Erosion. ARM efficiently enhances facial expression
representation from two different directions: 1) reducing the weight of eroded
features to offset the side effect of padding, and 2) sharing affinity features
over mini-batch to strengthen the representation learning. Experiments on
public benchmarks prove that our ARM boosts the performance of FER remarkably.
The validation accuracies are respectively 92.05% on RAF-DB, 65.2% on
Affect-Net, and 58.71% on SFEW, exceeding current state-of-the-art methods. Our
implementation and trained models are available at
https://github.com/JiaweiShiCV/Amend-Representation-Module.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jiawei Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Songhao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhiwei Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indoor Panorama Planar 3D Reconstruction via Divide and Conquer. (arXiv:2106.14166v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14166</id>
        <link href="http://arxiv.org/abs/2106.14166"/>
        <updated>2021-06-29T01:55:16.149Z</updated>
        <summary type="html"><![CDATA[Indoor panorama typically consists of human-made structures parallel or
perpendicular to gravity. We leverage this phenomenon to approximate the scene
in a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this
end, we propose an effective divide-and-conquer strategy that divides pixels
based on their plane orientation estimation; then, the succeeding instance
segmentation module conquers the task of planes clustering more easily in each
plane orientation group. Besides, parameters of V-planes depend on camera yaw
rotation, but translation-invariant CNNs are less aware of the yaw change. We
thus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We
create a benchmark for indoor panorama planar reconstruction by extending
existing 360 depth datasets with ground truth H\&V-planes (referred to as
PanoH&V dataset) and adopt state-of-the-art planar reconstruction methods to
predict H\&V-planes as our baselines. Our method outperforms the baselines by a
large margin on the proposed dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Cheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1"&gt;Chi-Wei Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Ning-Hsu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Min Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hwann-Tzong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Feature Desensitization. (arXiv:2006.04621v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04621</id>
        <link href="http://arxiv.org/abs/2006.04621"/>
        <updated>2021-06-29T01:55:16.143Z</updated>
        <summary type="html"><![CDATA[Neural networks are known to be vulnerable to adversarial attacks -- slight
but carefully constructed perturbations of the inputs which can drastically
impair the network's performance. Many defense methods have been proposed for
improving robustness of deep networks by training them on adversarially
perturbed inputs. However, these models often remain vulnerable to new types of
attacks not seen during training, and even to slightly stronger versions of
previously seen attacks. In this work, we propose a novel approach to
adversarial robustness, which builds upon the insights from the domain
adaptation field. Our method, called Adversarial Feature Desensitization (AFD),
aims at learning features that are invariant towards adversarial perturbations
of the inputs. This is achieved through a game where we learn features that are
both predictive and robust (insensitive to adversarial attacks), i.e. cannot be
used to discriminate between natural and adversarial data. Empirical results on
several benchmarks demonstrate the effectiveness of the proposed approach
against a wide range of attack types and attack strengths.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bashivan_P/0/1/0/all/0/1"&gt;Pouya Bashivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayat_R/0/1/0/all/0/1"&gt;Reza Bayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1"&gt;Adam Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1"&gt;Kartik Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faramarzi_M/0/1/0/all/0/1"&gt;Mojtaba Faramarzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laleh_T/0/1/0/all/0/1"&gt;Touraj Laleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richards_B/0/1/0/all/0/1"&gt;Blake Aaron Richards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Dimensional Uncertainty Quantification via Tensor Regression with Rank Determination and Adaptive Sampling. (arXiv:2103.17236v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17236</id>
        <link href="http://arxiv.org/abs/2103.17236"/>
        <updated>2021-06-29T01:55:16.137Z</updated>
        <summary type="html"><![CDATA[Fabrication process variations can significantly influence the performance
and yield of nano-scale electronic and photonic circuits. Stochastic spectral
methods have achieved great success in quantifying the impact of process
variations, but they suffer from the curse of dimensionality. Recently,
low-rank tensor methods have been developed to mitigate this issue, but two
fundamental challenges remain open: how to automatically determine the tensor
rank and how to adaptively pick the informative simulation samples. This paper
proposes a novel tensor regression method to address these two challenges. We
use a $\ell_{q}/ \ell_{2}$ group-sparsity regularization to determine the
tensor rank. The resulting optimization problem can be efficiently solved via
an alternating minimization solver. We also propose a two-stage adaptive
sampling method to reduce the simulation cost. Our method considers both
exploration and exploitation via the estimated Voronoi cell volume and
nonlinearity measurement respectively. The proposed model is verified with
synthetic and some realistic circuit benchmarks, on which our method can well
capture the uncertainty caused by 19 to 100 random variables with only 100 to
600 simulation samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+He_Z/0/1/0/all/0/1"&gt;Zichang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Differential Privacy and Federated Learning for BERT Models. (arXiv:2106.13973v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13973</id>
        <link href="http://arxiv.org/abs/2106.13973"/>
        <updated>2021-06-29T01:55:16.131Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing (NLP) techniques can be applied to help with the
diagnosis of medical conditions such as depression, using a collection of a
person's utterances. Depression is a serious medical illness that can have
adverse effects on how one feels, thinks, and acts, which can lead to emotional
and physical problems. Due to the sensitive nature of such data, privacy
measures need to be taken for handling and training models with such data. In
this work, we study the effects that the application of Differential Privacy
(DP) has, in both a centralized and a Federated Learning (FL) setup, on
training contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).
We offer insights on how to privately train NLP models and what architectures
and setups provide more desirable privacy utility trade-offs. We envisage this
work to be used in future healthcare and mental health studies to keep medical
history private. Therefore, we provide an open-source implementation of this
work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1"&gt;Priyam Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1"&gt;Tiasa Singha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1"&gt;Rakshit Naidu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1"&gt;Zumrut Muftuoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sahib Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1"&gt;Fatemehsadat Mireshghallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning swimming escape patterns for larval fish under energy constraints. (arXiv:2105.00771v2 [physics.flu-dyn] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00771</id>
        <link href="http://arxiv.org/abs/2105.00771"/>
        <updated>2021-06-29T01:55:16.126Z</updated>
        <summary type="html"><![CDATA[Swimming organisms can escape their predators by creating and harnessing
unsteady flow fields through their body motions. Stochastic optimization and
flow simulations have identified escape patterns that are consistent with those
observed in natural larval swimmers. However, these patterns have been limited
by the specification of a particular cost function and depend on a prescribed
functional form of the body motion. Here, we deploy reinforcement learning to
discover swimmer escape patterns for larval fish under energy constraints. The
identified patterns include the C-start mechanism, in addition to more
energetically efficient escapes. We find that maximizing distance with limited
energy requires swimming via short bursts of accelerating motion interlinked
with phases of gliding. The present, data efficient, reinforcement learning
algorithm results in an array of patterns that reveal practical flow
optimization principles for efficient swimming and the methodology can be
transferred to the control of aquatic robotic devices operating under energy
constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Mandralis_I/0/1/0/all/0/1"&gt;Ioannis Mandralis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Weber_P/0/1/0/all/0/1"&gt;Pascal Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Novati_G/0/1/0/all/0/1"&gt;Guido Novati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Koumoutsakos_P/0/1/0/all/0/1"&gt;Petros Koumoutsakos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning of Context-dependent Processing in Neural Networks. (arXiv:1810.01256v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.01256</id>
        <link href="http://arxiv.org/abs/1810.01256"/>
        <updated>2021-06-29T01:55:16.106Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are powerful tools in learning sophisticated but
fixed mapping rules between inputs and outputs, thereby limiting their
application in more complex and dynamic situations in which the mapping rules
are not kept the same but changing according to different contexts. To lift
such limits, we developed a novel approach involving a learning algorithm,
called orthogonal weights modification (OWM), with the addition of a
context-dependent processing (CDP) module. We demonstrated that with OWM to
overcome the problem of catastrophic forgetting, and the CDP module to learn
how to reuse a feature representation and a classifier for different contexts,
a single network can acquire numerous context-dependent mapping rules in an
online and continual manner, with as few as $\sim$10 samples to learn each.
This should enable highly compact systems to gradually learn myriad
regularities of the real world and eventually behave appropriately within it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1"&gt;Guanxiong Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bo Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shan Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rejoinder: Gaussian Differential Privacy. (arXiv:2104.01987v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01987</id>
        <link href="http://arxiv.org/abs/2104.01987"/>
        <updated>2021-06-29T01:55:16.098Z</updated>
        <summary type="html"><![CDATA[In this rejoinder, we aim to address two broad issues that cover most
comments made in the discussion. First, we discuss some theoretical aspects of
our work and comment on how this work might impact the theoretical foundation
of privacy-preserving data analysis. Taking a practical viewpoint, we next
discuss how f-differential privacy (f-DP) and Gaussian differential privacy
(GDP) can make a difference in a range of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jinshuo Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weijie J. Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT. (arXiv:2105.14625v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14625</id>
        <link href="http://arxiv.org/abs/2105.14625"/>
        <updated>2021-06-29T01:55:16.092Z</updated>
        <summary type="html"><![CDATA[A surrogate model based hyperparameter tuning approach for deep learning is
presented. This article demonstrates how the architecture-level parameters
(hyperparameters) of deep learning models that were implemented in
Keras/tensorflow can be optimized. The implementation of the tuning procedure
is 100% accessible from R, the software environment for statistical computing.
With a few lines of code, existing R packages (tfruns and SPOT) can be combined
to perform hyperparameter tuning. An elementary hyperparameter tuning task
(neural network and the MNIST data) is used to exemplify this approach]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1"&gt;Thomas Bartz-Beielstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hypothesis Testing Approach to Nonstationary Source Separation. (arXiv:2105.06958v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06958</id>
        <link href="http://arxiv.org/abs/2105.06958"/>
        <updated>2021-06-29T01:55:16.086Z</updated>
        <summary type="html"><![CDATA[The extraction of nonstationary signals from blind and semi-blind
multivariate observations is a recurrent problem. Numerous algorithms have been
developed for this problem, which are based on the exact or approximate joint
diagonalization of second or higher order cumulant matrices/tensors of
multichannel data. While a great body of research has been dedicated to joint
diagonalization algorithms, the selection of the diagonalized matrix/tensor set
remains highly problem-specific. Herein, various methods for nonstationarity
identification are reviewed and a new general framework based on hypothesis
testing is proposed, which results in a classification/clustering perspective
to semi-blind source separation of nonstationary components. The proposed
method is applied to noninvasive fetal ECG extraction, as case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sameni_R/0/1/0/all/0/1"&gt;Reza Sameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jutten_C/0/1/0/all/0/1"&gt;Christian Jutten&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two Sides of Meta-Learning Evaluation: In vs. Out of Distribution. (arXiv:2102.11503v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11503</id>
        <link href="http://arxiv.org/abs/2102.11503"/>
        <updated>2021-06-29T01:55:16.080Z</updated>
        <summary type="html"><![CDATA[We categorize meta-learning evaluation into two settings:
$\textit{in-distribution}$ [ID], in which the train and test tasks are sampled
$\textit{iid}$ from the same underlying task distribution, and
$\textit{out-of-distribution}$ [OOD], in which they are not. While most
meta-learning theory and some FSL applications follow the ID setting, we
identify that most existing few-shot classification benchmarks instead reflect
OOD evaluation, as they use disjoint sets of train (base) and test (novel)
classes for task generation. This discrepancy is problematic because -- as we
show on numerous benchmarks -- meta-learning methods that perform better on
existing OOD datasets may perform significantly worse in the ID setting. In
addition, in the OOD setting, even though current FSL benchmarks seem
befitting, our study highlights concerns in 1) reliably performing model
selection for a given meta-learning method, and 2) consistently comparing the
performance of different methods. To address these concerns, we provide
suggestions on how to construct FSL benchmarks to allow for ID evaluation as
well as more reliable OOD evaluation. Our work aims to inform the meta-learning
community about the importance and distinction of ID vs. OOD evaluation, as
well as the subtleties of OOD evaluation with current benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Setlur_A/0/1/0/all/0/1"&gt;Amrith Setlur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_O/0/1/0/all/0/1"&gt;Oscar Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1"&gt;Virginia Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Photonic-Circuits-Inspired Compact Network: Toward Real-Time Wireless Signal Classification at the Edge. (arXiv:2106.13865v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.13865</id>
        <link href="http://arxiv.org/abs/2106.13865"/>
        <updated>2021-06-29T01:55:16.064Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) methods are ubiquitous in wireless communication
systems and have proven powerful for applications including radio-frequency
(RF) fingerprinting, automatic modulation classification, and cognitive radio.
However, the large size of ML models can make them difficult to implement on
edge devices for latency-sensitive downstream tasks. In wireless communication
systems, ML data processing at a sub-millisecond scale will enable real-time
network monitoring to improve security and prevent infiltration. In addition,
compact and integratable hardware platforms which can implement ML models at
the chip scale will find much broader application to wireless communication
networks. Toward real-time wireless signal classification at the edge, we
propose a novel compact deep network that consists of a
photonic-hardware-inspired recurrent neural network model in combination with a
simplified convolutional classifier, and we demonstrate its application to the
identification of RF emitters by their random transmissions. With the proposed
model, we achieve 96.32% classification accuracy over a set of 30 identical
ZigBee devices when using 50 times fewer training parameters than an existing
state-of-the-art CNN classifier. Thanks to the large reduction in network size,
we demonstrate real-time RF fingerprinting with 0.219 ms latency using a
small-scale FPGA board, the PYNQ-Z1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hsuan-Tung Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lederman_J/0/1/0/all/0/1"&gt;Joshua Lederman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lima_T/0/1/0/all/0/1"&gt;Thomas Ferreira de Lima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chaoran Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shastri_B/0/1/0/all/0/1"&gt;Bhavin Shastri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rosenbluth_D/0/1/0/all/0/1"&gt;David Rosenbluth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prucnal_P/0/1/0/all/0/1"&gt;Paul Prucnal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Touch-based Curiosity for Sparse-Reward Tasks. (arXiv:2104.00442v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00442</id>
        <link href="http://arxiv.org/abs/2104.00442"/>
        <updated>2021-06-29T01:55:16.057Z</updated>
        <summary type="html"><![CDATA[Robots in many real-world settings have access to force/torque sensors in
their gripper and tactile sensing is often necessary in tasks that involve
contact-rich motion. In this work, we leverage surprise from mismatches in
touch feedback to guide exploration in hard sparse-reward reinforcement
learning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible
objects interactions are supposed to "feel" like. We encourage exploration by
rewarding interactions where the expectation and the experience don't match. In
our proposed method, an initial task-independent exploration phase is followed
by an on-task learning phase, in which the original interactions are relabeled
with on-task rewards. We test our approach on a range of touch-intensive robot
arm tasks (e.g. pushing objects, opening doors), which we also release as part
of this work. Across multiple experiments in a simulated setting, we
demonstrate that our method is able to learn these difficult tasks through
sparse reward and curiosity alone. We compare our cross-modal approach to
single-modality (touch- or vision-only) approaches as well as other
curiosity-based methods and find that our method performs better and is more
sample-efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1"&gt;Sai Rajeswar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_C/0/1/0/all/0/1"&gt;Cyril Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1"&gt;Nitin Surya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1"&gt;Florian Golemo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1"&gt;David Vazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinheiro_P/0/1/0/all/0/1"&gt;Pedro O. Pinheiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Information in Stochastic Networks and Queues. (arXiv:2105.08769v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08769</id>
        <link href="http://arxiv.org/abs/2105.08769"/>
        <updated>2021-06-29T01:55:16.051Z</updated>
        <summary type="html"><![CDATA[We review the role of information and learning in the stability and
optimization of queueing systems. In recent years, techniques from supervised
learning, bandit learning and reinforcement learning have been applied to
queueing systems supported by increasing role of information in decision
making. We present observations and new results that help rationalize the
application of these areas to queueing systems.

We prove that the MaxWeight and BackPressure policies are an application of
Blackwell's Approachability Theorem. This connects queueing theoretic results
with adversarial learning. We then discuss the requirements of statistical
learning for service parameter estimation. As an example, we show how queue
size regret can be bounded when applying a perceptron algorithm to classify
service. Next, we discuss the role of state information in improved decision
making. Here we contrast the roles of epistemic information (information on
uncertain parameters) and aleatoric information (information on an uncertain
state). Finally we review recent advances in the theory of reinforcement
learning and queueing, as well as, provide discussion on current research
challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Walton_N/0/1/0/all/0/1"&gt;Neil Walton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kuang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Remote Sensing Image Retrieval and Classification with Robust Fine-tuning. (arXiv:2102.13392v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13392</id>
        <link href="http://arxiv.org/abs/2102.13392"/>
        <updated>2021-06-29T01:55:16.045Z</updated>
        <summary type="html"><![CDATA[Advances in high resolution remote sensing image analysis are currently
hampered by the difficulty of gathering enough annotated data for training deep
learning methods, giving rise to a variety of small datasets and associated
dataset-specific methods. Moreover, typical tasks such as classification and
retrieval lack a systematic evaluation on standard benchmarks and training
datasets, which make it hard to identify durable and generalizable scientific
contributions. We aim at unifying remote sensing image retrieval and
classification with a new large-scale training and testing dataset, SF300,
including both vertical and oblique aerial images and made available to the
research community, and an associated fine-tuning method. We additionally
propose a new adversarial fine-tuning method for global descriptors. We show
that our framework systematically achieves a boost of retrieval and
classification performance on nine different datasets compared to an ImageNet
pretrained baseline, with currently no other method to compare to.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gominski_D/0/1/0/all/0/1"&gt;Dimitri Gominski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gouet_Brunet_V/0/1/0/all/0/1"&gt;Val&amp;#xe9;rie Gouet-Brunet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quasiconformal model with CNN features for large deformation image registration. (arXiv:2011.00731v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00731</id>
        <link href="http://arxiv.org/abs/2011.00731"/>
        <updated>2021-06-29T01:55:16.038Z</updated>
        <summary type="html"><![CDATA[Image registration has been widely studied over the past several decades,
with numerous applications in science, engineering and medicine. Most of the
conventional mathematical models for large deformation image registration rely
on prescribed landmarks, which usually require tedious manual labeling and are
prone to error. In recent years, there has been a surge of interest in the use
of machine learning for image registration. In this paper, we develop a novel
method for large deformation image registration by a fusion of quasiconformal
theory and convolutional neural network (CNN). More specifically, we propose a
quasiconformal energy model with a novel fidelity term that incorporates the
features extracted using a pre-trained CNN, thereby allowing us to obtain
meaningful registration results without any guidance of prescribed landmarks.
Moreover, unlike many prior image registration methods, the bijectivity of our
method is guaranteed by quasiconformal theory. Experimental results are
presented to demonstrate the effectiveness of the proposed method. More
broadly, our work sheds light on how rigorous mathematical theories and
practical machine learning approaches can be integrated for developing
computational methods with improved performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Law_H/0/1/0/all/0/1"&gt;Ho Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1"&gt;Gary P. T. Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Ka Chun Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1"&gt;Lok Ming Lui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistency Regularization for Adversarial Robustness. (arXiv:2103.04623v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04623</id>
        <link href="http://arxiv.org/abs/2103.04623"/>
        <updated>2021-06-29T01:55:16.022Z</updated>
        <summary type="html"><![CDATA[Adversarial training (AT) is currently one of the most successful methods to
obtain the adversarial robustness of deep neural networks. However, the
phenomenon of robust overfitting, i.e., the robustness starts to decrease
significantly during AT, has been problematic, not only making practitioners
consider a bag of tricks for a successful training, e.g., early stopping, but
also incurring a significant generalization gap in the robustness. In this
paper, we propose an effective regularization technique that prevents robust
overfitting by optimizing an auxiliary 'consistency' regularization loss during
AT. Specifically, it forces the predictive distributions after attacking from
two different augmentations of the same instance to be similar with each other.
Our experimental results demonstrate that such a simple regularization
technique brings significant improvements in the test robust accuracy of a wide
range of AT methods. More remarkably, we also show that our method could
significantly help the model to generalize its robustness against unseen
adversaries, e.g., other types or larger perturbations compared to those used
during training. Code is available at
https://github.com/alinlab/consistency-adversarial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tack_J/0/1/0/all/0/1"&gt;Jihoon Tack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Sihyun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1"&gt;Jongheon Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation. (arXiv:2104.06957v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06957</id>
        <link href="http://arxiv.org/abs/2104.06957"/>
        <updated>2021-06-29T01:55:16.016Z</updated>
        <summary type="html"><![CDATA[Fully convolutional U-shaped neural networks have largely been the dominant
approach for pixel-wise image segmentation. In this work, we tackle two defects
that hinder their deployment in real-world applications: 1) Predictions lack
uncertainty quantification that may be crucial to many decision-making systems;
2) Large memory storage and computational consumption demanding extensive
hardware resources. To address these issues and improve their practicality we
demonstrate a few-parameter compact Bayesian convolutional architecture, that
achieves a marginal improvement in accuracy in comparison to related work using
significantly fewer parameters and compute operations. The architecture
combines parameter-efficient operations such as separable convolutions,
bilinear interpolation, multi-scale feature propagation and Bayesian inference
for per-pixel uncertainty quantification through Monte Carlo Dropout. The best
performing configurations required fewer than 2.5 million parameters on diverse
challenging datasets with few observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1"&gt;Martin Ferianc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Divyansh Manocha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hongxiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1"&gt;Miguel Rodrigues&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan Images. (arXiv:2106.14403v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14403</id>
        <link href="http://arxiv.org/abs/2106.14403"/>
        <updated>2021-06-29T01:55:16.009Z</updated>
        <summary type="html"><![CDATA[We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice
images. In this framework, the slice images of a CT-scan volume are first
proprocessed using segmentation techniques to filter out images of closed lung,
and to remove the useless background. Then a resampling method is used to
select one or multiple sets of a fixed number of slice images for training and
validation. A 3D CNN network with BERT is used to classify this set of selected
slice images. In this network, an embedding feature is also extracted. In cases
where there are more than one set of slice images in a volume, the features of
all sets are extracted and pooled into a global feature vector for the whole
CT-scan volume. A simple multiple-layer perceptron (MLP) network is used to
further classify the aggregated feature vector. The models are trained and
evaluated on the provided training and validation datasets. On the validation
dataset, the accuracy is 0.9278 and the F1 score is 0.9261.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1"&gt;Weijun Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingfeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularizing towards Causal Invariance: Linear Models with Proxies. (arXiv:2103.02477v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02477</id>
        <link href="http://arxiv.org/abs/2103.02477"/>
        <updated>2021-06-29T01:55:16.003Z</updated>
        <summary type="html"><![CDATA[We propose a method for learning linear models whose predictive performance
is robust to causal interventions on unobserved variables, when noisy proxies
of those variables are available. Our approach takes the form of a
regularization term that trades off between in-distribution performance and
robustness to interventions. Under the assumption of a linear structural causal
model, we show that a single proxy can be used to create estimators that are
prediction optimal under interventions of bounded strength. This strength
depends on the magnitude of the measurement noise in the proxy, which is, in
general, not identifiable. In the case of two proxy variables, we propose a
modified estimator that is prediction optimal under interventions up to a known
strength. We further show how to extend these estimators to scenarios where
additional information about the "test time" intervention is available during
training. We evaluate our theoretical findings in synthetic experiments and
using real data of hourly pollution levels across several cities in China.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oberst_M/0/1/0/all/0/1"&gt;Michael Oberst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thams_N/0/1/0/all/0/1"&gt;Nikolaj Thams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jonas Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Multi-Armed Bandits with Adaptive Inference. (arXiv:2102.13202v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13202</id>
        <link href="http://arxiv.org/abs/2102.13202"/>
        <updated>2021-06-29T01:55:15.996Z</updated>
        <summary type="html"><![CDATA[During online decision making in Multi-Armed Bandits (MAB), one needs to
conduct inference on the true mean reward of each arm based on data collected
so far at each step. However, since the arms are adaptively selected--thereby
yielding non-iid data--conducting inference accurately is not straightforward.
In particular, sample averaging, which is used in the family of UCB and
Thompson sampling (TS) algorithms, does not provide a good choice as it suffers
from bias and a lack of good statistical properties (e.g. asymptotic
normality). Our thesis in this paper is that more sophisticated inference
schemes that take into account the adaptive nature of the sequentially
collected data can unlock further performance gains, even though both UCB and
TS type algorithms are optimal in the worst case. In particular, we propose a
variant of TS-style algorithms--which we call doubly adaptive TS--that
leverages recent advances in causal inference and adaptively reweights the
terms of a doubly robust estimator on the true mean reward of each arm. Through
20 synthetic domain experiments and a semi-synthetic experiment based on data
from an A/B test of a web service, we demonstrate that using an adaptive
inferential scheme (while still retaining the exploration efficacy of TS)
provides clear benefits in online decision making: the proposed DATS algorithm
has superior empirical performance to existing baselines (UCB and TS) in terms
of regret and sample complexity in identifying the best arm. In addition, we
also provide a finite-time regret bound of doubly adaptive TS that matches (up
to log factors) those of UCB and TS algorithms, thereby establishing that its
improved practical benefits do not come at the expense of worst-case
suboptimality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dimakopoulou_M/0/1/0/all/0/1"&gt;Maria Dimakopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhimei Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhengyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Contrastive Learning Automated. (arXiv:2106.07594v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07594</id>
        <link href="http://arxiv.org/abs/2106.07594"/>
        <updated>2021-06-29T01:55:15.970Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning on graph-structured data has drawn recent interest
for learning generalizable, transferable and robust representations from
unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged
with promising representation learning performance. Unfortunately, unlike its
counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data
augmentations, which have to be manually picked per dataset, by either rules of
thumb or trial-and-errors, owing to the diverse nature of graph data. That
significantly limits the more general applicability of GraphCL. Aiming to fill
in this crucial gap, this paper proposes a unified bi-level optimization
framework to automatically, adaptively and dynamically select data
augmentations when performing GraphCL on specific graph data. The general
framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as
min-max optimization. The selections of augmentations made by JOAO are shown to
be in general aligned with previous "best practices" observed from handcrafted
tuning: yet now being automated, more flexible and versatile. Moreover, we
propose a new augmentation-aware projection head mechanism, which will route
output features through different projection heads corresponding to different
augmentations chosen at each training step. Extensive experiments demonstrate
that JOAO performs on par with or sometimes better than the state-of-the-art
competitors including GraphCL, on multiple graph datasets of various scales and
types, yet without resorting to any laborious dataset-specific tuning on
augmentation selection. We release the code at
https://github.com/Shen-Lab/GraphCL_Automated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yuning You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MAAD-Face: A Massively Annotated Attribute Dataset for Face Images. (arXiv:2012.01030v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01030</id>
        <link href="http://arxiv.org/abs/2012.01030"/>
        <updated>2021-06-29T01:55:15.962Z</updated>
        <summary type="html"><![CDATA[Soft-biometrics play an important role in face biometrics and related fields
since these might lead to biased performances, threatens the user's privacy, or
are valuable for commercial aspects. Current face databases are specifically
constructed for the development of face recognition applications. Consequently,
these databases contain large amount of face images but lack in the number of
attribute annotations and the overall annotation correctness. In this work, we
propose MAADFace, a new face annotations database that is characterized by the
large number of its high-quality attribute annotations. MAADFace is build on
the VGGFace2 database and thus, consists of 3.3M faces of over 9k individuals.
Using a novel annotation transfer-pipeline that allows an accurate
label-transfer from multiple source-datasets to a target-dataset, MAAD-Face
consists of 123.9M attribute annotations of 47 different binary attributes.
Consequently, it provides 15 and 137 times more attribute labels than CelebA
and LFW. Our investigation on the annotation quality by three human evaluators
demonstrated the superiority of the MAAD-Face annotations over existing
databases. Additionally, we make use of the large amount of high-quality
annotations from MAAD-Face to study the viability of soft-biometrics for
recognition, providing insights about which attributes support genuine and
imposter decisions. The MAAD-Face annotations dataset is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terhorst_P/0/1/0/all/0/1"&gt;Philipp Terh&amp;#xf6;rst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fahrmann_D/0/1/0/all/0/1"&gt;Daniel F&amp;#xe4;hrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolf_J/0/1/0/all/0/1"&gt;Jan Niklas Kolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1"&gt;Naser Damer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1"&gt;Florian Kirchbuchner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1"&gt;Arjan Kuijper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Semantic Segmentation with Directional Context-aware Consistency. (arXiv:2106.14133v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14133</id>
        <link href="http://arxiv.org/abs/2106.14133"/>
        <updated>2021-06-29T01:55:15.955Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation has made tremendous progress in recent years. However,
satisfying performance highly depends on a large number of pixel-level
annotations. Therefore, in this paper, we focus on the semi-supervised
segmentation problem where only a small set of labeled data is provided with a
much larger collection of totally unlabeled images. Nevertheless, due to the
limited annotations, models may overly rely on the contexts available in the
training data, which causes poor generalization to the scenes unseen before. A
preferred high-level representation should capture the contextual information
while not losing self-awareness. Therefore, we propose to maintain the
context-aware consistency between features of the same identity but with
different contexts, making the representations robust to the varying
environments. Moreover, we present the Directional Contrastive Loss (DC Loss)
to accomplish the consistency in a pixel-to-pixel manner, only requiring the
feature with lower quality to be aligned towards its counterpart. In addition,
to avoid the false-negative samples and filter the uncertain positive samples,
we put forward two sampling strategies. Extensive experiments show that our
simple yet effective method surpasses current state-of-the-art methods by a
large margin and also generalizes well with extra image-level annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1"&gt;Xin Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhuotao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Li Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hengshuang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Independent Component Analysis for Continuous-Time Signals. (arXiv:2102.02876v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02876</id>
        <link href="http://arxiv.org/abs/2102.02876"/>
        <updated>2021-06-29T01:55:15.939Z</updated>
        <summary type="html"><![CDATA[We study the classical problem of recovering a multidimensional source
process from observations of nonlinear mixtures of this process. Assuming
statistical independence of the coordinate processes of the source, we show
that this recovery is possible for many popular models of stochastic processes
(up to order and monotone scaling of their coordinates) if the mixture is given
by a sufficiently differentiable, invertible function. Key to our approach is
the combination of tools from stochastic analysis and recent contrastive
learning approaches to nonlinear ICA. This yields a scalable method with widely
applicable theoretical guarantees for which our experiments indicate good
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Schell_A/0/1/0/all/0/1"&gt;Alexander Schell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oberhauser_H/0/1/0/all/0/1"&gt;Harald Oberhauser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Touch-based Curiosity for Sparse-Reward Tasks. (arXiv:2104.00442v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00442</id>
        <link href="http://arxiv.org/abs/2104.00442"/>
        <updated>2021-06-29T01:55:15.934Z</updated>
        <summary type="html"><![CDATA[Robots in many real-world settings have access to force/torque sensors in
their gripper and tactile sensing is often necessary in tasks that involve
contact-rich motion. In this work, we leverage surprise from mismatches in
touch feedback to guide exploration in hard sparse-reward reinforcement
learning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible
objects interactions are supposed to "feel" like. We encourage exploration by
rewarding interactions where the expectation and the experience don't match. In
our proposed method, an initial task-independent exploration phase is followed
by an on-task learning phase, in which the original interactions are relabeled
with on-task rewards. We test our approach on a range of touch-intensive robot
arm tasks (e.g. pushing objects, opening doors), which we also release as part
of this work. Across multiple experiments in a simulated setting, we
demonstrate that our method is able to learn these difficult tasks through
sparse reward and curiosity alone. We compare our cross-modal approach to
single-modality (touch- or vision-only) approaches as well as other
curiosity-based methods and find that our method performs better and is more
sample-efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1"&gt;Sai Rajeswar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_C/0/1/0/all/0/1"&gt;Cyril Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1"&gt;Nitin Surya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1"&gt;Florian Golemo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1"&gt;David Vazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinheiro_P/0/1/0/all/0/1"&gt;Pedro O. Pinheiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GCN-SL: Graph Convolutional Networks with Structure Learning for Graphs under Heterophily. (arXiv:2105.13795v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13795</id>
        <link href="http://arxiv.org/abs/2105.13795"/>
        <updated>2021-06-29T01:55:15.919Z</updated>
        <summary type="html"><![CDATA[In representation learning on the graph-structured data, under heterophily
(or low homophily), many popular GNNs may fail to capture long-range
dependencies, which leads to their performance degradation. To solve the
above-mentioned issue, we propose a graph convolutional networks with structure
learning (GCN-SL), and furthermore, the proposed approach can be applied to
node classification. The proposed GCN-SL contains two improvements:
corresponding to node features and edges, respectively. In the aspect of node
features, we propose an efficient-spectral-clustering (ESC) and an ESC with
anchors (ESC-ANCH) algorithms to efficiently aggregate feature representations
from all similar nodes. In the aspect of edges, we build a re-connected
adjacency matrix by using a special data preprocessing technique and similarity
learning, and the re-connected adjacency matrix can be optimized directly along
with GCN-SL parameters. Considering that the original adjacency matrix may
provide misleading information for aggregation in GCN, especially the graphs
being with a low level of homophily. The proposed GCN-SL can aggregate feature
representations from nearby nodes via re-connected adjacency matrix and is
applied to graphs with various levels of homophily. Experimental results on a
wide range of benchmark datasets illustrate that the proposed GCN-SL
outperforms the stateof-the-art GNN counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Mengying Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guizhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yuanchao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xinliang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collective Intelligence: Decentralized Learning for Android Malware Detection in IoT with Blockchain. (arXiv:2102.13376v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13376</id>
        <link href="http://arxiv.org/abs/2102.13376"/>
        <updated>2021-06-29T01:55:15.913Z</updated>
        <summary type="html"><![CDATA[The widespread significance of Android IoT devices is due to its flexibility
and hardware support features which revolutionized the digital world by
introducing exciting applications almost in all walks of daily life, such as
healthcare, smart cities, smart environments, safety, remote sensing, and many
more. Such versatile applicability gives incentive for more malware attacks. In
this paper, we propose a framework which continuously aggregates multiple user
trained models on non-overlapping data into single model. Specifically for
malware detection task, (i) we propose a novel user (local) neural network
(LNN) which trains on local distribution and (ii) then to assure the model
authenticity and quality, we propose a novel smart contract which enable
aggregation process over blokchain platform. The LNN model analyzes various
static and dynamic features of both malware and benign whereas the smart
contract verifies the malicious applications both for uploading and downloading
processes in the network using stored aggregated features of local models. In
this way, the proposed model not only improves malware detection accuracy using
decentralized model network but also model efficacy with blockchain. We
evaluate our approach with three state-of-the-art models and performed deep
analyses of extracted features of the relative model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Rajesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;WenYong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_J/0/1/0/all/0/1"&gt;Jay Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakria/0/1/0/all/0/1"&gt;Zakria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Ting Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_W/0/1/0/all/0/1"&gt;Waqar Ali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Source-Filter GAN: Unified Source-filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN. (arXiv:2104.04668v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04668</id>
        <link href="http://arxiv.org/abs/2104.04668"/>
        <updated>2021-06-29T01:55:15.879Z</updated>
        <summary type="html"><![CDATA[We propose a unified approach to data-driven source-filter modeling using a
single neural network for developing a neural vocoder capable of generating
high-quality synthetic speech waveforms while retaining flexibility of the
source-filter model to control their voice characteristics. Our proposed
network called unified source-filter generative adversarial networks (uSFGAN)
is developed by factorizing quasi-periodic parallel WaveGAN (QPPWG), one of the
neural vocoders based on a single neural network, into a source excitation
generation network and a vocal tract resonance filtering network by
additionally implementing a regularization loss. Moreover, inspired by neural
source filter (NSF), only a sinusoidal waveform is additionally used as the
simplest clue to generate a periodic source excitation waveform while
minimizing the effect of approximations in the source filter model. The
experimental results demonstrate that uSFGAN outperforms conventional neural
vocoders, such as QPPWG and NSF in both speech quality and pitch
controllability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoneyama_R/0/1/0/all/0/1"&gt;Reo Yoneyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi-Chiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification and understanding of cloud structures via satellite images with EfficientUNet. (arXiv:2009.12931v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12931</id>
        <link href="http://arxiv.org/abs/2009.12931"/>
        <updated>2021-06-29T01:55:15.798Z</updated>
        <summary type="html"><![CDATA[Climate change has been a common interest and the forefront of crucial
political discussion and decision-making for many years. Shallow clouds play a
significant role in understanding the Earth's climate, but they are challenging
to interpret and represent in a climate model. By classifying these cloud
structures, there is a better possibility of understanding the physical
structures of the clouds, which would improve the climate model generation,
resulting in a better prediction of climate change or forecasting weather
update. Clouds organise in many forms, which makes it challenging to build
traditional rule-based algorithms to separate cloud features. In this paper,
classification of cloud organization patterns was performed using a new
scaled-up version of Convolutional Neural Network (CNN) named as EfficientNet
as the encoder and UNet as decoder where they worked as feature extractor and
reconstructor of fine grained feature map and was used as a classifier, which
will help experts to understand how clouds will shape the future climate. By
using a segmentation model in a classification task, it was shown that with a
good encoder alongside UNet, it is possible to obtain good performance from
this dataset. Dice coefficient has been used for the final evaluation metric,
which gave the score of 66.26\% and 66.02\% for public and private (test set)
leaderboard on Kaggle competition respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ahmed_T/0/1/0/all/0/1"&gt;Tashin Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sabab_N/0/1/0/all/0/1"&gt;Noor Hossain Nuri Sabab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MTrans: Multi-Modal Transformer for Accelerated MR Imaging. (arXiv:2106.14248v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14248</id>
        <link href="http://arxiv.org/abs/2106.14248"/>
        <updated>2021-06-29T01:55:15.773Z</updated>
        <summary type="html"><![CDATA[Accelerating multi-modal magnetic resonance (MR) imaging is a new and
effective solution for fast MR imaging, providing superior performance in
restoring the target modality from its undersampled counterpart with guidance
from an auxiliary modality. However, existing works simply introduce the
auxiliary modality as prior information, lacking in-depth investigations on the
potential mechanisms for fusing two modalities. Further, they usually rely on
the convolutional neural networks (CNNs), which focus on local information and
prevent them from fully capturing the long-distance dependencies of global
knowledge. To this end, we propose a multi-modal transformer (MTrans), which is
capable of transferring multi-scale features from the target modality to the
auxiliary modality, for accelerated MR imaging. By restructuring the
transformer architecture, our MTrans gains a powerful ability to capture deep
multi-modal information. More specifically, the target modality and the
auxiliary modality are first split into two branches and then fused using a
multi-modal transformer module. This module is based on an improved multi-head
attention mechanism, named the cross attention module, which absorbs features
from the auxiliary modality that contribute to the target modality. Our
framework provides two appealing benefits: (i) MTrans is the first attempt at
using improved transformers for multi-modal MR imaging, affording more global
information compared with CNN-based methods. (ii) A new cross attention module
is proposed to exploit the useful information in each branch at different
scales. It affords both distinct structural information and subtle pixel-level
information, which supplement the target modality effectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chun-Mei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yunlu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1"&gt;Geng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Better Explanations of Class Activation Mapping. (arXiv:2102.05228v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05228</id>
        <link href="http://arxiv.org/abs/2102.05228"/>
        <updated>2021-06-29T01:55:15.744Z</updated>
        <summary type="html"><![CDATA[Increasing demands for understanding the internal behavior of convolutional
neural networks (CNNs) have led to remarkable improvements in explanation
methods. Particularly, several class activation mapping (CAM) based methods,
which generate visual explanation maps by a linear combination of activation
maps from CNNs, have been proposed. However, the majority of the methods lack a
clear theoretical basis on how they assign the coefficients of the linear
combination. In this paper, we revisit the intrinsic linearity of CAM with
respect to the activation maps; we construct an explanation model of CNN as a
linear function of binary variables that denote the existence of the
corresponding activation maps. With this approach, the explanation model can be
determined by additive feature attribution methods in an analytic manner. We
then demonstrate the adequacy of SHAP values, which is a unique solution for
the explanation model with a set of desirable properties, as the coefficients
of CAM. Since the exact SHAP values are unattainable, we introduce an efficient
approximation method, LIFT-CAM, based on DeepLIFT. Our proposed LIFT-CAM can
estimate the SHAP values of the activation maps with high speed and accuracy.
Furthermore, it greatly outperforms other previous CAM-based methods in both
qualitative and quantitative aspects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1"&gt;Hyungsik Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1"&gt;Youngrock Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometric Processing for Image-based 3D Object Modeling. (arXiv:2106.14307v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14307</id>
        <link href="http://arxiv.org/abs/2106.14307"/>
        <updated>2021-06-29T01:55:15.738Z</updated>
        <summary type="html"><![CDATA[Image-based 3D object modeling refers to the process of converting raw
optical images to 3D digital representations of the objects. Very often, such
models are desired to be dimensionally true, semantically labeled with
photorealistic appearance (reality-based modeling). Laser scanning was deemed
as the standard (and direct) way to obtaining highly accurate 3D measurements
of objects, while one would have to abide the high acquisition cost and its
unavailability on some of the platforms. Nowadays the image-based methods
backboned by the recently developed advanced dense image matching algorithms
and geo-referencing paradigms, are becoming the dominant approaches, due to its
high flexibility, availability and low cost. The largely automated geometric
processing of images in a 3D object reconstruction workflow, from
ordered/unordered raw imagery to textured meshes, is becoming a critical part
of the reality-based 3D modeling. This article summarizes the overall geometric
processing workflow, with focuses on introducing the state-of-the-art methods
of three major components of geometric processing: 1) geo-referencing; 2) Image
dense matching 3) texture mapping. Finally, we will draw conclusions and share
our outlooks of the topics discussed in this article.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Rongjun Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[False Negative Reduction in Video Instance Segmentation using Uncertainty Estimates. (arXiv:2106.14474v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14474</id>
        <link href="http://arxiv.org/abs/2106.14474"/>
        <updated>2021-06-29T01:55:15.732Z</updated>
        <summary type="html"><![CDATA[Instance segmentation of images is an important tool for automated scene
understanding. Neural networks are usually trained to optimize their overall
performance in terms of accuracy. Meanwhile, in applications such as automated
driving, an overlooked pedestrian seems more harmful than a falsely detected
one. In this work, we present a false negative detection method for image
sequences based on inconsistencies in time series of tracked instances given
the availability of image sequences in online applications. As the number of
instances can be greatly increased by this algorithm, we apply a false positive
pruning using uncertainty estimates aggregated over instances. To this end,
instance-wise metrics are constructed which characterize uncertainty and
geometry of a given instance or are predicated on depth estimation. The
proposed method serves as a post-processing step applicable to any neural
network that can also be trained on single frames only. In our tests, we obtain
an improved trade-off between false negative and false positive instances by
our fused detection approach in comparison to the use of an ordinary score
value provided by the instance segmentation network during inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maag_K/0/1/0/all/0/1"&gt;Kira Maag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-paced Principal Component Analysis. (arXiv:2106.13880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13880</id>
        <link href="http://arxiv.org/abs/2106.13880"/>
        <updated>2021-06-29T01:55:15.726Z</updated>
        <summary type="html"><![CDATA[Principal Component Analysis (PCA) has been widely used for dimensionality
reduction and feature extraction. Robust PCA (RPCA), under different robust
distance metrics, such as l1-norm and l2, p-norm, can deal with noise or
outliers to some extent. However, real-world data may display structures that
can not be fully captured by these simple functions. In addition, existing
methods treat complex and simple samples equally. By contrast, a learning
pattern typically adopted by human beings is to learn from simple to complex
and less to more. Based on this principle, we propose a novel method called
Self-paced PCA (SPCA) to further reduce the effect of noise and outliers.
Notably, the complexity of each sample is calculated at the beginning of each
iteration in order to integrate samples from simple to more complex into
training. Based on an alternating optimization, SPCA finds an optimal
projection matrix and filters out outliers iteratively. Theoretical analysis is
presented to show the rationality of SPCA. Extensive experiments on popular
data sets demonstrate that the proposed method can improve the state of-the-art
results considerably.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1"&gt;Zhao Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiangxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaofeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Ling Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time-Series Representation Learning via Temporal and Contextual Contrasting. (arXiv:2106.14112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14112</id>
        <link href="http://arxiv.org/abs/2106.14112"/>
        <updated>2021-06-29T01:55:15.720Z</updated>
        <summary type="html"><![CDATA[Learning decent representations from unlabeled time-series data with temporal
dynamics is a very challenging task. In this paper, we propose an unsupervised
Time-Series representation learning framework via Temporal and Contextual
Contrasting (TS-TCC), to learn time-series representation from unlabeled data.
First, the raw time-series data are transformed into two different yet
correlated views by using weak and strong augmentations. Second, we propose a
novel temporal contrasting module to learn robust temporal representations by
designing a tough cross-view prediction task. Last, to further learn
discriminative representations, we propose a contextual contrasting module
built upon the contexts from the temporal contrasting module. It attempts to
maximize the similarity among different contexts of the same sample while
minimizing similarity among contexts of different samples. Experiments have
been carried out on three real-world time-series datasets. The results manifest
that training a linear classifier on top of the features learned by our
proposed TS-TCC performs comparably with the supervised training. Additionally,
our proposed TS-TCC shows high efficiency in few-labeled data and transfer
learning scenarios. The code is publicly available at
https://github.com/emadeldeen24/TS-TCC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eldele_E/0/1/0/all/0/1"&gt;Emadeldeen Eldele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragab_M/0/1/0/all/0/1"&gt;Mohamed Ragab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenghua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Min Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1"&gt;Chee Keong Kwoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pairing Conceptual Modeling with Machine Learning. (arXiv:2106.14251v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.14251</id>
        <link href="http://arxiv.org/abs/2106.14251"/>
        <updated>2021-06-29T01:55:15.703Z</updated>
        <summary type="html"><![CDATA[Both conceptual modeling and machine learning have long been recognized as
important areas of research. With the increasing emphasis on digitizing and
processing large amounts of data for business and other applications, it would
be helpful to consider how these areas of research can complement each other.
To understand how they can be paired, we provide an overview of machine
learning foundations and development cycle. We then examine how conceptual
modeling can be applied to machine learning and propose a framework for
incorporating conceptual modeling into data science projects. The framework is
illustrated by applying it to a healthcare application. For the inverse
pairing, machine learning can impact conceptual modeling through text and rule
mining, as well as knowledge graphs. The pairing of conceptual modeling and
machine learning in this this way should help lay the foundations for future
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1"&gt;Wolfgang Maass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Storey_V/0/1/0/all/0/1"&gt;Veda C. Storey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confounder-Aware Visualization of ConvNets. (arXiv:1907.12727v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.12727</id>
        <link href="http://arxiv.org/abs/1907.12727"/>
        <updated>2021-06-29T01:55:15.697Z</updated>
        <summary type="html"><![CDATA[With recent advances in deep learning, neuroimaging studies increasingly rely
on convolutional networks (ConvNets) to predict diagnosis based on MR images.
To gain a better understanding of how a disease impacts the brain, the studies
visualize the salience maps of the ConvNet highlighting voxels within the brain
majorly contributing to the prediction. However, these salience maps are
generally confounded, i.e., some salient regions are more predictive of
confounding variables (such as age) than the diagnosis. To avoid such
misinterpretation, we propose in this paper an approach that aims to visualize
confounder-free saliency maps that only highlight voxels predictive of the
diagnosis. The approach incorporates univariate statistical tests to identify
confounding effects within the intermediate features learned by ConvNet. The
influence from the subset of confounded features is then removed by a novel
partial back-propagation procedure. We use this two-step approach to visualize
confounder-free saliency maps extracted from synthetic and two real datasets.
These experiments reveal the potential of our visualization in producing
unbiased model-interpretation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1"&gt;Adolf Pfefferbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1"&gt;Edith V. Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking convolutional neural networks for diagnosing Lyme disease from images. (arXiv:2106.14465v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14465</id>
        <link href="http://arxiv.org/abs/2106.14465"/>
        <updated>2021-06-29T01:55:15.690Z</updated>
        <summary type="html"><![CDATA[Lyme disease is one of the most common infectious vector-borne diseases in
the world. In the early stage, the disease manifests itself in most cases with
erythema migrans (EM) skin lesions. Better diagnosis of these early forms would
allow improving the prognosis by preventing the transition to a severe late
form thanks to appropriate antibiotic therapy. Recent studies show that
convolutional neural networks (CNNs) perform very well to identify skin lesions
from the image but, there is not much work for Lyme disease prediction from EM
lesion images. The main objective of this study is to extensively analyze the
effectiveness of CNNs for diagnosing Lyme disease from images and to find out
the best CNN architecture for the purpose. There is no publicly available EM
image dataset for Lyme dis]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1"&gt;Sk Imran Hossain&lt;/a&gt; (LIMOS), &lt;a href="http://arxiv.org/find/eess/1/au:+Herve_J/0/1/0/all/0/1"&gt;Jocelyn de Go&amp;#xeb;r de Herve&lt;/a&gt; (INRAE), &lt;a href="http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1"&gt;Md Shahriar Hassan&lt;/a&gt; (LIMOS), &lt;a href="http://arxiv.org/find/eess/1/au:+Martineau_D/0/1/0/all/0/1"&gt;Delphine Martineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Petrosyan_E/0/1/0/all/0/1"&gt;Evelina Petrosyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Corbain_V/0/1/0/all/0/1"&gt;Violaine Corbain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beytout_J/0/1/0/all/0/1"&gt;Jean Beytout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lebert_I/0/1/0/all/0/1"&gt;Isabelle Lebert&lt;/a&gt; (INRAE), &lt;a href="http://arxiv.org/find/eess/1/au:+Baux_E/0/1/0/all/0/1"&gt;Elisabeth Baux&lt;/a&gt; (CHRU Nancy), &lt;a href="http://arxiv.org/find/eess/1/au:+Cazorla_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Cazorla&lt;/a&gt; (CHU de Saint-Etienne), &lt;a href="http://arxiv.org/find/eess/1/au:+Eldin_C/0/1/0/all/0/1"&gt;Carole Eldin&lt;/a&gt; (IHU M&amp;#xe9;diterran&amp;#xe9;e Infection), &lt;a href="http://arxiv.org/find/eess/1/au:+Hansmann_Y/0/1/0/all/0/1"&gt;Yves Hansmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patrat_Delon_S/0/1/0/all/0/1"&gt;Solene Patrat-Delon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prazuck_T/0/1/0/all/0/1"&gt;Thierry Prazuck&lt;/a&gt; (CHR), &lt;a href="http://arxiv.org/find/eess/1/au:+Raffetin_A/0/1/0/all/0/1"&gt;Alice Raffetin&lt;/a&gt; (CHIV), &lt;a href="http://arxiv.org/find/eess/1/au:+Tattevin_P/0/1/0/all/0/1"&gt;Pierre Tattevin&lt;/a&gt; (CHU Rennes), &lt;a href="http://arxiv.org/find/eess/1/au:+VourcH_G/0/1/0/all/0/1"&gt;Gwena&amp;#xeb;l Vourc&amp;#x27;H&lt;/a&gt; (INRAE), &lt;a href="http://arxiv.org/find/eess/1/au:+Lesens_O/0/1/0/all/0/1"&gt;Olivier Lesens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguifo_E/0/1/0/all/0/1"&gt;Engelbert Nguifo&lt;/a&gt; (LIMOS)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14463</id>
        <link href="http://arxiv.org/abs/2106.14463"/>
        <updated>2021-06-29T01:55:15.682Z</updated>
        <summary type="html"><![CDATA[Extracting structured clinical information from free-text radiology reports
can enable the use of radiology report information for a variety of critical
healthcare applications. In our work, we present RadGraph, a dataset of
entities and relations in full-text chest X-ray radiology reports based on a
novel information extraction schema we designed to structure radiology reports.
We release a development dataset, which contains board-certified radiologist
annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579
entities and 10,889 relations), and a test dataset, which contains two
independent sets of board-certified radiologist annotations for 100 radiology
reports split equally across the MIMIC-CXR and CheXpert datasets. Using these
datasets, we train and test a deep learning model, RadGraph Benchmark, that
achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR
and CheXpert test sets respectively. Additionally, we release an inference
dataset, which contains annotations automatically generated by RadGraph
Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4
million relations) and 500 CheXpert reports (13,783 entities and 9,908
relations) with mappings to associated chest radiographs. Our freely available
dataset can facilitate a wide range of research in medical natural language
processing, as well as computer vision and multi-modal learning when linked to
chest radiographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saahil Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Ashwin Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1"&gt;Adriel Saporta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1"&gt;Steven QH Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1"&gt;Du Nguyen Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tan Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1"&gt;Pierre Chambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1"&gt;Curtis P. Langlotz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels. (arXiv:2008.10271v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10271</id>
        <link href="http://arxiv.org/abs/2008.10271"/>
        <updated>2021-06-29T01:55:15.675Z</updated>
        <summary type="html"><![CDATA[We present a novel multi-view training framework and CNN architecture for
combining information from multiple overlapping satellite images and noisy
training labels derived from OpenStreetMap (OSM) to semantically label
buildings and roads across large geographic regions (100 km$^2$). Our approach
to multi-view semantic segmentation yields a 4-7% improvement in the per-class
IoU scores compared to the traditional approaches that use the views
independently of one another. A unique (and, perhaps, surprising) property of
our system is that modifications that are added to the tail-end of the CNN for
learning from the multi-view data can be discarded at the time of inference
with a relatively small penalty in the overall performance. This implies that
the benefits of training using multiple views are absorbed by all the layers of
the network. Additionally, our approach only adds a small overhead in terms of
the GPU-memory consumption even when training with as many as 32 views per
scene. The system we present is end-to-end automated, which facilitates
comparing the classifiers trained directly on true orthophotos vis-a-vis first
training them on the off-nadir images and subsequently translating the
predicted labels to geographical coordinates. With no human supervision, our
IoU scores for the buildings and roads classes are 0.8 and 0.64 respectively
which are better than state-of-the-art approaches that use OSM labels and that
are not completely automated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Comandur_B/0/1/0/all/0/1"&gt;Bharath Comandur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1"&gt;Avinash C. Kak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Generalizable Skills via Automated Generation of Diverse Tasks. (arXiv:2106.13935v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.13935</id>
        <link href="http://arxiv.org/abs/2106.13935"/>
        <updated>2021-06-29T01:55:15.661Z</updated>
        <summary type="html"><![CDATA[The learning efficiency and generalization ability of an intelligent agent
can be greatly improved by utilizing a useful set of skills. However, the
design of robot skills can often be intractable in real-world applications due
to the prohibitive amount of effort and expertise that it requires. In this
work, we introduce Skill Learning In Diversified Environments (SLIDE), a method
to discover generalizable skills via automated generation of a diverse set of
tasks. As opposed to prior work on unsupervised discovery of skills which
incentivizes the skills to produce different outcomes in the same environment,
our method pairs each skill with a unique task produced by a trainable task
generator. To encourage generalizable skills to emerge, our method trains each
skill to specialize in the paired task and maximizes the diversity of the
generated tasks. A task discriminator defined on the robot behaviors in the
generated tasks is jointly trained to estimate the evidence lower bound of the
diversity objective. The learned skills can then be composed in a hierarchical
reinforcement learning algorithm to solve unseen target tasks. We demonstrate
that the proposed method can effectively learn a variety of robot skills in two
tabletop manipulation domains. Our results suggest that the learned skills can
effectively improve the robot's performance in various unseen target tasks
compared to existing reinforcement learning and skill learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1"&gt;Kuan Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregating Incomplete and Noisy Rankings. (arXiv:2011.00810v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00810</id>
        <link href="http://arxiv.org/abs/2011.00810"/>
        <updated>2021-06-29T01:55:15.655Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning the true ordering of a set of
alternatives from largely incomplete and noisy rankings. We introduce a natural
generalization of both the classical Mallows model of ranking distributions and
the extensively studied model of noisy pairwise comparisons. Our selective
Mallows model outputs a noisy ranking on any given subset of alternatives,
based on an underlying Mallows distribution. Assuming a sequence of subsets
where each pair of alternatives appears frequently enough, we obtain strong
asymptotically tight upper and lower bounds on the sample complexity of
learning the underlying complete ranking and the (identities and the) ranking
of the top-k alternatives from selective Mallows rankings. Moreover, building
on the work of (Braverman and Mossel, 2009), we show how to efficiently compute
the maximum likelihood complete ranking from selective Mallows rankings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1"&gt;Dimitris Fotakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalavasis_A/0/1/0/all/0/1"&gt;Alkis Kalavasis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stavropoulos_K/0/1/0/all/0/1"&gt;Konstantinos Stavropoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Steerable 3D Spherical Neurons. (arXiv:2106.13863v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13863</id>
        <link href="http://arxiv.org/abs/2106.13863"/>
        <updated>2021-06-29T01:55:15.649Z</updated>
        <summary type="html"><![CDATA[Emerging from low-level vision theory, steerable filters found their
counterpart in deep learning. Earlier works used the steering theorems and
presented convolutional networks equivariant to rigid transformations. In our
work, we propose a steerable feed-forward learning-based approach that consists
of spherical decision surfaces and operates on point clouds. Due to the
inherent geometric 3D structure of our theory, we derive a 3D steerability
constraint for its atomic parts, the hypersphere neurons. Exploiting the
rotational equivariance, we show how the model parameters are fully steerable
at inference time. The proposed spherical filter banks enable to make
equivariant and, after online optimization, invariant class predictions for
known synthetic point sets in unknown orientations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1"&gt;Pavlo Melnyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1"&gt;Michael Felsberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1"&gt;M&amp;#xe5;rten Wadenb&amp;#xe4;ck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Forests for dependent data. (arXiv:2007.15421v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15421</id>
        <link href="http://arxiv.org/abs/2007.15421"/>
        <updated>2021-06-29T01:55:15.643Z</updated>
        <summary type="html"><![CDATA[Random forest (RF) is one of the most popular methods for estimating
regression functions. The local nature of the RF algorithm, based on intra-node
means and variances, is ideal when errors are i.i.d. For dependent error
processes like time series and spatial settings where data in all the nodes
will be correlated, operating locally ignores this dependence. Also, RF will
involve resampling of correlated data, violating the principles of bootstrap.
Theoretically, consistency of RF has been established for i.i.d. errors, but
little is known about the case of dependent errors.

We propose RF-GLS, a novel extension of RF for dependent error processes in
the same way Generalized Least Squares (GLS) fundamentally extends Ordinary
Least Squares (OLS) for linear models under dependence. The key to this
extension is the equivalent representation of the local decision-making in a
regression tree as a global OLS optimization which is then replaced with a GLS
loss to create a GLS-style regression tree. This also synergistically addresses
the resampling issue, as the use of GLS loss amounts to resampling uncorrelated
contrasts (pre-whitened data) instead of the correlated data. For spatial
settings, RF-GLS can be used in conjunction with Gaussian Process correlated
errors to generate kriging predictions at new locations. RF becomes a special
case of RF-GLS with an identity working covariance matrix.

We establish consistency of RF-GLS under beta- (absolutely regular) mixing
error processes and show that this general result subsumes important cases like
autoregressive time series and spatial Matern Gaussian Processes. As a
byproduct, we also establish consistency of RF for beta-mixing processes, which
to our knowledge, is the first such result for RF under dependence.

We empirically demonstrate the improvement achieved by RF-GLS over RF for
both estimation and prediction under dependence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Saha_A/0/1/0/all/0/1"&gt;Arkajyoti Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1"&gt;Sumanta Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Datta_A/0/1/0/all/0/1"&gt;Abhirup Datta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LSMI-Sinkhorn: Semi-supervised Mutual Information Estimation with Optimal Transport. (arXiv:1909.02373v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.02373</id>
        <link href="http://arxiv.org/abs/1909.02373"/>
        <updated>2021-06-29T01:55:15.637Z</updated>
        <summary type="html"><![CDATA[Estimating mutual information is an important statistics and machine learning
problem. To estimate the mutual information from data, a common practice is
preparing a set of paired samples $\{(\mathbf{x}_i,\mathbf{y}_i)\}_{i=1}^n
\stackrel{\mathrm{i.i.d.}}{\sim} p(\mathbf{x},\mathbf{y})$. However, in many
situations, it is difficult to obtain a large number of data pairs. To address
this problem, we propose the semi-supervised Squared-loss Mutual Information
(SMI) estimation method using a small number of paired samples and the
available unpaired ones. We first represent SMI through the density ratio
function, where the expectation is approximated by the samples from marginals
and its assignment parameters. The objective is formulated using the optimal
transport problem and quadratic programming. Then, we introduce the
Least-Squares Mutual Information with Sinkhorn (LSMI-Sinkhorn) algorithm for
efficient optimization. Through experiments, we first demonstrate that the
proposed method can estimate the SMI without a large number of paired samples.
Then, we show the effectiveness of the proposed LSMI-Sinkhorn algorithm on
various types of machine learning problems such as image matching and photo
album summarization. Code can be found at
https://github.com/csyanbin/LSMI-Sinkhorn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanbin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yamada_M/0/1/0/all/0/1"&gt;Makoto Yamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yao-Hung Hubert Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1"&gt;Tam Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Role of Contextual Information in Best Arm Identification. (arXiv:2106.14077v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14077</id>
        <link href="http://arxiv.org/abs/2106.14077"/>
        <updated>2021-06-29T01:55:15.620Z</updated>
        <summary type="html"><![CDATA[We study the best-arm identification problem with fixed confidence when
contextual (covariate) information is available in stochastic bandits. Although
we can use contextual information in each round, we are interested in the
marginalized mean reward over the contextual distribution. Our goal is to
identify the best arm with a minimal number of samplings under a given value of
the error rate. We show the instance-specific sample complexity lower bounds
for the problem. Then, we propose a context-aware version of the
"Track-and-Stop" strategy, wherein the proportion of the arm draws tracks the
set of optimal allocations and prove that the expected number of arm draws
matches the lower bound asymptotically. We demonstrate that contextual
information can be used to improve the efficiency of the identification of the
best marginalized mean reward compared with the results of Garivier & Kaufmann
(2016). We experimentally confirm that context information contributes to
faster best-arm identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1"&gt;Masahiro Kato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ariu_K/0/1/0/all/0/1"&gt;Kaito Ariu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Sparse Coding using Hierarchical Riemannian Pursuit. (arXiv:2104.10314v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10314</id>
        <link href="http://arxiv.org/abs/2104.10314"/>
        <updated>2021-06-29T01:55:15.614Z</updated>
        <summary type="html"><![CDATA[Sparse coding is a class of unsupervised methods for learning a sparse
representation of the input data in the form of a linear combination of a
dictionary and a sparse code. This learning framework has led to
state-of-the-art results in various image and video processing tasks. However,
classical methods learn the dictionary and the sparse code based on alternative
optimizations, usually without theoretical guarantees for either optimality or
convergence due to non-convexity of the problem. Recent works on sparse coding
with a complete dictionary provide strong theoretical guarantees thanks to the
development of the non-convex optimization. However, initial non-convex
approaches learn the dictionary in the sparse coding problem sequentially in an
atom-by-atom manner, which leads to a long execution time. More recent works
seek to directly learn the entire dictionary at once, which substantially
reduces the execution time. However, the associated recovery performance is
degraded with a finite number of data samples. In this paper, we propose an
efficient sparse coding scheme with a two-stage optimization. The proposed
scheme leverages the global and local Riemannian geometry of the two-stage
optimization problem and facilitates fast implementation for superb dictionary
recovery performance by a finite number of samples without atom-by-atom
calculation. We further prove that, with high probability, the proposed scheme
can exactly recover any atom in the target dictionary with a finite number of
samples if it is adopted to recover one atom of the dictionary. An application
on wireless sensor data compression is also proposed. Experiments on both
synthetic and real-world data verify the efficiency and effectiveness of the
proposed scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Ye Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1"&gt;Vincent Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1"&gt;Songfu Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closed-form Continuous-Depth Models. (arXiv:2106.13898v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13898</id>
        <link href="http://arxiv.org/abs/2106.13898"/>
        <updated>2021-06-29T01:55:15.608Z</updated>
        <summary type="html"><![CDATA[Continuous-depth neural models, where the derivative of the model's hidden
state is defined by a neural network, have enabled strong sequential data
processing capabilities. However, these models rely on advanced numerical
differential equation (DE) solvers resulting in a significant overhead both in
terms of computational cost and model complexity. In this paper, we present a
new family of models, termed Closed-form Continuous-depth (CfC) networks, that
are simple to describe and at least one order of magnitude faster while
exhibiting equally strong modeling abilities compared to their ODE-based
counterparts. The models are hereby derived from the analytical closed-form
solution of an expressive subset of time-continuous models, thus alleviating
the need for complex DE solvers all together. In our experimental evaluations,
we demonstrate that CfC networks outperform advanced, recurrent models over a
diverse set of time-series prediction tasks, including those with long-term
dependencies and irregularly sampled data. We believe our findings open new
opportunities to train and deploy rich, continuous neural models in
resource-constrained settings, which demand both performance and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1"&gt;Ramin Hasani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1"&gt;Mathias Lechner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1"&gt;Alexander Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1"&gt;Lucas Liebenwein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tschaikowski_M/0/1/0/all/0/1"&gt;Max Tschaikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teschl_G/0/1/0/all/0/1"&gt;Gerald Teschl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Score-Based Change Detection for Gradient-Based Learning Machines. (arXiv:2106.14122v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.14122</id>
        <link href="http://arxiv.org/abs/2106.14122"/>
        <updated>2021-06-29T01:55:15.603Z</updated>
        <summary type="html"><![CDATA[The widespread use of machine learning algorithms calls for automatic change
detection algorithms to monitor their behavior over time. As a machine learning
algorithm learns from a continuous, possibly evolving, stream of data, it is
desirable and often critical to supplement it with a companion change detection
algorithm to facilitate its monitoring and control. We present a generic
score-based change detection method that can detect a change in any number of
components of a machine learning model trained via empirical risk minimization.
This proposed statistical hypothesis test can be readily implemented for such
models designed within a differentiable programming framework. We establish the
consistency of the hypothesis test and show how to calibrate it to achieve a
prescribed false alarm rate. We illustrate the versatility of the approach on
synthetic and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Salmon_J/0/1/0/all/0/1"&gt;Joseph Salmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1"&gt;Zaid Harchaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection. (arXiv:2106.14447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14447</id>
        <link href="http://arxiv.org/abs/2106.14447"/>
        <updated>2021-06-29T01:55:15.597Z</updated>
        <summary type="html"><![CDATA[With rapidly evolving internet technologies and emerging tools, sports
related videos generated online are increasing at an unprecedentedly fast pace.
To automate sports video editing/highlight generation process, a key task is to
precisely recognize and locate the events in the long untrimmed videos. In this
tech report, we present a two-stage paradigm to detect what and when events
happen in soccer broadcast videos. Specifically, we fine-tune multiple action
recognition models on soccer data to extract high-level semantic features, and
design a transformer based temporal detection module to locate the target
events. This approach achieved the state-of-the-art performance in both two
tasks, i.e., action spotting and replay grounding, in the SoccerNet-v2
Challenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features
are released at https://github.com/baidu-research/vidpress-sports. By sharing
these features with the broader community, we hope to accelerate the research
into soccer video understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1"&gt;Le Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhiyu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1"&gt;Bo He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jingyu Xin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning stochastic object models from medical imaging measurements by use of advanced AmbientGANs. (arXiv:2106.14324v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14324</id>
        <link href="http://arxiv.org/abs/2106.14324"/>
        <updated>2021-06-29T01:55:15.581Z</updated>
        <summary type="html"><![CDATA[In order to objectively assess new medical imaging technologies via
computer-simulations, it is important to account for all sources of variability
that contribute to image data. One important source of variability that can
significantly limit observer performance is associated with the variability in
the ensemble of objects to-be-imaged. This source of variability can be
described by stochastic object models (SOMs), which are generative models that
can be employed to sample from a distribution of to-be-virtually-imaged
objects. It is generally desirable to establish SOMs from experimental imaging
measurements acquired by use of a well-characterized imaging system, but this
task has remained challenging. Deep generative neural networks, such as
generative adversarial networks (GANs) hold potential for such tasks. To
establish SOMs from imaging measurements, an AmbientGAN has been proposed that
augments a GAN with a measurement operator. However, the original AmbientGAN
could not immediately benefit from modern training procedures and GAN
architectures, which limited its ability to be applied to realistically sized
medical image data. To circumvent this, in this work, a modified AmbientGAN
training strategy is proposed that is suitable for modern progressive or
multi-resolution training approaches such as employed in the Progressive
Growing of GANs and Style-based GANs. AmbientGANs established by use of the
proposed training procedure are systematically validated in a controlled way by
use of computer-simulated measurement data corresponding to a stylized imaging
system. Finally, emulated single-coil experimental magnetic resonance imaging
data are employed to demonstrate the methods under less stylized conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Weimin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1"&gt;Sayantan Bhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1"&gt;Frank J. Brooks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1"&gt;Mark A. Anastasio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certified Robustness via Randomized Smoothing over Multiplicative Parameters. (arXiv:2106.14432v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14432</id>
        <link href="http://arxiv.org/abs/2106.14432"/>
        <updated>2021-06-29T01:55:15.575Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach of randomized smoothing over multiplicative
parameters. Using this method we construct certifiably robust classifiers with
respect to a gamma-correction perturbation and compare the result with
classifiers obtained via Gaussian smoothing. To the best of our knowledge it is
the first work concerning certified robustness against the multiplicative
gamma-correction transformation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muravev_N/0/1/0/all/0/1"&gt;Nikita Muravev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1"&gt;Aleksandr Petiushko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Moment Loss for Medical Image Segmentation. (arXiv:2106.14178v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14178</id>
        <link href="http://arxiv.org/abs/2106.14178"/>
        <updated>2021-06-29T01:55:15.569Z</updated>
        <summary type="html"><![CDATA[Location information is proven to benefit the deep learning models on
capturing the manifold structure of target objects, and accordingly boosts the
accuracy of medical image segmentation. However, most existing methods encode
the location information in an implicit way, e.g. the distance transform maps,
which describe the relative distance from each pixel to the contour boundary,
for the network to learn. These implicit approaches do not fully exploit the
position information (i.e. absolute location) of targets. In this paper, we
propose a novel loss function, namely residual moment (RM) loss, to explicitly
embed the location information of segmentation targets during the training of
deep learning networks. Particularly, motivated by image moments, the
segmentation prediction map and ground-truth map are weighted by coordinate
information. Then our RM loss encourages the networks to maintain the
consistency between the two weighted maps, which promotes the segmentation
networks to easily locate the targets and extract manifold-structure-related
features. We validate the proposed RM loss by conducting extensive experiments
on two publicly available datasets, i.e., 2D optic cup and disk segmentation
and 3D left atrial segmentation. The experimental results demonstrate the
effectiveness of our RM loss, which significantly boosts the accuracy of
segmentation networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Quanziang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renzhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuexiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessment Modeling: Fundamental Pre-training Tasks for Interactive Educational Systems. (arXiv:2002.05505v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05505</id>
        <link href="http://arxiv.org/abs/2002.05505"/>
        <updated>2021-06-29T01:55:15.563Z</updated>
        <summary type="html"><![CDATA[Like many other domains in Artificial Intelligence (AI), there are specific
tasks in the field of AI in Education (AIEd) for which labels are scarce and
expensive, such as predicting exam score or review correctness. A common way of
circumventing label-scarce problems is pre-training a model to learn
representations of the contents of learning items. However, such methods fail
to utilize the full range of student interaction data available and do not
model student learning behavior. To this end, we propose Assessment Modeling, a
class of fundamental pre-training tasks for general interactive educational
systems. An assessment is a feature of student-system interactions which can
serve as a pedagogical evaluation. Examples include the correctness and
timeliness of a student's answer. Assessment Modeling is the prediction of
assessments conditioned on the surrounding context of interactions. Although it
is natural to pre-train on interactive features available in large amounts,
limiting the prediction targets to assessments focuses the tasks' relevance to
the label-scarce educational problems and reduces less-relevant noise. While
the effectiveness of different combinations of assessments is open for
exploration, we suggest Assessment Modeling as a first-order guiding principle
for selecting proper pre-training tasks for label-scarce educational problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Youngduck Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Youngnam Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1"&gt;Junghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1"&gt;Jineon Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1"&gt;Dongmin Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hangyeol Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shim_Y/0/1/0/all/0/1"&gt;Yugeun Shim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seewoo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jonghun Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bae_C/0/1/0/all/0/1"&gt;Chan Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Byungsoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1"&gt;Jaewe Heo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Deep Ensembles for Blind Image Quality Assessment. (arXiv:2106.14008v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14008</id>
        <link href="http://arxiv.org/abs/2106.14008"/>
        <updated>2021-06-29T01:55:15.557Z</updated>
        <summary type="html"><![CDATA[Ensemble methods are generally regarded to be better than a single model if
the base learners are deemed to be "accurate" and "diverse." Here we
investigate a semi-supervised ensemble learning strategy to produce
generalizable blind image quality assessment models. We train a multi-head
convolutional network for quality prediction by maximizing the accuracy of the
ensemble (as well as the base learners) on labeled data, and the disagreement
(i.e., diversity) among them on unlabeled data, both implemented by the
fidelity loss. We conduct extensive experiments to demonstrate the advantages
of employing unlabeled data for BIQA, especially in model generalization and
failure identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhihua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dingquan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kede Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mode-wise Tensor Decompositions: Multi-dimensional Generalizations of CUR Decompositions. (arXiv:2103.11037v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11037</id>
        <link href="http://arxiv.org/abs/2103.11037"/>
        <updated>2021-06-29T01:55:15.540Z</updated>
        <summary type="html"><![CDATA[Low rank tensor approximation is a fundamental tool in modern machine
learning and data science. In this paper, we study the characterization,
perturbation analysis, and an efficient sampling strategy for two primary
tensor CUR approximations, namely Chidori and Fiber CUR. We characterize exact
tensor CUR decompositions for low multilinear rank tensors. We also present
theoretical error bounds of the tensor CUR approximations when (adversarial or
Gaussian) noise appears. Moreover, we show that low cost uniform sampling is
sufficient for tensor CUR approximations if the tensor has an incoherent
structure. Empirical performance evaluations, with both synthetic and
real-world datasets, establish the speed advantage of the tensor CUR
approximations over other state-of-the-art low multilinear rank tensor
approximations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cai_H/0/1/0/all/0/1"&gt;HanQin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hamm_K/0/1/0/all/0/1"&gt;Keaton Hamm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longxiu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Needell_D/0/1/0/all/0/1"&gt;Deanna Needell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning with Echo State Networks. (arXiv:2105.07674v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07674</id>
        <link href="http://arxiv.org/abs/2105.07674"/>
        <updated>2021-06-29T01:55:15.533Z</updated>
        <summary type="html"><![CDATA[Continual Learning (CL) refers to a learning setup where data is non
stationary and the model has to learn without forgetting existing knowledge.
The study of CL for sequential patterns revolves around trained recurrent
networks. In this work, instead, we introduce CL in the context of Echo State
Networks (ESNs), where the recurrent component is kept fixed. We provide the
first evaluation of catastrophic forgetting in ESNs and we highlight the
benefits in using CL strategies which are not applicable to trained recurrent
models. Our results confirm the ESN as a promising model for CL and open to its
use in streaming scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1"&gt;Andrea Cossu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1"&gt;Antonio Carta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1"&gt;Claudio Gallicchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1"&gt;Vincenzo Lomonaco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Implications of the No-Free-Lunch Theorems for Meta-induction. (arXiv:2103.11956v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11956</id>
        <link href="http://arxiv.org/abs/2103.11956"/>
        <updated>2021-06-29T01:55:15.528Z</updated>
        <summary type="html"><![CDATA[The important recent book by G. Schurz appreciates that the no-free-lunch
theorems (NFL) have major implications for the problem of (meta) induction.
Here I review the NFL theorems, emphasizing that they do not only concern the
case where there is a uniform prior -- they prove that there are "as many
priors" (loosely speaking) for which any induction algorithm $A$
out-generalizes some induction algorithm $B$ as vice-versa. Importantly though,
in addition to the NFL theorems, there are many \textit{free lunch} theorems.
In particular, the NFL theorems can only be used to compare the
\textit{marginal} expected performance of an induction algorithm $A$ with the
marginal expected performance of an induction algorithm $B$. There is a rich
set of free lunches which instead concern the statistical correlations among
the generalization errors of induction algorithms. As I describe, the
meta-induction algorithms that Schurz advocate as a "solution to Hume's
problem" are just an example of such a free lunch based on correlations among
the generalization errors of induction algorithms. I end by pointing out that
the prior that Schurz advocates, which is uniform over bit frequencies rather
than bit patterns, is contradicted by thousands of experiments in statistical
physics and by the great success of the maximum entropy procedure in inductive
inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolpert_D/0/1/0/all/0/1"&gt;David H. Wolpert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Universal Generalized PageRank Graph Neural Network. (arXiv:2006.07988v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07988</id>
        <link href="http://arxiv.org/abs/2006.07988"/>
        <updated>2021-06-29T01:55:15.522Z</updated>
        <summary type="html"><![CDATA[In many important graph data processing applications the acquired information
includes both node features and observations of the graph topology. Graph
neural networks (GNNs) are designed to exploit both sources of evidence but
they do not optimally trade-off their utility and integrate them in a manner
that is also universal. Here, universality refers to independence on homophily
or heterophily graph assumptions. We address these issues by introducing a new
Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR
weights so as to jointly optimize node feature and topological information
extraction, regardless of the extent to which the node labels are homophilic or
heterophilic. Learned GPR weights automatically adjust to the node label
pattern, irrelevant on the type of initialization, and thereby guarantee
excellent learning performance for label patterns that are usually hard to
handle. Furthermore, they allow one to avoid feature over-smoothing, a process
which renders feature information nondiscriminative, without requiring the
network to be shallow. Our accompanying theoretical analysis of the GPR-GNN
method is facilitated by novel synthetic benchmark datasets generated by the
so-called contextual stochastic block model. We also compare the performance of
our GNN architecture with that of several state-of-the-art GNNs on the problem
of node-classification, using well-known benchmark homophilic and heterophilic
datasets. The results demonstrate that GPR-GNN offers significant performance
improvement compared to existing techniques on both synthetic and benchmark
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1"&gt;Eli Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jianhao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1"&gt;Olgica Milenkovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm is Experiment: Machine Learning, Market Design, and Policy Eligibility Rules. (arXiv:2104.12909v2 [econ.EM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12909</id>
        <link href="http://arxiv.org/abs/2104.12909"/>
        <updated>2021-06-29T01:55:15.515Z</updated>
        <summary type="html"><![CDATA[Algorithms produce a growing portion of decisions and recommendations both in
policy and business. Such algorithmic decisions are natural experiments
(conditionally quasi-randomly assigned instruments) since the algorithms make
decisions based only on observable input variables. We use this observation to
develop a treatment-effect estimator for a class of stochastic and
deterministic decision-making algorithms. Our estimator is shown to be
consistent and asymptotically normal for well-defined causal effects. A key
special case of our estimator is a multidimensional regression discontinuity
design. We apply our estimator to evaluate the effect of the Coronavirus Aid,
Relief, and Economic Security (CARES) Act, where more than \$175 billion worth
of relief funding is allocated to hospitals via an algorithmic rule. Our
estimates suggest that the relief funding has little effect on COVID-19-related
hospital activity levels. Naive OLS and IV estimates exhibit substantial
selection bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Narita_Y/0/1/0/all/0/1"&gt;Yusuke Narita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Yata_K/0/1/0/all/0/1"&gt;Kohei Yata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural-symbolic Approach for Ontology-mediated Query Answering. (arXiv:2106.14052v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.14052</id>
        <link href="http://arxiv.org/abs/2106.14052"/>
        <updated>2021-06-29T01:55:15.493Z</updated>
        <summary type="html"><![CDATA[Recently, low-dimensional vector space representations of knowledge graphs
(KGs) have been applied to find answers to conjunctive queries (CQs) over
incomplete KGs. However, the current methods only focus on inductive reasoning,
i.e. answering CQs by predicting facts based on patterns learned from the data,
and lack the ability of deductive reasoning by applying external domain
knowledge. Such (expert or commonsense) domain knowledge is an invaluable
resource which can be used to advance machine intelligence. To address this
shortcoming, we introduce a neural-symbolic method for ontology-mediated CQ
answering over incomplete KGs that operates in the embedding space. More
specifically, we propose various data augmentation strategies to generate
training queries using query-rewriting based methods and then exploit a novel
loss function for training the model. The experimental results demonstrate the
effectiveness of our training strategies and the new loss function, i.e., our
method significantly outperforms the baseline in the settings that require both
inductive and deductive reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Andresel_M/0/1/0/all/0/1"&gt;Medina Andresel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domokos_C/0/1/0/all/0/1"&gt;Csaba Domokos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stepanova_D/0/1/0/all/0/1"&gt;Daria Stepanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Trung-Kien Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI based Presentation Creator With Customized Audio Content Delivery. (arXiv:2106.14213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14213</id>
        <link href="http://arxiv.org/abs/2106.14213"/>
        <updated>2021-06-29T01:55:15.485Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an architecture to solve a novel problem statement
that has stemmed more so in recent times with an increase in demand for virtual
content delivery due to the COVID-19 pandemic. All educational institutions,
workplaces, research centers, etc. are trying to bridge the gap of
communication during these socially distanced times with the use of online
content delivery. The trend now is to create presentations, and then
subsequently deliver the same using various virtual meeting platforms. The time
being spent in such creation of presentations and delivering is what we try to
reduce and eliminate through this paper which aims to use Machine Learning (ML)
algorithms and Natural Language Processing (NLP) modules to automate the
process of creating a slides-based presentation from a document, and then use
state-of-the-art voice cloning models to deliver the content in the desired
author's voice. We consider a structured document such as a research paper to
be the content that has to be presented. The research paper is first summarized
using BERT summarization techniques and condensed into bullet points that go
into the slides. Tacotron inspired architecture with Encoder, Synthesizer, and
a Generative Adversarial Network (GAN) based vocoder, is used to convey the
contents of the slides in the author's voice (or any customized voice). Almost
all learning has now been shifted to online mode, and professionals are now
working from the comfort of their homes. Due to the current situation, teachers
and professionals have shifted to presentations to help them in imparting
information. In this paper, we aim to reduce the considerable amount of time
that is taken in creating a presentation by automating this process and
subsequently delivering this presentation in a customized voice, using a
content delivery mechanism that can clone any voice using a short audio clip.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansoor_M/0/1/0/all/0/1"&gt;Muvazima Mansoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Srikanth Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinath_R/0/1/0/all/0/1"&gt;Ramamoorthy Srinath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smart Inference for Multidigit Convolutional Neural Network based Barcode Decoding. (arXiv:2004.06297v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.06297</id>
        <link href="http://arxiv.org/abs/2004.06297"/>
        <updated>2021-06-29T01:55:15.479Z</updated>
        <summary type="html"><![CDATA[Barcodes are ubiquitous and have been used in most of critical daily
activities for decades. However, most of traditional decoders require
well-founded barcode under a relatively standard condition. While wilder
conditioned barcodes such as underexposed, occluded, blurry, wrinkled and
rotated are commonly captured in reality, those traditional decoders show
weakness of recognizing. Several works attempted to solve those challenging
barcodes, but many limitations still exist. This work aims to solve the
decoding problem using deep convolutional neural network with the possibility
of running on portable devices. Firstly, we proposed a special modification of
inference based on the feature of having checksum and test-time augmentation,
named as Smart Inference (SI) in prediction phase of a trained model. SI
considerably boosts accuracy and reduces the false prediction for trained
models. Secondly, we have created a large practical evaluation dataset of real
captured 1D barcode under various challenging conditions to test our methods
vigorously, which is publicly available for other researchers. The experiments'
results demonstrated the SI effectiveness with the highest accuracy of 95.85%
which outperformed many existing decoders on the evaluation set. Finally, we
successfully minimized the best model by knowledge distillation to a shallow
model which is shown to have high accuracy (90.85%) with good inference speed
of 34.2 ms per image on a real edge device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1"&gt;Thao Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolcha_Y/0/1/0/all/0/1"&gt;Yalew Tolcha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_T/0/1/0/all/0/1"&gt;Tae Joon Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daeyoung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Midpoint Regularization: from High Uncertainty Training to Conservative Classification. (arXiv:2106.13913v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13913</id>
        <link href="http://arxiv.org/abs/2106.13913"/>
        <updated>2021-06-29T01:55:15.473Z</updated>
        <summary type="html"><![CDATA[Label Smoothing (LS) improves model generalization through penalizing models
from generating overconfident output distributions. For each training sample
the LS strategy smooths the one-hot encoded training signal by distributing its
distribution mass over the non-ground truth classes. We extend this technique
by considering example pairs, coined PLS. PLS first creates midpoint samples by
averaging random sample pairs and then learns a smoothing distribution during
training for each of these midpoint samples, resulting in midpoints with high
uncertainty labels for training. We empirically show that PLS significantly
outperforms LS, achieving up to 30% of relative classification error reduction.
We also visualize that PLS produces very low winning softmax scores for both in
and out of distribution samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Classification with CondenseNeXt for ARM-Based Computing Platforms. (arXiv:2106.14102v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14102</id>
        <link href="http://arxiv.org/abs/2106.14102"/>
        <updated>2021-06-29T01:55:15.467Z</updated>
        <summary type="html"><![CDATA[In this paper, we demonstrate the implementation of our ultra-efficient deep
convolutional neural network architecture: CondenseNeXt on NXP BlueBox, an
autonomous driving development platform developed for self-driving vehicles. We
show that CondenseNeXt is remarkably efficient in terms of FLOPs, designed for
ARM-based embedded computing platforms with limited computational resources and
can perform image classification without the need of a CUDA enabled GPU.
CondenseNeXt utilizes the state-of-the-art depthwise separable convolution and
model compression techniques to achieve a remarkable computational efficiency.
Extensive analyses are conducted on CIFAR-10, CIFAR-100 and ImageNet datasets
to verify the performance of CondenseNeXt Convolutional Neural Network (CNN)
architecture. It achieves state-of-the-art image classification performance on
three benchmark datasets including CIFAR-10 (4.79% top-1 error), CIFAR-100
(21.98% top-1 error) and ImageNet (7.91% single model, single crop top-5
error). CondenseNeXt achieves final trained model size improvement of 2.9+ MB
and up to 59.98% reduction in forward FLOPs compared to CondenseNet and can
perform image classification on ARM-Based computing platforms without needing a
CUDA enabled GPU support, with outstanding efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalgaonkar_P/0/1/0/all/0/1"&gt;Priyank Kalgaonkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sharkawy_M/0/1/0/all/0/1"&gt;Mohamed El-Sharkawy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SymbolicGPT: A Generative Transformer Model for Symbolic Regression. (arXiv:2106.14131v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14131</id>
        <link href="http://arxiv.org/abs/2106.14131"/>
        <updated>2021-06-29T01:55:15.451Z</updated>
        <summary type="html"><![CDATA[Symbolic regression is the task of identifying a mathematical expression that
best fits a provided dataset of input and output values. Due to the richness of
the space of mathematical expressions, symbolic regression is generally a
challenging problem. While conventional approaches based on genetic evolution
algorithms have been used for decades, deep learning-based methods are
relatively new and an active research area. In this work, we present
SymbolicGPT, a novel transformer-based language model for symbolic regression.
This model exploits the advantages of probabilistic language models like GPT,
including strength in performance and flexibility. Through comprehensive
experiments, we show that our model performs strongly compared to competing
models with respect to the accuracy, running time, and data efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valipour_M/0/1/0/all/0/1"&gt;Mojtaba Valipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_B/0/1/0/all/0/1"&gt;Bowen You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panju_M/0/1/0/all/0/1"&gt;Maysum Panju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Network Representation Learning with Principal Component Analysis. (arXiv:2106.14238v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.14238</id>
        <link href="http://arxiv.org/abs/2106.14238"/>
        <updated>2021-06-29T01:55:15.445Z</updated>
        <summary type="html"><![CDATA[We consider the problem of interpretable network representation learning for
samples of network-valued data. We propose the Principal Component Analysis for
Networks (PCAN) algorithm to identify statistically meaningful low-dimensional
representations of a network sample via subgraph count statistics. The PCAN
procedure provides an interpretable framework for which one can readily
visualize, explore, and formulate predictive models for network samples. We
furthermore introduce a fast sampling-based algorithm, sPCAN, which is
significantly more computationally efficient than its counterpart, but still
enjoys advantages of interpretability. We investigate the relationship between
these two methods and analyze their large-sample properties under the common
regime where the sample of networks is a collection of kernel-based random
graphs. We show that under this regime, the embeddings of the sPCAN method
enjoy a central limit theorem and moreover that the population level embeddings
of PCAN and sPCAN are equivalent. We assess PCAN's ability to visualize,
cluster, and classify observations in network samples arising in nature,
including functional connectivity network samples and dynamic networks
describing the political co-voting habits of the U.S. Senate. Our analyses
reveal that our proposed algorithm provides informative and discriminatory
features describing the networks in each sample. The PCAN and sPCAN methods
build on the current literature of network representation learning and set the
stage for a new line of research in interpretable learning on network-valued
data. Publicly available software for the PCAN and sPCAN methods are available
at https://www.github.com/jihuilee/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wilson_J/0/1/0/all/0/1"&gt;James D. Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jihui Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14174</id>
        <link href="http://arxiv.org/abs/2106.14174"/>
        <updated>2021-06-29T01:55:15.437Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis benefits various applications such as
human-computer interaction and recommendation systems. It aims to infer the
users' bipolar ideas using visual, textual, and acoustic signals. Although
researchers affirm the association between cognitive cues and emotional
manifestations, most of the current multimodal approaches in sentiment analysis
disregard user-specific aspects. To tackle this issue, we devise a novel method
to perform multimodal sentiment prediction using cognitive cues, such as
personality. Our framework constructs an adaptive tree by hierarchically
dividing users and trains the LSTM-based submodels, utilizing an
attention-based fusion to transfer cognitive-oriented knowledge within the
tree. Subsequently, the framework consumes the conclusive agglomerative
knowledge from the adaptive tree to predict final sentiments. We also devise a
dynamic dropout method to facilitate data sharing between neighboring nodes,
reducing data sparsity. The empirical results on real-world datasets determine
that our proposed model for sentiment prediction can surpass trending rivals.
Moreover, compared to other ensemble approaches, the proposed transfer-based
algorithm can better utilize the latent cognitive cues and foster the
prediction outcomes. Based on the given extrinsic and intrinsic analysis
results, we note that compared to other theoretical-based techniques, the
proposed hierarchical clustering approach can better group the users within the
adaptive tree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1"&gt;Sana Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Saeid Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1"&gt;Raziyeh Zall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1"&gt;Mohammad Reza Kangavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1"&gt;Sara Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1"&gt;Wen Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pastprop-RNN: improved predictions of the future by correcting the past. (arXiv:2106.13881v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13881</id>
        <link href="http://arxiv.org/abs/2106.13881"/>
        <updated>2021-06-29T01:55:15.430Z</updated>
        <summary type="html"><![CDATA[Forecasting accuracy is reliant on the quality of available past data. Data
disruptions can adversely affect the quality of the generated model (e.g.
unexpected events such as out-of-stock products when forecasting demand). We
address this problem by pastcasting: predicting how data should have been in
the past to explain the future better. We propose Pastprop-LSTM, a data-centric
backpropagation algorithm that assigns part of the responsibility for errors to
the training data and changes it accordingly. We test three variants of
Pastprop-LSTM on forecasting competition datasets, M4 and M5, plus the Numenta
Anomaly Benchmark. Empirical evaluation indicates that the proposed method can
improve forecasting accuracy, especially when the prediction errors of standard
LSTM are high. It also demonstrates the potential of the algorithm on datasets
containing anomalies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baptista_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Baptista&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baghoussi_Y/0/1/0/all/0/1"&gt;Yassine Baghoussi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1"&gt;Carlos Soares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendes_Moreira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Mendes-Moreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arantes_M/0/1/0/all/0/1"&gt;Miguel Arantes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Detection Algorithm for Large Barkhausen Jumps in Cluttered Environment. (arXiv:2106.14148v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14148</id>
        <link href="http://arxiv.org/abs/2106.14148"/>
        <updated>2021-06-29T01:55:15.424Z</updated>
        <summary type="html"><![CDATA[Modern magnetic sensor arrays conventionally utilize state of the art low
power magnetometers such as parallel and orthogonal fluxgates. Low power
fluxgates tend to have large Barkhausen jumps that appear as a dc jump in the
fluxgate output. This phenomenon deteriorates the signal fidelity and
effectively increases the internal sensor noise. Even if sensors that are more
prone to dc jumps can be screened during production, the conventional noise
measurement does not always catch the dc jump because of its sparsity.
Moreover, dc jumps persist in almost all the sensor cores although at a slower
but still intolerable rate. Even if dc jumps can be easily detected in a
shielded environment, when deployed in presence of natural noise and clutter,
it can be hard to positively detect them. This work fills this gap and presents
algorithms that distinguish dc jumps embedded in natural magnetic field data.
To improve robustness to noise, we developed two machine learning algorithms
that employ temporal and statistical physical-based features of a pre-acquired
and well-known experimental data set. The first algorithm employs a support
vector machine classifier, while the second is based on a neural network
architecture. We compare these new approaches to a more classical kernel-based
method. To that purpose, the receiver operating characteristic curve is
generated, which allows diagnosis ability of the different classifiers by
comparing their performances across various operation points. The accuracy of
the machine learning-based algorithms over the classic method is highly
emphasized. In addition, high generalization and robustness of the neural
network can be concluded, based on the rapid convergence of the corresponding
receiver operating characteristic curves.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alimi_R/0/1/0/all/0/1"&gt;Roger Alimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1"&gt;Amir Ivry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1"&gt;Elad Fisher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_E/0/1/0/all/0/1"&gt;Eyal Weiss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TiVGAN: Text to Image to Video Generation with Step-by-Step Evolutionary Generator. (arXiv:2009.02018v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.02018</id>
        <link href="http://arxiv.org/abs/2009.02018"/>
        <updated>2021-06-29T01:55:15.417Z</updated>
        <summary type="html"><![CDATA[Advances in technology have led to the development of methods that can create
desired visual multimedia. In particular, image generation using deep learning
has been extensively studied across diverse fields. In comparison, video
generation, especially on conditional inputs, remains a challenging and less
explored area. To narrow this gap, we aim to train our model to produce a video
corresponding to a given text description. We propose a novel training
framework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN),
which evolves frame-by-frame and finally produces a full-length video. In the
first phase, we focus on creating a high-quality single video frame while
learning the relationship between the text and an image. As the steps proceed,
our model is trained gradually on more number of consecutive frames.This
step-by-step learning process helps stabilize the training and enables the
creation of high-resolution video based on conditional text descriptions.
Qualitative and quantitative experimental results on various datasets
demonstrate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Doyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_D/0/1/0/all/0/1"&gt;Donggyu Joo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junmo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral-Spatial Graph Reasoning Network for Hyperspectral Image Classification. (arXiv:2106.13952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13952</id>
        <link href="http://arxiv.org/abs/2106.13952"/>
        <updated>2021-06-29T01:55:15.400Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a spectral-spatial graph reasoning network (SSGRN)
for hyperspectral image (HSI) classification. Concretely, this network contains
two parts that separately named spatial graph reasoning subnetwork (SAGRN) and
spectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral
graph contexts, respectively. Different from the previous approaches
implementing superpixel segmentation on the original image or attempting to
obtain the category features under the guide of label image, we perform the
superpixel segmentation on intermediate features of the network to adaptively
produce the homogeneous regions to get the effective descriptors. Then, we
adopt a similar idea in spectral part that reasonably aggregating the channels
to generate spectral descriptors for spectral graph contexts capturing. All
graph reasoning procedures in SAGRN and SEGRN are achieved through graph
convolution. To guarantee the global perception ability of the proposed
methods, all adjacent matrices in graph reasoning are obtained with the help of
non-local self-attention mechanism. At last, by combining the extracted spatial
and spectral graph contexts, we obtain the SSGRN to achieve a high accuracy
classification. Extensive quantitative and qualitative experiments on three
public HSI benchmarks demonstrate the competitiveness of the proposed methods
compared with other state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bo Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liangpei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predictive Control Using Learned State Space Models via Rolling Horizon Evolution. (arXiv:2106.13911v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13911</id>
        <link href="http://arxiv.org/abs/2106.13911"/>
        <updated>2021-06-29T01:55:15.394Z</updated>
        <summary type="html"><![CDATA[A large part of the interest in model-based reinforcement learning derives
from the potential utility to acquire a forward model capable of strategic long
term decision making. Assuming that an agent succeeds in learning a useful
predictive model, it still requires a mechanism to harness it to generate and
select among competing simulated plans. In this paper, we explore this theme
combining evolutionary algorithmic planning techniques with models learned via
deep learning and variational inference. We demonstrate the approach with an
agent that reliably performs online planning in a set of visual navigation
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1"&gt;Alvaro Ovalle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucas_S/0/1/0/all/0/1"&gt;Simon M. Lucas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Time-Delay Feedback Neural Network for Discriminating Small, Fast-Moving Targets in Complex Dynamic Environments. (arXiv:2001.05846v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.05846</id>
        <link href="http://arxiv.org/abs/2001.05846"/>
        <updated>2021-06-29T01:55:15.002Z</updated>
        <summary type="html"><![CDATA[Discriminating small moving objects within complex visual environments is a
significant challenge for autonomous micro robots that are generally limited in
computational power. By exploiting their highly evolved visual systems, flying
insects can effectively detect mates and track prey during rapid pursuits, even
though the small targets equate to only a few pixels in their visual field. The
high degree of sensitivity to small target movement is supported by a class of
specialized neurons called small target motion detectors (STMDs). Existing
STMD-based computational models normally comprise four sequentially arranged
neural layers interconnected via feedforward loops to extract information on
small target motion from raw visual inputs. However, feedback, another
important regulatory circuit for motion perception, has not been investigated
in the STMD pathway and its functional roles for small target motion detection
are not clear. In this paper, we propose an STMD-based neural network with
feedback connection (Feedback STMD), where the network output is temporally
delayed, then fed back to the lower layers to mediate neural responses. We
compare the properties of the model with and without the time-delay feedback
loop, and find it shows preference for high-velocity objects. Extensive
experiments suggest that the Feedback STMD achieves superior detection
performance for fast-moving small targets, while significantly suppressing
background false positive movements which display lower velocities. The
proposed feedback model provides an effective solution in robotic visual
systems for detecting fast-moving small targets that are always salient and
potentially threatening.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huatian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jiannan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Cheng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jigen Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1"&gt;Shigang Yue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Virtual Normal: Enforcing Geometric Constraints for Accurate and Robust Depth Prediction. (arXiv:2103.04216v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04216</id>
        <link href="http://arxiv.org/abs/2103.04216"/>
        <updated>2021-06-29T01:55:14.996Z</updated>
        <summary type="html"><![CDATA[Monocular depth prediction plays a crucial role in understanding 3D scene
geometry. Although recent methods have achieved impressive progress in terms of
evaluation metrics such as the pixel-wise relative error, most methods neglect
the geometric constraints in the 3D space. In this work, we show the importance
of the high-order 3D geometric constraints for depth prediction. By designing a
loss term that enforces a simple geometric constraint, namely, virtual normal
directions determined by randomly sampled three points in the reconstructed 3D
space, we significantly improve the accuracy and robustness of monocular depth
estimation. Significantly, the virtual normal loss can not only improve the
performance of learning metric depth, but also disentangle the scale
information and enrich the model with better shape information. Therefore, when
not having access to absolute metric depth training data, we can use virtual
normal to learn a robust affine-invariant depth generated on diverse scenes. In
experiments, We show state-of-the-art results of learning metric depth on NYU
Depth-V2 and KITTI. From the high-quality predicted depth, we are now able to
recover good 3D structures of the scene such as the point cloud and surface
normal directly, eliminating the necessity of relying on additional models as
was previously done. To demonstrate the excellent generalizability of learning
affine-invariant depth on diverse data with the virtual normal loss, we
construct a large-scale and diverse dataset for training affine-invariant
depth, termed Diverse Scene Depth dataset (DiverseDepth), and test on five
datasets with the zero-shot test setting. Code is available at:
https://git.io/Depth]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wei Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yifan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Radar Voxel Fusion for 3D Object Detection. (arXiv:2106.14087v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14087</id>
        <link href="http://arxiv.org/abs/2106.14087"/>
        <updated>2021-06-29T01:55:14.991Z</updated>
        <summary type="html"><![CDATA[Automotive traffic scenes are complex due to the variety of possible
scenarios, objects, and weather conditions that need to be handled. In contrast
to more constrained environments, such as automated underground trains,
automotive perception systems cannot be tailored to a narrow field of specific
tasks but must handle an ever-changing environment with unforeseen events. As
currently no single sensor is able to reliably perceive all relevant activity
in the surroundings, sensor data fusion is applied to perceive as much
information as possible. Data fusion of different sensors and sensor modalities
on a low abstraction level enables the compensation of sensor weaknesses and
misdetections among the sensors before the information-rich sensor data are
compressed and thereby information is lost after a sensor-individual object
detection. This paper develops a low-level sensor fusion network for 3D object
detection, which fuses lidar, camera, and radar data. The fusion network is
trained and evaluated on the nuScenes data set. On the test set, fusion of
radar data increases the resulting AP (Average Precision) detection score by
about 5.1% in comparison to the baseline lidar network. The radar sensor fusion
proves especially beneficial in inclement conditions such as rain and night
scenes. Fusing additional camera data contributes positively only in
conjunction with the radar fusion, which shows that interdependencies of the
sensors are important for the detection result. Additionally, the paper
proposes a novel loss to handle the discontinuity of a simple yaw
representation for object detection. Our updated loss increases the detection
and orientation estimation performance for all sensor input configurations. The
code for this research has been made available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nobis_F/0/1/0/all/0/1"&gt;Felix Nobis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiei_E/0/1/0/all/0/1"&gt;Ehsan Shafiei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karle_P/0/1/0/all/0/1"&gt;Phillip Karle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Betz_J/0/1/0/all/0/1"&gt;Johannes Betz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lienkamp_M/0/1/0/all/0/1"&gt;Markus Lienkamp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14269</id>
        <link href="http://arxiv.org/abs/2106.14269"/>
        <updated>2021-06-29T01:55:14.984Z</updated>
        <summary type="html"><![CDATA[In large technology companies, the requirements for managing and organizing
technical documents created by engineers and managers in supporting relevant
decision making have increased dramatically in recent years, which has led to a
higher demand for more scalable, accurate, and automated document
classification. Prior studies have primarily focused on processing text for
classification and small-scale databases. This paper describes a novel
multimodal deep learning architecture, called TechDoc, for technical document
classification, which utilizes both natural language and descriptive images to
train hierarchical classifiers. The architecture synthesizes convolutional
neural networks and recurrent neural networks through an integrated training
process. We applied the architecture to a large multimodal technical document
database and trained the model for classifying documents based on the
hierarchical International Patent Classification system. Our results show that
the trained neural network presents a greater classification accuracy than
those using a single modality and several earlier text classification methods.
The trained model can potentially be scaled to millions of real-world technical
documents with both text and figures, which is useful for data and knowledge
management in large technology companies and organizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shuo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jianxi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1"&gt;Christopher L. Magee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Compound Transformer for Accurate Biomedical Image Segmentation. (arXiv:2106.14385v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14385</id>
        <link href="http://arxiv.org/abs/2106.14385"/>
        <updated>2021-06-29T01:55:14.978Z</updated>
        <summary type="html"><![CDATA[The recent vision transformer(i.e.for image classification) learns non-local
attentive interaction of different patch tokens. However, prior arts miss
learning the cross-scale dependencies of different pixels, the semantic
correspondence of different labels, and the consistency of the feature
representations and semantic embeddings, which are critical for biomedical
segmentation. In this paper, we tackle the above issues by proposing a unified
transformer network, termed Multi-Compound Transformer (MCTrans), which
incorporates rich feature learning and semantic structure mining into a unified
framework. Specifically, MCTrans embeds the multi-scale convolutional features
as a sequence of tokens and performs intra- and inter-scale self-attention,
rather than single-scale attention in previous works. In addition, a learnable
proxy embedding is also introduced to model semantic relationship and feature
enhancement by using self-attention and cross-attention, respectively. MCTrans
can be easily plugged into a UNet-like network and attains a significant
improvement over the state-of-the-art methods in biomedical image segmentation
in six standard benchmarks. For example, MCTrans outperforms UNet by 3.64%,
3.71%, 4.34%, 2.8%, 1.88%, 1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis,
Kavirs, ISIC2018 dataset, respectively. Code is available at
https://github.com/JiYuanFeng/MCTrans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yuanfeng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruimao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huijie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingyun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaoting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-guided Progressive Mapping for Profile Face Recognition. (arXiv:2106.14124v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14124</id>
        <link href="http://arxiv.org/abs/2106.14124"/>
        <updated>2021-06-29T01:55:14.970Z</updated>
        <summary type="html"><![CDATA[The past few years have witnessed great progress in the domain of face
recognition thanks to advances in deep learning. However, cross pose face
recognition remains a significant challenge. It is difficult for many deep
learning algorithms to narrow the performance gap caused by pose variations;
the main reasons for this relate to the intra-class discrepancy between face
images in different poses and the pose imbalances of training datasets.
Learning pose-robust features by traversing to the feature space of frontal
faces provides an effective and cheap way to alleviate this problem. In this
paper, we present a method for progressively transforming profile face
representations to the canonical pose with an attentive pair-wise loss.
Firstly, to reduce the difficulty of directly transforming the profile face
features into a frontal pose, we propose to learn the feature residual between
the source pose and its nearby pose in a block-byblock fashion, and thus
traversing to the feature space of a smaller pose by adding the learned
residual. Secondly, we propose an attentive pair-wise loss to guide the feature
transformation progressing in the most effective direction. Finally, our
proposed progressive module and attentive pair-wise loss are light-weight and
easy to implement, adding only about 7:5% extra parameters. Evaluations on the
CFP and CPLFW datasets demonstrate the superiority of our proposed method. Code
is available at https://github.com/hjy1312/AGPM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junyang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Changxing Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dizygotic Conditional Variational AutoEncoder for Multi-Modal and Partial Modality Absent Few-Shot Learning. (arXiv:2106.14467v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14467</id>
        <link href="http://arxiv.org/abs/2106.14467"/>
        <updated>2021-06-29T01:55:14.955Z</updated>
        <summary type="html"><![CDATA[Data augmentation is a powerful technique for improving the performance of
the few-shot classification task. It generates more samples as supplements, and
then this task can be transformed into a common supervised learning issue for
solution. However, most mainstream data augmentation based approaches only
consider the single modality information, which leads to the low diversity and
quality of generated features. In this paper, we present a novel multi-modal
data augmentation approach named Dizygotic Conditional Variational AutoEncoder
(DCVAE) for addressing the aforementioned issue. DCVAE conducts feature
synthesis via pairing two Conditional Variational AutoEncoders (CVAEs) with the
same seed but different modality conditions in a dizygotic symbiosis manner.
Subsequently, the generated features of two CVAEs are adaptively combined to
yield the final feature, which can be converted back into its paired conditions
while ensuring these conditions are consistent with the original conditions not
only in representation but also in function. DCVAE essentially provides a new
idea of data augmentation in various multi-modal scenarios by exploiting the
complement of different modality prior information. Extensive experimental
results demonstrate our work achieves state-of-the-art performances on
miniImageNet, CIFAR-FS and CUB datasets, and is able to work well in the
partial modality absence case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Sheng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04830</id>
        <link href="http://arxiv.org/abs/2012.04830"/>
        <updated>2021-06-29T01:55:14.946Z</updated>
        <summary type="html"><![CDATA[Cataract is one of the leading causes of reversible visual impairment and
blindness globally. Over the years, researchers have achieved significant
progress in developing state-of-the-art artificial intelligence techniques for
automatic cataract classification and grading, helping clinicians prevent and
treat cataract in time. This paper provides a comprehensive survey of recent
advances in machine learning for cataract classification and grading based on
ophthalmic images. We summarize existing literature from two research
directions: conventional machine learning techniques and deep learning
techniques. This paper also provides insights into existing works of both
merits and limitations. In addition, we discuss several challenges of automatic
cataract classification and grading based on machine learning techniques and
present possible solutions to these challenges for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiansheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zunjie Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1"&gt;Risa Higashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rail-5k: a Real-World Dataset for Rail Surface Defects Detection. (arXiv:2106.14366v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14366</id>
        <link href="http://arxiv.org/abs/2106.14366"/>
        <updated>2021-06-29T01:55:14.938Z</updated>
        <summary type="html"><![CDATA[This paper presents the Rail-5k dataset for benchmarking the performance of
visual algorithms in a real-world application scenario, namely the rail surface
defects detection task. We collected over 5k high-quality images from railways
across China, and annotated 1100 images with the help from railway experts to
identify the most common 13 types of rail defects. The dataset can be used for
two settings both with unique challenges, the first is the fully-supervised
setting using the 1k+ labeled images for training, fine-grained nature and
long-tailed distribution of defect classes makes it hard for visual algorithms
to tackle. The second is the semi-supervised learning setting facilitated by
the 4k unlabeled images, these 4k images are uncurated containing possible
image corruptions and domain shift with the labeled images, which can not be
easily tackle by previous semi-supervised learning methods. We believe our
dataset could be a valuable benchmark for evaluating robustness and reliability
of visual algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shaozuo Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Siwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Bingchen Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co$^2$L: Contrastive Continual Learning. (arXiv:2106.14413v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14413</id>
        <link href="http://arxiv.org/abs/2106.14413"/>
        <updated>2021-06-29T01:55:14.925Z</updated>
        <summary type="html"><![CDATA[Recent breakthroughs in self-supervised learning show that such algorithms
learn visual representations that can be transferred better to unseen tasks
than joint-training methods relying on task-specific supervision. In this
paper, we found that the similar holds in the continual learning con-text:
contrastively learned representations are more robust against the catastrophic
forgetting than jointly trained representations. Based on this novel
observation, we propose a rehearsal-based continual learning algorithm that
focuses on continually learning and maintaining transferable representations.
More specifically, the proposed scheme (1) learns representations using the
contrastive learning objective, and (2) preserves learned representations using
a self-supervised distillation step. We conduct extensive experimental
validations under popular benchmark image classification datasets, where our
method sets the new state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_H/0/1/0/all/0/1"&gt;Hyuntak Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jaeho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knee Osteoarthritis Severity Prediction using an Attentive Multi-Scale Deep Convolutional Neural Network. (arXiv:2106.14292v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14292</id>
        <link href="http://arxiv.org/abs/2106.14292"/>
        <updated>2021-06-29T01:55:14.920Z</updated>
        <summary type="html"><![CDATA[Knee Osteoarthritis (OA) is a destructive joint disease identified by joint
stiffness, pain, and functional disability concerning millions of lives across
the globe. It is generally assessed by evaluating physical symptoms, medical
history, and other joint screening tests like radiographs, Magnetic Resonance
Imaging (MRI), and Computed Tomography (CT) scans. Unfortunately, the
conventional methods are very subjective, which forms a barrier in detecting
the disease progression at an early stage. This paper presents a deep
learning-based framework, namely OsteoHRNet, that automatically assesses the
Knee OA severity in terms of Kellgren and Lawrence (KL) grade classification
from X-rays. As a primary novelty, the proposed approach is built upon one of
the most recent deep models, called the High-Resolution Network (HRNet), to
capture the multi-scale features of knee X-rays. In addition, we have also
incorporated an attention mechanism to filter out the counterproductive
features and boost the performance further. Our proposed model has achieved the
best multiclass accuracy of 71.74% and MAE of 0.311 on the baseline cohort of
the OAI dataset, which is a remarkable gain over the existing best-published
works. We have also employed the Gradient-based Class Activation Maps
(Grad-CAMs) visualization to justify the proposed network learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jain_R/0/1/0/all/0/1"&gt;Rohit Kumar Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Prasen Kumar Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gaj_S/0/1/0/all/0/1"&gt;Sibaji Gaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1"&gt;Arijit Sur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghosh_P/0/1/0/all/0/1"&gt;Palash Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Deep Neural Network Saliency Visualizations with Gradual Extrapolation. (arXiv:2104.04945v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04945</id>
        <link href="http://arxiv.org/abs/2104.04945"/>
        <updated>2021-06-29T01:55:14.914Z</updated>
        <summary type="html"><![CDATA[In this paper, an enhancement technique for the class activation mapping
methods such as gradient-weighted class activation maps or excitation
backpropagation is proposed to present the visual explanations of decisions
from convolutional neural network-based models. The proposed idea, called
Gradual Extrapolation, can supplement any method that generates a heatmap
picture by sharpening the output. Instead of producing a coarse localization
map that highlights the important predictive regions in the image, the proposed
method outputs the specific shape that most contributes to the model output.
Thus, the proposed method improves the accuracy of saliency maps. The effect
has been achieved by the gradual propagation of the crude map obtained in the
deep layer through all preceding layers with respect to their activations. In
validation tests conducted on a selected set of images, the faithfulness,
interpretability, and applicability of the method are evaluated. The proposed
technique significantly improves the localization detection of the neural
networks attention at low additional computational costs. Furthermore, the
proposed method is applicable to a variety deep neural network models. The code
for the method can be found at
https://github.com/szandala/gradual-extrapolation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Szandala_T/0/1/0/all/0/1"&gt;Tomasz Szandala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models. (arXiv:2102.08239v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08239</id>
        <link href="http://arxiv.org/abs/2102.08239"/>
        <updated>2021-06-29T01:55:14.908Z</updated>
        <summary type="html"><![CDATA[Interpretability is a critical factor in applying complex deep learning
models to advance the understanding of brain disorders in neuroimaging studies.
To interpret the decision process of a trained classifier, existing techniques
typically rely on saliency maps to quantify the voxel-wise or feature-level
importance for classification through partial derivatives. Despite providing
some level of localization, these maps are not human-understandable from the
neuroscience perspective as they do not inform the specific meaning of the
alteration linked to the brain disorder. Inspired by the image-to-image
translation scheme, we propose to train simulator networks that can warp a
given image to inject or remove patterns of the disease. These networks are
trained such that the classifier produces consistently increased or decreased
prediction logits for the simulated images. Moreover, we propose to couple all
the simulators into a unified model based on conditional convolution. We
applied our approach to interpreting classifiers trained on a synthetic dataset
and two neuroimaging datasets to visualize the effect of the Alzheimer's
disease and alcohol use disorder. Compared to the saliency maps generated by
baseline approaches, our simulations and visualizations based on the Jacobian
determinants of the warping field reveal meaningful and understandable patterns
related to the diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zixuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory Guided Road Detection. (arXiv:2106.14184v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14184</id>
        <link href="http://arxiv.org/abs/2106.14184"/>
        <updated>2021-06-29T01:55:14.902Z</updated>
        <summary type="html"><![CDATA[In self driving car applications, there is a requirement to predict the
location of the lane given an input RGB front facing image. In this paper, we
propose an architecture that allows us to increase the speed and robustness of
road detection without a large hit in accuracy by introducing an underlying
shared feature space that is propagated over time, which serves as a flowing
dynamic memory. By utilizing the gist of previous frames, we train the network
to predict the current road with a greater accuracy and lesser deviation from
previous frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_P/0/1/0/all/0/1"&gt;Praveen Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1"&gt;Rwik Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1"&gt;Varun Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards End-to-End Text Spotting in Natural Scenes. (arXiv:1906.06013v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.06013</id>
        <link href="http://arxiv.org/abs/1906.06013"/>
        <updated>2021-06-29T01:55:14.897Z</updated>
        <summary type="html"><![CDATA[Text spotting in natural scene images is of great importance for many image
understanding tasks. It includes two sub-tasks: text detection and recognition.
In this work, we propose a unified network that simultaneously localizes and
recognizes text with a single forward pass, avoiding intermediate processes
such as image cropping and feature re-calculation, word separation, and
character grouping.

In contrast to existing approaches that consider text detection and
recognition as two distinct tasks and tackle them one by one, the proposed
framework settles these two tasks concurrently. The whole framework can be
trained end-to-end and is able to handle text of arbitrary shapes. The
convolutional features are calculated only once and shared by both detection
and recognition modules. Through multi-task training, the learned features
become more discriminate and improve the overall performance. By employing the
$2$D attention model in word recognition, the irregularity of text can be
robustly addressed. It provides the spatial location for each character, which
not only helps local feature extraction in word recognition, but also indicates
an orientation angle to refine text localization. Our proposed method has
achieved state-of-the-art performance on several standard text spotting
benchmarks, including both regular and irregular ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Moment Loss for Medical Image Segmentation. (arXiv:2106.14178v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14178</id>
        <link href="http://arxiv.org/abs/2106.14178"/>
        <updated>2021-06-29T01:55:14.887Z</updated>
        <summary type="html"><![CDATA[Location information is proven to benefit the deep learning models on
capturing the manifold structure of target objects, and accordingly boosts the
accuracy of medical image segmentation. However, most existing methods encode
the location information in an implicit way, e.g. the distance transform maps,
which describe the relative distance from each pixel to the contour boundary,
for the network to learn. These implicit approaches do not fully exploit the
position information (i.e. absolute location) of targets. In this paper, we
propose a novel loss function, namely residual moment (RM) loss, to explicitly
embed the location information of segmentation targets during the training of
deep learning networks. Particularly, motivated by image moments, the
segmentation prediction map and ground-truth map are weighted by coordinate
information. Then our RM loss encourages the networks to maintain the
consistency between the two weighted maps, which promotes the segmentation
networks to easily locate the targets and extract manifold-structure-related
features. We validate the proposed RM loss by conducting extensive experiments
on two publicly available datasets, i.e., 2D optic cup and disk segmentation
and 3D left atrial segmentation. The experimental results demonstrate the
effectiveness of our RM loss, which significantly boosts the accuracy of
segmentation networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Quanziang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renzhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuexiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation Based Regression for Object Distance Estimation. (arXiv:2106.14208v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14208</id>
        <link href="http://arxiv.org/abs/2106.14208"/>
        <updated>2021-06-29T01:55:14.881Z</updated>
        <summary type="html"><![CDATA[In this study, we propose a novel approach to predict the distances of the
detected objects in an observed scene. The proposed approach modifies the
recently proposed Convolutional Support Estimator Networks (CSENs). CSENs are
designed to compute a direct mapping for the Support Estimation (SE) task in a
representation-based classification problem. We further propose and demonstrate
that representation-based methods (sparse or collaborative representation) can
be used in well-designed regression problems. To the best of our knowledge,
this is the first representation-based method proposed for performing a
regression task by utilizing the modified CSENs; and hence, we name this novel
approach as Representation-based Regression (RbR). The initial version of CSENs
has a proxy mapping stage (i.e., a coarse estimation for the support set) that
is required for the input. In this study, we improve the CSEN model by
proposing Compressive Learning CSEN (CL-CSEN) that has the ability to jointly
optimize the so-called proxy mapping stage along with convolutional layers. The
experimental evaluations using the KITTI 3D Object Detection distance
estimation dataset show that the proposed method can achieve a significantly
improved distance estimation performance over all competing methods. Finally,
the software implementations of the methods are publicly shared at
https://github.com/meteahishali/CSENDistance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahishali_M/0/1/0/all/0/1"&gt;Mete Ahishali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamac_M/0/1/0/all/0/1"&gt;Mehmet Yamac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1"&gt;Moncef Gabbouj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Classification with CondenseNeXt for ARM-Based Computing Platforms. (arXiv:2106.14102v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14102</id>
        <link href="http://arxiv.org/abs/2106.14102"/>
        <updated>2021-06-29T01:55:14.875Z</updated>
        <summary type="html"><![CDATA[In this paper, we demonstrate the implementation of our ultra-efficient deep
convolutional neural network architecture: CondenseNeXt on NXP BlueBox, an
autonomous driving development platform developed for self-driving vehicles. We
show that CondenseNeXt is remarkably efficient in terms of FLOPs, designed for
ARM-based embedded computing platforms with limited computational resources and
can perform image classification without the need of a CUDA enabled GPU.
CondenseNeXt utilizes the state-of-the-art depthwise separable convolution and
model compression techniques to achieve a remarkable computational efficiency.
Extensive analyses are conducted on CIFAR-10, CIFAR-100 and ImageNet datasets
to verify the performance of CondenseNeXt Convolutional Neural Network (CNN)
architecture. It achieves state-of-the-art image classification performance on
three benchmark datasets including CIFAR-10 (4.79% top-1 error), CIFAR-100
(21.98% top-1 error) and ImageNet (7.91% single model, single crop top-5
error). CondenseNeXt achieves final trained model size improvement of 2.9+ MB
and up to 59.98% reduction in forward FLOPs compared to CondenseNet and can
perform image classification on ARM-Based computing platforms without needing a
CUDA enabled GPU support, with outstanding efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalgaonkar_P/0/1/0/all/0/1"&gt;Priyank Kalgaonkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sharkawy_M/0/1/0/all/0/1"&gt;Mohamed El-Sharkawy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14256</id>
        <link href="http://arxiv.org/abs/2106.14256"/>
        <updated>2021-06-29T01:55:14.832Z</updated>
        <summary type="html"><![CDATA[Background: Transrectal ultrasound guided systematic biopsies of the prostate
is a routine procedure to establish a prostate cancer diagnosis. However, the
10-12 prostate core biopsies only sample a relatively small volume of the
prostate, and tumour lesions in regions between biopsy cores can be missed,
leading to a well-known low sensitivity to detect clinically relevant cancer.
As a proof-of-principle, we developed and validated a deep convolutional neural
network model to distinguish between morphological patterns in benign prostate
biopsy whole slide images from men with and without established cancer.
Methods: This study included 14,354 hematoxylin and eosin stained whole slide
images from benign prostate biopsies from 1,508 men in two groups: men without
an established prostate cancer (PCa) diagnosis and men with at least one core
biopsy diagnosed with PCa. 80% of the participants were assigned as training
data and used for model optimization (1,211 men), and the remaining 20% (297
men) as a held-out test set used to evaluate model performance. An ensemble of
10 deep convolutional neural network models was optimized for classification of
biopsies from men with and without established cancer. Hyperparameter
optimization and model selection was performed by cross-validation in the
training data . Results: Area under the receiver operating characteristic curve
(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy
level and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a
specificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:
The developed model has the ability to detect men with risk of missed PCa due
to under-sampling of the prostate. The proposed model has the potential to
reduce the number of false negative cases in routine systematic prostate
biopsies and to indicate men who could benefit from MRI-guided re-biopsy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1"&gt;Boing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yinxi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weitz_P/0/1/0/all/0/1"&gt;Philippe Weitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lindberg_J/0/1/0/all/0/1"&gt;Johan Lindberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Egevad_L/0/1/0/all/0/1"&gt;Lars Egevad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gronberg_H/0/1/0/all/0/1"&gt;Henrik Gr&amp;#xf6;nberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eklund_M/0/1/0/all/0/1"&gt;Martin Eklund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rantalainen_M/0/1/0/all/0/1"&gt;Mattias Rantalainen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Conceptual Blending with Large-scale Language and Vision Models. (arXiv:2106.14127v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14127</id>
        <link href="http://arxiv.org/abs/2106.14127"/>
        <updated>2021-06-29T01:55:14.822Z</updated>
        <summary type="html"><![CDATA[We ask the question: to what extent can recent large-scale language and image
generation models blend visual concepts? Given an arbitrary object, we identify
a relevant object and generate a single-sentence description of the blend of
the two using a language model. We then generate a visual depiction of the
blend using a text-based image generation model. Quantitative and qualitative
evaluations demonstrate the superiority of language models over classical
methods for conceptual blending, and of recent large-scale image generation
models over prior models for the visual depiction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Songwei Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Deep Neural Network based Photometry Framework for Wide Field Small Aperture Telescopes. (arXiv:2106.14349v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2106.14349</id>
        <link href="http://arxiv.org/abs/2106.14349"/>
        <updated>2021-06-29T01:55:14.809Z</updated>
        <summary type="html"><![CDATA[Wide field small aperture telescopes (WFSATs) are mainly used to obtain
scientific information of point--like and streak--like celestial objects.
However, qualities of images obtained by WFSATs are seriously affected by the
background noise and variable point spread functions. Developing high speed and
high efficiency data processing method is of great importance for further
scientific research. In recent years, deep neural networks have been proposed
for detection and classification of celestial objects and have shown better
performance than classical methods. In this paper, we further extend abilities
of the deep neural network based astronomical target detection framework to
make it suitable for photometry and astrometry. We add new branches into the
deep neural network to obtain types, magnitudes and positions of different
celestial objects at the same time. Tested with simulated data, we find that
our neural network has better performance in photometry than classical methods.
Because photometry and astrometry are regression algorithms, which would obtain
high accuracy measurements instead of rough classification results, the
accuracy of photometry and astrometry results would be affected by different
observation conditions. To solve this problem, we further propose to use
reference stars to train our deep neural network with transfer learning
strategy when observation conditions change. The photometry framework proposed
in this paper could be used as an end--to--end quick data processing framework
for WFSATs, which can further increase response speed and scientific outputs of
WFSATs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jia_P/0/1/0/all/0/1"&gt;Peng Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yongyang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Image Classifier Can Suffice Video Understanding. (arXiv:2106.14104v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14104</id>
        <link href="http://arxiv.org/abs/2106.14104"/>
        <updated>2021-06-29T01:55:14.793Z</updated>
        <summary type="html"><![CDATA[We propose a new perspective on video understanding by casting the video
recognition problem as an image recognition task. We show that an image
classifier alone can suffice for video understanding without temporal modeling.
Our approach is simple and universal. It composes input frames into a super
image to train an image classifier to fulfill the task of action recognition,
in exactly the same way as classifying an image. We prove the viability of such
an idea by demonstrating strong and promising performance on four public
datasets including Kinetics400, Something-to-something (V2), MiT and Jester,
using a recently developed vision transformer. We also experiment with the
prevalent ResNet image classifiers in computer vision to further validate our
idea. The results on Kinetics400 are comparable to some of the best-performed
CNN approaches based on spatio-temporal modeling. our code and models will be
made available at https://github.com/IBM/sifar-pytorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1"&gt;Quanfu Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chun-Fu/0/1/0/all/0/1"&gt;Chun-Fu&lt;/a&gt; (Richard) &lt;a href="http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1"&gt;Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Mesh Representations via Binary Space Partitioning Tree Networks. (arXiv:2106.14274v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14274</id>
        <link href="http://arxiv.org/abs/2106.14274"/>
        <updated>2021-06-29T01:55:14.777Z</updated>
        <summary type="html"><![CDATA[Polygonal meshes are ubiquitous, but have only played a relatively minor role
in the deep learning revolution. State-of-the-art neural generative models for
3D shapes learn implicit functions and generate meshes via expensive
iso-surfacing. We overcome these challenges by employing a classical spatial
data structure from computer graphics, Binary Space Partitioning (BSP), to
facilitate 3D learning. The core operation of BSP involves recursive
subdivision of 3D space to obtain convex sets. By exploiting this property, we
devise BSP-Net, a network that learns to represent a 3D shape via convex
decomposition without supervision. The network is trained to reconstruct a
shape using a set of convexes obtained from a BSP-tree built over a set of
planes, where the planes and convexes are both defined by learned network
weights. BSP-Net directly outputs polygonal meshes from the inferred convexes.
The generated meshes are watertight, compact (i.e., low-poly), and well suited
to represent sharp geometry. We show that the reconstruction quality by BSP-Net
is competitive with those from state-of-the-art methods while using much fewer
primitives. We also explore variations to BSP-Net including using a more
generic decoder for reconstruction, more general primitives than planes, as
well as training a generative model with variational auto-encoders. Code is
available at https://github.com/czq142857/BSP-NET-original.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiqin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1"&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects. (arXiv:2106.14440v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14440</id>
        <link href="http://arxiv.org/abs/2106.14440"/>
        <updated>2021-06-29T01:55:14.751Z</updated>
        <summary type="html"><![CDATA[Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in
human environments is an important yet challenging task for future
home-assistant robots. The space of 3D articulated objects is exceptionally
rich in their myriad semantic categories, diverse shape geometry, and
complicated part functionality. Previous works mostly abstract kinematic
structure with estimated joint parameters and part poses as the visual
representations for manipulating 3D articulated objects. In this paper, we
propose object-centric actionable visual priors as a novel
perception-interaction handshaking point that the perception system outputs
more actionable guidance than kinematic structure estimation, by predicting
dense geometry-aware, interaction-aware, and task-aware visual action
affordance and trajectory proposals. We design an interaction-for-perception
framework VAT-Mart to learn such actionable visual representations by
simultaneously training a curiosity-driven reinforcement learning policy
exploring diverse interaction trajectories and a perception module summarizing
and generalizing the explored knowledge for pointwise predictions among diverse
shapes. Experiments prove the effectiveness of the proposed approach using the
large-scale PartNet-Mobility dataset in SAPIEN environment and show promising
generalization capabilities to novel test shapes, unseen object categories, and
real-world data. Project page: https://hyperplane-lab.github.io/vat-mart]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Ruihai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1"&gt;Kaichun Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zizheng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1"&gt;Qingnan Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuelin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hao Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time 3D Object Detection using Feature Map Flow. (arXiv:2106.14101v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14101</id>
        <link href="http://arxiv.org/abs/2106.14101"/>
        <updated>2021-06-29T01:55:14.743Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a real-time 3D detection approach considering
time-spatial feature map aggregation from different time steps of deep neural
model inference (named feature map flow, FMF). Proposed approach improves the
quality of 3D detection center-based baseline and provides real-time
performance on the nuScenes and Waymo benchmark. Code is available at
https://github.com/YoushaaMurhij/FMFNet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murhij_Y/0/1/0/all/0/1"&gt;Youshaa Murhij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudin_D/0/1/0/all/0/1"&gt;Dmitry Yudin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection. (arXiv:2106.14447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14447</id>
        <link href="http://arxiv.org/abs/2106.14447"/>
        <updated>2021-06-29T01:55:14.736Z</updated>
        <summary type="html"><![CDATA[With rapidly evolving internet technologies and emerging tools, sports
related videos generated online are increasing at an unprecedentedly fast pace.
To automate sports video editing/highlight generation process, a key task is to
precisely recognize and locate the events in the long untrimmed videos. In this
tech report, we present a two-stage paradigm to detect what and when events
happen in soccer broadcast videos. Specifically, we fine-tune multiple action
recognition models on soccer data to extract high-level semantic features, and
design a transformer based temporal detection module to locate the target
events. This approach achieved the state-of-the-art performance in both two
tasks, i.e., action spotting and replay grounding, in the SoccerNet-v2
Challenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features
are released at https://github.com/baidu-research/vidpress-sports. By sharing
these features with the broader community, we hope to accelerate the research
into soccer video understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1"&gt;Le Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhiyu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1"&gt;Bo He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jingyu Xin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DenseTNT: Waymo Open Dataset Motion Prediction Challenge 1st Place Solution. (arXiv:2106.14160v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14160</id>
        <link href="http://arxiv.org/abs/2106.14160"/>
        <updated>2021-06-29T01:55:14.730Z</updated>
        <summary type="html"><![CDATA[In autonomous driving, goal-based multi-trajectory prediction methods are
proved to be effective recently, where they first score goal candidates, then
select a final set of goals, and finally complete trajectories based on the
selected goals. However, these methods usually involve goal predictions based
on sparse predefined anchors. In this work, we propose an anchor-free model,
named DenseTNT, which performs dense goal probability estimation for trajectory
prediction. Our model achieves state-of-the-art performance, and ranks 1st on
the Waymo Open Dataset Motion Prediction Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Junru Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MQA: Answering the Question via Robotic Manipulation. (arXiv:2003.04641v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04641</id>
        <link href="http://arxiv.org/abs/2003.04641"/>
        <updated>2021-06-29T01:55:14.712Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel task, Manipulation Question Answering
(MQA), where the robot performs manipulation actions to change the environment
in order to answer a given question. To solve this problem, a framework
consisting of a QA module and a manipulation module is proposed. For the QA
module, we adopt the method for the Visual Question Answering (VQA) task. For
the manipulation module, a Deep Q Network (DQN) model is designed to generate
manipulation actions for the robot to interact with the environment. We
consider the situation where the robot continuously manipulating objects inside
a bin until the answer to the question is found. Besides, a novel dataset that
contains a variety of object models, scenarios and corresponding
question-answer pairs is established in a simulation environment. Extensive
experiments have been conducted to validate the effectiveness of the proposed
framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yuhong Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1"&gt;Di Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaofeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Naifu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huaping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Fuchun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tiny Video Networks. (arXiv:1910.06961v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.06961</id>
        <link href="http://arxiv.org/abs/1910.06961"/>
        <updated>2021-06-29T01:55:14.704Z</updated>
        <summary type="html"><![CDATA[Video understanding is a challenging problem with great impact on the
abilities of autonomous agents working in the real-world. Yet, solutions so far
have been computationally intensive, with the fastest algorithms running for
more than half a second per video snippet on powerful GPUs. We propose a novel
idea on video architecture learning - Tiny Video Networks - which automatically
designs highly efficient models for video understanding. The tiny video models
run with competitive performance for as low as 37 milliseconds per video on a
CPU and 10 milliseconds on a standard GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1"&gt;AJ Piergiovanni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1"&gt;Anelia Angelova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1"&gt;Michael S. Ryoo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling semantic features of macromolecules in Cryo-Electron Tomography. (arXiv:2106.14192v1 [q-bio.BM])]]></title>
        <id>http://arxiv.org/abs/2106.14192</id>
        <link href="http://arxiv.org/abs/2106.14192"/>
        <updated>2021-06-29T01:55:14.697Z</updated>
        <summary type="html"><![CDATA[Cryo-electron tomography (Cryo-ET) is a 3D imaging technique that enables the
systemic study of shape, abundance, and distribution of macromolecular
structures in single cells in near-atomic resolution. However, the systematic
and efficient $\textit{de novo}$ recognition and recovery of macromolecular
structures captured by Cryo-ET are very challenging due to the structural
complexity and imaging limits. Even macromolecules with identical structures
have various appearances due to different orientations and imaging limits, such
as noise and the missing wedge effect. Explicitly disentangling the semantic
features of macromolecules is crucial for performing several downstream
analyses on the macromolecules. This paper has addressed the problem by
proposing a 3D Spatial Variational Autoencoder that explicitly disentangle the
structure, orientation, and shift of macromolecules. Extensive experiments on
both synthesized and real cryo-ET datasets and cross-domain evaluations
demonstrate the efficacy of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Yi_K/0/1/0/all/0/1"&gt;Kai Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Pang_J/0/1/0/all/0/1"&gt;Jianye Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yungeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiangrui Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1"&gt;Min Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems. (arXiv:2106.14386v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.14386</id>
        <link href="http://arxiv.org/abs/2106.14386"/>
        <updated>2021-06-29T01:55:14.690Z</updated>
        <summary type="html"><![CDATA[This paper presents Kimera-Multi, the first multi-robot system that (i) is
robust and capable of identifying and rejecting incorrect inter and intra-robot
loop closures resulting from perceptual aliasing, (ii) is fully distributed and
only relies on local (peer-to-peer) communication to achieve distributed
localization and mapping, and (iii) builds a globally consistent
metric-semantic 3D mesh model of the environment in real-time, where faces of
the mesh are annotated with semantic labels. Kimera-Multi is implemented by a
team of robots equipped with visual-inertial sensors. Each robot builds a local
trajectory estimate and a local mesh using Kimera. When communication is
available, robots initiate a distributed place recognition and robust pose
graph optimization protocol based on a novel distributed graduated
non-convexity algorithm. The proposed protocol allows the robots to improve
their local trajectory estimates by leveraging inter-robot loop closures while
being robust to outliers. Finally, each robot uses its improved trajectory
estimate to correct the local mesh using mesh deformation techniques.

We demonstrate Kimera-Multi in photo-realistic simulations, SLAM benchmarking
datasets, and challenging outdoor datasets collected using ground robots. Both
real and simulated experiments involve long trajectories (e.g., up to 800
meters per robot). The experiments show that Kimera-Multi (i) outperforms the
state of the art in terms of robustness and accuracy, (ii) achieves estimation
errors comparable to a centralized SLAM system while being fully distributed,
(iii) is parsimonious in terms of communication bandwidth, (iv) produces
accurate metric-semantic 3D meshes, and (v) is modular and can be also used for
standard 3D reconstruction (i.e., without semantic labels) or for trajectory
estimation (i.e., without reconstructing a 3D mesh).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yulun Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arias_F/0/1/0/all/0/1"&gt;Fernando Herrera Arias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nieto_Granda_C/0/1/0/all/0/1"&gt;Carlos Nieto-Granda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1"&gt;Jonathan P. How&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1"&gt;Luca Carlone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Class-based Expansion Learning For Image Classification. (arXiv:2106.14412v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14412</id>
        <link href="http://arxiv.org/abs/2106.14412"/>
        <updated>2021-06-29T01:55:14.683Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel image process scheme called class-based
expansion learning for image classification, which aims at improving the
supervision-stimulation frequency for the samples of the confusing classes.
Class-based expansion learning takes a bottom-up growing strategy in a
class-based expansion optimization fashion, which pays more attention to the
quality of learning the fine-grained classification boundaries for the
preferentially selected classes. Besides, we develop a class confusion
criterion to select the confusing class preferentially for training. In this
way, the classification boundaries of the confusing classes are frequently
stimulated, resulting in a fine-grained form. Experimental results demonstrate
the effectiveness of the proposed scheme on several benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hanbin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior-Induced Information Alignment for Image Matting. (arXiv:2106.14439v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14439</id>
        <link href="http://arxiv.org/abs/2106.14439"/>
        <updated>2021-06-29T01:55:14.667Z</updated>
        <summary type="html"><![CDATA[Image matting is an ill-posed problem that aims to estimate the opacity of
foreground pixels in an image. However, most existing deep learning-based
methods still suffer from the coarse-grained details. In general, these
algorithms are incapable of felicitously distinguishing the degree of
exploration between deterministic domains (certain FG and BG pixels) and
undetermined domains (uncertain in-between pixels), or inevitably lose
information in the continuous sampling process, leading to a sub-optimal
result. In this paper, we propose a novel network named Prior-Induced
Information Alignment Matting Network (PIIAMatting), which can efficiently
model the distinction of pixel-wise response maps and the correlation of
layer-wise feature maps. It mainly consists of a Dynamic Gaussian Modulation
mechanism (DGM) and an Information Alignment strategy (IA). Specifically, the
DGM can dynamically acquire a pixel-wise domain response map learned from the
prior distribution. The response map can present the relationship between the
opacity variation and the convergence process during training. On the other
hand, the IA comprises an Information Match Module (IMM) and an Information
Aggregation Module (IAM), jointly scheduled to match and aggregate the adjacent
layer-wise features adaptively. Besides, we also develop a Multi-Scale
Refinement (MSR) module to integrate multi-scale receptive field information at
the refinement stage to recover the fluctuating appearance details. Extensive
quantitative and qualitative evaluations demonstrate that the proposed
PIIAMatting performs favourably against state-of-the-art image matting methods
on the Alphamatting.com, Composition-1K and Distinctions-646 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiake Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+and_Y/0/1/0/all/0/1"&gt;Yong Tang and&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Darker than Black-Box: Face Reconstruction from Similarity Queries. (arXiv:2106.14290v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14290</id>
        <link href="http://arxiv.org/abs/2106.14290"/>
        <updated>2021-06-29T01:55:14.659Z</updated>
        <summary type="html"><![CDATA[Several methods for inversion of face recognition models were recently
presented, attempting to reconstruct a face from deep templates. Although some
of these approaches work in a black-box setup using only face embeddings,
usually, on the end-user side, only similarity scores are provided. Therefore,
these algorithms are inapplicable in such scenarios. We propose a novel
approach that allows reconstructing the face querying only similarity scores of
the black-box model. While our algorithm operates in a more general setup,
experiments show that it is query efficient and outperforms the existing
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Razzhigaev_A/0/1/0/all/0/1"&gt;Anton Razzhigaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kireev_K/0/1/0/all/0/1"&gt;Klim Kireev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udovichenko_I/0/1/0/all/0/1"&gt;Igor Udovichenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1"&gt;Aleksandr Petiushko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Image Recognition for Non-images. (arXiv:2106.14350v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14350</id>
        <link href="http://arxiv.org/abs/2106.14350"/>
        <updated>2021-06-29T01:55:14.653Z</updated>
        <summary type="html"><![CDATA[Powerful deep learning algorithms open an opportunity for solving non-image
Machine Learning (ML) problems by transforming these problems to into the image
recognition problems. The CPC-R algorithm presented in this chapter converts
non-image data into images by visualizing non-image data. Then deep learning
CNN algorithms solve the learning problems on these images. The design of the
CPC-R algorithm allows preserving all high-dimensional information in 2-D
images. The use of pair values mapping instead of single value mapping used in
the alternative approaches allows encoding each n-D point with 2 times fewer
visual elements. The attributes of an n-D point are divided into pairs of its
values and each pair is visualized as 2-D points in the same 2-D Cartesian
coordinates. Next, grey scale or color intensity values are assigned to each
pair to encode the order of pairs. This is resulted in the heatmap image. The
computational experiments with CPC-R are conducted for different CNN
architectures, and methods to optimize the CPC-R images showing that the
combined CPC-R and deep learning CNN algorithms are able to solve non-image ML
problems reaching high accuracy on the benchmark datasets. This chapter expands
our prior work by adding more experiments to test accuracy of classification,
exploring saliency and informativeness of discovered features to test their
interpretability, and generalizing the approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1"&gt;Boris Kovalerchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalla_D/0/1/0/all/0/1"&gt;Divya Chandrika Kalla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1"&gt;Bedant Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking convolutional neural networks for diagnosing Lyme disease from images. (arXiv:2106.14465v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14465</id>
        <link href="http://arxiv.org/abs/2106.14465"/>
        <updated>2021-06-29T01:55:14.648Z</updated>
        <summary type="html"><![CDATA[Lyme disease is one of the most common infectious vector-borne diseases in
the world. In the early stage, the disease manifests itself in most cases with
erythema migrans (EM) skin lesions. Better diagnosis of these early forms would
allow improving the prognosis by preventing the transition to a severe late
form thanks to appropriate antibiotic therapy. Recent studies show that
convolutional neural networks (CNNs) perform very well to identify skin lesions
from the image but, there is not much work for Lyme disease prediction from EM
lesion images. The main objective of this study is to extensively analyze the
effectiveness of CNNs for diagnosing Lyme disease from images and to find out
the best CNN architecture for the purpose. There is no publicly available EM
image dataset for Lyme dis]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1"&gt;Sk Imran Hossain&lt;/a&gt; (LIMOS), &lt;a href="http://arxiv.org/find/eess/1/au:+Herve_J/0/1/0/all/0/1"&gt;Jocelyn de Go&amp;#xeb;r de Herve&lt;/a&gt; (INRAE), &lt;a href="http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1"&gt;Md Shahriar Hassan&lt;/a&gt; (LIMOS), &lt;a href="http://arxiv.org/find/eess/1/au:+Martineau_D/0/1/0/all/0/1"&gt;Delphine Martineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Petrosyan_E/0/1/0/all/0/1"&gt;Evelina Petrosyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Corbain_V/0/1/0/all/0/1"&gt;Violaine Corbain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beytout_J/0/1/0/all/0/1"&gt;Jean Beytout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lebert_I/0/1/0/all/0/1"&gt;Isabelle Lebert&lt;/a&gt; (INRAE), &lt;a href="http://arxiv.org/find/eess/1/au:+Baux_E/0/1/0/all/0/1"&gt;Elisabeth Baux&lt;/a&gt; (CHRU Nancy), &lt;a href="http://arxiv.org/find/eess/1/au:+Cazorla_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Cazorla&lt;/a&gt; (CHU de Saint-Etienne), &lt;a href="http://arxiv.org/find/eess/1/au:+Eldin_C/0/1/0/all/0/1"&gt;Carole Eldin&lt;/a&gt; (IHU M&amp;#xe9;diterran&amp;#xe9;e Infection), &lt;a href="http://arxiv.org/find/eess/1/au:+Hansmann_Y/0/1/0/all/0/1"&gt;Yves Hansmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patrat_Delon_S/0/1/0/all/0/1"&gt;Solene Patrat-Delon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prazuck_T/0/1/0/all/0/1"&gt;Thierry Prazuck&lt;/a&gt; (CHR), &lt;a href="http://arxiv.org/find/eess/1/au:+Raffetin_A/0/1/0/all/0/1"&gt;Alice Raffetin&lt;/a&gt; (CHIV), &lt;a href="http://arxiv.org/find/eess/1/au:+Tattevin_P/0/1/0/all/0/1"&gt;Pierre Tattevin&lt;/a&gt; (CHU Rennes), &lt;a href="http://arxiv.org/find/eess/1/au:+VourcH_G/0/1/0/all/0/1"&gt;Gwena&amp;#xeb;l Vourc&amp;#x27;H&lt;/a&gt; (INRAE), &lt;a href="http://arxiv.org/find/eess/1/au:+Lesens_O/0/1/0/all/0/1"&gt;Olivier Lesens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguifo_E/0/1/0/all/0/1"&gt;Engelbert Nguifo&lt;/a&gt; (LIMOS)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image content dependent semi-fragile watermarking with localized tamper detection. (arXiv:2106.14150v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14150</id>
        <link href="http://arxiv.org/abs/2106.14150"/>
        <updated>2021-06-29T01:55:14.640Z</updated>
        <summary type="html"><![CDATA[Content-independent watermarks and block-wise independency can be considered
as vulnerabilities in semi-fragile watermarking methods. In this paper to
achieve the objectives of semi-fragile watermarking techniques, a method is
proposed to not have the mentioned shortcomings. In the proposed method, the
watermark is generated by relying on image content and a key. Furthermore, the
embedding scheme causes the watermarked blocks to become dependent on each
other, using a key. In the embedding phase, the image is partitioned into
non-overlapping blocks. In order to detect and separate the different types of
attacks more precisely, the proposed method embeds three copies of each
watermark bit into LWT coefficients of each 4x4 block. In the authentication
phase, by voting between the extracted bits the error maps are created; these
maps indicate image authenticity and reveal the modified regions. Also, in
order to automate the authentication, the images are classified into four
categories using seven features. Classification accuracy in the experiments is
97.97 percent. It is noted that our experiments demonstrate that the proposed
method is robust against JPEG compression and is competitive with a
state-of-the-art semi-fragile watermarking method, in terms of robustness and
semi-fragility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Samira Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1"&gt;Mojtaba Mahdavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning without Forgetting for 3D Point Cloud Objects. (arXiv:2106.14275v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14275</id>
        <link href="http://arxiv.org/abs/2106.14275"/>
        <updated>2021-06-29T01:55:14.621Z</updated>
        <summary type="html"><![CDATA[When we fine-tune a well-trained deep learning model for a new set of
classes, the network learns new concepts but gradually forgets the knowledge of
old training. In some real-life applications, we may be interested in learning
new classes without forgetting the capability of previous experience. Such
learning without forgetting problem is often investigated using 2D image
recognition tasks. In this paper, considering the growth of depth camera
technology, we address the same problem for the 3D point cloud object data.
This problem becomes more challenging in the 3D domain than 2D because of the
unavailability of large datasets and powerful pretrained backbone models. We
investigate knowledge distillation techniques on 3D data to reduce catastrophic
forgetting of the previous training. Moreover, we improve the distillation
process by using semantic word vectors of object classes. We observe that
exploring the interrelation of old and new knowledge during training helps to
learn new concepts without forgetting old ones. Experimenting on three 3D point
cloud recognition backbones (PointNet, DGCNN, and PointConv) and synthetic
(ModelNet40, ModelNet10) and real scanned (ScanObjectNN) datasets, we establish
new baseline results on learning without forgetting for 3D data. This research
will instigate many future works in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1"&gt;Townim Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalisha_M/0/1/0/all/0/1"&gt;Mahira Jalisha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheraghian_A/0/1/0/all/0/1"&gt;Ali Cheraghian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1"&gt;Shafin Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matching Point Sets with Quantum Circuit Learning. (arXiv:2102.06697v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06697</id>
        <link href="http://arxiv.org/abs/2102.06697"/>
        <updated>2021-06-29T01:55:14.564Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a parameterised quantum circuit learning approach to
point set matching problem. In contrast to previous annealing-based methods, we
propose a quantum circuit-based framework whose parameters are optimised via
descending the gradients w.r.t a kernel-based loss function. We formulate the
shape matching problem into a distribution learning task; that is, to learn the
distribution of the optimal transformation parameters. We show that this
framework is able to find multiple optimal solutions for symmetric shapes and
is more accurate, scalable and robust than the previous annealing-based method.
Code, data and pre-trained weights are available at the project page:
\href{https://hansen7.github.io/qKC}{https://hansen7.github.io/qKC}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noormandipour_M/0/1/0/all/0/1"&gt;Mohammadreza Noormandipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hanchen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Reconstruction through Fusion of Cross-View Images. (arXiv:2106.14306v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14306</id>
        <link href="http://arxiv.org/abs/2106.14306"/>
        <updated>2021-06-29T01:55:14.548Z</updated>
        <summary type="html"><![CDATA[3D recovery from multi-stereo and stereo images, as an important application
of the image-based perspective geometry, serves many applications in computer
vision, remote sensing and Geomatics. In this chapter, the authors utilize the
imaging geometry and present approaches that perform 3D reconstruction from
cross-view images that are drastically different in their viewpoints. We
introduce our framework that takes ground-view images and satellite images for
full 3D recovery, which includes necessary methods in satellite and
ground-based point cloud generation from images, 3D data co-registration,
fusion and mesh generation. We demonstrate our proposed framework on a dataset
consisting of twelve satellite images and 150k video frames acquired through a
vehicle-mounted Go-pro camera and demonstrate the reconstruction results. We
have also compared our results with results generated from an intuitive
processing pipeline that involves typical geo-registration and meshing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Rongjun Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1"&gt;Xiao Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elhashash_M/0/1/0/all/0/1"&gt;Mostafa Elhashash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blind Non-Uniform Motion Deblurring using Atrous Spatial Pyramid Deformable Convolution and Deblurring-Reblurring Consistency. (arXiv:2106.14336v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14336</id>
        <link href="http://arxiv.org/abs/2106.14336"/>
        <updated>2021-06-29T01:55:14.542Z</updated>
        <summary type="html"><![CDATA[Many deep learning based methods are designed to remove non-uniform
(spatially variant) motion blur caused by object motion and camera shake
without knowing the blur kernel. Some methods directly output the latent sharp
image in one stage, while others utilize a multi-stage strategy (\eg
multi-scale, multi-patch, or multi-temporal) to gradually restore the sharp
image. However, these methods have the following two main issues: 1) The
computational cost of multi-stage is high; 2) The same convolution kernel is
applied in different regions, which is not an ideal choice for non-uniform
blur. Hence, non-uniform motion deblurring is still a challenging and open
problem. In this paper, we propose a new architecture which consists of
multiple Atrous Spatial Pyramid Deformable Convolution (ASPDC) modules to
deblur an image end-to-end with more flexibility. Multiple ASPDC modules
implicitly learn the pixel-specific motion with different dilation rates in the
same layer to handle movements of different magnitude. To improve the training,
we also propose a reblurring network to map the deblurred output back to the
blurred input, which constrains the solution space. Our experimental results
show that the proposed method outperforms state-of-the-art methods on the
benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1"&gt;Dong Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masoumzadeh_A/0/1/0/all/0/1"&gt;Abbas Masoumzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yee-Hong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images. (arXiv:2106.14207v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14207</id>
        <link href="http://arxiv.org/abs/2106.14207"/>
        <updated>2021-06-29T01:55:14.524Z</updated>
        <summary type="html"><![CDATA[Diabetes foot ulceration (DFU) and amputation are a cause of significant
morbidity. The prevention of DFU may be achieved by the identification of
patients at risk of DFU and the institution of preventative measures through
education and offloading. Several studies have reported that thermogram images
may help to detect an increase in plantar temperature prior to DFU. However,
the distribution of plantar temperature may be heterogeneous, making it
difficult to quantify and utilize to predict outcomes. We have compared a
machine learning-based scoring technique with feature selection and
optimization techniques and learning classifiers to several state-of-the-art
Convolutional Neural Networks (CNNs) on foot thermogram images and propose a
robust solution to identify the diabetic foot. A comparatively shallow CNN
model, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram
image-based classification and the AdaBoost Classifier used 10 features and
achieved an F1 score of 97 %. A comparison of the inference time for the
best-performing networks confirmed that the proposed algorithm can be deployed
as a smartphone application to allow the user to monitor the progression of the
DFU in a home setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1"&gt;Amith Khandakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Reaz_M/0/1/0/all/0/1"&gt;Mamun Bin Ibne Reaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sawal Hamid Md Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Md Anwarul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tawsifur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alfkey_R/0/1/0/all/0/1"&gt;Rashad Alfkey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bakar_A/0/1/0/all/0/1"&gt;Ahmad Ashrif A. Bakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Malik_R/0/1/0/all/0/1"&gt;Rayaz A. Malik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Detection Algorithm for Large Barkhausen Jumps in Cluttered Environment. (arXiv:2106.14148v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14148</id>
        <link href="http://arxiv.org/abs/2106.14148"/>
        <updated>2021-06-29T01:55:14.518Z</updated>
        <summary type="html"><![CDATA[Modern magnetic sensor arrays conventionally utilize state of the art low
power magnetometers such as parallel and orthogonal fluxgates. Low power
fluxgates tend to have large Barkhausen jumps that appear as a dc jump in the
fluxgate output. This phenomenon deteriorates the signal fidelity and
effectively increases the internal sensor noise. Even if sensors that are more
prone to dc jumps can be screened during production, the conventional noise
measurement does not always catch the dc jump because of its sparsity.
Moreover, dc jumps persist in almost all the sensor cores although at a slower
but still intolerable rate. Even if dc jumps can be easily detected in a
shielded environment, when deployed in presence of natural noise and clutter,
it can be hard to positively detect them. This work fills this gap and presents
algorithms that distinguish dc jumps embedded in natural magnetic field data.
To improve robustness to noise, we developed two machine learning algorithms
that employ temporal and statistical physical-based features of a pre-acquired
and well-known experimental data set. The first algorithm employs a support
vector machine classifier, while the second is based on a neural network
architecture. We compare these new approaches to a more classical kernel-based
method. To that purpose, the receiver operating characteristic curve is
generated, which allows diagnosis ability of the different classifiers by
comparing their performances across various operation points. The accuracy of
the machine learning-based algorithms over the classic method is highly
emphasized. In addition, high generalization and robustness of the neural
network can be concluded, based on the rapid convergence of the corresponding
receiver operating characteristic curves.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alimi_R/0/1/0/all/0/1"&gt;Roger Alimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1"&gt;Amir Ivry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1"&gt;Elad Fisher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_E/0/1/0/all/0/1"&gt;Eyal Weiss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SDOF-Tracker: Fast and Accurate Multiple Human Tracking by Skipped-Detection and Optical-Flow. (arXiv:2106.14259v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14259</id>
        <link href="http://arxiv.org/abs/2106.14259"/>
        <updated>2021-06-29T01:55:14.511Z</updated>
        <summary type="html"><![CDATA[Multiple human tracking is a fundamental problem for scene understanding.
Although both accuracy and speed are required in real-world applications,
recent tracking methods based on deep learning have focused on accuracy and
require substantial running time. This study aims to improve running speed by
performing human detection at a certain frame interval because it accounts for
most of the running time. The question is how to maintain accuracy while
skipping human detection. In this paper, we propose a method that complements
the detection results with optical flow, based on the fact that someone's
appearance does not change much between adjacent frames. To maintain the
tracking accuracy, we introduce robust interest point selection within human
regions and a tracking termination metric calculated by the distribution of the
interest points. On the MOT20 dataset in the MOTChallenge, the proposed
SDOF-Tracker achieved the best performance in terms of the total running speed
while maintaining the MOTA metric. Our code is available at
https://anonymous.4open.science/r/sdof-tracker-75AE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nishimura_H/0/1/0/all/0/1"&gt;Hitoshi Nishimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komorita_S/0/1/0/all/0/1"&gt;Satoshi Komorita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawanishi_Y/0/1/0/all/0/1"&gt;Yasutomo Kawanishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murase_H/0/1/0/all/0/1"&gt;Hiroshi Murase&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DONet: Learning Category-Level 6D Object Pose and Size Estimation from Depth Observation. (arXiv:2106.14193v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14193</id>
        <link href="http://arxiv.org/abs/2106.14193"/>
        <updated>2021-06-29T01:55:14.465Z</updated>
        <summary type="html"><![CDATA[We propose a method of Category-level 6D Object Pose and Size Estimation
(COPSE) from a single depth image, without external pose-annotated real-world
training data. While previous works exploit visual cues in RGB(D) images, our
method makes inferences based on the rich geometric information of the object
in the depth channel alone. Essentially, our framework explores such geometric
information by learning the unified 3D Orientation-Consistent Representations
(3D-OCR) module, and further enforced by the property of Geometry-constrained
Reflection Symmetry (GeoReS) module. The magnitude information of object size
and the center point is finally estimated by Mirror-Paired Dimensional
Estimation (MPDE) module. Extensive experiments on the category-level NOCS
benchmark demonstrate that our framework competes with state-of-the-art
approaches that require labeled real-world images. We also deploy our approach
to a physical Baxter robot to perform manipulation tasks on unseen but
category-known instances, and the results further validate the efficacy of our
proposed model. Our videos are available in the supplementary material.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haitao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zichang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1"&gt;Chilam Cheang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lingwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1"&gt;Xiangyang Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Domain Expansion for Face Anti-Spoofing. (arXiv:2106.14162v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14162</id>
        <link href="http://arxiv.org/abs/2106.14162"/>
        <updated>2021-06-29T01:55:14.458Z</updated>
        <summary type="html"><![CDATA[Face anti-spoofing (FAS) is an indispensable and widely used module in face
recognition systems. Although high accuracy has been achieved, a FAS system
will never be perfect due to the non-stationary applied environments and the
potential emergence of new types of presentation attacks in real-world
applications. In practice, given a handful of labeled samples from a new
deployment scenario (target domain) and abundant labeled face images in the
existing source domain, the FAS system is expected to perform well in the new
scenario without sacrificing the performance on the original domain. To this
end, we identify and address a more practical problem: Few-Shot Domain
Expansion for Face Anti-Spoofing (FSDE-FAS). This problem is challenging since
with insufficient target domain training samples, the model may suffer from
both overfitting to the target domain and catastrophic forgetting of the source
domain. To address the problem, this paper proposes a Style transfer-based
Augmentation for Semantic Alignment (SASA) framework. We propose to augment the
target data by generating auxiliary samples based on photorealistic style
transfer. With the assistant of the augmented data, we further propose a
carefully designed mechanism to align different domains from both
instance-level and distribution-level, and then stabilize the performance on
the source domain with a less-forgetting constraint. Two benchmarks are
proposed to simulate the FSDE-FAS scenarios, and the experimental results show
that the proposed SASA method outperforms state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bowen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zhenfei Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1"&gt;Jing Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Story in Your Eyes: An Individual-difference-aware Model for Cross-person Gaze Estimation. (arXiv:2106.14183v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14183</id>
        <link href="http://arxiv.org/abs/2106.14183"/>
        <updated>2021-06-29T01:55:14.396Z</updated>
        <summary type="html"><![CDATA[We propose a novel method on refining cross-person gaze prediction task with
eye/face images only by explicitly modelling the person-specific differences.
Specifically, we first assume that we can obtain some initial gaze prediction
results with existing method, which we refer to as InitNet, and then introduce
three modules, the Validity Module (VM), Self-Calibration (SC) and
Person-specific Transform (PT)) Module. By predicting the reliability of
current eye/face images, our VM is able to identify invalid samples, e.g. eye
blinking images, and reduce their effects in our modelling process. Our SC and
PT module then learn to compensate for the differences on valid samples only.
The former models the translation offsets by bridging the gap between initial
predictions and dataset-wise distribution. And the later learns more general
person-specific transformation by incorporating the information from existing
initial predictions of the same person. We validate our ideas on three publicly
available datasets, EVE, XGaze and MPIIGaze and demonstrate that our proposed
method outperforms the SOTA methods significantly on all of them, e.g.
respectively 21.7%, 36.0% and 32.9% relative performance improvements. We won
the GAZE 2021 Competition on the EVE dataset. Our code can be found here
https://github.com/bjj9/EVE_SCPT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Jun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Buyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jun Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-Training Quantization for Vision Transformer. (arXiv:2106.14156v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14156</id>
        <link href="http://arxiv.org/abs/2106.14156"/>
        <updated>2021-06-29T01:55:14.390Z</updated>
        <summary type="html"><![CDATA[Recently, transformer has achieved remarkable performance on a variety of
computer vision applications. Compared with mainstream convolutional neural
networks, vision transformers are often of sophisticated architectures for
extracting powerful feature representations, which are more difficult to be
developed on mobile devices. In this paper, we present an effective
post-training quantization algorithm for reducing the memory storage and
computational costs of vision transformers. Basically, the quantization task
can be regarded as finding the optimal low-bit quantization intervals for
weights and inputs, respectively. To preserve the functionality of the
attention mechanism, we introduce a ranking loss into the conventional
quantization objective that aims to keep the relative order of the
self-attention results after quantization. Moreover, we thoroughly analyze the
relationship between quantization loss of different layers and the feature
diversity, and explore a mixed-precision quantization scheme by exploiting the
nuclear norm of each attention map and output feature. The effectiveness of the
proposed method is verified on several benchmark models and datasets, which
outperforms the state-of-the-art post-training quantization algorithms. For
instance, we can obtain an 81.29\% top-1 accuracy using DeiT-B model on
ImageNet dataset with about 8-bit quantization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenhua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Siwei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning stochastic object models from medical imaging measurements by use of advanced AmbientGANs. (arXiv:2106.14324v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14324</id>
        <link href="http://arxiv.org/abs/2106.14324"/>
        <updated>2021-06-29T01:55:14.384Z</updated>
        <summary type="html"><![CDATA[In order to objectively assess new medical imaging technologies via
computer-simulations, it is important to account for all sources of variability
that contribute to image data. One important source of variability that can
significantly limit observer performance is associated with the variability in
the ensemble of objects to-be-imaged. This source of variability can be
described by stochastic object models (SOMs), which are generative models that
can be employed to sample from a distribution of to-be-virtually-imaged
objects. It is generally desirable to establish SOMs from experimental imaging
measurements acquired by use of a well-characterized imaging system, but this
task has remained challenging. Deep generative neural networks, such as
generative adversarial networks (GANs) hold potential for such tasks. To
establish SOMs from imaging measurements, an AmbientGAN has been proposed that
augments a GAN with a measurement operator. However, the original AmbientGAN
could not immediately benefit from modern training procedures and GAN
architectures, which limited its ability to be applied to realistically sized
medical image data. To circumvent this, in this work, a modified AmbientGAN
training strategy is proposed that is suitable for modern progressive or
multi-resolution training approaches such as employed in the Progressive
Growing of GANs and Style-based GANs. AmbientGANs established by use of the
proposed training procedure are systematically validated in a controlled way by
use of computer-simulated measurement data corresponding to a stylized imaging
system. Finally, emulated single-coil experimental magnetic resonance imaging
data are employed to demonstrate the methods under less stylized conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Weimin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1"&gt;Sayantan Bhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1"&gt;Frank J. Brooks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1"&gt;Mark A. Anastasio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory. (arXiv:2106.02317v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02317</id>
        <link href="http://arxiv.org/abs/2106.02317"/>
        <updated>2021-06-29T01:55:14.378Z</updated>
        <summary type="html"><![CDATA[Dialogue policy learning, a subtask that determines the content of system
response generation and then the degree of task completion, is essential for
task-oriented dialogue systems. However, the unbalanced distribution of system
actions in dialogue datasets often causes difficulty in learning to generate
desired actions and responses. In this paper, we propose a
retrieve-and-memorize framework to enhance the learning of system actions.
Specially, we first design a neural context-aware retrieval module to retrieve
multiple candidate system actions from the training set given a dialogue
context. Then, we propose a memory-augmented multi-decoder network to generate
the system actions conditioned on the candidate actions, which allows the
network to adaptively select key information in the candidate actions and
ignore noises. We conduct experiments on the large-scale multi-domain
task-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1. Experimental
results show that our method achieves competitive performance among several
state-of-the-art models in the context-to-response generation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yunyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1"&gt;Xiaojun Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jianxing Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A CNN Segmentation-Based Approach to Object Detection and Tracking in Ultrasound Scans with Application to the Vagus Nerve Detection. (arXiv:2106.13849v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13849</id>
        <link href="http://arxiv.org/abs/2106.13849"/>
        <updated>2021-06-29T01:55:14.359Z</updated>
        <summary type="html"><![CDATA[Ultrasound scanning is essential in several medical diagnostic and
therapeutic applications. It is used to visualize and analyze anatomical
features and structures that influence treatment plans. However, it is both
labor intensive, and its effectiveness is operator dependent. Real-time
accurate and robust automatic detection and tracking of anatomical structures
while scanning would significantly impact diagnostic and therapeutic procedures
to be consistent and efficient. In this paper, we propose a deep learning
framework to automatically detect and track a specific anatomical target
structure in ultrasound scans. Our framework is designed to be accurate and
robust across subjects and imaging devices, to operate in real-time, and to not
require a large training set. It maintains a localization precision and recall
higher than 90% when trained on training sets that are as small as 20% in size
of the original training set. The framework backbone is a weakly trained
segmentation neural network based on U-Net. We tested the framework on two
different ultrasound datasets with the aim to detect and track the Vagus nerve,
where it outperformed current state-of-the-art real-time object detection
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Battal_A/0/1/0/all/0/1"&gt;Abdullah F. Al-Battal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yan Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morton_T/0/1/0/all/0/1"&gt;Timothy Morton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1"&gt;Chen Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+1_Y/0/1/0/all/0/1"&gt;Yifeng Bu 1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerman_I/0/1/0/all/0/1"&gt;Imanuel R Lerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madhavan_R/0/1/0/all/0/1"&gt;Radhika Madhavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Truong Q. Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14118</id>
        <link href="http://arxiv.org/abs/2106.14118"/>
        <updated>2021-06-29T01:55:14.344Z</updated>
        <summary type="html"><![CDATA[State of the art architectures for untrimmed video Temporal Action
Localization (TAL) have only considered RGB and Flow modalities, leaving the
information-rich audio modality totally unexploited. Audio fusion has been
explored for the related but arguably easier problem of trimmed (clip-level)
action recognition. However, TAL poses a unique set of challenges. In this
paper, we propose simple but effective fusion-based approaches for TAL. To the
best of our knowledge, our work is the first to jointly consider audio and
video modalities for supervised TAL. We experimentally show that our schemes
consistently improve performance for state of the art video-only TAL
approaches. Specifically, they help achieve new state of the art performance on
large-scale benchmark datasets - ActivityNet-1.3 (52.73 mAP@0.5) and THUMOS14
(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion
schemes, modality combinations and TAL architectures. Our code, models and
associated data will be made available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1"&gt;Anurag Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1"&gt;Jazib Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1"&gt;Dolton Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptive YOLO for One-Stage Cross-Domain Detection. (arXiv:2106.13939v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13939</id>
        <link href="http://arxiv.org/abs/2106.13939"/>
        <updated>2021-06-29T01:55:14.323Z</updated>
        <summary type="html"><![CDATA[Domain shift is a major challenge for object detectors to generalize well to
real world applications. Emerging techniques of domain adaptation for two-stage
detectors help to tackle this problem. However, two-stage detectors are not the
first choice for industrial applications due to its long time consumption. In
this paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve
cross-domain performance for one-stage detectors. Image level features
alignment is used to strictly match for local features like texture, and
loosely match for global features like illumination. Multi-scale instance level
features alignment is presented to reduce instance domain shift effectively ,
such as variations in object appearance and viewpoint. A consensus
regularization to these domain classifiers is employed to help the network
generate domain-invariant detections. We evaluate our proposed method on
popular datasets like Cityscapes, KITTI, SIM10K and etc.. The results
demonstrate significant improvement when tested under different cross-domain
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shizhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuo_H/0/1/0/all/0/1"&gt;Hongya Tuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_Z/0/1/0/all/0/1"&gt;Zhongliang Jing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An XAI Approach to Deep Learning Models in the Detection of Ductal Carcinoma in Situ. (arXiv:2106.14186v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14186</id>
        <link href="http://arxiv.org/abs/2106.14186"/>
        <updated>2021-06-29T01:55:14.277Z</updated>
        <summary type="html"><![CDATA[During the last decade or so, there has been an insurgence in the deep
learning community to solve health-related issues, particularly breast cancer.
Following the Camelyon-16 challenge in 2016, several researchers have dedicated
their time to build Convolutional Neural Networks (CNNs) to help radiologists
and other clinicians diagnose breast cancer. In particular, there has been an
emphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage
breast cancer. Large companies have given their fair share of research into
this subject, among these Google Deepmind who developed a model in 2020 that
has proven to be better than radiologists themselves to diagnose breast cancer
correctly.

We found that among the issues which exist, there is a need for an
explanatory system that goes through the hidden layers of a CNN to highlight
those pixels that contributed to the classification of a mammogram. We then
chose an open-source, reasonably successful project developed by Prof. Shen,
using the CBIS-DDSM image database to run our experiments on. It was later
improved using the Resnet-50 and VGG-16 patch-classifiers, analytically
comparing the outcome of both. The results showed that the Resnet-50 one
converged earlier in the experiments.

Following the research by Montavon and Binder, we used the DeepTaylor
Layer-wise Relevance Propagation (LRP) model to highlight those pixels and
regions within a mammogram which contribute most to its classification. This is
represented as a map of those pixels in the original image, which contribute to
the diagnosis and the extent to which they contribute to the final
classification. The most significant advantage of this algorithm is that it
performs exceptionally well with the Resnet-50 patch classifier architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ferla_M/0/1/0/all/0/1"&gt;Michele La Ferla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Montebello_M/0/1/0/all/0/1"&gt;Matthew Montebello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seychell_D/0/1/0/all/0/1"&gt;Dylan Seychell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Midpoint Regularization: from High Uncertainty Training to Conservative Classification. (arXiv:2106.13913v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13913</id>
        <link href="http://arxiv.org/abs/2106.13913"/>
        <updated>2021-06-29T01:55:14.271Z</updated>
        <summary type="html"><![CDATA[Label Smoothing (LS) improves model generalization through penalizing models
from generating overconfident output distributions. For each training sample
the LS strategy smooths the one-hot encoded training signal by distributing its
distribution mass over the non-ground truth classes. We extend this technique
by considering example pairs, coined PLS. PLS first creates midpoint samples by
averaging random sample pairs and then learns a smoothing distribution during
training for each of these midpoint samples, resulting in midpoints with high
uncertainty labels for training. We empirically show that PLS significantly
outperforms LS, achieving up to 30% of relative classification error reduction.
We also visualize that PLS produces very low winning softmax scores for both in
and out of distribution samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PGST: a Polyglot Gender Style Transfer method. (arXiv:2009.01040v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.01040</id>
        <link href="http://arxiv.org/abs/2009.01040"/>
        <updated>2021-06-29T01:55:14.265Z</updated>
        <summary type="html"><![CDATA[Recent developments in Text Style Transfer have led this field to be more
highlighted than ever. The task of transferring an input's style to another is
accompanied by plenty of challenges (e.g., fluency and content preservation)
that need to be taken care of. In this research, we introduce PGST, a novel
polyglot text style transfer approach in the gender domain, composed of
different constitutive elements. In contrast to prior studies, it is feasible
to apply a style transfer method in multiple languages by fulfilling our
method's predefined elements. We have proceeded with a pre-trained word
embedding for token replacement purposes, a character-based token classifier
for gender exchange purposes, and a beam search algorithm for extracting the
most fluent combination. Since different approaches are introduced in our
research, we determine a trade-off value for evaluating different models'
success in faking our gender identification model with transferred text. To
demonstrate our method's multilingual applicability, we applied our method on
both English and Persian corpora and ended up defeating our proposed gender
identification model by 45.6% and 39.2%, respectively. While this research's
focus is not limited to a specific language, our obtained evaluation results
are highly competitive in an analogy among English state of the art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1"&gt;Reza Khanmohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1"&gt;Seyed Abolghasem Mirroshandel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition. (arXiv:2106.14373v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14373</id>
        <link href="http://arxiv.org/abs/2106.14373"/>
        <updated>2021-06-29T01:55:14.249Z</updated>
        <summary type="html"><![CDATA[Research on overlapped and discontinuous named entity recognition (NER) has
received increasing attention. The majority of previous work focuses on either
overlapped or discontinuous entities. In this paper, we propose a novel
span-based model that can recognize both overlapped and discontinuous entities
jointly. The model includes two major steps. First, entity fragments are
recognized by traversing over all possible text spans, thus, overlapped
entities can be recognized. Second, we perform relation classification to judge
whether a given pair of entity fragments to be overlapping or succession. In
this way, we can recognize not only discontinuous entities, and meanwhile
doubly check the overlapped entities. As a whole, our model can be regarded as
a relation extraction paradigm essentially. Experimental results on multiple
benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly
competitive for overlapped and discontinuous NER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhichao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Meishan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1"&gt;Donghong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Change Detection for Geodatabase Updating. (arXiv:2106.14309v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14309</id>
        <link href="http://arxiv.org/abs/2106.14309"/>
        <updated>2021-06-29T01:55:14.242Z</updated>
        <summary type="html"><![CDATA[The geodatabase (vectorized data) nowadays becomes a rather standard digital
city infrastructure; however, updating geodatabase efficiently and economically
remains a fundamental and practical issue in the geospatial industry. The cost
of building a geodatabase is extremely high and labor intensive, and very often
the maps we use have several months and even years of latency. One solution is
to develop more automated methods for (vectorized) geospatial data generation,
which has been proven a difficult task in the past decades. An alternative
solution is to first detect the differences between the new data and the
existing geospatial data, and then only update the area identified as changes.
The second approach is becoming more favored due to its high practicality and
flexibility. A highly relevant technique is change detection. This article aims
to provide an overview the state-of-the-art change detection methods in the
field of Remote Sensing and Geomatics to support the task of updating
geodatabases. Data used for change detection are highly disparate, we therefore
structure our review intuitively based on the dimension of the data, being 1)
change detection with 2D data; 2) change detection with 3D data. Conclusions
will be drawn based on the reviewed efforts in the field, and we will share our
outlooks of the topic of updating geodatabases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Rongjun Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Knowledge-Grounded Dialog System Based on Pre-Trained Language Models. (arXiv:2106.14444v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14444</id>
        <link href="http://arxiv.org/abs/2106.14444"/>
        <updated>2021-06-29T01:55:14.236Z</updated>
        <summary type="html"><![CDATA[We present a knowledge-grounded dialog system developed for the ninth Dialog
System Technology Challenge (DSTC9) Track 1 - Beyond Domain APIs: Task-oriented
Conversational Modeling with Unstructured Knowledge Access. We leverage
transfer learning with existing language models to accomplish the tasks in this
challenge track. Specifically, we divided the task into four sub-tasks and
fine-tuned several Transformer models on each of the sub-tasks. We made
additional changes that yielded gains in both performance and efficiency,
including the combination of the model with traditional entity-matching
techniques, and the addition of a pointer network to the output layer of the
language model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weijie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoxuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haipang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1"&gt;Sanhui Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gongfeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying High Accuracy Regions in Traffic Camera Images to Enhance the Estimation of Road Traffic Metrics: A Quadtree Based Method. (arXiv:2106.14049v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14049</id>
        <link href="http://arxiv.org/abs/2106.14049"/>
        <updated>2021-06-29T01:55:14.227Z</updated>
        <summary type="html"><![CDATA[The growing number of real-time camera feeds in urban areas has made it
possible to provide high-quality traffic data for effective transportation
planning, operations, and management. However, deriving reliable traffic
metrics from these camera feeds has been a challenge due to the limitations of
current vehicle detection techniques, as well as the various camera conditions
such as height and resolution. In this work, a quadtree based algorithm is
developed to continuously partition the image extent until only regions with
high detection accuracy are remained. These regions are referred to as the
high-accuracy identification regions (HAIR) in this paper. We demonstrate how
the use of the HAIR can improve the accuracy of traffic density estimates using
images from traffic cameras at different heights and resolutions in Central
Ohio. Our experiments show that the proposed algorithm can be used to derive
robust HAIR where vehicle detection accuracy is 41 percent higher than that in
the original image extent. The use of the HAIR also significantly improves the
traffic density estimation with an overall decrease of 49 percent in root mean
squared error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1"&gt;Nningchuan Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Conditional Predictors for Domain Adaptation. (arXiv:2106.13899v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13899</id>
        <link href="http://arxiv.org/abs/2106.13899"/>
        <updated>2021-06-29T01:55:14.219Z</updated>
        <summary type="html"><![CDATA[Learning guarantees often rely on assumptions of i.i.d. data, which will
likely be violated in practice once predictors are deployed to perform
real-world tasks. Domain adaptation approaches thus appeared as a useful
framework yielding extra flexibility in that distinct train and test data
distributions are supported, provided that other assumptions are satisfied such
as covariate shift, which expects the conditional distributions over labels to
be independent of the underlying data distribution. Several approaches were
introduced in order to induce generalization across varying train and test data
sources, and those often rely on the general idea of domain-invariance, in such
a way that the data-generating distributions are to be disregarded by the
prediction model. In this contribution, we tackle the problem of generalizing
across data sources by approaching it from the opposite direction: we consider
a conditional modeling approach in which predictions, in addition to being
dependent on the input data, use information relative to the underlying
data-generating distribution. For instance, the model has an explicit mechanism
to adapt to changing environments and/or new data sources. We argue that such
an approach is more generally applicable than current domain adaptation methods
since it does not require extra assumptions such as covariate shift and further
yields simpler training algorithms that avoid a common source of training
instabilities caused by minimax formulations, often employed in
domain-invariant methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1"&gt;Joao Monteiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibert_X/0/1/0/all/0/1"&gt;Xavier Gibert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jianqiao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1"&gt;Vincent Dumoulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dar-Shyang Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Saying the Unseen: Video Descriptions via Dialog Agents. (arXiv:2106.14069v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14069</id>
        <link href="http://arxiv.org/abs/2106.14069"/>
        <updated>2021-06-29T01:55:14.202Z</updated>
        <summary type="html"><![CDATA[Current vision and language tasks usually take complete visual data (e.g.,
raw images or videos) as input, however, practical scenarios may often consist
the situations where part of the visual information becomes inaccessible due to
various reasons e.g., restricted view with fixed camera or intentional vision
block for security concerns. As a step towards the more practical application
scenarios, we introduce a novel task that aims to describe a video using the
natural language dialog between two agents as a supplementary information
source given incomplete visual data. Different from most existing
vision-language tasks where AI systems have full access to images or video
clips, which may reveal sensitive information such as recognizable human faces
or voices, we intentionally limit the visual input for AI systems and seek a
more secure and transparent information medium, i.e., the natural language
dialog, to supplement the missing visual information. Specifically, one of the
intelligent agents - Q-BOT - is given two semantic segmented frames from the
beginning and the end of the video, as well as a finite number of opportunities
to ask relevant natural language questions before describing the unseen video.
A-BOT, the other agent who has access to the entire video, assists Q-BOT to
accomplish the goal by answering the asked questions. We introduce two
different experimental settings with either a generative (i.e., agents generate
questions and answers freely) or a discriminative (i.e., agents select the
questions and answers from candidates) internal dialog generation process. With
the proposed unified QA-Cooperative networks, we experimentally demonstrate the
knowledge transfer process between the two dialog agents and the effectiveness
of using the natural language dialog as a supplement for incomplete implicit
visions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Ye Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yan Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining atmospheric data. (arXiv:2106.13992v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13992</id>
        <link href="http://arxiv.org/abs/2106.13992"/>
        <updated>2021-06-29T01:55:14.196Z</updated>
        <summary type="html"><![CDATA[This paper overviews two interdependent issues important for mining remote
sensing data (e.g. images) obtained from atmospheric monitoring missions. The
first issue relates the building new public datasets and benchmarks, which are
hot priority of the remote sensing community. The second issue is the
investigation of deep learning methodologies for atmospheric data
classification based on vast amount of data without annotations and with
localized annotated data provided by sparse observing networks at the surface.
The targeted application is air quality assessment and prediction. Air quality
is defined as the pollution level linked with several atmospheric constituents
such as gases and aerosols. There are dependency relationships between the bad
air quality, caused by air pollution, and the public health. The target
application is the development of a fast prediction model for local and
regional air quality assessment and tracking. The results of mining data will
have significant implication for citizen and decision makers by providing a
fast prediction and reliable air quality monitoring system able to cover the
local and regional scale through intelligent extrapolation of sparse
ground-based in situ measurement networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1"&gt;Chaabane Djeraba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedi_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Riedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Stream Reciprocal Disentanglement Learning for Domain Adaption Person Re-Identification. (arXiv:2106.13929v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13929</id>
        <link href="http://arxiv.org/abs/2106.13929"/>
        <updated>2021-06-29T01:55:14.191Z</updated>
        <summary type="html"><![CDATA[Since human-labeled samples are free for the target set, unsupervised person
re-identification (Re-ID) has attracted much attention in recent years, by
additionally exploiting the source set. However, due to the differences on
camera styles, illumination and backgrounds, there exists a large gap between
source domain and target domain, introducing a great challenge on cross-domain
matching. To tackle this problem, in this paper we propose a novel method named
Dual-stream Reciprocal Disentanglement Learning (DRDL), which is quite
efficient in learning domain-invariant features. In DRDL, two encoders are
first constructed for id-related and id-unrelated feature extractions, which
are respectively measured by their associated classifiers. Furthermore,
followed by an adversarial learning strategy, both streams reciprocally and
positively effect each other, so that the id-related features and id-unrelated
features are completely disentangled from a given image, allowing the encoder
to be powerful enough to obtain the discriminative but domain-invariant
features. In contrast to existing approaches, our proposed method is free from
image generation, which not only reduces the computational complexity
remarkably, but also removes redundant information from id-related features.
Extensive experiments substantiate the superiority of our proposed method
compared with the state-of-the-arts. The source code has been released in
https://github.com/lhf12278/DRDL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huafeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaixiong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinxing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guangming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhengtao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;David Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Raw-to-Raw Mapping. (arXiv:2106.13883v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13883</id>
        <link href="http://arxiv.org/abs/2106.13883"/>
        <updated>2021-06-29T01:55:14.184Z</updated>
        <summary type="html"><![CDATA[The raw-RGB colors of a camera sensor vary due to the spectral sensitivity
differences across different sensor makes and models. This paper focuses on the
task of mapping between different sensor raw-RGB color spaces. Prior work
addressed this problem using a pairwise calibration to achieve accurate color
mapping. Although being accurate, this approach is less practical as it
requires: (1) capturing pair of images by both camera devices with a color
calibration object placed in each new scene; (2) accurate image alignment or
manual annotation of the color calibration object. This paper aims to tackle
color mapping in the raw space through a more practical setup. Specifically, we
present a semi-supervised raw-to-raw mapping method trained on a small set of
paired images alongside an unpaired set of images captured by each camera
device. Through extensive experiments, we show that our method achieves better
results compared to other domain adaptation alternatives in addition to the
single-calibration solution. We have generated a new dataset of raw images from
two different smartphone cameras as part of this effort. Our dataset includes
unpaired and paired sets for our semi-supervised training and evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1"&gt;Mahmoud Afifi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1"&gt;Abdullah Abuolaim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShapeEditer: a StyleGAN Encoder for Face Swapping. (arXiv:2106.13984v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13984</id>
        <link href="http://arxiv.org/abs/2106.13984"/>
        <updated>2021-06-29T01:55:14.176Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel encoder, called ShapeEditor, for
high-resolution, realistic and high-fidelity face exchange. First of all, in
order to ensure sufficient clarity and authenticity, our key idea is to use an
advanced pretrained high-quality random face image generator, i.e. StyleGAN, as
backbone. Secondly, we design ShapeEditor, a two-step encoder, to make the
swapped face integrate the identity and attribute of the input faces. In the
first step, we extract the identity vector of the source image and the
attribute vector of the target image respectively; in the second step, we map
the concatenation of identity vector and attribute vector into the
$\mathcal{W+}$ potential space. In addition, for learning to map into the
latent space of StyleGAN, we propose a set of self-supervised loss functions
with which the training data do not need to be labeled manually. Extensive
experiments on the test dataset show that the results of our method not only
have a great advantage in clarity and authenticity than other state-of-the-art
methods, but also reflect the sufficient integration of identity and attribute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based Token Classification and Span Prediction Techniques. (arXiv:2102.12254v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12254</id>
        <link href="http://arxiv.org/abs/2102.12254"/>
        <updated>2021-06-29T01:55:14.161Z</updated>
        <summary type="html"><![CDATA[Toxicity detection of text has been a popular NLP task in the recent years.
In SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic
spans within passages. Most state-of-the-art span detection approaches employ
various techniques, each of which can be broadly classified into Token
Classification or Span Prediction approaches. In our paper, we explore simple
versions of both of these approaches and their performance on the task.
Specifically, we use BERT-based models -- BERT, RoBERTa, and SpanBERT for both
approaches. We also combine these approaches and modify them to bring
improvements for Toxic Spans prediction. To this end, we investigate results on
four hybrid approaches -- Multi-Span, Span+Token, LSTM-CRF, and a combination
of predicted offsets using union/intersection. Additionally, we perform a
thorough ablative analysis and analyze our observed results. Our best
submission -- a combination of SpanBERT Span Predictor and RoBERTa Token
Classifier predictions -- achieves an F1 score of 0.6753 on the test set. Our
best post-eval F1 score is 0.6895 on intersection of predicted offsets from
top-3 RoBERTa Token Classification checkpoints. These approaches improve the
performance by 3% on average than those of the shared baseline models -- RNNSL
and SpaCy NER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1"&gt;Gunjan Chhablani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1"&gt;Harshit Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1"&gt;Yash Bhartia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suthaharan_S/0/1/0/all/0/1"&gt;Shan Suthaharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06963</id>
        <link href="http://arxiv.org/abs/2106.06963"/>
        <updated>2021-06-29T01:55:14.155Z</updated>
        <summary type="html"><![CDATA[Automatically generating radiology reports can improve current clinical
practice in diagnostic radiology. On one hand, it can relieve radiologists from
the heavy burden of report writing; On the other hand, it can remind
radiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.
Yet, this task remains a challenging job for data-driven neural networks, due
to the serious visual and textual data biases. To this end, we propose a
Posterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to
imitate the working patterns of radiologists, who will first examine the
abnormal regions and assign the disease topic tags to the abnormal regions, and
then rely on the years of prior medical knowledge and prior working experience
accumulations to write reports. Thus, the PPKED includes three modules:
Posterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and
Multi-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior
knowledge, which provides explicit abnormal visual regions to alleviate visual
data bias; PrKE explores the prior knowledge from the prior medical knowledge
graph (medical knowledge) and prior radiology reports (working experience) to
alleviate textual data bias. The explored knowledge is distilled by the MKD to
generate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our
method is able to outperform previous state-of-the-art models on these two
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1"&gt;Wei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Pose Transfer with Dynamic Details using Neural Video Rendering. (arXiv:2106.14132v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14132</id>
        <link href="http://arxiv.org/abs/2106.14132"/>
        <updated>2021-06-29T01:55:14.149Z</updated>
        <summary type="html"><![CDATA[Pose transfer of human videos aims to generate a high fidelity video of a
target person imitating actions of a source person. A few studies have made
great progress either through image translation with deep latent features or
neural rendering with explicit 3D features. However, both of them rely on large
amounts of training data to generate realistic results, and the performance
degrades on more accessible internet videos due to insufficient training
frames. In this paper, we demonstrate that the dynamic details can be preserved
even trained from short monocular videos. Overall, we propose a neural video
rendering framework coupled with an image-translation-based dynamic details
generation network (D2G-Net), which fully utilizes both the stability of
explicit 3D features and the capacity of learning components. To be specific, a
novel texture representation is presented to encode both the static and
pose-varying appearance characteristics, which is then mapped to the image
space and rendered as a detail-rich frame in the neural rendering stage.
Moreover, we introduce a concise temporal loss in the training stage to
suppress the detail flickering that is made more visible due to high-quality
dynamic details generated by our method. Through extensive comparisons, we
demonstrate that our neural human video renderer is capable of achieving both
clearer dynamic details and more robust performance even on accessible short
videos with only 2k - 4k frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yang-tian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hao-zhi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yu-kun Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lin Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning. (arXiv:2106.14019v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14019</id>
        <link href="http://arxiv.org/abs/2106.14019"/>
        <updated>2021-06-29T01:55:14.142Z</updated>
        <summary type="html"><![CDATA[Despite the success of various text generation metrics such as BERTScore, it
is still difficult to evaluate the image captions without enough reference
captions due to the diversity of the descriptions. In this paper, we introduce
a new metric UMIC, an Unreferenced Metric for Image Captioning which does not
require reference captions to evaluate image captions. Based on
Vision-and-Language BERT, we train UMIC to discriminate negative captions via
contrastive learning. Also, we observe critical problems of the previous
benchmark dataset (i.e., human annotations) on image captioning metric, and
introduce a new collection of human annotations on the generated captions. We
validate UMIC on four datasets, including our new dataset, and show that UMIC
has a higher correlation than all previous metrics that require multiple
references. We release the benchmark dataset and pre-trained models to compute
the UMIC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hwanhee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Seunghyun Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1"&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Trung Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1"&gt;Kyomin Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OffRoadTranSeg: Semi-Supervised Segmentation using Transformers on OffRoad environments. (arXiv:2106.13963v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13963</id>
        <link href="http://arxiv.org/abs/2106.13963"/>
        <updated>2021-06-29T01:55:14.136Z</updated>
        <summary type="html"><![CDATA[We present OffRoadTranSeg, the first end-to-end framework for semi-supervised
segmentation in unstructured outdoor environment using transformers and
automatic data selection for labelling. The offroad segmentation is a scene
understanding approach that is widely used in autonomous driving. The popular
offroad segmentation method is to use fully connected convolution layers and
large labelled data, however, due to class imbalance, there will be several
mismatches and also some classes may not be detected. Our approach is to do the
task of offroad segmentation in a semi-supervised manner. The aim is to provide
a model where self supervised vision transformer is used to fine-tune offroad
datasets with self-supervised data collection for labelling using depth
estimation. The proposed method is validated on RELLIS-3D and RUGD offroad
datasets. The experiments show that OffRoadTranSeg outperformed other state of
the art models, and also solves the RELLIS-3D class imbalance problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Anukriti Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kartikeya Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sujit_P/0/1/0/all/0/1"&gt;P.B. Sujit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Zero-Shot Learning using Multimodal Variational Auto-Encoder with Semantic Concepts. (arXiv:2106.14082v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14082</id>
        <link href="http://arxiv.org/abs/2106.14082"/>
        <updated>2021-06-29T01:55:14.121Z</updated>
        <summary type="html"><![CDATA[With the ever-increasing amount of data, the central challenge in multimodal
learning involves limitations of labelled samples. For the task of
classification, techniques such as meta-learning, zero-shot learning, and
few-shot learning showcase the ability to learn information about novel classes
based on prior knowledge. Recent techniques try to learn a cross-modal mapping
between the semantic space and the image space. However, they tend to ignore
the local and global semantic knowledge. To overcome this problem, we propose a
Multimodal Variational Auto-Encoder (M-VAE) which can learn the shared latent
space of image features and the semantic space. In our approach we concatenate
multimodal data to a single embedding before passing it to the VAE for learning
the latent space. We propose the use of a multi-modal loss during the
reconstruction of the feature embedding through the decoder. Our approach is
capable to correlating modalities and exploit the local and global semantic
knowledge for novel sample predictions. Our experimental results using a MLP
classifier on four benchmark datasets show that our proposed model outperforms
the current state-of-the-art approaches for generalized zero-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bendre_N/0/1/0/all/0/1"&gt;Nihar Bendre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_K/0/1/0/all/0/1"&gt;Kevin Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1"&gt;Peyman Najafirad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAMS: Color-Aware Multi-Style Transfer. (arXiv:2106.13920v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13920</id>
        <link href="http://arxiv.org/abs/2106.13920"/>
        <updated>2021-06-29T01:55:14.116Z</updated>
        <summary type="html"><![CDATA[Image style transfer aims to manipulate the appearance of a source image, or
"content" image, to share similar texture and colors of a target "style" image.
Ideally, the style transfer manipulation should also preserve the semantic
content of the source image. A commonly used approach to assist in transferring
styles is based on Gram matrix optimization. One problem of Gram matrix-based
optimization is that it does not consider the correlation between colors and
their styles. Specifically, certain textures or structures should be associated
with specific colors. This is particularly challenging when the target style
image exhibits multiple style types. In this work, we propose a color-aware
multi-style transfer method that generates aesthetically pleasing results while
preserving the style-color correlation between style and generated images. We
achieve this desired outcome by introducing a simple but efficient modification
to classic Gram matrix-based style transfer optimization. A nice feature of our
method is that it enables the users to manually select the color associations
between the target style and content image for more transfer flexibility. We
validated our method with several qualitative comparisons, including a user
study conducted with 30 participants. In comparison with prior work, our method
is simple, easy to implement, and achieves visually appealing results when
targeting images that have multiple styles. Source code is available at
https://github.com/mahmoudnafifi/color-aware-style-transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1"&gt;Mahmoud Afifi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1"&gt;Abdullah Abuolaim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussien_M/0/1/0/all/0/1"&gt;Mostafa Hussien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1"&gt;Marcus A. Brubaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1"&gt;Michael S. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverting and Understanding Object Detectors. (arXiv:2106.13933v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13933</id>
        <link href="http://arxiv.org/abs/2106.13933"/>
        <updated>2021-06-29T01:55:14.109Z</updated>
        <summary type="html"><![CDATA[As a core problem in computer vision, the performance of object detection has
improved drastically in the past few years. Despite their impressive
performance, object detectors suffer from a lack of interpretability.
Visualization techniques have been developed and widely applied to introspect
the decisions made by other kinds of deep learning models; however, visualizing
object detectors has been underexplored. In this paper, we propose using
inversion as a primary tool to understand modern object detectors and develop
an optimization-based approach to layout inversion, allowing us to generate
synthetic images recognized by trained detectors as containing a desired
configuration of objects. We reveal intriguing properties of detectors by
applying our layout inversion technique to a variety of modern object
detectors, and further investigate them via validation experiments: they rely
on qualitatively different features for classification and regression; they
learn canonical motifs of commonly co-occurring objects; they use diff erent
visual cues to recognize objects of varying sizes. We hope our insights can
help practitioners improve object detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_A/0/1/0/all/0/1"&gt;Ang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1"&gt;Justin Johnson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Table2Charts: Recommending Charts by Learning Shared Table Representations. (arXiv:2008.11015v4 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11015</id>
        <link href="http://arxiv.org/abs/2008.11015"/>
        <updated>2021-06-29T01:55:14.104Z</updated>
        <summary type="html"><![CDATA[It is common for people to create different types of charts to explore a
multi-dimensional dataset (table). However, to recommend commonly composed
charts in real world, one should take the challenges of efficiency, imbalanced
data and table context into consideration. In this paper, we propose
Table2Charts framework which learns common patterns from a large corpus of
(table, charts) pairs. Based on deep Q-learning with copying mechanism and
heuristic searching, Table2Charts does table-to-sequence generation, where each
sequence follows a chart template. On a large spreadsheet corpus with 165k
tables and 266k charts, we show that Table2Charts could learn a shared
representation of table fields so that recommendation tasks on different chart
types could mutually enhance each other. Table2Charts outperforms other chart
recommendation systems in both multi-type task (with doubled recall numbers
R@3=0.61 and R@1=0.43) and human evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingtao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xinyi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuejiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1"&gt;Wei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13884</id>
        <link href="http://arxiv.org/abs/2106.13884"/>
        <updated>2021-06-29T01:55:14.097Z</updated>
        <summary type="html"><![CDATA[When trained at sufficient scale, auto-regressive language models exhibit the
notable ability to learn a new language task after being prompted with just a
few examples. Here, we present a simple, yet effective, approach for
transferring this few-shot learning ability to a multimodal setting (vision and
language). Using aligned image and caption data, we train a vision encoder to
represent each image as a sequence of continuous embeddings, such that a
pre-trained, frozen language model prompted with this prefix generates the
appropriate caption. The resulting system is a multimodal few-shot learner,
with the surprising ability to learn a variety of new tasks when conditioned on
examples, represented as a sequence of multiple interleaved image and text
embeddings. We demonstrate that it can rapidly learn words for new objects and
novel visual categories, do visual question-answering with only a handful of
examples, and make use of outside knowledge, by measuring a single model on a
variety of established and new benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1"&gt;Maria Tsimpoukelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1"&gt;Jacob Menick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1"&gt;Serkan Cabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1"&gt;S.M. Ali Eslami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1"&gt;Felix Hill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Closer Look at How Fine-tuning Changes BERT. (arXiv:2106.14282v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14282</id>
        <link href="http://arxiv.org/abs/2106.14282"/>
        <updated>2021-06-29T01:55:14.079Z</updated>
        <summary type="html"><![CDATA[Given the prevalence of pre-trained contextualized representations in today's
NLP, there have been several efforts to understand what information such
representations contain. A common strategy to use such representations is to
fine-tune them for an end task. However, how fine-tuning for a task changes the
underlying space is less studied. In this work, we study the English BERT
family and use two probing techniques to analyze how fine-tuning changes the
space. Our experiments reveal that fine-tuning improves performance because it
pushes points associated with a label away from other labels. By comparing the
representations before and after fine-tuning, we also discover that fine-tuning
does not change the representations arbitrarily; instead, it adjusts the
representations to downstream tasks while preserving the original structure.
Finally, using carefully constructed experiments, we show that fine-tuning can
encode training sets in a representation, suggesting an overfitting problem of
a new kind.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yichu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1"&gt;Vivek Srikumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating topic modeling and word embedding to characterize violent deaths. (arXiv:2106.14365v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14365</id>
        <link href="http://arxiv.org/abs/2106.14365"/>
        <updated>2021-06-29T01:55:14.072Z</updated>
        <summary type="html"><![CDATA[There is an escalating need for methods to identify latent patterns in text
data from many domains. We introduce a new method to identify topics in a
corpus and represent documents as topic sequences. Discourse Atom Topic
Modeling draws on advances in theoretical machine learning to integrate topic
modeling and word embedding, capitalizing on the distinct capabilities of each.
We first identify a set of vectors ("discourse atoms") that provide a sparse
representation of an embedding space. Atom vectors can be interpreted as latent
topics: Through a generative model, atoms map onto distributions over words;
one can also infer the topic that generated a sequence of words. We illustrate
our method with a prominent example of underutilized text: the U.S. National
Violent Death Reporting System (NVDRS). The NVDRS summarizes violent death
incidents with structured variables and unstructured narratives. We identify
225 latent topics in the narratives (e.g., preparation for death and physical
aggression); many of these topics are not captured by existing structured
variables. Motivated by known patterns in suicide and homicide by gender, and
recent research on gender biases in semantic space, we identify the gender bias
of our topics (e.g., a topic about pain medication is feminine). We then
compare the gender bias of topics to their prevalence in narratives of female
versus male victims. Results provide a detailed quantitative picture of
reporting about lethal violence and its gendered nature. Our method offers a
flexible and broadly applicable approach to model topics in text data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1"&gt;Alina Arseniev-Koehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cochran_S/0/1/0/all/0/1"&gt;Susan D. Cochran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mays_V/0/1/0/all/0/1"&gt;Vickie M. Mays&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1"&gt;Jacob Gates Foster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Steerable 3D Spherical Neurons. (arXiv:2106.13863v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13863</id>
        <link href="http://arxiv.org/abs/2106.13863"/>
        <updated>2021-06-29T01:55:14.063Z</updated>
        <summary type="html"><![CDATA[Emerging from low-level vision theory, steerable filters found their
counterpart in deep learning. Earlier works used the steering theorems and
presented convolutional networks equivariant to rigid transformations. In our
work, we propose a steerable feed-forward learning-based approach that consists
of spherical decision surfaces and operates on point clouds. Due to the
inherent geometric 3D structure of our theory, we derive a 3D steerability
constraint for its atomic parts, the hypersphere neurons. Exploiting the
rotational equivariance, we show how the model parameters are fully steerable
at inference time. The proposed spherical filter banks enable to make
equivariant and, after online optimization, invariant class predictions for
known synthetic point sets in unknown orientations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1"&gt;Pavlo Melnyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1"&gt;Michael Felsberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1"&gt;M&amp;#xe5;rten Wadenb&amp;#xe4;ck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-paced Principal Component Analysis. (arXiv:2106.13880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13880</id>
        <link href="http://arxiv.org/abs/2106.13880"/>
        <updated>2021-06-29T01:55:14.057Z</updated>
        <summary type="html"><![CDATA[Principal Component Analysis (PCA) has been widely used for dimensionality
reduction and feature extraction. Robust PCA (RPCA), under different robust
distance metrics, such as l1-norm and l2, p-norm, can deal with noise or
outliers to some extent. However, real-world data may display structures that
can not be fully captured by these simple functions. In addition, existing
methods treat complex and simple samples equally. By contrast, a learning
pattern typically adopted by human beings is to learn from simple to complex
and less to more. Based on this principle, we propose a novel method called
Self-paced PCA (SPCA) to further reduce the effect of noise and outliers.
Notably, the complexity of each sample is calculated at the beginning of each
iteration in order to integrate samples from simple to more complex into
training. Based on an alternating optimization, SPCA finds an optimal
projection matrix and filters out outliers iteratively. Theoretical analysis is
presented to show the rationality of SPCA. Extensive experiments on popular
data sets demonstrate that the proposed method can improve the state of-the-art
results considerably.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1"&gt;Zhao Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiangxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaofeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Ling Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EARLIN: Early Out-of-Distribution Detection for Resource-efficient Collaborative Inference. (arXiv:2106.13842v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13842</id>
        <link href="http://arxiv.org/abs/2106.13842"/>
        <updated>2021-06-29T01:55:14.050Z</updated>
        <summary type="html"><![CDATA[Collaborative inference enables resource-constrained edge devices to make
inferences by uploading inputs (e.g., images) to a server (i.e., cloud) where
the heavy deep learning models run. While this setup works cost-effectively for
successful inferences, it severely underperforms when the model faces input
samples on which the model was not trained (known as Out-of-Distribution (OOD)
samples). If the edge devices could, at least, detect that an input sample is
an OOD, that could potentially save communication and computation resources by
not uploading those inputs to the server for inference workload. In this paper,
we propose a novel lightweight OOD detection approach that mines important
features from the shallow layers of a pretrained CNN model and detects an input
sample as ID (In-Distribution) or OOD based on a distance function defined on
the reduced feature space. Our technique (a) works on pretrained models without
any retraining of those models, and (b) does not expose itself to any OOD
dataset (all detection parameters are obtained from the ID training dataset).
To this end, we develop EARLIN (EARLy OOD detection for Collaborative
INference) that takes a pretrained model and partitions the model at the OOD
detection layer and deploys the considerably small OOD part on an edge device
and the rest on the cloud. By experimenting using real datasets and a prototype
implementation, we show that our technique achieves better results than other
approaches in terms of overall accuracy and cost when tested against popular
OOD datasets on top of popular deep learning models pretrained on benchmark
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nimi_S/0/1/0/all/0/1"&gt;Sumaiya Tabassum Nimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arefeen_M/0/1/0/all/0/1"&gt;Md Adnan Arefeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1"&gt;Md Yusuf Sarwar Uddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yugyung Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-driven Compliant Manipulation for Reliable, High-Precision Assembly Tasks. (arXiv:2106.14070v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.14070</id>
        <link href="http://arxiv.org/abs/2106.14070"/>
        <updated>2021-06-29T01:55:14.043Z</updated>
        <summary type="html"><![CDATA[Highly constrained manipulation tasks continue to be challenging for
autonomous robots as they require high levels of precision, typically less than
1mm, which is often incompatible with what can be achieved by traditional
perception systems. This paper demonstrates that the combination of
state-of-the-art object tracking with passively adaptive mechanical hardware
can be leveraged to complete precision manipulation tasks with tight,
industrially-relevant tolerances (0.25mm). The proposed control method closes
the loop through vision by tracking the relative 6D pose of objects in the
relevant workspace. It adjusts the control reference of both the compliant
manipulator and the hand to complete object insertion tasks via within-hand
manipulation. Contrary to previous efforts for insertion, our method does not
require expensive force sensors, precision manipulators, or time-consuming,
online learning, which is data hungry. Instead, this effort leverages
mechanical compliance and utilizes an object agnostic manipulation model of the
hand learned offline, off-the-shelf motion planning, and an RGBD-based object
tracker trained solely with synthetic data. These features allow the proposed
system to easily generalize and transfer to new tasks and environments. This
paper describes in detail the system components and showcases its efficacy with
extensive experiments involving tight tolerance peg-in-hole insertion tasks of
various geometries as well as open-world constrained placement tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_A/0/1/0/all/0/1"&gt;Andrew S. Morgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1"&gt;Bowen Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Junchi Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boularias_A/0/1/0/all/0/1"&gt;Abdeslam Boularias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dollar_A/0/1/0/all/0/1"&gt;Aaron M. Dollar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1"&gt;Kostas Bekris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13948</id>
        <link href="http://arxiv.org/abs/2106.13948"/>
        <updated>2021-06-29T01:55:14.024Z</updated>
        <summary type="html"><![CDATA[Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1"&gt;Jonathan Francis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1"&gt;Nariaki Kitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1"&gt;Felix Labelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1"&gt;Ingrid Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Temporal Context and Human Movement Dynamics for Online Action Detection in Videos. (arXiv:2106.13967v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13967</id>
        <link href="http://arxiv.org/abs/2106.13967"/>
        <updated>2021-06-29T01:55:14.017Z</updated>
        <summary type="html"><![CDATA[Nowadays, the interaction between humans and robots is constantly expanding,
requiring more and more human motion recognition applications to operate in
real time. However, most works on temporal action detection and recognition
perform these tasks in offline manner, i.e. temporally segmented videos are
classified as a whole. In this paper, based on the recently proposed framework
of Temporal Recurrent Networks, we explore how temporal context and human
movement dynamics can be effectively employed for online action detection. Our
approach uses various state-of-the-art architectures and appropriately combines
the extracted features in order to improve action detection. We evaluate our
method on a challenging but widely used dataset for temporal action
localization, THUMOS'14. Our experiments show significant improvement over the
baseline method, achieving state-of-the art results on THUMOS'14.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasileiou_V/0/1/0/all/0/1"&gt;Vasiliki I. Vasileiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kardaris_N/0/1/0/all/0/1"&gt;Nikolaos Kardaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1"&gt;Petros Maragos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic. (arXiv:2106.14190v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14190</id>
        <link href="http://arxiv.org/abs/2106.14190"/>
        <updated>2021-06-29T01:55:14.012Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and
ResNeXt-56 are severely over-parameterized, necessitating a consequent increase
in the computational resources required for model training which scales
exponentially for increments in model depth. In this paper, we propose an
Entropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust
and simple, yet effective in resolving the problem of over-parameterization
with regards to network depth of CNN model. The EBCLE heuristic employs a
priori knowledge of the entropic data distribution of input datasets to
determine an upper bound for convolutional network depth, beyond which identity
transformations are prevalent offering insignificant contributions for
enhancing model performance. Restricting depth redundancies by forcing feature
compression and abstraction restricts over-parameterization while decreasing
training time by 24.99% - 78.59% without degradation in model performance. We
present empirical evidence to emphasize the relative effectiveness of broader,
yet shallower models trained using the EBCLE heuristic, which maintains or
outperforms baseline classification accuracies of narrower yet deeper models.
The EBCLE heuristic is architecturally agnostic and EBCLE based CNN models
restrict depth redundancies resulting in enhanced utilization of the available
computational resources. The proposed EBCLE heuristic is a compelling technique
for researchers to analytically justify their HyperParameter (HP) choices for
CNNs. Empirical validation of the EBCLE heuristic in training CNN models was
established on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10,
MNIST) and four network architectures (DenseNet, ResNet, ResNeXt and
EfficientNet B0-B2) with appropriate statistical tests employed to infer any
conclusive claims presented in this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gowdra_N/0/1/0/all/0/1"&gt;Nidhi Gowdra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1"&gt;Roopak Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MacDonell_S/0/1/0/all/0/1"&gt;Stephen MacDonell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1"&gt;Wei Qi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In-N-Out: Towards Good Initialization for Inpainting and Outpainting. (arXiv:2106.13953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13953</id>
        <link href="http://arxiv.org/abs/2106.13953"/>
        <updated>2021-06-29T01:55:14.003Z</updated>
        <summary type="html"><![CDATA[In computer vision, recovering spatial information by filling in masked
regions, e.g., inpainting, has been widely investigated for its usability and
wide applicability to other various applications: image inpainting, image
extrapolation, and environment map estimation. Most of them are studied
separately depending on the applications. Our focus, however, is on
accommodating the opposite task, e.g., image outpainting, which would benefit
the target applications, e.g., image inpainting. Our self-supervision method,
In-N-Out, is summarized as a training approach that leverages the knowledge of
the opposite task into the target model. We empirically show that In-N-Out --
which explores the complementary information -- effectively takes advantage
over the traditional pipelines where only task-specific learning takes place in
training. In experiments, we compare our method to the traditional procedure
and analyze the effectiveness of our method on different applications: image
inpainting, image extrapolation, and environment map estimation. For these
tasks, we demonstrate that In-N-Out consistently improves the performance of
the recent works with In-N-Out self-supervision to their training procedure.
Also, we show that our approach achieves better results than an existing
training approach for outpainting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jo_C/0/1/0/all/0/1"&gt;Changho Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Im_W/0/1/0/all/0/1"&gt;Woobin Im&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sung-Eui Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation. (arXiv:2106.14033v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14033</id>
        <link href="http://arxiv.org/abs/2106.14033"/>
        <updated>2021-06-29T01:55:13.988Z</updated>
        <summary type="html"><![CDATA[The recurrent mechanism has recently been introduced into U-Net in various
medical image segmentation tasks. Existing studies have focused on promoting
network recursion via reusing building blocks. Although network parameters
could be greatly saved, computational costs still increase inevitably in
accordance with the pre-set iteration time. In this work, we study a
multi-scale upgrade of a bi-directional skip connected network and then
automatically discover an efficient architecture by a novel two-phase Neural
Architecture Search (NAS) algorithm, namely BiX-NAS. Our proposed method
reduces the network computational cost by sifting out ineffective multi-scale
features at different levels and iterations. We evaluate BiX-NAS on two
segmentation tasks using three different medical image datasets, and the
experimental results show that our BiX-NAS searched architecture achieves the
state-of-the-art performance with significantly lower computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tiange Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dongnan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1"&gt;Weidong Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interflow: Aggregating Multi-layer Feature Mappings with Attention Mechanism. (arXiv:2106.14073v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14073</id>
        <link href="http://arxiv.org/abs/2106.14073"/>
        <updated>2021-06-29T01:55:13.979Z</updated>
        <summary type="html"><![CDATA[Traditionally, CNN models possess hierarchical structures and utilize the
feature mapping of the last layer to obtain the prediction output. However, it
can be difficulty to settle the optimal network depth and make the middle
layers learn distinguished features. This paper proposes the Interflow
algorithm specially for traditional CNN models. Interflow divides CNNs into
several stages according to the depth and makes predictions by the feature
mappings in each stage. Subsequently, we input these prediction branches into a
well-designed attention module, which learns the weights of these prediction
branches, aggregates them and obtains the final output. Interflow weights and
fuses the features learned in both shallower and deeper layers, making the
feature information at each stage processed reasonably and effectively,
enabling the middle layers to learn more distinguished features, and enhancing
the model representation ability. In addition, Interflow can alleviate gradient
vanishing problem, lower the difficulty of network depth selection, and lighten
possible over-fitting problem by introducing attention mechanism. Besides, it
can avoid network degradation as a byproduct. Compared with the original model,
the CNN model with Interflow achieves higher test accuracy on multiple
benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhicheng Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Deep Ensembles for Blind Image Quality Assessment. (arXiv:2106.14008v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14008</id>
        <link href="http://arxiv.org/abs/2106.14008"/>
        <updated>2021-06-29T01:55:13.973Z</updated>
        <summary type="html"><![CDATA[Ensemble methods are generally regarded to be better than a single model if
the base learners are deemed to be "accurate" and "diverse." Here we
investigate a semi-supervised ensemble learning strategy to produce
generalizable blind image quality assessment models. We train a multi-head
convolutional network for quality prediction by maximizing the accuracy of the
ensemble (as well as the base learners) on labeled data, and the disagreement
(i.e., diversity) among them on unlabeled data, both implemented by the
fidelity loss. We conduct extensive experiments to demonstrate the advantages
of employing unlabeled data for BIQA, especially in model generalization and
failure identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhihua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dingquan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kede Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00250</id>
        <link href="http://arxiv.org/abs/2106.00250"/>
        <updated>2021-06-29T01:55:13.966Z</updated>
        <summary type="html"><![CDATA[Multimodal Machine Translation (MMT) enriches the source text with visual
information for translation. It has gained popularity in recent years, and
several pipelines have been proposed in the same direction. Yet, the task lacks
quality datasets to illustrate the contribution of visual modality in the
translation systems. In this paper, we propose our system under the team name
Volta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We
also participate in the textual-only subtask of the same language pair for
which we use mBART, a pretrained multilingual sequence-to-sequence model. For
multimodal translation, we propose to enhance the textual input by bringing the
visual information to a textual domain by extracting object tags from the
image. We also explore the robustness of our system by systematically degrading
the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test
set and challenge set of the multimodal task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1"&gt;Radhika Mamidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers. (arXiv:2106.13870v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13870</id>
        <link href="http://arxiv.org/abs/2106.13870"/>
        <updated>2021-06-29T01:55:13.960Z</updated>
        <summary type="html"><![CDATA[We propose a method to estimate the uncertainty of the outcome of an image
classifier on a given input datum. Deep neural networks commonly used for image
classification are deterministic maps from an input image to an output class.
As such, their outcome on a given datum involves no uncertainty, so we must
specify what variability we are referring to when defining, measuring and
interpreting "confidence." To this end, we introduce the Wellington Posterior,
which is the distribution of outcomes that would have been obtained in response
to data that could have been generated by the same scene that produced the
given image. Since there are infinitely many scenes that could have generated
the given image, the Wellington Posterior requires induction from scenes other
than the one portrayed. We explore alternate methods using data augmentation,
ensembling, and model linearization. Additional alternatives include generative
adversarial networks, conditional prior networks, and supervised single-view
reconstruction. We test these alternatives against the empirical posterior
obtained by inferring the class of temporally adjacent frames in a video. These
developments are only a small step towards assessing the reliability of deep
network classifiers in a manner that is compatible with safety-critical
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1"&gt;Stephanie Tsuei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1"&gt;Aditya Golatkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11347</id>
        <link href="http://arxiv.org/abs/2105.11347"/>
        <updated>2021-06-29T01:55:13.954Z</updated>
        <summary type="html"><![CDATA[In this article, we present a description of our systems as a part of our
participation in the shared task namely Artificial Intelligence for Legal
Assistance (AILA 2019). This is an integral event of Forum for Information
Retrieval Evaluation-2019. The outcomes of this track would be helpful for the
automation of the working process of the Indian Judiciary System. The manual
working procedures and documentation at any level (from lower to higher court)
of the judiciary system are very complex in nature. The systems produced as a
part of this track would assist the law practitioners. It would be helpful for
common men too. This kind of track also opens the path of research of Natural
Language Processing (NLP) in the judicial domain. This track defined two
problems such as Task 1: Identifying relevant prior cases for a given situation
and Task 2: Identifying the most relevant statutes for a given situation. We
tackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As
per the results declared by the task organizers, we are in 3rd and a modest
position in Task 1 and Task 2 respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1"&gt;Baban Gain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1"&gt;Dibyanayan Bandyopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1"&gt;Arkadipta De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1"&gt;Tanik Saikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonuniform Defocus Removal for Image Classification. (arXiv:2106.13864v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13864</id>
        <link href="http://arxiv.org/abs/2106.13864"/>
        <updated>2021-06-29T01:55:13.936Z</updated>
        <summary type="html"><![CDATA[We propose and study the single-frame anisoplanatic deconvolution problem
associated with image classification using machine learning algorithms, named
the nonuniform defocus removal (NDR) problem. Mathematical analysis of the NDR
problem is done and the so-called defocus removal (DR) algorithm for solving it
is proposed. Global convergence of the DR algorithm is established without
imposing any unverifiable assumption. Numerical results on simulation data show
significant features of DR including solvability, noise robustness,
convergence, model insensitivity and computational efficiency. Physical
relevance of the NDR problem and practicability of the DR algorithm are tested
on experimental data. Back to the application that originally motivated the
investigation of the NDR problem, we show that the DR algorithm can improve the
accuracy of classifying distorted images using convolutional neural networks.
The key difference of this paper compared to most existing works on
single-frame anisoplanatic deconvolution is that the new method does not
require the data image to be decomposable into isoplanatic subregions.
Therefore, solution approaches partitioning the image into isoplanatic zones
are not applicable to the NDR problem and those handling the entire image such
as the DR algorithm need to be developed and analyzed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thao_N/0/1/0/all/0/1"&gt;Nguyen Hieu Thao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soloviev_O/0/1/0/all/0/1"&gt;Oleg Soloviev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noom_J/0/1/0/all/0/1"&gt;Jacques Noom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verhaegen_M/0/1/0/all/0/1"&gt;Michel Verhaegen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral-Spatial Graph Reasoning Network for Hyperspectral Image Classification. (arXiv:2106.13952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13952</id>
        <link href="http://arxiv.org/abs/2106.13952"/>
        <updated>2021-06-29T01:55:13.930Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a spectral-spatial graph reasoning network (SSGRN)
for hyperspectral image (HSI) classification. Concretely, this network contains
two parts that separately named spatial graph reasoning subnetwork (SAGRN) and
spectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral
graph contexts, respectively. Different from the previous approaches
implementing superpixel segmentation on the original image or attempting to
obtain the category features under the guide of label image, we perform the
superpixel segmentation on intermediate features of the network to adaptively
produce the homogeneous regions to get the effective descriptors. Then, we
adopt a similar idea in spectral part that reasonably aggregating the channels
to generate spectral descriptors for spectral graph contexts capturing. All
graph reasoning procedures in SAGRN and SEGRN are achieved through graph
convolution. To guarantee the global perception ability of the proposed
methods, all adjacent matrices in graph reasoning are obtained with the help of
non-local self-attention mechanism. At last, by combining the extracted spatial
and spectral graph contexts, we obtain the SSGRN to achieve a high accuracy
classification. Extensive quantitative and qualitative experiments on three
public HSI benchmarks demonstrate the competitiveness of the proposed methods
compared with other state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bo Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liangpei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph-based approach to derive the geodesic distance on Statistical manifolds: Application to Multimedia Information Retrieval. (arXiv:2106.14060v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14060</id>
        <link href="http://arxiv.org/abs/2106.14060"/>
        <updated>2021-06-29T01:55:13.923Z</updated>
        <summary type="html"><![CDATA[In this paper, we leverage the properties of non-Euclidean Geometry to define
the Geodesic distance (GD) on the space of statistical manifolds. The Geodesic
distance is a real and intuitive similarity measure that is a good alternative
to the purely statistical and extensively used Kullback-Leibler divergence
(KLD). Despite the effectiveness of the GD, a closed-form does not exist for
many manifolds, since the geodesic equations are hard to solve. This explains
that the major studies have been content to use numerical approximations.
Nevertheless, most of those do not take account of the manifold properties,
which leads to a loss of information and thus to low performances. We propose
an approximation of the Geodesic distance through a graph-based method. This
latter permits to well represent the structure of the statistical manifold, and
respects its geometrical properties. Our main aim is to compare the graph-based
approximation to the state of the art approximations. Thus, the proposed
approach is evaluated for two statistical manifolds, namely the Weibull
manifold and the Gamma manifold, considering the Content-Based Texture
Retrieval application on different databases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abbad_Z/0/1/0/all/0/1"&gt;Zakariae Abbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maliani_A/0/1/0/all/0/1"&gt;Ahmed Drissi El Maliani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alaoui_S/0/1/0/all/0/1"&gt;Said Ouatik El Alaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassouni_M/0/1/0/all/0/1"&gt;Mohammed El Hassouni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functional Classwise Principal Component Analysis: A Novel Classification Framework. (arXiv:2106.13959v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13959</id>
        <link href="http://arxiv.org/abs/2106.13959"/>
        <updated>2021-06-29T01:55:13.916Z</updated>
        <summary type="html"><![CDATA[In recent times, functional data analysis (FDA) has been successfully applied
in the field of high dimensional data classification. In this paper, we present
a novel classification framework using functional data and classwise Principal
Component Analysis (PCA). Our proposed method can be used in high dimensional
time series data which typically suffers from small sample size problem. Our
method extracts a piece wise linear functional feature space and is
particularly suitable for hard classification problems.The proposed framework
converts time series data into functional data and uses classwise functional
PCA for feature extraction followed by classification using a Bayesian linear
classifier. We demonstrate the efficacy of our proposed method by applying it
to both synthetic data sets and real time series data from diverse fields
including but not limited to neuroscience, food science, medical sciences and
chemometrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chatterjee_A/0/1/0/all/0/1"&gt;Avishek Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mazumder_S/0/1/0/all/0/1"&gt;Satyaki Mazumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Das_K/0/1/0/all/0/1"&gt;Koel Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building a Video-and-Language Dataset with Human Actions for Multimodal Logical Inference. (arXiv:2106.14137v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14137</id>
        <link href="http://arxiv.org/abs/2106.14137"/>
        <updated>2021-06-29T01:55:13.898Z</updated>
        <summary type="html"><![CDATA[This paper introduces a new video-and-language dataset with human actions for
multimodal logical inference, which focuses on intentional and aspectual
expressions that describe dynamic human actions. The dataset consists of 200
videos, 5,554 action labels, and 1,942 action triplets of the form <subject,
predicate, object> that can be translated into logical semantic
representations. The dataset is expected to be useful for evaluating multimodal
inference systems between videos and semantically complicated sentences
including negation and quantification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_R/0/1/0/all/0/1"&gt;Riko Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yanaka_H/0/1/0/all/0/1"&gt;Hitomi Yanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mineshima_K/0/1/0/all/0/1"&gt;Koji Mineshima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekki_D/0/1/0/all/0/1"&gt;Daisuke Bekki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LRG at SemEval-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting. (arXiv:2102.12255v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12255</id>
        <link href="http://arxiv.org/abs/2102.12255"/>
        <updated>2021-06-29T01:55:13.893Z</updated>
        <summary type="html"><![CDATA[In this article, we present our methodologies for SemEval-2021 Task-4:
Reading Comprehension of Abstract Meaning. Given a fill-in-the-blank-type
question and a corresponding context, the task is to predict the most suitable
word from a list of 5 options. There are three sub-tasks within this task:
Imperceptibility (subtask-I), Non-Specificity (subtask-II), and Intersection
(subtask-III). We use encoders of transformers-based models pre-trained on the
masked language modelling (MLM) task to build our Fill-in-the-blank (FitB)
models. Moreover, to model imperceptibility, we define certain linguistic
features, and to model non-specificity, we leverage information from hypernyms
and hyponyms provided by a lexical database. Specifically, for non-specificity,
we try out augmentation techniques, and other statistical techniques. We also
propose variants, namely Chunk Voting and Max Context, to take care of input
length restrictions for BERT, etc. Additionally, we perform a thorough ablation
study, and use Integrated Gradients to explain our predictions on a few
samples. Our best submissions achieve accuracies of 75.31% and 77.84%, on the
test sets for subtask-I and subtask-II, respectively. For subtask-III, we
achieve accuracies of 65.64% and 62.27%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1"&gt;Harshit Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1"&gt;Gunjan Chhablani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1"&gt;Yash Bhartia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1"&gt;Tirtharaj Dash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Descriptive Modeling of Textiles using FE Simulations and Deep Learning. (arXiv:2106.13982v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13982</id>
        <link href="http://arxiv.org/abs/2106.13982"/>
        <updated>2021-06-29T01:55:13.881Z</updated>
        <summary type="html"><![CDATA[In this work we propose a novel and fully automated method for extracting the
yarn geometrical features in woven composites so that a direct parametrization
of the textile reinforcement is achieved (e.g., FE mesh). Thus, our aim is not
only to perform yarn segmentation from tomographic images but rather to provide
a complete descriptive modeling of the fabric. As such, this direct approach
improves on previous methods that use voxel-wise masks as intermediate
representations followed by re-meshing operations (yarn envelope estimation).
The proposed approach employs two deep neural network architectures (U-Net and
Mask RCNN). First, we train the U-Net to generate synthetic CT images from the
corresponding FE simulations. This allows to generate large quantities of
annotated data without requiring costly manual annotations. This data is then
used to train the Mask R-CNN, which is focused on predicting contour points
around each of the yarns in the image. Experimental results show that our
method is accurate and robust for performing yarn instance segmentation on CT
images, this is further validated by quantitative and qualitative analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendoza_A/0/1/0/all/0/1"&gt;Arturo Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trullo_R/0/1/0/all/0/1"&gt;Roger Trullo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wielhorski_Y/0/1/0/all/0/1"&gt;Yanneck Wielhorski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantics-aware Multi-modal Domain Translation:From LiDAR Point Clouds to Panoramic Color Images. (arXiv:2106.13974v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13974</id>
        <link href="http://arxiv.org/abs/2106.13974"/>
        <updated>2021-06-29T01:55:13.874Z</updated>
        <summary type="html"><![CDATA[In this work, we present a simple yet effective framework to address the
domain translation problem between different sensor modalities with unique data
formats. By relying only on the semantics of the scene, our modular generative
framework can, for the first time, synthesize a panoramic color image from a
given full 3D LiDAR point cloud. The framework starts with semantic
segmentation of the point cloud, which is initially projected onto a spherical
surface. The same semantic segmentation is applied to the corresponding camera
image. Next, our new conditional generative model adversarially learns to
translate the predicted LiDAR segment maps to the camera image counterparts.
Finally, generated image segments are processed to render the panoramic scene
images. We provide a thorough quantitative evaluation on the SemanticKitti
dataset and show that our proposed framework outperforms other strong baseline
models.

Our source code is available at
https://github.com/halmstad-University/TITAN-NET]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cortinhal_T/0/1/0/all/0/1"&gt;Tiago Cortinhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurnaz_F/0/1/0/all/0/1"&gt;Fatih Kurnaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1"&gt;Eren Aksoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perspective-corrected Spatial Referring Expression Generation for Human-Robot Interaction. (arXiv:2104.01558v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01558</id>
        <link href="http://arxiv.org/abs/2104.01558"/>
        <updated>2021-06-29T01:55:13.859Z</updated>
        <summary type="html"><![CDATA[Intelligent robots designed to interact with humans in real scenarios need to
be able to refer to entities actively by natural language. In spatial referring
expression generation, the ambiguity is unavoidable due to the diversity of
reference frames, which will lead to an understanding gap between humans and
robots. To narrow this gap, in this paper, we propose a novel
perspective-corrected spatial referring expression generation (PcSREG) approach
for human-robot interaction by considering the selection of reference frames.
The task of referring expression generation is simplified into the process of
generating diverse spatial relation units. First, we pick out all landmarks in
these spatial relation units according to the entropy of preference and allow
its updating through a stack model. Then all possible referring expressions are
generated according to different reference frame strategies. Finally, we
evaluate every expression using a probabilistic referring expression resolution
model and find the best expression that satisfies both of the appropriateness
and effectiveness. We implement the proposed approach on a robot system and
empirical experiments show that our approach can generate more effective
spatial referring expressions for practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingjiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chengli Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chunlin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Speech Translation with Pre-trained Models and Adapters: UPC at IWSLT 2021. (arXiv:2105.04512v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04512</id>
        <link href="http://arxiv.org/abs/2105.04512"/>
        <updated>2021-06-29T01:55:13.784Z</updated>
        <summary type="html"><![CDATA[This paper describes the submission to the IWSLT 2021 offline speech
translation task by the UPC Machine Translation group. The task consists of
building a system capable of translating English audio recordings extracted
from TED talks into German text. Submitted systems can be either cascade or
end-to-end and use a custom or given segmentation. Our submission is an
end-to-end speech translation system, which combines pre-trained models
(Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder,
and uses an efficient fine-tuning technique, which trains only 20% of its total
parameters. We show that adding an Adapter to the system and pre-training it,
can increase the convergence speed and the final result, with which we achieve
a BLEU score of 27.3 on the MuST-C test set. Our final model is an ensemble
that obtains 28.22 BLEU score on the same set. Our submission also uses a
custom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for
identifying periods of untranscribable text and can bring improvements of 2.5
to 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the
given segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1"&gt;Gerard I. G&amp;#xe1;llego&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1"&gt;Ioannis Tsiamas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1"&gt;Carlos Escolano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; A. R. Fonollosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1"&gt;Marta R. Costa-juss&amp;#xe0;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing the Generalization for Intent Classification and Out-of-Domain Detection in SLU. (arXiv:2106.14464v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14464</id>
        <link href="http://arxiv.org/abs/2106.14464"/>
        <updated>2021-06-29T01:55:13.709Z</updated>
        <summary type="html"><![CDATA[Intent classification is a major task in spoken language understanding (SLU).
Since most models are built with pre-collected in-domain (IND) training
utterances, their ability to detect unsupported out-of-domain (OOD) utterances
has a critical effect in practical use. Recent works have shown that using
extra data and labels can improve the OOD detection performance, yet it could
be costly to collect such data. This paper proposes to train a model with only
IND data while supporting both IND intent classification and OOD detection. Our
method designs a novel domain-regularized module (DRM) to reduce the
overconfident phenomenon of a vanilla classifier, achieving a better
generalization in both cases. Besides, DRM can be used as a drop-in replacement
for the last layer in any neural network-based intent classifier, providing a
low-cost strategy for a significant improvement. The evaluation on four
datasets shows that our method built on BERT and RoBERTa models achieves
state-of-the-art performance against existing approaches and the strong
baselines we created for the comparisons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yilin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1"&gt;Avik Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hongxia Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparsely Overlapped Speech Training in the Time Domain: Joint Learning of Target Speech Separation and Personal VAD Benefits. (arXiv:2106.14371v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.14371</id>
        <link href="http://arxiv.org/abs/2106.14371"/>
        <updated>2021-06-29T01:55:13.536Z</updated>
        <summary type="html"><![CDATA[Target speech separation is the process of filtering a certain speaker's
voice out of speech mixtures according to the additional speaker identity
information provided. Recent works have made considerable improvement by
processing signals in the time domain directly. The majority of them take fully
overlapped speech mixtures for training. However, since most real-life
conversations occur randomly and are sparsely overlapped, we argue that
training with different overlap ratio data benefits. To do so, an unavoidable
problem is that the popularly used SI-SNR loss has no definition for silent
sources. This paper proposes the weighted SI-SNR loss, together with the joint
learning of target speech separation and personal VAD. The weighted SI-SNR loss
imposes a weight factor that is proportional to the target speaker's duration
and returns zero when the target speaker is absent. Meanwhile, the personal VAD
generates masks and sets non-target speech to silence. Experiments show that
our proposed method outperforms the baseline by 1.73 dB in terms of SDR on
fully overlapped speech, as well as by 4.17 dB and 0.9 dB on sparsely
overlapped speech of clean and noisy conditions. Besides, with slight
degradation in performance, our model could reduce the time costs in inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qingjian Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Luyuan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chen Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junjie Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14463</id>
        <link href="http://arxiv.org/abs/2106.14463"/>
        <updated>2021-06-29T01:55:13.456Z</updated>
        <summary type="html"><![CDATA[Extracting structured clinical information from free-text radiology reports
can enable the use of radiology report information for a variety of critical
healthcare applications. In our work, we present RadGraph, a dataset of
entities and relations in full-text chest X-ray radiology reports based on a
novel information extraction schema we designed to structure radiology reports.
We release a development dataset, which contains board-certified radiologist
annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579
entities and 10,889 relations), and a test dataset, which contains two
independent sets of board-certified radiologist annotations for 100 radiology
reports split equally across the MIMIC-CXR and CheXpert datasets. Using these
datasets, we train and test a deep learning model, RadGraph Benchmark, that
achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR
and CheXpert test sets respectively. Additionally, we release an inference
dataset, which contains annotations automatically generated by RadGraph
Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4
million relations) and 500 CheXpert reports (13,783 entities and 9,908
relations) with mappings to associated chest radiographs. Our freely available
dataset can facilitate a wide range of research in medical natural language
processing, as well as computer vision and multi-modal learning when linked to
chest radiographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saahil Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Ashwin Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1"&gt;Adriel Saporta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1"&gt;Steven QH Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1"&gt;Du Nguyen Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tan Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1"&gt;Pierre Chambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1"&gt;Curtis P. Langlotz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers. (arXiv:2012.15828v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15828</id>
        <link href="http://arxiv.org/abs/2012.15828"/>
        <updated>2021-06-29T01:55:13.441Z</updated>
        <summary type="html"><![CDATA[We generalize deep self-attention distillation in MiniLM (Wang et al., 2020)
by only using self-attention relation distillation for task-agnostic
compression of pretrained Transformers. In particular, we define multi-head
self-attention relations as scaled dot-product between the pairs of query, key,
and value vectors within each self-attention module. Then we employ the above
relational knowledge to train the student model. Besides its simplicity and
unified principle, more favorably, there is no restriction in terms of the
number of student's attention heads, while most previous work has to guarantee
the same head number between teacher and student. Moreover, the fine-grained
self-attention relations tend to fully exploit the interaction knowledge
learned by Transformer. In addition, we thoroughly examine the layer selection
strategy for teacher models, rather than just relying on the last layer as in
MiniLM. We conduct extensive experiments on compressing both monolingual and
multilingual pretrained models. Experimental results demonstrate that our
models distilled from base-size and large-size teachers (BERT, RoBERTa and
XLM-R) outperform the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hangbo Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaohan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word2Box: Learning Word Representation Using Box Embeddings. (arXiv:2106.14361v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14361</id>
        <link href="http://arxiv.org/abs/2106.14361"/>
        <updated>2021-06-29T01:55:13.434Z</updated>
        <summary type="html"><![CDATA[Learning vector representations for words is one of the most fundamental
topics in NLP, capable of capturing syntactic and semantic relationships useful
in a variety of downstream NLP tasks. Vector representations can be limiting,
however, in that typical scoring such as dot product similarity intertwines
position and magnitude of the vector in space. Exciting innovations in the
space of representation learning have proposed alternative fundamental
representations, such as distributions, hyperbolic vectors, or regions. Our
model, Word2Box, takes a region-based approach to the problem of word
representation, representing words as $n$-dimensional rectangles. These
representations encode position and breadth independently and provide
additional geometric operations such as intersection and containment which
allow them to model co-occurrence patterns vectors struggle with. We
demonstrate improved performance on various word similarity tasks, particularly
on less common words, and perform a qualitative analysis exploring the
additional unique expressivity provided by Word2Box.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1"&gt;Shib Sankar Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1"&gt;Michael Boratko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atmakuri_S/0/1/0/all/0/1"&gt;Shriya Atmakuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Lorraine Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Dhruvesh Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1"&gt;Andrew McCallum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SymbolicGPT: A Generative Transformer Model for Symbolic Regression. (arXiv:2106.14131v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14131</id>
        <link href="http://arxiv.org/abs/2106.14131"/>
        <updated>2021-06-29T01:55:13.427Z</updated>
        <summary type="html"><![CDATA[Symbolic regression is the task of identifying a mathematical expression that
best fits a provided dataset of input and output values. Due to the richness of
the space of mathematical expressions, symbolic regression is generally a
challenging problem. While conventional approaches based on genetic evolution
algorithms have been used for decades, deep learning-based methods are
relatively new and an active research area. In this work, we present
SymbolicGPT, a novel transformer-based language model for symbolic regression.
This model exploits the advantages of probabilistic language models like GPT,
including strength in performance and flexibility. Through comprehensive
experiments, we show that our model performs strongly compared to competing
models with respect to the accuracy, running time, and data efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valipour_M/0/1/0/all/0/1"&gt;Mojtaba Valipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_B/0/1/0/all/0/1"&gt;Bowen You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panju_M/0/1/0/all/0/1"&gt;Maysum Panju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Effectiveness of Reviews in E-commerce Top-N Recommendation. (arXiv:2106.09665v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09665</id>
        <link href="http://arxiv.org/abs/2106.09665"/>
        <updated>2021-06-29T01:55:13.421Z</updated>
        <summary type="html"><![CDATA[Modern E-commerce websites contain heterogeneous sources of information, such
as numerical ratings, textual reviews and images. These information can be
utilized to assist recommendation. Through textual reviews, a user explicitly
express her affinity towards the item. Previous researchers found that by using
the information extracted from these reviews, we can better profile the users'
explicit preferences as well as the item features, leading to the improvement
of recommendation performance. However, most of the previous algorithms were
only utilizing the review information for explicit-feedback problem i.e. rating
prediction, and when it comes to implicit-feedback ranking problem such as
top-N recommendation, the usage of review information has not been fully
explored. Seeing this gap, in this work, we investigate the effectiveness of
textual review information for top-N recommendation under E-commerce settings.
We adapt several SOTA review-based rating prediction models for top-N
recommendation tasks and compare them to existing top-N recommendation models
from both performance and efficiency. We find that models utilizing only review
information can not achieve better performances than vanilla implicit-feedback
matrix factorization method. When utilizing review information as a regularizer
or auxiliary information, the performance of implicit-feedback matrix
factorization method can be further improved. However, the optimal model
structure to utilize textual reviews for E-commerce top-N recommendation is yet
to be determined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhichao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Hansi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingyao Ai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Case Study of LLVM-Based Analysis for Optimizing SIMD Code Generation. (arXiv:2106.14332v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.14332</id>
        <link href="http://arxiv.org/abs/2106.14332"/>
        <updated>2021-06-29T01:55:13.415Z</updated>
        <summary type="html"><![CDATA[This paper presents a methodology for using LLVM-based tools to tune the
DCA++ (dynamical clusterapproximation) application that targets the new ARM
A64FX processor. The goal is to describethe changes required for the new
architecture and generate efficient single instruction/multiple data(SIMD)
instructions that target the new Scalable Vector Extension instruction set.
During manualtuning, the authors used the LLVM tools to improve code
parallelization by using OpenMP SIMD,refactored the code and applied
transformation that enabled SIMD optimizations, and ensured thatthe correct
libraries were used to achieve optimal performance. By applying these code
changes, codespeed was increased by 1.98X and 78 GFlops were achieved on the
A64FX processor. The authorsaim to automatize parts of the efforts in the
OpenMP Advisor tool, which is built on top of existingand newly introduced LLVM
tooling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_J/0/1/0/all/0/1"&gt;Joseph Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1"&gt;Weile Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgakoudis_G/0/1/0/all/0/1"&gt;Giorgis Georgakoudis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doerfert_J/0/1/0/all/0/1"&gt;Johannes Doerfert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_O/0/1/0/all/0/1"&gt;Oscar Hernandez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Current Landscape of the Russian Sentiment Corpora. (arXiv:2106.14434v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14434</id>
        <link href="http://arxiv.org/abs/2106.14434"/>
        <updated>2021-06-29T01:55:13.399Z</updated>
        <summary type="html"><![CDATA[Currently, there are more than a dozen Russian-language corpora for sentiment
analysis, differing in the source of the texts, domain, size, number and ratio
of sentiment classes, and annotation method. This work examines publicly
available Russian-language corpora, presents their qualitative and quantitative
characteristics, which make it possible to get an idea of the current landscape
of the corpora for sentiment analysis. The ranking of corpora by annotation
quality is proposed, which can be useful when choosing corpora for training and
testing. The influence of the training dataset on the performance of sentiment
analysis is investigated based on the use of the deep neural network model
BERT. The experiments with review corpora allow us to conclude that on average
the quality of models increases with an increase in the number of training
corpora. For the first time, quality scores were obtained for the corpus of
reviews of ROMIP seminars based on the BERT model. Also, the study proposes the
task of the building a universal model for sentiment analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kotelnikov_E/0/1/0/all/0/1"&gt;Evgeny Kotelnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13884</id>
        <link href="http://arxiv.org/abs/2106.13884"/>
        <updated>2021-06-29T01:55:13.393Z</updated>
        <summary type="html"><![CDATA[When trained at sufficient scale, auto-regressive language models exhibit the
notable ability to learn a new language task after being prompted with just a
few examples. Here, we present a simple, yet effective, approach for
transferring this few-shot learning ability to a multimodal setting (vision and
language). Using aligned image and caption data, we train a vision encoder to
represent each image as a sequence of continuous embeddings, such that a
pre-trained, frozen language model prompted with this prefix generates the
appropriate caption. The resulting system is a multimodal few-shot learner,
with the surprising ability to learn a variety of new tasks when conditioned on
examples, represented as a sequence of multiple interleaved image and text
embeddings. We demonstrate that it can rapidly learn words for new objects and
novel visual categories, do visual question-answering with only a handful of
examples, and make use of outside knowledge, by measuring a single model on a
variety of established and new benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1"&gt;Maria Tsimpoukelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1"&gt;Jacob Menick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1"&gt;Serkan Cabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1"&gt;S.M. Ali Eslami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1"&gt;Felix Hill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Parsing Natural Language into Relational Algebra. (arXiv:2106.13858v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13858</id>
        <link href="http://arxiv.org/abs/2106.13858"/>
        <updated>2021-06-29T01:55:13.386Z</updated>
        <summary type="html"><![CDATA[Natural interface to database (NLIDB) has been researched a lot during the
past decades. In the core of NLIDB, is a semantic parser used to convert
natural language into SQL. Solutions from traditional NLP methodology focuses
on grammar rule pattern learning and pairing via intermediate logic forms.
Although those methods give an acceptable performance on certain specific
database and parsing tasks, they are hard to generalize and scale. On the other
hand, recent progress in neural deep learning seems to provide a promising
direction towards building a general NLIDB system. Unlike the traditional
approach, those neural methodologies treat the parsing problem as a
sequence-to-sequence learning problem. In this paper, we experimented on
several sequence-to-sequence learning models and evaluate their performance on
general database parsing task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Ayush Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages. (arXiv:2106.13822v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13822</id>
        <link href="http://arxiv.org/abs/2106.13822"/>
        <updated>2021-06-29T01:55:13.378Z</updated>
        <summary type="html"><![CDATA[Contemporary works on abstractive text summarization have focused primarily
on high-resource languages like English, mostly due to the limited availability
of datasets for low/mid-resource ones. In this work, we present XL-Sum, a
comprehensive and diverse dataset comprising 1 million professionally annotated
article-summary pairs from BBC, extracted using a set of carefully designed
heuristics. The dataset covers 44 languages ranging from low to high-resource,
for many of which no public dataset is currently available. XL-Sum is highly
abstractive, concise, and of high quality, as indicated by human and intrinsic
evaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model,
with XL-Sum and experiment on multilingual and low-resource summarization
tasks. XL-Sum induces competitive results compared to the ones obtained using
similar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10
languages we benchmark on, with some of them exceeding 15, as obtained by
multilingual training. Additionally, training on low-resource languages
individually also provides competitive performance. To the best of our
knowledge, XL-Sum is the largest abstractive summarization dataset in terms of
the number of samples collected from a single source and the number of
languages covered. We are releasing our dataset and models to encourage future
research on multilingual abstractive summarization. The resources can be found
at \url{https://github.com/csebuetnlp/xl-sum}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1"&gt;Tahmid Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1"&gt;Abhik Bhattacharjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Md Saiful Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samin_K/0/1/0/all/0/1"&gt;Kazi Samin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan-Fang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yong-Bin Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;M. Sohel Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1"&gt;Rifat Shahriyar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Persian Causality Corpus (PerCause) and the Causality Detection Benchmark. (arXiv:2106.14165v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14165</id>
        <link href="http://arxiv.org/abs/2106.14165"/>
        <updated>2021-06-29T01:55:13.370Z</updated>
        <summary type="html"><![CDATA[Recognizing causal elements and causal relations in text is one of the
challenging issues in natural language processing; specifically, in low
resource languages such as Persian. In this research we prepare a causality
human annotated corpus for the Persian language which consists of 4446
sentences and 5128 causal relations and three labels of cause, effect and
causal mark -- if possibl -- are specified for each relation. We have used this
corpus to train a system for detecting causal elements boundaries. Also, we
present a causality detection benchmark for three machine learning methods and
two deep learning systems based on this corpus. Performance evaluations
indicate that our best total result is obtained through CRF classifier which
has F-measure of 0.76 and the best accuracy obtained through Bi-LSTM-CRF deep
learning method with Accuracy equal to %91.4.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahimi_Z/0/1/0/all/0/1"&gt;Zeinab Rahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ShamsFard_M/0/1/0/all/0/1"&gt;Mehrnoush ShamsFard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Traditional Machine Learning and Deep Learning Models for Argumentation Mining in Russian Texts. (arXiv:2106.14438v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14438</id>
        <link href="http://arxiv.org/abs/2106.14438"/>
        <updated>2021-06-29T01:55:13.355Z</updated>
        <summary type="html"><![CDATA[Argumentation mining is a field of computational linguistics that is devoted
to extracting from texts and classifying arguments and relations between them,
as well as constructing an argumentative structure. A significant obstacle to
research in this area for the Russian language is the lack of annotated
Russian-language text corpora. This article explores the possibility of
improving the quality of argumentation mining using the extension of the
Russian-language version of the Argumentative Microtext Corpus (ArgMicro) based
on the machine translation of the Persuasive Essays Corpus (PersEssays). To
make it possible to use these two corpora combined, we propose a Joint Argument
Annotation Scheme based on the schemes used in ArgMicro and PersEssays. We
solve the problem of classifying argumentative discourse units (ADUs) into two
classes - "pro" ("for") and "opp" ("against") using traditional machine
learning techniques (SVM, Bagging and XGBoost) and a deep neural network (BERT
model). An ensemble of XGBoost and BERT models was proposed, which showed the
highest performance of ADUs classification for both corpora.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fishcheva_I/0/1/0/all/0/1"&gt;Irina Fishcheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goloviznina_V/0/1/0/all/0/1"&gt;Valeriya Goloviznina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotelnikov_E/0/1/0/all/0/1"&gt;Evgeny Kotelnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy. (arXiv:2106.13945v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13945</id>
        <link href="http://arxiv.org/abs/2106.13945"/>
        <updated>2021-06-29T01:55:13.349Z</updated>
        <summary type="html"><![CDATA[In recent years, reference-based and supervised summarization evaluation
metrics have been widely explored. However, collecting human-annotated
references and ratings are costly and time-consuming. To avoid these
limitations, we propose a training-free and reference-free summarization
evaluation metric. Our metric consists of a centrality-weighted relevance score
and a self-referenced redundancy score. The relevance score is computed between
the pseudo reference built from the source document and the given summary,
where the pseudo reference content is weighted by the sentence centrality to
provide importance guidance. Besides an $F_1$-based relevance score, we also
design an $F_\beta$-based variant that pays more attention to the recall score.
As for the redundancy score of the summary, we compute a self-masked similarity
score with the summary itself to evaluate the redundant information in the
summary. Finally, we combine the relevance and redundancy scores to produce the
final evaluation score of the given summary. Extensive experiments show that
our methods can significantly outperform existing methods on both
multi-document and single-document summarization evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Piji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KGRefiner: Knowledge Graph Refinement for Improving Accuracy of Translational Link Prediction Methods. (arXiv:2106.14233v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14233</id>
        <link href="http://arxiv.org/abs/2106.14233"/>
        <updated>2021-06-29T01:55:13.343Z</updated>
        <summary type="html"><![CDATA[Link prediction is the task of predicting missing relations between entities
of the knowledge graph by inferring from the facts contained in it. Recent work
in link prediction has attempted to provide a model for increasing link
prediction accuracy by using more layers in neural network architecture or
methods that add to the computational complexity of models. This paper we
proposed a method for refining the knowledge graph, which makes the knowledge
graph more informative, and link prediction operations can be performed more
accurately using relatively fast translational models. Translational link
prediction models, such as TransE, TransH, TransD, etc., have much less
complexity than deep learning approaches. This method uses the hierarchy of
relationships and also the hierarchy of entities in the knowledge graph to add
the entity information as a new entity to the graph and connect it to the nodes
which contain this information in their hierarchy. Our experiments show that
our method can significantly increase the performance of translational link
prediction methods in H@10, MR, MRR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saeedizade_M/0/1/0/all/0/1"&gt;Mohammad Javad Saeedizade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torabian_N/0/1/0/all/0/1"&gt;Najmeh Torabian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minaei_Bidgoli_B/0/1/0/all/0/1"&gt;Behrouz Minaei-Bidgoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Conceptual Blending with Large-scale Language and Vision Models. (arXiv:2106.14127v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14127</id>
        <link href="http://arxiv.org/abs/2106.14127"/>
        <updated>2021-06-29T01:55:13.334Z</updated>
        <summary type="html"><![CDATA[We ask the question: to what extent can recent large-scale language and image
generation models blend visual concepts? Given an arbitrary object, we identify
a relevant object and generate a single-sentence description of the blend of
the two using a language model. We then generate a visual depiction of the
blend using a text-based image generation model. Quantitative and qualitative
evaluations demonstrate the superiority of language models over classical
methods for conceptual blending, and of recent large-scale image generation
models over prior models for the visual depiction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Songwei Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Differential Privacy and Federated Learning for BERT Models. (arXiv:2106.13973v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13973</id>
        <link href="http://arxiv.org/abs/2106.13973"/>
        <updated>2021-06-29T01:55:13.326Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing (NLP) techniques can be applied to help with the
diagnosis of medical conditions such as depression, using a collection of a
person's utterances. Depression is a serious medical illness that can have
adverse effects on how one feels, thinks, and acts, which can lead to emotional
and physical problems. Due to the sensitive nature of such data, privacy
measures need to be taken for handling and training models with such data. In
this work, we study the effects that the application of Differential Privacy
(DP) has, in both a centralized and a Federated Learning (FL) setup, on
training contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).
We offer insights on how to privately train NLP models and what architectures
and setups provide more desirable privacy utility trade-offs. We envisage this
work to be used in future healthcare and mental health studies to keep medical
history private. Therefore, we provide an open-source implementation of this
work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1"&gt;Priyam Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1"&gt;Tiasa Singha Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1"&gt;Rakshit Naidu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1"&gt;Zumrut Muftuoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sahib Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1"&gt;Fatemehsadat Mireshghallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Research Trends in Inorganic Materials Literature Using NLP. (arXiv:2106.14157v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14157</id>
        <link href="http://arxiv.org/abs/2106.14157"/>
        <updated>2021-06-29T01:55:13.309Z</updated>
        <summary type="html"><![CDATA[In the field of inorganic materials science, there is a growing demand to
extract knowledge such as physical properties and synthesis processes of
materials by machine-reading a large number of papers. This is because
materials researchers refer to many papers in order to come up with promising
terms of experiments for material synthesis. However, there are only a few
systems that can extract material names and their properties. This study
proposes a large-scale natural language processing (NLP) pipeline for
extracting material names and properties from materials science literature to
enable the search and retrieval of results in materials science. Therefore, we
propose a label definition for extracting material names and properties and
accordingly build a corpus containing 836 annotated paragraphs extracted from
301 papers for training a named entity recognition (NER) model. Experimental
results demonstrate the utility of this NER model; it achieves successful
extraction with a micro-F1 score of 78.1%. To demonstrate the efficacy of our
approach, we present a thorough evaluation on a real-world automatically
annotated corpus by applying our trained NER model to 12,895 materials science
papers. We analyze the trend in materials science by visualizing the outputs of
the NLP pipeline. For example, the country-by-year analysis indicates that in
recent years, the number of papers on "MoS2," a material used in perovskite
solar cells, has been increasing rapidly in China but decreasing in the United
States. Further, according to the conditions-by-year analysis, the processing
temperature of the catalyst material "PEDOT:PSS" is shifting below 200 degree,
and the number of reports with a processing time exceeding 5 h is increasing
slightly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuniyoshi_F/0/1/0/all/0/1"&gt;Fusataka Kuniyoshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozawa_J/0/1/0/all/0/1"&gt;Jun Ozawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1"&gt;Makoto Miwa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Persian Rhetorical Structure Theory. (arXiv:2106.13833v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13833</id>
        <link href="http://arxiv.org/abs/2106.13833"/>
        <updated>2021-06-29T01:55:13.303Z</updated>
        <summary type="html"><![CDATA[Over the past years, interest in discourse analysis and discourse parsing has
steadily grown, and many discourse-annotated corpora and, as a result,
discourse parsers have been built. In this paper, we present a
discourse-annotated corpus for the Persian language built in the framework of
Rhetorical Structure Theory as well as a discourse parser built upon the DPLP
parser, an open-source discourse parser. Our corpus consists of 150
journalistic texts, each text having an average of around 400 words. Corpus
texts were annotated using 18 discourse relations and based on the annotation
guideline of the English RST Discourse Treebank corpus. Our text-level
discourse parser is trained using gold segmentation and is built upon the DPLP
discourse parser, which uses a large-margin transition-based approach to solve
the problem of discourse parsing. The performance of our discourse parser in
span (S), nuclearity (N) and relation (R) detection is around 78%, 64%, 44%
respectively, in terms of F1 measure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahmohammadi_S/0/1/0/all/0/1"&gt;Sara Shahmohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veisi_H/0/1/0/all/0/1"&gt;Hadi Veisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darzi_A/0/1/0/all/0/1"&gt;Ali Darzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Political Ideology and Polarization of Policy Positions: A Multi-dimensional Approach. (arXiv:2106.14387v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14387</id>
        <link href="http://arxiv.org/abs/2106.14387"/>
        <updated>2021-06-29T01:55:13.297Z</updated>
        <summary type="html"><![CDATA[Analyzing political ideology and polarization is of critical importance in
advancing our understanding of the political context in society. Recent
research has made great strides towards understanding the ideological bias
(i.e., stance) of news media along a left-right spectrum. In this work, we take
a novel approach and study the ideology of the policy under discussion teasing
apart the nuanced co-existence of stance and ideology. Aligned with the
theoretical accounts in political science, we treat ideology as a
multi-dimensional construct, and introduce the first diachronic dataset of news
articles whose political ideology under discussion is annotated by trained
political scientists and linguists at the paragraph-level. We showcase that
this framework enables quantitative analysis of polarization, a temporal,
multifaceted measure of ideological distance. We further present baseline
models for ideology prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sinno_B/0/1/0/all/0/1"&gt;Barea Sinno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oviedo_B/0/1/0/all/0/1"&gt;Bernardo Oviedo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atwell_K/0/1/0/all/0/1"&gt;Katherine Atwell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1"&gt;Malihe Alikhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Jessy Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Cascade Dual-Decoder Model for Joint Entity and Relation Extraction. (arXiv:2106.14163v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14163</id>
        <link href="http://arxiv.org/abs/2106.14163"/>
        <updated>2021-06-29T01:55:13.290Z</updated>
        <summary type="html"><![CDATA[Extracting relational triples from texts is a fundamental task in knowledge
graph construction. The popular way of existing methods is to jointly extract
entities and relations using a single model, which often suffers from the
overlapping triple problem. That is, there are multiple relational triples that
share the same entities within one sentence. In this work, we propose an
effective cascade dual-decoder approach to extract overlapping relational
triples, which includes a text-specific relation decoder and a
relation-corresponded entity decoder. Our approach is straightforward: the
text-specific relation decoder detects relations from a sentence according to
its text semantics and treats them as extra features to guide the entity
extraction; for each extracted relation, which is with trainable embedding, the
relation-corresponded entity decoder detects the corresponding head and tail
entities using a span-based tagging scheme. In this way, the overlapping triple
problem is tackled naturally. Experiments on two public datasets demonstrate
that our proposed approach outperforms state-of-the-art methods and achieves
better F1 scores under the strict evaluation metric. Our implementation is
available at https://github.com/prastunlp/DualDec.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lianbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Huimin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiliang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving Spatiotemporal PDEs. (arXiv:2106.14103v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14103</id>
        <link href="http://arxiv.org/abs/2106.14103"/>
        <updated>2021-06-29T01:55:13.282Z</updated>
        <summary type="html"><![CDATA[Partial differential equations (PDEs) play a fundamental role in modeling and
simulating problems across a wide range of disciplines. Recent advances in deep
learning have shown the great potential of physics-informed neural networks
(PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis.
However, the majority of existing PINN methods, based on fully-connected NNs,
pose intrinsic limitations to low-dimensional spatiotemporal parameterizations.
Moreover, since the initial/boundary conditions (I/BCs) are softly imposed via
penalty, the solution quality heavily relies on hyperparameter tuning. To this
end, we propose the novel physics-informed convolutional-recurrent learning
architectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled
data. Specifically, an encoder-decoder convolutional long short-term memory
network is proposed for low-dimensional spatial feature extraction and temporal
evolution learning. The loss function is defined as the aggregated discretized
PDE residuals, while the I/BCs are hard-encoded in the network to ensure
forcible satisfaction (e.g., periodic boundary padding). The networks are
further enhanced by autoregressive and residual connections that explicitly
simulate time marching. The performance of our proposed methods has been
assessed by solving three nonlinear PDEs (e.g., 2D Burgers' equations, the
$\lambda$-$\omega$ and FitzHugh Nagumo reaction-diffusion equations), and
compared against the start-of-the-art baseline algorithms. The numerical
results demonstrate the superiority of our proposed methodology in the context
of solution accuracy, extrapolability and generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_C/0/1/0/all/0/1"&gt;Chengping Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianxun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Dialogue State Tracking by Masked Hierarchical Transformer. (arXiv:2106.14433v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14433</id>
        <link href="http://arxiv.org/abs/2106.14433"/>
        <updated>2021-06-29T01:55:13.250Z</updated>
        <summary type="html"><![CDATA[This paper describes our approach to DSTC 9 Track 2: Cross-lingual
Multi-domain Dialog State Tracking, the task goal is to build a Cross-lingual
dialog state tracker with a training set in rich resource language and a
testing set in low resource language. We formulate a method for joint learning
of slot operation classification task and state tracking task respectively.
Furthermore, we design a novel mask mechanism for fusing contextual information
about dialogue, the results show the proposed model achieves excellent
performance on DSTC Challenge II with a joint accuracy of 62.37% and 23.96% in
MultiWOZ(en - zh) dataset and CrossWOZ(zh - en) dataset, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1"&gt;Min Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiasheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingyao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haipang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13948</id>
        <link href="http://arxiv.org/abs/2106.13948"/>
        <updated>2021-06-29T01:55:13.244Z</updated>
        <summary type="html"><![CDATA[Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1"&gt;Jonathan Francis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1"&gt;Nariaki Kitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1"&gt;Felix Labelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1"&gt;Ingrid Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image content dependent semi-fragile watermarking with localized tamper detection. (arXiv:2106.14150v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14150</id>
        <link href="http://arxiv.org/abs/2106.14150"/>
        <updated>2021-06-29T01:55:13.227Z</updated>
        <summary type="html"><![CDATA[Content-independent watermarks and block-wise independency can be considered
as vulnerabilities in semi-fragile watermarking methods. In this paper to
achieve the objectives of semi-fragile watermarking techniques, a method is
proposed to not have the mentioned shortcomings. In the proposed method, the
watermark is generated by relying on image content and a key. Furthermore, the
embedding scheme causes the watermarked blocks to become dependent on each
other, using a key. In the embedding phase, the image is partitioned into
non-overlapping blocks. In order to detect and separate the different types of
attacks more precisely, the proposed method embeds three copies of each
watermark bit into LWT coefficients of each 4x4 block. In the authentication
phase, by voting between the extracted bits the error maps are created; these
maps indicate image authenticity and reveal the modified regions. Also, in
order to automate the authentication, the images are classified into four
categories using seven features. Classification accuracy in the experiments is
97.97 percent. It is noted that our experiments demonstrate that the proposed
method is robust against JPEG compression and is competitive with a
state-of-the-art semi-fragile watermarking method, in terms of robustness and
semi-fragility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Samira Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1"&gt;Mojtaba Mahdavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Recommendation with Graph Neural Networks. (arXiv:2106.14226v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.14226</id>
        <link href="http://arxiv.org/abs/2106.14226"/>
        <updated>2021-06-29T01:55:13.218Z</updated>
        <summary type="html"><![CDATA[Sequential recommendation aims to leverage users' historical behaviors to
predict their next interaction. Existing works have not yet addressed two main
challenges in sequential recommendation. First, user behaviors in their rich
historical sequences are often implicit and noisy preference signals, they
cannot sufficiently reflect users' actual preferences. In addition, users'
dynamic preferences often change rapidly over time, and hence it is difficult
to capture user patterns in their historical sequences. In this work, we
propose a graph neural network model called SURGE (short for SeqUential
Recommendation with Graph neural nEtworks) to address these two issues.
Specifically, SURGE integrates different types of preferences in long-term user
behaviors into clusters in the graph by re-constructing loose item sequences
into tight item-item interest graphs based on metric learning. This helps
explicitly distinguish users' core interests, by forming dense clusters in the
interest graph. Then, we perform cluster-aware and query-aware graph
convolutional propagation and graph pooling on the constructed graph. It
dynamically fuses and extracts users' current activated core interests from
noisy user behavior sequences. We conduct extensive experiments on both public
and proprietary industrial datasets. Experimental results demonstrate
significant performance gains of our proposed method compared to
state-of-the-art methods. Further studies on sequence length confirm that our
method can model long behavioral sequences effectively and efficiently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jianxin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chen Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_Y/0/1/0/all/0/1"&gt;Yiqun Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1"&gt;Yanan Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1"&gt;Depeng Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rationale-Inspired Natural Language Explanations with Commonsense. (arXiv:2106.13876v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13876</id>
        <link href="http://arxiv.org/abs/2106.13876"/>
        <updated>2021-06-29T01:55:13.210Z</updated>
        <summary type="html"><![CDATA[Explainable machine learning models primarily justify predicted labels using
either extractive rationales (i.e., subsets of input features) or free-text
natural language explanations (NLEs) as abstractive justifications. While NLEs
can be more comprehensive than extractive rationales, machine-generated NLEs
have been shown to sometimes lack commonsense knowledge. Here, we show that
commonsense knowledge can act as a bridge between extractive rationales and
NLEs, rendering both types of explanations better. More precisely, we introduce
a unified framework, called RExC (Rationale-Inspired Explanations with
Commonsense), that (1) extracts rationales as a set of features responsible for
machine predictions, (2) expands the extractive rationales using available
commonsense resources, and (3) uses the expanded knowledge to generate natural
language explanations. Our framework surpasses by a large margin the previous
state-of-the-art in generating NLEs across five tasks in both natural language
processing and vision-language understanding, with human annotators
consistently rating the explanations generated by RExC to be more
comprehensive, grounded in commonsense, and overall preferred compared to
previous state-of-the-art models. Moreover, our work shows that
commonsense-grounded explanations can enhance both task performance and
rationales extraction capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1"&gt;Bodhisattwa Prasad Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1"&gt;Oana-Maria Camburu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning. (arXiv:2106.14019v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14019</id>
        <link href="http://arxiv.org/abs/2106.14019"/>
        <updated>2021-06-29T01:55:13.194Z</updated>
        <summary type="html"><![CDATA[Despite the success of various text generation metrics such as BERTScore, it
is still difficult to evaluate the image captions without enough reference
captions due to the diversity of the descriptions. In this paper, we introduce
a new metric UMIC, an Unreferenced Metric for Image Captioning which does not
require reference captions to evaluate image captions. Based on
Vision-and-Language BERT, we train UMIC to discriminate negative captions via
contrastive learning. Also, we observe critical problems of the previous
benchmark dataset (i.e., human annotations) on image captioning metric, and
introduce a new collection of human annotations on the generated captions. We
validate UMIC on four datasets, including our new dataset, and show that UMIC
has a higher correlation than all previous metrics that require multiple
references. We release the benchmark dataset and pre-trained models to compute
the UMIC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hwanhee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Seunghyun Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1"&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Trung Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1"&gt;Kyomin Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PeCoQ: A Dataset for Persian Complex Question Answering over Knowledge Graph. (arXiv:2106.14167v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14167</id>
        <link href="http://arxiv.org/abs/2106.14167"/>
        <updated>2021-06-29T01:55:13.181Z</updated>
        <summary type="html"><![CDATA[Question answering systems may find the answers to users' questions from
either unstructured texts or structured data such as knowledge graphs.
Answering questions using supervised learning approaches including deep
learning models need large training datasets. In recent years, some datasets
have been presented for the task of Question answering over knowledge graphs,
which is the focus of this paper. Although many datasets in English were
proposed, there have been a few question-answering datasets in Persian. This
paper introduces \textit{PeCoQ}, a dataset for Persian question answering. This
dataset contains 10,000 complex questions and answers extracted from the
Persian knowledge graph, FarsBase. For each question, the SPARQL query and two
paraphrases that were written by linguists are provided as well. There are
different types of complexities in the dataset, such as multi-relation,
multi-entity, ordinal, and temporal constraints. In this paper, we discuss the
dataset's characteristics and describe our methodology for building it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Etezadi_R/0/1/0/all/0/1"&gt;Romina Etezadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1"&gt;Mehrnoush Shamsfard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Draw Me a Flower: Grounding Formal Abstract Structures Stated in Informal Natural Language. (arXiv:2106.14321v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14321</id>
        <link href="http://arxiv.org/abs/2106.14321"/>
        <updated>2021-06-29T01:55:13.172Z</updated>
        <summary type="html"><![CDATA[Forming and interpreting abstraction is a core process in human
communication. In particular, when giving and performing complex instructions
stated in natural language (NL), people may naturally evoke abstract constructs
such as objects, loops, conditions and functions to convey their intentions in
an efficient and precise way. Yet, interpreting and grounding abstraction
stated in NL has not been systematically studied in NLP/AI. To elicit
naturally-occurring abstractions in NL we develop the Hexagons referential
game, where players describe increasingly complex images on a two-dimensional
Hexagons board, and other players need to follow these instructions to recreate
the images. Using this game we collected the Hexagons dataset, which consists
of 164 images and over 3000 naturally-occurring instructions, rich with diverse
abstractions. Results of our baseline models on an instruction-to-execution
task derived from the Hexagons dataset confirm that higher-level abstractions
in NL are indeed more challenging for current systems to process. Thus, this
dataset exposes a new and challenging dimension for grounded semantic parsing,
and we propose it for the community as a future benchmark to explore more
sophisticated and high-level communication within NLP applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lachmy_R/0/1/0/all/0/1"&gt;Royi Lachmy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1"&gt;Valentina Pyatkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1"&gt;Reut Tsarfaty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Remote Sensing Image Retrieval and Classification with Robust Fine-tuning. (arXiv:2102.13392v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13392</id>
        <link href="http://arxiv.org/abs/2102.13392"/>
        <updated>2021-06-29T01:55:13.161Z</updated>
        <summary type="html"><![CDATA[Advances in high resolution remote sensing image analysis are currently
hampered by the difficulty of gathering enough annotated data for training deep
learning methods, giving rise to a variety of small datasets and associated
dataset-specific methods. Moreover, typical tasks such as classification and
retrieval lack a systematic evaluation on standard benchmarks and training
datasets, which make it hard to identify durable and generalizable scientific
contributions. We aim at unifying remote sensing image retrieval and
classification with a new large-scale training and testing dataset, SF300,
including both vertical and oblique aerial images and made available to the
research community, and an associated fine-tuning method. We additionally
propose a new adversarial fine-tuning method for global descriptors. We show
that our framework systematically achieves a boost of retrieval and
classification performance on nine different datasets compared to an ImageNet
pretrained baseline, with currently no other method to compare to.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gominski_D/0/1/0/all/0/1"&gt;Dimitri Gominski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gouet_Brunet_V/0/1/0/all/0/1"&gt;Val&amp;#xe9;rie Gouet-Brunet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Sequential Recommendation Consistency with Self-Supervised Imitation. (arXiv:2106.14031v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.14031</id>
        <link href="http://arxiv.org/abs/2106.14031"/>
        <updated>2021-06-29T01:55:13.150Z</updated>
        <summary type="html"><![CDATA[Most sequential recommendation models capture the features of consecutive
items in a user-item interaction history. Though effective, their
representation expressiveness is still hindered by the sparse learning signals.
As a result, the sequential recommender is prone to make inconsistent
predictions. In this paper, we propose a model, \textbf{SSI}, to improve
sequential recommendation consistency with Self-Supervised Imitation.
Precisely, we extract the consistency knowledge by utilizing three
self-supervised pre-training tasks, where temporal consistency and persona
consistency capture user-interaction dynamics in terms of the chronological
order and persona sensitivities, respectively. Furthermore, to provide the
model with a global perspective, global session consistency is introduced by
maximizing the mutual information among global and local interaction sequences.
Finally, to comprehensively take advantage of all three independent aspects of
consistency-enhanced knowledge, we establish an integrated imitation learning
framework. The consistency knowledge is effectively internalized and
transferred to the student model by imitating the conventional prediction logit
as well as the consistency-enhanced item representations. In addition, the
flexible self-supervised imitation framework can also benefit other student
recommenders. Experiments on four real-world datasets show that SSI effectively
outperforms the state-of-the-art sequential recommendation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongshen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yonghao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiaofang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhuoye Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1"&gt;Bo Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI based Presentation Creator With Customized Audio Content Delivery. (arXiv:2106.14213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14213</id>
        <link href="http://arxiv.org/abs/2106.14213"/>
        <updated>2021-06-29T01:55:13.143Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an architecture to solve a novel problem statement
that has stemmed more so in recent times with an increase in demand for virtual
content delivery due to the COVID-19 pandemic. All educational institutions,
workplaces, research centers, etc. are trying to bridge the gap of
communication during these socially distanced times with the use of online
content delivery. The trend now is to create presentations, and then
subsequently deliver the same using various virtual meeting platforms. The time
being spent in such creation of presentations and delivering is what we try to
reduce and eliminate through this paper which aims to use Machine Learning (ML)
algorithms and Natural Language Processing (NLP) modules to automate the
process of creating a slides-based presentation from a document, and then use
state-of-the-art voice cloning models to deliver the content in the desired
author's voice. We consider a structured document such as a research paper to
be the content that has to be presented. The research paper is first summarized
using BERT summarization techniques and condensed into bullet points that go
into the slides. Tacotron inspired architecture with Encoder, Synthesizer, and
a Generative Adversarial Network (GAN) based vocoder, is used to convey the
contents of the slides in the author's voice (or any customized voice). Almost
all learning has now been shifted to online mode, and professionals are now
working from the comfort of their homes. Due to the current situation, teachers
and professionals have shifted to presentations to help them in imparting
information. In this paper, we aim to reduce the considerable amount of time
that is taken in creating a presentation by automating this process and
subsequently delivering this presentation in a customized voice, using a
content delivery mechanism that can clone any voice using a short audio clip.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansoor_M/0/1/0/all/0/1"&gt;Muvazima Mansoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Srikanth Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinath_R/0/1/0/all/0/1"&gt;Ramamoorthy Srinath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11347</id>
        <link href="http://arxiv.org/abs/2105.11347"/>
        <updated>2021-06-29T01:55:13.121Z</updated>
        <summary type="html"><![CDATA[In this article, we present a description of our systems as a part of our
participation in the shared task namely Artificial Intelligence for Legal
Assistance (AILA 2019). This is an integral event of Forum for Information
Retrieval Evaluation-2019. The outcomes of this track would be helpful for the
automation of the working process of the Indian Judiciary System. The manual
working procedures and documentation at any level (from lower to higher court)
of the judiciary system are very complex in nature. The systems produced as a
part of this track would assist the law practitioners. It would be helpful for
common men too. This kind of track also opens the path of research of Natural
Language Processing (NLP) in the judicial domain. This track defined two
problems such as Task 1: Identifying relevant prior cases for a given situation
and Task 2: Identifying the most relevant statutes for a given situation. We
tackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As
per the results declared by the task organizers, we are in 3rd and a modest
position in Task 1 and Task 2 respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1"&gt;Baban Gain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1"&gt;Dibyanayan Bandyopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1"&gt;Arkadipta De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1"&gt;Tanik Saikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14174</id>
        <link href="http://arxiv.org/abs/2106.14174"/>
        <updated>2021-06-29T01:55:13.112Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis benefits various applications such as
human-computer interaction and recommendation systems. It aims to infer the
users' bipolar ideas using visual, textual, and acoustic signals. Although
researchers affirm the association between cognitive cues and emotional
manifestations, most of the current multimodal approaches in sentiment analysis
disregard user-specific aspects. To tackle this issue, we devise a novel method
to perform multimodal sentiment prediction using cognitive cues, such as
personality. Our framework constructs an adaptive tree by hierarchically
dividing users and trains the LSTM-based submodels, utilizing an
attention-based fusion to transfer cognitive-oriented knowledge within the
tree. Subsequently, the framework consumes the conclusive agglomerative
knowledge from the adaptive tree to predict final sentiments. We also devise a
dynamic dropout method to facilitate data sharing between neighboring nodes,
reducing data sparsity. The empirical results on real-world datasets determine
that our proposed model for sentiment prediction can surpass trending rivals.
Moreover, compared to other ensemble approaches, the proposed transfer-based
algorithm can better utilize the latent cognitive cues and foster the
prediction outcomes. Based on the given extrinsic and intrinsic analysis
results, we note that compared to other theoretical-based techniques, the
proposed hierarchical clustering approach can better group the users within the
adaptive tree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1"&gt;Sana Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Saeid Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1"&gt;Raziyeh Zall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1"&gt;Mohammad Reza Kangavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1"&gt;Sara Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1"&gt;Wen Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14269</id>
        <link href="http://arxiv.org/abs/2106.14269"/>
        <updated>2021-06-29T01:55:13.104Z</updated>
        <summary type="html"><![CDATA[In large technology companies, the requirements for managing and organizing
technical documents created by engineers and managers in supporting relevant
decision making have increased dramatically in recent years, which has led to a
higher demand for more scalable, accurate, and automated document
classification. Prior studies have primarily focused on processing text for
classification and small-scale databases. This paper describes a novel
multimodal deep learning architecture, called TechDoc, for technical document
classification, which utilizes both natural language and descriptive images to
train hierarchical classifiers. The architecture synthesizes convolutional
neural networks and recurrent neural networks through an integrated training
process. We applied the architecture to a large multimodal technical document
database and trained the model for classifying documents based on the
hierarchical International Patent Classification system. Our results show that
the trained neural network presents a greater classification accuracy than
those using a single modality and several earlier text classification methods.
The trained model can potentially be scaled to millions of real-world technical
documents with both text and figures, which is useful for data and knowledge
management in large technology companies and organizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shuo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jianxi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1"&gt;Christopher L. Magee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intent Disentanglement and Feature Self-supervision for Novel Recommendation. (arXiv:2106.14388v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.14388</id>
        <link href="http://arxiv.org/abs/2106.14388"/>
        <updated>2021-06-29T01:55:13.094Z</updated>
        <summary type="html"><![CDATA[One key property in recommender systems is the long-tail distribution in
user-item interactions where most items only have few user feedback. Improving
the recommendation of tail items can promote novelty and bring positive effects
to both users and providers, and thus is a desirable property of recommender
systems. Current novel recommendation studies over-emphasize the importance of
tail items without differentiating the degree of users' intent on popularity
and often incur a sharp decline of accuracy. Moreover, none of existing methods
has ever taken the extreme case of tail items, i.e., cold-start items without
any interaction, into consideration.

In this work, we first disclose the mechanism that drives a user's
interaction towards popular or niche items by disentangling her intent into
conformity influence (popularity) and personal interests (preference). We then
present a unified end-to-end framework to simultaneously optimize accuracy and
novelty targets based on the disentangled intent of popularity and that of
preference. We further develop a new paradigm for novel recommendation of
cold-start items which exploits the self-supervised learning technique to model
the correlation between collaborative features and content features. We conduct
extensive experimental results on three real-world datasets. The results
demonstrate that our proposed model yields significant improvements over the
state-of-the-art baselines in terms of accuracy, novelty, coverage, and
trade-off.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1"&gt;Tieyun Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yile Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1"&gt;Ke Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhiyong Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interplay between Upsampling and Regularization for Provider Fairness in Recommender Systems. (arXiv:2006.04279v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04279</id>
        <link href="http://arxiv.org/abs/2006.04279"/>
        <updated>2021-06-29T01:55:13.076Z</updated>
        <summary type="html"><![CDATA[Considering the impact of recommendations on item providers is one of the
duties of multi-sided recommender systems. Item providers are key stakeholders
in online platforms, and their earnings and plans are influenced by the
exposure their items receive in recommended lists. Prior work showed that
certain minority groups of providers, characterized by a common sensitive
attribute (e.g., gender or race), are being disproportionately affected by
indirect and unintentional discrimination. Our study in this paper handles a
situation where ($i$) the same provider is associated with multiple items of a
list suggested to a user, ($ii$) an item is created by more than one provider
jointly, and ($iii$) predicted user-item relevance scores are biasedly
estimated for items of provider groups. Under this scenario, we assess
disparities in relevance, visibility, and exposure, by simulating diverse
representations of the minority group in the catalog and the interactions.
Based on emerged unfair outcomes, we devise a treatment that combines
observation upsampling and loss regularization, while learning user-item
relevance scores. Experiments on real-world data demonstrate that our treatment
leads to lower disparate relevance. The resulting recommended lists show fairer
visibility and exposure, higher minority item coverage, and negligible loss in
recommendation utility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boratto_L/0/1/0/all/0/1"&gt;Ludovico Boratto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fenu_G/0/1/0/all/0/1"&gt;Gianni Fenu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marras_M/0/1/0/all/0/1"&gt;Mirko Marras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.14174</id>
        <link href="http://arxiv.org/abs/2106.14174"/>
        <updated>2021-06-29T01:55:13.055Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis benefits various applications such as
human-computer interaction and recommendation systems. It aims to infer the
users' bipolar ideas using visual, textual, and acoustic signals. Although
researchers affirm the association between cognitive cues and emotional
manifestations, most of the current multimodal approaches in sentiment analysis
disregard user-specific aspects. To tackle this issue, we devise a novel method
to perform multimodal sentiment prediction using cognitive cues, such as
personality. Our framework constructs an adaptive tree by hierarchically
dividing users and trains the LSTM-based submodels, utilizing an
attention-based fusion to transfer cognitive-oriented knowledge within the
tree. Subsequently, the framework consumes the conclusive agglomerative
knowledge from the adaptive tree to predict final sentiments. We also devise a
dynamic dropout method to facilitate data sharing between neighboring nodes,
reducing data sparsity. The empirical results on real-world datasets determine
that our proposed model for sentiment prediction can surpass trending rivals.
Moreover, compared to other ensemble approaches, the proposed transfer-based
algorithm can better utilize the latent cognitive cues and foster the
prediction outcomes. Based on the given extrinsic and intrinsic analysis
results, we note that compared to other theoretical-based techniques, the
proposed hierarchical clustering approach can better group the users within the
adaptive tree.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1"&gt;Sana Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Saeid Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1"&gt;Raziyeh Zall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1"&gt;Mohammad Reza Kangavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1"&gt;Sara Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1"&gt;Wen Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting race and gender bias in visual representation of AI on web search engines. (arXiv:2106.14072v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.14072</id>
        <link href="http://arxiv.org/abs/2106.14072"/>
        <updated>2021-06-29T01:55:13.044Z</updated>
        <summary type="html"><![CDATA[Web search engines influence perception of social reality by filtering and
ranking information. However, their outputs are often subjected to bias that
can lead to skewed representation of subjects such as professional occupations
or gender. In our paper, we use a mixed-method approach to investigate presence
of race and gender bias in representation of artificial intelligence (AI) in
image search results coming from six different search engines. Our findings
show that search engines prioritize anthropomorphic images of AI that portray
it as white, whereas non-white images of AI are present only in non-Western
search engines. By contrast, gender representation of AI is more diverse and
less skewed towards a specific gender that can be attributed to higher
awareness about gender bias in search outputs. Our observations indicate both
the the need and the possibility for addressing bias in representation of
societally relevant subjects, such as technological innovation, and emphasize
the importance of designing new approaches for detecting bias in information
retrieval systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makhortykh_M/0/1/0/all/0/1"&gt;Mykola Makhortykh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urman_A/0/1/0/all/0/1"&gt;Aleksandra Urman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulloa_R/0/1/0/all/0/1"&gt;Roberto Ulloa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.14118</id>
        <link href="http://arxiv.org/abs/2106.14118"/>
        <updated>2021-06-29T01:55:13.025Z</updated>
        <summary type="html"><![CDATA[State of the art architectures for untrimmed video Temporal Action
Localization (TAL) have only considered RGB and Flow modalities, leaving the
information-rich audio modality totally unexploited. Audio fusion has been
explored for the related but arguably easier problem of trimmed (clip-level)
action recognition. However, TAL poses a unique set of challenges. In this
paper, we propose simple but effective fusion-based approaches for TAL. To the
best of our knowledge, our work is the first to jointly consider audio and
video modalities for supervised TAL. We experimentally show that our schemes
consistently improve performance for state of the art video-only TAL
approaches. Specifically, they help achieve new state of the art performance on
large-scale benchmark datasets - ActivityNet-1.3 (52.73 mAP@0.5) and THUMOS14
(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion
schemes, modality combinations and TAL architectures. Our code, models and
associated data will be made available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1"&gt;Anurag Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1"&gt;Jazib Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1"&gt;Dolton Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1"&gt;Ravi Kiran Sarvadevabhatla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.14463</id>
        <link href="http://arxiv.org/abs/2106.14463"/>
        <updated>2021-06-29T01:55:13.015Z</updated>
        <summary type="html"><![CDATA[Extracting structured clinical information from free-text radiology reports
can enable the use of radiology report information for a variety of critical
healthcare applications. In our work, we present RadGraph, a dataset of
entities and relations in full-text chest X-ray radiology reports based on a
novel information extraction schema we designed to structure radiology reports.
We release a development dataset, which contains board-certified radiologist
annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579
entities and 10,889 relations), and a test dataset, which contains two
independent sets of board-certified radiologist annotations for 100 radiology
reports split equally across the MIMIC-CXR and CheXpert datasets. Using these
datasets, we train and test a deep learning model, RadGraph Benchmark, that
achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR
and CheXpert test sets respectively. Additionally, we release an inference
dataset, which contains annotations automatically generated by RadGraph
Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4
million relations) and 500 CheXpert reports (13,783 entities and 9,908
relations) with mappings to associated chest radiographs. Our freely available
dataset can facilitate a wide range of research in medical natural language
processing, as well as computer vision and multi-modal learning when linked to
chest radiographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saahil Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Ashwin Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1"&gt;Adriel Saporta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1"&gt;Steven QH Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1"&gt;Du Nguyen Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tan Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1"&gt;Pierre Chambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1"&gt;Curtis P. Langlotz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Query-graph with Cross-gating Attention Model for Text-to-Audio Grounding. (arXiv:2106.14136v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.14136</id>
        <link href="http://arxiv.org/abs/2106.14136"/>
        <updated>2021-06-29T01:55:12.995Z</updated>
        <summary type="html"><![CDATA[In this paper, we address the text-to-audio grounding issue, namely,
grounding the segments of the sound event described by a natural language query
in the untrimmed audio. This is a newly proposed but challenging audio-language
task, since it requires to not only precisely localize all the on- and off-sets
of the desired segments in the audio, but to perform comprehensive acoustic and
linguistic understandings and reason the multimodal interactions between the
audio and query. To tackle those problems, the existing method treats the query
holistically as a single unit by a global query representation, which fails to
highlight the keywords that contain rich semantics. Besides, this method has
not fully exploited interactions between the query and audio. Moreover, since
the audio and queries are arbitrary and variable in length, many meaningless
parts of them are not filtered out in this method, which hinders the grounding
of the desired segments.

To this end, we propose a novel Query Graph with Cross-gating Attention
(QGCA) model, which models the comprehensive relations between the words in
query through a novel query graph. Besides, to capture the fine-grained
interactions between audio and query, a cross-modal attention module that
assigns higher weights to the keywords is introduced to generate the
snippet-specific query representations. Finally, we also design a cross-gating
module to emphasize the crucial parts as well as weaken the irrelevant ones in
the audio and query. We extensively evaluate the proposed QGCA model on the
public Audiogrounding dataset with significant improvements over several
state-of-the-art methods. Moreover, further ablation study shows the consistent
effectiveness of different modules in the proposed QGCA model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haoyu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jihua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qinghai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhiyong Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Synthetic Data for Opinion-free Blind Image Quality Assessment in the Wild. (arXiv:2106.14076v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.14076</id>
        <link href="http://arxiv.org/abs/2106.14076"/>
        <updated>2021-06-29T01:55:12.531Z</updated>
        <summary type="html"><![CDATA[Nowadays, most existing blind image quality assessment (BIQA) models 1) are
developed for synthetically-distorted images and often generalize poorly to
authentic ones; 2) heavily rely on human ratings, which are prohibitively
labor-expensive to collect. Here, we propose an $opinion$-$free$ BIQA method
that learns from synthetically-distorted images and multiple agents to assess
the perceptual quality of authentically-distorted ones captured in the wild
without relying on human labels. Specifically, we first assemble a large number
of image pairs from synthetically-distorted images and use a set of
full-reference image quality assessment (FR-IQA) models to assign pseudo-binary
labels of each pair indicating which image has higher quality as the
supervisory signal. We then train a convolutional neural network (CNN)-based
BIQA model to rank the perceptual quality, optimized for consistency with the
binary labels. Since there exists domain shift between the synthetically- and
authentically-distorted images, an unsupervised domain adaptation (UDA) module
is introduced to alleviate this issue. Extensive experiments demonstrate the
effectiveness of our proposed $opinion$-$free$ BIQA model, yielding
state-of-the-art performance in terms of correlation with human opinion scores,
as well as gMAD competition. Codes will be made publicly available upon
acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhihua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhiri Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zekuang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiangguo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuming Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Immersive Stories for Health Information: Design Considerations from Binge Drinking in VR. (arXiv:2106.13921v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.13921</id>
        <link href="http://arxiv.org/abs/2106.13921"/>
        <updated>2021-06-29T01:55:12.509Z</updated>
        <summary type="html"><![CDATA[Immersive stories for health are 360-degree videos that intend to alter
viewer perceptions about behaviors detrimental to health. They have potential
to inform public health at scale, however, immersive story design is still in
early stages and largely devoid of best practices. This paper presents a focus
group study with 147 viewers of an immersive story about binge drinking
experienced through VR headsets and mobile phones. The objective of the study
is to identify aspects of immersive story design that influence attitudes
towards the health issue exhibited, and to understand how health information is
consumed in immersive stories. Findings emphasize the need for an immersive
story to provide reasoning behind character engagement in the focal health
behavior, to show the main character clearly engaging in the behavior, and to
enable viewers to experience escalating symptoms of the behavior before the
penultimate health consequence. Findings also show how the design of supporting
characters can inadvertently distract viewers and lead them to justify the
detrimental behavior being exhibited. The paper concludes with design
considerations for enabling immersive stories to better inform public
perception of health issues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zytko_D/0/1/0/all/0/1"&gt;Douglas Zytko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zexin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1"&gt;Jacob Gleason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lundquist_N/0/1/0/all/0/1"&gt;Nathaniel Lundquist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1"&gt;Medina Taylor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text. (arXiv:2106.14014v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.14014</id>
        <link href="http://arxiv.org/abs/2106.14014"/>
        <updated>2021-06-29T01:55:12.484Z</updated>
        <summary type="html"><![CDATA[Video represents the majority of internet traffic today leading to a
continuous technological arms race between generating higher quality content,
transmitting larger file sizes and supporting network infrastructure. Adding to
this is the recent COVID-19 pandemic fueled surge in the use of video
conferencing tools. Since videos take up substantial bandwidth (~100 Kbps to
few Mbps), improved video compression can have a substantial impact on network
performance for live and pre-recorded content, providing broader access to
multimedia content worldwide. In this work, we present a novel video
compression pipeline, called Txt2Vid, which substantially reduces data
transmission rates by compressing webcam videos ("talking-head videos") to a
text transcript. The text is transmitted and decoded into a realistic
reconstruction of the original video using recent advances in deep learning
based voice cloning and lip syncing models. Our generative pipeline achieves
two to three orders of magnitude reduction in the bitrate as compared to the
standard audio-video codecs (encoders-decoders), while maintaining equivalent
Quality-of-Experience based on a subjective evaluation by users (n=242) in an
online study. The code for this work is available at
https://github.com/tpulkit/txt2vid.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tandon_P/0/1/0/all/0/1"&gt;Pulkit Tandon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chandak_S/0/1/0/all/0/1"&gt;Shubham Chandak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pataranutaporn_P/0/1/0/all/0/1"&gt;Pat Pataranutaporn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yimeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mapuranga_A/0/1/0/all/0/1"&gt;Anesu M. Mapuranga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maes_P/0/1/0/all/0/1"&gt;Pattie Maes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weissman_T/0/1/0/all/0/1"&gt;Tsachy Weissman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sra_M/0/1/0/all/0/1"&gt;Misha Sra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Attention Self-supervised Contrastive Learning based Three-stage Model for Hand Shape Feature Representation in Cued Speech. (arXiv:2106.14016v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.14016</id>
        <link href="http://arxiv.org/abs/2106.14016"/>
        <updated>2021-06-29T01:55:12.438Z</updated>
        <summary type="html"><![CDATA[Cued Speech (CS) is a communication system for deaf people or hearing
impaired people, in which a speaker uses it to aid a lipreader in phonetic
level by clarifying potentially ambiguous mouth movements with hand shape and
positions. Feature extraction of multi-modal CS is a key step in CS
recognition. Recent supervised deep learning based methods suffer from noisy CS
data annotations especially for hand shape modality. In this work, we first
propose a self-supervised contrastive learning method to learn the feature
representation of image without using labels. Secondly, a small amount of
manually annotated CS data are used to fine-tune the first module. Thirdly, we
present a module, which combines Bi-LSTM and self-attention networks to further
learn sequential features with temporal and contextual information. Besides, to
enlarge the volume and the diversity of the current limited CS datasets, we
build a new British English dataset containing 5 native CS speakers. Evaluation
results on both French and British English datasets show that our model
achieves over 90% accuracy in hand shape recognition. Significant improvements
of 8.75% (for French) and 10.09% (for British English) are achieved in CS
phoneme recognition correctness compared with the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianrong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1"&gt;Nan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1"&gt;Mei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuewei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1"&gt;Qiang Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Li Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03040</id>
        <link href="http://arxiv.org/abs/2001.03040"/>
        <updated>2021-06-28T01:57:57.979Z</updated>
        <summary type="html"><![CDATA[This paper establishes the optimal approximation error characterization of
deep ReLU networks for smooth functions in terms of both width and depth
simultaneously. To that end, we first prove that multivariate polynomials can
be approximated by deep ReLU networks of width $\mathcal{O}(N)$ and depth
$\mathcal{O}(L)$ with an approximation error $\mathcal{O}(N^{-L})$. Through
local Taylor expansions and their deep ReLU network approximations, we show
that deep ReLU networks of width $\mathcal{O}(N\ln N)$ and depth
$\mathcal{O}(L\ln L)$ can approximate $f\in C^s([0,1]^d)$ with a nearly optimal
approximation error $\mathcal{O}(\|f\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our
estimate is non-asymptotic in the sense that it is valid for arbitrary width
and depth specified by $N\in\mathbb{N}^+$ and $L\in\mathbb{N}^+$, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jianfeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zuowei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework. (arXiv:2006.13365v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13365</id>
        <link href="http://arxiv.org/abs/2006.13365"/>
        <updated>2021-06-28T01:57:57.973Z</updated>
        <summary type="html"><![CDATA[The heterogeneity in recently published knowledge graph embedding models'
implementations, training, and evaluation has made fair and thorough
comparisons difficult. In order to assess the reproducibility of previously
published results, we re-implemented and evaluated 21 interaction models in the
PyKEEN software package. Here, we outline which results could be reproduced
with their reported hyper-parameters, which could only be reproduced with
alternate hyper-parameters, and which could not be reproduced at all as well as
provide insight as to why this might be the case.

We then performed a large-scale benchmarking on four datasets with several
thousands of experiments and 24,804 GPU hours of computation time. We present
insights gained as to best practices, best configurations for each model, and
where improvements could be made over previously published best configurations.
Our results highlight that the combination of model architecture, training
approach, loss function, and the explicit modeling of inverse relations is
crucial for a model's performances, and not only determined by the model
architecture. We provide evidence that several architectures can obtain results
competitive to the state-of-the-art when configured carefully. We have made all
code, experimental configurations, results, and analyses that lead to our
interpretations available at https://github.com/pykeen/pykeen and
https://github.com/pykeen/benchmarking]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1"&gt;Mehdi Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1"&gt;Max Berrendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1"&gt;Charles Tapley Hoyt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vermue_L/0/1/0/all/0/1"&gt;Laurent Vermue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharifzadeh_S/0/1/0/all/0/1"&gt;Sahand Sharifzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1"&gt;Asja Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Driven Out-of-Distribution Detection with Statistical Guarantees for Robot Learning. (arXiv:2106.13703v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.13703</id>
        <link href="http://arxiv.org/abs/2106.13703"/>
        <updated>2021-06-28T01:57:57.965Z</updated>
        <summary type="html"><![CDATA[Our goal is to perform out-of-distribution (OOD) detection, i.e., to detect
when a robot is operating in environments that are drawn from a different
distribution than the environments used to train the robot. We leverage
Probably Approximately Correct (PAC)-Bayes theory in order to train a policy
with a guaranteed bound on performance on the training distribution. Our key
idea for OOD detection then relies on the following intuition: violation of the
performance bound on test environments provides evidence that the robot is
operating OOD. We formalize this via statistical techniques based on p-values
and concentration inequalities. The resulting approach (i) provides guaranteed
confidence bounds on OOD detection, and (ii) is task-driven and sensitive only
to changes that impact the robot's performance. We demonstrate our approach on
a simulated example of grasping objects with unfamiliar poses or shapes. We
also present both simulation and hardware experiments for a drone performing
vision-based obstacle avoidance in unfamiliar environments (including wind
disturbances and different obstacle densities). Our examples demonstrate that
we can perform task-driven OOD detection within just a handful of trials.
Comparisons with baselines also demonstrate the advantages of our approach in
terms of providing statistical guarantees and being insensitive to
task-irrelevant distribution shifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farid_A/0/1/0/all/0/1"&gt;Alec Farid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veer_S/0/1/0/all/0/1"&gt;Sushant Veer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1"&gt;Anirudha Majumdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems. (arXiv:2106.13781v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13781</id>
        <link href="http://arxiv.org/abs/2106.13781"/>
        <updated>2021-06-28T01:57:57.960Z</updated>
        <summary type="html"><![CDATA[Stochastic nested optimization, including stochastic compositional, min-max
and bilevel optimization, is gaining popularity in many machine learning
applications. While the three problems share the nested structure, existing
works often treat them separately, and thus develop problem-specific algorithms
and their analyses. Among various exciting developments, simple SGD-type
updates (potentially on multiple variables) are still prevalent in solving this
class of nested problems, but they are believed to have slower convergence rate
compared to that of the non-nested problems. This paper unifies several
SGD-type updates for stochastic nested problems into a single SGD approach that
we term ALternating Stochastic gradient dEscenT (ALSET) method. By leveraging
the hidden smoothness of the problem, this paper presents a tighter analysis of
ALSET for stochastic nested problems. Under the new analysis, to achieve an
$\epsilon$-stationary point of the nested problem, it requires ${\cal
O}(\epsilon^{-2})$ samples. Under certain regularity conditions, applying our
results to stochastic compositional, min-max and reinforcement learning
problems either improves or matches the best-known sample complexity in the
respective cases. Our results explain why simple SGD-type algorithms in
stochastic nested problems all work very well in practice without the need for
further modifications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuejiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boolean learning under noise-perturbations in hardware neural networks. (arXiv:2003.12319v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12319</id>
        <link href="http://arxiv.org/abs/2003.12319"/>
        <updated>2021-06-28T01:57:57.953Z</updated>
        <summary type="html"><![CDATA[A high efficiency hardware integration of neural networks benefits from
realizing nonlinearity, network connectivity and learning fully in a physical
substrate. Multiple systems have recently implemented some or all of these
operations, yet the focus was placed on addressing technological challenges.
Fundamental questions regarding learning in hardware neural networks remain
largely unexplored. Noise in particular is unavoidable in such architectures,
and here we investigate its interaction with a learning algorithm using an
opto-electronic recurrent neural network. We find that noise strongly modifies
the system's path during convergence, and surprisingly fully decorrelates the
final readout weight matrices. This highlights the importance of understanding
architecture, noise and learning algorithm as interacting players, and
therefore identifies the need for mathematical tools for noisy, analogue system
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Andreoli_L/0/1/0/all/0/1"&gt;Louis Andreoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porte_X/0/1/0/all/0/1"&gt;Xavier Porte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chretien_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Chr&amp;#xe9;tien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacquot_M/0/1/0/all/0/1"&gt;Maxime Jacquot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larger_L/0/1/0/all/0/1"&gt;Laurent Larger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunner_D/0/1/0/all/0/1"&gt;Daniel Brunner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing Generalization of SGD via Disagreement. (arXiv:2106.13799v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13799</id>
        <link href="http://arxiv.org/abs/2106.13799"/>
        <updated>2021-06-28T01:57:57.932Z</updated>
        <summary type="html"><![CDATA[We empirically show that the test error of deep networks can be estimated by
simply training the same architecture on the same training set but with a
different run of Stochastic Gradient Descent (SGD), and measuring the
disagreement rate between the two networks on unlabeled test data. This builds
on -- and is a stronger version of -- the observation in Nakkiran & Bansal '20,
which requires the second run to be on an altogether fresh training set. We
further theoretically show that this peculiar phenomenon arises from the
\emph{well-calibrated} nature of \emph{ensembles} of SGD-trained models. This
finding not only provides a simple empirical measure to directly predict the
test error using unlabeled test data, but also establishes a new conceptual
connection between generalization and calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yiding Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagarajan_V/0/1/0/all/0/1"&gt;Vaishnavh Nagarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baek_C/0/1/0/all/0/1"&gt;Christina Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1"&gt;J. Zico Kolter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Mean Field Games, with Applications to Economics. (arXiv:2106.13755v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.13755</id>
        <link href="http://arxiv.org/abs/2106.13755"/>
        <updated>2021-06-28T01:57:57.924Z</updated>
        <summary type="html"><![CDATA[Mean field games (MFG) and mean field control problems (MFC) are frameworks
to study Nash equilibria or social optima in games with a continuum of agents.
These problems can be used to approximate competitive or cooperative games with
a large finite number of agents and have found a broad range of applications,
in particular in economics. In recent years, the question of learning in MFG
and MFC has garnered interest, both as a way to compute solutions and as a way
to model how large populations of learners converge to an equilibrium. Of
particular interest is the setting where the agents do not know the model,
which leads to the development of reinforcement learning (RL) methods. After
reviewing the literature on this topic, we present a two timescale approach
with RL for MFG and MFC, which relies on a unified Q-learning algorithm. The
main novelty of this method is to simultaneously update an action-value
function and a distribution but with different rates, in a model-free fashion.
Depending on the ratio of the two learning rates, the algorithm learns either
the MFG or the MFC solution. To illustrate this method, we apply it to a mean
field problem of accumulated consumption in finite horizon with HARA utility
function, and to a trader's optimal liquidation problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Angiuli_A/0/1/0/all/0/1"&gt;Andrea Angiuli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fouque_J/0/1/0/all/0/1"&gt;Jean-Pierre Fouque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lauriere_M/0/1/0/all/0/1"&gt;Mathieu Lauriere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the Lockdown Effects on Air Quality during COVID-19 Era. (arXiv:2106.13750v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13750</id>
        <link href="http://arxiv.org/abs/2106.13750"/>
        <updated>2021-06-28T01:57:57.919Z</updated>
        <summary type="html"><![CDATA[In this work we investigate the short-term variations in air quality
emissions, attributed to the prevention measures, applied in different cities,
to mitigate the COVID-19 spread. In particular, we emphasize on the
concentration effects regarding specific pollutant gases, such as carbon
monoxide (CO), ozone (O3), nitrogen dioxide (NO2) and sulphur dioxide (SO2).
The assessment of the impact of lockdown on air quality focused on four
European Cities (Athens, Gladsaxe, Lodz and Rome). Available data on pollutant
factors were obtained using global satellite observations. The level of the
employed prevention measures is employed using the Oxford COVID-19 Government
Response Tracker. The second part of the analysis employed a variety of machine
learning tools, utilized for estimating the concentration of each pollutant,
two days ahead. The results showed that a weak to moderate correlation exists
between the corresponding measures and the pollutant factors and that it is
possible to create models which can predict the behaviour of the pollutant
gases under daily human activities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kavouras_I/0/1/0/all/0/1"&gt;Ioannis Kavouras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1"&gt;Eftychios Protopapadakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaselimia_M/0/1/0/all/0/1"&gt;Maria Kaselimia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sardis_E/0/1/0/all/0/1"&gt;Emmanuel Sardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1"&gt;Nikolaos Doulamis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Mixture Density Networks. (arXiv:2012.03085v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03085</id>
        <link href="http://arxiv.org/abs/2012.03085"/>
        <updated>2021-06-28T01:57:57.911Z</updated>
        <summary type="html"><![CDATA[We introduce the Graph Mixture Density Networks, a new family of machine
learning models that can fit multimodal output distributions conditioned on
graphs of arbitrary topology. By combining ideas from mixture models and graph
representation learning, we address a broader class of challenging conditional
density estimation problems that rely on structured data. In this respect, we
evaluate our method on a new benchmark application that leverages random graphs
for stochastic epidemic simulations. We show a significant improvement in the
likelihood of epidemic outcomes when taking into account both multimodality and
structure. The empirical analysis is complemented by two real-world regression
tasks showing the effectiveness of our approach in modeling the output
prediction uncertainty. Graph Mixture Density Networks open appealing research
opportunities in the study of structure-dependent phenomena that exhibit
non-trivial conditional output distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Errica_F/0/1/0/all/0/1"&gt;Federico Errica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1"&gt;Alessio Micheli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interval and fuzzy physics-informed neural networks for uncertain fields. (arXiv:2106.13727v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.13727</id>
        <link href="http://arxiv.org/abs/2106.13727"/>
        <updated>2021-06-28T01:57:57.905Z</updated>
        <summary type="html"><![CDATA[Temporally and spatially dependent uncertain parameters are regularly
encountered in engineering applications. Commonly these uncertainties are
accounted for using random fields and processes which require knowledge about
the appearing probability distributions functions which is not readily
available. In these cases non-probabilistic approaches such as interval
analysis and fuzzy set theory are helpful uncertainty measures. Partial
differential equations involving fuzzy and interval fields are traditionally
solved using the finite element method where the input fields are sampled using
some basis function expansion methods. This approach however is problematic, as
it is reliant on knowledge about the spatial correlation fields. In this work
we utilize physics-informed neural networks (PINNs) to solve interval and fuzzy
partial differential equations. The resulting network structures termed
interval physics-informed neural networks (iPINNs) and fuzzy physics-informed
neural networks (fPINNs) show promising results for obtaining bounded solutions
of equations involving spatially uncertain parameter fields. In contrast to
finite element approaches, no correlation length specification of the input
fields as well as no averaging via Monte-Carlo simulations are necessary. In
fact, information about the input interval fields is obtained directly as a
byproduct of the presented solution scheme. Furthermore, all major advantages
of PINNs are retained, i.e. meshfree nature of the scheme, and ease of inverse
problem set-up.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Fuhg_J/0/1/0/all/0/1"&gt;Jan Niklas Fuhg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fau_A/0/1/0/all/0/1"&gt;Am&amp;#xe9;lie Fau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bouklas_N/0/1/0/all/0/1"&gt;Nikolaos Bouklas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-player Multi-armed Bandits with Collision-Dependent Reward Distributions. (arXiv:2106.13669v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.13669</id>
        <link href="http://arxiv.org/abs/2106.13669"/>
        <updated>2021-06-28T01:57:57.891Z</updated>
        <summary type="html"><![CDATA[We study a new stochastic multi-player multi-armed bandits (MP-MAB) problem,
where the reward distribution changes if a collision occurs on the arm.
Existing literature always assumes a zero reward for involved players if
collision happens, but for applications such as cognitive radio, the more
realistic scenario is that collision reduces the mean reward but not
necessarily to zero. We focus on the more practical no-sensing setting where
players do not perceive collisions directly, and propose the Error-Correction
Collision Communication (EC3) algorithm that models implicit communication as a
reliable communication over noisy channel problem, for which random coding
error exponent is used to establish the optimal regret that no communication
protocol can beat. Finally, optimizing the tradeoff between code length and
decoding error rate leads to a regret that approaches the centralized MP-MAB
regret, which represents a natural lower bound. Experiments with practical
error-correction codes on both synthetic and real-world datasets demonstrate
the superiority of EC3. In particular, the results show that the choice of
coding schemes has a profound impact on the regret performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chengshuai Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Cong Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DAEMON: Dataset-Agnostic Explainable Malware Classification Using Multi-Stage Feature Mining. (arXiv:2008.01855v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01855</id>
        <link href="http://arxiv.org/abs/2008.01855"/>
        <updated>2021-06-28T01:57:57.885Z</updated>
        <summary type="html"><![CDATA[Numerous metamorphic and polymorphic malicious variants are generated
automatically on a daily basis by mutation engines that transform the code of a
malicious program while retaining its functionality, in order to evade
signature-based detection. These automatic processes have greatly increased the
number of malware variants, deeming their fully-manual analysis impossible.
Malware classification is the task of determining to which family a new
malicious variant belongs. Variants of the same malware family show similar
behavioral patterns. Thus, classifying newly discovered malicious programs and
applications helps assess the risks they pose. Moreover, malware classification
facilitates determining which of the newly discovered variants should undergo
manual analysis by a security expert, in order to determine whether they belong
to a new family (e.g., one whose members exploit a zero-day vulnerability) or
are simply the result of a concept drift within a known malicious family. This
motivated intense research in recent years on devising high-accuracy automatic
tools for malware classification. In this work, we present DAEMON - a novel
dataset-agnostic malware classifier. A key property of DAEMON is that the type
of features it uses and the manner in which they are mined facilitate
understanding the distinctive behavior of malware families, making its
classification decisions explainable. We've optimized DAEMON using a
large-scale dataset of x86 binaries, belonging to a mix of several malware
families targeting computers running Windows. We then re-trained it and applied
it, without any algorithmic change, feature re-engineering or parameter tuning,
to two other large-scale datasets of malicious Android applications consisting
of numerous malware families. DAEMON obtained highly accurate classification
results on all datasets, establishing that it is also platform-agnostic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korine_R/0/1/0/all/0/1"&gt;Ron Korine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hendler_D/0/1/0/all/0/1"&gt;Danny Hendler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Learning-based Observers for Unknown Nonlinear Systems using Bayesian Optimization. (arXiv:2005.05888v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05888</id>
        <link href="http://arxiv.org/abs/2005.05888"/>
        <updated>2021-06-28T01:57:57.879Z</updated>
        <summary type="html"><![CDATA[Data generated from dynamical systems with unknown dynamics enable the
learning of state observers that are: robust to modeling error, computationally
tractable to design, and capable of operating with guaranteed performance. In
this paper, a modular design methodology is formulated, that consists of three
design phases: (i) an initial robust observer design that enables one to learn
the dynamics without allowing the state estimation error to diverge (hence,
safe); (ii) a learning phase wherein the unmodeled components are estimated
using Bayesian optimization and Gaussian processes; and, (iii) a re-design
phase that leverages the learned dynamics to improve convergence rate of the
state estimation error. The potential of our proposed learning-based observer
is demonstrated on a benchmark nonlinear system. Additionally, certificates of
guaranteed estimation performance are provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chakrabarty_A/0/1/0/all/0/1"&gt;Ankush Chakrabarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benosman_M/0/1/0/all/0/1"&gt;Mouhacine Benosman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single Image Texture Translation for Data Augmentation. (arXiv:2106.13804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13804</id>
        <link href="http://arxiv.org/abs/2106.13804"/>
        <updated>2021-06-28T01:57:57.872Z</updated>
        <summary type="html"><![CDATA[Recent advances in image synthesis enables one to translate images by
learning the mapping between a source domain and a target domain. Existing
methods tend to learn the distributions by training a model on a variety of
datasets, with results evaluated largely in a subjective manner. Relatively few
works in this area, however, study the potential use of semantic image
translation methods for image recognition tasks. In this paper, we explore the
use of Single Image Texture Translation (SITT) for data augmentation. We first
propose a lightweight model for translating texture to images based on a single
input of source texture, allowing for fast training and testing. Based on SITT,
we then explore the use of augmented data in long-tailed and few-shot image
classification tasks. We find the proposed method is capable of translating
input data into a target domain, leading to consistent improved image
recognition performance. Finally, we examine how SITT and related image
translation methods can provide a basis for a data-efficient, augmentation
engineering approach to model training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tsung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1"&gt;Serge Belongie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VEGN: Variant Effect Prediction with Graph Neural Networks. (arXiv:2106.13642v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13642</id>
        <link href="http://arxiv.org/abs/2106.13642"/>
        <updated>2021-06-28T01:57:57.866Z</updated>
        <summary type="html"><![CDATA[Genetic mutations can cause disease by disrupting normal gene function.
Identifying the disease-causing mutations from millions of genetic variants
within an individual patient is a challenging problem. Computational methods
which can prioritize disease-causing mutations have, therefore, enormous
applications. It is well-known that genes function through a complex regulatory
network. However, existing variant effect prediction models only consider a
variant in isolation. In contrast, we propose VEGN, which models variant effect
prediction using a graph neural network (GNN) that operates on a heterogeneous
graph with genes and variants. The graph is created by assigning variants to
genes and connecting genes with an gene-gene interaction network. In this
context, we explore an approach where a gene-gene graph is given and another
where VEGN learns the gene-gene graph and therefore operates both on given and
learnt edges. The graph neural network is trained to aggregate information
between genes, and between genes and variants. Variants can exchange
information via the genes they connect to. This approach improves the
performance of existing state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1"&gt;Carolin Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1"&gt;Mathias Niepert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranger21: a synergistic deep learning optimizer. (arXiv:2106.13731v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13731</id>
        <link href="http://arxiv.org/abs/2106.13731"/>
        <updated>2021-06-28T01:57:57.858Z</updated>
        <summary type="html"><![CDATA[As optimizers are critical to the performances of neural networks, every year
a large number of papers innovating on the subject are published. However,
while most of these publications provide incremental improvements to existing
algorithms, they tend to be presented as new optimizers rather than composable
algorithms. Thus, many worthwhile improvements are rarely seen out of their
initial publication. Taking advantage of this untapped potential, we introduce
Ranger21, a new optimizer which combines AdamW with eight components, carefully
selected after reviewing and testing ideas from the literature. We found that
the resulting optimizer provides significantly improved validation accuracy and
training speed, smoother training curves, and is even able to train a ResNet50
on ImageNet2012 without Batch Normalization layers. A problem on which AdamW
stays systematically stuck in a bad initial state.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wright_L/0/1/0/all/0/1"&gt;Less Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demeure_N/0/1/0/all/0/1"&gt;Nestor Demeure&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transient Stability Analysis with Physics-Informed Neural Networks. (arXiv:2106.13638v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13638</id>
        <link href="http://arxiv.org/abs/2106.13638"/>
        <updated>2021-06-28T01:57:57.842Z</updated>
        <summary type="html"><![CDATA[Solving the ordinary differential equations that govern the power system is
an indispensable part in transient stability analysis. However, the
traditionally applied methods either carry a significant computational burden,
require model simplifications, or use overly conservative surrogate models.
Neural networks can circumvent these limitations but are faced with high
demands on the used datasets. Furthermore, they are agnostic to the underlying
governing equations. Physics-informed neural network tackle this problem and we
explore their advantages and challenges in this paper. We illustrate the
findings on the Kundur two-area system and highlight possible pathways forward
in developing this method further.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stiasny_J/0/1/0/all/0/1"&gt;Jochen Stiasny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misyris_G/0/1/0/all/0/1"&gt;Georgios S. Misyris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1"&gt;Spyros Chatzivasileiadis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Neural Networks: Essentials. (arXiv:2106.13594v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13594</id>
        <link href="http://arxiv.org/abs/2106.13594"/>
        <updated>2021-06-28T01:57:57.835Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks utilize probabilistic layers that capture
uncertainty over weights and activations, and are trained using Bayesian
inference. Since these probabilistic layers are designed to be drop-in
replacement of their deterministic counter parts, Bayesian neural networks
provide a direct and natural way to extend conventional deep neural networks to
support probabilistic deep learning. However, it is nontrivial to understand,
design and train Bayesian neural networks due to their complexities. We discuss
the essentials of Bayesian neural networks including duality (deep neural
networks, probabilistic models), approximate Bayesian inference, Bayesian
priors, Bayesian posteriors, and deep variational learning. We use TensorFlow
Probability APIs and code examples for illustration. The main problem with
Bayesian neural networks is that the architecture of deep neural networks makes
it quite redundant, and costly, to account for uncertainty for a large number
of successive layers. Hybrid Bayesian neural networks, which use few
probabilistic layers judicially positioned in the networks, provide a practical
solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Daniel T. Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conjugate Energy-Based Models. (arXiv:2106.13798v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13798</id>
        <link href="http://arxiv.org/abs/2106.13798"/>
        <updated>2021-06-28T01:57:57.828Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose conjugate energy-based models (CEBMs), a new class
of energy-based models that define a joint density over data and latent
variables. The joint density of a CEBM decomposes into an intractable
distribution over data and a tractable posterior over latent variables. CEBMs
have similar use cases as variational autoencoders, in the sense that they
learn an unsupervised mapping from data to latent variables. However, these
models omit a generator network, which allows them to learn more flexible
notions of similarity between data points. Our experiments demonstrate that
conjugate EBMs achieve competitive results in terms of image modelling,
predictive power of latent space, and out-of-domain detection on a variety of
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmaeili_B/0/1/0/all/0/1"&gt;Babak Esmaeili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wick_M/0/1/0/all/0/1"&gt;Michael Wick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tristan_J/0/1/0/all/0/1"&gt;Jean-Baptiste Tristan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1"&gt;Jan-Willem van de Meent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Connecting Sphere Manifolds Hierarchically for Regularization. (arXiv:2106.13549v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13549</id>
        <link href="http://arxiv.org/abs/2106.13549"/>
        <updated>2021-06-28T01:57:57.814Z</updated>
        <summary type="html"><![CDATA[This paper considers classification problems with hierarchically organized
classes. We force the classifier (hyperplane) of each class to belong to a
sphere manifold, whose center is the classifier of its super-class. Then,
individual sphere manifolds are connected based on their hierarchical
relations. Our technique replaces the last layer of a neural network by
combining a spherical fully-connected layer with a hierarchical layer. This
regularization is shown to improve the performance of widely used deep neural
network architectures (ResNet and DenseNet) on publicly available datasets
(CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scieur_D/0/1/0/all/0/1"&gt;Damien Scieur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngsung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Clipping for Federated Learning: Convergence and Client-Level Differential Privacy. (arXiv:2106.13673v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13673</id>
        <link href="http://arxiv.org/abs/2106.13673"/>
        <updated>2021-06-28T01:57:57.807Z</updated>
        <summary type="html"><![CDATA[Providing privacy protection has been one of the primary motivations of
Federated Learning (FL). Recently, there has been a line of work on
incorporating the formal privacy notion of differential privacy with FL. To
guarantee the client-level differential privacy in FL algorithms, the clients'
transmitted model updates have to be clipped before adding privacy noise. Such
clipping operation is substantially different from its counterpart of gradient
clipping in the centralized differentially private SGD and has not been
well-understood. In this paper, we first empirically demonstrate that the
clipped FedAvg can perform surprisingly well even with substantial data
heterogeneity when training neural networks, which is partly because the
clients' updates become similar for several popular deep architectures. Based
on this key observation, we provide the convergence analysis of a differential
private (DP) FedAvg algorithm and highlight the relationship between clipping
bias and the distribution of the clients' updates. To the best of our
knowledge, this is the first work that rigorously investigates theoretical and
empirical issues regarding the clipping operation in FL algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1"&gt;Mingyi Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jinfeng Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Residual Echo Suppression with A Tunable Tradeoff Between Signal Distortion and Echo Suppression. (arXiv:2106.13531v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13531</id>
        <link href="http://arxiv.org/abs/2106.13531"/>
        <updated>2021-06-28T01:57:57.800Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a residual echo suppression method using a UNet
neural network that directly maps the outputs of a linear acoustic echo
canceler to the desired signal in the spectral domain. This system embeds a
design parameter that allows a tunable tradeoff between the desired-signal
distortion and residual echo suppression in double-talk scenarios. The system
employs 136 thousand parameters, and requires 1.6 Giga floating-point
operations per second and 10 Mega-bytes of memory. The implementation satisfies
both the timing requirements of the AEC challenge and the computational and
memory limitations of on-device applications. Experiments are conducted with
161~h of data from the AEC challenge database and from real independent
recordings. We demonstrate the performance of the proposed system in real-life
conditions and compare it with two competing methods regarding echo suppression
and desired-signal distortion, generalization to various environments, and
robustness to high echo levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1"&gt;Amir Ivry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Israel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1"&gt;Baruch Berdugo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privileged Zero-Shot AutoML. (arXiv:2106.13743v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13743</id>
        <link href="http://arxiv.org/abs/2106.13743"/>
        <updated>2021-06-28T01:57:57.792Z</updated>
        <summary type="html"><![CDATA[This work improves the quality of automated machine learning (AutoML) systems
by using dataset and function descriptions while significantly decreasing
computation time from minutes to milliseconds by using a zero-shot approach.
Given a new dataset and a well-defined machine learning task, humans begin by
reading a description of the dataset and documentation for the algorithms to be
used. This work is the first to use these textual descriptions, which we call
privileged information, for AutoML. We use a pre-trained Transformer model to
process the privileged text and demonstrate that using this information
improves AutoML performance. Thus, our approach leverages the progress of
unsupervised representation learning in natural language processing to provide
a significant boost to AutoML. We demonstrate that using only textual
descriptions of the data and functions achieves reasonable classification
performance, and adding textual descriptions to data meta-features improves
classification across tabular datasets. To achieve zero-shot AutoML we train a
graph neural network with these description embeddings and the data
meta-features. Each node represents a training dataset, which we use to predict
the best machine learning pipeline for a new test dataset in a zero-shot
fashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a
supervised learning task and dataset. In contrast, most AutoML systems require
tens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces
running and prediction times from minutes to milliseconds, consistently across
datasets. By speeding up AutoML by orders of magnitude this work demonstrates
real-time AutoML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Nikhil Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kates_B/0/1/0/all/0/1"&gt;Brandon Kates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1"&gt;Jeff Mentch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharkar_A/0/1/0/all/0/1"&gt;Anant Kharkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1"&gt;Madeleine Udell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1"&gt;Iddo Drori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data efficiency in graph networks through equivariance. (arXiv:2106.13786v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13786</id>
        <link href="http://arxiv.org/abs/2106.13786"/>
        <updated>2021-06-28T01:57:57.784Z</updated>
        <summary type="html"><![CDATA[We introduce a novel architecture for graph networks which is equivariant to
any transformation in the coordinate embeddings that preserves the distance
between neighbouring nodes. In particular, it is equivariant to the Euclidean
and conformal orthogonal groups in $n$-dimensions. Thanks to its equivariance
properties, the proposed model is extremely more data efficient with respect to
classical graph architectures and also intrinsically equipped with a better
inductive bias. We show that, learning on a minimal amount of data, the
architecture we propose can perfectly generalise to unseen data in a synthetic
problem, while much more training data are required from a standard model to
reach comparable performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1"&gt;Francesco Farina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1"&gt;Emma Slade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Coupled Topic Modeling over Sequential Documents. (arXiv:2106.13732v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13732</id>
        <link href="http://arxiv.org/abs/2106.13732"/>
        <updated>2021-06-28T01:57:57.771Z</updated>
        <summary type="html"><![CDATA[The abundant sequential documents such as online archival, social media and
news feeds are streamingly updated, where each chunk of documents is
incorporated with smoothly evolving yet dependent topics. Such digital texts
have attracted extensive research on dynamic topic modeling to infer hidden
evolving topics and their temporal dependencies. However, most of the existing
approaches focus on single-topic-thread evolution and ignore the fact that a
current topic may be coupled with multiple relevant prior topics. In addition,
these approaches also incur the intractable inference problem when inferring
latent parameters, resulting in a high computational cost and performance
degradation. In this work, we assume that a current topic evolves from all
prior topics with corresponding coupling weights, forming the
multi-topic-thread evolution. Our method models the dependencies between
evolving topics and thoroughly encodes their complex multi-couplings across
time steps. To conquer the intractable inference challenge, a new solution with
a set of novel data augmentation techniques is proposed, which successfully
discomposes the multi-couplings between evolving topics. A fully conjugate
model is thus obtained to guarantee the effectiveness and efficiency of the
inference technique. A novel Gibbs sampler with a backward-forward filter
algorithm efficiently learns latent timeevolving parameters in a closed-form.
In addition, the latent Indian Buffet Process (IBP) compound distribution is
exploited to automatically infer the overall topic number and customize the
sparse topic proportions for each sequential document without bias. The
proposed method is evaluated on both synthetic and real-world datasets against
the competitive baselines, demonstrating its superiority over the baselines in
terms of the low per-word perplexity, high coherent topics, and better document
time prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jinjin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zhiguo Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Domain Active Learning: A Comparative Study. (arXiv:2106.13516v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13516</id>
        <link href="http://arxiv.org/abs/2106.13516"/>
        <updated>2021-06-28T01:57:57.760Z</updated>
        <summary type="html"><![CDATA[Building classifiers on multiple domains is a practical problem in the real
life. Instead of building classifiers one by one, multi-domain learning (MDL)
simultaneously builds classifiers on multiple domains. MDL utilizes the
information shared among the domains to improve the performance. As a
supervised learning problem, the labeling effort is still high in MDL problems.
Usually, this high labeling cost issue could be relieved by using active
learning. Thus, it is natural to utilize active learning to reduce the labeling
effort in MDL, and we refer this setting as multi-domain active learning
(MDAL). However, there are only few works which are built on this setting. And
when the researches have to face this problem, there is no off-the-shelf
solutions. Under this circumstance, combining the current multi-domain learning
models and single-domain active learning strategies might be a preliminary
solution for MDAL problem. To find out the potential of this preliminary
solution, a comparative study over 5 models and 4 selection strategies is made
in this paper. To the best of our knowledge, this is the first work provides
the formal definition of MDAL. Besides, this is the first comparative work for
MDAL problem. From the results, the Multinomial Adversarial Networks (MAN)
model with a simple best vs second best (BvSB) uncertainty strategy shows its
superiority in most cases. We take this combination as our off-the-shelf
recommendation for the MDAL problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Rui He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Ke Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Learning and Data Mining to Leverage Community Knowledge for the Engineering of Stable Metal-Organic Frameworks. (arXiv:2106.13327v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2106.13327</id>
        <link href="http://arxiv.org/abs/2106.13327"/>
        <updated>2021-06-28T01:57:57.470Z</updated>
        <summary type="html"><![CDATA[Although the tailored metal active sites and porous architectures of MOFs
hold great promise for engineering challenges ranging from gas separations to
catalysis, a lack of understanding of how to improve their stability limits
their use in practice. To overcome this limitation, we extract thousands of
published reports of the key aspects of MOF stability necessary for their
practical application: the ability to withstand high temperatures without
degrading and the capacity to be activated by removal of solvent molecules.
From nearly 4,000 manuscripts, we use natural language processing and automated
image analysis to obtain over 2,000 solvent-removal stability measures and
3,000 thermal degradation temperatures. We analyze the relationships between
stability properties and the chemical and geometric structures in this set to
identify limits of prior heuristics derived from smaller sets of MOFs. By
training predictive machine learning (ML, i.e., Gaussian process and artificial
neural network) models to encode the structure-property relationships with
graph- and pore-structure-based representations, we are able to make
predictions of stability orders of magnitude faster than conventional
physics-based modeling or experiment. Interpretation of important features in
ML models provides insights that we use to identify strategies to engineer
increased stability into typically unstable 3d-containing MOFs that are
frequently targeted for catalytic applications. We expect our approach to
accelerate the time to discovery of stable, practical MOF materials for a wide
range of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Nandy_A/0/1/0/all/0/1"&gt;Aditya Nandy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenru Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1"&gt;Heather J. Kulik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Recurrent Neural Network for Multistep Time Series Forecasting. (arXiv:2104.12311v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12311</id>
        <link href="http://arxiv.org/abs/2104.12311"/>
        <updated>2021-06-28T01:57:57.449Z</updated>
        <summary type="html"><![CDATA[Time series forecasting based on deep architectures has been gaining
popularity in recent years due to their ability to model complex non-linear
temporal dynamics. The recurrent neural network is one such model capable of
handling variable-length input and output. In this paper, we leverage recent
advances in deep generative models and the concept of state space models to
propose a stochastic adaptation of the recurrent neural network for
multistep-ahead time series forecasting, which is trained with stochastic
gradient variational Bayes. In our model design, the transition function of the
recurrent neural network, which determines the evolution of the hidden states,
is stochastic rather than deterministic as in a regular recurrent neural
network; this is achieved by incorporating a latent random variable into the
transition process which captures the stochasticity of the temporal dynamics.
Our model preserves the architectural workings of a recurrent neural network
for which all relevant information is encapsulated in its hidden states, and
this flexibility allows our model to be easily integrated into any deep
architecture for sequential modelling. We test our model on a wide range of
datasets from finance to healthcare; results show that the stochastic recurrent
neural network consistently outperforms its deterministic counterpart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zexuan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barucca_P/0/1/0/all/0/1"&gt;Paolo Barucca&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Geolocation Prediction of Tweets with Human Machine Collaboration. (arXiv:2106.13411v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13411</id>
        <link href="http://arxiv.org/abs/2106.13411"/>
        <updated>2021-06-28T01:57:57.431Z</updated>
        <summary type="html"><![CDATA[Twitter is a useful resource to analyze peoples' opinions on various topics.
Often these topics are correlated or associated with locations from where these
Tweet posts are made. For example, restaurant owners may need to know where
their target customers eat with respect to the sentiment of the posts made
related to food, policy planners may need to analyze citizens' opinion on
relevant issues such as crime, safety, congestion, etc. with respect to
specific parts of the city, or county or state. As promising as this is, less
than $1\%$ of the crawled Tweet posts come with geolocation tags. That makes
accurate prediction of Tweet posts for the non geo-tagged tweets very critical
to analyze data in various domains. In this research, we utilized millions of
Twitter posts and end-users domain expertise to build a set of deep neural
network models using natural language processing (NLP) techniques, that
predicts the geolocation of non geo-tagged Tweet posts at various level of
granularities such as neighborhood, zipcode, and longitude with latitudes. With
multiple neural architecture experiments, and a collaborative human-machine
workflow design, our ongoing work on geolocation detection shows promising
results that empower end-users to correlate relationship between variables of
choice with the location information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_F/0/1/0/all/0/1"&gt;Florina Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Subhajit Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications. (arXiv:2106.07268v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07268</id>
        <link href="http://arxiv.org/abs/2106.07268"/>
        <updated>2021-06-28T01:57:57.407Z</updated>
        <summary type="html"><![CDATA[Various incremental learning (IL) approaches have been proposed to help deep
learning models learn new tasks/classes continuously without forgetting what
was learned previously (i.e., avoid catastrophic forgetting). With the growing
number of deployed audio sensing applications that need to dynamically
incorporate new tasks and changing input distribution from users, the ability
of IL on-device becomes essential for both efficiency and user privacy.

However, prior works suffer from high computational costs and storage demands
which hinders the deployment of IL on-device. In this work, to overcome these
limitations, we develop an end-to-end and on-device IL framework, FastICARL,
that incorporates an exemplar-based IL and quantization in the context of
audio-based applications. We first employ k-nearest-neighbor to reduce the
latency of IL. Then, we jointly utilize a quantization technique to decrease
the storage requirements of IL. We implement FastICARL on two types of mobile
devices and demonstrate that FastICARL remarkably decreases the IL time up to
78-92% and the storage requirements by 2-4 times without sacrificing its
performance. FastICARL enables complete on-device IL, ensuring user privacy as
the user data does not need to leave the device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1"&gt;Young D. Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1"&gt;Jagmohan Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1"&gt;Cecilia Mascolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Linear Temporal Properties from Noisy Data: A MaxSAT Approach. (arXiv:2104.15083v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.15083</id>
        <link href="http://arxiv.org/abs/2104.15083"/>
        <updated>2021-06-28T01:57:57.400Z</updated>
        <summary type="html"><![CDATA[We address the problem of inferring descriptions of system behavior using
Linear Temporal Logic (LTL) from a finite set of positive and negative
examples. Most of the existing approaches for solving such a task rely on
predefined templates for guiding the structure of the inferred formula. The
approaches that can infer arbitrary LTL formulas, on the other hand, are not
robust to noise in the data. To alleviate such limitations, we devise two
algorithms for inferring concise LTL formulas even in the presence of noise.
Our first algorithm infers minimal LTL formulas by reducing the inference
problem to a problem in maximum satisfiability and then using off-the-shelf
MaxSAT solvers to find a solution. To the best of our knowledge, we are the
first to incorporate the usage of MaxSAT solvers for inferring formulas in LTL.
Our second learning algorithm relies on the first algorithm to derive a
decision tree over LTL formulas based on a decision tree learning algorithm. We
have implemented both our algorithms and verified that our algorithms are
efficient in extracting concise LTL descriptions even in the presence of noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gaglione_J/0/1/0/all/0/1"&gt;Jean-Rapha&amp;#xeb;l Gaglione&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1"&gt;Daniel Neider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rajarshi Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor-based framework for training flexible neural networks. (arXiv:2106.13542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13542</id>
        <link href="http://arxiv.org/abs/2106.13542"/>
        <updated>2021-06-28T01:57:57.385Z</updated>
        <summary type="html"><![CDATA[Activation functions (AFs) are an important part of the design of neural
networks (NNs), and their choice plays a predominant role in the performance of
a NN. In this work, we are particularly interested in the estimation of
flexible activation functions using tensor-based solutions, where the AFs are
expressed as a weighted sum of predefined basis functions. To do so, we propose
a new learning algorithm which solves a constrained coupled matrix-tensor
factorization (CMTF) problem. This technique fuses the first and zeroth order
information of the NN, where the first-order information is contained in a
Jacobian tensor, following a constrained canonical polyadic decomposition
(CPD). The proposed algorithm can handle different decomposition bases. The
goal of this method is to compress large pretrained NN models, by replacing
subnetworks, {\em i.e.,} one or multiple layers of the original network, by a
new flexible layer. The approach is applied to a pretrained convolutional
neural network (CNN) used for character classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zniyed_Y/0/1/0/all/0/1"&gt;Yassine Zniyed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usevich_K/0/1/0/all/0/1"&gt;Konstantin Usevich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miron_S/0/1/0/all/0/1"&gt;Sebastian Miron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brie_D/0/1/0/all/0/1"&gt;David Brie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sketching Datasets for Large-Scale Learning (long version). (arXiv:2008.01839v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01839</id>
        <link href="http://arxiv.org/abs/2008.01839"/>
        <updated>2021-06-28T01:57:57.360Z</updated>
        <summary type="html"><![CDATA[This article considers "compressive learning," an approach to large-scale
machine learning where datasets are massively compressed before learning (e.g.,
clustering, classification, or regression) is performed. In particular, a
"sketch" is first constructed by computing carefully chosen nonlinear random
features (e.g., random Fourier features) and averaging them over the whole
dataset. Parameters are then learned from the sketch, without access to the
original dataset. This article surveys the current state-of-the-art in
compressive learning, including the main concepts and algorithms, their
connections with established signal-processing methods, existing theoretical
guarantees -- on both information preservation and privacy preservation, and
important open problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chatalic_A/0/1/0/all/0/1"&gt;Antoine Chatalic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1"&gt;Nicolas Keriven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schellekens_V/0/1/0/all/0/1"&gt;Vincent Schellekens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jacques_L/0/1/0/all/0/1"&gt;Laurent Jacques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schniter_P/0/1/0/all/0/1"&gt;Philip Schniter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning in Bandits with Latent Continuity. (arXiv:2102.02472v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02472</id>
        <link href="http://arxiv.org/abs/2102.02472"/>
        <updated>2021-06-28T01:57:57.354Z</updated>
        <summary type="html"><![CDATA[Structured stochastic multi-armed bandits provide accelerated regret rates
over the standard unstructured bandit problems. Most structured bandits,
however, assume the knowledge of the structural parameter such as Lipschitz
continuity, which is often not available. To cope with the latent structural
parameter, we consider a transfer learning setting in which an agent must learn
to transfer the structural information from the prior tasks to the next task,
which is inspired by practical problems such as rate adaptation in wireless
link. We propose a novel framework to provably and accurately estimate the
Lipschitz constant based on previous tasks and fully exploit it for the new
task at hand. We analyze the efficiency of the proposed framework in two folds:
(i) the sample complexity of our estimator matches with the
information-theoretic fundamental limit; and (ii) our regret bound on the new
task is close to that of the oracle algorithm with the full knowledge of the
Lipschitz constant under mild assumptions. Our analysis reveals a set of useful
insights on transfer learning for latent Lipschitzconstants such as the
fundamental challenge a learner faces. Our numerical evaluations confirm our
theoretical findings and show the superiority of the proposed framework
compared to baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyejin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1"&gt;Seiyun Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_K/0/1/0/all/0/1"&gt;Kwang-Sung Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ok_J/0/1/0/all/0/1"&gt;Jungseul Ok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dealing with Expert Bias in Collective Decision-Making. (arXiv:2106.13539v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.13539</id>
        <link href="http://arxiv.org/abs/2106.13539"/>
        <updated>2021-06-28T01:57:57.348Z</updated>
        <summary type="html"><![CDATA[Quite some real-world problems can be formulated as decision-making problems
wherein one must repeatedly make an appropriate choice from a set of
alternatives. Expert judgements, whether human or artificial, can help in
taking correct decisions, especially when exploration of alternative solutions
is costly. As expert opinions might deviate, the problem of finding the right
alternative can be approached as a collective decision making problem (CDM).
Current state-of-the-art approaches to solve CDM are limited by the quality of
the best expert in the group, and perform poorly if experts are not qualified
or if they are overly biased, thus potentially derailing the decision-making
process. In this paper, we propose a new algorithmic approach based on
contextual multi-armed bandit problems (CMAB) to identify and counteract such
biased expertises. We explore homogeneous, heterogeneous and polarised expert
groups and show that this approach is able to effectively exploit the
collective expertise, irrespective of whether the provided advice is directly
conducive to good performance, outperforming state-of-the-art methods,
especially when the quality of the provided expertise degrades. Our novel
CMAB-inspired approach achieves a higher final performance and does so while
converging more rapidly than previous adaptive algorithms, especially when
heterogeneous expertise is readily available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abels_A/0/1/0/all/0/1"&gt;Axel Abels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenaerts_T/0/1/0/all/0/1"&gt;Tom Lenaerts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trianni_V/0/1/0/all/0/1"&gt;Vito Trianni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1"&gt;Ann Now&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio Attacks and Defenses against AED Systems -- A Practical Study. (arXiv:2106.07428v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07428</id>
        <link href="http://arxiv.org/abs/2106.07428"/>
        <updated>2021-06-28T01:57:57.340Z</updated>
        <summary type="html"><![CDATA[Audio Event Detection (AED) Systems capture audio from the environment and
employ some deep learning algorithms for detecting the presence of a specific
sound of interest. In this paper, we evaluate deep learning-based AED systems
against evasion attacks through adversarial examples. We run multiple security
critical AED tasks, implemented as CNNs classifiers, and then generate audio
adversarial examples using two different types of noise, namely background and
white noise, that can be used by the adversary to evade detection. We also
examine the robustness of existing third-party AED capable devices, such as
Nest devices manufactured by Google, which run their own black-box deep
learning models.

We show that an adversary can focus on audio adversarial inputs to cause AED
systems to misclassify, similarly to what has been previously done by works
focusing on adversarial examples from the image domain. We then, seek to
improve classifiers' robustness through countermeasures to the attacks. We
employ adversarial training and a custom denoising technique. We show that
these countermeasures, when applied to audio input, can be successful, either
in isolation or in combination, generating relevant increases of nearly fifty
percent in the performance of the classifiers when these are under attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santos_R/0/1/0/all/0/1"&gt;Rodrigo dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1"&gt;Shirin Nilizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A mechanistic-based data-driven approach to accelerate structural topology optimization through finite element convolutional neural network (FE-CNN). (arXiv:2106.13652v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13652</id>
        <link href="http://arxiv.org/abs/2106.13652"/>
        <updated>2021-06-28T01:57:57.334Z</updated>
        <summary type="html"><![CDATA[In this paper, a mechanistic data-driven approach is proposed to accelerate
structural topology optimization, employing an in-house developed finite
element convolutional neural network (FE-CNN). Our approach can be divided into
two stages: offline training, and online optimization. During offline training,
a mapping function is built between high and low resolution representations of
a given design domain. The mapping is expressed by a FE-CNN, which targets a
common objective function value (e.g., structural compliance) across design
domains of differing resolutions. During online optimization, an arbitrary
design domain of high resolution is reduced to low resolution through the
trained mapping function. The original high-resolution domain is thus designed
by computations performed on only the low-resolution version, followed by an
inverse mapping back to the high-resolution domain. Numerical examples
demonstrate that this approach can accelerate optimization by up to an order of
magnitude in computational time. Our proposed approach therefore shows great
potential to overcome the curse-of-dimensionality incurred by density-based
structural topology optimization. The limitation of our present approach is
also discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_T/0/1/0/all/0/1"&gt;Tianle Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zongliang Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elkhodary_K/0/1/0/all/0/1"&gt;Khalil I. Elkhodary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A proximal-proximal majorization-minimization algorithm for nonconvex tuning-free robust regression problems. (arXiv:2106.13683v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.13683</id>
        <link href="http://arxiv.org/abs/2106.13683"/>
        <updated>2021-06-28T01:57:57.317Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a proximal-proximal majorization-minimization
(PPMM) algorithm for nonconvex tuning-free robust regression problems. The
basic idea is to apply the proximal majorization-minimization algorithm to
solve the nonconvex problem with the inner subproblems solved by a sparse
semismooth Newton (SSN) method based proximal point algorithm (PPA). We must
emphasize that the main difficulty in the design of the algorithm lies in how
to overcome the singular difficulty of the inner subproblem. Furthermore, we
also prove that the PPMM algorithm converges to a d-stationary point. Due to
the Kurdyka-Lojasiewicz (KL) property of the problem, we present the
convergence rate of the PPMM algorithm. Numerical experiments demonstrate that
our proposed algorithm outperforms the existing state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Tang_P/0/1/0/all/0/1"&gt;Peipei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Graph Augmentation to Improve Graph Contrastive Learning. (arXiv:2106.05819v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05819</id>
        <link href="http://arxiv.org/abs/2106.05819"/>
        <updated>2021-06-28T01:57:57.310Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning of graph neural networks (GNN) is in great need
because of the widespread label scarcity issue in real-world graph/network
data. Graph contrastive learning (GCL), by training GNNs to maximize the
correspondence between the representations of the same graph in its different
augmented forms, may yield robust and transferable GNNs even without using
labels. However, GNNs trained by traditional GCL often risk capturing redundant
graph features and thus may be brittle and provide sub-par performance in
downstream tasks. Here, we propose a novel principle, termed adversarial-GCL
(AD-GCL), which enables GNNs to avoid capturing redundant information during
the training by optimizing adversarial graph augmentation strategies used in
GCL. We pair AD-GCL with theoretical explanations and design a practical
instantiation based on trainable edge-dropping graph augmentation. We
experimentally validate AD-GCL by comparing with the state-of-the-art GCL
methods and achieve performance gains of up-to $14\%$ in unsupervised, $6\%$ in
transfer, and $3\%$ in semi-supervised learning settings overall with 18
different benchmark datasets for the tasks of molecule property regression and
classification, and social network classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1"&gt;Susheel Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1"&gt;Cong Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1"&gt;Jennifer Neville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning with Multifidelity Modeling for Efficient Rare Event Simulation. (arXiv:2106.13790v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13790</id>
        <link href="http://arxiv.org/abs/2106.13790"/>
        <updated>2021-06-28T01:57:57.302Z</updated>
        <summary type="html"><![CDATA[While multifidelity modeling provides a cost-effective way to conduct
uncertainty quantification with computationally expensive models, much greater
efficiency can be achieved by adaptively deciding the number of required
high-fidelity (HF) simulations, depending on the type and complexity of the
problem and the desired accuracy in the results. We propose a framework for
active learning with multifidelity modeling emphasizing the efficient
estimation of rare events. Our framework works by fusing a low-fidelity (LF)
prediction with an HF-inferred correction, filtering the corrected LF
prediction to decide whether to call the high-fidelity model, and for enhanced
subsequent accuracy, adapting the correction for the LF prediction after every
HF model call. The framework does not make any assumptions as to the LF model
type or its correlations with the HF model. In addition, for improved
robustness when estimating smaller failure probabilities, we propose using
dynamic active learning functions that decide when to call the HF model. We
demonstrate our framework using several academic case studies and two finite
element (FE) model case studies: estimating Navier-Stokes velocities using the
Stokes approximation and estimating stresses in a transversely isotropic model
subjected to displacements via a coarsely meshed isotropic model. Across these
case studies, not only did the proposed framework estimate the failure
probabilities accurately, but compared with either Monte Carlo or a standard
variance reduction method, it also required only a small fraction of the calls
to the HF model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dhulipala_S/0/1/0/all/0/1"&gt;S. L. N. Dhulipala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shields_M/0/1/0/all/0/1"&gt;M. D. Shields&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spencer_B/0/1/0/all/0/1"&gt;B. W. Spencer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bolisetti_C/0/1/0/all/0/1"&gt;C. Bolisetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Slaughter_A/0/1/0/all/0/1"&gt;A. E. Slaughter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Laboure_V/0/1/0/all/0/1"&gt;V. M. Laboure&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chakroborty_P/0/1/0/all/0/1"&gt;P. Chakroborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Self-Learning Framework for Bladder Cancer Grading Using Histopathological Images. (arXiv:2106.13559v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13559</id>
        <link href="http://arxiv.org/abs/2106.13559"/>
        <updated>2021-06-28T01:57:57.294Z</updated>
        <summary type="html"><![CDATA[Recently, bladder cancer has been significantly increased in terms of
incidence and mortality. Currently, two subtypes are known based on tumour
growth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC).
In this work, we focus on the MIBC subtype because it is of the worst prognosis
and can spread to adjacent organs. We present a self-learning framework to
grade bladder cancer from histological images stained via immunohistochemical
techniques. Specifically, we propose a novel Deep Convolutional Embedded
Attention Clustering (DCEAC) which allows classifying histological patches into
different severity levels of the disease, according to the patterns established
in the literature. The proposed DCEAC model follows a two-step fully
unsupervised learning methodology to discern between non-tumour, mild and
infiltrative patterns from high-resolution samples of 512x512 pixels. Our
system outperforms previous clustering-based methods by including a
convolutional attention module, which allows refining the features of the
latent space before the classification stage. The proposed network exceeds
state-of-the-art approaches by 2-3% across different metrics, achieving a final
average accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported
class activation maps evidence that our model is able to learn by itself the
same patterns that clinicians consider relevant, without incurring prior
annotation steps. This fact supposes a breakthrough in muscle-invasive bladder
cancer grading which bridges the gap with respect to train the model on
labelled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1"&gt;Gabriel Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Esteve_A/0/1/0/all/0/1"&gt;Anna Esteve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramos_D/0/1/0/all/0/1"&gt;David Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Inertial Block Majorization Minimization Framework for Nonsmooth Nonconvex Optimization. (arXiv:2010.12133v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12133</id>
        <link href="http://arxiv.org/abs/2010.12133"/>
        <updated>2021-06-28T01:57:57.286Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce TITAN, a novel inerTIal block majorizaTion
minimizAtioN framework for non-smooth non-convex optimization problems. To the
best of our knowledge, TITAN is the first framework of block-coordinate update
method that relies on the majorization-minimization framework while embedding
inertial force to each step of the block updates. The inertial force is
obtained via an extrapolation operator that subsumes heavy-ball and
Nesterov-type accelerations for block proximal gradient methods as special
cases. By choosing various surrogate functions, such as proximal, Lipschitz
gradient, Bregman, quadratic, and composite surrogate functions, and by varying
the extrapolation operator, TITAN produces a rich set of inertial
block-coordinate update methods. We study sub-sequential convergence as well as
global convergence for the generated sequence of TITAN. We illustrate the
effectiveness of TITAN on two important machine learning problems, namely
sparse non-negative matrix factorization and matrix completion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hien_L/0/1/0/all/0/1"&gt;Le Thi Khanh Hien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Phan_D/0/1/0/all/0/1"&gt;Duy Nhat Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gillis_N/0/1/0/all/0/1"&gt;Nicolas Gillis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Counterfactual Explanations in Tree Ensembles. (arXiv:2106.06631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06631</id>
        <link href="http://arxiv.org/abs/2106.06631"/>
        <updated>2021-06-28T01:57:57.258Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations are usually generated through heuristics that are
sensitive to the search's initial conditions. The absence of guarantees of
performance and robustness hinders trustworthiness. In this paper, we take a
disciplined approach towards counterfactual explanations for tree ensembles. We
advocate for a model-based search aiming at "optimal" explanations and propose
efficient mixed-integer programming approaches. We show that isolation forests
can be modeled within our framework to focus the search on plausible
explanations with a low outlier score. We provide comprehensive coverage of
additional constraints that model important objectives, heterogeneous data
types, structural constraints on the feature space, along with resource and
actionability restrictions. Our experimental analyses demonstrate that the
proposed search approach requires a computational effort that is orders of
magnitude smaller than previous mathematical programming algorithms. It scales
up to large data sets and tree ensembles, where it provides, within seconds,
systematic explanations grounded on well-defined models solved to optimality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parmentier_A/0/1/0/all/0/1"&gt;Axel Parmentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_T/0/1/0/all/0/1"&gt;Thibaut Vidal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals. (arXiv:2106.13695v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13695</id>
        <link href="http://arxiv.org/abs/2106.13695"/>
        <updated>2021-06-28T01:57:57.252Z</updated>
        <summary type="html"><![CDATA[Data augmentation is a key element of deep learning pipelines, as it informs
the network during training about transformations of the input data that keep
the label unchanged. Manually finding adequate augmentation methods and
parameters for a given pipeline is however rapidly cumbersome. In particular,
while intuition can guide this decision for images, the design and choice of
augmentation policies remains unclear for more complex types of data, such as
neuroscience signals. Moreover, label independent strategies might not be
suitable for such structured data and class-dependent augmentations might be
necessary. This idea has been surprisingly unexplored in the literature, while
it is quite intuitive: changing the color of a car image does not change the
object class to be predicted, but doing the same to the picture of an orange
does. This paper aims to increase the generalization power added through
class-wise data augmentation. Yet, as seeking transformations depending on the
class largely increases the complexity of the task, using gradient-free
optimization techniques as done by most existing automatic approaches becomes
intractable for real-world datasets. For this reason we propose to use
differentiable data augmentation amenable to gradient-based learning. EEG
signals are a perfect example of data for which good augmentation policies are
mostly unknown. In this work, we demonstrate the relevance of our approach on
the clinically relevant sleep staging classification task, for which we also
propose differentiable transformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rommel_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Rommel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1"&gt;Thomas Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1"&gt;Alexandre Gramfort&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Self-Attentive Gated RNNs for Real-Time Speaker Separation. (arXiv:2106.13493v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.13493</id>
        <link href="http://arxiv.org/abs/2106.13493"/>
        <updated>2021-06-28T01:57:57.243Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have recently shown great success in the task of blind
source separation, both under monaural and binaural settings. Although these
methods were shown to produce high-quality separations, they were mainly
applied under offline settings, in which the model has access to the full input
signal while separating the signal. In this study, we convert a non-causal
state-of-the-art separation model into a causal and real-time model and
evaluate its performance under both online and offline settings. We compare the
performance of the proposed model to several baseline methods under anechoic,
noisy, and noisy-reverberant recording conditions while exploring both monaural
and binaural inputs and outputs. Our findings shed light on the relative
difference between causal and non-causal models when performing separation. Our
stateful implementation for online separation leads to a minor drop in
performance compared to the offline model; 0.8dB for monaural inputs and 0.3dB
for binaural inputs while reaching a real-time factor of 0.65. Samples can be
found under the following link:
https://kwanum.github.io/sagrnnc-stream-results/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kabeli_O/0/1/0/all/0/1"&gt;Ori Kabeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1"&gt;Yossi Adi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhenyu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1"&gt;Buye Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Anurag Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness and Generalization to Nearest Categories. (arXiv:2011.08485v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08485</id>
        <link href="http://arxiv.org/abs/2011.08485"/>
        <updated>2021-06-28T01:57:57.235Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness has emerged as a desirable property for neural
networks. Prior work shows that robust networks perform well in some
out-of-distribution generalization tasks, such as transfer learning and outlier
detection. We uncover a different kind of out-of-distribution generalization
property of such networks, and find that they also do well in a task that we
call nearest category generalization (NCG) - given an out-of-distribution
input, they tend to predict the same label as that of the closest training
example. We empirically show that this happens even when the
out-of-distribution inputs lie outside the robustness radius of the training
data, which suggests that these networks may generalize better along unseen
directions on the natural image manifold than arbitrary unseen directions. We
examine how performance changes when we change the robustness regions during
training. We then design experiments to investigate the connection between
out-of-distribution detection and nearest category generalization. Taken
together, our work provides evidence that robust neural networks may resemble
nearest neighbor classifiers in their behavior on out-of-distribution data. The
code is available at
https://github.com/yangarbiter/nearest-category-generalization]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yao-Yuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1"&gt;Cyrus Rashtchian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1"&gt;Kamalika Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised embedding of trajectories captures the latent structure of mobility. (arXiv:2012.02785v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02785</id>
        <link href="http://arxiv.org/abs/2012.02785"/>
        <updated>2021-06-28T01:57:57.225Z</updated>
        <summary type="html"><![CDATA[Human mobility drives major societal phenomena including epidemics,
economies, and innovation. Historically, mobility was constrained by geographic
distance, however, in the globalizing world, language, culture, and history are
increasingly important. We propose using the neural embedding model word2vec
for studying mobility and capturing its complexity. Word2ec is shown to be
mathematically equivalent to the gravity model of mobility, and using three
human trajectory datasets, we demonstrate that it encodes nuanced relationships
between locations into a vector-space, providing a measure of effective
distance that outperforms baselines. Focusing on the case of scientific
mobility, we show that embeddings uncover cultural, linguistic, and
hierarchical relationships at multiple levels of granularity. Connecting neural
embeddings to the gravity model opens up new avenues for the study of mobility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murray_D/0/1/0/all/0/1"&gt;Dakota Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jisung Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kojaku_S/0/1/0/all/0/1"&gt;Sadamori Kojaku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costas_R/0/1/0/all/0/1"&gt;Rodrigo Costas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1"&gt;Woo-Sung Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milojevic_S/0/1/0/all/0/1"&gt;Sta&amp;#x161;a Milojevi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1"&gt;Yong-Yeol Ahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-stationary Online Learning with Memory and Non-stochastic Control. (arXiv:2102.03758v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03758</id>
        <link href="http://arxiv.org/abs/2102.03758"/>
        <updated>2021-06-28T01:57:57.207Z</updated>
        <summary type="html"><![CDATA[We study the problem of Online Convex Optimization (OCO) with memory, which
allows loss functions to depend on past decisions and thus captures temporal
effects of learning problems. In this paper, we introduce dynamic policy regret
as the performance measure to design algorithms robust to non-stationary
environments, which competes algorithms' decisions with a sequence of changing
comparators. We propose a novel algorithm for OCO with memory that provably
enjoys an optimal dynamic policy regret. The key technical challenge is how to
control the switching cost, the cumulative movements of player's decisions,
which is neatly addressed by a novel decomposition of dynamic policy regret and
an appropriate meta-expert structure. Furthermore, we apply the results to the
problem of online non-stochastic control, i.e., controlling a linear dynamical
system with adversarial disturbance and convex loss functions. We derive a
novel gradient-based controller with dynamic policy regret guarantees, which is
the first controller competitive to a sequence of changing policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhi-Hua Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Competitive Memory: A Neural System for Online Task-Free Lifelong Learning. (arXiv:2106.13300v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13300</id>
        <link href="http://arxiv.org/abs/2106.13300"/>
        <updated>2021-06-28T01:57:57.199Z</updated>
        <summary type="html"><![CDATA[In this article, we propose a novel form of unsupervised learning, continual
competitive memory (CCM), as well as a computational framework to unify related
neural models that operate under the principles of competition. The resulting
neural system is shown to offer an effective approach for combating
catastrophic forgetting in online continual classification problems. We
demonstrate that the proposed CCM system not only outperforms other competitive
learning neural models but also yields performance that is competitive with
several modern, state-of-the-art lifelong learning approaches on benchmarks
such as Split MNIST and Split NotMNIST. CCM yields a promising path forward for
acquiring representations that are robust to interference from data streams,
especially when the task is unknown to the model and must be inferred without
external guidance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1"&gt;Alexander G. Ororbia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15082</id>
        <link href="http://arxiv.org/abs/2105.15082"/>
        <updated>2021-06-28T01:57:57.192Z</updated>
        <summary type="html"><![CDATA[Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Le Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Di Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Lin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape registration in the time of transformers. (arXiv:2106.13679v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13679</id>
        <link href="http://arxiv.org/abs/2106.13679"/>
        <updated>2021-06-28T01:57:57.183Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a transformer-based procedure for the efficient
registration of non-rigid 3D point clouds. The proposed approach is data-driven
and adopts for the first time the transformer architecture in the registration
task. Our method is general and applies to different settings. Given a fixed
template with some desired properties (e.g. skinning weights or other animation
cues), we can register raw acquired data to it, thereby transferring all the
template properties to the input geometry. Alternatively, given a pair of
shapes, our method can register the first onto the second (or vice-versa),
obtaining a high-quality dense correspondence between the two. In both
contexts, the quality of our results enables us to target real applications
such as texture transfer and shape interpolation. Furthermore, we also show
that including an estimation of the underlying density of the surface eases the
learning process. By exploiting the potential of this architecture, we can
train our model requiring only a sparse set of ground truth correspondences
($10\sim20\%$ of the total points). The proposed model and the analysis that we
perform pave the way for future exploration of transformer-based architectures
for registration and matching applications. Qualitative and quantitative
evaluations demonstrate that our pipeline outperforms state-of-the-art methods
for deformable and unordered 3D data registration on different datasets and
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1"&gt;Giovanni Trappolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1"&gt;Luca Cosmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1"&gt;Luca Moschella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1"&gt;Riccardo Marin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1"&gt;Emanuele Rodol&amp;#xe0;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session-aware Linear Item-Item Models for Session-based Recommendation. (arXiv:2103.16104v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16104</id>
        <link href="http://arxiv.org/abs/2103.16104"/>
        <updated>2021-06-28T01:57:57.173Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation aims at predicting the next item given a
sequence of previous items consumed in the session, e.g., on e-commerce or
multimedia streaming services. Specifically, session data exhibits some unique
characteristics, i.e., session consistency and sequential dependency over items
within the session, repeated item consumption, and session timeliness. In this
paper, we propose simple-yet-effective linear models for considering the
holistic aspects of the sessions. The comprehensive nature of our models helps
improve the quality of session-based recommendation. More importantly, it
provides a generalized framework for reflecting different perspectives of
session data. Furthermore, since our models can be solved by closed-form
solutions, they are highly scalable. Experimental results demonstrate that the
proposed linear models show competitive or state-of-the-art performance in
various metrics on several real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1"&gt;Minjin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_j/0/1/0/all/0/1"&gt;jinhong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joonseok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1"&gt;Hyunjung Shim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jongwuk Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior Image-Constrained Reconstruction using Style-Based Generative Models. (arXiv:2102.12525v2 [eess.IV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2102.12525</id>
        <link href="http://arxiv.org/abs/2102.12525"/>
        <updated>2021-06-28T01:57:57.154Z</updated>
        <summary type="html"><![CDATA[Obtaining a useful estimate of an object from highly incomplete imaging
measurements remains a holy grail of imaging science. Deep learning methods
have shown promise in learning object priors or constraints to improve the
conditioning of an ill-posed imaging inverse problem. In this study, a
framework for estimating an object of interest that is semantically related to
a known prior image, is proposed. An optimization problem is formulated in the
disentangled latent space of a style-based generative model, and semantically
meaningful constraints are imposed using the disentangled latent representation
of the prior image. Stable recovery from incomplete measurements with the help
of a prior image is theoretically analyzed. Numerical experiments demonstrating
the superior performance of our approach as compared to related methods are
presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1"&gt;Varun A. Kelkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1"&gt;Mark A. Anastasio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantitative approximation results for complex-valued neural networks. (arXiv:2102.13092v2 [math.FA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13092</id>
        <link href="http://arxiv.org/abs/2102.13092"/>
        <updated>2021-06-28T01:57:57.147Z</updated>
        <summary type="html"><![CDATA[Until recently, applications of neural networks in machine learning have
almost exclusively relied on real-valued networks. It was recently observed,
however, that complex-valued neural networks (CVNNs) exhibit superior
performance in applications in which the input is naturally complex-valued,
such as MRI fingerprinting. While the mathematical theory of real-valued
networks has, by now, reached some level of maturity, this is far from true for
complex-valued networks. In this paper, we analyze the expressivity of
complex-valued networks by providing explicit quantitative error bounds for
approximating $C^n$ functions on compact subsets of $\mathbb{C}^d$ by
complex-valued neural networks that employ the modReLU activation function,
given by $\sigma(z) = \mathrm{ReLU}(|z| - 1) \, \mathrm{sgn} (z)$, which is one
of the most popular complex activation functions used in practice. We show that
the derived approximation rates are optimal (up to log factors) in the class of
modReLU networks with weights of moderate growth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Caragea_A/0/1/0/all/0/1"&gt;A. Caragea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lee_D/0/1/0/all/0/1"&gt;D.G. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Maly_J/0/1/0/all/0/1"&gt;J. Maly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pfander_G/0/1/0/all/0/1"&gt;G. Pfander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Voigtlaender_F/0/1/0/all/0/1"&gt;F. Voigtlaender&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast quantum state reconstruction via accelerated non-convex programming. (arXiv:2104.07006v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07006</id>
        <link href="http://arxiv.org/abs/2104.07006"/>
        <updated>2021-06-28T01:57:57.141Z</updated>
        <summary type="html"><![CDATA[We propose a new quantum state reconstruction method that combines ideas from
compressed sensing, non-convex optimization, and acceleration methods. The
algorithm, called Momentum-Inspired Factored Gradient Descent (\texttt{MiFGD}),
extends the applicability of quantum tomography for larger systems. Despite
being a non-convex method, \texttt{MiFGD} converges \emph{provably} to the true
density matrix at a linear rate, in the absence of experimental and statistical
noise, and under common assumptions. With this manuscript, we present the
method, prove its convergence property and provide Frobenius norm bound
guarantees with respect to the true density matrix. From a practical point of
view, we benchmark the algorithm performance with respect to other existing
methods, in both synthetic and real experiments performed on an IBM's quantum
processing unit. We find that the proposed algorithm performs orders of
magnitude faster than state of the art approaches, with the same or better
accuracy. In both synthetic and real experiments, we observed accurate and
robust reconstruction, despite experimental and statistical noise in the
tomographic data. Finally, we provide a ready-to-use code for state tomography
of multi-qubit systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junhyung Lyle Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kollias_G/0/1/0/all/0/1"&gt;George Kollias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kalev_A/0/1/0/all/0/1"&gt;Amir Kalev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wei_K/0/1/0/all/0/1"&gt;Ken X. Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kyrillidis_A/0/1/0/all/0/1"&gt;Anastasios Kyrillidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masksembles for Uncertainty Estimation. (arXiv:2012.08334v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08334</id>
        <link href="http://arxiv.org/abs/2012.08334"/>
        <updated>2021-06-28T01:57:57.134Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have amply demonstrated their prowess but estimating the
reliability of their predictions remains challenging. Deep Ensembles are widely
considered as being one of the best methods for generating uncertainty
estimates but are very expensive to train and evaluate. MC-Dropout is another
popular alternative, which is less expensive, but also less reliable. Our
central intuition is that there is a continuous spectrum of ensemble-like
models of which MC-Dropout and Deep Ensembles are extreme examples. The first
uses an effectively infinite number of highly correlated models while the
second relies on a finite number of independent models.

To combine the benefits of both, we introduce Masksembles. Instead of
randomly dropping parts of the network as in MC-dropout, Masksemble relies on a
fixed number of binary masks, which are parameterized in a way that allows to
change correlations between individual models. Namely, by controlling the
overlap between the masks and their density one can choose the optimal
configuration for the task at hand. This leads to a simple and easy to
implement method with performance on par with Ensembles at a fraction of the
cost. We experimentally validate Masksembles on two widely used datasets,
CIFAR10 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Durasov_N/0/1/0/all/0/1"&gt;Nikita Durasov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1"&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1"&gt;Pierre Baque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-NILM: A Federated Learning-based Non-Intrusive Load Monitoring Method for Privacy-Protection. (arXiv:2105.11085v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11085</id>
        <link href="http://arxiv.org/abs/2105.11085"/>
        <updated>2021-06-28T01:57:57.126Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) is essential for understanding
customer's power consumption patterns and may find wide applications like
carbon emission reduction and energy conservation. The training of NILM models
requires massive load data containing different types of appliances. However,
inadequate load data and the risk of power consumer privacy breaches may be
encountered by local data owners during the NILM model training. To prevent
such potential risks, a novel NILM method named Fed-NILM which is based on
Federated Learning (FL) is proposed in this paper. In Fed-NILM, local model
parameters instead of local load data are shared among multiple data owners.
The global model is obtained by weighted averaging the parameters. Experiments
based on two measured load datasets are conducted to explore the generalization
ability of Fed-NILM. Besides, a comparison of Fed-NILM with locally-trained
NILMs and the centrally-trained NILM is conducted. The experimental results
show that Fed-NILM has superior performance in scalability and convergence.
Fed-NILM outperforms locally-trained NILMs operated by local data owners and
approximates the centrally-trained NILM which is trained on the entire load
dataset without privacy protection. The proposed Fed-NILM significantly
improves the co-modeling capabilities of local data owners while protecting
power consumers' privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haijin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Caomingzhe Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Junhua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guolong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1"&gt;Fushuan Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Probabilistic Relaxation of Discrete Variables for Soft Detection with Low Complexity: CMDNet. (arXiv:2102.12756v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12756</id>
        <link href="http://arxiv.org/abs/2102.12756"/>
        <updated>2021-06-28T01:57:57.120Z</updated>
        <summary type="html"><![CDATA[Following the great success of Machine Learning (ML), especially Deep Neural
Networks (DNNs), in many research domains in 2010s, several ML-based approaches
were proposed for detection in large inverse linear problems, e.g., massive
MIMO systems. The main motivation behind is that the complexity of Maximum
A-Posteriori (MAP) detection grows exponentially with system dimensions.
Instead of using DNNs, essentially being a black-box, we take a slightly
different approach and introduce a probabilistic Continuous relaxation of
disCrete variables to MAP detection. Enabling close approximation and
continuous optimization, we derive an iterative detection algorithm: Concrete
MAP Detection (CMD). Furthermore, extending CMD by the idea of deep unfolding
into CMDNet, we allow for (online) optimization of a small number of parameters
to different working points while limiting complexity. In contrast to recent
DNN-based approaches, we select the optimization criterion and output of CMDNet
based on information theory and are thus able to learn approximate
probabilities of the individual optimal detector. This is crucial for soft
decoding in today's communication systems. Numerical simulation results in MIMO
systems reveal CMDNet to feature a promising accuracy complexity trade-off
compared to State of the Art. Notably, we demonstrate CMDNet's soft outputs to
be reliable for decoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Beck_E/0/1/0/all/0/1"&gt;Edgar Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bockelmann_C/0/1/0/all/0/1"&gt;Carsten Bockelmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dekorsy_A/0/1/0/all/0/1"&gt;Armin Dekorsy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From inexact optimization to learning via gradient concentration. (arXiv:2106.05397v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05397</id>
        <link href="http://arxiv.org/abs/2106.05397"/>
        <updated>2021-06-28T01:57:57.100Z</updated>
        <summary type="html"><![CDATA[Optimization was recently shown to control the inductive bias in a learning
process, a property referred to as implicit, or iterative regularization. The
estimator obtained iteratively minimizing the training error can generalise
well with no need of further penalties or constraints. In this paper, we
investigate this phenomenon in the context of linear models with smooth loss
functions. In particular, we investigate and propose a proof technique
combining ideas from inexact optimization and probability theory, specifically
gradient concentration. The proof is easy to follow and allows to obtain sharp
learning bounds. More generally, it highlights a way to develop optimization
results into learning guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Stankewitz_B/0/1/0/all/0/1"&gt;Bernhard Stankewitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mucke_N/0/1/0/all/0/1"&gt;Nicole M&amp;#xfc;cke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1"&gt;Lorenzo Rosasco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in the Eyes of the Data: Certifying Machine-Learning Models. (arXiv:2009.01534v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.01534</id>
        <link href="http://arxiv.org/abs/2009.01534"/>
        <updated>2021-06-28T01:57:57.083Z</updated>
        <summary type="html"><![CDATA[We present a framework that allows to certify the fairness degree of a model
based on an interactive and privacy-preserving test. The framework verifies any
trained model, regardless of its training process and architecture. Thus, it
allows us to evaluate any deep learning model on multiple fairness definitions
empirically. We tackle two scenarios, where either the test data is privately
available only to the tester or is publicly known in advance, even to the model
creator. We investigate the soundness of the proposed approach using
theoretical analysis and present statistical guarantees for the interactive
test. Finally, we provide a cryptographic technique to automate fairness
testing and certified inference with only black-box access to the model at hand
while hiding the participants' sensitive data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Segal_S/0/1/0/all/0/1"&gt;Shahar Segal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1"&gt;Yossi Adi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinkas_B/0/1/0/all/0/1"&gt;Benny Pinkas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baum_C/0/1/0/all/0/1"&gt;Carsten Baum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_C/0/1/0/all/0/1"&gt;Chaya Ganesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1"&gt;Joseph Keshet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyperNP: Interactive Visual Exploration of Multidimensional Projection Hyperparameters. (arXiv:2106.13777v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13777</id>
        <link href="http://arxiv.org/abs/2106.13777"/>
        <updated>2021-06-28T01:57:57.070Z</updated>
        <summary type="html"><![CDATA[Projection algorithms such as t-SNE or UMAP are useful for the visualization
of high dimensional data, but depend on hyperparameters which must be tuned
carefully. Unfortunately, iteratively recomputing projections to find the
optimal hyperparameter value is computationally intensive and unintuitive due
to the stochastic nature of these methods. In this paper we propose HyperNP, a
scalable method that allows for real-time interactive hyperparameter
exploration of projection methods by training neural network approximations.
HyperNP can be trained on a fraction of the total data instances and
hyperparameter configurations and can compute projections for new data and
hyperparameters at interactive speeds. HyperNP is compact in size and fast to
compute, thus allowing it to be embedded in lightweight visualization systems
such as web browsers. We evaluate the performance of the HyperNP across three
datasets in terms of performance and speed. The results suggest that HyperNP is
accurate, scalable, interactive, and appropriate for use in real-world
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Appleby_G/0/1/0/all/0/1"&gt;Gabriel Appleby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espadoto_M/0/1/0/all/0/1"&gt;Mateus Espadoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Rui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goree_S/0/1/0/all/0/1"&gt;Samuel Goree&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Telea_A/0/1/0/all/0/1"&gt;Alexandru Telea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_E/0/1/0/all/0/1"&gt;Erik W Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1"&gt;Remco Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Multimodal ELBO. (arXiv:2105.02470v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02470</id>
        <link href="http://arxiv.org/abs/2105.02470"/>
        <updated>2021-06-28T01:57:57.063Z</updated>
        <summary type="html"><![CDATA[Multiple data types naturally co-occur when describing real-world phenomena
and learning from them is a long-standing goal in machine learning research.
However, existing self-supervised generative models approximating an ELBO are
not able to fulfill all desired requirements of multimodal models: their
posterior approximation functions lead to a trade-off between the semantic
coherence and the ability to learn the joint data distribution. We propose a
new, generalized ELBO formulation for multimodal data that overcomes these
limitations. The new objective encompasses two previous methods as special
cases and combines their benefits without compromises. In extensive
experiments, we demonstrate the advantage of the proposed method compared to
state-of-the-art models in self-supervised, generative learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sutter_T/0/1/0/all/0/1"&gt;Thomas M. Sutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daunhawer_I/0/1/0/all/0/1"&gt;Imant Daunhawer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1"&gt;Julia E. Vogt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NSL: Hybrid Interpretable Learning From Noisy Raw Data. (arXiv:2012.05023v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05023</id>
        <link href="http://arxiv.org/abs/2012.05023"/>
        <updated>2021-06-28T01:57:57.025Z</updated>
        <summary type="html"><![CDATA[Inductive Logic Programming (ILP) systems learn generalised, interpretable
rules in a data-efficient manner utilising existing background knowledge.
However, current ILP systems require training examples to be specified in a
structured logical format. Neural networks learn from unstructured data,
although their learned models may be difficult to interpret and are vulnerable
to data perturbations at run-time. This paper introduces a hybrid
neural-symbolic learning framework, called NSL, that learns interpretable rules
from labelled unstructured data. NSL combines pre-trained neural networks for
feature extraction with FastLAS, a state-of-the-art ILP system for rule
learning under the answer set semantics. Features extracted by the neural
components define the structured context of labelled examples and the
confidence of the neural predictions determines the level of noise of the
examples. Using the scoring function of FastLAS, NSL searches for short,
interpretable rules that generalise over such noisy examples. We evaluate our
framework on propositional and first-order classification tasks using the MNIST
dataset as raw data. Specifically, we demonstrate that NSL is able to learn
robust rules from perturbed MNIST data and achieve comparable or superior
accuracy when compared to neural network and random forest baselines whilst
being more general and interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1"&gt;Daniel Cunnington&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1"&gt;Alessandra Russo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Mark Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1"&gt;Jorge Lobo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplan_L/0/1/0/all/0/1"&gt;Lance Kaplan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Origin of Information-Seeking Exploration in Probabilistic Objectives for Control. (arXiv:2103.06859v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06859</id>
        <link href="http://arxiv.org/abs/2103.06859"/>
        <updated>2021-06-28T01:57:57.018Z</updated>
        <summary type="html"><![CDATA[The exploration-exploitation trade-off is central to the description of
adaptive behaviour in fields ranging from machine learning, to biology, to
economics. While many approaches have been taken, one approach to solving this
trade-off has been to equip or propose that agents possess an intrinsic
'exploratory drive' which is often implemented in terms of maximizing the
agents information gain about the world -- an approach which has been widely
studied in machine learning and cognitive science. In this paper we
mathematically investigate the nature and meaning of such approaches and
demonstrate that this combination of utility maximizing and information-seeking
behaviour arises from the minimization of an entirely difference class of
objectives we call divergence objectives. We propose a dichotomy in the
objective functions underlying adaptive behaviour between \emph{evidence}
objectives, which correspond to well-known reward or utility maximizing
objectives in the literature, and \emph{divergence} objectives which instead
seek to minimize the divergence between the agent's expected and desired
futures, and argue that this new class of divergence objectives could form the
mathematical foundation for a much richer understanding of the exploratory
components of adaptive and intelligent action, beyond simply greedy utility
maximization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Millidge_B/0/1/0/all/0/1"&gt;Beren Millidge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tschantz_A/0/1/0/all/0/1"&gt;Alexander Tschantz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1"&gt;Anil Seth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1"&gt;Christopher Buckley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of Hereditary Cancers Using Neural Networks. (arXiv:2106.13682v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13682</id>
        <link href="http://arxiv.org/abs/2106.13682"/>
        <updated>2021-06-28T01:57:56.949Z</updated>
        <summary type="html"><![CDATA[Family history is a major risk factor for many types of cancer. Mendelian
risk prediction models translate family histories into cancer risk predictions
based on knowledge of cancer susceptibility genes. These models are widely used
in clinical practice to help identify high-risk individuals. Mendelian models
leverage the entire family history, but they rely on many assumptions about
cancer susceptibility genes that are either unrealistic or challenging to
validate due to low mutation prevalence. Training more flexible models, such as
neural networks, on large databases of pedigrees can potentially lead to
accuracy gains. In this paper, we develop a framework to apply neural networks
to family history data and investigate their ability to learn inherited
susceptibility to cancer. While there is an extensive literature on neural
networks and their state-of-the-art performance in many tasks, there is little
work applying them to family history data. We propose adaptations of
fully-connected neural networks and convolutional neural networks to pedigrees.
In data simulated under Mendelian inheritance, we demonstrate that our proposed
neural network models are able to achieve nearly optimal prediction
performance. Moreover, when the observed family history includes misreported
cancer diagnoses, neural networks are able to outperform the Mendelian BRCAPRO
model embedding the correct inheritance laws. Using a large dataset of over
200,000 family histories, the Risk Service cohort, we train prediction models
for future risk of breast cancer. We validate the models using data from the
Cancer Genetics Network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Guan_Z/0/1/0/all/0/1"&gt;Zoe Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Parmigiani_G/0/1/0/all/0/1"&gt;Giovanni Parmigiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Braun_D/0/1/0/all/0/1"&gt;Danielle Braun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Trippa_L/0/1/0/all/0/1"&gt;Lorenzo Trippa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joslim: Joint Widths and Weights Optimization for Slimmable Neural Networks. (arXiv:2007.11752v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11752</id>
        <link href="http://arxiv.org/abs/2007.11752"/>
        <updated>2021-06-28T01:57:56.934Z</updated>
        <summary type="html"><![CDATA[Slimmable neural networks provide a flexible trade-off front between
prediction error and computational requirement (such as the number of
floating-point operations or FLOPs) with the same storage requirement as a
single model. They are useful for reducing maintenance overhead for deploying
models to devices with different memory constraints and are useful for
optimizing the efficiency of a system with many CNNs. However, existing
slimmable network approaches either do not optimize layer-wise widths or
optimize the shared-weights and layer-wise widths independently, thereby
leaving significant room for improvement by joint width and weight
optimization. In this work, we propose a general framework to enable joint
optimization for both width configurations and weights of slimmable networks.
Our framework subsumes conventional and NAS-based slimmable methods as special
cases and provides flexibility to improve over existing methods. From a
practical standpoint, we propose Joslim, an algorithm that jointly optimizes
both the widths and weights for slimmable nets, which outperforms existing
methods for optimizing slimmable networks across various networks, datasets,
and objectives. Quantitatively, improvements up to 1.7% and 8% in top-1
accuracy on the ImageNet dataset can be attained for MobileNetV2 considering
FLOPs and memory footprint, respectively. Our results highlight the potential
of optimizing the channel counts for different layers jointly with the weights
for slimmable networks. Code available at https://github.com/cmu-enyac/Joslim.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1"&gt;Ting-Wu Chin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1"&gt;Ari S. Morcos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1"&gt;Diana Marculescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliable Graph Neural Network Explanations Through Adversarial Training. (arXiv:2106.13427v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13427</id>
        <link href="http://arxiv.org/abs/2106.13427"/>
        <updated>2021-06-28T01:57:56.911Z</updated>
        <summary type="html"><![CDATA[Graph neural network (GNN) explanations have largely been facilitated through
post-hoc introspection. While this has been deemed successful, many post-hoc
explanation methods have been shown to fail in capturing a model's learned
representation. Due to this problem, it is worthwhile to consider how one might
train a model so that it is more amenable to post-hoc analysis. Given the
success of adversarial training in the computer vision domain to train models
with more reliable representations, we propose a similar training paradigm for
GNNs and analyze the respective impact on a model's explanations. In instances
without ground truth labels, we also determine how well an explanation method
is utilizing a model's learned representation through a new metric and
demonstrate adversarial training can help better extract domain-relevant
insights in chemistry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loveland_D/0/1/0/all/0/1"&gt;Donald Loveland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shusen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hiszpanski_A/0/1/0/all/0/1"&gt;Anna Hiszpanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yong Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subgraph Federated Learning with Missing Neighbor Generation. (arXiv:2106.13430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13430</id>
        <link href="http://arxiv.org/abs/2106.13430"/>
        <updated>2021-06-28T01:57:56.893Z</updated>
        <summary type="html"><![CDATA[Graphs have been widely used in data mining and machine learning due to their
unique representation of real-world objects and their interactions. As graphs
are getting bigger and bigger nowadays, it is common to see their subgraphs
separately collected and stored in multiple local systems. Therefore, it is
natural to consider the subgraph federated learning setting, where each local
system holding a small subgraph that may be biased from the distribution of the
whole graph. Hence, the subgraph federated learning aims to collaboratively
train a powerful and generalizable graph mining model without directly sharing
their graph data. In this work, towards the novel yet realistic setting of
subgraph federated learning, we propose two major techniques: (1) FedSage,
which trains a GraphSage model based on FedAvg to integrate node features, link
structures, and task labels on multiple local subgraphs; (2) FedSage+, which
trains a missing neighbor generator along FedSage to deal with missing links
across local subgraphs. Empirical results on four real-world graph datasets
with synthesized subgraph federated learning settings demonstrate the
effectiveness and efficiency of our proposed techniques. At the same time,
consistent theoretical implications are made towards their generalization
ability on the global graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Ke Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1"&gt;Siu Ming Yiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building Intelligent Autonomous Navigation Agents. (arXiv:2106.13415v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13415</id>
        <link href="http://arxiv.org/abs/2106.13415"/>
        <updated>2021-06-28T01:57:56.885Z</updated>
        <summary type="html"><![CDATA[Breakthroughs in machine learning in the last decade have led to `digital
intelligence', i.e. machine learning models capable of learning from vast
amounts of labeled data to perform several digital tasks such as speech
recognition, face recognition, machine translation and so on. The goal of this
thesis is to make progress towards designing algorithms capable of `physical
intelligence', i.e. building intelligent autonomous navigation agents capable
of learning to perform complex navigation tasks in the physical world involving
visual perception, natural language understanding, reasoning, planning, and
sequential decision making. Despite several advances in classical navigation
methods in the last few decades, current navigation agents struggle at
long-term semantic navigation tasks. In the first part of the thesis, we
discuss our work on short-term navigation using end-to-end reinforcement
learning to tackle challenges such as obstacle avoidance, semantic perception,
language grounding, and reasoning. In the second part, we present a new class
of navigation methods based on modular learning and structured explicit map
representations, which leverage the strengths of both classical and end-to-end
learning methods, to tackle long-term navigation tasks. We show that these
methods are able to effectively tackle challenges such as localization,
mapping, long-term planning, exploration and learning semantic priors. These
modular learning methods are capable of long-term spatial and semantic
understanding and achieve state-of-the-art results on various navigation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1"&gt;Devendra Singh Chaplot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Approximation of Residual Flows in Maximum Mean Discrepancy. (arXiv:2103.05793v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05793</id>
        <link href="http://arxiv.org/abs/2103.05793"/>
        <updated>2021-06-28T01:57:56.876Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are a class of flexible deep generative models that offer
easy likelihood computation. Despite their empirical success, there is little
theoretical understanding of their expressiveness. In this work, we study
residual flows, a class of normalizing flows composed of Lipschitz residual
blocks. We prove residual flows are universal approximators in maximum mean
discrepancy. We provide upper bounds on the number of residual blocks to
achieve approximation under different assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhifeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1"&gt;Kamalika Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coded-InvNet for Resilient Prediction Serving Systems. (arXiv:2106.06445v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06445</id>
        <link href="http://arxiv.org/abs/2106.06445"/>
        <updated>2021-06-28T01:57:56.870Z</updated>
        <summary type="html"><![CDATA[Inspired by a new coded computation algorithm for invertible functions, we
propose Coded-InvNet a new approach to design resilient prediction serving
systems that can gracefully handle stragglers or node failures. Coded-InvNet
leverages recent findings in the deep learning literature such as invertible
neural networks, Manifold Mixup, and domain translation algorithms, identifying
interesting research directions that span across machine learning and systems.
Our experimental results show that Coded-InvNet can outperform existing
approaches, especially when the compute resource overhead is as low as 10%. For
instance, without knowing which of the ten workers is going to fail, our
algorithm can design a backup task so that it can correctly recover the missing
prediction result with an accuracy of 85.9%, significantly outperforming the
previous SOTA by 32.5%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1"&gt;Tuan Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kangwook Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Combination of Linear and Spectral Estimators for Generalized Linear Models. (arXiv:2008.03326v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03326</id>
        <link href="http://arxiv.org/abs/2008.03326"/>
        <updated>2021-06-28T01:57:56.851Z</updated>
        <summary type="html"><![CDATA[We study the problem of recovering an unknown signal $\boldsymbol x$ given
measurements obtained from a generalized linear model with a Gaussian sensing
matrix. Two popular solutions are based on a linear estimator $\hat{\boldsymbol
x}^{\rm L}$ and a spectral estimator $\hat{\boldsymbol x}^{\rm s}$. The former
is a data-dependent linear combination of the columns of the measurement
matrix, and its analysis is quite simple. The latter is the principal
eigenvector of a data-dependent matrix, and a recent line of work has studied
its performance. In this paper, we show how to optimally combine
$\hat{\boldsymbol x}^{\rm L}$ and $\hat{\boldsymbol x}^{\rm s}$. At the heart
of our analysis is the exact characterization of the joint empirical
distribution of $(\boldsymbol x, \hat{\boldsymbol x}^{\rm L}, \hat{\boldsymbol
x}^{\rm s})$ in the high-dimensional limit. This allows us to compute the
Bayes-optimal combination of $\hat{\boldsymbol x}^{\rm L}$ and
$\hat{\boldsymbol x}^{\rm s}$, given the limiting distribution of the signal
$\boldsymbol x$. When the distribution of the signal is Gaussian, then the
Bayes-optimal combination has the form $\theta\hat{\boldsymbol x}^{\rm
L}+\hat{\boldsymbol x}^{\rm s}$ and we derive the optimal combination
coefficient. In order to establish the limiting distribution of $(\boldsymbol
x, \hat{\boldsymbol x}^{\rm L}, \hat{\boldsymbol x}^{\rm s})$, we design and
analyze an Approximate Message Passing (AMP) algorithm whose iterates give
$\hat{\boldsymbol x}^{\rm L}$ and approach $\hat{\boldsymbol x}^{\rm s}$.
Numerical simulations demonstrate the improvement of the proposed combination
with respect to the two methods considered separately.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mondelli_M/0/1/0/all/0/1"&gt;Marco Mondelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thrampoulidis_C/0/1/0/all/0/1"&gt;Christos Thrampoulidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venkataramanan_R/0/1/0/all/0/1"&gt;Ramji Venkataramanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of Graph-Based Approaches for Semi-Supervised Time Series Classification. (arXiv:2104.08153v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08153</id>
        <link href="http://arxiv.org/abs/2104.08153"/>
        <updated>2021-06-28T01:57:56.841Z</updated>
        <summary type="html"><![CDATA[Time series data play an important role in many applications and their
analysis reveals crucial information for understanding the underlying
processes. Among the many time series learning tasks of great importance, we
here focus on semi-supervised learning based on a graph representation of the
data. Two main aspects are involved in this task. A suitable distance measure
to evaluate the similarities between time series, and a learning method to make
predictions based on these distances. However, the relationship between the two
aspects has never been studied systematically in the context of graph-based
learning. We describe four different distance measures, including (Soft) DTW
and MPDist, a distance measure based on the Matrix Profile, as well as four
successful semi-supervised learning methods, including the graph Allen--Cahn
method and a Graph Convolutional Neural Network. We then compare the
performance of the algorithms on binary classification data sets. In our
findings we compare the chosen graph-based methods using all distance measures
and observe that the results vary strongly with respect to the accuracy. As
predicted by the ``no free lunch'' theorem, no clear best combination to employ
in all cases is found. Our study provides a reproducible framework for future
work in the direction of semi-supervised learning for time series with a focus
on graph representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alfke_D/0/1/0/all/0/1"&gt;Dominik Alfke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gondos_M/0/1/0/all/0/1"&gt;Miriam Gondos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peroche_L/0/1/0/all/0/1"&gt;Lucile Peroche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoll_M/0/1/0/all/0/1"&gt;Martin Stoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Matrix Factorization with Grouping Effect. (arXiv:2106.13681v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13681</id>
        <link href="http://arxiv.org/abs/2106.13681"/>
        <updated>2021-06-28T01:57:56.814Z</updated>
        <summary type="html"><![CDATA[Although many techniques have been applied to matrix factorization (MF), they
may not fully exploit the feature structure. In this paper, we incorporate the
grouping effect into MF and propose a novel method called Robust Matrix
Factorization with Grouping effect (GRMF). The grouping effect is a
generalization of the sparsity effect, which conducts denoising by clustering
similar values around multiple centers instead of just around 0. Compared with
existing algorithms, the proposed GRMF can automatically learn the grouping
structure and sparsity in MF without prior knowledge, by introducing a
naturally adjustable non-convex regularization to achieve simultaneous sparsity
and grouping effect. Specifically, GRMF uses an efficient alternating
minimization framework to perform MF, in which the original non-convex problem
is first converted into a convex problem through Difference-of-Convex (DC)
programming, and then solved by Alternating Direction Method of Multipliers
(ADMM). In addition, GRMF can be easily extended to the Non-negative Matrix
Factorization (NMF) settings. Extensive experiments have been conducted using
real-world data sets with outliers and contaminated noise, where the
experimental results show that GRMF has promoted performance and robustness,
compared to five benchmark algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haiyan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Luwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepLoc: A Ubiquitous Accurate and Low-Overhead Outdoor Cellular Localization System. (arXiv:2106.13632v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13632</id>
        <link href="http://arxiv.org/abs/2106.13632"/>
        <updated>2021-06-28T01:57:56.808Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed fast growth in outdoor location-based services.
While GPS is considered a ubiquitous localization system, it is not supported
by low-end phones, requires direct line of sight to the satellites, and can
drain the phone battery quickly.

In this paper, we propose DeepLoc: a deep learning-based outdoor localization
system that obtains GPS-like localization accuracy without its limitations. In
particular, DeepLoc leverages the ubiquitous cellular signals received from the
different cell towers heard by the mobile device as hints to localize it. To do
that, crowd-sensed geo-tagged received signal strength information coming from
different cell towers is used to train a deep model that is used to infer the
user's position. As part of DeepLoc design, we introduce modules to address a
number of practical challenges including scaling the data collection to large
areas, handling the inherent noise in the cellular signal and geo-tagged data,
as well as providing enough data that is required for deep learning models with
low-overhead.

We implemented DeepLoc on different Android devices. Evaluation results in
realistic urban and rural environments show that DeepLoc can achieve a median
localization accuracy within 18.8m in urban areas and within 15.7m in rural
areas. This accuracy outperforms the state-of-the-art cellular-based systems by
more than 470% and comes with 330% savings in power compared to the GPS. This
highlights the promise of DeepLoc as a ubiquitous accurate and low-overhead
localization system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shokry_A/0/1/0/all/0/1"&gt;Ahmed Shokry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torki_M/0/1/0/all/0/1"&gt;Marwan Torki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Youssef_M/0/1/0/all/0/1"&gt;Moustafa Youssef&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Perception-Action-Communication Loops with Convolutional and Graph Neural Networks. (arXiv:2106.13358v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.13358</id>
        <link href="http://arxiv.org/abs/2106.13358"/>
        <updated>2021-06-28T01:57:56.797Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a perception-action-communication loop design using
Vision-based Graph Aggregation and Inference (VGAI). This multi-agent
decentralized learning-to-control framework maps raw visual observations to
agent actions, aided by local communication among neighboring agents. Our
framework is implemented by a cascade of a convolutional and a graph neural
network (CNN / GNN), addressing agent-level visual perception and feature
learning, as well as swarm-level communication, local information aggregation
and agent action inference, respectively. By jointly training the CNN and GNN,
image features and communication messages are learned in conjunction to better
address the specific task. We use imitation learning to train the VGAI
controller in an offline phase, relying on a centralized expert controller.
This results in a learned VGAI controller that can be deployed in a distributed
manner for online execution. Additionally, the controller exhibits good scaling
properties, with training in smaller teams and application in larger teams.
Through a multi-agent flocking application, we demonstrate that VGAI yields
performance comparable to or better than other decentralized controllers, using
only the visual input modality and without accessing precise location or motion
state information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1"&gt;Ting-Kuei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1"&gt;Fernando Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1"&gt;Brian M. Sadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying malicious accounts in Blockchains using Domain Names and associated temporal properties. (arXiv:2106.13420v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.13420</id>
        <link href="http://arxiv.org/abs/2106.13420"/>
        <updated>2021-06-28T01:57:56.777Z</updated>
        <summary type="html"><![CDATA[The rise in the adoption of blockchain technology has led to increased
illegal activities by cyber-criminals costing billions of dollars. Many machine
learning algorithms are applied to detect such illegal behavior. These
algorithms are often trained on the transaction behavior and, in some cases,
trained on the vulnerabilities that exist in the system. In our approach, we
study the feasibility of using metadata such as Domain Name (DN) associated
with the account in the blockchain and identify whether an account should be
tagged malicious or not. Here, we leverage the temporal aspects attached to the
DNs. Our results identify 144930 DNs that show malicious behavior, and out of
these, 54114 DNs show persistent malicious behavior over time. Nonetheless,
none of these identified malicious DNs were reported in new officially tagged
malicious blockchain DNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_R/0/1/0/all/0/1"&gt;Rohit Kumar Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1"&gt;Rachit Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Sandeep Kumar Shukla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Branch Prediction as a Reinforcement Learning Problem: Why, How and Case Studies. (arXiv:2106.13429v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13429</id>
        <link href="http://arxiv.org/abs/2106.13429"/>
        <updated>2021-06-28T01:57:56.767Z</updated>
        <summary type="html"><![CDATA[Recent years have seen stagnating improvements to branch predictor (BP)
efficacy and a dearth of fresh ideas in branch predictor design, calling for
fresh thinking in this area. This paper argues that looking at BP from the
viewpoint of Reinforcement Learning (RL) facilitates systematic reasoning
about, and exploration of, BP designs. We describe how to apply the RL
formulation to branch predictors, show that existing predictors can be
succinctly expressed in this formulation, and study two RL-based variants of
conventional BPs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zouzias_A/0/1/0/all/0/1"&gt;Anastasios Zouzias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalaitzidis_K/0/1/0/all/0/1"&gt;Kleovoulos Kalaitzidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grot_B/0/1/0/all/0/1"&gt;Boris Grot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decomposed Mutual Information Estimation for Contrastive Representation Learning. (arXiv:2106.13401v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13401</id>
        <link href="http://arxiv.org/abs/2106.13401"/>
        <updated>2021-06-28T01:57:56.759Z</updated>
        <summary type="html"><![CDATA[Recent contrastive representation learning methods rely on estimating mutual
information (MI) between multiple views of an underlying context. E.g., we can
derive multiple views of a given image by applying data augmentation, or we can
split a sequence into views comprising the past and future of some step in the
sequence. Contrastive lower bounds on MI are easy to optimize, but have a
strong underestimation bias when estimating large amounts of MI. We propose
decomposing the full MI estimation problem into a sum of smaller estimation
problems by splitting one of the views into progressively more informed
subviews and by applying the chain rule on MI between the decomposed views.
This expression contains a sum of unconditional and conditional MI terms, each
measuring modest chunks of the total MI, which facilitates approximation via
contrastive bounds. To maximize the sum, we formulate a contrastive lower bound
on the conditional MI which can be approximated efficiently. We refer to our
general approach as Decomposed Estimation of Mutual Information (DEMI). We show
that DEMI can capture a larger amount of MI than standard non-decomposed
contrastive bounds in a synthetic setting, and learns better representations in
a vision domain and for dialogue generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1"&gt;Alessandro Sordoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1"&gt;Nouha Dziri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schulz_H/0/1/0/all/0/1"&gt;Hannes Schulz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1"&gt;Geoff Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1"&gt;Phil Bachman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tachet_R/0/1/0/all/0/1"&gt;Remi Tachet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant Weight Matrices. (arXiv:2104.06214v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06214</id>
        <link href="http://arxiv.org/abs/2104.06214"/>
        <updated>2021-06-28T01:57:56.745Z</updated>
        <summary type="html"><![CDATA[In recent years, Graph Neural Networks (GNNs) appear to be state-of-the-art
algorithms for analyzing non-euclidean graph data. By applying deep-learning to
extract high-level representations from graph structures, GNNs achieve
extraordinary accuracy and great generalization ability in various tasks.
However, with the ever-increasing graph sizes, more and more complicated GNN
layers, and higher feature dimensions, the computational complexity of GNNs
grows exponentially. How to inference GNNs in real time has become a
challenging problem, especially for some resource-limited edge-computing
platforms.

To tackle this challenge, we propose BlockGNN, a software-hardware co-design
approach to realize efficient GNN acceleration. At the algorithm level, we
propose to leverage block-circulant weight matrices to greatly reduce the
complexity of various GNN models. At the hardware design level, we propose a
pipelined CirCore architecture, which supports efficient block-circulant
matrices computation. Basing on CirCore, we present a novel BlockGNN
accelerator to compute various GNNs with low latency. Moreover, to determine
the optimal configurations for diverse deployed tasks, we also introduce a
performance and resource model that helps choose the optimal hardware
parameters automatically. Comprehensive experiments on the ZC706 FPGA platform
demonstrate that on various GNN tasks, BlockGNN achieves up to $8.3\times$
speedup compared to the baseline HyGCN architecture and $111.9\times$ energy
reduction compared to the Intel Xeon CPU platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhe Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Bizhao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yijin Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guangyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1"&gt;Guojie Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Deep-Learning-Based Voice Activity Detectors and Room Impulse Response Models in Reverberant Environments. (arXiv:2106.13511v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13511</id>
        <link href="http://arxiv.org/abs/2106.13511"/>
        <updated>2021-06-28T01:57:56.726Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep-learning-based voice activity detectors (VADs) are
often trained with anechoic data. However, real acoustic environments are
generally reverberant, which causes the performance to significantly
deteriorate. To mitigate this mismatch between training data and real data, we
simulate an augmented training set that contains nearly five million
utterances. This extension comprises of anechoic utterances and their
reverberant modifications, generated by convolutions of the anechoic utterances
with a variety of room impulse responses (RIRs). We consider five different
models to generate RIRs, and five different VADs that are trained with the
augmented training set. We test all trained systems in three different real
reverberant environments. Experimental results show $20\%$ increase on average
in accuracy, precision and recall for all detectors and response models,
compared to anechoic training. Furthermore, one of the RIR models consistently
yields better performance than the other models, for all the tested VADs.
Additionally, one of the VADs consistently outperformed the other VADs in all
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1"&gt;Amir Ivry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Israel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1"&gt;Baruch Berdugo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for High-Impedance Fault Detection: Convolutional Autoencoders. (arXiv:2106.13276v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13276</id>
        <link href="http://arxiv.org/abs/2106.13276"/>
        <updated>2021-06-28T01:57:56.679Z</updated>
        <summary type="html"><![CDATA[High-impedance faults (HIF) are difficult to detect because of their low
current amplitude and highly diverse characteristics. In recent years, machine
learning (ML) has been gaining popularity in HIF detection because ML
techniques learn patterns from data and successfully detect HIFs. However, as
these methods are based on supervised learning, they fail to reliably detect
any scenario, fault or non-fault, not present in the training data.
Consequently, this paper takes advantage of unsupervised learning and proposes
a convolutional autoencoder framework for HIF detection (CAE-HIFD). Contrary to
the conventional autoencoders that learn from normal behavior, the
convolutional autoencoder (CAE) in CAE-HIFD learns only from the HIF signals
eliminating the need for presence of diverse non-HIF scenarios in the CAE
training. CAE distinguishes HIFs from non-HIF operating conditions by employing
cross-correlation. To discriminate HIFs from transient disturbances such as
capacitor or load switching, CAE-HIFD uses kurtosis, a statistical measure of
the probability distribution shape. The performance evaluation studies
conducted using the IEEE 13-node test feeder indicate that the CAE-HIFD
reliably detects HIFs, outperforms the state-of-the-art HIF detection
techniques, and is robust against noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rai_K/0/1/0/all/0/1"&gt;Khushwant Rai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hojatpanah_F/0/1/0/all/0/1"&gt;Farnam Hojatpanah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ajaei_F/0/1/0/all/0/1"&gt;Firouz Badrkhani Ajaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grolinger_K/0/1/0/all/0/1"&gt;Katarina Grolinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-based Design of Inferential Sensors for Petrochemical Industry. (arXiv:2106.13503v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13503</id>
        <link href="http://arxiv.org/abs/2106.13503"/>
        <updated>2021-06-28T01:57:56.673Z</updated>
        <summary type="html"><![CDATA[Inferential (or soft) sensors are used in industry to infer the values of
imprecisely and rarely measured (or completely unmeasured) variables from
variables measured online (e.g., pressures, temperatures). The main challenge,
akin to classical model overfitting, in designing an effective inferential
sensor is the selection of a correct structure of the sensor. The sensor
structure is represented by the number of inputs to the sensor, which
correspond to the variables measured online and their (simple) combinations.
This work is focused on the design of inferential sensors for product
composition of an industrial distillation column in two oil refinery units, a
Fluid Catalytic Cracking unit and a Vacuum Gasoil Hydrogenation unit. As the
first design step, we use several well-known data pre-treatment (gross error
detection) methods and compare the ability of these approaches to indicate
systematic errors and outliers in the available industrial data. We then study
effectiveness of various methods for design of the inferential sensors taking
into account the complexity and accuracy of the resulting model. The
effectiveness analysis indicates that the improvements achieved over the
current inferential sensors are up to 19 %.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mojto_M/0/1/0/all/0/1"&gt;Martin Mojto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lubusky_K/0/1/0/all/0/1"&gt;Karol &amp;#x13d;ubu&amp;#x161;k&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fikar_M/0/1/0/all/0/1"&gt;Miroslav Fikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulen_R/0/1/0/all/0/1"&gt;Radoslav Paulen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single and Union Non-parallel Support Vector Machine Frameworks. (arXiv:1910.09734v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09734</id>
        <link href="http://arxiv.org/abs/1910.09734"/>
        <updated>2021-06-28T01:57:56.661Z</updated>
        <summary type="html"><![CDATA[Considering the classification problem, we summarize the nonparallel support
vector machines with the nonparallel hyperplanes to two types of frameworks.
The first type constructs the hyperplanes separately. It solves a series of
small optimization problems to obtain a series of hyperplanes, but is hard to
measure the loss of each sample. The other type constructs all the hyperplanes
simultaneously, and it solves one big optimization problem with the ascertained
loss of each sample. We give the characteristics of each framework and compare
them carefully. In addition, based on the second framework, we construct a
max-min distance-based nonparallel support vector machine for multiclass
classification problem, called NSVM. It constructs hyperplanes with large
distance margin by solving an optimization problem. Experimental results on
benchmark data sets show the advantages of our NSVM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Na Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yuan-Hai Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huajun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yu-Ting Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Ling-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiu_N/0/1/0/all/0/1"&gt;Naihua Xiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1"&gt;Nai-Yang Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Source-Criticism Debiasing Method for GloVe Embeddings. (arXiv:2106.13382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13382</id>
        <link href="http://arxiv.org/abs/2106.13382"/>
        <updated>2021-06-28T01:57:56.648Z</updated>
        <summary type="html"><![CDATA[It is well-documented that word embeddings trained on large public corpora
consistently exhibit known human social biases. Although many methods for
debiasing exist, almost all fixate on completely eliminating biased information
from the embeddings and often diminish training set size in the process. In
this paper, we present a simple yet effective method for debiasing GloVe word
embeddings (Pennington et al., 2014) which works by incorporating explicit
information about training set bias rather than removing biased data outright.
Our method runs quickly and efficiently with the help of a fast bias gradient
approximation method from Brunet et al. (2019). As our approach is akin to the
notion of 'source criticism' in the humanities, we term our method
Source-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size
on Word Embedding Association Test (WEAT) sets without sacrificing training
data or TOP-1 performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McGovern_H/0/1/0/all/0/1"&gt;Hope McGovern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Acoustic Echo Cancellation with Deep Learning. (arXiv:2106.13754v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13754</id>
        <link href="http://arxiv.org/abs/2106.13754"/>
        <updated>2021-06-28T01:57:56.620Z</updated>
        <summary type="html"><![CDATA[We propose a nonlinear acoustic echo cancellation system, which aims to model
the echo path from the far-end signal to the near-end microphone in two parts.
Inspired by the physical behavior of modern hands-free devices, we first
introduce a novel neural network architecture that is specifically designed to
model the nonlinear distortions these devices induce between receiving and
playing the far-end signal. To account for variations between devices, we
construct this network with trainable memory length and nonlinear activation
functions that are not parameterized in advance, but are rather optimized
during the training stage using the training data. Second, the network is
succeeded by a standard adaptive linear filter that constantly tracks the echo
path between the loudspeaker output and the microphone. During training, the
network and filter are jointly optimized to learn the network parameters. This
system requires 17 thousand parameters that consume 500 Million floating-point
operations per second and 40 Kilo-bytes of memory. It also satisfies hands-free
communication timing requirements on a standard neural processor, which renders
it adequate for embedding on hands-free communication devices. Using 280 hours
of real and synthetic data, experiments show advantageous performance compared
to competing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1"&gt;Amir Ivry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Israel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1"&gt;Baruch Berdugo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Primordial non-Gaussianity from the Completed SDSS-IV extended Baryon Oscillation Spectroscopic Survey I: Catalogue Preparation and Systematic Mitigation. (arXiv:2106.13724v1 [astro-ph.CO])]]></title>
        <id>http://arxiv.org/abs/2106.13724</id>
        <link href="http://arxiv.org/abs/2106.13724"/>
        <updated>2021-06-28T01:57:56.611Z</updated>
        <summary type="html"><![CDATA[We investigate the large-scale clustering of the final spectroscopic sample
of quasars from the recently completed extended Baryon Oscillation
Spectroscopic Survey (eBOSS). The sample contains $343708$ objects in the
redshift range $0.8<z<2.2$ and $72667$ objects with redshifts $2.2<z<3.5$,
covering an effective area of $4699~{\rm deg}^{2}$. We develop a neural
network-based approach to mitigate spurious fluctuations in the density field
caused by spatial variations in the quality of the imaging data used to select
targets for follow-up spectroscopy. Simulations are used with the same angular
and radial distributions as the real data to estimate covariance matrices,
perform error analyses, and assess residual systematic uncertainties. We
measure the mean density contrast and cross-correlations of the eBOSS quasars
against maps of potential sources of imaging systematics to address algorithm
effectiveness, finding that the neural network-based approach outperforms
standard linear regression. Stellar density is one of the most important
sources of spurious fluctuations, and a new template constructed using data
from the Gaia spacecraft provides the best match to the observed quasar
clustering. The end-product from this work is a new value-added quasar
catalogue with the improved weights to correct for nonlinear imaging systematic
effects, which will be made public. Our quasar catalogue is used to measure the
local-type primordial non-Gaussianity in our companion paper, Mueller et al. in
preparation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rezaie_M/0/1/0/all/0/1"&gt;Mehdi Rezaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ross_A/0/1/0/all/0/1"&gt;Ashley J. Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Seo_H/0/1/0/all/0/1"&gt;Hee-Jong Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Mueller_E/0/1/0/all/0/1"&gt;Eva-Maria Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Percival_W/0/1/0/all/0/1"&gt;Will J. Percival&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Merz_G/0/1/0/all/0/1"&gt;Grant Merz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Katebi_R/0/1/0/all/0/1"&gt;Reza Katebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bunescu_R/0/1/0/all/0/1"&gt;Razvan C. Bunescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bautista_J/0/1/0/all/0/1"&gt;Julian Bautista&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Brownstein_J/0/1/0/all/0/1"&gt;Joel R. Brownstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Burtin_E/0/1/0/all/0/1"&gt;Etienne Burtin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Dawson_K/0/1/0/all/0/1"&gt;Kyle Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gil_Marin_H/0/1/0/all/0/1"&gt;H&amp;#xe9;ctor Gil-Mar&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hou_J/0/1/0/all/0/1"&gt;Jiamin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lyke_E/0/1/0/all/0/1"&gt;Eleanor B. Lyke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Macorra_A/0/1/0/all/0/1"&gt;Axel de la Macorra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rossi_G/0/1/0/all/0/1"&gt;Graziano Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Schneider_D/0/1/0/all/0/1"&gt;Donald P. Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Zarrouk_P/0/1/0/all/0/1"&gt;Pauline Zarrouk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Gong-Bo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent. (arXiv:2106.13792v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13792</id>
        <link href="http://arxiv.org/abs/2106.13792"/>
        <updated>2021-06-28T01:57:56.601Z</updated>
        <summary type="html"><![CDATA[Although the optimization objectives for learning neural networks are highly
non-convex, gradient-based methods have been wildly successful at learning
neural networks in practice. This juxtaposition has led to a number of recent
studies on provable guarantees for neural networks trained by gradient descent.
Unfortunately, the techniques in these works are often highly specific to the
problem studied in each setting, relying on different assumptions on the
distribution, optimization parameters, and network architectures, making it
difficult to generalize across different settings. In this work, we propose a
unified non-convex optimization framework for the analysis of neural network
training. We introduce the notions of proxy convexity and proxy
Polyak-Lojasiewicz (PL) inequalities, which are satisfied if the original
objective function induces a proxy objective function that is implicitly
minimized when using gradient methods. We show that stochastic gradient descent
(SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads
to efficient guarantees for proxy objective functions. We further show that
many existing guarantees for neural networks trained by gradient descent can be
unified through proxy convexity and proxy PL inequalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voice Activity Detection for Transient Noisy Environment Based on Diffusion Nets. (arXiv:2106.13763v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13763</id>
        <link href="http://arxiv.org/abs/2106.13763"/>
        <updated>2021-06-28T01:57:56.595Z</updated>
        <summary type="html"><![CDATA[We address voice activity detection in acoustic environments of transients
and stationary noises, which often occur in real life scenarios. We exploit
unique spatial patterns of speech and non-speech audio frames by independently
learning their underlying geometric structure. This process is done through a
deep encoder-decoder based neural network architecture. This structure involves
an encoder that maps spectral features with temporal information to their
low-dimensional representations, which are generated by applying the diffusion
maps method. The encoder feeds a decoder that maps the embedded data back into
the high-dimensional space. A deep neural network, which is trained to separate
speech from non-speech frames, is obtained by concatenating the decoder to the
encoder, resembling the known Diffusion nets architecture. Experimental results
show enhanced performance compared to competing voice activity detection
methods. The improvement is achieved in both accuracy, robustness and
generalization ability. Our model performs in a real-time manner and can be
integrated into audio-based communication systems. We also present a batch
algorithm which obtains an even higher accuracy for off-line applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1"&gt;Amir Ivry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1"&gt;Baruch Berdugo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Israel Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Private Adaptive Gradient Methods for Convex Optimization. (arXiv:2106.13756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13756</id>
        <link href="http://arxiv.org/abs/2106.13756"/>
        <updated>2021-06-28T01:57:56.585Z</updated>
        <summary type="html"><![CDATA[We study adaptive methods for differentially private convex optimization,
proposing and analyzing differentially private variants of a Stochastic
Gradient Descent (SGD) algorithm with adaptive stepsizes, as well as the
AdaGrad algorithm. We provide upper bounds on the regret of both algorithms and
show that the bounds are (worst-case) optimal. As a consequence of our
development, we show that our private versions of AdaGrad outperform adaptive
SGD, which in turn outperforms traditional SGD in scenarios with non-isotropic
gradients where (non-private) Adagrad provably outperforms SGD. The major
challenge is that the isotropic noise typically added for privacy dominates the
signal in gradient geometry for high-dimensional problems; approaches to this
that effectively optimize over lower-dimensional subspaces simply ignore the
actual problems that varying gradient geometries introduce. In contrast, we
study non-isotropic clipping and noise addition, developing a principled
theoretical approach; the consequent procedures also enjoy significantly
stronger empirical performance than prior approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asi_H/0/1/0/all/0/1"&gt;Hilal Asi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1"&gt;John Duchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_A/0/1/0/all/0/1"&gt;Alireza Fallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javidbakht_O/0/1/0/all/0/1"&gt;Omid Javidbakht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1"&gt;Kunal Talwar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Graph Signal Decomposition. (arXiv:2106.13517v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13517</id>
        <link href="http://arxiv.org/abs/2106.13517"/>
        <updated>2021-06-28T01:57:56.574Z</updated>
        <summary type="html"><![CDATA[Temporal graph signals are multivariate time series with individual
components associated with nodes of a fixed graph structure. Data of this kind
arises in many domains including activity of social network users, sensor
network readings over time, and time course gene expression within the
interaction network of a model organism. Traditional matrix decomposition
methods applied to such data fall short of exploiting structural regularities
encoded in the underlying graph and also in the temporal patterns of the
signal. How can we take into account such structure to obtain a succinct and
interpretable representation of temporal graph signals?

We propose a general, dictionary-based framework for temporal graph signal
decomposition (TGSD). The key idea is to learn a low-rank, joint encoding of
the data via a combination of graph and time dictionaries. We propose a highly
scalable decomposition algorithm for both complete and incomplete data, and
demonstrate its advantage for matrix decomposition, imputation of missing
values, temporal interpolation, clustering, period estimation, and rank
estimation in synthetic and real-world data ranging from traffic patterns to
social media activity. Our framework achieves 28% reduction in RMSE compared to
baselines for temporal interpolation when as many as 75% of the observations
are missing. It scales best among baselines taking under 20 seconds on 3.5
million data points and produces the most parsimonious models. To the best of
our knowledge, TGSD is the first framework to jointly model graph signals by
temporal and graph dictionaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McNeil_M/0/1/0/all/0/1"&gt;Maxwell McNeil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogdanov_P/0/1/0/all/0/1"&gt;Petko Bogdanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jitter: Random Jittering Loss Function. (arXiv:2106.13749v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13749</id>
        <link href="http://arxiv.org/abs/2106.13749"/>
        <updated>2021-06-28T01:57:56.567Z</updated>
        <summary type="html"><![CDATA[Regularization plays a vital role in machine learning optimization. One novel
regularization method called flooding makes the training loss fluctuate around
the flooding level. It intends to make the model continue to random walk until
it comes to a flat loss landscape to enhance generalization. However, the
hyper-parameter flooding level of the flooding method fails to be selected
properly and uniformly. We propose a novel method called Jitter to improve it.
Jitter is essentially a kind of random loss function. Before training, we
randomly sample the Jitter Point from a specific probability distribution. The
flooding level should be replaced by Jitter point to obtain a new target
function and train the model accordingly. As Jitter point acting as a random
factor, we actually add some randomness to the loss function, which is
consistent with the fact that there exists innumerable random behaviors in the
learning process of the machine learning model and is supposed to make the
model more robust. In addition, Jitter performs random walk randomly which
divides the loss curve into small intervals and then flipping them over,
ideally making the loss curve much flatter and enhancing generalization
ability. Moreover, Jitter can be a domain-, task-, and model-independent
regularization method and train the model effectively after the training error
reduces to zero. Our experimental results show that Jitter method can improve
model performance more significantly than the previous flooding method and make
the test loss curve descend twice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhicheng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chenglei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Sidan Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fostering Diversity in Spatial Evolutionary Generative Adversarial Networks. (arXiv:2106.13590v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13590</id>
        <link href="http://arxiv.org/abs/2106.13590"/>
        <updated>2021-06-28T01:57:56.471Z</updated>
        <summary type="html"><![CDATA[Generative adversary networks (GANs) suffer from training pathologies such as
instability and mode collapse, which mainly arise from a lack of diversity in
their adversarial interactions. Co-evolutionary GAN (CoE-GAN) training
algorithms have shown to be resilient to these pathologies. This article
introduces Mustangs, a spatially distributed CoE-GAN, which fosters diversity
by using different loss functions during the training. Experimental analysis on
MNIST and CelebA demonstrated that Mustangs trains statistically more accurate
generators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toutouh_J/0/1/0/all/0/1"&gt;Jamal Toutouh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hemberg_E/0/1/0/all/0/1"&gt;Erik Hemberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1"&gt;Una-May O&amp;#x27;Reilly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InteL-VAEs: Adding Inductive Biases to Variational Auto-Encoders via Intermediary Latents. (arXiv:2106.13746v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13746</id>
        <link href="http://arxiv.org/abs/2106.13746"/>
        <updated>2021-06-28T01:57:56.452Z</updated>
        <summary type="html"><![CDATA[We introduce a simple and effective method for learning VAEs with
controllable inductive biases by using an intermediary set of latent variables.
This allows us to overcome the limitations of the standard Gaussian prior
assumption. In particular, it allows us to impose desired properties like
sparsity or clustering on learned representations, and incorporate prior
information into the learned model. Our approach, which we refer to as the
Intermediary Latent Space VAE (InteL-VAE), is based around controlling the
stochasticity of the encoding process with the intermediary latent variables,
before deterministically mapping them forward to our target latent
representation, from which reconstruction is performed. This allows us to
maintain all the advantages of the traditional VAE framework, while
incorporating desired prior information, inductive biases, and even topological
information through the latent mapping. We show that this, in turn, allows
InteL-VAEs to learn both better generative models and representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Miao_N/0/1/0/all/0/1"&gt;Ning Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mathieu_E/0/1/0/all/0/1"&gt;Emile Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Siddharth_N/0/1/0/all/0/1"&gt;N. Siddharth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Re-parameterizing VAEs for stability. (arXiv:2106.13739v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13739</id>
        <link href="http://arxiv.org/abs/2106.13739"/>
        <updated>2021-06-28T01:57:56.430Z</updated>
        <summary type="html"><![CDATA[We propose a theoretical approach towards the training numerical stability of
Variational AutoEncoders (VAE). Our work is motivated by recent studies
empowering VAEs to reach state of the art generative results on complex image
datasets. These very deep VAE architectures, as well as VAEs using more complex
output distributions, highlight a tendency to haphazardly produce high training
gradients as well as NaN losses. The empirical fixes proposed to train them
despite their limitations are neither fully theoretically grounded nor
generally sufficient in practice. Building on this, we localize the source of
the problem at the interface between the model's neural networks and their
output probabilistic distributions. We explain a common source of instability
stemming from an incautious formulation of the encoded Normal distribution's
variance, and apply the same approach on other, less obvious sources. We show
that by implementing small changes to the way we parameterize the Normal
distributions on which they rely, VAEs can securely be trained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1"&gt;David Dehaene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Brossard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic annotation for computational pathology: Multidisciplinary experience and best practice recommendations. (arXiv:2106.13689v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13689</id>
        <link href="http://arxiv.org/abs/2106.13689"/>
        <updated>2021-06-28T01:57:56.412Z</updated>
        <summary type="html"><![CDATA[Recent advances in whole slide imaging (WSI) technology have led to the
development of a myriad of computer vision and artificial intelligence (AI)
based diagnostic, prognostic, and predictive algorithms. Computational
Pathology (CPath) offers an integrated solution to utilize information embedded
in pathology WSIs beyond what we obtain through visual assessment. For
automated analysis of WSIs and validation of machine learning (ML) models,
annotations at the slide, tissue and cellular levels are required. The
annotation of important visual constructs in pathology images is an important
component of CPath projects. Improper annotations can result in algorithms
which are hard to interpret and can potentially produce inaccurate and
inconsistent results. Despite the crucial role of annotations in CPath
projects, there are no well-defined guidelines or best practices on how
annotations should be carried out. In this paper, we address this shortcoming
by presenting the experience and best practices acquired during the execution
of a large-scale annotation exercise involving a multidisciplinary team of
pathologists, ML experts and researchers as part of the Pathology image data
Lake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a
real-world case study along with examples of different types of annotations,
diagnostic algorithm, annotation data dictionary and annotation constructs. The
analyses reported in this work highlight best practice recommendations that can
be used as annotation guidelines over the lifecycle of a CPath project.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wahab_N/0/1/0/all/0/1"&gt;Noorul Wahab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miligy_I/0/1/0/all/0/1"&gt;Islam M Miligy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dodd_K/0/1/0/all/0/1"&gt;Katherine Dodd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahota_H/0/1/0/all/0/1"&gt;Harvir Sahota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Toss_M/0/1/0/all/0/1"&gt;Michael Toss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wenqi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1"&gt;Mostafa Jahanifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilal_M/0/1/0/all/0/1"&gt;Mohsin Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1"&gt;Simon Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1"&gt;Young Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hadjigeorghiou_G/0/1/0/all/0/1"&gt;Giorgos Hadjigeorghiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhalerao_A/0/1/0/all/0/1"&gt;Abhir Bhalerao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lashen_A/0/1/0/all/0/1"&gt;Ayat Lashen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ibrahim_A/0/1/0/all/0/1"&gt;Asmaa Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katayama_A/0/1/0/all/0/1"&gt;Ayaka Katayama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebili_H/0/1/0/all/0/1"&gt;Henry O Ebili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parkin_M/0/1/0/all/0/1"&gt;Matthew Parkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sorell_T/0/1/0/all/0/1"&gt;Tom Sorell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1"&gt;Shan E Ahmed Raza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hero_E/0/1/0/all/0/1"&gt;Emily Hero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eldaly_H/0/1/0/all/0/1"&gt;Hesham Eldaly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsang_Y/0/1/0/all/0/1"&gt;Yee Wah Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Kishore Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Snead_D/0/1/0/all/0/1"&gt;David Snead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rakha_E/0/1/0/all/0/1"&gt;Emad Rakha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1"&gt;Nasir Rajpoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1"&gt;Fayyaz Minhas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Goal Reinforcement Learning environments for simulated Franka Emika Panda robot. (arXiv:2106.13687v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13687</id>
        <link href="http://arxiv.org/abs/2106.13687"/>
        <updated>2021-06-28T01:57:56.281Z</updated>
        <summary type="html"><![CDATA[This technical report presents panda-gym, a set Reinforcement Learning (RL)
environments for the Franka Emika Panda robot integrated with OpenAI Gym. Five
tasks are included: reach, push, slide, pick & place and stack. They all follow
a Multi-Goal RL framework, allowing to use goal-oriented RL algorithms. To
foster open-research, we chose to use the open-source physics engine PyBullet.
The implementation chosen for this package allows to define very easily new
tasks or new robots. This report also presents a baseline of results obtained
with state-of-the-art model-free off-policy algorithms. panda-gym is
open-source at https://github.com/qgallouedec/panda-gym.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gallouedec_Q/0/1/0/all/0/1"&gt;Quentin Gallou&amp;#xe9;dec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cazin_N/0/1/0/all/0/1"&gt;Nicolas Cazin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dellandrea_E/0/1/0/all/0/1"&gt;Emmanuel Dellandr&amp;#xe9;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effects of boundary conditions in fully convolutional networks for learning spatio-temporal dynamics. (arXiv:2106.11160v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11160</id>
        <link href="http://arxiv.org/abs/2106.11160"/>
        <updated>2021-06-28T01:57:56.274Z</updated>
        <summary type="html"><![CDATA[Accurate modeling of boundary conditions is crucial in computational physics.
The ever increasing use of neural networks as surrogates for physics-related
problems calls for an improved understanding of boundary condition treatment,
and its influence on the network accuracy. In this paper, several strategies to
impose boundary conditions (namely padding, improved spatial context, and
explicit encoding of physical boundaries) are investigated in the context of
fully convolutional networks applied to recurrent tasks. These strategies are
evaluated on two spatio-temporal evolving problems modeled by partial
differential equations: the 2D propagation of acoustic waves (hyperbolic PDE)
and the heat equation (parabolic PDE). Results reveal a high sensitivity of
both accuracy and stability on the boundary implementation in such recurrent
tasks. It is then demonstrated that the choice of the optimal padding strategy
is directly linked to the data semantics. Furthermore, the inclusion of
additional input spatial context or explicit physics-based rules allows a
better handling of boundaries in particular for large number of recurrences,
resulting in more robust and stable neural networks, while facilitating the
design and versatility of such networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alguacil_A/0/1/0/all/0/1"&gt;Antonio Alguacil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_G/0/1/0/all/0/1"&gt;Gon&amp;#xe7;alves Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauerheim_M/0/1/0/all/0/1"&gt;Michael Bauerheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacob_M/0/1/0/all/0/1"&gt;Marc C. Jacob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Moreau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using Tailored Prototypical Neural Networks. (arXiv:2106.13551v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13551</id>
        <link href="http://arxiv.org/abs/2106.13551"/>
        <updated>2021-06-28T01:57:56.261Z</updated>
        <summary type="html"><![CDATA[Glaucoma is one of the leading causes of blindness worldwide and Optical
Coherence Tomography (OCT) is the quintessential imaging technique for its
detection. Unlike most of the state-of-the-art studies focused on glaucoma
detection, in this paper, we propose, for the first time, a novel framework for
glaucoma grading using raw circumpapillary B-scans. In particular, we set out a
new OCT-based hybrid network which combines hand-driven and deep learning
algorithms. An OCT-specific descriptor is proposed to extract hand-crafted
features related to the retinal nerve fibre layer (RNFL). In parallel, an
innovative CNN is developed using skip-connections to include tailored residual
and attention modules to refine the automatic features of the latent space. The
proposed architecture is used as a backbone to conduct a novel few-shot
learning based on static and dynamic prototypical networks. The k-shot paradigm
is redefined giving rise to a supervised end-to-end system which provides
substantial improvements discriminating between healthy, early and advanced
glaucoma samples. The training and evaluation processes of the dynamic
prototypical network are addressed from two fused databases acquired via
Heidelberg Spectralis system. Validation and testing results reach a
categorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively.
Besides, the high performance reported by the proposed model for glaucoma
detection deserves a special mention. The findings from the class activation
maps are directly in line with the clinicians' opinion since the heatmaps
pointed out the RNFL as the most relevant structure for glaucoma diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1"&gt;Gabriel Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Amor_R/0/1/0/all/0/1"&gt;Roc&amp;#xed;o del Amor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Verdu_Monedero_R/0/1/0/all/0/1"&gt;Rafael Verd&amp;#xfa;-Monedero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Morales_Sanchez_J/0/1/0/all/0/1"&gt;Juan Morales-S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realistic molecule optimization on a learned graph manifold. (arXiv:2106.13318v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.13318</id>
        <link href="http://arxiv.org/abs/2106.13318"/>
        <updated>2021-06-28T01:57:56.255Z</updated>
        <summary type="html"><![CDATA[Deep learning based molecular graph generation and optimization has recently
been attracting attention due to its great potential for de novo drug design.
On the one hand, recent models are able to efficiently learn a given graph
distribution, and many approaches have proven very effective to produce a
molecule that maximizes a given score. On the other hand, it was shown by
previous studies that generated optimized molecules are often unrealistic, even
with the inclusion of mechanics to enforce similarity to a dataset of real drug
molecules. In this work we use a hybrid approach, where the dataset
distribution is learned using an autoregressive model while the score
optimization is done using the Metropolis algorithm, biased toward the learned
distribution. We show that the resulting method, that we call learned realism
sampling (LRS), produces empirically more realistic molecules and outperforms
all recent baselines in the task of molecule optimization with similarity
constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Brossard_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Brossard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Frigo_O/0/1/0/all/0/1"&gt;Oriel Frigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Dehaene_D/0/1/0/all/0/1"&gt;David Dehaene&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-training Converts Weak Learners to Strong Learners in Mixture Models. (arXiv:2106.13805v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13805</id>
        <link href="http://arxiv.org/abs/2106.13805"/>
        <updated>2021-06-28T01:57:56.249Z</updated>
        <summary type="html"><![CDATA[We consider a binary classification problem when the data comes from a
mixture of two isotropic distributions satisfying concentration and
anti-concentration properties enjoyed by log-concave distributions among
others. We show that there exists a universal constant $C_{\mathrm{err}}>0$
such that if a pseudolabeler $\boldsymbol{\beta}_{\mathrm{pl}}$ can achieve
classification error at most $C_{\mathrm{err}}$, then for any $\varepsilon>0$,
an iterative self-training algorithm initialized at $\boldsymbol{\beta}_0 :=
\boldsymbol{\beta}_{\mathrm{pl}}$ using pseudolabels $\hat y =
\mathrm{sgn}(\langle \boldsymbol{\beta}_t, \mathbf{x}\rangle)$ and using at
most $\tilde O(d/\varepsilon^2)$ unlabeled examples suffices to learn the
Bayes-optimal classifier up to $\varepsilon$ error, where $d$ is the ambient
dimension. That is, self-training converts weak learners to strong learners
using only unlabeled examples. We additionally show that by running gradient
descent on the logistic loss one can obtain a pseudolabeler
$\boldsymbol{\beta}_{\mathrm{pl}}$ with classification error $C_{\mathrm{err}}$
using only $O(d)$ labeled examples (i.e., independent of $\varepsilon$).
Together our results imply that mixture models can be learned to within
$\varepsilon$ of the Bayes-optimal accuracy using at most $O(d)$ labeled
examples and $\tilde O(d/\varepsilon^2)$ unlabeled examples by way of a
semi-supervised self-training algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1"&gt;Difan Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zixiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Learning for Citation Purpose Classification. (arXiv:2106.13275v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13275</id>
        <link href="http://arxiv.org/abs/2106.13275"/>
        <updated>2021-06-28T01:57:56.242Z</updated>
        <summary type="html"><![CDATA[We present our entry into the 2021 3C Shared Task Citation Context
Classification based on Purpose competition. The goal of the competition is to
classify a citation in a scientific article based on its purpose. This task is
important because it could potentially lead to more comprehensive ways of
summarizing the purpose and uses of scientific articles, but it is also
difficult, mainly due to the limited amount of available training data in which
the purposes of each citation have been hand-labeled, along with the
subjectivity of these labels. Our entry in the competition is a multi-task
model that combines multiple modules designed to handle the problem from
different perspectives, including hand-generated linguistic features, TF-IDF
features, and an LSTM-with-attention model. We also provide an ablation study
and feature analysis whose insights could lead to future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oesterling_A/0/1/0/all/0/1"&gt;Alex Oesterling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_A/0/1/0/all/0/1"&gt;Angikar Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Haoyang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_R/0/1/0/all/0/1"&gt;Rui Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baig_Y/0/1/0/all/0/1"&gt;Yasa Baig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semenova_L/0/1/0/all/0/1"&gt;Lesia Semenova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1"&gt;Cynthia Rudin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Interpretable Criminal Charge Prediction and Algorithmic Bias. (arXiv:2106.13456v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13456</id>
        <link href="http://arxiv.org/abs/2106.13456"/>
        <updated>2021-06-28T01:57:56.217Z</updated>
        <summary type="html"><![CDATA[While predictive policing has become increasingly common in assisting with
decisions in the criminal justice system, the use of these results is still
controversial. Some software based on deep learning lacks accuracy (e.g., in
F-1), and many decision processes are not transparent causing doubt about
decision bias, such as perceived racial, age, and gender disparities. This
paper addresses bias issues with post-hoc explanations to provide a trustable
prediction of whether a person will receive future criminal charges given one's
previous criminal records by learning temporal behavior patterns over twenty
years. Bi-LSTM relieves the vanishing gradient problem, and attentional
mechanisms allows learning and interpretation of feature importance. Our
approach shows consistent and reliable prediction precision and recall on a
real-life dataset. Our analysis of the importance of each input feature shows
the critical causal impact on decision-making, suggesting that criminal
histories are statistically significant factors, while identifiers, such as
race, gender, and age, are not. Finally, our algorithm indicates that a suspect
tends to gradually rather than suddenly increase crime severity level over
time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Abdul Rafae Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varsanyi_P/0/1/0/all/0/1"&gt;Peter Varsanyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pabreja_R/0/1/0/all/0/1"&gt;Rachit Pabreja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masksembles for Uncertainty Estimation. (arXiv:2012.08334v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08334</id>
        <link href="http://arxiv.org/abs/2012.08334"/>
        <updated>2021-06-28T01:57:56.209Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have amply demonstrated their prowess but estimating the
reliability of their predictions remains challenging. Deep Ensembles are widely
considered as being one of the best methods for generating uncertainty
estimates but are very expensive to train and evaluate. MC-Dropout is another
popular alternative, which is less expensive, but also less reliable. Our
central intuition is that there is a continuous spectrum of ensemble-like
models of which MC-Dropout and Deep Ensembles are extreme examples. The first
uses an effectively infinite number of highly correlated models while the
second relies on a finite number of independent models.

To combine the benefits of both, we introduce Masksembles. Instead of
randomly dropping parts of the network as in MC-dropout, Masksemble relies on a
fixed number of binary masks, which are parameterized in a way that allows to
change correlations between individual models. Namely, by controlling the
overlap between the masks and their density one can choose the optimal
configuration for the task at hand. This leads to a simple and easy to
implement method with performance on par with Ensembles at a fraction of the
cost. We experimentally validate Masksembles on two widely used datasets,
CIFAR10 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Durasov_N/0/1/0/all/0/1"&gt;Nikita Durasov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1"&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1"&gt;Pierre Baque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimising simulations for diphoton production at hadron colliders using amplitude neural networks. (arXiv:2106.09474v2 [hep-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09474</id>
        <link href="http://arxiv.org/abs/2106.09474"/>
        <updated>2021-06-28T01:57:56.202Z</updated>
        <summary type="html"><![CDATA[Machine learning technology has the potential to dramatically optimise event
generation and simulations. We continue to investigate the use of neural
networks to approximate matrix elements for high-multiplicity scattering
processes. We focus on the case of loop-induced diphoton production through
gluon fusion and develop a realistic simulation method that can be applied to
hadron collider observables. Neural networks are trained using the one-loop
amplitudes implemented in the NJet C++ library and interfaced to the Sherpa
Monte Carlo event generator where we perform a detailed study for $2\to3$ and
$2\to4$ scattering problems. We also consider how the trained networks perform
when varying the kinematic cuts effecting the phase space and the reliability
of the neural network simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Aylett_Bullock_J/0/1/0/all/0/1"&gt;Joseph Aylett-Bullock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Badger_S/0/1/0/all/0/1"&gt;Simon Badger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Moodie_R/0/1/0/all/0/1"&gt;Ryan Moodie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the (Un-)Avoidability of Adversarial Examples. (arXiv:2106.13326v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13326</id>
        <link href="http://arxiv.org/abs/2106.13326"/>
        <updated>2021-06-28T01:57:56.195Z</updated>
        <summary type="html"><![CDATA[The phenomenon of adversarial examples in deep learning models has caused
substantial concern over their reliability. While many deep neural networks
have shown impressive performance in terms of predictive accuracy, it has been
shown that in many instances an imperceptible perturbation can falsely flip the
network's prediction. Most research has then focused on developing defenses
against adversarial attacks or learning under a worst-case adversarial loss. In
this work, we take a step back and aim to provide a framework for determining
whether a model's label change under small perturbation is justified (and when
it is not). We carefully argue that adversarial robustness should be defined as
a locally adaptive measure complying with the underlying distribution. We then
suggest a definition for an adaptive robust loss, derive an empirical version
of it, and develop a resulting data-augmentation framework. We prove that our
adaptive data-augmentation maintains consistency of 1-nearest neighbor
classification under deterministic labels and provide illustrative empirical
evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Sadia Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urner_R/0/1/0/all/0/1"&gt;Ruth Urner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MDP Playground: A Design and Debug Testbed for Reinforcement Learning. (arXiv:1909.07750v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.07750</id>
        <link href="http://arxiv.org/abs/1909.07750"/>
        <updated>2021-06-28T01:57:56.178Z</updated>
        <summary type="html"><![CDATA[We present \emph{MDP Playground}, an efficient testbed for Reinforcement
Learning (RL) agents with \textit{orthogonal} dimensions that can be controlled
independently to challenge agents in different ways and obtain varying degrees
of hardness in generated environments. We consider and allow control over a
wide variety of dimensions, including \textit{delayed rewards},
\textit{rewardable sequences}, \textit{density of rewards},
\textit{stochasticity}, \textit{image representations}, \textit{irrelevant
features}, \textit{time unit}, \textit{action range} and more. We define a
parameterised collection of fast-to-run toy environments in \textit{OpenAI Gym}
by varying these dimensions and propose to use these for the initial design and
development of agents. We also provide wrappers that inject these dimensions
into complex environments from \textit{Atari} and \textit{Mujoco} to allow for
evaluating agent robustness. We further provide various example use-cases and
instructions on how to use \textit{MDP Playground} to design and debug agents.
We believe that \textit{MDP Playground} is a valuable testbed for researchers
designing new, adaptive and intelligent RL agents and those wanting to unit
test their agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_R/0/1/0/all/0/1"&gt;Raghu Rajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1"&gt;Jessica Lizeth Borja Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guttikonda_S/0/1/0/all/0/1"&gt;Suresh Guttikonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1"&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Biedenkapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartz_J/0/1/0/all/0/1"&gt;Jan Ole von Hartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. (arXiv:2005.05144v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05144</id>
        <link href="http://arxiv.org/abs/2005.05144"/>
        <updated>2021-06-28T01:57:56.172Z</updated>
        <summary type="html"><![CDATA[Speech provides a natural way for human-computer interaction. In particular,
speech synthesis systems are popular in different applications, such as
personal assistants, GPS applications, screen readers and accessibility tools.
However, not all languages are on the same level when in terms of resources and
systems for speech synthesis. This work consists of creating publicly available
resources for Brazilian Portuguese in the form of a novel dataset along with
deep learning models for end-to-end speech synthesis. Such dataset has 10.5
hours from a single speaker, from which a Tacotron 2 model with the RTISI-LA
vocoder presented the best performance, achieving a 4.03 MOS value. The
obtained results are comparable to related works covering English language and
the state-of-the-art in Portuguese.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1"&gt;Edresson Casanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1"&gt;Arnaldo Candido Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1"&gt;Christopher Shulby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_F/0/1/0/all/0/1"&gt;Frederico Santos de Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Teixeira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Teixeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1"&gt;Moacir Antonelli Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1"&gt;Sandra Maria Aluisio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks. (arXiv:2106.13264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13264</id>
        <link href="http://arxiv.org/abs/2106.13264"/>
        <updated>2021-06-28T01:57:56.165Z</updated>
        <summary type="html"><![CDATA[Hypergraphs are used to model higher-order interactions amongst agents and
there exist many practically relevant instances of hypergraph datasets. To
enable efficient processing of hypergraph-structured data, several hypergraph
neural network platforms have been proposed for learning hypergraph properties
and structure, with a special focus on node classification. However, almost all
existing methods use heuristic propagation rules and offer suboptimal
performance on many datasets. We propose AllSet, a new hypergraph neural
network paradigm that represents a highly general framework for (hyper)graph
neural networks and for the first time implements hypergraph neural network
layers as compositions of two multiset functions that can be efficiently
learned for each task and each dataset. Furthermore, AllSet draws on new
connections between hypergraph neural networks and recent advances in deep
learning of multiset functions. In particular, the proposed architecture
utilizes Deep Sets and Set Transformer architectures that allow for significant
modeling flexibility and offer high expressive power. To evaluate the
performance of AllSet, we conduct the most extensive experiments to date
involving ten known benchmarking datasets and three newly curated datasets that
represent significant challenges for hypergraph node classification. The
results demonstrate that AllSet has the unique ability to consistently either
match or outperform all other hypergraph neural networks across the tested
datasets. Our implementation and dataset will be released upon acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1"&gt;Eli Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jianhao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1"&gt;Olgica Milenkovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Graph Classification over Non-IID Graphs. (arXiv:2106.13423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13423</id>
        <link href="http://arxiv.org/abs/2106.13423"/>
        <updated>2021-06-28T01:57:56.158Z</updated>
        <summary type="html"><![CDATA[Federated learning has emerged as an important paradigm for training machine
learning models in different domains. For graph-level tasks such as graph
classification, graphs can also be regarded as a special type of data samples,
which can be collected and stored in separate local systems. Similar to other
domains, multiple local systems, each holding a small set of graphs, may
benefit from collaboratively training a powerful graph mining model, such as
the popular graph neural networks (GNNs). To provide more motivation towards
such endeavors, we analyze real-world graphs from different domains to confirm
that they indeed share certain graph properties that are statistically
significant compared with random graphs. However, we also find that different
sets of graphs, even from the same domain or same dataset, are non-IID
regarding both graph structures and node features. To handle this, we propose a
graph clustering federated learning (GCFL) framework that dynamically finds
clusters of local systems based on the gradients of GNNs, and theoretically
justify that such clusters can reduce the structure and feature heterogeneity
among graphs owned by the local systems. Moreover, we observe the gradients of
GNNs to be rather fluctuating in GCFL which impedes high-quality clustering,
and design a gradient sequence-based clustering mechanism based on dynamic time
warping (GCFL+). Extensive experimental results and in-depth analysis
demonstrate the effectiveness of our proposed frameworks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Han Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Littlestone Classes are Privately Online Learnable. (arXiv:2106.13513v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13513</id>
        <link href="http://arxiv.org/abs/2106.13513"/>
        <updated>2021-06-28T01:57:56.152Z</updated>
        <summary type="html"><![CDATA[We consider the problem of online classification under a privacy constraint.
In this setting a learner observes sequentially a stream of labelled examples
$(x_t, y_t)$, for $1 \leq t \leq T$, and returns at each iteration $t$ a
hypothesis $h_t$ which is used to predict the label of each new example $x_t$.
The learner's performance is measured by her regret against a known hypothesis
class $\mathcal{H}$. We require that the algorithm satisfies the following
privacy constraint: the sequence $h_1, \ldots, h_T$ of hypotheses output by the
algorithm needs to be an $(\epsilon, \delta)$-differentially private function
of the whole input sequence $(x_1, y_1), \ldots, (x_T, y_T)$. We provide the
first non-trivial regret bound for the realizable setting. Specifically, we
show that if the class $\mathcal{H}$ has constant Littlestone dimension then,
given an oblivious sequence of labelled examples, there is a private learner
that makes in expectation at most $O(\log T)$ mistakes -- comparable to the
optimal mistake bound in the non-private case, up to a logarithmic factor.
Moreover, for general values of the Littlestone dimension $d$, the same mistake
bound holds but with a doubly-exponential in $d$ factor. A recent line of work
has demonstrated a strong connection between classes that are online learnable
and those that are differentially-private learnable. Our results strengthen
this connection and show that an online learning algorithm can in fact be
directly privatized (in the realizable setting). We also discuss an adaptive
setting and provide a sublinear regret bound of $O(\sqrt{T})$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1"&gt;Noah Golowich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1"&gt;Roi Livni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A hybrid model-based and learning-based approach for classification using limited number of training samples. (arXiv:2106.13436v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13436</id>
        <link href="http://arxiv.org/abs/2106.13436"/>
        <updated>2021-06-28T01:57:56.131Z</updated>
        <summary type="html"><![CDATA[The fundamental task of classification given a limited number of training
data samples is considered for physical systems with known parametric
statistical models. The standalone learning-based and statistical model-based
classifiers face major challenges towards the fulfillment of the classification
task using a small training set. Specifically, classifiers that solely rely on
the physics-based statistical models usually suffer from their inability to
properly tune the underlying unobservable parameters, which leads to a
mismatched representation of the system's behaviors. Learning-based
classifiers, on the other hand, typically rely on a large number of training
data from the underlying physical process, which might not be feasible in most
practical scenarios. In this paper, a hybrid classification method -- termed
HyPhyLearn -- is proposed that exploits both the physics-based statistical
models and the learning-based classifiers. The proposed solution is based on
the conjecture that HyPhyLearn would alleviate the challenges associated with
the individual approaches of learning-based and statistical model-based
classifiers by fusing their respective strengths. The proposed hybrid approach
first estimates the unobservable model parameters using the available
(suboptimal) statistical estimation procedures, and subsequently use the
physics-based statistical models to generate synthetic data. Then, the training
data samples are incorporated with the synthetic data in a learning-based
classifier that is based on domain-adversarial training of neural networks.
Specifically, in order to address the mismatch problem, the classifier learns a
mapping from the training data and the synthetic data to a common feature
space. Simultaneously, the classifier is trained to find discriminative
features within this space in order to fulfill the classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nooraiepour_A/0/1/0/all/0/1"&gt;Alireza Nooraiepour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajwa_W/0/1/0/all/0/1"&gt;Waheed U. Bajwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandayam_N/0/1/0/all/0/1"&gt;Narayan B. Mandayam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of geophysical properties of rocks on rare well data and attributes of seismic waves by machine learning methods on the example of the Achimov formation. (arXiv:2106.13274v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2106.13274</id>
        <link href="http://arxiv.org/abs/2106.13274"/>
        <updated>2021-06-28T01:57:56.124Z</updated>
        <summary type="html"><![CDATA[Purpose of this research is to forecast the development of sand bodies in
productive sediments based on well log data and seismic attributes. The object
of the study is the productive intervals of Achimov sedimentary complex in the
part of oil field located in Western Siberia. The research shows a
technological stack of machine learning algorithms, methods for enriching the
source data with synthetic ones and algorithms for creating new features. The
result was the model of regression relationship between the values of natural
radioactivity of rocks and seismic wave field attributes with an acceptable
prediction quality. Acceptable quality of the forecast is confirmed both by
model cross validation, and by the data obtained following the results of new
well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Ivlev_D/0/1/0/all/0/1"&gt;Dmitry Ivlev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vulnerability and Transaction behavior based detection of Malicious Smart Contracts. (arXiv:2106.13422v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.13422</id>
        <link href="http://arxiv.org/abs/2106.13422"/>
        <updated>2021-06-28T01:57:56.117Z</updated>
        <summary type="html"><![CDATA[Smart Contracts (SCs) in Ethereum can automate tasks and provide different
functionalities to a user. Such automation is enabled by the `Turing-complete'
nature of the programming language (Solidity) in which SCs are written. This
also opens up different vulnerabilities and bugs in SCs that malicious actors
exploit to carry out malicious or illegal activities on the cryptocurrency
platform. In this work, we study the correlation between malicious activities
and the vulnerabilities present in SCs and find that some malicious activities
are correlated with certain types of vulnerabilities. We then develop and study
the feasibility of a scoring mechanism that corresponds to the severity of the
vulnerabilities present in SCs to determine if it is a relevant feature to
identify suspicious SCs. We analyze the utility of severity score towards
detection of suspicious SCs using unsupervised machine learning (ML) algorithms
across different temporal granularities and identify behavioral changes. In our
experiments with on-chain SCs, we were able to find a total of 1094 benign SCs
across different granularities which behave similar to malicious SCs, with the
inclusion of the smart contract vulnerability scores in the feature set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1"&gt;Rachit Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thapliyal_T/0/1/0/all/0/1"&gt;Tanmay Thapliyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Sandeep Kumar Shukla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15779</id>
        <link href="http://arxiv.org/abs/2007.15779"/>
        <updated>2021-06-28T01:57:56.105Z</updated>
        <summary type="html"><![CDATA[Pretraining large neural language models, such as BERT, has led to impressive
gains on many natural language processing (NLP) tasks. However, most
pretraining efforts focus on general domain corpora, such as newswire and Web.
A prevailing assumption is that even domain-specific pretraining can benefit by
starting from general-domain language models. In this paper, we challenge this
assumption by showing that for domains with abundant unlabeled text, such as
biomedicine, pretraining language models from scratch results in substantial
gains over continual pretraining of general-domain language models. To
facilitate this investigation, we compile a comprehensive biomedical NLP
benchmark from publicly-available datasets. Our experiments show that
domain-specific pretraining serves as a solid foundation for a wide range of
biomedical NLP tasks, leading to new state-of-the-art results across the board.
Further, in conducting a thorough evaluation of modeling choices, both for
pretraining and task-specific fine-tuning, we discover that some common
practices are unnecessary with BERT models, such as using complex tagging
schemes in named entity recognition (NER). To help accelerate research in
biomedical NLP, we have released our state-of-the-art pretrained and
task-specific models for the community, and created a leaderboard featuring our
BLURB benchmark (short for Biomedical Language Understanding & Reasoning
Benchmark) at https://aka.ms/BLURB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1"&gt;Robert Tinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1"&gt;Michael Lucas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1"&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1"&gt;Tristan Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1"&gt;Hoifung Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Robot Deep Reinforcement Learning for Mobile Navigation. (arXiv:2106.13280v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.13280</id>
        <link href="http://arxiv.org/abs/2106.13280"/>
        <updated>2021-06-28T01:57:56.092Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning algorithms require large and diverse datasets in
order to learn successful policies for perception-based mobile navigation.
However, gathering such datasets with a single robot can be prohibitively
expensive. Collecting data with multiple different robotic platforms with
possibly different dynamics is a more scalable approach to large-scale data
collection. But how can deep reinforcement learning algorithms leverage such
heterogeneous datasets? In this work, we propose a deep reinforcement learning
algorithm with hierarchically integrated models (HInt). At training time, HInt
learns separate perception and dynamics models, and at test time, HInt
integrates the two models in a hierarchical manner and plans actions with the
integrated model. This method of planning with hierarchically integrated models
allows the algorithm to train on datasets gathered by a variety of different
platforms, while respecting the physical capabilities of the deployment robot
at test time. Our mobile navigation experiments show that HInt outperforms
conventional hierarchical policies and single-source approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1"&gt;Katie Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kahn_G/0/1/0/all/0/1"&gt;Gregory Kahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Transformer Architecture Search. (arXiv:2106.13700v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13700</id>
        <link href="http://arxiv.org/abs/2106.13700"/>
        <updated>2021-06-28T01:57:56.072Z</updated>
        <summary type="html"><![CDATA[Recently, transformers have shown great superiority in solving computer
vision tasks by modeling images as a sequence of manually-split patches with
self-attention mechanism. However, current architectures of vision transformers
(ViTs) are simply inherited from natural language processing (NLP) tasks and
have not been sufficiently investigated and optimized. In this paper, we make a
further step by examining the intrinsic structure of transformers for vision
tasks and propose an architecture search method, dubbed ViTAS, to search for
the optimal architecture with similar hardware budgets. Concretely, we design a
new effective yet efficient weight sharing paradigm for ViTs, such that
architectures with different token embedding, sequence size, number of heads,
width, and depth can be derived from a single super-transformer. Moreover, to
cater for the variance of distinct architectures, we introduce \textit{private}
class token and self-attention maps in the super-transformer. In addition, to
adapt the searching for different budgets, we propose to search the sampling
probability of identity operation. Experimental results show that our ViTAS
attains excellent results compared to existing pure transformer architectures.
For example, with $1.3$G FLOPs budget, our searched architecture achieves
$74.7\%$ top-$1$ accuracy on ImageNet and is $2.5\%$ superior than the current
baseline ViT architecture. Code is available at
\url{https://github.com/xiusu/ViTAS}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth Estimation. (arXiv:2106.13323v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13323</id>
        <link href="http://arxiv.org/abs/2106.13323"/>
        <updated>2021-06-28T01:57:56.059Z</updated>
        <summary type="html"><![CDATA[Advanced machine learning techniques have been used in remote sensing (RS)
applications such as crop mapping and yield prediction, but remain
under-utilized for tracking crop progress. In this study, we demonstrate the
use of agronomic knowledge of crop growth drivers in a Long Short-Term
Memory-based, Domain-guided neural network (DgNN) for in-season crop progress
estimation. The DgNN uses a branched structure and attention to separate
independent crop growth drivers and capture their varying importance throughout
the growing season. The DgNN is implemented for corn, using RS data in Iowa for
the period 2003-2019, with USDA crop progress reports used as ground truth.
State-wide DgNN performance shows significant improvement over sequential and
dense-only NN structures, and a widely-used Hidden Markov Model method. The
DgNN had a 3.5% higher Nash-Sutfliffe efficiency over all growth stages and 33%
more weeks with highest cosine similarity than the other NNs during test years.
The DgNN and Sequential NN were more robust during periods of abnormal crop
progress, though estimating the Silking-Grainfill transition was difficult for
all methods. Finally, Uniform Manifold Approximation and Projection
visualizations of layer activations showed how LSTM-based NNs separate crop
growth time-series differently from a dense-only structure. Results from this
study exhibit both the viability of NNs in crop growth stage estimation (CGSE)
and the benefits of using domain knowledge. The DgNN methodology presented here
can be extended to provide near-real time CGSE of other crops.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Worrall_G/0/1/0/all/0/1"&gt;George Worrall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1"&gt;Anand Rangarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1"&gt;Jasmeet Judge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joslim: Joint Widths and Weights Optimization for Slimmable Neural Networks. (arXiv:2007.11752v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11752</id>
        <link href="http://arxiv.org/abs/2007.11752"/>
        <updated>2021-06-28T01:57:56.052Z</updated>
        <summary type="html"><![CDATA[Slimmable neural networks provide a flexible trade-off front between
prediction error and computational requirement (such as the number of
floating-point operations or FLOPs) with the same storage requirement as a
single model. They are useful for reducing maintenance overhead for deploying
models to devices with different memory constraints and are useful for
optimizing the efficiency of a system with many CNNs. However, existing
slimmable network approaches either do not optimize layer-wise widths or
optimize the shared-weights and layer-wise widths independently, thereby
leaving significant room for improvement by joint width and weight
optimization. In this work, we propose a general framework to enable joint
optimization for both width configurations and weights of slimmable networks.
Our framework subsumes conventional and NAS-based slimmable methods as special
cases and provides flexibility to improve over existing methods. From a
practical standpoint, we propose Joslim, an algorithm that jointly optimizes
both the widths and weights for slimmable nets, which outperforms existing
methods for optimizing slimmable networks across various networks, datasets,
and objectives. Quantitatively, improvements up to 1.7% and 8% in top-1
accuracy on the ImageNet dataset can be attained for MobileNetV2 considering
FLOPs and memory footprint, respectively. Our results highlight the potential
of optimizing the channel counts for different layers jointly with the weights
for slimmable networks. Code available at https://github.com/cmu-enyac/Joslim.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1"&gt;Ting-Wu Chin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1"&gt;Ari S. Morcos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1"&gt;Diana Marculescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A variational autoencoder approach for choice set generation and implicit perception of alternatives in choice modeling. (arXiv:2106.13319v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.13319</id>
        <link href="http://arxiv.org/abs/2106.13319"/>
        <updated>2021-06-28T01:57:56.042Z</updated>
        <summary type="html"><![CDATA[This paper derives the generalized extreme value (GEV) model with implicit
availability/perception (IAP) of alternatives and proposes a variational
autoencoder (VAE) approach for choice set generation and implicit perception of
alternatives. Specifically, the cross-nested logit (CNL) model with IAP is
derived as an example of IAP-GEV models. The VAE approach is adapted to model
the choice set generation process, in which the likelihood of perceiving chosen
alternatives in the choice set is maximized. The VAE approach for route choice
set generation is exemplified using a real dataset. IAP- CNL model estimated
has the best performance in terms of goodness-of-fit and prediction
performance, compared to multinomial logit models and conventional choice set
generation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1"&gt;Rui Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekhor_S/0/1/0/all/0/1"&gt;Shlomo Bekhor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote. (arXiv:2106.13624v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13624</id>
        <link href="http://arxiv.org/abs/2106.13624"/>
        <updated>2021-06-28T01:57:56.035Z</updated>
        <summary type="html"><![CDATA[We present a new second-order oracle bound for the expected risk of a
weighted majority vote. The bound is based on a novel parametric form of the
Chebyshev-Cantelli inequality (a.k.a.\ one-sided Chebyshev's), which is
amenable to efficient minimization. The new form resolves the optimization
challenge faced by prior oracle bounds based on the Chebyshev-Cantelli
inequality, the C-bounds [Germain et al., 2015], and, at the same time, it
improves on the oracle bound based on second order Markov's inequality
introduced by Masegosa et al. [2020]. We also derive the PAC-Bayes-Bennett
inequality, which we use for empirical estimation of the oracle bound. The
PAC-Bayes-Bennett inequality improves on the PAC-Bayes-Bernstein inequality by
Seldin et al. [2012]. We provide an empirical evaluation demonstrating that the
new bounds can improve on the work by Masegosa et al. [2020]. Both the
parametric form of the Chebyshev-Cantelli inequality and the PAC-Bayes-Bennett
inequality may be of independent interest for the study of concentration of
measure in other domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi-Shan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masegosa_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s R. Masegosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzen_S/0/1/0/all/0/1"&gt;Stephan S. Lorenzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1"&gt;Christian Igel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seldin_Y/0/1/0/all/0/1"&gt;Yevgeny Seldin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phoneme-aware and Channel-wise Attentive Learning for Text DependentSpeaker Verification. (arXiv:2106.13514v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13514</id>
        <link href="http://arxiv.org/abs/2106.13514"/>
        <updated>2021-06-28T01:57:56.011Z</updated>
        <summary type="html"><![CDATA[This paper proposes a multi-task learning network with phoneme-aware and
channel-wise attentive learning strategies for text-dependent Speaker
Verification (SV). In the proposed structure, the frame-level multi-task
learning along with the segment-level adversarial learning is adopted for
speaker embedding extraction. The phoneme-aware attentive pooling is exploited
on frame-level features in the main network for speaker classifier, with the
corresponding posterior probability for the phoneme distribution in the
auxiliary subnet. Further, the introduction of Squeeze and Excitation
(SE-block) performs dynamic channel-wise feature recalibration, which improves
the representational ability. The proposed method exploits speaker
idiosyncrasies associated with pass-phrases, and is further improved by the
phoneme-aware attentive pooling and SE-block from temporal and channel-wise
aspects, respectively. The experiments conducted on RSR2015 Part 1 database
confirm that the proposed system achieves outstanding results for textdependent
SV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1"&gt;Qingyang Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Promises and Pitfalls of Black-Box Concept Learning Models. (arXiv:2106.13314v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13314</id>
        <link href="http://arxiv.org/abs/2106.13314"/>
        <updated>2021-06-28T01:57:56.005Z</updated>
        <summary type="html"><![CDATA[Machine learning models that incorporate concept learning as an intermediate
step in their decision making process can match the performance of black-box
predictive models while retaining the ability to explain outcomes in human
understandable terms. However, we demonstrate that the concept representations
learned by these models encode information beyond the pre-defined concepts, and
that natural mitigation strategies do not fully work, rendering the
interpretation of the downstream prediction misleading. We describe the
mechanism underlying the information leakage and suggest recourse for
mitigating its effects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahinpei_A/0/1/0/all/0/1"&gt;Anita Mahinpei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1"&gt;Justin Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lage_I/0/1/0/all/0/1"&gt;Isaac Lage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1"&gt;Finale Doshi-Velez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Weiwei Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13393</id>
        <link href="http://arxiv.org/abs/2106.13393"/>
        <updated>2021-06-28T01:57:55.995Z</updated>
        <summary type="html"><![CDATA[Self-Rating Depression Scale (SDS) questionnaire has frequently been used for
efficient depression preliminary screening. However, the uncontrollable
self-administered measure can be easily affected by insouciantly or deceptively
answering, and producing the different results with the clinician-administered
Hamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,
facial expression (FE) and actions play a vital role in clinician-administered
evaluation, while FE and action are underexplored for self-administered
evaluations. In this work, we collect a novel dataset of 200 subjects to
evidence the validity of self-rating questionnaires with their corresponding
question-wise video recording. To automatically interpret depression from the
SDS evaluation and the paired video, we propose an end-to-end hierarchical
framework for the long-term variable-length video, which is also conditioned on
the questionnaire results and the answering time. Specifically, we resort to a
hierarchical model which utilizes a 3D CNN for local temporal pattern
exploration and a redundancy-aware self-attention (RAS) scheme for
question-wise global feature aggregation. Targeting for the redundant long-term
FE video processing, our RAS is able to effectively exploit the correlations of
each video clip within a question set to emphasize the discriminative
information and eliminate the redundancy based on feature pair-wise affinity.
Then, the question-wise video feature is concatenated with the questionnaire
scores for final depression detection. Our thorough evaluations also show the
validity of fusing SDS evaluation and its video recording, and the superiority
of our framework to the conventional state-of-the-art temporal modeling
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wanqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Lizhong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hui Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalCity: Complex Simulations with Agency for Causal Discovery and Reasoning. (arXiv:2106.13364v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.13364</id>
        <link href="http://arxiv.org/abs/2106.13364"/>
        <updated>2021-06-28T01:57:55.987Z</updated>
        <summary type="html"><![CDATA[The ability to perform causal and counterfactual reasoning are central
properties of human intelligence. Decision-making systems that can perform
these types of reasoning have the potential to be more generalizable and
interpretable. Simulations have helped advance the state-of-the-art in this
domain, by providing the ability to systematically vary parameters (e.g.,
confounders) and generate examples of the outcomes in the case of
counterfactual scenarios. However, simulating complex temporal causal events in
multi-agent scenarios, such as those that exist in driving and vehicle
navigation, is challenging. To help address this, we present a high-fidelity
simulation environment that is designed for developing algorithms for causal
discovery and counterfactual reasoning in the safety-critical context. A core
component of our work is to introduce \textit{agency}, such that it is simple
to define and create complex scenarios using high-level definitions. The
vehicles then operate with agency to complete these objectives, meaning
low-level behaviors need only be controlled if necessary. We perform
experiments with three state-of-the-art methods to create baselines and
highlight the affordances of this environment. Finally, we highlight challenges
and opportunities for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1"&gt;Daniel McDuff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yale Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jiyoung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1"&gt;Vibhav Vineet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1"&gt;Sai Vemprala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gyde_N/0/1/0/all/0/1"&gt;Nicholas Gyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1"&gt;Hadi Salman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shuang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kwanghoon Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1"&gt;Ashish Kapoor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Binary Matrix Factorisation and Completion via Integer Programming. (arXiv:2106.13434v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.13434</id>
        <link href="http://arxiv.org/abs/2106.13434"/>
        <updated>2021-06-28T01:57:55.979Z</updated>
        <summary type="html"><![CDATA[Binary matrix factorisation is an essential tool for identifying discrete
patterns in binary data. In this paper we consider the rank-k binary matrix
factorisation problem (k-BMF) under Boolean arithmetic: we are given an n x m
binary matrix X with possibly missing entries and need to find two binary
matrices A and B of dimension n x k and k x m respectively, which minimise the
distance between X and the Boolean product of A and B in the squared Frobenius
distance. We present a compact and two exponential size integer programs (IPs)
for k-BMF and show that the compact IP has a weak LP relaxation, while the
exponential size LPs have a stronger equivalent LP relaxation. We introduce a
new objective function, which differs from the traditional squared Frobenius
objective in attributing a weight to zero entries of the input matrix that is
proportional to the number of times the zero is erroneously covered in a rank-k
factorisation. For one of the exponential size IPs we describe a computational
approach based on column generation. Experimental results on synthetic and real
word datasets suggest that our integer programming approach is competitive
against available methods for k-BMF and provides accurate low-error
factorisations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kovacs_R/0/1/0/all/0/1"&gt;Reka A. Kovacs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gunluk_O/0/1/0/all/0/1"&gt;Oktay Gunluk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hauser_R/0/1/0/all/0/1"&gt;Raphael A. Hauser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What will it take to generate fairness-preserving explanations?. (arXiv:2106.13346v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13346</id>
        <link href="http://arxiv.org/abs/2106.13346"/>
        <updated>2021-06-28T01:57:55.961Z</updated>
        <summary type="html"><![CDATA[In situations where explanations of black-box models may be useful, the
fairness of the black-box is also often a relevant concern. However, the link
between the fairness of the black-box model and the behavior of explanations
for the black-box is unclear. We focus on explanations applied to tabular
datasets, suggesting that explanations do not necessarily preserve the fairness
properties of the black-box algorithm. In other words, explanation algorithms
can ignore or obscure critical relevant properties, creating incorrect or
misleading explanations. More broadly, we propose future research directions
for evaluating and generating explanations such that they are informative and
relevant from a fairness perspective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jessica Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Sohini Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1"&gt;Stephen H. Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multivariate Medians for Image and Shape Analysis. (arXiv:1911.00143v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.00143</id>
        <link href="http://arxiv.org/abs/1911.00143"/>
        <updated>2021-06-28T01:57:55.954Z</updated>
        <summary type="html"><![CDATA[Having been studied since long by statisticians, multivariate median concepts
found their way into the image processing literature in the course of the last
decades, being used to construct robust and efficient denoising filters for
multivariate images such as colour images but also matrix-valued images. Based
on the similarities between image and geometric data as results of the sampling
of continuous physical quantities, it can be expected that the understanding of
multivariate median filters for images provides a starting point for the
development of shape processing techniques. This paper presents an overview of
multivariate median concepts relevant for image and shape processing. It
focusses on their mathematical principles and discusses important properties
especially in the context of image processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Welk_M/0/1/0/all/0/1"&gt;Martin Welk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized One-Class Learning Using Pairs of Complementary Classifiers. (arXiv:2106.13272v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13272</id>
        <link href="http://arxiv.org/abs/2106.13272"/>
        <updated>2021-06-28T01:57:55.949Z</updated>
        <summary type="html"><![CDATA[One-class learning is the classic problem of fitting a model to the data for
which annotations are available only for a single class. In this paper, we
explore novel objectives for one-class learning, which we collectively refer to
as Generalized One-class Discriminative Subspaces (GODS). Our key idea is to
learn a pair of complementary classifiers to flexibly bound the one-class data
distribution, where the data belongs to the positive half-space of one of the
classifiers in the complementary pair and to the negative half-space of the
other. To avoid redundancy while allowing non-linearity in the classifier
decision surfaces, we propose to design each classifier as an orthonormal frame
and seek to learn these frames via jointly optimizing for two conflicting
objectives, namely: i) to minimize the distance between the two frames, and ii)
to maximize the margin between the frames and the data. The learned orthonormal
frames will thus characterize a piecewise linear decision surface that allows
for efficient inference, while our objectives seek to bound the data within a
minimal volume that maximizes the decision margin, thereby robustly capturing
the data distribution. We explore several variants of our formulation under
different constraints on the constituent classifiers, including kernelized
feature maps. We demonstrate the empirical benefits of our approach via
experiments on data from several applications in computer vision, such as
anomaly detection in video sequences, human poses, and human activities. We
also explore the generality and effectiveness of GODS for non-vision tasks via
experiments on several UCI datasets, demonstrating state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1"&gt;Anoop Cherian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jue Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models. (arXiv:2106.13353v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13353</id>
        <link href="http://arxiv.org/abs/2106.13353"/>
        <updated>2021-06-28T01:57:55.943Z</updated>
        <summary type="html"><![CDATA[Prompting language models (LMs) with training examples and task descriptions
has been seen as critical to recent successes in few-shot learning. In this
work, we show that finetuning LMs in the few-shot setting can considerably
reduce the need for prompt engineering. In fact, one can use null prompts,
prompts that contain neither task-specific templates nor training examples, and
achieve competitive accuracy to manually-tuned prompts across a wide range of
tasks. While finetuning LMs does introduce new parameters for each downstream
task, we show that this memory overhead can be substantially reduced:
finetuning only the bias terms can achieve comparable or better accuracy than
standard finetuning while only updating 0.1% of the parameters. All in all, we
recommend finetuning LMs for few-shot learning as it is more accurate, robust
to different prompts, and can be made nearly as efficient as using frozen LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1"&gt;Robert L. Logan IV&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balazevic_I/0/1/0/all/0/1"&gt;Ivana Bala&amp;#x17e;evi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1"&gt;Fabio Petroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1"&gt;Sebastian Riedel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-based multi-parameter mapping. (arXiv:2102.01604v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01604</id>
        <link href="http://arxiv.org/abs/2102.01604"/>
        <updated>2021-06-28T01:57:55.936Z</updated>
        <summary type="html"><![CDATA[Quantitative MR imaging is increasingly favoured for its richer information
content and standardised measures. However, computing quantitative parameter
maps, such as those encoding longitudinal relaxation rate (R1), apparent
transverse relaxation rate (R2*) or magnetisation-transfer saturation (MTsat),
involves inverting a highly non-linear function. Many methods for deriving
parameter maps assume perfect measurements and do not consider how noise is
propagated through the estimation procedure, resulting in needlessly noisy
maps. Instead, we propose a probabilistic generative (forward) model of the
entire dataset, which is formulated and inverted to jointly recover (log)
parameter maps with a well-defined probabilistic interpretation (e.g., maximum
likelihood or maximum a posteriori). The second order optimisation we propose
for model fitting achieves rapid and stable convergence thanks to a novel
approximate Hessian. We demonstrate the utility of our flexible framework in
the context of recovering more accurate maps from data acquired using the
popular multi-parameter mapping protocol. We also show how to incorporate a
joint total variation prior to further decrease the noise in the maps, noting
that the probabilistic formulation allows the uncertainty on the recovered
parameter maps to be estimated. Our implementation uses a PyTorch backend and
benefits from GPU acceleration. It is available at
https://github.com/balbasty/nitorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balbastre_Y/0/1/0/all/0/1"&gt;Yael Balbastre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brudfors_M/0/1/0/all/0/1"&gt;Mikael Brudfors&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azzarito_M/0/1/0/all/0/1"&gt;Michela Azzarito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambert_C/0/1/0/all/0/1"&gt;Christian Lambert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callaghan_M/0/1/0/all/0/1"&gt;Martina F. Callaghan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashburner_J/0/1/0/all/0/1"&gt;John Ashburner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disease Progression Modeling Workbench 360. (arXiv:2106.13265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13265</id>
        <link href="http://arxiv.org/abs/2106.13265"/>
        <updated>2021-06-28T01:57:55.919Z</updated>
        <summary type="html"><![CDATA[In this work we introduce Disease Progression Modeling workbench 360 (DPM360)
opensource clinical informatics framework for collaborative research and
delivery of healthcare AI. DPM360, when fully developed, will manage the entire
modeling life cycle, from data analysis (e.g., cohort identification) to
machine learning algorithm development and prototyping. DPM360 augments the
advantages of data model standardization and tooling (OMOP-CDM, Athena, ATLAS)
provided by the widely-adopted OHDSI initiative with a powerful machine
learning training framework, and a mechanism for rapid prototyping through
automatic deployment of models as containerized services to a cloud
environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suryanarayanan_P/0/1/0/all/0/1"&gt;Parthasarathy Suryanarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1"&gt;Prithwish Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madan_P/0/1/0/all/0/1"&gt;Piyush Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bore_K/0/1/0/all/0/1"&gt;Kibichii Bore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogallo_W/0/1/0/all/0/1"&gt;William Ogallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rachita Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghalwash_M/0/1/0/all/0/1"&gt;Mohamed Ghalwash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buleje_I/0/1/0/all/0/1"&gt;Italo Buleje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remy_S/0/1/0/all/0/1"&gt;Sekou Remy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahatma_S/0/1/0/all/0/1"&gt;Shilpa Mahatma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_P/0/1/0/all/0/1"&gt;Pablo Meyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jianying Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate Speech Detection in Clubhouse. (arXiv:2106.13238v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13238</id>
        <link href="http://arxiv.org/abs/2106.13238"/>
        <updated>2021-06-28T01:57:55.907Z</updated>
        <summary type="html"><![CDATA[With high prevalence of offensive language against the minorities in social
media, counter hate speech generation is considered as an automatic way to
tackle this challenge. The counter hate speeches are supposed to appear as a
third voice to educate people and keep the social red lines bold without
limiting the freedom of speech principles. The counter hate speech generation
is based on the optimistic assumption that, any attempt to intervene the hate
speeches in social media can play a positive role in this context. Beyond that,
previous works ignored to investigate the sequence of comments before and after
counter speech. To the best of our knowledge, no attempt has been made to
measure the counter hate speech impact from statistical point of view. In this
paper, we take the first step in this direction by measuring the counter hate
speech impact on the next comments in terms of Google Perspective Scores.
Furthermore, our experiments show that, counter hate speech can cause negative
impacts, a phenomena which is called aggression in social media.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansourifar_H/0/1/0/all/0/1"&gt;Hadi Mansourifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alsagheer_D/0/1/0/all/0/1"&gt;Dana Alsagheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fathi_R/0/1/0/all/0/1"&gt;Reza Fathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weidong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1"&gt;Lan Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometric learning of the conformational dynamics of molecules using dynamic graph neural networks. (arXiv:2106.13277v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13277</id>
        <link href="http://arxiv.org/abs/2106.13277"/>
        <updated>2021-06-28T01:57:55.901Z</updated>
        <summary type="html"><![CDATA[We apply a temporal edge prediction model for weighted dynamic graphs to
predict time-dependent changes in molecular structure. Each molecule is
represented as a complete graph in which each atom is a vertex and all vertex
pairs are connected by an edge weighted by the Euclidean distance between atom
pairs. We ingest a sequence of complete molecular graphs into a dynamic graph
neural network (GNN) to predict the graph at the next time step. Our dynamic
GNN predicts atom-to-atom distances with a mean absolute error of 0.017 \r{A},
which is considered ``chemically accurate'' for molecular simulations. We also
explored the transferability of a trained network to new molecular systems and
found that finetuning with less than 10% of the total trajectory provides a
mean absolute error of the same order of magnitude as that when training from
scratch on the full molecular trajectory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashby_M/0/1/0/all/0/1"&gt;Michael Hunter Ashby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilbrey_J/0/1/0/all/0/1"&gt;Jenna A. Bilbrey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Gradual Argumentation Frameworks using Genetic Algorithms. (arXiv:2106.13585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13585</id>
        <link href="http://arxiv.org/abs/2106.13585"/>
        <updated>2021-06-28T01:57:55.884Z</updated>
        <summary type="html"><![CDATA[Gradual argumentation frameworks represent arguments and their relationships
in a weighted graph. Their graphical structure and intuitive semantics makes
them a potentially interesting tool for interpretable machine learning. It has
been noted recently that their mechanics are closely related to neural
networks, which allows learning their weights from data by standard deep
learning frameworks. As a first proof of concept, we propose a genetic
algorithm to simultaneously learn the structure of argumentative classification
models. To obtain a well interpretable model, the fitness function balances
sparseness and accuracy of the classifier. We discuss our algorithm and present
first experimental results on standard benchmarks from the UCI machine learning
repository. Our prototype learns argumentative classification models that are
comparable to decision trees in terms of learning performance and
interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spieler_J/0/1/0/all/0/1"&gt;Jonathan Spieler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1"&gt;Nico Potyka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1"&gt;Steffen Staab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerated Computation of a High Dimensional Kolmogorov-Smirnov Distance. (arXiv:2106.13706v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2106.13706</id>
        <link href="http://arxiv.org/abs/2106.13706"/>
        <updated>2021-06-28T01:57:55.839Z</updated>
        <summary type="html"><![CDATA[Statistical testing is widespread and critical for a variety of scientific
disciplines. The advent of machine learning and the increase of computing power
has increased the interest in the analysis and statistical testing of
multidimensional data. We extend the powerful Kolmogorov-Smirnov two sample
test to a high dimensional form in a similar manner to Fasano (Fasano, 1987).
We call our result the d-dimensional Kolmogorov-Smirnov test (ddKS) and provide
three novel contributions therewith: we develop an analytical equation for the
significance of a given ddKS score, we provide an algorithm for computation of
ddKS on modern computing hardware that is of constant time complexity for small
sample sizes and dimensions, and we provide two approximate calculations of
ddKS: one that reduces the time complexity to linear at larger sample sizes,
and another that reduces the time complexity to linear with increasing
dimension. We perform power analysis of ddKS and its approximations on a corpus
of datasets and compare to other common high dimensional two sample tests and
distances: Hotelling's T^2 test and Kullback-Leibler divergence. Our ddKS test
performs well for all datasets, dimensions, and sizes tested, whereas the other
tests and distances fail to reject the null hypothesis on at least one dataset.
We therefore conclude that ddKS is a powerful multidimensional two sample test
for general use, and can be calculated in a fast and efficient manner using our
parallel or approximate methods. Open source implementations of all methods
described in this work are located at https://github.com/pnnl/ddks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hagen_A/0/1/0/all/0/1"&gt;Alex Hagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jackson_S/0/1/0/all/0/1"&gt;Shane Jackson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kahn_J/0/1/0/all/0/1"&gt;James Kahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Strube_J/0/1/0/all/0/1"&gt;Jan Strube&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Haide_I/0/1/0/all/0/1"&gt;Isabel Haide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pazdernik_K/0/1/0/all/0/1"&gt;Karl Pazdernik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hainje_C/0/1/0/all/0/1"&gt;Connor Hainje&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limitations of machine learning for building energy prediction: ASHRAE Great Energy Predictor III Kaggle competition error analysis. (arXiv:2106.13475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13475</id>
        <link href="http://arxiv.org/abs/2106.13475"/>
        <updated>2021-06-28T01:57:55.796Z</updated>
        <summary type="html"><![CDATA[Machine learning for building energy prediction has exploded in popularity in
recent years, yet understanding its limitations and potential for improvement
are lacking. The ASHRAE Great Energy Predictor III (GEPIII) Kaggle competition
was the largest building energy meter machine learning competition ever held
with 4,370 participants who submitted 39,403 predictions. The test data set
included two years of hourly electricity, hot water, chilled water, and steam
readings from 2,380 meters in 1,448 buildings at 16 locations. This paper
analyzes the various sources and types of residual model error from an
aggregation of the competition's top 50 solutions. This analysis reveals the
limitations for machine learning using the standard model inputs of historical
meter, weather, and basic building metadata. The types of error are classified
according to the amount of time errors occur in each instance, abrupt versus
gradual behavior, the magnitude of error, and whether the error existed on
single buildings or several buildings at once from a single location. The
results show machine learning models have errors within a range of
acceptability on 79.1% of the test data. Lower magnitude model errors occur in
16.1% of the test data. These discrepancies can likely be addressed through
additional training data sources or innovations in machine learning. Higher
magnitude errors occur in 4.8% of the test data and are unlikely to be
accurately predicted regardless of innovation. There is a diversity of error
behavior depending on the energy meter type (electricity prediction models have
unacceptable error in under 10% of test data, while hot water is over 60%) and
building use type (public service less than 14%, while technology/science is
just over 46%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1"&gt;Clayton Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picchetti_B/0/1/0/all/0/1"&gt;Bianca Picchetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Chun Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantelic_J/0/1/0/all/0/1"&gt;Jovan Pantelic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Inference in High-Dimensional Time-Serieswith the Orthogonal Stochastic Linear Mixing Model. (arXiv:2106.13379v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13379</id>
        <link href="http://arxiv.org/abs/2106.13379"/>
        <updated>2021-06-28T01:57:55.788Z</updated>
        <summary type="html"><![CDATA[Many modern time-series datasets contain large numbers of output response
variables sampled for prolonged periods of time. For example, in neuroscience,
the activities of 100s-1000's of neurons are recorded during behaviors and in
response to sensory stimuli. Multi-output Gaussian process models leverage the
nonparametric nature of Gaussian processes to capture structure across multiple
outputs. However, this class of models typically assumes that the correlations
between the output response variables are invariant in the input space.
Stochastic linear mixing models (SLMM) assume the mixture coefficients depend
on input, making them more flexible and effective to capture complex output
dependence. However, currently, the inference for SLMMs is intractable for
large datasets, making them inapplicable to several modern time-series
problems. In this paper, we propose a new regression framework, the orthogonal
stochastic linear mixing model (OSLMM) that introduces an orthogonal constraint
amongst the mixing coefficients. This constraint reduces the computational
burden of inference while retaining the capability to handle complex output
dependence. We provide Markov chain Monte Carlo inference procedures for both
SLMM and OSLMM and demonstrate superior model scalability and reduced
prediction error of OSLMM compared with state-of-the-art methods on several
real-world applications. In neurophysiology recordings, we use the inferred
latent functions for compact visualization of population responses to auditory
stimuli, and demonstrate superior results compared to a competing method
(GPFA). Together, these results demonstrate that OSLMM will be useful for the
analysis of diverse, large-scale time-series datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1"&gt;Rui Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1"&gt;Kristofer Bouchard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multifidelity Modeling for Physics-Informed Neural Networks (PINNs). (arXiv:2106.13361v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.13361</id>
        <link href="http://arxiv.org/abs/2106.13361"/>
        <updated>2021-06-28T01:57:55.781Z</updated>
        <summary type="html"><![CDATA[Multifidelity simulation methodologies are often used in an attempt to
judiciously combine low-fidelity and high-fidelity simulation results in an
accuracy-increasing, cost-saving way. Candidates for this approach are
simulation methodologies for which there are fidelity differences connected
with significant computational cost differences. Physics-informed Neural
Networks (PINNs) are candidates for these types of approaches due to the
significant difference in training times required when different fidelities
(expressed in terms of architecture width and depth as well as optimization
criteria) are employed. In this paper, we propose a particular multifidelity
approach applied to PINNs that exploits low-rank structure. We demonstrate that
width, depth, and optimization criteria can be used as parameters related to
model fidelity, and show numerical justification of cost differences in
training due to fidelity parameter choices. We test our multifidelity scheme on
various canonical forward PDE models that have been presented in the emerging
PINNs literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Penwarden_M/0/1/0/all/0/1"&gt;Michael Penwarden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhe_S/0/1/0/all/0/1"&gt;Shandian Zhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Narayan_A/0/1/0/all/0/1"&gt;Akil Narayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kirby_R/0/1/0/all/0/1"&gt;Robert M. Kirby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09862</id>
        <link href="http://arxiv.org/abs/2106.09862"/>
        <updated>2021-06-28T01:57:55.764Z</updated>
        <summary type="html"><![CDATA[Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly
used to visualize and quantify left atrial (LA) scars. The position and extent
of scars provide important information of the pathophysiology and progression
of atrial fibrillation (AF). Hence, LA scar segmentation and quantification
from LGE MRI can be useful in computer-assisted diagnosis and treatment
stratification of AF patients. Since manual delineation can be time-consuming
and subject to intra- and inter-expert variability, automating this computing
is highly desired, which nevertheless is still challenging and
under-researched.

This paper aims to provide a systematic review on computing methods for LA
cavity, wall, scar and ablation gap segmentation and quantification from LGE
MRI, and the related literature for AF studies. Specifically, we first
summarize AF-related imaging techniques, particularly LGE MRI. Then, we review
the methodologies of the four computing tasks in detail, and summarize the
validation strategies applied in each task. Finally, the possible future
developments are outlined, with a brief survey on the potential clinical
applications of the aforementioned methods. The review shows that the research
into this topic is still in early stages. Although several methods have been
proposed, especially for LA segmentation, there is still large scope for
further algorithmic developments due to performance issues related to the high
variability of enhancement appearance and differences in image acquisition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1"&gt;Veronika A. Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1"&gt;Julia A. Schnabel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Noisy Client Learning. (arXiv:2106.13239v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13239</id>
        <link href="http://arxiv.org/abs/2106.13239"/>
        <updated>2021-06-28T01:57:55.744Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) collaboratively aggregates a shared global model
depending on multiple local clients, while keeping the training data
decentralized in order to preserve data privacy. However, standard FL methods
ignore the noisy client issue, which may harm the overall performance of the
aggregated model. In this paper, we first analyze the noisy client statement,
and then model noisy clients with different noise distributions (e.g.,
Bernoulli and truncated Gaussian distributions). To learn with noisy clients,
we propose a simple yet effective FL framework, named Federated Noisy Client
Learning (Fed-NCL), which is a plug-and-play algorithm and contains two main
components: a data quality measurement (DQM) to dynamically quantify the data
quality of each participating client, and a noise robust aggregation (NRA) to
adaptively aggregate the local models of each client by jointly considering the
amount of local training data and the data quality of each client. Our Fed-NCL
can be easily applied in any standard FL workflow to handle the noisy client
issue. Experimental results on various datasets demonstrate that our algorithm
boosts the performances of different state-of-the-art systems with noisy
clients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Li Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Cheng-Zhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13301</id>
        <link href="http://arxiv.org/abs/2106.13301"/>
        <updated>2021-06-28T01:57:55.721Z</updated>
        <summary type="html"><![CDATA[Physics perception very often faces the problem that only limited data or
partial measurements on the scene are available. In this work, we propose a
strategy to learn the full state of sloshing liquids from measurements of the
free surface. Our approach is based on recurrent neural networks (RNN) that
project the limited information available to a reduced-order manifold so as to
not only reconstruct the unknown information, but also to be capable of
performing fluid reasoning about future scenarios in real time. To obtain
physically consistent predictions, we train deep neural networks on the
reduced-order manifold that, through the employ of inductive biases, ensure the
fulfillment of the principles of thermodynamics. RNNs learn from history the
required hidden information to correlate the limited information with the
latent space where the simulation occurs. Finally, a decoder returns data back
to the high-dimensional manifold, so as to provide the user with insightful
information in the form of augmented reality. This algorithm is connected to a
computer vision system to test the performance of the proposed methodology with
real information, resulting in a system capable of understanding and predicting
future states of the observed fluid in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1"&gt;Beatriz Moya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1"&gt;Alberto Badias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1"&gt;David Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1"&gt;Francisco Chinesta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1"&gt;Elias Cueto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13797</id>
        <link href="http://arxiv.org/abs/2106.13797"/>
        <updated>2021-06-28T01:57:55.715Z</updated>
        <summary type="html"><![CDATA[Transformer in computer vision has recently shown encouraging progress. In
this work, we improve the original Pyramid Vision Transformer (PVTv1) by adding
three improvement designs, which include (1) locally continuous features with
convolutions, (2) position encodings with zero paddings, and (3) linear
complexity attention layers with average pooling. With these simple
modifications, our PVTv2 significantly improves PVTv1 on classification,
detection, and segmentation. Moreover, PVTv2 achieves much better performance
than recent works, including Swin Transformer, under ImageNet-1K pre-training.
We hope this work will make state-of-the-art vision Transformer research more
accessible. Code is available at https://github.com/whai362/PVT .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1"&gt;Kaitao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Ding Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. (arXiv:2103.03231v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03231</id>
        <link href="http://arxiv.org/abs/2103.03231"/>
        <updated>2021-06-28T01:57:55.677Z</updated>
        <summary type="html"><![CDATA[The recent research explosion around implicit neural representations, such as
NeRF, shows that there is immense potential for implicitly storing high-quality
scene and lighting information in compact neural networks. However, one major
limitation preventing the use of NeRF in real-time rendering applications is
the prohibitive computational cost of excessive network evaluations along each
view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural
representations closer to practical rendering of synthetic content in real-time
applications, such as games and virtual reality. We show that the number of
samples required for each view ray can be significantly reduced when samples
are placed around surfaces in the scene without compromising image quality. To
this end, we propose a depth oracle network that predicts ray sample locations
for each view ray with a single network evaluation. We show that using a
classification network around logarithmically discretized and spherically
warped depth values is essential to encode surface locations rather than
directly estimating depth. The combination of these techniques leads to DONeRF,
our compact dual network design with a depth oracle network as its first step
and a locally sampled shading network for ray accumulation. With DONeRF, we
reduce the inference costs by up to 48x compared to NeRF when conditioning on
available ground truth depth information. Compared to concurrent acceleration
methods for raymarching-based neural representations, DONeRF does not require
additional memory for explicit caching or acceleration structures, and can
render interactively (20 frames per second) on a single GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neff_T/0/1/0/all/0/1"&gt;Thomas Neff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stadlbauer_P/0/1/0/all/0/1"&gt;Pascal Stadlbauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parger_M/0/1/0/all/0/1"&gt;Mathias Parger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurz_A/0/1/0/all/0/1"&gt;Andreas Kurz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1"&gt;Joerg H. Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaitanya_C/0/1/0/all/0/1"&gt;Chakravarty R. Alla Chaitanya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplanyan_A/0/1/0/all/0/1"&gt;Anton Kaplanyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinberger_M/0/1/0/all/0/1"&gt;Markus Steinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings. (arXiv:2106.13302v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13302</id>
        <link href="http://arxiv.org/abs/2106.13302"/>
        <updated>2021-06-28T01:57:55.663Z</updated>
        <summary type="html"><![CDATA[This article introduces byteSteady -- a fast model for classification using
byte-level n-gram embeddings. byteSteady assumes that each input comes as a
sequence of bytes. A representation vector is produced using the averaged
embedding vectors of byte-level n-grams, with a pre-defined set of n. The
hashing trick is used to reduce the number of embedding vectors. This input
representation vector is then fed into a linear classifier. A straightforward
application of byteSteady is text classification. We also apply byteSteady to
one type of non-language data -- DNA sequences for gene classification. For
both problems we achieved competitive classification results against strong
baselines, suggesting that byteSteady can be applied to both language and
non-language data. Furthermore, we find that simple compression using Huffman
coding does not significantly impact the results, which offers an
accuracy-speed trade-off previously unexplored in machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1"&gt;Alexandre Drouin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Raymond Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post Selections Using Test Sets (PSUTS) and How Developmental Networks Avoid Them. (arXiv:2106.13233v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13233</id>
        <link href="http://arxiv.org/abs/2106.13233"/>
        <updated>2021-06-28T01:57:55.645Z</updated>
        <summary type="html"><![CDATA[This paper raises a rarely reported practice in Artificial Intelligence (AI)
called Post Selection Using Test Sets (PSUTS). Consequently, the popular
error-backprop methodology in deep learning lacks an acceptable generalization
power. All AI methods fall into two broad schools, connectionist and symbolic.
The PSUTS fall into two kinds, machine PSUTS and human PSUTS. The connectionist
school received criticisms for its "scruffiness" due to a huge number of
network parameters and now the worse machine PSUTS; but the seemingly "clean"
symbolic school seems more brittle because of a weaker generalization power
using human PSUTS. This paper formally defines what PSUTS is, analyzes why
error-backprop methods with random initial weights suffer from severe local
minima, why PSUTS violates well-established research ethics, and how every
paper that used PSUTS should have at least transparently reported PSUTS. For
improved transparency in future publications, this paper proposes a new
standard for performance evaluation of AI, called developmental errors for all
networks trained, along with Three Learning Conditions: (1) an incremental
learning architecture, (2) a training experience and (3) a limited amount of
computational resources. Developmental Networks avoid PSUTS and are not
"scruffy" because they drive Emergent Turing Machines and are optimal in the
sense of maximum-likelihood across lifetime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1"&gt;Juyang Weng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single and Union Non-parallel Support Vector Machine Frameworks. (arXiv:1910.09734v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09734</id>
        <link href="http://arxiv.org/abs/1910.09734"/>
        <updated>2021-06-28T01:57:55.627Z</updated>
        <summary type="html"><![CDATA[Considering the classification problem, we summarize the nonparallel support
vector machines with the nonparallel hyperplanes to two types of frameworks.
The first type constructs the hyperplanes separately. It solves a series of
small optimization problems to obtain a series of hyperplanes, but is hard to
measure the loss of each sample. The other type constructs all the hyperplanes
simultaneously, and it solves one big optimization problem with the ascertained
loss of each sample. We give the characteristics of each framework and compare
them carefully. In addition, based on the second framework, we construct a
max-min distance-based nonparallel support vector machine for multiclass
classification problem, called NSVM. It constructs hyperplanes with large
distance margin by solving an optimization problem. Experimental results on
benchmark data sets show the advantages of our NSVM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Na Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yuan-Hai Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huajun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yu-Ting Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Ling-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiu_N/0/1/0/all/0/1"&gt;Naihua Xiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1"&gt;Nai-Yang Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance-Aware Private Mean Estimation Without Private Covariance Estimation. (arXiv:2106.13329v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13329</id>
        <link href="http://arxiv.org/abs/2106.13329"/>
        <updated>2021-06-28T01:57:55.585Z</updated>
        <summary type="html"><![CDATA[We present two sample-efficient differentially private mean estimators for
$d$-dimensional (sub)Gaussian distributions with unknown covariance.
Informally, given $n \gtrsim d/\alpha^2$ samples from such a distribution with
mean $\mu$ and covariance $\Sigma$, our estimators output $\tilde\mu$ such that
$\| \tilde\mu - \mu \|_{\Sigma} \leq \alpha$, where $\| \cdot \|_{\Sigma}$ is
the Mahalanobis distance. All previous estimators with the same guarantee
either require strong a priori bounds on the covariance matrix or require
$\Omega(d^{3/2})$ samples.

Each of our estimators is based on a simple, general approach to designing
differentially private mechanisms, but with novel technical steps to make the
estimator private and sample-efficient. Our first estimator samples a point
with approximately maximum Tukey depth using the exponential mechanism, but
restricted to the set of points of large Tukey depth. Proving that this
mechanism is private requires a novel analysis. Our second estimator perturbs
the empirical mean of the data set with noise calibrated to the empirical
covariance, without releasing the covariance itself. Its sample complexity
guarantees hold more generally for subgaussian distributions, albeit with a
slightly worse dependence on the privacy parameter. For both estimators,
careful preprocessing of the data is required to satisfy differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1"&gt;Gavin Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1"&gt;Marco Gaboardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1"&gt;Adam Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1"&gt;Jonathan Ullman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakynthinou_L/0/1/0/all/0/1"&gt;Lydia Zakynthinou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-and-Under Complete Convolutional RNN for MRI Reconstruction. (arXiv:2106.08886v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08886</id>
        <link href="http://arxiv.org/abs/2106.08886"/>
        <updated>2021-06-28T01:57:55.574Z</updated>
        <summary type="html"><![CDATA[Reconstructing magnetic resonance (MR) images from undersampled data is a
challenging problem due to various artifacts introduced by the under-sampling
operation. Recent deep learning-based methods for MR image reconstruction
usually leverage a generic auto-encoder architecture which captures low-level
features at the initial layers and high-level features at the deeper layers.
Such networks focus much on global features which may not be optimal to
reconstruct the fully-sampled image. In this paper, we propose an
Over-and-Under Complete Convolutional Recurrent Neural Network (OUCR), which
consists of an overcomplete and an undercomplete Convolutional Recurrent Neural
Network(CRNN). The overcomplete branch gives special attention in learning
local structures by restraining the receptive field of the network. Combining
it with the undercomplete branch leads to a network which focuses more on
low-level features without losing out on the global structures. Extensive
experiments on two datasets demonstrate that the proposed method achieves
significant improvements over the compressed sensing and popular deep
learning-based methods with less number of trainable parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1"&gt;Pengfei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1"&gt;Puyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shanshan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior Image-Constrained Reconstruction using Style-Based Generative Models. (arXiv:2102.12525v2 [eess.IV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2102.12525</id>
        <link href="http://arxiv.org/abs/2102.12525"/>
        <updated>2021-06-28T01:57:55.566Z</updated>
        <summary type="html"><![CDATA[Obtaining a useful estimate of an object from highly incomplete imaging
measurements remains a holy grail of imaging science. Deep learning methods
have shown promise in learning object priors or constraints to improve the
conditioning of an ill-posed imaging inverse problem. In this study, a
framework for estimating an object of interest that is semantically related to
a known prior image, is proposed. An optimization problem is formulated in the
disentangled latent space of a style-based generative model, and semantically
meaningful constraints are imposed using the disentangled latent representation
of the prior image. Stable recovery from incomplete measurements with the help
of a prior image is theoretically analyzed. Numerical experiments demonstrating
the superior performance of our approach as compared to related methods are
presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1"&gt;Varun A. Kelkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1"&gt;Mark A. Anastasio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation Using Physics-Based Data Augmentation. (arXiv:2103.05690v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05690</id>
        <link href="http://arxiv.org/abs/2103.05690"/>
        <updated>2021-06-28T01:57:55.546Z</updated>
        <summary type="html"><![CDATA[In current clinical practice, noisy and artifact-ridden weekly cone-beam
computed tomography (CBCT) images are only used for patient setup during
radiotherapy. Treatment planning is done once at the beginning of the treatment
using high-quality planning CT (pCT) images and manual contours for
organs-at-risk (OARs) structures. If the quality of the weekly CBCT images can
be improved while simultaneously segmenting OAR structures, this can provide
critical information for adapting radiotherapy mid-treatment as well as for
deriving biomarkers for treatment response. Using a novel physics-based data
augmentation strategy, we synthesize a large dataset of perfectly/inherently
registered planning CT and synthetic-CBCT pairs for locally advanced lung
cancer patient cohort, which are then used in a multitask 3D deep learning
framework to simultaneously segment and translate real weekly CBCT images to
high-quality planning CT-like images. We compared the synthetic CT and OAR
segmentations generated by the model to real planning CT and manual OAR
segmentations and showed promising results. The real week 1 (baseline) CBCT
images which had an average MAE of 162.77 HU compared to pCT images are
translated to synthetic CT images that exhibit a drastically improved average
MAE of 29.31 HU and average structural similarity of 92% with the pCT images.
The average DICE scores of the 3D organs-at-risk segmentations are: lungs 0.96,
heart 0.88, spinal cord 0.83 and esophagus 0.66. This approach could allow
clinicians to adjust treatment plans using only the routine low-quality CBCT
images, potentially improving patient outcomes. Our code, data, and pre-trained
models will be made available via our physics-based data augmentation library,
Physics-ArX, at https://github.com/nadeemlab/Physics-ArX.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_N/0/1/0/all/0/1"&gt;Navdeep Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1"&gt;Sadegh R Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengpeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Si-Yuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1"&gt;Anthony Yezzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1"&gt;Saad Nadeem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Projection-wise Disentangling for Fair and Interpretable Representation Learning: Application to 3D Facial Shape Analysis. (arXiv:2106.13734v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13734</id>
        <link href="http://arxiv.org/abs/2106.13734"/>
        <updated>2021-06-28T01:57:55.532Z</updated>
        <summary type="html"><![CDATA[Confounding bias is a crucial problem when applying machine learning to
practice, especially in clinical practice. We consider the problem of learning
representations independent to multiple biases. In literature, this is mostly
solved by purging the bias information from learned representations. We however
expect this strategy to harm the diversity of information in the
representation, and thus limiting its prospective usage (e.g., interpretation).
Therefore, we propose to mitigate the bias while keeping almost all information
in the latent representations, which enables us to observe and interpret them
as well. To achieve this, we project latent features onto a learned vector
direction, and enforce the independence between biases and projected features
rather than all learned features. To interpret the mapping between projected
features and input data, we propose projection-wise disentangling: a sampling
and reconstruction along the learned vector direction. The proposed method was
evaluated on the analysis of 3D facial shape and patient characteristics
(N=5011). Experiments showed that this conceptually simple method achieved
state-of-the-art fair prediction performance and interpretability, showing its
great potential for clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bron_E/0/1/0/all/0/1"&gt;Esther Bron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessen_W/0/1/0/all/0/1"&gt;Wiro Niessen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolvius_E/0/1/0/all/0/1"&gt;Eppo Wolvius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roshchupkin_G/0/1/0/all/0/1"&gt;Gennady Roshchupkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Multi-level Stroke Control for Neural Style Transfer. (arXiv:2106.13787v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13787</id>
        <link href="http://arxiv.org/abs/2106.13787"/>
        <updated>2021-06-28T01:57:55.447Z</updated>
        <summary type="html"><![CDATA[We present StyleTune, a mobile app for interactive multi-level control of
neural style transfers that facilitates creative adjustments of style elements
and enables high output fidelity. In contrast to current mobile neural style
transfer apps, StyleTune supports users to adjust both the size and orientation
of style elements, such as brushstrokes and texture patches, on a global as
well as local level. To this end, we propose a novel stroke-adaptive
feed-forward style transfer network, that enables control over stroke size and
intensity and allows a larger range of edits than current approaches. For
additional level-of-control, we propose a network agnostic method for
stroke-orientation adjustment by utilizing the rotation-variance of CNNs. To
achieve high output fidelity, we further add a patch-based style transfer
method that enables users to obtain output resolutions of more than 20
Megapixel. Our approach empowers users to create many novel results that are
not possible with current mobile neural style transfer apps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reimann_M/0/1/0/all/0/1"&gt;Max Reimann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buchheim_B/0/1/0/all/0/1"&gt;Benito Buchheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semmo_A/0/1/0/all/0/1"&gt;Amir Semmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dollner_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen D&amp;#xf6;llner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1"&gt;Matthias Trapp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiFace: A Generic Training Mechanism for Boosting Face Recognition Performance. (arXiv:2101.09899v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09899</id>
        <link href="http://arxiv.org/abs/2101.09899"/>
        <updated>2021-06-28T01:57:55.435Z</updated>
        <summary type="html"><![CDATA[Deep Convolutional Neural Networks (DCNNs) and their variants have been
widely used in large scale face recognition(FR) recently. Existing methods have
achieved good performance on many FR benchmarks. However, most of them suffer
from two major problems. First, these methods converge quite slowly since they
optimize the loss functions in a high-dimensional and sparse Gaussian Sphere.
Second, the high dimensionality of features, despite the powerful descriptive
ability, brings difficulty to the optimization, which may lead to a sub-optimal
local optimum. To address these problems, we propose a simple yet efficient
training mechanism called MultiFace, where we approximate the original
high-dimensional features by the ensemble of low-dimensional features. The
proposed mechanism is also generic and can be easily applied to many advanced
FR models. Moreover, it brings the benefits of good interpretability to FR
models via the clustering effect. In detail, the ensemble of these
low-dimensional features can capture complementary yet discriminative
information, which can increase the intra-class compactness and inter-class
separability. Experimental results show that the proposed mechanism can
accelerate 2-3 times with the softmax loss and 1.2-1.5 times with Arcface or
Cosface, while achieving state-of-the-art performances in several benchmark
datasets. Especially, the significant improvements on large-scale
datasets(e.g., IJB and MageFace) demonstrate the flexibility of our new
training mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tszhang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_K/0/1/0/all/0/1"&gt;Kun Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic annotation for computational pathology: Multidisciplinary experience and best practice recommendations. (arXiv:2106.13689v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13689</id>
        <link href="http://arxiv.org/abs/2106.13689"/>
        <updated>2021-06-28T01:57:55.427Z</updated>
        <summary type="html"><![CDATA[Recent advances in whole slide imaging (WSI) technology have led to the
development of a myriad of computer vision and artificial intelligence (AI)
based diagnostic, prognostic, and predictive algorithms. Computational
Pathology (CPath) offers an integrated solution to utilize information embedded
in pathology WSIs beyond what we obtain through visual assessment. For
automated analysis of WSIs and validation of machine learning (ML) models,
annotations at the slide, tissue and cellular levels are required. The
annotation of important visual constructs in pathology images is an important
component of CPath projects. Improper annotations can result in algorithms
which are hard to interpret and can potentially produce inaccurate and
inconsistent results. Despite the crucial role of annotations in CPath
projects, there are no well-defined guidelines or best practices on how
annotations should be carried out. In this paper, we address this shortcoming
by presenting the experience and best practices acquired during the execution
of a large-scale annotation exercise involving a multidisciplinary team of
pathologists, ML experts and researchers as part of the Pathology image data
Lake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a
real-world case study along with examples of different types of annotations,
diagnostic algorithm, annotation data dictionary and annotation constructs. The
analyses reported in this work highlight best practice recommendations that can
be used as annotation guidelines over the lifecycle of a CPath project.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wahab_N/0/1/0/all/0/1"&gt;Noorul Wahab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miligy_I/0/1/0/all/0/1"&gt;Islam M Miligy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dodd_K/0/1/0/all/0/1"&gt;Katherine Dodd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahota_H/0/1/0/all/0/1"&gt;Harvir Sahota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Toss_M/0/1/0/all/0/1"&gt;Michael Toss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wenqi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1"&gt;Mostafa Jahanifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilal_M/0/1/0/all/0/1"&gt;Mohsin Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1"&gt;Simon Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1"&gt;Young Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hadjigeorghiou_G/0/1/0/all/0/1"&gt;Giorgos Hadjigeorghiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhalerao_A/0/1/0/all/0/1"&gt;Abhir Bhalerao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lashen_A/0/1/0/all/0/1"&gt;Ayat Lashen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ibrahim_A/0/1/0/all/0/1"&gt;Asmaa Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katayama_A/0/1/0/all/0/1"&gt;Ayaka Katayama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebili_H/0/1/0/all/0/1"&gt;Henry O Ebili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parkin_M/0/1/0/all/0/1"&gt;Matthew Parkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sorell_T/0/1/0/all/0/1"&gt;Tom Sorell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1"&gt;Shan E Ahmed Raza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hero_E/0/1/0/all/0/1"&gt;Emily Hero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eldaly_H/0/1/0/all/0/1"&gt;Hesham Eldaly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsang_Y/0/1/0/all/0/1"&gt;Yee Wah Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Kishore Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Snead_D/0/1/0/all/0/1"&gt;David Snead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rakha_E/0/1/0/all/0/1"&gt;Emad Rakha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1"&gt;Nasir Rajpoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1"&gt;Fayyaz Minhas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Document Image Classification Using Region-Based Graph Neural Network. (arXiv:2106.13802v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13802</id>
        <link href="http://arxiv.org/abs/2106.13802"/>
        <updated>2021-06-28T01:57:55.419Z</updated>
        <summary type="html"><![CDATA[Document image classification remains a popular research area because it can
be commercialized in many enterprise applications across different industries.
Recent advancements in large pre-trained computer vision and language models
and graph neural networks has lent document image classification many tools.
However using large pre-trained models usually requires substantial computing
resources which could defeat the cost-saving advantages of automatic document
image classification. In the paper we propose an efficient document image
classification framework that uses graph convolution neural networks and
incorporates textual, visual and layout information of the document. We have
rigorously benchmarked our proposed algorithm against several state-of-art
vision and language models on both publicly available dataset and a real-life
insurance document classification dataset. Empirical results on both publicly
available and real-world data show that our methods achieve near SOTA
performance yet require much less computing resources and time for model
training and inference. This results in solutions than offer better cost
advantages, especially in scalable deployment for enterprise applications. The
results showed that our algorithm can achieve classification performance quite
close to SOTA. We also provide comprehensive comparisons of computing
resources, model sizes, train and inference time between our proposed methods
and baselines. In addition we delineate the cost per image using our method and
other baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mandivarapu_J/0/1/0/all/0/1"&gt;Jaya Krishna Mandivarapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bunch_E/0/1/0/all/0/1"&gt;Eric Bunch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1"&gt;Qian You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1"&gt;Glenn Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Re-parameterizing VAEs for stability. (arXiv:2106.13739v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13739</id>
        <link href="http://arxiv.org/abs/2106.13739"/>
        <updated>2021-06-28T01:57:55.399Z</updated>
        <summary type="html"><![CDATA[We propose a theoretical approach towards the training numerical stability of
Variational AutoEncoders (VAE). Our work is motivated by recent studies
empowering VAEs to reach state of the art generative results on complex image
datasets. These very deep VAE architectures, as well as VAEs using more complex
output distributions, highlight a tendency to haphazardly produce high training
gradients as well as NaN losses. The empirical fixes proposed to train them
despite their limitations are neither fully theoretically grounded nor
generally sufficient in practice. Building on this, we localize the source of
the problem at the interface between the model's neural networks and their
output probabilistic distributions. We explain a common source of instability
stemming from an incautious formulation of the encoded Normal distribution's
variance, and apply the same approach on other, less obvious sources. We show
that by implementing small changes to the way we parameterize the Normal
distributions on which they rely, VAEs can securely be trained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1"&gt;David Dehaene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Brossard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis. (arXiv:2103.11078v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11078</id>
        <link href="http://arxiv.org/abs/2103.11078"/>
        <updated>2021-06-28T01:57:55.390Z</updated>
        <summary type="html"><![CDATA[Generating high-fidelity talking head video by fitting with the input audio
sequence is a challenging problem that receives considerable attentions
recently. In this paper, we address this problem with the aid of neural scene
representation networks. Our method is completely different from existing
methods that rely on intermediate representations like 2D landmarks or 3D face
models to bridge the gap between audio input and video output. Specifically,
the feature of input audio signal is directly fed into a conditional implicit
function to generate a dynamic neural radiance field, from which a
high-fidelity talking-head video corresponding to the audio signal is
synthesized using volume rendering. Another advantage of our framework is that
not only the head (with hair) region is synthesized as previous methods did,
but also the upper body is generated via two individual neural radiance fields.
Experimental results demonstrate that our novel framework can (1) produce
high-fidelity and natural results, and (2) support free adjustment of audio
signals, viewing directions, and background images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yudong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Keyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Sen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongjin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hujun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Juyong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Moment Retrieval with Text Query Considering Many-to-Many Correspondence Using Potentially Relevant Pair. (arXiv:2106.13566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13566</id>
        <link href="http://arxiv.org/abs/2106.13566"/>
        <updated>2021-06-28T01:57:55.383Z</updated>
        <summary type="html"><![CDATA[In this paper we undertake the task of text-based video moment retrieval from
a corpus of videos. To train the model, text-moment paired datasets were used
to learn the correct correspondences. In typical training methods, ground-truth
text-moment pairs are used as positive pairs, whereas other pairs are regarded
as negative pairs. However, aside from the ground-truth pairs, some text-moment
pairs should be regarded as positive. In this case, one text annotation can be
positive for many video moments. Conversely, one video moment can be
corresponded to many text annotations. Thus, there are many-to-many
correspondences between the text annotations and video moments. Based on these
correspondences, we can form potentially relevant pairs, which are not given as
ground truth yet are not negative; effectively incorporating such relevant
pairs into training can improve the retrieval performance. The text query
should describe what is happening in a video moment. Hence, different video
moments annotated with similar texts, which contain a similar action, are
likely to hold the similar action, thus these pairs can be considered as
potentially relevant pairs. In this paper, we propose a novel training method
that takes advantage of potentially relevant pairs, which are detected based on
linguistic analysis about text annotation. Experiments on two benchmark
datasets revealed that our method improves the retrieval performance both
quantitatively and qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maeoki_S/0/1/0/all/0/1"&gt;Sho Maeoki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1"&gt;Yusuke Mukuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image-to-image Transformation with Auxiliary Condition. (arXiv:2106.13696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13696</id>
        <link href="http://arxiv.org/abs/2106.13696"/>
        <updated>2021-06-28T01:57:55.361Z</updated>
        <summary type="html"><![CDATA[The performance of image recognition like human pose detection, trained with
simulated images would usually get worse due to the divergence between real and
simulated data. To make the distribution of a simulated image close to that of
real one, there are several works applying GAN-based image-to-image
transformation methods, e.g., SimGAN and CycleGAN. However, these methods would
not be sensitive enough to the various change in pose and shape of subjects,
especially when the training data are imbalanced, e.g., some particular poses
and shapes are minor in the training data. To overcome this problem, we propose
to introduce the label information of subjects, e.g., pose and type of objects
in the training of CycleGAN, and lead it to obtain label-wise transforamtion
models. We evaluate our proposed method called Label-CycleGAN, through
experiments on the digit image transformation from SVHN to MNIST and the
surveillance camera image transformation from simulated to real images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1"&gt;Robert Leer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roma_H/0/1/0/all/0/1"&gt;Hessi Roma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amelia_J/0/1/0/all/0/1"&gt;James Amelia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Pose and Shape Estimation for Category-level 3D Object Perception. (arXiv:2104.08383v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08383</id>
        <link href="http://arxiv.org/abs/2104.08383"/>
        <updated>2021-06-28T01:57:55.354Z</updated>
        <summary type="html"><![CDATA[We consider a category-level perception problem, where one is given 3D sensor
data picturing an object of a given category (e.g. a car), and has to
reconstruct the pose and shape of the object despite intra-class variability
(i.e. different car models have different shapes). We consider an active shape
model, where -- for an object category -- we are given a library of potential
CAD models describing objects in that category, and we adopt a standard
formulation where pose and shape estimation are formulated as a non-convex
optimization. Our first contribution is to provide the first certifiably
optimal solver for pose and shape estimation. In particular, we show that
rotation estimation can be decoupled from the estimation of the object
translation and shape, and we demonstrate that (i) the optimal object rotation
can be computed via a tight (small-size) semidefinite relaxation, and (ii) the
translation and shape parameters can be computed in closed-form given the
rotation. Our second contribution is to add an outlier rejection layer to our
solver, hence making it robust to a large number of misdetections. Towards this
goal, we wrap our optimal solver in a robust estimation scheme based on
graduated non-convexity. To further enhance robustness to outliers, we also
develop the first graph-theoretic formulation to prune outliers in
category-level perception, which removes outliers via convex hull and maximum
clique computations; the resulting approach is robust to 70%-90% outliers. Our
third contribution is an extensive experimental evaluation. Besides providing
an ablation study on a simulated dataset and on the PASCAL3D+ dataset, we
combine our solver with a deep-learned keypoint detector, and show that the
resulting approach improves over the state of the art in vehicle pose
estimation in the ApolloScape datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jingnan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Heng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1"&gt;Luca Carlone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Self-Learning Framework for Bladder Cancer Grading Using Histopathological Images. (arXiv:2106.13559v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13559</id>
        <link href="http://arxiv.org/abs/2106.13559"/>
        <updated>2021-06-28T01:57:55.346Z</updated>
        <summary type="html"><![CDATA[Recently, bladder cancer has been significantly increased in terms of
incidence and mortality. Currently, two subtypes are known based on tumour
growth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC).
In this work, we focus on the MIBC subtype because it is of the worst prognosis
and can spread to adjacent organs. We present a self-learning framework to
grade bladder cancer from histological images stained via immunohistochemical
techniques. Specifically, we propose a novel Deep Convolutional Embedded
Attention Clustering (DCEAC) which allows classifying histological patches into
different severity levels of the disease, according to the patterns established
in the literature. The proposed DCEAC model follows a two-step fully
unsupervised learning methodology to discern between non-tumour, mild and
infiltrative patterns from high-resolution samples of 512x512 pixels. Our
system outperforms previous clustering-based methods by including a
convolutional attention module, which allows refining the features of the
latent space before the classification stage. The proposed network exceeds
state-of-the-art approaches by 2-3% across different metrics, achieving a final
average accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported
class activation maps evidence that our model is able to learn by itself the
same patterns that clinicians consider relevant, without incurring prior
annotation steps. This fact supposes a breakthrough in muscle-invasive bladder
cancer grading which bridges the gap with respect to train the model on
labelled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1"&gt;Gabriel Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Esteve_A/0/1/0/all/0/1"&gt;Anna Esteve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramos_D/0/1/0/all/0/1"&gt;David Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Zero Shot" Point Cloud Upsampling. (arXiv:2106.13765v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13765</id>
        <link href="http://arxiv.org/abs/2106.13765"/>
        <updated>2021-06-28T01:57:55.328Z</updated>
        <summary type="html"><![CDATA[Point cloud upsampling using deep learning has been paid various efforts in
the past few years. Recent supervised deep learning methods are restricted to
the size of training data and is limited in terms of covering all shapes of
point clouds. Besides, the acquisition of such amount of data is unrealistic,
and the network generally performs less powerful than expected on unseen
records. In this paper, we present an unsupervised approach to upsample point
clouds internally referred as "Zero Shot" Point Cloud Upsampling (ZSPU) at
holistic level. Our approach is solely based on the internal information
provided by a particular point cloud without patching in both self-training and
testing phases. This single-stream design significantly reduces the training
time of the upsampling task, by learning the relation between low-resolution
(LR) point clouds and their high (original) resolution (HR) counterparts. This
association will provide super-resolution (SR) outputs when original point
clouds are loaded as input. We demonstrate competitive performance on benchmark
point cloud datasets when compared to other upsampling methods. Furthermore,
ZSPU achieves superior qualitative results on shapes with complex local details
or high curvatures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kaiyue Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1"&gt;Ming Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arslanturk_S/0/1/0/all/0/1"&gt;Suzan Arslanturk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Transformer Architecture Search. (arXiv:2106.13700v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13700</id>
        <link href="http://arxiv.org/abs/2106.13700"/>
        <updated>2021-06-28T01:57:55.295Z</updated>
        <summary type="html"><![CDATA[Recently, transformers have shown great superiority in solving computer
vision tasks by modeling images as a sequence of manually-split patches with
self-attention mechanism. However, current architectures of vision transformers
(ViTs) are simply inherited from natural language processing (NLP) tasks and
have not been sufficiently investigated and optimized. In this paper, we make a
further step by examining the intrinsic structure of transformers for vision
tasks and propose an architecture search method, dubbed ViTAS, to search for
the optimal architecture with similar hardware budgets. Concretely, we design a
new effective yet efficient weight sharing paradigm for ViTs, such that
architectures with different token embedding, sequence size, number of heads,
width, and depth can be derived from a single super-transformer. Moreover, to
cater for the variance of distinct architectures, we introduce \textit{private}
class token and self-attention maps in the super-transformer. In addition, to
adapt the searching for different budgets, we propose to search the sampling
probability of identity operation. Experimental results show that our ViTAS
attains excellent results compared to existing pure transformer architectures.
For example, with $1.3$G FLOPs budget, our searched architecture achieves
$74.7\%$ top-$1$ accuracy on ImageNet and is $2.5\%$ superior than the current
baseline ViT architecture. Code is available at
\url{https://github.com/xiusu/ViTAS}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Pattern Loss based Diversified Attention Network for Cross-Modal Retrieval. (arXiv:2106.13552v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13552</id>
        <link href="http://arxiv.org/abs/2106.13552"/>
        <updated>2021-06-28T01:57:55.288Z</updated>
        <summary type="html"><![CDATA[Cross-modal retrieval aims to enable flexible retrieval experience by
combining multimedia data such as image, video, text, and audio. One core of
unsupervised approaches is to dig the correlations among different object
representations to complete satisfied retrieval performance without requiring
expensive labels. In this paper, we propose a Graph Pattern Loss based
Diversified Attention Network(GPLDAN) for unsupervised cross-modal retrieval to
deeply analyze correlations among representations. First, we propose a
diversified attention feature projector by considering the interaction between
different representations to generate multiple representations of an instance.
Then, we design a novel graph pattern loss to explore the correlations among
different representations, in this graph all possible distances between
different representations are considered. In addition, a modality classifier is
added to explicitly declare the corresponding modalities of features before
fusion and guide the network to enhance discrimination ability. We test GPLDAN
on four public datasets. Compared with the state-of-the-art cross-modal
retrieval methods, the experimental results demonstrate the performance and
competitiveness of GPLDAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xueying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1"&gt;Yibing Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using Tailored Prototypical Neural Networks. (arXiv:2106.13551v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13551</id>
        <link href="http://arxiv.org/abs/2106.13551"/>
        <updated>2021-06-28T01:57:55.282Z</updated>
        <summary type="html"><![CDATA[Glaucoma is one of the leading causes of blindness worldwide and Optical
Coherence Tomography (OCT) is the quintessential imaging technique for its
detection. Unlike most of the state-of-the-art studies focused on glaucoma
detection, in this paper, we propose, for the first time, a novel framework for
glaucoma grading using raw circumpapillary B-scans. In particular, we set out a
new OCT-based hybrid network which combines hand-driven and deep learning
algorithms. An OCT-specific descriptor is proposed to extract hand-crafted
features related to the retinal nerve fibre layer (RNFL). In parallel, an
innovative CNN is developed using skip-connections to include tailored residual
and attention modules to refine the automatic features of the latent space. The
proposed architecture is used as a backbone to conduct a novel few-shot
learning based on static and dynamic prototypical networks. The k-shot paradigm
is redefined giving rise to a supervised end-to-end system which provides
substantial improvements discriminating between healthy, early and advanced
glaucoma samples. The training and evaluation processes of the dynamic
prototypical network are addressed from two fused databases acquired via
Heidelberg Spectralis system. Validation and testing results reach a
categorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively.
Besides, the high performance reported by the proposed model for glaucoma
detection deserves a special mention. The findings from the class activation
maps are directly in line with the clinicians' opinion since the heatmaps
pointed out the RNFL as the most relevant structure for glaucoma diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1"&gt;Gabriel Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Amor_R/0/1/0/all/0/1"&gt;Roc&amp;#xed;o del Amor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Verdu_Monedero_R/0/1/0/all/0/1"&gt;Rafael Verd&amp;#xfa;-Monedero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Morales_Sanchez_J/0/1/0/all/0/1"&gt;Juan Morales-S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training. (arXiv:2106.13488v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13488</id>
        <link href="http://arxiv.org/abs/2106.13488"/>
        <updated>2021-06-28T01:57:55.274Z</updated>
        <summary type="html"><![CDATA[Vision-Language Pre-training (VLP) aims to learn multi-modal representations
from image-text pairs and serves for downstream vision-language tasks in a
fine-tuning fashion. The dominant VLP models adopt a CNN-Transformer
architecture, which embeds images with a CNN, and then aligns images and text
with a Transformer. Visual relationship between visual contents plays an
important role in image understanding and is the basic for inter-modal
alignment learning. However, CNNs have limitations in visual relation learning
due to local receptive field's weakness in modeling long-range dependencies.
Thus the two objectives of learning visual relation and inter-modal alignment
are encapsulated in the same Transformer network. Such design might restrict
the inter-modal alignment learning in the Transformer by ignoring the
specialized characteristic of each objective. To tackle this, we propose a
fully Transformer visual embedding for VLP to better learn visual relation and
further promote inter-modal alignment. Specifically, we propose a metric named
Inter-Modality Flow (IMF) to measure the interaction between vision and
language modalities (i.e., inter-modality). We also design a novel masking
optimization mechanism named Masked Feature Regression (MFR) in Transformer to
further promote the inter-modality learning. To the best of our knowledge, this
is the first study to explore the benefit of Transformer for visual feature
learning in VLP. We verify our method on a wide range of vision-language tasks,
including Visual Question Answering (VQA), Visual Entailment and Visual
Reasoning. Our approach not only outperforms the state-of-the-art VLP
performance, but also shows benefits on the IMF metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hongwei Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yupan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Houwen Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jianlong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Animatable Neural Radiance Fields from Monocular RGB Video. (arXiv:2106.13629v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13629</id>
        <link href="http://arxiv.org/abs/2106.13629"/>
        <updated>2021-06-28T01:57:55.255Z</updated>
        <summary type="html"><![CDATA[We present animatable neural radiance fields for detailed human avatar
creation from monocular videos. Our approach extends neural radiance fields
(NeRF) to the dynamic scenes with human movements via introducing explicit
pose-guided deformation while learning the scene representation network. In
particular, we estimate the human pose for each frame and learn a constant
canonical space for the detailed human template, which enables natural shape
deformation from the observation space to the canonical space under the
explicit control of the pose parameters. To compensate for inaccurate pose
estimation, we introduce the pose refinement strategy that updates the initial
pose during the learning process, which not only helps to learn more accurate
human reconstruction but also accelerates the convergence. In experiments we
show that the proposed approach achieves 1) implicit human geometry and
appearance reconstruction with high-quality details, 2) photo-realistic
rendering of the human from arbitrary views, and 3) animation of the human with
arbitrary poses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianchuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ying Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1"&gt;Di Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1"&gt;Xuefei Zhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1"&gt;Linchao Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Huchuan Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single Image Texture Translation for Data Augmentation. (arXiv:2106.13804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13804</id>
        <link href="http://arxiv.org/abs/2106.13804"/>
        <updated>2021-06-28T01:57:55.244Z</updated>
        <summary type="html"><![CDATA[Recent advances in image synthesis enables one to translate images by
learning the mapping between a source domain and a target domain. Existing
methods tend to learn the distributions by training a model on a variety of
datasets, with results evaluated largely in a subjective manner. Relatively few
works in this area, however, study the potential use of semantic image
translation methods for image recognition tasks. In this paper, we explore the
use of Single Image Texture Translation (SITT) for data augmentation. We first
propose a lightweight model for translating texture to images based on a single
input of source texture, allowing for fast training and testing. Based on SITT,
we then explore the use of augmented data in long-tailed and few-shot image
classification tasks. We find the proposed method is capable of translating
input data into a target domain, leading to consistent improved image
recognition performance. Finally, we examine how SITT and related image
translation methods can provide a basis for a data-efficient, augmentation
engineering approach to model training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tsung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1"&gt;Serge Belongie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection. (arXiv:2106.13365v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13365</id>
        <link href="http://arxiv.org/abs/2106.13365"/>
        <updated>2021-06-28T01:57:55.237Z</updated>
        <summary type="html"><![CDATA[The detection of 3D objects from LiDAR data is a critical component in most
autonomous driving systems. Safe, high speed driving needs larger detection
ranges, which are enabled by new LiDARs. These larger detection ranges require
more efficient and accurate detection models. Towards this goal, we propose
Range Sparse Net (RSN), a simple, efficient, and accurate 3D object detector in
order to tackle real time 3D object detection in this extended detection
regime. RSN predicts foreground points from range images and applies sparse
convolutions on the selected foreground points to detect objects. The
lightweight 2D convolutions on dense range images results in significantly
fewer selected foreground points, thus enabling the later sparse convolutions
in RSN to efficiently operate. Combining features from the range image further
enhance detection accuracy. RSN runs at more than 60 frames per second on a
150m x 150m detection region on Waymo Open Dataset (WOD) while being more
accurate than previously published detectors. As of 11/2020, RSN is ranked
first in the WOD leaderboard based on the APH/LEVEL 1 metrics for LiDAR-based
pedestrian and vehicle detection, while being several times faster than
alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1"&gt;Pei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiyue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuning Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1"&gt;Gamaleldin Elsayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1"&gt;Alex Bewley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1"&gt;Cristian Sminchisescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1"&gt;Dragomir Anguelov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SRPN: similarity-based region proposal networks for nuclei and cells detection in histology images. (arXiv:2106.13556v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13556</id>
        <link href="http://arxiv.org/abs/2106.13556"/>
        <updated>2021-06-28T01:57:55.231Z</updated>
        <summary type="html"><![CDATA[The detection of nuclei and cells in histology images is of great value in
both clinical practice and pathological studies. However, multiple reasons such
as morphological variations of nuclei or cells make it a challenging task where
conventional object detection methods cannot obtain satisfactory performance in
many cases. A detection task consists of two sub-tasks, classification and
localization. Under the condition of dense object detection, classification is
a key to boost the detection performance. Considering this, we propose
similarity based region proposal networks (SRPN) for nuclei and cells detection
in histology images. In particular, a customized convolution layer termed as
embedding layer is designed for network building. The embedding layer is added
into the region proposal networks, enabling the networks to learn
discriminative features based on similarity learning. Features obtained by
similarity learning can significantly boost the classification performance
compared to conventional methods. SRPN can be easily integrated into standard
convolutional neural networks architectures such as the Faster R-CNN and
RetinaNet. We test the proposed approach on tasks of multi-organ nuclei
detection and signet ring cells detection in histological images. Experimental
results show that networks applying similarity learning achieved superior
performance on both tasks when compared to their counterparts. In particular,
the proposed SRPN achieve state-of-the-art performance on the MoNuSeg benchmark
for nuclei segmentation and detection while compared to previous methods, and
on the signet ring cell detection benchmark when compared with baselines. The
sourcecode is publicly available at:
https://github.com/sigma10010/nuclei_cells_det.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yibao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xingru Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qianni Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Countering Adversarial Examples: Combining Input Transformation and Noisy Training. (arXiv:2106.13394v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13394</id>
        <link href="http://arxiv.org/abs/2106.13394"/>
        <updated>2021-06-28T01:57:55.222Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that neural network (NN) based image classifiers
are highly vulnerable to adversarial examples, which poses a threat to
security-sensitive image recognition task. Prior work has shown that JPEG
compression can combat the drop in classification accuracy on adversarial
examples to some extent. But, as the compression ratio increases, traditional
JPEG compression is insufficient to defend those attacks but can cause an
abrupt accuracy decline to the benign images. In this paper, with the aim of
fully filtering the adversarial perturbations, we firstly make modifications to
traditional JPEG compression algorithm which becomes more favorable for NN.
Specifically, based on an analysis of the frequency coefficient, we design a
NN-favored quantization table for compression. Considering compression as a
data augmentation strategy, we then combine our model-agnostic preprocess with
noisy training. We fine-tune the pre-trained model by training with images
encoded at different compression levels, thus generating multiple classifiers.
Finally, since lower (higher) compression ratio can remove both perturbations
and original features slightly (aggressively), we use these trained multiple
models for model ensemble. The majority vote of the ensemble of models is
adopted as final predictions. Experiments results show our method can improve
defense efficiency while maintaining original accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Pan Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Modeling for Multi-task Visual Learning. (arXiv:2106.13409v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13409</id>
        <link href="http://arxiv.org/abs/2106.13409"/>
        <updated>2021-06-28T01:57:55.202Z</updated>
        <summary type="html"><![CDATA[Generative modeling has recently shown great promise in computer vision, but
it has mostly focused on synthesizing visually realistic images. In this paper,
motivated by multi-task learning of shareable feature representations, we
consider a novel problem of learning a shared generative model that is useful
across various visual perception tasks. Correspondingly, we propose a general
multi-task oriented generative modeling (MGM) framework, by coupling a
discriminative multi-task network with a generative network. While it is
challenging to synthesize both RGB images and pixel-level annotations in
multi-task scenarios, our framework enables us to use synthesized images paired
with only weak annotations (i.e., image-level scene labels) to facilitate
multiple visual tasks. Experimental evaluation on challenging multi-task
benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework
improves the performance of all the tasks by large margins, consistently
outperforming state-of-the-art multi-task approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1"&gt;Zhipeng Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1"&gt;Martial Hebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NP-DRAW: A Non-Parametric Structured Latent Variable Modelfor Image Generation. (arXiv:2106.13435v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13435</id>
        <link href="http://arxiv.org/abs/2106.13435"/>
        <updated>2021-06-28T01:57:55.196Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a non-parametric structured latent variable model
for image generation, called NP-DRAW, which sequentially draws on a latent
canvas in a part-by-part fashion and then decodes the image from the canvas.
Our key contributions are as follows. 1) We propose a non-parametric prior
distribution over the appearance of image parts so that the latent variable
``what-to-draw'' per step becomes a categorical random variable. This improves
the expressiveness and greatly eases the learning compared to Gaussians used in
the literature. 2) We model the sequential dependency structure of parts via a
Transformer, which is more powerful and easier to train compared to RNNs used
in the literature. 3) We propose an effective heuristic parsing algorithm to
pre-train the prior. Experiments on MNIST, Omniglot, CIFAR-10, and CelebA show
that our method significantly outperforms previous structured image models like
DRAW and AIR and is competitive to other generic generative models. Moreover,
we show that our model's inherent compositionality and interpretability bring
significant benefits in the low-data learning regime and latent space editing.
Code is available at \url{https://github.com/ZENGXH/NPDRAW}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiaohui Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1"&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard Zemel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1"&gt;Renjie Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape registration in the time of transformers. (arXiv:2106.13679v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13679</id>
        <link href="http://arxiv.org/abs/2106.13679"/>
        <updated>2021-06-28T01:57:55.190Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a transformer-based procedure for the efficient
registration of non-rigid 3D point clouds. The proposed approach is data-driven
and adopts for the first time the transformer architecture in the registration
task. Our method is general and applies to different settings. Given a fixed
template with some desired properties (e.g. skinning weights or other animation
cues), we can register raw acquired data to it, thereby transferring all the
template properties to the input geometry. Alternatively, given a pair of
shapes, our method can register the first onto the second (or vice-versa),
obtaining a high-quality dense correspondence between the two. In both
contexts, the quality of our results enables us to target real applications
such as texture transfer and shape interpolation. Furthermore, we also show
that including an estimation of the underlying density of the surface eases the
learning process. By exploiting the potential of this architecture, we can
train our model requiring only a sparse set of ground truth correspondences
($10\sim20\%$ of the total points). The proposed model and the analysis that we
perform pave the way for future exploration of transformer-based architectures
for registration and matching applications. Qualitative and quantitative
evaluations demonstrate that our pipeline outperforms state-of-the-art methods
for deformable and unordered 3D data registration on different datasets and
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1"&gt;Giovanni Trappolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1"&gt;Luca Cosmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1"&gt;Luca Moschella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1"&gt;Riccardo Marin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1"&gt;Emanuele Rodol&amp;#xe0;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Pretraining and Self-Supervision for a Deep Learning-based Analysis of Diabetic Retinopathy. (arXiv:2106.13497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13497</id>
        <link href="http://arxiv.org/abs/2106.13497"/>
        <updated>2021-06-28T01:57:55.181Z</updated>
        <summary type="html"><![CDATA[There is an increasing number of medical use-cases where classification
algorithms based on deep neural networks reach performance levels that are
competitive with human medical experts. To alleviate the challenges of small
dataset sizes, these systems often rely on pretraining. In this work, we aim to
assess the broader implications of these approaches. For diabetic retinopathy
grading as exemplary use case, we compare the impact of different training
procedures including recently established self-supervised pretraining methods
based on contrastive learning. To this end, we investigate different aspects
such as quantitative performance, statistics of the learned feature
representations, interpretability and robustness to image distortions. Our
results indicate that models initialized from ImageNet pretraining report a
significant increase in performance, generalization and robustness to image
distortions. In particular, self-supervised models show further benefits to
supervised models. Self-supervised models with initialization from ImageNet
pretraining not only report higher performance, they also reduce overfitting to
large lesions along with improvements in taking into account minute lesions
indicative of the progression of the disease. Understanding the effects of
pretraining in a broader sense that goes beyond simple performance comparisons
is of crucial importance for the broader medical imaging community beyond the
use-case considered in this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1"&gt;Vignesh Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strodthoff_N/0/1/0/all/0/1"&gt;Nils Strodthoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jackie Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To the Point: Efficient 3D Object Detection in the Range Image with Graph Convolution Kernels. (arXiv:2106.13381v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13381</id>
        <link href="http://arxiv.org/abs/2106.13381"/>
        <updated>2021-06-28T01:57:55.175Z</updated>
        <summary type="html"><![CDATA[3D object detection is vital for many robotics applications. For tasks where
a 2D perspective range image exists, we propose to learn a 3D representation
directly from this range image view. To this end, we designed a 2D
convolutional network architecture that carries the 3D spherical coordinates of
each pixel throughout the network. Its layers can consume any arbitrary
convolution kernel in place of the default inner product kernel and exploit the
underlying local geometry around each pixel. We outline four such kernels: a
dense kernel according to the bag-of-words paradigm, and three graph kernels
inspired by recent graph neural network advances: the Transformer, the
PointNet, and the Edge Convolution. We also explore cross-modality fusion with
the camera image, facilitated by operating in the perspective range image view.
Our method performs competitively on the Waymo Open Dataset and improves the
state-of-the-art AP for pedestrian detection from 69.7% to 75.5%. It is also
efficient in that our smallest model, which still outperforms the popular
PointPillars in quality, requires 180 times fewer FLOPS and model parameters]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuning Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1"&gt;Pei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1"&gt;Jiquan Ngiam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiyue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1"&gt;Benjamin Caine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1"&gt;Vijay Vasudevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1"&gt;Dragomir Anguelov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Connecting Sphere Manifolds Hierarchically for Regularization. (arXiv:2106.13549v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13549</id>
        <link href="http://arxiv.org/abs/2106.13549"/>
        <updated>2021-06-28T01:57:55.155Z</updated>
        <summary type="html"><![CDATA[This paper considers classification problems with hierarchically organized
classes. We force the classifier (hyperplane) of each class to belong to a
sphere manifold, whose center is the classifier of its super-class. Then,
individual sphere manifolds are connected based on their hierarchical
relations. Our technique replaces the last layer of a neural network by
combining a spherical fully-connected layer with a hierarchical layer. This
regularization is shown to improve the performance of widely used deep neural
network architectures (ResNet and DenseNet) on publicly available datasets
(CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scieur_D/0/1/0/all/0/1"&gt;Damien Scieur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngsung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversifying Semantic Image Synthesis and Editing via Class- and Layer-wise VAEs. (arXiv:2106.13416v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13416</id>
        <link href="http://arxiv.org/abs/2106.13416"/>
        <updated>2021-06-28T01:57:55.148Z</updated>
        <summary type="html"><![CDATA[Semantic image synthesis is a process for generating photorealistic images
from a single semantic mask. To enrich the diversity of multimodal image
synthesis, previous methods have controlled the global appearance of an output
image by learning a single latent space. However, a single latent code is often
insufficient for capturing various object styles because object appearance
depends on multiple factors. To handle individual factors that determine object
styles, we propose a class- and layer-wise extension to the variational
autoencoder (VAE) framework that allows flexible control over each object class
at the local to global levels by learning multiple latent spaces. Furthermore,
we demonstrate that our method generates images that are both plausible and
more diverse compared to state-of-the-art methods via extensive experiments
with real and synthetic datasets inthree different domains. We also show that
our method enables a wide range of applications in image synthesis and editing
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Endo_Y/0/1/0/all/0/1"&gt;Yuki Endo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanamori_Y/0/1/0/all/0/1"&gt;Yoshihiro Kanamori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiview Video Compression Using Advanced HEVC Screen Content Coding. (arXiv:2106.13574v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.13574</id>
        <link href="http://arxiv.org/abs/2106.13574"/>
        <updated>2021-06-28T01:57:55.139Z</updated>
        <summary type="html"><![CDATA[The paper presents a new approach to multiview video coding using Screen
Content Coding. It is assumed that for a time instant the frames corresponding
to all views are packed into a single frame, i.e. the frame-compatible approach
to multiview coding is applied. For such coding scenario, the paper
demonstrates that Screen Content Coding can be efficiently used for multiview
video coding. Two approaches are considered: the first using standard HEVC
Screen Content Coding, and the second using Advanced Screen Content Coding. The
latter is the original proposal of the authors that exploits quarter-pel motion
vectors and other nonstandard extensions of HEVC Screen Content Coding. The
experimental results demonstrate that multiview video coding even using
standard HEVC Screen Content Coding is much more efficient than simulcast HEVC
coding. The proposed Advanced Screen Content Coding provides virtually the same
coding efficiency as MV-HEVC, which is the state-of-the-art multiview video
compression technique. The authors suggest that Advanced Screen Content Coding
can be efficiently used within the new Versatile Video Coding (VVC) technology.
Nevertheless a reference multiview extension of VVC does not exist yet,
therefore, for VVC-based coding, the experimental comparisons are left for
future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samelak_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Samelak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domanski_M/0/1/0/all/0/1"&gt;Marek Doma&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partially fake it till you make it: mixing real and fake thermal images for improved object detection. (arXiv:2106.13603v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13603</id>
        <link href="http://arxiv.org/abs/2106.13603"/>
        <updated>2021-06-28T01:57:55.114Z</updated>
        <summary type="html"><![CDATA[In this paper we propose a novel data augmentation approach for visual
content domains that have scarce training datasets, compositing synthetic 3D
objects within real scenes. We show the performance of the proposed system in
the context of object detection in thermal videos, a domain where 1) training
datasets are very limited compared to visible spectrum datasets and 2) creating
full realistic synthetic scenes is extremely cumbersome and expensive due to
the difficulty in modeling the thermal properties of the materials of the
scene. We compare different augmentation strategies, including state of the art
approaches obtained through RL techniques, the injection of simulated data and
the employment of a generative model, and study how to best combine our
proposed augmentation with these other techniques.Experimental results
demonstrate the effectiveness of our approach, and our single-modality detector
achieves state-of-the-art results on the FLIR ADAS dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bongini_F/0/1/0/all/0/1"&gt;Francesco Bongini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berlincioni_L/0/1/0/all/0/1"&gt;Lorenzo Berlincioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1"&gt;Marco Bertini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1"&gt;Alberto Del Bimbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Eye Tracking. (arXiv:2106.13387v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13387</id>
        <link href="http://arxiv.org/abs/2106.13387"/>
        <updated>2021-06-28T01:57:55.106Z</updated>
        <summary type="html"><![CDATA[Model-based eye tracking has been a dominant approach for eye gaze tracking
because of its ability to generalize to different subjects, without the need of
any training data and eye gaze annotations. Model-based eye tracking, however,
is susceptible to eye feature detection errors, in particular for eye tracking
in the wild. To address this issue, we propose a Bayesian framework for
model-based eye tracking. The proposed system consists of a cascade-Bayesian
Convolutional Neural Network (c-BCNN) to capture the probabilistic
relationships between eye appearance and its landmarks, and a geometric eye
model to estimate eye gaze from the eye landmarks. Given a testing eye image,
the Bayesian framework can generate, through Bayesian inference, the eye gaze
distribution without explicit landmark detection and model training, based on
which it not only estimates the most likely eye gaze but also its uncertainty.
Furthermore, with Bayesian inference instead of point-based inference, our
model can not only generalize better to different sub-jects, head poses, and
environments but also is robust to image noise and landmark detection errors.
Finally, with the estimated gaze uncertainty, we can construct a cascade
architecture that allows us to progressively improve gaze estimation accuracy.
Compared to state-of-the-art model-based and learning-based methods, the
proposed Bayesian framework demonstrates significant improvement in
generalization capability across several benchmark datasets and in accuracy and
robustness under challenging real-world conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1"&gt;Qiang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalCity: Complex Simulations with Agency for Causal Discovery and Reasoning. (arXiv:2106.13364v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.13364</id>
        <link href="http://arxiv.org/abs/2106.13364"/>
        <updated>2021-06-28T01:57:55.087Z</updated>
        <summary type="html"><![CDATA[The ability to perform causal and counterfactual reasoning are central
properties of human intelligence. Decision-making systems that can perform
these types of reasoning have the potential to be more generalizable and
interpretable. Simulations have helped advance the state-of-the-art in this
domain, by providing the ability to systematically vary parameters (e.g.,
confounders) and generate examples of the outcomes in the case of
counterfactual scenarios. However, simulating complex temporal causal events in
multi-agent scenarios, such as those that exist in driving and vehicle
navigation, is challenging. To help address this, we present a high-fidelity
simulation environment that is designed for developing algorithms for causal
discovery and counterfactual reasoning in the safety-critical context. A core
component of our work is to introduce \textit{agency}, such that it is simple
to define and create complex scenarios using high-level definitions. The
vehicles then operate with agency to complete these objectives, meaning
low-level behaviors need only be controlled if necessary. We perform
experiments with three state-of-the-art methods to create baselines and
highlight the affordances of this environment. Finally, we highlight challenges
and opportunities for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1"&gt;Daniel McDuff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yale Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jiyoung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1"&gt;Vibhav Vineet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1"&gt;Sai Vemprala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gyde_N/0/1/0/all/0/1"&gt;Nicholas Gyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1"&gt;Hadi Salman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shuang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kwanghoon Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1"&gt;Ashish Kapoor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HAN: An Efficient Hierarchical Self-Attention Network for Skeleton-Based Gesture Recognition. (arXiv:2106.13391v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13391</id>
        <link href="http://arxiv.org/abs/2106.13391"/>
        <updated>2021-06-28T01:57:55.010Z</updated>
        <summary type="html"><![CDATA[Previous methods for skeleton-based gesture recognition mostly arrange the
skeleton sequence into a pseudo picture or spatial-temporal graph and apply
deep Convolutional Neural Network (CNN) or Graph Convolutional Network (GCN)
for feature extraction. Although achieving superior results, these methods have
inherent limitations in dynamically capturing local features of interactive
hand parts, and the computing efficiency still remains a serious issue. In this
work, the self-attention mechanism is introduced to alleviate this problem.
Considering the hierarchical structure of hand joints, we propose an efficient
hierarchical self-attention network (HAN) for skeleton-based gesture
recognition, which is based on pure self-attention without any CNN, RNN or GCN
operators. Specifically, the joint self-attention module is used to capture
spatial features of fingers, the finger self-attention module is designed to
aggregate features of the whole hand. In terms of temporal features, the
temporal self-attention module is utilized to capture the temporal dynamics of
the fingers and the entire hand. Finally, these features are fused by the
fusion self-attention module for gesture classification. Experiments show that
our method achieves competitive results on three gesture recognition datasets
with much lower computational complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Ying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1"&gt;Shiming Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chunhong Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13301</id>
        <link href="http://arxiv.org/abs/2106.13301"/>
        <updated>2021-06-28T01:57:55.003Z</updated>
        <summary type="html"><![CDATA[Physics perception very often faces the problem that only limited data or
partial measurements on the scene are available. In this work, we propose a
strategy to learn the full state of sloshing liquids from measurements of the
free surface. Our approach is based on recurrent neural networks (RNN) that
project the limited information available to a reduced-order manifold so as to
not only reconstruct the unknown information, but also to be capable of
performing fluid reasoning about future scenarios in real time. To obtain
physically consistent predictions, we train deep neural networks on the
reduced-order manifold that, through the employ of inductive biases, ensure the
fulfillment of the principles of thermodynamics. RNNs learn from history the
required hidden information to correlate the limited information with the
latent space where the simulation occurs. Finally, a decoder returns data back
to the high-dimensional manifold, so as to provide the user with insightful
information in the form of augmented reality. This algorithm is connected to a
computer vision system to test the performance of the proposed methodology with
real information, resulting in a system capable of understanding and predicting
future states of the observed fluid in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1"&gt;Beatriz Moya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1"&gt;Alberto Badias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1"&gt;David Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1"&gt;Francisco Chinesta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1"&gt;Elias Cueto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FOVQA: Blind Foveated Video Quality Assessment. (arXiv:2106.13328v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13328</id>
        <link href="http://arxiv.org/abs/2106.13328"/>
        <updated>2021-06-28T01:57:54.996Z</updated>
        <summary type="html"><![CDATA[Previous blind or No Reference (NR) video quality assessment (VQA) models
largely rely on features drawn from natural scene statistics (NSS), but under
the assumption that the image statistics are stationary in the spatial domain.
Several of these models are quite successful on standard pictures. However, in
Virtual Reality (VR) applications, foveated video compression is regaining
attention, and the concept of space-variant quality assessment is of interest,
given the availability of increasingly high spatial and temporal resolution
contents and practical ways of measuring gaze direction. Distortions from
foveated video compression increase with increased eccentricity, implying that
the natural scene statistics are space-variant. Towards advancing the
development of foveated compression / streaming algorithms, we have devised a
no-reference (NR) foveated video quality assessment model, called FOVQA, which
is based on new models of space-variant natural scene statistics (NSS) and
natural video statistics (NVS). Specifically, we deploy a space-variant
generalized Gaussian distribution (SV-GGD) model and a space-variant
asynchronous generalized Gaussian distribution (SV-AGGD) model of mean
subtracted contrast normalized (MSCN) coefficients and products of neighboring
MSCN coefficients, respectively. We devise a foveated video quality predictor
that extracts radial basis features, and other features that capture
perceptually annoying rapid quality fall-offs. We find that FOVQA achieves
state-of-the-art (SOTA) performance on the new 2D LIVE-FBT-FCVR database, as
compared with other leading FIQA / VQA models. we have made our implementation
of FOVQA available at: this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yize Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patney_A/0/1/0/all/0/1"&gt;Anjul Patney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Webb_R/0/1/0/all/0/1"&gt;Richard Webb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1"&gt;Alan Bovik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Unsupervised Clustering of Hyperspectral Images of Geological Targets in the Near Infrared. (arXiv:2106.13315v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13315</id>
        <link href="http://arxiv.org/abs/2106.13315"/>
        <updated>2021-06-28T01:57:54.988Z</updated>
        <summary type="html"><![CDATA[The application of infrared hyperspectral imagery to geological problems is
becoming more popular as data become more accessible and cost-effective.
Clustering and classifying spectrally similar materials is often a first step
in applications ranging from economic mineral exploration on Earth to planetary
exploration on Mars. Semi-manual classification guided by expertly developed
spectral parameters can be time consuming and biased, while supervised methods
require abundant labeled data and can be difficult to generalize. Here we
develop a fully unsupervised workflow for feature extraction and clustering
informed by both expert spectral geologist input and quantitative metrics. Our
pipeline uses a lightweight autoencoder followed by Gaussian mixture modeling
to map the spectral diversity within any image. We validate the performance of
our pipeline at submillimeter-scale with expert-labelled data from the Oman
ophiolite drill core and evaluate performance at meters-scale with partially
classified orbital data of Jezero Crater on Mars (the landing site for the
Perseverance rover). We additionally examine the effects of various
preprocessing techniques used in traditional analysis of hyperspectral imagery.
This pipeline provides a fast and accurate clustering map of similar geological
materials and consistently identifies and separates major mineral classes in
both laboratory imagery and remote sensing imagery. We refer to our pipeline as
"Generalized Pipeline for Spectroscopic Unsupervised clustering of Minerals
(GyPSUM)."]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gao_A/0/1/0/all/0/1"&gt;Angela F. Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rasmussen_B/0/1/0/all/0/1"&gt;Brandon Rasmussen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kulits_P/0/1/0/all/0/1"&gt;Peter Kulits&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scheller_E/0/1/0/all/0/1"&gt;Eva L. Scheller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Greenberger_R/0/1/0/all/0/1"&gt;Rebecca Greenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ehlmann_B/0/1/0/all/0/1"&gt;Bethany L. Ehlmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth Estimation. (arXiv:2106.13323v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13323</id>
        <link href="http://arxiv.org/abs/2106.13323"/>
        <updated>2021-06-28T01:57:54.963Z</updated>
        <summary type="html"><![CDATA[Advanced machine learning techniques have been used in remote sensing (RS)
applications such as crop mapping and yield prediction, but remain
under-utilized for tracking crop progress. In this study, we demonstrate the
use of agronomic knowledge of crop growth drivers in a Long Short-Term
Memory-based, Domain-guided neural network (DgNN) for in-season crop progress
estimation. The DgNN uses a branched structure and attention to separate
independent crop growth drivers and capture their varying importance throughout
the growing season. The DgNN is implemented for corn, using RS data in Iowa for
the period 2003-2019, with USDA crop progress reports used as ground truth.
State-wide DgNN performance shows significant improvement over sequential and
dense-only NN structures, and a widely-used Hidden Markov Model method. The
DgNN had a 3.5% higher Nash-Sutfliffe efficiency over all growth stages and 33%
more weeks with highest cosine similarity than the other NNs during test years.
The DgNN and Sequential NN were more robust during periods of abnormal crop
progress, though estimating the Silking-Grainfill transition was difficult for
all methods. Finally, Uniform Manifold Approximation and Projection
visualizations of layer activations showed how LSTM-based NNs separate crop
growth time-series differently from a dense-only structure. Results from this
study exhibit both the viability of NNs in crop growth stage estimation (CGSE)
and the benefits of using domain knowledge. The DgNN methodology presented here
can be extended to provide near-real time CGSE of other crops.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Worrall_G/0/1/0/all/0/1"&gt;George Worrall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1"&gt;Anand Rangarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1"&gt;Jasmeet Judge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13393</id>
        <link href="http://arxiv.org/abs/2106.13393"/>
        <updated>2021-06-28T01:57:54.947Z</updated>
        <summary type="html"><![CDATA[Self-Rating Depression Scale (SDS) questionnaire has frequently been used for
efficient depression preliminary screening. However, the uncontrollable
self-administered measure can be easily affected by insouciantly or deceptively
answering, and producing the different results with the clinician-administered
Hamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,
facial expression (FE) and actions play a vital role in clinician-administered
evaluation, while FE and action are underexplored for self-administered
evaluations. In this work, we collect a novel dataset of 200 subjects to
evidence the validity of self-rating questionnaires with their corresponding
question-wise video recording. To automatically interpret depression from the
SDS evaluation and the paired video, we propose an end-to-end hierarchical
framework for the long-term variable-length video, which is also conditioned on
the questionnaire results and the answering time. Specifically, we resort to a
hierarchical model which utilizes a 3D CNN for local temporal pattern
exploration and a redundancy-aware self-attention (RAS) scheme for
question-wise global feature aggregation. Targeting for the redundant long-term
FE video processing, our RAS is able to effectively exploit the correlations of
each video clip within a question set to emphasize the discriminative
information and eliminate the redundancy based on feature pair-wise affinity.
Then, the question-wise video feature is concatenated with the questionnaire
scores for final depression detection. Our thorough evaluations also show the
validity of fusing SDS evaluation and its video recording, and the superiority
of our framework to the conventional state-of-the-art temporal modeling
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wanqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Lizhong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hui Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Meta-learning with Disentanglement for Domain-generalised Medical Image Segmentation. (arXiv:2106.13292v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13292</id>
        <link href="http://arxiv.org/abs/2106.13292"/>
        <updated>2021-06-28T01:57:54.877Z</updated>
        <summary type="html"><![CDATA[Generalising deep models to new data from new centres (termed here domains)
remains a challenge. This is largely attributed to shifts in data statistics
(domain shifts) between source and unseen domains. Recently, gradient-based
meta-learning approaches where the training data are split into meta-train and
meta-test sets to simulate and handle the domain shifts during training have
shown improved generalisation performance. However, the current fully
supervised meta-learning approaches are not scalable for medical image
segmentation, where large effort is required to create pixel-wise annotations.
Meanwhile, in a low data regime, the simulated domain shifts may not
approximate the true domain shifts well across source and unseen domains. To
address this problem, we propose a novel semi-supervised meta-learning
framework with disentanglement. We explicitly model the representations related
to domain shifts. Disentangling the representations and combining them to
reconstruct the input image allows unlabeled data to be used to better
approximate the true domain shifts for meta-learning. Hence, the model can
achieve better generalisation performance, especially when there is a limited
amount of labeled data. Experiments show that the proposed method is robust on
different segmentation tasks and achieves state-of-the-art generalisation
performance on two public benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1"&gt;Spyridon Thermos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1"&gt;Alison O&amp;#x27;Neil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios A. Tsaftaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Picture May Be Worth a Hundred Words for Visual Question Answering. (arXiv:2106.13445v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13445</id>
        <link href="http://arxiv.org/abs/2106.13445"/>
        <updated>2021-06-28T01:57:54.820Z</updated>
        <summary type="html"><![CDATA[How far can we go with textual representations for understanding pictures? In
image understanding, it is essential to use concise but detailed image
representations. Deep visual features extracted by vision models, such as
Faster R-CNN, are prevailing used in multiple tasks, and especially in visual
question answering (VQA). However, conventional deep visual features may
struggle to convey all the details in an image as we humans do. Meanwhile, with
recent language models' progress, descriptive text may be an alternative to
this problem. This paper delves into the effectiveness of textual
representations for image understanding in the specific context of VQA. We
propose to take description-question pairs as input, instead of deep visual
features, and fed them into a language-only Transformer model, simplifying the
process and the computational cost. We also experiment with data augmentation
techniques to increase the diversity in the training set and avoid learning
statistical bias. Extensive evaluations have shown that textual representations
require only about a hundred words to compete with deep visual features on both
VQA 2.0 and VQA-CP v2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hirota_Y/0/1/0/all/0/1"&gt;Yusuke Hirota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1"&gt;Noa Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otani_M/0/1/0/all/0/1"&gt;Mayu Otani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1"&gt;Chenhui Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1"&gt;Yuta Nakashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taniguchi_I/0/1/0/all/0/1"&gt;Ittetsu Taniguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onoye_T/0/1/0/all/0/1"&gt;Takao Onoye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building Intelligent Autonomous Navigation Agents. (arXiv:2106.13415v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13415</id>
        <link href="http://arxiv.org/abs/2106.13415"/>
        <updated>2021-06-28T01:57:54.800Z</updated>
        <summary type="html"><![CDATA[Breakthroughs in machine learning in the last decade have led to `digital
intelligence', i.e. machine learning models capable of learning from vast
amounts of labeled data to perform several digital tasks such as speech
recognition, face recognition, machine translation and so on. The goal of this
thesis is to make progress towards designing algorithms capable of `physical
intelligence', i.e. building intelligent autonomous navigation agents capable
of learning to perform complex navigation tasks in the physical world involving
visual perception, natural language understanding, reasoning, planning, and
sequential decision making. Despite several advances in classical navigation
methods in the last few decades, current navigation agents struggle at
long-term semantic navigation tasks. In the first part of the thesis, we
discuss our work on short-term navigation using end-to-end reinforcement
learning to tackle challenges such as obstacle avoidance, semantic perception,
language grounding, and reasoning. In the second part, we present a new class
of navigation methods based on modular learning and structured explicit map
representations, which leverage the strengths of both classical and end-to-end
learning methods, to tackle long-term navigation tasks. We show that these
methods are able to effectively tackle challenges such as localization,
mapping, long-term planning, exploration and learning semantic priors. These
modular learning methods are capable of long-term spatial and semantic
understanding and achieve state-of-the-art results on various navigation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1"&gt;Devendra Singh Chaplot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Based Generative Cooperative Saliency Prediction. (arXiv:2106.13389v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13389</id>
        <link href="http://arxiv.org/abs/2106.13389"/>
        <updated>2021-06-28T01:57:54.791Z</updated>
        <summary type="html"><![CDATA[Conventional saliency prediction models typically learn a deterministic
mapping from images to the corresponding ground truth saliency maps. In this
paper, we study the saliency prediction problem from the perspective of
generative models by learning a conditional probability distribution over
saliency maps given an image, and treating the prediction as a sampling
process. Specifically, we propose a generative cooperative saliency prediction
framework based on the generative cooperative networks, where a conditional
latent variable model and a conditional energy-based model are jointly trained
to predict saliency in a cooperative manner. We call our model the SalCoopNets.
The latent variable model serves as a fast but coarse predictor to efficiently
produce an initial prediction, which is then refined by the iterative Langevin
revision of the energy-based model that serves as a fine predictor. Such a
coarse-to-fine cooperative saliency prediction strategy offers the best of both
worlds. Moreover, we generalize our framework to the scenario of weakly
supervised saliency prediction, where saliency annotation of training images is
partially observed, by proposing a cooperative learning while recovering
strategy. Lastly, we show that the learned energy function can serve as a
refinement module that can refine the results of other pre-trained saliency
prediction models. Experimental results show that our generative model can
achieve state-of-the-art performance. Our code is publicly available at:
\url{https://github.com/JingZhang617/SalCoopNets}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jianwen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zilong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1"&gt;Nick Barnes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Object-oriented Spatio-Temporal Reasoning for Video Question Answering. (arXiv:2106.13432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13432</id>
        <link href="http://arxiv.org/abs/2106.13432"/>
        <updated>2021-06-28T01:57:54.783Z</updated>
        <summary type="html"><![CDATA[Video Question Answering (Video QA) is a powerful testbed to develop new AI
capabilities. This task necessitates learning to reason about objects,
relations, and events across visual and linguistic domains in space-time.
High-level reasoning demands lifting from associative visual pattern
recognition to symbol-like manipulation over objects, their behavior and
interactions. Toward reaching this goal we propose an object-oriented reasoning
approach in that video is abstracted as a dynamic stream of interacting
objects. At each stage of the video event flow, these objects interact with
each other, and their interactions are reasoned about with respect to the query
and under the overall context of a video. This mechanism is materialized into a
family of general-purpose neural units and their multi-level architecture
called Hierarchical Object-oriented Spatio-Temporal Reasoning (HOSTR) networks.
This neural model maintains the objects' consistent lifelines in the form of a
hierarchically nested spatio-temporal graph. Within this graph, the dynamic
interactive object-oriented representations are built up along the video
sequence, hierarchically abstracted in a bottom-up manner, and converge toward
the key information for the correct answer. The method is evaluated on multiple
major Video QA datasets and establishes new state-of-the-arts in these tasks.
Analysis into the model's behavior indicates that object-oriented reasoning is
a reliable, interpretable and efficient approach to Video QA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1"&gt;Long Hoang Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Thao Minh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1"&gt;Vuong Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13266</id>
        <link href="http://arxiv.org/abs/2106.13266"/>
        <updated>2021-06-28T01:57:54.776Z</updated>
        <summary type="html"><![CDATA[In this paper, we address the problem of high performance and computationally
efficient content-based video retrieval in large-scale datasets. Current
methods typically propose either: (i) fine-grained approaches employing
spatio-temporal representations and similarity calculations, achieving high
performance at a high computational cost or (ii) coarse-grained approaches
representing/indexing videos as global vectors, where the spatio-temporal
structure is lost, providing low performance but also having low computational
cost. In this work, we propose a Knowledge Distillation framework, which we
call Distill-and-Select (DnS), that starting from a well-performing
fine-grained Teacher Network learns: a) Student Networks at different retrieval
performance and computational efficiency trade-offs and b) a Selection Network
that at test time rapidly directs samples to the appropriate student to
maintain both high retrieval performance and high computational efficiency. We
train several students with different architectures and arrive at different
trade-offs of performance and efficiency, i.e., speed and storage requirements,
including fine-grained students that store index videos using binary
representations. Importantly, the proposed scheme allows Knowledge Distillation
in large, unlabelled datasets -- this leads to good students. We evaluate DnS
on five public datasets on three different video retrieval tasks and
demonstrate a) that our students achieve state-of-the-art performance in
several cases and b) that our DnS framework provides an excellent trade-off
between retrieval performance, computational speed, and storage space. In
specific configurations, our method achieves similar mAP with the teacher but
is 20 times faster and requires 240 times less storage space. Our collected
dataset and implementation are publicly available:
https://github.com/mever-team/distill-and-select.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1"&gt;Giorgos Kordopatis-Zilos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1"&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1"&gt;Symeon Papadopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1"&gt;Ioannis Kompatsiaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1"&gt;Ioannis Patras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized One-Class Learning Using Pairs of Complementary Classifiers. (arXiv:2106.13272v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13272</id>
        <link href="http://arxiv.org/abs/2106.13272"/>
        <updated>2021-06-28T01:57:54.764Z</updated>
        <summary type="html"><![CDATA[One-class learning is the classic problem of fitting a model to the data for
which annotations are available only for a single class. In this paper, we
explore novel objectives for one-class learning, which we collectively refer to
as Generalized One-class Discriminative Subspaces (GODS). Our key idea is to
learn a pair of complementary classifiers to flexibly bound the one-class data
distribution, where the data belongs to the positive half-space of one of the
classifiers in the complementary pair and to the negative half-space of the
other. To avoid redundancy while allowing non-linearity in the classifier
decision surfaces, we propose to design each classifier as an orthonormal frame
and seek to learn these frames via jointly optimizing for two conflicting
objectives, namely: i) to minimize the distance between the two frames, and ii)
to maximize the margin between the frames and the data. The learned orthonormal
frames will thus characterize a piecewise linear decision surface that allows
for efficient inference, while our objectives seek to bound the data within a
minimal volume that maximizes the decision margin, thereby robustly capturing
the data distribution. We explore several variants of our formulation under
different constraints on the constituent classifiers, including kernelized
feature maps. We demonstrate the empirical benefits of our approach via
experiments on data from several applications in computer vision, such as
anomaly detection in video sequences, human poses, and human activities. We
also explore the generality and effectiveness of GODS for non-vision tasks via
experiments on several UCI datasets, demonstrating state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1"&gt;Anoop Cherian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jue Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Summarization for Chat Logs with Topic-Oriented Ranking and Context-Aware Auto-Encoders. (arXiv:2012.07300v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07300</id>
        <link href="http://arxiv.org/abs/2012.07300"/>
        <updated>2021-06-28T01:57:54.755Z</updated>
        <summary type="html"><![CDATA[Automatic chat summarization can help people quickly grasp important
information from numerous chat messages. Unlike conventional documents, chat
logs usually have fragmented and evolving topics. In addition, these logs
contain a quantity of elliptical and interrogative sentences, which make the
chat summarization highly context dependent. In this work, we propose a novel
unsupervised framework called RankAE to perform chat summarization without
employing manually labeled data. RankAE consists of a topic-oriented ranking
strategy that selects topic utterances according to centrality and diversity
simultaneously, as well as a denoising auto-encoder that is carefully designed
to generate succinct but context-informative summaries based on the selected
utterances. To evaluate the proposed method, we collect a large-scale dataset
of chat logs from a customer service environment and build an annotated set
only for model evaluation. Experimental results show that RankAE significantly
outperforms other unsupervised methods and is able to generate high-quality
summaries in terms of relevance and topic coverage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yicheng Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lujun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yangyang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhuoren Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exploratory Analysis of the Relation Between Offensive Language and Mental Health. (arXiv:2105.14888v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14888</id>
        <link href="http://arxiv.org/abs/2105.14888"/>
        <updated>2021-06-28T01:57:54.735Z</updated>
        <summary type="html"><![CDATA[In this paper, we analyze the interplay between the use of offensive language
and mental health. We acquired publicly available datasets created for
offensive language identification and depression detection and we train
computational models to compare the use of offensive language in social media
posts written by groups of individuals with and without self-reported
depression diagnosis. We also look at samples written by groups of individuals
whose posts show signs of depression according to recent related studies. Our
analysis indicates that offensive language is more frequently used in the
samples written by individuals with self-reported depression as well as
individuals showing signs of depression. The results discussed here open new
avenues in research in politeness/offensiveness and mental health.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1"&gt;Ana-Maria Bucur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1"&gt;Marcos Zampieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1"&gt;Liviu P. Dinu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech. (arXiv:2103.09474v4 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09474</id>
        <link href="http://arxiv.org/abs/2103.09474"/>
        <updated>2021-06-28T01:57:54.702Z</updated>
        <summary type="html"><![CDATA[Previous works on neural text-to-speech (TTS) have been addressed on limited
speed in training and inference time, robustness for difficult synthesis
conditions, expressiveness, and controllability. Although several approaches
resolve some limitations, there has been no attempt to solve all weaknesses at
once. In this paper, we propose STYLER, an expressive and controllable TTS
framework with high-speed and robust synthesis. Our novel audio-text aligning
method called Mel Calibrator and excluding autoregressive decoding enable rapid
training and inference and robust synthesis on unseen data. Also, disentangled
style factor modeling under supervision enlarges the controllability in
synthesizing process leading to expressive TTS. On top of it, a novel noise
modeling pipeline using domain adversarial training and Residual Decoding
empowers noise-robust style transfer, decomposing the noise without any
additional label. Various experiments demonstrate that STYLER is more effective
in speed and robustness than expressive TTS with autoregressive decoding and
more expressive and controllable than reading style non-autoregressive TTS.
Synthesis samples and experiment results are provided via our demo page, and
code is available publicly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1"&gt;Keon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyumin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daeyoung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Emergent Fake News Detection via Meta Neural Process Networks. (arXiv:2106.13711v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13711</id>
        <link href="http://arxiv.org/abs/2106.13711"/>
        <updated>2021-06-28T01:57:54.687Z</updated>
        <summary type="html"><![CDATA[Fake news travels at unprecedented speeds, reaches global audiences and puts
users and communities at great risk via social media platforms. Deep learning
based models show good performance when trained on large amounts of labeled
data on events of interest, whereas the performance of models tends to degrade
on other events due to domain shift. Therefore, significant challenges are
posed for existing detection approaches to detect fake news on emergent events,
where large-scale labeled datasets are difficult to obtain. Moreover, adding
the knowledge from newly emergent events requires to build a new model from
scratch or continue to fine-tune the model, which can be challenging,
expensive, and unrealistic for real-world settings. In order to address those
challenges, we propose an end-to-end fake news detection framework named
MetaFEND, which is able to learn quickly to detect fake news on emergent events
with a few verified posts. Specifically, the proposed model integrates
meta-learning and neural process methods together to enjoy the benefits of
these approaches. In particular, a label embedding module and a hard attention
mechanism are proposed to enhance the effectiveness by handling categorical
information and trimming irrelevant posts. Extensive experiments are conducted
on multimedia datasets collected from Twitter and Weibo. The experimental
results show our proposed MetaFEND model can detect fake news on never-seen
events effectively and outperform the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fenglong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_K/0/1/0/all/0/1"&gt;Kishlay Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jing Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privileged Zero-Shot AutoML. (arXiv:2106.13743v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13743</id>
        <link href="http://arxiv.org/abs/2106.13743"/>
        <updated>2021-06-28T01:57:54.647Z</updated>
        <summary type="html"><![CDATA[This work improves the quality of automated machine learning (AutoML) systems
by using dataset and function descriptions while significantly decreasing
computation time from minutes to milliseconds by using a zero-shot approach.
Given a new dataset and a well-defined machine learning task, humans begin by
reading a description of the dataset and documentation for the algorithms to be
used. This work is the first to use these textual descriptions, which we call
privileged information, for AutoML. We use a pre-trained Transformer model to
process the privileged text and demonstrate that using this information
improves AutoML performance. Thus, our approach leverages the progress of
unsupervised representation learning in natural language processing to provide
a significant boost to AutoML. We demonstrate that using only textual
descriptions of the data and functions achieves reasonable classification
performance, and adding textual descriptions to data meta-features improves
classification across tabular datasets. To achieve zero-shot AutoML we train a
graph neural network with these description embeddings and the data
meta-features. Each node represents a training dataset, which we use to predict
the best machine learning pipeline for a new test dataset in a zero-shot
fashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a
supervised learning task and dataset. In contrast, most AutoML systems require
tens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces
running and prediction times from minutes to milliseconds, consistently across
datasets. By speeding up AutoML by orders of magnitude this work demonstrates
real-time AutoML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Nikhil Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kates_B/0/1/0/all/0/1"&gt;Brandon Kates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1"&gt;Jeff Mentch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharkar_A/0/1/0/all/0/1"&gt;Anant Kharkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1"&gt;Madeleine Udell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1"&gt;Iddo Drori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free-viewpoint Indoor Neural Relighting from Multi-view Stereo. (arXiv:2106.13299v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2106.13299</id>
        <link href="http://arxiv.org/abs/2106.13299"/>
        <updated>2021-06-28T01:57:54.640Z</updated>
        <summary type="html"><![CDATA[We introduce a neural relighting algorithm for captured indoors scenes, that
allows interactive free-viewpoint navigation. Our method allows illumination to
be changed synthetically, while coherently rendering cast shadows and complex
glossy materials. We start with multiple images of the scene and a 3D mesh
obtained by multi-view stereo (MVS) reconstruction. We assume that lighting is
well-explained as the sum of a view-independent diffuse component and a
view-dependent glossy term concentrated around the mirror reflection direction.
We design a convolutional network around input feature maps that facilitate
learning of an implicit representation of scene materials and illumination,
enabling both relighting and free-viewpoint navigation. We generate these input
maps by exploiting the best elements of both image-based and physically-based
rendering. We sample the input views to estimate diffuse scene irradiance, and
compute the new illumination caused by user-specified light sources using path
tracing. To facilitate the network's understanding of materials and synthesize
plausible glossy reflections, we reproject the views and compute mirror images.
We train the network on a synthetic dataset where each scene is also
reconstructed with MVS. We show results of our algorithm relighting real indoor
scenes and performing free-viewpoint navigation with complex and realistic
glossy reflections, which so far remained out of reach for view-synthesis
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1"&gt;Julien Philip&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgenthaler_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Morgenthaler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1"&gt;Micha&amp;#xeb;l Gharbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drettakis_G/0/1/0/all/0/1"&gt;George Drettakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extended Parallel Corpus for Amharic-English Machine Translation. (arXiv:2104.03543v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03543</id>
        <link href="http://arxiv.org/abs/2104.03543"/>
        <updated>2021-06-28T01:57:54.598Z</updated>
        <summary type="html"><![CDATA[This paper describes the acquisition, preprocessing, segmentation, and
alignment of an Amharic-English parallel corpus. It will be useful for machine
translation of an under-resourced language, Amharic. The corpus is larger than
previously compiled corpora; it is released for research purposes. We trained
neural machine translation and phrase-based statistical machine translation
models using the corpus. In the automatic evaluation, neural machine
translation models outperform phrase-based statistical machine translation
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gezmu_A/0/1/0/all/0/1"&gt;Andargachew Mekonnen Gezmu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1"&gt;Andreas N&amp;#xfc;rnberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bati_T/0/1/0/all/0/1"&gt;Tesfaye Bayu Bati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Noisy Client Learning. (arXiv:2106.13239v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13239</id>
        <link href="http://arxiv.org/abs/2106.13239"/>
        <updated>2021-06-28T01:57:54.589Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) collaboratively aggregates a shared global model
depending on multiple local clients, while keeping the training data
decentralized in order to preserve data privacy. However, standard FL methods
ignore the noisy client issue, which may harm the overall performance of the
aggregated model. In this paper, we first analyze the noisy client statement,
and then model noisy clients with different noise distributions (e.g.,
Bernoulli and truncated Gaussian distributions). To learn with noisy clients,
we propose a simple yet effective FL framework, named Federated Noisy Client
Learning (Fed-NCL), which is a plug-and-play algorithm and contains two main
components: a data quality measurement (DQM) to dynamically quantify the data
quality of each participating client, and a noise robust aggregation (NRA) to
adaptively aggregate the local models of each client by jointly considering the
amount of local training data and the data quality of each client. Our Fed-NCL
can be easily applied in any standard FL workflow to handle the noisy client
issue. Experimental results on various datasets demonstrate that our algorithm
boosts the performances of different state-of-the-art systems with noisy
clients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Li Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Cheng-Zhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. (arXiv:2005.05144v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05144</id>
        <link href="http://arxiv.org/abs/2005.05144"/>
        <updated>2021-06-28T01:57:54.581Z</updated>
        <summary type="html"><![CDATA[Speech provides a natural way for human-computer interaction. In particular,
speech synthesis systems are popular in different applications, such as
personal assistants, GPS applications, screen readers and accessibility tools.
However, not all languages are on the same level when in terms of resources and
systems for speech synthesis. This work consists of creating publicly available
resources for Brazilian Portuguese in the form of a novel dataset along with
deep learning models for end-to-end speech synthesis. Such dataset has 10.5
hours from a single speaker, from which a Tacotron 2 model with the RTISI-LA
vocoder presented the best performance, achieving a 4.03 MOS value. The
obtained results are comparable to related works covering English language and
the state-of-the-art in Portuguese.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1"&gt;Edresson Casanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1"&gt;Arnaldo Candido Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1"&gt;Christopher Shulby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_F/0/1/0/all/0/1"&gt;Frederico Santos de Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Teixeira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Teixeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1"&gt;Moacir Antonelli Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1"&gt;Sandra Maria Aluisio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15779</id>
        <link href="http://arxiv.org/abs/2007.15779"/>
        <updated>2021-06-28T01:57:54.572Z</updated>
        <summary type="html"><![CDATA[Pretraining large neural language models, such as BERT, has led to impressive
gains on many natural language processing (NLP) tasks. However, most
pretraining efforts focus on general domain corpora, such as newswire and Web.
A prevailing assumption is that even domain-specific pretraining can benefit by
starting from general-domain language models. In this paper, we challenge this
assumption by showing that for domains with abundant unlabeled text, such as
biomedicine, pretraining language models from scratch results in substantial
gains over continual pretraining of general-domain language models. To
facilitate this investigation, we compile a comprehensive biomedical NLP
benchmark from publicly-available datasets. Our experiments show that
domain-specific pretraining serves as a solid foundation for a wide range of
biomedical NLP tasks, leading to new state-of-the-art results across the board.
Further, in conducting a thorough evaluation of modeling choices, both for
pretraining and task-specific fine-tuning, we discover that some common
practices are unnecessary with BERT models, such as using complex tagging
schemes in named entity recognition (NER). To help accelerate research in
biomedical NLP, we have released our state-of-the-art pretrained and
task-specific models for the community, and created a leaderboard featuring our
BLURB benchmark (short for Biomedical Language Understanding & Reasoning
Benchmark) at https://aka.ms/BLURB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1"&gt;Robert Tinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1"&gt;Michael Lucas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1"&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1"&gt;Tristan Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1"&gt;Hoifung Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy. (arXiv:2106.13553v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13553</id>
        <link href="http://arxiv.org/abs/2106.13553"/>
        <updated>2021-06-28T01:57:54.559Z</updated>
        <summary type="html"><![CDATA[This paper presents a multilingual study of word meaning representations in
context. We assess the ability of both static and contextualized models to
adequately represent different lexical-semantic relations, such as homonymy and
synonymy. To do so, we created a new multilingual dataset that allows us to
perform a controlled evaluation of several factors such as the impact of the
surrounding context or the overlap between words, conveying the same or
different senses. A systematic assessment on four scenarios shows that the best
monolingual models based on Transformers can adequately disambiguate homonyms
in context. However, as they rely heavily on context, these models fail at
representing words with different senses when occurring in similar sentences.
Experiments are performed in Galician, Portuguese, English, and Spanish, and
both the dataset (with more than 3,000 evaluation items) and new models are
freely released with this study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1"&gt;Marcos Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation. (arXiv:2006.10369v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10369</id>
        <link href="http://arxiv.org/abs/2006.10369"/>
        <updated>2021-06-28T01:57:54.517Z</updated>
        <summary type="html"><![CDATA[Much recent effort has been invested in non-autoregressive neural machine
translation, which appears to be an efficient alternative to state-of-the-art
autoregressive machine translation on modern GPUs. In contrast to the latter,
where generation is sequential, the former allows generation to be parallelized
across target token positions. Some of the latest non-autoregressive models
have achieved impressive translation quality-speed tradeoffs compared to
autoregressive baselines. In this work, we reexamine this tradeoff and argue
that autoregressive baselines can be substantially sped up without loss in
accuracy. Specifically, we study autoregressive models with encoders and
decoders of varied depths. Our extensive experiments show that given a
sufficiently deep encoder, a single-layer autoregressive decoder can
substantially outperform strong non-autoregressive models with comparable
inference speed. We show that the speed disadvantage for autoregressive
baselines compared to non-autoregressive methods has been overestimated in
three aspects: suboptimal layer allocation, insufficient speed measurement, and
lack of knowledge distillation. Our results establish a new protocol for future
research toward fast, accurate machine translation. Our code is available at
https://github.com/jungokasai/deep-shallow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1"&gt;Jungo Kasai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1"&gt;Nikolaos Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1"&gt;James Cross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masked Proxy Loss For Text-Independent Speaker Verification. (arXiv:2011.04491v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04491</id>
        <link href="http://arxiv.org/abs/2011.04491"/>
        <updated>2021-06-28T01:57:54.488Z</updated>
        <summary type="html"><![CDATA[Open-set speaker recognition can be regarded as a metric learning problem,
which is to maximize inter-class variance and minimize intra-class variance.
Supervised metric learning can be categorized into entity-based learning and
proxy-based learning. Most of the existing metric learning objectives like
Contrastive, Triplet, Prototypical, GE2E, etc all belong to the former
division, the performance of which is either highly dependent on sample mining
strategy or restricted by insufficient label information in the mini-batch.
Proxy-based losses mitigate both shortcomings, however, fine-grained
connections among entities are either not or indirectly leveraged. This paper
proposes a Masked Proxy (MP) loss which directly incorporates both proxy-based
relationships and pair-based relationships. We further propose Multinomial
Masked Proxy (MMP) loss to leverage the hardness of speaker pairs. These
methods have been applied to evaluate on VoxCeleb test set and reach
state-of-the-art Equal Error Rate(EER).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1"&gt;Jiachen Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aiswarya Vinod Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhamyal_H/0/1/0/all/0/1"&gt;Hira Dhamyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1"&gt;Bhiksha Raj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rita Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Sample Replacements for ELECTRA Pre-Training. (arXiv:2106.13715v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13715</id>
        <link href="http://arxiv.org/abs/2106.13715"/>
        <updated>2021-06-28T01:57:54.421Z</updated>
        <summary type="html"><![CDATA[ELECTRA pretrains a discriminator to detect replaced tokens, where the
replacements are sampled from a generator trained with masked language
modeling. Despite the compelling performance, ELECTRA suffers from the
following two issues. First, there is no direct feedback loop from
discriminator to generator, which renders replacement sampling inefficient.
Second, the generator's prediction tends to be over-confident along with
training, making replacements biased to correct tokens. In this paper, we
propose two methods to improve replacement sampling for ELECTRA pre-training.
Specifically, we augment sampling with a hardness prediction mechanism, so that
the generator can encourage the discriminator to learn what it has not
acquired. We also prove that efficient sampling reduces the training variance
of the discriminator. Moreover, we propose to use a focal loss for the
generator in order to relieve oversampling of correct tokens as replacements.
Experimental results show that our method improves ELECTRA pre-training on
various downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1"&gt;Yaru Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hangbo Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentiment Progression based Searching and Indexing of Literary Textual Artefacts. (arXiv:2106.13767v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13767</id>
        <link href="http://arxiv.org/abs/2106.13767"/>
        <updated>2021-06-28T01:57:54.388Z</updated>
        <summary type="html"><![CDATA[Literary artefacts are generally indexed and searched based on titles, meta
data and keywords over the years. This searching and indexing works well when
user/reader already knows about that particular creative textual artefact or
document. This indexing and search hardly takes into account interest and
emotional makeup of readers and its mapping to books. When a person is looking
for a literary textual artefact, he/she might be looking for not only
information but also to seek the joy of reading. In case of literary artefacts,
progression of emotions across the key events could prove to be the key for
indexing and searching. In this paper, we establish clusters among literary
artefacts based on computational relationships among sentiment progressions
using intelligent text analysis. We have created a database of 1076 English
titles + 20 Marathi titles and also used database
this http URL with 16559 titles and their
summaries. We have proposed Sentiment Progression based Indexing for searching
and recommending books. This can be used to create personalized clusters of
book titles of interest to readers. The analysis clearly suggests better
searching and indexing when we are targeting book lovers looking for a
particular type of book or creative artefact. This indexing and searching can
find many real-life applications for recommending books.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1"&gt;Hrishikesh Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alicea_B/0/1/0/all/0/1"&gt;Bradly Alicea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15082</id>
        <link href="http://arxiv.org/abs/2105.15082"/>
        <updated>2021-06-28T01:57:54.318Z</updated>
        <summary type="html"><![CDATA[Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Le Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Di Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Lin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Models are Good Translators. (arXiv:2106.13627v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13627</id>
        <link href="http://arxiv.org/abs/2106.13627"/>
        <updated>2021-06-28T01:57:54.292Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed the rapid advance in neural machine translation
(NMT), the core of which lies in the encoder-decoder architecture. Inspired by
the recent progress of large-scale pre-trained language models on machine
translation in a limited scenario, we firstly demonstrate that a single
language model (LM4MT) can achieve comparable performance with strong
encoder-decoder NMT models on standard machine translation benchmarks, using
the same training data and similar amount of model parameters. LM4MT can also
easily utilize source-side texts as additional supervision. Though modeling the
source- and target-language texts with the same mechanism, LM4MT can provide
unified representations for both source and target sentences, which can better
transfer knowledge across languages. Extensive experiments on pivot-based and
zero-shot translation tasks show that LM4MT can outperform the encoder-decoder
NMT model by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1"&gt;Zhixing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topic-Oriented Spoken Dialogue Summarization for Customer Service with Saliency-Aware Topic Modeling. (arXiv:2012.07311v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07311</id>
        <link href="http://arxiv.org/abs/2012.07311"/>
        <updated>2021-06-28T01:57:54.260Z</updated>
        <summary type="html"><![CDATA[In a customer service system, dialogue summarization can boost service
efficiency by automatically creating summaries for long spoken dialogues in
which customers and agents try to address issues about specific topics. In this
work, we focus on topic-oriented dialogue summarization, which generates highly
abstractive summaries that preserve the main ideas from dialogues. In spoken
dialogues, abundant dialogue noise and common semantics could obscure the
underlying informative content, making the general topic modeling approaches
difficult to apply. In addition, for customer service, role-specific
information matters and is an indispensable part of a summary. To effectively
perform topic modeling on dialogues and capture multi-role information, in this
work we propose a novel topic-augmented two-stage dialogue summarizer (TDS)
jointly with a saliency-aware neural topic model (SATM) for topic-oriented
summarization of customer service dialogues. Comprehensive studies on a
real-world Chinese customer service dataset demonstrated the superiority of our
method against several strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yicheng Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lujun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yangyang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1"&gt;Minlong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhuoren Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders. (arXiv:2106.13736v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13736</id>
        <link href="http://arxiv.org/abs/2106.13736"/>
        <updated>2021-06-28T01:57:54.252Z</updated>
        <summary type="html"><![CDATA[While pretrained encoders have achieved success in various natural language
understanding (NLU) tasks, there is a gap between these pretrained encoders and
natural language generation (NLG). NLG tasks are often based on the
encoder-decoder framework, where the pretrained encoders can only benefit part
of it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual
encoder-decoder model that regards the decoder as the task layer of
off-the-shelf pretrained encoders. Specifically, we augment the pretrained
multilingual encoder with a decoder and pre-train it in a self-supervised way.
To take advantage of both the large-scale monolingual data and bilingual data,
we adopt the span corruption and translation span corruption as the
pre-training tasks. Experiments show that DeltaLM outperforms various strong
baselines on both natural language generation and translation tasks, including
machine translation, abstractive text summarization, data-to-text, and question
generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shuming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaohan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1"&gt;Alexandre Muzio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1"&gt;Saksham Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1"&gt;Hany Hassan Awadalla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xia Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to marry a star: probabilistic constraints for meaning in context. (arXiv:2009.07936v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07936</id>
        <link href="http://arxiv.org/abs/2009.07936"/>
        <updated>2021-06-28T01:57:54.245Z</updated>
        <summary type="html"><![CDATA[In this paper, we derive a notion of 'word meaning in context' which accounts
for the wide range of lexical shifts and ambiguities observed in utterance
interpretation. We characterize the lexical comprehension process as a
combination of cognitive semantics and Discourse Representation Theory,
formalized as a 'situation description system': a probabilistic model which
takes utterance understanding to be the mental process of describing one or
more situations that would account for an observed utterance. Our model uses
insights from different types of generative models to capture the interplay of
local and global contexts and their joint influence upon the lexical
representation of sentence constituents. We implement the system using a
directed graphical model, and apply it to examples containing various
contextualisation phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Erk_K/0/1/0/all/0/1"&gt;Katrin Erk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbelot_A/0/1/0/all/0/1"&gt;Aurelie Herbelot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation. (arXiv:2010.10907v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10907</id>
        <link href="http://arxiv.org/abs/2010.10907"/>
        <updated>2021-06-28T01:57:54.237Z</updated>
        <summary type="html"><![CDATA[In Neural Machine Translation (and, more generally, conditional language
modeling), the generation of a target token is influenced by two types of
context: the source and the prefix of the target sequence. While many attempts
to understand the internal workings of NMT models have been made, none of them
explicitly evaluates relative source and target contributions to a generation
decision. We argue that this relative contribution can be evaluated by adopting
a variant of Layerwise Relevance Propagation (LRP). Its underlying
'conservation principle' makes relevance propagation unique: differently from
other methods, it evaluates not an abstract quantity reflecting token
importance, but the proportion of each token's influence. We extend LRP to the
Transformer and conduct an analysis of NMT models which explicitly evaluates
the source and target relative contributions to the generation process. We
analyze changes in these contributions when conditioning on different types of
prefixes, when varying the training objective or the amount of training data,
and during the training process. We find that models trained with more data
tend to rely on source information more and to have more sharp token
contributions; the training process is non-monotonic with several stages of
different nature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voita_E/0/1/0/all/0/1"&gt;Elena Voita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1"&gt;Rico Sennrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1"&gt;Ivan Titov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Task Effects in Human Reading with Neural Attention. (arXiv:1808.00054v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1808.00054</id>
        <link href="http://arxiv.org/abs/1808.00054"/>
        <updated>2021-06-28T01:57:54.194Z</updated>
        <summary type="html"><![CDATA[Humans read by making a sequence of fixations and saccades. They often skip
words, without apparent detriment to understanding. We offer a novel
explanation for skipping: readers optimize a tradeoff between performing a
language-related task and fixating as few words as possible. We propose a
neural architecture that combines an attention module (deciding whether to skip
words) and a task module (memorizing the input). We show that our model
predicts human skipping behavior, while also modeling reading times well, even
though it skips 40% of the input. A key prediction of our model is that
different reading tasks should result in different skipping behaviors. We
confirm this prediction in an eye-tracking experiment in which participants
answers questions about a text. We are able to capture these experimental
results using the our model, replacing the memorization module with a task
module that performs neural question answering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1"&gt;Michael Hahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1"&gt;Frank Keller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Geolocation Prediction of Tweets with Human Machine Collaboration. (arXiv:2106.13411v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13411</id>
        <link href="http://arxiv.org/abs/2106.13411"/>
        <updated>2021-06-28T01:57:54.127Z</updated>
        <summary type="html"><![CDATA[Twitter is a useful resource to analyze peoples' opinions on various topics.
Often these topics are correlated or associated with locations from where these
Tweet posts are made. For example, restaurant owners may need to know where
their target customers eat with respect to the sentiment of the posts made
related to food, policy planners may need to analyze citizens' opinion on
relevant issues such as crime, safety, congestion, etc. with respect to
specific parts of the city, or county or state. As promising as this is, less
than $1\%$ of the crawled Tweet posts come with geolocation tags. That makes
accurate prediction of Tweet posts for the non geo-tagged tweets very critical
to analyze data in various domains. In this research, we utilized millions of
Twitter posts and end-users domain expertise to build a set of deep neural
network models using natural language processing (NLP) techniques, that
predicts the geolocation of non geo-tagged Tweet posts at various level of
granularities such as neighborhood, zipcode, and longitude with latitudes. With
multiple neural architecture experiments, and a collaborative human-machine
workflow design, our ongoing work on geolocation detection shows promising
results that empower end-users to correlate relationship between variables of
choice with the location information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_F/0/1/0/all/0/1"&gt;Florina Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Subhajit Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manually Annotated Spelling Error Corpus for Amharic. (arXiv:2106.13521v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13521</id>
        <link href="http://arxiv.org/abs/2106.13521"/>
        <updated>2021-06-28T01:57:54.098Z</updated>
        <summary type="html"><![CDATA[This paper presents a manually annotated spelling error corpus for Amharic,
lingua franca in Ethiopia. The corpus is designed to be used for the evaluation
of spelling error detection and correction. The misspellings are tagged as
non-word and real-word errors. In addition, the contextual information
available in the corpus makes it useful in dealing with both types of spelling
errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gezmu_A/0/1/0/all/0/1"&gt;Andargachew Mekonnen Gezmu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lema_T/0/1/0/all/0/1"&gt;Tirufat Tesifaye Lema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seyoum_B/0/1/0/all/0/1"&gt;Binyam Ephrem Seyoum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1"&gt;Andreas N&amp;#xfc;rnberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preliminary study on using vector quantization latent spaces for TTS/VC systems with consistent performance. (arXiv:2106.13479v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13479</id>
        <link href="http://arxiv.org/abs/2106.13479"/>
        <updated>2021-06-28T01:57:54.043Z</updated>
        <summary type="html"><![CDATA[Generally speaking, the main objective when training a neural speech
synthesis system is to synthesize natural and expressive speech from the output
layer of the neural network without much attention given to the hidden layers.
However, by learning useful latent representation, the system can be used for
many more practical scenarios. In this paper, we investigate the use of
quantized vectors to model the latent linguistic embedding and compare it with
the continuous counterpart. By enforcing different policies over the latent
spaces in the training, we are able to obtain a latent linguistic embedding
that takes on different properties while having a similar performance in terms
of quality and speaker similarity. Our experiments show that the voice cloning
system built with vector quantization has only a small degradation in terms of
perceptive evaluations, but has a discrete latent space that is useful for
reducing the representation bit-rate, which is desirable for data transferring,
or limiting the information leaking, which is important for speaker
anonymization and other tasks of that nature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luong_H/0/1/0/all/0/1"&gt;Hieu-Thi Luong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1"&gt;Junichi Yamagishi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParaLaw Nets -- Cross-lingual Sentence-level Pretraining for Legal Text Processing. (arXiv:2106.13403v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13403</id>
        <link href="http://arxiv.org/abs/2106.13403"/>
        <updated>2021-06-28T01:57:54.010Z</updated>
        <summary type="html"><![CDATA[Ambiguity is a characteristic of natural language, which makes expression
ideas flexible. However, in a domain that requires accurate statements, it
becomes a barrier. Specifically, a single word can have many meanings and
multiple words can have the same meaning. When translating a text into a
foreign language, the translator needs to determine the exact meaning of each
element in the original sentence to produce the correct translation sentence.
From that observation, in this paper, we propose ParaLaw Nets, a pretrained
model family using sentence-level cross-lingual information to reduce ambiguity
and increase the performance in legal text processing. This approach achieved
the best result in the Question Answering task of COLIEE-2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha-Thanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vu Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1"&gt;Thi-Hai-Yen Vuong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1"&gt;Quan Minh Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Chau Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1"&gt;Binh Tran Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Minh Le Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1"&gt;Ken Satoh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models. (arXiv:2106.13353v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13353</id>
        <link href="http://arxiv.org/abs/2106.13353"/>
        <updated>2021-06-28T01:57:54.004Z</updated>
        <summary type="html"><![CDATA[Prompting language models (LMs) with training examples and task descriptions
has been seen as critical to recent successes in few-shot learning. In this
work, we show that finetuning LMs in the few-shot setting can considerably
reduce the need for prompt engineering. In fact, one can use null prompts,
prompts that contain neither task-specific templates nor training examples, and
achieve competitive accuracy to manually-tuned prompts across a wide range of
tasks. While finetuning LMs does introduce new parameters for each downstream
task, we show that this memory overhead can be substantially reduced:
finetuning only the bias terms can achieve comparable or better accuracy than
standard finetuning while only updating 0.1% of the parameters. All in all, we
recommend finetuning LMs for few-shot learning as it is more accurate, robust
to different prompts, and can be made nearly as efficient as using frozen LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1"&gt;Robert L. Logan IV&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balazevic_I/0/1/0/all/0/1"&gt;Ivana Bala&amp;#x17e;evi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1"&gt;Fabio Petroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1"&gt;Sebastian Riedel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Language Model for Web-scale Retrieval in Baidu Search. (arXiv:2106.03373v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03373</id>
        <link href="http://arxiv.org/abs/2106.03373"/>
        <updated>2021-06-28T01:57:53.900Z</updated>
        <summary type="html"><![CDATA[Retrieval is a crucial stage in web search that identifies a small set of
query-relevant candidates from a billion-scale corpus. Discovering more
semantically-related candidates in the retrieval stage is very promising to
expose more high-quality results to the end users. However, it still remains
non-trivial challenges of building and deploying effective retrieval models for
semantic matching in real search engine. In this paper, we describe the
retrieval system that we developed and deployed in Baidu Search. The system
exploits the recent state-of-the-art Chinese pretrained language model, namely
Enhanced Representation through kNowledge IntEgration (ERNIE), which
facilitates the system with expressive semantic matching. In particular, we
developed an ERNIE-based retrieval model, which is equipped with 1) expressive
Transformer-based semantic encoders, and 2) a comprehensive multi-stage
training paradigm. More importantly, we present a practical system workflow for
deploying the model in web-scale retrieval. Eventually, the system is fully
deployed into production, where rigorous offline and online experiments were
conducted. The results show that the system can perform high-quality candidate
retrieval, especially for those tail queries with uncommon demands. Overall,
the new retrieval system facilitated by pretrained language model (i.e., ERNIE)
can largely improve the usability and applicability of our search engine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yiding Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaxiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Weixue Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Suqi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yukun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Daiting Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhicong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"&gt;Dawei Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session-aware Linear Item-Item Models for Session-based Recommendation. (arXiv:2103.16104v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16104</id>
        <link href="http://arxiv.org/abs/2103.16104"/>
        <updated>2021-06-28T01:57:53.803Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation aims at predicting the next item given a
sequence of previous items consumed in the session, e.g., on e-commerce or
multimedia streaming services. Specifically, session data exhibits some unique
characteristics, i.e., session consistency and sequential dependency over items
within the session, repeated item consumption, and session timeliness. In this
paper, we propose simple-yet-effective linear models for considering the
holistic aspects of the sessions. The comprehensive nature of our models helps
improve the quality of session-based recommendation. More importantly, it
provides a generalized framework for reflecting different perspectives of
session data. Furthermore, since our models can be solved by closed-form
solutions, they are highly scalable. Experimental results demonstrate that the
proposed linear models show competitive or state-of-the-art performance in
various metrics on several real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1"&gt;Minjin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_j/0/1/0/all/0/1"&gt;jinhong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joonseok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1"&gt;Hyunjung Shim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jongwuk Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains. (arXiv:2106.13474v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13474</id>
        <link href="http://arxiv.org/abs/2106.13474"/>
        <updated>2021-06-28T01:57:53.783Z</updated>
        <summary type="html"><![CDATA[Large pre-trained models have achieved great success in many natural language
processing tasks. However, when they are applied in specific domains, these
models suffer from domain shift and bring challenges in fine-tuning and online
serving for latency and capacity constraints. In this paper, we present a
general approach to developing small, fast and effective pre-trained models for
specific domains. This is achieved by adapting the off-the-shelf general
pre-trained models and performing task-agnostic knowledge distillation in
target domains. Specifically, we propose domain-specific vocabulary expansion
in the adaptation stage and employ corpus level occurrence probability to
choose the size of incremental vocabulary automatically. Then we systematically
explore different strategies to compress the large pre-trained models for
specific domains. We conduct our experiments in the biomedical and computer
science domain. The experimental results demonstrate that our approach achieves
better performance over the BERT BASE model in domain-specific tasks while 3.3x
smaller and 5.1x faster than BERT BASE. The code and pre-trained models are
available at https://aka.ms/adalm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaohan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentiment Progression based Searching and Indexing of Literary Textual Artefacts. (arXiv:2106.13767v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13767</id>
        <link href="http://arxiv.org/abs/2106.13767"/>
        <updated>2021-06-28T01:57:53.746Z</updated>
        <summary type="html"><![CDATA[Literary artefacts are generally indexed and searched based on titles, meta
data and keywords over the years. This searching and indexing works well when
user/reader already knows about that particular creative textual artefact or
document. This indexing and search hardly takes into account interest and
emotional makeup of readers and its mapping to books. When a person is looking
for a literary textual artefact, he/she might be looking for not only
information but also to seek the joy of reading. In case of literary artefacts,
progression of emotions across the key events could prove to be the key for
indexing and searching. In this paper, we establish clusters among literary
artefacts based on computational relationships among sentiment progressions
using intelligent text analysis. We have created a database of 1076 English
titles + 20 Marathi titles and also used database
this http URL with 16559 titles and their
summaries. We have proposed Sentiment Progression based Indexing for searching
and recommending books. This can be used to create personalized clusters of
book titles of interest to readers. The analysis clearly suggests better
searching and indexing when we are targeting book lovers looking for a
particular type of book or creative artefact. This indexing and searching can
find many real-life applications for recommending books.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1"&gt;Hrishikesh Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alicea_B/0/1/0/all/0/1"&gt;Bradly Alicea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021. (arXiv:2106.13405v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13405</id>
        <link href="http://arxiv.org/abs/2106.13405"/>
        <updated>2021-06-28T01:57:53.734Z</updated>
        <summary type="html"><![CDATA[COLIEE is an annual competition in automatic computerized legal text
processing. Automatic legal document processing is an ambitious goal, and the
structure and semantics of the law are often far more complex than everyday
language. In this article, we survey and report our methods and experimental
results in using deep learning in legal document processing. The results show
the difficulties as well as potentials in this family of approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha-Thanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1"&gt;Thi-Hai-Yen Vuong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1"&gt;Quan Minh Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Chau Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1"&gt;Binh Tran Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vu Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Minh Le Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1"&gt;Ken Satoh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings. (arXiv:2106.13302v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13302</id>
        <link href="http://arxiv.org/abs/2106.13302"/>
        <updated>2021-06-28T01:57:53.715Z</updated>
        <summary type="html"><![CDATA[This article introduces byteSteady -- a fast model for classification using
byte-level n-gram embeddings. byteSteady assumes that each input comes as a
sequence of bytes. A representation vector is produced using the averaged
embedding vectors of byte-level n-grams, with a pre-defined set of n. The
hashing trick is used to reduce the number of embedding vectors. This input
representation vector is then fed into a linear classifier. A straightforward
application of byteSteady is text classification. We also apply byteSteady to
one type of non-language data -- DNA sequences for gene classification. For
both problems we achieved competitive classification results against strong
baselines, suggesting that byteSteady can be applied to both language and
non-language data. Furthermore, we find that simple compression using Huffman
coding does not significantly impact the results, which offers an
accuracy-speed trade-off previously unexplored in machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1"&gt;Alexandre Drouin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Raymond Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13375</id>
        <link href="http://arxiv.org/abs/2106.13375"/>
        <updated>2021-06-28T01:57:53.704Z</updated>
        <summary type="html"><![CDATA[Information overload is a prevalent challenge in many high-value domains. A
prominent case in point is the explosion of the biomedical literature on
COVID-19, which swelled to hundreds of thousands of papers in a matter of
months. In general, biomedical literature expands by two papers every minute,
totalling over a million new papers every year. Search in the biomedical realm,
and many other vertical domains is challenging due to the scarcity of direct
supervision from click logs. Self-supervised learning has emerged as a
promising direction to overcome the annotation bottleneck. We propose a general
approach for vertical search based on domain-specific pretraining and present a
case study for the biomedical domain. Despite being substantially simpler and
not using any relevance labels for training or development, our method performs
comparably or better than the best systems in the official TREC-COVID
evaluation, a COVID-related biomedical search competition. Using distributed
computing in modern cloud infrastructure, our system can scale to tens of
millions of articles on PubMed and has been deployed as Microsoft Biomedical
Search, a new search experience for biomedical literature:
https://aka.ms/biomedsearch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1"&gt;Tristan Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1"&gt;Robert Tinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1"&gt;Cliff Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1"&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1"&gt;Richard Rogahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yang Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1"&gt;Eric Horvitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1"&gt;Paul N. Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1"&gt;Hoifung Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modern Perspective on Query Likelihood with Deep Generative Retrieval Models. (arXiv:2106.13618v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13618</id>
        <link href="http://arxiv.org/abs/2106.13618"/>
        <updated>2021-06-28T01:57:53.674Z</updated>
        <summary type="html"><![CDATA[Existing neural ranking models follow the text matching paradigm, where
document-to-query relevance is estimated through predicting the matching score.
Drawing from the rich literature of classical generative retrieval models, we
introduce and formalize the paradigm of deep generative retrieval models
defined via the cumulative probabilities of generating query terms. This
paradigm offers a grounded probabilistic view on relevance estimation while
still enabling the use of modern neural architectures. In contrast to the
matching paradigm, the probabilistic nature of generative rankers readily
offers a fine-grained measure of uncertainty. We adopt several current neural
generative models in our framework and introduce a novel generative ranker
(T-PGN), which combines the encoding capacity of Transformers with the Pointer
Generator Network model. We conduct an extensive set of evaluation experiments
on passage retrieval, leveraging the MS MARCO Passage Re-ranking and TREC Deep
Learning 2019 Passage Re-ranking collections. Our results show the
significantly higher performance of the T-PGN model when compared with other
generative models. Lastly, we demonstrate that exploiting the uncertainty
information of deep generative rankers opens new perspectives to
query/collection understanding, and significantly improves the cut-off
prediction task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lesota_O/0/1/0/all/0/1"&gt;Oleg Lesota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1"&gt;Navid Rekabsaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1"&gt;Daniel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grasserbauer_K/0/1/0/all/0/1"&gt;Klaus Antonius Grasserbauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1"&gt;Carsten Eickhoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1"&gt;Markus Schedl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Source-Criticism Debiasing Method for GloVe Embeddings. (arXiv:2106.13382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13382</id>
        <link href="http://arxiv.org/abs/2106.13382"/>
        <updated>2021-06-28T01:57:53.661Z</updated>
        <summary type="html"><![CDATA[It is well-documented that word embeddings trained on large public corpora
consistently exhibit known human social biases. Although many methods for
debiasing exist, almost all fixate on completely eliminating biased information
from the embeddings and often diminish training set size in the process. In
this paper, we present a simple yet effective method for debiasing GloVe word
embeddings (Pennington et al., 2014) which works by incorporating explicit
information about training set bias rather than removing biased data outright.
Our method runs quickly and efficiently with the help of a fast bias gradient
approximation method from Brunet et al. (2019). As our approach is akin to the
notion of 'source criticism' in the humanities, we term our method
Source-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size
on Word Embedding Association Test (WEAT) sets without sacrificing training
data or TOP-1 performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McGovern_H/0/1/0/all/0/1"&gt;Hope McGovern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Emergent Fake News Detection via Meta Neural Process Networks. (arXiv:2106.13711v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13711</id>
        <link href="http://arxiv.org/abs/2106.13711"/>
        <updated>2021-06-28T01:57:53.608Z</updated>
        <summary type="html"><![CDATA[Fake news travels at unprecedented speeds, reaches global audiences and puts
users and communities at great risk via social media platforms. Deep learning
based models show good performance when trained on large amounts of labeled
data on events of interest, whereas the performance of models tends to degrade
on other events due to domain shift. Therefore, significant challenges are
posed for existing detection approaches to detect fake news on emergent events,
where large-scale labeled datasets are difficult to obtain. Moreover, adding
the knowledge from newly emergent events requires to build a new model from
scratch or continue to fine-tune the model, which can be challenging,
expensive, and unrealistic for real-world settings. In order to address those
challenges, we propose an end-to-end fake news detection framework named
MetaFEND, which is able to learn quickly to detect fake news on emergent events
with a few verified posts. Specifically, the proposed model integrates
meta-learning and neural process methods together to enjoy the benefits of
these approaches. In particular, a label embedding module and a hard attention
mechanism are proposed to enhance the effectiveness by handling categorical
information and trimming irrelevant posts. Extensive experiments are conducted
on multimedia datasets collected from Twitter and Weibo. The experimental
results show our proposed MetaFEND model can detect fake news on never-seen
events effectively and outperform the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fenglong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_K/0/1/0/all/0/1"&gt;Kishlay Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jing Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VOGUE: Answer Verbalization through Multi-Task Learning. (arXiv:2106.13316v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13316</id>
        <link href="http://arxiv.org/abs/2106.13316"/>
        <updated>2021-06-28T01:57:53.600Z</updated>
        <summary type="html"><![CDATA[In recent years, there have been significant developments in Question
Answering over Knowledge Graphs (KGQA). Despite all the notable advancements,
current KGQA systems only focus on answer generation techniques and not on
answer verbalization. However, in real-world scenarios (e.g., voice assistants
such as Alexa, Siri, etc.), users prefer verbalized answers instead of a
generated response. This paper addresses the task of answer verbalization for
(complex) question answering over knowledge graphs. In this context, we propose
a multi-task-based answer verbalization framework: VOGUE (Verbalization thrOuGh
mUlti-task lEarning). The VOGUE framework attempts to generate a verbalized
answer using a hybrid approach through a multi-task learning paradigm. Our
framework can generate results based on using questions and queries as inputs
concurrently. VOGUE comprises four modules that are trained simultaneously
through multi-task learning. We evaluate our framework on existing datasets for
answer verbalization, and it outperforms all current baselines on both BLEU and
METEOR scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1"&gt;Endri Kacupaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Premnadh_S/0/1/0/all/0/1"&gt;Shyamnath Premnadh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kuldeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maleshkova_M/0/1/0/all/0/1"&gt;Maria Maleshkova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Language Model based Ranking in Baidu Search. (arXiv:2105.11108v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11108</id>
        <link href="http://arxiv.org/abs/2105.11108"/>
        <updated>2021-06-28T01:57:53.587Z</updated>
        <summary type="html"><![CDATA[As the heart of a search engine, the ranking system plays a crucial role in
satisfying users' information demands. More recently, neural rankers fine-tuned
from pre-trained language models (PLMs) establish state-of-the-art ranking
effectiveness. However, it is nontrivial to directly apply these PLM-based
rankers to the large-scale web search system due to the following challenging
issues:(1) the prohibitively expensive computations of massive neural PLMs,
especially for long texts in the web-document, prohibit their deployments in an
online ranking system that demands extremely low latency;(2) the discrepancy
between existing ranking-agnostic pre-training objectives and the ad-hoc
retrieval scenarios that demand comprehensive relevance modeling is another
main barrier for improving the online ranking system;(3) a real-world search
engine typically involves a committee of ranking components, and thus the
compatibility of the individually fine-tuned ranking model is critical for a
cooperative ranking system. In this work, we contribute a series of
successfully applied techniques in tackling these exposed issues when deploying
the state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the
online search engine system. We first articulate a novel practice to
cost-efficiently summarize the web document and contextualize the resultant
summary content with the query using a cheap yet powerful Pyramid-ERNIE
architecture. Then we endow an innovative paradigm to finely exploit the
large-scale noisy and biased post-click behavioral data for relevance-oriented
pre-training. We also propose a human-anchored fine-tuning strategy tailored
for the online ranking system, aiming to stabilize the ranking signals across
various online components. Extensive offline and online experimental results
show that the proposed techniques significantly boost the search engine's
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1"&gt;Lixin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengqiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hengyi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1"&gt;Dehong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Suqi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Daiting Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhifan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weiyue Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhicong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"&gt;Dawei Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning. (arXiv:2106.13386v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13386</id>
        <link href="http://arxiv.org/abs/2106.13386"/>
        <updated>2021-06-28T01:57:53.503Z</updated>
        <summary type="html"><![CDATA[Fairness in recommendation has attracted increasing attention due to bias and
discrimination possibly caused by traditional recommenders. In Interactive
Recommender Systems (IRS), user preferences and the system's fairness status
are constantly changing over time. Existing fairness-aware recommenders mainly
consider fairness in static settings. Directly applying existing methods to IRS
will result in poor recommendation. To resolve this problem, we propose a
reinforcement learning based framework, FairRec, to dynamically maintain a
long-term balance between accuracy and fairness in IRS. User preferences and
the system's fairness status are jointly compressed into the state
representation to generate recommendations. FairRec aims at maximizing our
designed cumulative reward that combines accuracy and fairness. Extensive
experiments validate that FairRec can improve fairness, while preserving good
recommendation quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1"&gt;Ben Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive query expansion for professional search applications. (arXiv:2106.13528v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13528</id>
        <link href="http://arxiv.org/abs/2106.13528"/>
        <updated>2021-06-28T01:57:53.495Z</updated>
        <summary type="html"><![CDATA[Knowledge workers (such as healthcare information professionals, patent
agents and recruitment professionals) undertake work tasks where search forms a
core part of their duties. In these instances, the search task is often complex
and time-consuming and requires specialist expert knowledge to formulate
accurate search strategies. Interactive features such as query expansion can
play a key role in supporting these tasks. However, generating query
suggestions within a professional search context requires that consideration be
given to the specialist, structured nature of the search strategies they
employ. In this paper, we investigate a variety of query expansion methods
applied to a collection of Boolean search strategies used in a variety of
real-world professional search tasks. The results demonstrate the utility of
context-free distributional language models and the value of using linguistic
cues such as ngram order to optimise the balance between precision and recall.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Russell_Rose_T/0/1/0/all/0/1"&gt;Tony Russell-Rose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gooch_P/0/1/0/all/0/1"&gt;Philip Gooch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1"&gt;Udo Kruschwitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition. (arXiv:2106.13686v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.13686</id>
        <link href="http://arxiv.org/abs/2106.13686"/>
        <updated>2021-06-28T01:57:53.477Z</updated>
        <summary type="html"><![CDATA[Cued Speech (CS) is a visual communication system for the deaf or hearing
impaired people. It combines lip movements with hand cues to obtain a complete
phonetic repertoire. Current deep learning based methods on automatic CS
recognition suffer from a common problem, which is the data scarcity. Until
now, there are only two public single speaker datasets for French (238
sentences) and British English (97 sentences). In this work, we propose a
cross-modal knowledge distillation method with teacher-student structure, which
transfers audio speech information to CS to overcome the limited data problem.
Firstly, we pretrain a teacher model for CS recognition with a large amount of
open source audio speech data, and simultaneously pretrain the feature
extractors for lips and hands using CS data. Then, we distill the knowledge
from teacher model to the student model with frame-level and sequence-level
distillation strategies. Importantly, for frame-level, we exploit multi-task
learning to weigh losses automatically, to obtain the balance coefficient.
Besides, we establish a five-speaker British English CS dataset for the first
time. The proposed method is evaluated on French and British English CS
datasets, showing superior CS recognition performance to the state-of-the-art
(SOTA) by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianrong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Ziyue Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuewei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1"&gt;Mei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1"&gt;Qiang Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Li Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Coupled Topic Modeling over Sequential Documents. (arXiv:2106.13732v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13732</id>
        <link href="http://arxiv.org/abs/2106.13732"/>
        <updated>2021-06-28T01:57:53.232Z</updated>
        <summary type="html"><![CDATA[The abundant sequential documents such as online archival, social media and
news feeds are streamingly updated, where each chunk of documents is
incorporated with smoothly evolving yet dependent topics. Such digital texts
have attracted extensive research on dynamic topic modeling to infer hidden
evolving topics and their temporal dependencies. However, most of the existing
approaches focus on single-topic-thread evolution and ignore the fact that a
current topic may be coupled with multiple relevant prior topics. In addition,
these approaches also incur the intractable inference problem when inferring
latent parameters, resulting in a high computational cost and performance
degradation. In this work, we assume that a current topic evolves from all
prior topics with corresponding coupling weights, forming the
multi-topic-thread evolution. Our method models the dependencies between
evolving topics and thoroughly encodes their complex multi-couplings across
time steps. To conquer the intractable inference challenge, a new solution with
a set of novel data augmentation techniques is proposed, which successfully
discomposes the multi-couplings between evolving topics. A fully conjugate
model is thus obtained to guarantee the effectiveness and efficiency of the
inference technique. A novel Gibbs sampler with a backward-forward filter
algorithm efficiently learns latent timeevolving parameters in a closed-form.
In addition, the latent Indian Buffet Process (IBP) compound distribution is
exploited to automatically infer the overall topic number and customize the
sparse topic proportions for each sequential document without bias. The
proposed method is evaluated on both synthetic and real-world datasets against
the competitive baselines, demonstrating its superiority over the baselines in
terms of the low per-word perplexity, high coherent topics, and better document
time prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jinjin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zhiguo Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13266</id>
        <link href="http://arxiv.org/abs/2106.13266"/>
        <updated>2021-06-28T01:57:53.203Z</updated>
        <summary type="html"><![CDATA[In this paper, we address the problem of high performance and computationally
efficient content-based video retrieval in large-scale datasets. Current
methods typically propose either: (i) fine-grained approaches employing
spatio-temporal representations and similarity calculations, achieving high
performance at a high computational cost or (ii) coarse-grained approaches
representing/indexing videos as global vectors, where the spatio-temporal
structure is lost, providing low performance but also having low computational
cost. In this work, we propose a Knowledge Distillation framework, which we
call Distill-and-Select (DnS), that starting from a well-performing
fine-grained Teacher Network learns: a) Student Networks at different retrieval
performance and computational efficiency trade-offs and b) a Selection Network
that at test time rapidly directs samples to the appropriate student to
maintain both high retrieval performance and high computational efficiency. We
train several students with different architectures and arrive at different
trade-offs of performance and efficiency, i.e., speed and storage requirements,
including fine-grained students that store index videos using binary
representations. Importantly, the proposed scheme allows Knowledge Distillation
in large, unlabelled datasets -- this leads to good students. We evaluate DnS
on five public datasets on three different video retrieval tasks and
demonstrate a) that our students achieve state-of-the-art performance in
several cases and b) that our DnS framework provides an excellent trade-off
between retrieval performance, computational speed, and storage space. In
specific configurations, our method achieves similar mAP with the teacher but
is 20 times faster and requires 240 times less storage space. Our collected
dataset and implementation are publicly available:
https://github.com/mever-team/distill-and-select.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1"&gt;Giorgos Kordopatis-Zilos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1"&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1"&gt;Symeon Papadopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1"&gt;Ioannis Kompatsiaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1"&gt;Ioannis Patras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TableSense: Spreadsheet Table Detection with Convolutional Neural Networks. (arXiv:2106.13500v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13500</id>
        <link href="http://arxiv.org/abs/2106.13500"/>
        <updated>2021-06-28T01:57:53.189Z</updated>
        <summary type="html"><![CDATA[Spreadsheet table detection is the task of detecting all tables on a given
sheet and locating their respective ranges. Automatic table detection is a key
enabling technique and an initial step in spreadsheet data intelligence.
However, the detection task is challenged by the diversity of table structures
and table layouts on the spreadsheet. Considering the analogy between a cell
matrix as spreadsheet and a pixel matrix as image, and encouraged by the
successful application of Convolutional Neural Networks (CNN) in computer
vision, we have developed TableSense, a novel end-to-end framework for
spreadsheet table detection. First, we devise an effective cell featurization
scheme to better leverage the rich information in each cell; second, we develop
an enhanced convolutional neural network model for table detection to meet the
domain-specific requirement on precise table boundary detection; third, we
propose an effective uncertainty metric to guide an active learning based smart
sampling algorithm, which enables the efficient build-up of a training dataset
with 22,176 tables on 10,220 sheets with broad coverage of diverse table
structures and layouts. Our evaluation shows that TableSense is highly
effective with 91.3\% recall and 86.5\% precision in EoB-2 metric, a
significant improvement over both the current detection algorithm that are used
in commodity spreadsheet tools and state-of-the-art convolutional neural
networks in computer vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Haoyu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shijie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhouyu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13375</id>
        <link href="http://arxiv.org/abs/2106.13375"/>
        <updated>2021-06-28T01:57:53.178Z</updated>
        <summary type="html"><![CDATA[Information overload is a prevalent challenge in many high-value domains. A
prominent case in point is the explosion of the biomedical literature on
COVID-19, which swelled to hundreds of thousands of papers in a matter of
months. In general, biomedical literature expands by two papers every minute,
totalling over a million new papers every year. Search in the biomedical realm,
and many other vertical domains is challenging due to the scarcity of direct
supervision from click logs. Self-supervised learning has emerged as a
promising direction to overcome the annotation bottleneck. We propose a general
approach for vertical search based on domain-specific pretraining and present a
case study for the biomedical domain. Despite being substantially simpler and
not using any relevance labels for training or development, our method performs
comparably or better than the best systems in the official TREC-COVID
evaluation, a COVID-related biomedical search competition. Using distributed
computing in modern cloud infrastructure, our system can scale to tens of
millions of articles on PubMed and has been deployed as Microsoft Biomedical
Search, a new search experience for biomedical literature:
https://aka.ms/biomedsearch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1"&gt;Tristan Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1"&gt;Robert Tinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1"&gt;Cliff Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1"&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1"&gt;Richard Rogahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yang Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1"&gt;Eric Horvitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1"&gt;Paul N. Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1"&gt;Hoifung Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13266</id>
        <link href="http://arxiv.org/abs/2106.13266"/>
        <updated>2021-06-28T01:57:53.164Z</updated>
        <summary type="html"><![CDATA[In this paper, we address the problem of high performance and computationally
efficient content-based video retrieval in large-scale datasets. Current
methods typically propose either: (i) fine-grained approaches employing
spatio-temporal representations and similarity calculations, achieving high
performance at a high computational cost or (ii) coarse-grained approaches
representing/indexing videos as global vectors, where the spatio-temporal
structure is lost, providing low performance but also having low computational
cost. In this work, we propose a Knowledge Distillation framework, which we
call Distill-and-Select (DnS), that starting from a well-performing
fine-grained Teacher Network learns: a) Student Networks at different retrieval
performance and computational efficiency trade-offs and b) a Selection Network
that at test time rapidly directs samples to the appropriate student to
maintain both high retrieval performance and high computational efficiency. We
train several students with different architectures and arrive at different
trade-offs of performance and efficiency, i.e., speed and storage requirements,
including fine-grained students that store index videos using binary
representations. Importantly, the proposed scheme allows Knowledge Distillation
in large, unlabelled datasets -- this leads to good students. We evaluate DnS
on five public datasets on three different video retrieval tasks and
demonstrate a) that our students achieve state-of-the-art performance in
several cases and b) that our DnS framework provides an excellent trade-off
between retrieval performance, computational speed, and storage space. In
specific configurations, our method achieves similar mAP with the teacher but
is 20 times faster and requires 240 times less storage space. Our collected
dataset and implementation are publicly available:
https://github.com/mever-team/distill-and-select.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1"&gt;Giorgos Kordopatis-Zilos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1"&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1"&gt;Symeon Papadopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1"&gt;Ioannis Kompatsiaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1"&gt;Ioannis Patras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13393</id>
        <link href="http://arxiv.org/abs/2106.13393"/>
        <updated>2021-06-28T01:57:53.133Z</updated>
        <summary type="html"><![CDATA[Self-Rating Depression Scale (SDS) questionnaire has frequently been used for
efficient depression preliminary screening. However, the uncontrollable
self-administered measure can be easily affected by insouciantly or deceptively
answering, and producing the different results with the clinician-administered
Hamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,
facial expression (FE) and actions play a vital role in clinician-administered
evaluation, while FE and action are underexplored for self-administered
evaluations. In this work, we collect a novel dataset of 200 subjects to
evidence the validity of self-rating questionnaires with their corresponding
question-wise video recording. To automatically interpret depression from the
SDS evaluation and the paired video, we propose an end-to-end hierarchical
framework for the long-term variable-length video, which is also conditioned on
the questionnaire results and the answering time. Specifically, we resort to a
hierarchical model which utilizes a 3D CNN for local temporal pattern
exploration and a redundancy-aware self-attention (RAS) scheme for
question-wise global feature aggregation. Targeting for the redundant long-term
FE video processing, our RAS is able to effectively exploit the correlations of
each video clip within a question set to emphasize the discriminative
information and eliminate the redundancy based on feature pair-wise affinity.
Then, the question-wise video feature is concatenated with the questionnaire
scores for final depression detection. Our thorough evaluations also show the
validity of fusing SDS evaluation and its video recording, and the superiority
of our framework to the conventional state-of-the-art temporal modeling
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wanqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Lizhong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hui Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Usage-based Summaries of Learning Videos. (arXiv:2106.13504v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.13504</id>
        <link href="http://arxiv.org/abs/2106.13504"/>
        <updated>2021-06-28T01:57:53.067Z</updated>
        <summary type="html"><![CDATA[Much of the delivery of University education is now by synchronous or
asynchronous video. For students, one of the challenges is managing the sheer
volume of such video material as video presentations of taught material are
difficult to abbreviate and summarise because they do not have highlights which
stand out. Apart from video bookmarks there are no tools available to determine
which parts of video content should be replayed at revision time or just before
examinations. We have developed and deployed a digital library for managing
video learning material which has many dozens of hours of short-form video
content from a range of taught courses for hundreds of students at
undergraduate level. Through a web browser we allow students to access and play
these videos and we log their anonymised playback usage. From these logs we
score to each segment of each video based on the amount of playback it receives
from across all students, whether the segment has been re-wound and re-played
in the same student session, whether the on-screen window is the window in
focus on the student's desktop/laptop, and speed of playback. We also
incorporate negative scoring if a video segment is skipped or fast-forward, and
overarching all this we include a decay function based on recency of playback,
so the most recent days of playback contribute more to the video segment
scores. For each video in the library we present a usage-based graph which
allows students to see which parts of each video attract the most playback from
their peers, which helps them select material at revision time. Usage of the
system is fully anonymised and GDPR-compliant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hyowon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scriney_M/0/1/0/all/0/1"&gt;Michael Scriney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F. Smeaton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiview Video Compression Using Advanced HEVC Screen Content Coding. (arXiv:2106.13574v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.13574</id>
        <link href="http://arxiv.org/abs/2106.13574"/>
        <updated>2021-06-28T01:57:53.036Z</updated>
        <summary type="html"><![CDATA[The paper presents a new approach to multiview video coding using Screen
Content Coding. It is assumed that for a time instant the frames corresponding
to all views are packed into a single frame, i.e. the frame-compatible approach
to multiview coding is applied. For such coding scenario, the paper
demonstrates that Screen Content Coding can be efficiently used for multiview
video coding. Two approaches are considered: the first using standard HEVC
Screen Content Coding, and the second using Advanced Screen Content Coding. The
latter is the original proposal of the authors that exploits quarter-pel motion
vectors and other nonstandard extensions of HEVC Screen Content Coding. The
experimental results demonstrate that multiview video coding even using
standard HEVC Screen Content Coding is much more efficient than simulcast HEVC
coding. The proposed Advanced Screen Content Coding provides virtually the same
coding efficiency as MV-HEVC, which is the state-of-the-art multiview video
compression technique. The authors suggest that Advanced Screen Content Coding
can be efficiently used within the new Versatile Video Coding (VVC) technology.
Nevertheless a reference multiview extension of VVC does not exist yet,
therefore, for VVC-based coding, the experimental comparisons are left for
future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samelak_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Samelak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domanski_M/0/1/0/all/0/1"&gt;Marek Doma&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Efficient Locomotion via Learned Gait Transitions. (arXiv:2104.04644v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04644</id>
        <link href="http://arxiv.org/abs/2104.04644"/>
        <updated>2021-06-25T02:00:47.701Z</updated>
        <summary type="html"><![CDATA[We focus on the problem of developing energy efficient controllers for
quadrupedal robots. Animals can actively switch gaits at different speeds to
lower their energy consumption. In this paper, we devise a hierarchical
learning framework, in which distinctive locomotion gaits and natural gait
transitions emerge automatically with a simple reward of energy minimization.
We use reinforcement learning to train a high-level gait policy that specifies
gait patterns of each foot, while the low-level whole-body controller optimizes
the motor commands so that the robot can walk at a desired velocity using that
gait pattern. We test our learning framework on a quadruped robot and
demonstrate automatic gait transitions, from walking to trotting and to
fly-trotting, as the robot increases its speed. We show that the learned
hierarchical controller consumes much less energy across a wide range of
locomotion speed than baseline controllers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuxiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tingnan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coumans_E/0/1/0/all/0/1"&gt;Erwin Coumans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1"&gt;Jie Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1"&gt;Byron Boots&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Black-box Metrics with Iterative Example Weighting. (arXiv:2102.09492v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09492</id>
        <link href="http://arxiv.org/abs/2102.09492"/>
        <updated>2021-06-25T02:00:47.695Z</updated>
        <summary type="html"><![CDATA[We consider learning to optimize a classification metric defined by a
black-box function of the confusion matrix. Such black-box learning settings
are ubiquitous, for example, when the learner only has query access to the
metric of interest, or in noisy-label and domain adaptation applications where
the learner must evaluate the metric via performance evaluation using a small
validation sample. Our approach is to adaptively learn example weights on the
training dataset such that the resulting weighted objective best approximates
the metric on the validation sample. We show how to model and estimate the
example weights and use them to iteratively post-shift a pre-trained class
probability estimator to construct a classifier. We also analyze the resulting
procedure's statistical properties. Experiments on various label noise, domain
shift, and fair classification setups confirm that our proposal compares
favorably to the state-of-the-art baselines for each application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hiranandani_G/0/1/0/all/0/1"&gt;Gaurush Hiranandani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_J/0/1/0/all/0/1"&gt;Jatin Mathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1"&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fard_M/0/1/0/all/0/1"&gt;Mahdi Milani Fard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1"&gt;Oluwasanmi Koyejo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09696</id>
        <link href="http://arxiv.org/abs/2106.09696"/>
        <updated>2021-06-25T02:00:47.678Z</updated>
        <summary type="html"><![CDATA[Understanding the semantics of human movement -- the what, how and why of the
movement -- is an important problem that requires datasets of human actions
with semantic labels. Existing datasets take one of two approaches. Large-scale
video datasets contain many action labels but do not contain ground-truth 3D
human motion. Alternatively, motion-capture (mocap) datasets have precise body
motions but are limited to a small number of actions. To address this, we
present BABEL, a large dataset with language labels describing the actions
being performed in mocap sequences. BABEL consists of action labels for about
43 hours of mocap sequences from AMASS. Action labels are at two levels of
abstraction -- sequence labels describe the overall action in the sequence, and
frame labels describe all actions in every frame of the sequence. Each frame
label is precisely aligned with the duration of the corresponding action in the
mocap sequence, and multiple actions can overlap. There are over 28k sequence
labels, and 63k frame labels in BABEL, which belong to over 250 unique action
categories. Labels from BABEL can be leveraged for tasks like action
recognition, temporal action localization, motion synthesis, etc. To
demonstrate the value of BABEL as a benchmark, we evaluate the performance of
models on 3D action recognition. We demonstrate that BABEL poses interesting
learning challenges that are applicable to real-world scenarios, and can serve
as a useful benchmark of progress in 3D action recognition. The dataset,
baseline method, and evaluation code is made available, and supported for
academic research purposes at https://babel.is.tue.mpg.de/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1"&gt;Abhinanda R. Punnakkal&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1"&gt;Arjun Chandrasekaran&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1"&gt;Nikos Athanasiou&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1"&gt;Alejandra Quiros-Ramirez&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt; (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Quantum Singular Value Decomposition. (arXiv:2006.02336v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02336</id>
        <link href="http://arxiv.org/abs/2006.02336"/>
        <updated>2021-06-25T02:00:47.602Z</updated>
        <summary type="html"><![CDATA[Singular value decomposition is central to many problems in engineering and
scientific fields. Several quantum algorithms have been proposed to determine
the singular values and their associated singular vectors of a given matrix.
Although these algorithms are promising, the required quantum subroutines and
resources are too costly on near-term quantum devices. In this work, we propose
a variational quantum algorithm for singular value decomposition (VQSVD). By
exploiting the variational principles for singular values and the Ky Fan
Theorem, we design a novel loss function such that two quantum neural networks
(or parameterized quantum circuits) could be trained to learn the singular
vectors and output the corresponding singular values. Furthermore, we conduct
numerical simulations of VQSVD for random matrices as well as its applications
in image compression of handwritten digits. Finally, we discuss the
applications of our algorithm in recommendation systems and polar
decomposition. Our work explores new avenues for quantum information processing
beyond the conventional protocols that only works for Hermitian data, and
reveals the capability of matrix decomposition on near-term quantum devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhixin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Youle Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Vector-valued Learning: From Theory to Algorithm. (arXiv:1909.04883v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.04883</id>
        <link href="http://arxiv.org/abs/1909.04883"/>
        <updated>2021-06-25T02:00:47.597Z</updated>
        <summary type="html"><![CDATA[Vector-valued learning, where the output space admits a vector-valued
structure, is an important problem that covers a broad family of important
domains, e.g. multi-label learning and multi-class classification. Using local
Rademacher complexity and unlabeled data, we derive novel data-dependent excess
risk bounds for learning vector-valued functions in both the kernel space and
linear space. The derived bounds are much sharper than existing ones, where
convergence rates are improved from $\mathcal{O}(1/\sqrt{n})$ to
$\mathcal{O}(1/\sqrt{n+u}),$ and $\mathcal{O}(1/n)$ in special cases. Motivated
by our theoretical analysis, we propose a unified framework for learning
vector-valued functions, incorporating both local Rademacher complexity and
Laplacian regularization. Empirical results on a wide number of benchmark
datasets show that the proposed algorithm significantly outperforms baseline
methods, which coincides with our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding and Mitigating Social Biases in Language Models. (arXiv:2106.13219v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13219</id>
        <link href="http://arxiv.org/abs/2106.13219"/>
        <updated>2021-06-25T02:00:47.592Z</updated>
        <summary type="html"><![CDATA[As machine learning methods are deployed in real-world settings such as
healthcare, legal systems, and social science, it is crucial to recognize how
they shape social biases and stereotypes in these sensitive decision-making
processes. Among such real-world deployments are large-scale pretrained
language models (LMs) that can be potentially dangerous in manifesting
undesirable representational biases - harmful biases resulting from
stereotyping that propagate negative generalizations involving gender, race,
religion, and other social constructs. As a step towards improving the fairness
of LMs, we carefully define several sources of representational biases before
proposing new benchmarks and metrics to measure them. With these tools, we
propose steps towards mitigating social biases during text generation. Our
empirical results and human evaluation demonstrate effectiveness in mitigating
bias while retaining crucial contextual information for high-fidelity text
generation, thereby pushing forward the performance-fairness Pareto frontier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chiyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Non-parametric Bayesian Hawkes Processes. (arXiv:1810.03730v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.03730</id>
        <link href="http://arxiv.org/abs/1810.03730"/>
        <updated>2021-06-25T02:00:47.587Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop an efficient nonparametric Bayesian estimation of
the kernel function of Hawkes processes. The non-parametric Bayesian approach
is important because it provides flexible Hawkes kernels and quantifies their
uncertainty. Our method is based on the cluster representation of Hawkes
processes. Utilizing the stationarity of the Hawkes process, we efficiently
sample random branching structures and thus, we split the Hawkes process into
clusters of Poisson processes. We derive two algorithms -- a block Gibbs
sampler and a maximum a posteriori estimator based on expectation maximization
-- and we show that our methods have a linear time complexity, both
theoretically and empirically. On synthetic data, we show our methods to be
able to infer flexible Hawkes triggering kernels. On two large-scale Twitter
diffusion datasets, we show that our methods outperform the current
state-of-the-art in goodness-of-fit and that the time complexity is linear in
the size of the dataset. We also observe that on diffusions related to online
videos, the learned kernels reflect the perceived longevity for different
content types such as music or pets videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1"&gt;Christian Walder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1"&gt;Marian-Andrei Rizoiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lexing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FF-NSL: Feed-Forward Neural-Symbolic Learner. (arXiv:2106.13103v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13103</id>
        <link href="http://arxiv.org/abs/2106.13103"/>
        <updated>2021-06-25T02:00:47.568Z</updated>
        <summary type="html"><![CDATA[Inductive Logic Programming (ILP) aims to learn generalised, interpretable
hypotheses in a data-efficient manner. However, current ILP systems require
training examples to be specified in a structured logical form. This paper
introduces a neural-symbolic learning framework, called Feed-Forward
Neural-Symbolic Learner (FF-NSL), that integrates state-of-the-art ILP systems
based on the Answer Set semantics, with neural networks, in order to learn
interpretable hypotheses from labelled unstructured data. FF-NSL uses a
pre-trained neural network to extract symbolic facts from unstructured data and
an ILP system to learn a hypothesis that performs a downstream classification
task. In order to evaluate the applicability of our approach to real-world
applications, the framework is evaluated on tasks where distributional shifts
are introduced to unstructured input data, for which pre-trained neural
networks are likely to predict incorrectly and with high confidence.
Experimental results show that FF-NSL outperforms baseline approaches such as a
random forest and deep neural networks by learning more accurate and
interpretable hypotheses with fewer examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1"&gt;Daniel Cunnington&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Mark Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1"&gt;Alessandra Russo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1"&gt;Jorge Lobo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Swin Transformer. (arXiv:2106.13230v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13230</id>
        <link href="http://arxiv.org/abs/2106.13230"/>
        <updated>2021-06-25T02:00:47.562Z</updated>
        <summary type="html"><![CDATA[The vision community is witnessing a modeling shift from CNNs to
Transformers, where pure Transformer architectures have attained top accuracy
on the major video recognition benchmarks. These video models are all built on
Transformer layers that globally connect patches across the spatial and
temporal dimensions. In this paper, we instead advocate an inductive bias of
locality in video Transformers, which leads to a better speed-accuracy
trade-off compared to previous approaches which compute self-attention globally
even with spatial-temporal factorization. The locality of the proposed video
architecture is realized by adapting the Swin Transformer designed for the
image domain, while continuing to leverage the power of pre-trained image
models. Our approach achieves state-of-the-art accuracy on a broad range of
video recognition benchmarks, including on action recognition (84.9 top-1
accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less
pre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1
accuracy on Something-Something v2). The code and models will be made publicly
available at https://github.com/SwinTransformer/Video-Swin-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ze Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1"&gt;Jia Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yue Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yixuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Stephen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models. (arXiv:2106.00553v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00553</id>
        <link href="http://arxiv.org/abs/2106.00553"/>
        <updated>2021-06-25T02:00:47.557Z</updated>
        <summary type="html"><![CDATA[In recent years, implicit deep learning has emerged as a method to increase
the depth of deep neural networks. While their training is memory-efficient,
they are still significantly slower to train than their explicit counterparts.
In Deep Equilibrium Models (DEQs), the training is performed as a bi-level
problem, and its computational complexity is partially driven by the iterative
inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy
to tackle this computational bottleneck from which many bi-level problems
suffer. The main idea is to use the quasi-Newton matrices from the forward pass
to efficiently approximate the inverse Jacobian matrix in the direction needed
for the gradient computation. We provide a theorem that motivates using our
method with the original forward algorithms. In addition, by modifying these
forward algorithms, we further provide theoretical guarantees that our method
asymptotically estimates the true implicit gradient. We empirically study this
approach in many settings, ranging from hyperparameter optimization to large
Multiscale DEQs applied to CIFAR and ImageNet. We show that it reduces the
computational cost of the backward pass by up to two orders of magnitude. All
this is achieved while retaining the excellent performance of the original
models in hyperparameter optimization and on CIFAR, and giving encouraging and
competitive results on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramzi_Z/0/1/0/all/0/1"&gt;Zaccharie Ramzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannel_F/0/1/0/all/0/1"&gt;Florian Mannel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1"&gt;Shaojie Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Starck_J/0/1/0/all/0/1"&gt;Jean-Luc Starck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciuciu_P/0/1/0/all/0/1"&gt;Philippe Ciuciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1"&gt;Thomas Moreau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiclass Disease Predictions Based on Integrated Clinical and Genomics Datasets. (arXiv:2006.07879v1 [q-bio.GN] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2006.07879</id>
        <link href="http://arxiv.org/abs/2006.07879"/>
        <updated>2021-06-25T02:00:47.552Z</updated>
        <summary type="html"><![CDATA[Clinical predictions using clinical data by computational methods are common
in bioinformatics. However, clinical predictions using information from
genomics datasets as well is not a frequently observed phenomenon in research.
Precision medicine research requires information from all available datasets to
provide intelligent clinical solutions. In this paper, we have attempted to
create a prediction model which uses information from both clinical and
genomics datasets. We have demonstrated multiclass disease predictions based on
combined clinical and genomics datasets using machine learning methods. We have
created an integrated dataset, using a clinical (ClinVar) and a genomics (gene
expression) dataset, and trained it using instance-based learner to predict
clinical diseases. We have used an innovative but simple way for multiclass
classification, where the number of output classes is as high as 75. We have
used Principal Component Analysis for feature selection. The classifier
predicted diseases with 73\% accuracy on the integrated dataset. The results
were consistent and competent when compared with other classification models.
The results show that genomics information can be reliably included in datasets
for clinical predictions and it can prove to be valuable in clinical
diagnostics and precision medicine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Subhani_M/0/1/0/all/0/1"&gt;Moeez M. Subhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Anjum_A/0/1/0/all/0/1"&gt;Ashiq Anjum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Study of Transformers for Source Code. (arXiv:2010.07987v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07987</id>
        <link href="http://arxiv.org/abs/2010.07987"/>
        <updated>2021-06-25T02:00:47.547Z</updated>
        <summary type="html"><![CDATA[Initially developed for natural language processing (NLP), Transformers are
now widely used for source code processing, due to the format similarity
between source code and text. In contrast to natural language, source code is
strictly structured, i.e., it follows the syntax of the programming language.
Several recent works develop Transformer modifications for capturing syntactic
information in source code. The drawback of these works is that they do not
compare to each other and consider different tasks. In this work, we conduct a
thorough empirical study of the capabilities of Transformers to utilize
syntactic information in different tasks. We consider three tasks (code
completion, function naming and bug fixing) and re-implement different
syntax-capturing modifications in a unified framework. We show that
Transformers are able to make meaningful predictions based purely on syntactic
information and underline the best practices of taking the syntactic
information into account for improving the performance of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1"&gt;Nadezhda Chirkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1"&gt;Sergey Troshin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Autoencoder-Based Vehicle Trajectory Prediction with an Interpretable Latent Space. (arXiv:2103.13726v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13726</id>
        <link href="http://arxiv.org/abs/2103.13726"/>
        <updated>2021-06-25T02:00:47.533Z</updated>
        <summary type="html"><![CDATA[This paper introduces the Descriptive Variational Autoencoder (DVAE), an
unsupervised and end-to-end trainable neural network for predicting vehicle
trajectories that provides partial interpretability. The novel approach is
based on the architecture and objective of common variational autoencoders. By
introducing expert knowledge within the decoder part of the autoencoder, the
encoder learns to extract latent parameters that provide a graspable meaning in
human terms. Such an interpretable latent space enables the validation by
expert defined rule sets. The evaluation of the DVAE is performed using the
publicly available highD dataset for highway traffic scenarios. In comparison
to a conventional variational autoencoder with equivalent complexity, the
proposed model provides a similar prediction accuracy but with the great
advantage of having an interpretable latent space. For crucial decision making
and assessing trustworthiness of a prediction this property is highly
desirable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neumeier_M/0/1/0/all/0/1"&gt;Marion Neumeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tollkuhn_A/0/1/0/all/0/1"&gt;Andreas Tollk&amp;#xfc;hn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berberich_T/0/1/0/all/0/1"&gt;Thomas Berberich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1"&gt;Michael Botsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04641</id>
        <link href="http://arxiv.org/abs/2106.04641"/>
        <updated>2021-06-25T02:00:47.527Z</updated>
        <summary type="html"><![CDATA[Transfer learning methods, and in particular domain adaptation, help exploit
labeled data in one domain to improve the performance of a certain task in
another domain. However, it is still not clear what factors affect the success
of domain adaptation. This paper models adaptation success and selection of the
most suitable source domains among several candidates in text similarity. We
use descriptive domain information and cross-domain similarity metrics as
predictive features. While mostly positive, the results also point to some
domains where adaptation success was difficult to predict.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1"&gt;Nicolai Pogrebnyakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1"&gt;Shohreh Shaghaghian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decision Transformer: Reinforcement Learning via Sequence Modeling. (arXiv:2106.01345v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01345</id>
        <link href="http://arxiv.org/abs/2106.01345"/>
        <updated>2021-06-25T02:00:47.522Z</updated>
        <summary type="html"><![CDATA[We introduce a framework that abstracts Reinforcement Learning (RL) as a
sequence modeling problem. This allows us to draw upon the simplicity and
scalability of the Transformer architecture, and associated advances in
language modeling such as GPT-x and BERT. In particular, we present Decision
Transformer, an architecture that casts the problem of RL as conditional
sequence modeling. Unlike prior approaches to RL that fit value functions or
compute policy gradients, Decision Transformer simply outputs the optimal
actions by leveraging a causally masked Transformer. By conditioning an
autoregressive model on the desired return (reward), past states, and actions,
our Decision Transformer model can generate future actions that achieve the
desired return. Despite its simplicity, Decision Transformer matches or exceeds
the performance of state-of-the-art model-free offline RL baselines on Atari,
OpenAI Gym, and Key-to-Door tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lili Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kevin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1"&gt;Aravind Rajeswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kimin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1"&gt;Aditya Grover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1"&gt;Michael Laskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1"&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Gradient Methods for the Noisy Linear Quadratic Regulator over a Finite Horizon. (arXiv:2011.10300v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10300</id>
        <link href="http://arxiv.org/abs/2011.10300"/>
        <updated>2021-06-25T02:00:47.517Z</updated>
        <summary type="html"><![CDATA[We explore reinforcement learning methods for finding the optimal policy in
the linear quadratic regulator (LQR) problem. In particular, we consider the
convergence of policy gradient methods in the setting of known and unknown
parameters. We are able to produce a global linear convergence guarantee for
this approach in the setting of finite time horizon and stochastic state
dynamics under weak assumptions. The convergence of a projected policy gradient
method is also established in order to handle problems with constraints. We
illustrate the performance of the algorithm with two examples. The first
example is the optimal liquidation of a holding in an asset. We show results
for the case where we assume a model for the underlying dynamics and where we
apply the method to the data directly. The empirical evidence suggests that the
policy gradient method can learn the global optimal solution for a larger class
of stochastic systems containing the LQR framework and that it is more robust
with respect to model mis-specification when compared to a model-based
approach. The second example is an LQR system in a higher dimensional setting
with synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hambly_B/0/1/0/all/0/1"&gt;Ben Hambly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huining Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Privacy and Byzantine Resilience in SGD: Do They Add Up?. (arXiv:2102.08166v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08166</id>
        <link href="http://arxiv.org/abs/2102.08166"/>
        <updated>2021-06-25T02:00:47.512Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of combining Byzantine resilience with
privacy in machine learning (ML). Specifically, we study if a distributed
implementation of the renowned Stochastic Gradient Descent (SGD) learning
algorithm is feasible with both differential privacy (DP) and
$(\alpha,f)$-Byzantine resilience. To the best of our knowledge, this is the
first work to tackle this problem from a theoretical point of view. A key
finding of our analyses is that the classical approaches to these two
(seemingly) orthogonal issues are incompatible. More precisely, we show that a
direct composition of these techniques makes the guarantees of the resulting
SGD algorithm depend unfavourably upon the number of parameters of the ML
model, making the training of large models practically infeasible. We validate
our theoretical results through numerical experiments on publicly-available
datasets; showing that it is impractical to ensure DP and Byzantine resilience
simultaneously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1"&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nirupam Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinot_R/0/1/0/all/0/1"&gt;Rafa&amp;#xeb;l Pinot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Rouault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stephan_J/0/1/0/all/0/1"&gt;John Stephan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-based Counterfactual Explanations for Time Series Classification. (arXiv:2009.13211v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13211</id>
        <link href="http://arxiv.org/abs/2009.13211"/>
        <updated>2021-06-25T02:00:47.498Z</updated>
        <summary type="html"><![CDATA[In recent years, there has been a rapidly expanding focus on explaining the
predictions made by black-box AI systems that handle image and tabular data.
However, considerably less attention has been paid to explaining the
predictions of opaque AI systems handling time series data. In this paper, we
advance a novel model-agnostic, case-based technique -- Native Guide -- that
generates counterfactual explanations for time series classifiers. Given a
query time series, $T_{q}$, for which a black-box classification system
predicts class, $c$, a counterfactual time series explanation shows how $T_{q}$
could change, such that the system predicts an alternative class, $c'$. The
proposed instance-based technique adapts existing counterfactual instances in
the case-base by highlighting and modifying discriminative areas of the time
series that underlie the classification. Quantitative and qualitative results
from two comparative experiments indicate that Native Guide generates
plausible, proximal, sparse and diverse explanations that are better than those
produced by key benchmark counterfactual methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Delaney_E/0/1/0/all/0/1"&gt;Eoin Delaney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greene_D/0/1/0/all/0/1"&gt;Derek Greene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keane_M/0/1/0/all/0/1"&gt;Mark T. Keane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoencoding Under Normalization Constraints. (arXiv:2105.05735v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05735</id>
        <link href="http://arxiv.org/abs/2105.05735"/>
        <updated>2021-06-25T02:00:47.492Z</updated>
        <summary type="html"><![CDATA[Likelihood is a standard estimate for outlier detection. The specific role of
the normalization constraint is to ensure that the out-of-distribution (OOD)
regime has a small likelihood when samples are learned using maximum
likelihood. Because autoencoders do not possess such a process of
normalization, they often fail to recognize outliers even when they are
obviously OOD. We propose the Normalized Autoencoder (NAE), a normalized
probabilistic model constructed from an autoencoder. The probability density of
NAE is defined using the reconstruction error of an autoencoder, which is
differently defined in the conventional energy-based model. In our model,
normalization is enforced by suppressing the reconstruction of negative
samples, significantly improving the outlier detection performance. Our
experimental results confirm the efficacy of NAE, both in detecting outliers
and in generating in-distribution samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sangwoong Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_Y/0/1/0/all/0/1"&gt;Yung-Kyun Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1"&gt;Frank Chongwoo Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Isotonic regression with unknown permutations: Statistics, computation, and adaptation. (arXiv:2009.02609v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.02609</id>
        <link href="http://arxiv.org/abs/2009.02609"/>
        <updated>2021-06-25T02:00:47.487Z</updated>
        <summary type="html"><![CDATA[Motivated by models for multiway comparison data, we consider the problem of
estimating a coordinate-wise isotonic function on the domain $[0, 1]^d$ from
noisy observations collected on a uniform lattice, but where the design points
have been permuted along each dimension. While the univariate and bivariate
versions of this problem have received significant attention, our focus is on
the multivariate case $d \geq 3$. We study both the minimax risk of estimation
(in empirical $L_2$ loss) and the fundamental limits of adaptation (quantified
by the adaptivity index) to a family of piecewise constant functions. We
provide a computationally efficient Mirsky partition estimator that is minimax
optimal while also achieving the smallest adaptivity index possible for
polynomial time procedures. Thus, from a worst-case perspective and in sharp
contrast to the bivariate case, the latent permutations in the model do not
introduce significant computational difficulties over and above vanilla
isotonic regression. On the other hand, the fundamental limits of adaptation
are significantly different with and without unknown permutations: Assuming a
hardness conjecture from average-case complexity theory, a
statistical-computational gap manifests in the former case. In a complementary
direction, we show that natural modifications of existing estimators fail to
satisfy at least one of the desiderata of optimal worst-case statistical
performance, computational efficiency, and fast adaptation. Along the way to
showing our results, we improve adaptation results in the special case $d = 2$
and establish some properties of estimators for vanilla isotonic regression,
both of which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Pananjady_A/0/1/0/all/0/1"&gt;Ashwin Pananjady&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Samworth_R/0/1/0/all/0/1"&gt;Richard J. Samworth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Federated Learning Approach to Anomaly Detection in Smart Buildings. (arXiv:2010.10293v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10293</id>
        <link href="http://arxiv.org/abs/2010.10293"/>
        <updated>2021-06-25T02:00:47.480Z</updated>
        <summary type="html"><![CDATA[Internet of Things (IoT) sensors in smart buildings are becoming increasingly
ubiquitous, making buildings more livable, energy efficient, and sustainable.
These devices sense the environment and generate multivariate temporal data of
paramount importance for detecting anomalies and improving the prediction of
energy usage in smart buildings. However, detecting these anomalies in
centralized systems is often plagued by a huge delay in response time. To
overcome this issue, we formulate the anomaly detection problem in a federated
learning setting by leveraging the multi-task learning paradigm, which aims at
solving multiple tasks simultaneously while taking advantage of the
similarities and differences across tasks. We propose a novel privacy-by-design
federated learning model using a stacked long short-time memory (LSTM) model,
and we demonstrate that it is more than twice as fast during training
convergence compared to the centralized LSTM. The effectiveness of our
federated learning approach is demonstrated on three real-world datasets
generated by the IoT production system at General Electric Current smart
building, achieving state-of-the-art performance compared to baseline methods
in both classification and regression tasks. Our experimental results
demonstrate the effectiveness of the proposed framework in reducing the overall
training cost without compromising the prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sater_R/0/1/0/all/0/1"&gt;Raed Abdel Sater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1"&gt;A. Ben Hamza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rank $2r$ iterative least squares: efficient recovery of ill-conditioned low rank matrices from few entries. (arXiv:2002.01849v2 [math.OC] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2002.01849</id>
        <link href="http://arxiv.org/abs/2002.01849"/>
        <updated>2021-06-25T02:00:47.466Z</updated>
        <summary type="html"><![CDATA[We present a new, simple and computationally efficient iterative method for
low rank matrix completion. Our method is inspired by the class of
factorization-type iterative algorithms, but substantially differs from them in
the way the problem is cast. Precisely, given a target rank $r$, instead of
optimizing on the manifold of rank $r$ matrices, we allow our interim estimated
matrix to have a specific over-parametrized rank $2r$ structure. Our algorithm,
denoted R2RILS for rank $2r$ iterative least squares, has low memory
requirements, and at each iteration it solves a computationally cheap sparse
least-squares problem. We motivate our algorithm by its theoretical analysis
for the simplified case of a rank-1 matrix. Empirically, R2RILS is able to
recover ill conditioned low rank matrices from very few observations -- near
the information limit, and it is stable to additive noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bauch_J/0/1/0/all/0/1"&gt;Jonathan Bauch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nadler_B/0/1/0/all/0/1"&gt;Boaz Nadler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zilber_P/0/1/0/all/0/1"&gt;Pini Zilber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SALT: Sea lice Adaptive Lattice Tracking -- An Unsupervised Approach to Generate an Improved Ocean Model. (arXiv:2106.13202v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.13202</id>
        <link href="http://arxiv.org/abs/2106.13202"/>
        <updated>2021-06-25T02:00:47.455Z</updated>
        <summary type="html"><![CDATA[Warming oceans due to climate change are leading to increased numbers of
ectoparasitic copepods, also known as sea lice, which can cause significant
ecological loss to wild salmon populations and major economic loss to
aquaculture sites. The main transport mechanism driving the spread of sea lice
populations are near-surface ocean currents. Present strategies to estimate the
distribution of sea lice larvae are computationally complex and limit
full-scale analysis. Motivated to address this challenge, we propose SALT: Sea
lice Adaptive Lattice Tracking approach for efficient estimation of sea lice
dispersion and distribution in space and time. Specifically, an adaptive
spatial mesh is generated by merging nodes in the lattice graph of the Ocean
Model based on local ocean properties, thus enabling highly efficient graph
representation. SALT demonstrates improved efficiency while maintaining
consistent results with the standard method, using near-surface current data
for Hardangerfjord, Norway. The proposed SALT technique shows promise for
enhancing proactive aquaculture management through predictive modelling of sea
lice infestation pressure maps in a changing climate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Park_J/0/1/0/all/0/1"&gt;Ju An Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Voleti_V/0/1/0/all/0/1"&gt;Vikram Voleti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Thomas_K/0/1/0/all/0/1"&gt;Kathryn E. Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deglint_J/0/1/0/all/0/1"&gt;Jason L. Deglint&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wav2vec-C: A Self-supervised Model for Speech Representation Learning. (arXiv:2103.08393v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08393</id>
        <link href="http://arxiv.org/abs/2103.08393"/>
        <updated>2021-06-25T02:00:47.438Z</updated>
        <summary type="html"><![CDATA[Wav2vec-C introduces a novel representation learning technique combining
elements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized
representations from partially masked speech encoding using a contrastive loss
in a way similar to Wav2vec 2.0. However, the quantization process is
regularized by an additional consistency network that learns to reconstruct the
input features to the wav2vec 2.0 network from the quantized representations in
a way similar to a VQ-VAE model. The proposed self-supervised model is trained
on 10k hours of unlabeled data and subsequently used as the speech encoder in a
RNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one
of only a few studies of self-supervised learning on speech tasks with a large
volume of real far-field labeled data. The Wav2vec-C encoded representations
achieves, on average, twice the error reduction over baseline and a higher
codebook utilization in comparison to wav2vec 2.0]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sadhu_S/0/1/0/all/0/1"&gt;Samik Sadhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1"&gt;Che-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mallidi_S/0/1/0/all/0/1"&gt;Sri Harish Mallidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1"&gt;Minhua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rastrow_A/0/1/0/all/0/1"&gt;Ariya Rastrow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maas_R/0/1/0/all/0/1"&gt;Roland Maas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Matrix Approximation with Radial Basis Function Components. (arXiv:2106.02018v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02018</id>
        <link href="http://arxiv.org/abs/2106.02018"/>
        <updated>2021-06-25T02:00:47.425Z</updated>
        <summary type="html"><![CDATA[We introduce and investigate matrix approximation by decomposition into a sum
of radial basis function (RBF) components. An RBF component is a generalization
of the outer product between a pair of vectors, where an RBF function replaces
the scalar multiplication between individual vector elements. Even though the
RBF functions are positive definite, the summation across components is not
restricted to convex combinations and allows us to compute the decomposition
for any real matrix that is not necessarily symmetric or positive definite. We
formulate the problem of seeking such a decomposition as an optimization
problem with a nonlinear and non-convex loss function. Several modern versions
of the gradient descent method, including their scalable stochastic
counterparts, are used to solve this problem. We provide extensive empirical
evidence of the effectiveness of the RBF decomposition and that of the
gradient-based fitting algorithm. While being conceptually motivated by
singular value decomposition (SVD), our proposed nonlinear counterpart
outperforms SVD by drastically reducing the memory required to approximate a
data matrix with the same L2 error for a wide range of matrix types. For
example, it leads to 2 to 6 times memory save for Gaussian noise, graph
adjacency matrices, and kernel matrices. Moreover, this proximity-based
decomposition can offer additional interpretability in applications that
involve, e.g., capturing the inner low-dimensional structure of the data,
retaining graph connectivity structure, and preserving the acutance of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebrova_E/0/1/0/all/0/1"&gt;Elizaveta Rebrova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yu-Hang Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-space Conditioned Translation Networks for Directional Synthesis of Diffusion Weighted Images from Multi-modal Structural MRI. (arXiv:2106.13188v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13188</id>
        <link href="http://arxiv.org/abs/2106.13188"/>
        <updated>2021-06-25T02:00:47.419Z</updated>
        <summary type="html"><![CDATA[Current deep learning approaches for diffusion MRI modeling circumvent the
need for densely-sampled diffusion-weighted images (DWIs) by directly
predicting microstructural indices from sparsely-sampled DWIs. However, they
implicitly make unrealistic assumptions of static $q$-space sampling during
training and reconstruction. Further, such approaches can restrict downstream
usage of variably sampled DWIs for usages including the estimation of
microstructural indices or tractography. We propose a generative adversarial
translation framework for high-quality DWI synthesis with arbitrary $q$-space
sampling given commonly acquired structural images (e.g., B0, T1, T2). Our
translation network linearly modulates its internal representations conditioned
on continuous $q$-space information, thus removing the need for fixed sampling
schemes. Moreover, this approach enables downstream estimation of high-quality
microstructural maps from arbitrarily subsampled DWIs, which may be
particularly important in cases with sparsely sampled DWIs. Across several
recent methodologies, the proposed approach yields improved DWI synthesis
accuracy and fidelity with enhanced downstream utility as quantified by the
accuracy of scalar microstructure indices estimated from the synthesized
images. Code is available at
https://github.com/mengweiren/q-space-conditioned-dwi-synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mengwei Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1"&gt;Heejong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1"&gt;Neel Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gerig_G/0/1/0/all/0/1"&gt;Guido Gerig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PocketNet: A Smaller Neural Network for Medical Image Analysis. (arXiv:2104.10745v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10745</id>
        <link href="http://arxiv.org/abs/2104.10745"/>
        <updated>2021-06-25T02:00:47.411Z</updated>
        <summary type="html"><![CDATA[Medical imaging deep learning models are often large and complex, requiring
specialized hardware to train and evaluate these models. To address such
issues, we propose the PocketNet paradigm to reduce the size of deep learning
models by throttling the growth of the number of channels in convolutional
neural networks. We demonstrate that, for a range of segmentation and
classification tasks, PocketNet architectures produce results comparable to
that of conventional neural networks while reducing the number of parameters by
multiple orders of magnitude, using up to 90% less GPU memory, and speeding up
training times by up to 40%, thereby allowing such models to be trained and
deployed in resource-constrained settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1"&gt;Adrian Celaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Actor_J/0/1/0/all/0/1"&gt;Jonas A. Actor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Muthusivarajan_R/0/1/0/all/0/1"&gt;Rajarajeswari Muthusivarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gates_E/0/1/0/all/0/1"&gt;Evan Gates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1"&gt;Caroline Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schellingerhout_D/0/1/0/all/0/1"&gt;Dawid Schellingerhout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1"&gt;Beatrice Riviere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1"&gt;David Fuentes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning Agents. (arXiv:2103.13798v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13798</id>
        <link href="http://arxiv.org/abs/2103.13798"/>
        <updated>2021-06-25T02:00:47.406Z</updated>
        <summary type="html"><![CDATA[As modern games continue growing both in size and complexity, it has become
more challenging to ensure that all the relevant content is tested and that any
potential issue is properly identified and fixed. Attempting to maximize
testing coverage using only human participants, however, results in a tedious
and hard to orchestrate process which normally slows down the development
cycle. Complementing playtesting via autonomous agents has shown great promise
accelerating and simplifying this process. This paper addresses the problem of
automatically exploring and testing a given scenario using reinforcement
learning agents trained to maximize game state coverage. Each of these agents
is rewarded based on the novelty of its actions, thus encouraging a curious and
exploratory behaviour on a complex 3D scenario where previously proposed
exploration techniques perform poorly. The curious agents are able to learn the
complex navigation mechanics required to reach the different areas around the
map, thus providing the necessary data to identify potential issues. Moreover,
the paper also explores different visualization strategies and evaluates how to
make better use of the collected data to drive design decisions and to
recognize possible problems and oversights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gordillo_C/0/1/0/all/0/1"&gt;Camilo Gordillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergdahl_J/0/1/0/all/0/1"&gt;Joakim Bergdahl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tollmar_K/0/1/0/all/0/1"&gt;Konrad Tollmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gisslen_L/0/1/0/all/0/1"&gt;Linus Gissl&amp;#xe9;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Threats Analysis to Secure Federated Learning. (arXiv:2106.13076v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13076</id>
        <link href="http://arxiv.org/abs/2106.13076"/>
        <updated>2021-06-25T02:00:47.389Z</updated>
        <summary type="html"><![CDATA[Federated learning is emerging as a machine learning technique that trains a
model across multiple decentralized parties. It is renowned for preserving
privacy as the data never leaves the computational devices, and recent
approaches further enhance its privacy by hiding messages transferred in
encryption. However, we found that despite the efforts, federated learning
remains privacy-threatening, due to its interactive nature across different
parties. In this paper, we analyze the privacy threats in industrial-level
federated learning frameworks with secure computation, and reveal such threats
widely exist in typical machine learning models such as linear regression,
logistic regression and decision tree. For the linear and logistic regression,
we show through theoretical analysis that it is possible for the attacker to
invert the entire private input of the victim, given very few information. For
the decision tree model, we launch an attack to infer the range of victim's
private inputs. All attacks are evaluated on popular federated learning
frameworks and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yifan Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Liyao Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junhan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Cen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinbing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers. (arXiv:2106.12620v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12620</id>
        <link href="http://arxiv.org/abs/2106.12620"/>
        <updated>2021-06-25T02:00:47.371Z</updated>
        <summary type="html"><![CDATA[The self-attention-based model, transformer, is recently becoming the leading
backbone in the field of computer vision. In spite of the impressive success
made by transformers in a variety of vision tasks, it still suffers from heavy
computation and intensive memory cost. To address this limitation, this paper
presents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).
We start by observing a large amount of redundant computation, mainly spent on
uncorrelated input patches, and then introduce an interpretable module to
dynamically and gracefully drop these redundant patches. This novel framework
is then extended to a hierarchical structure, where uncorrelated tokens at
different stages are gradually removed, resulting in a considerable shrinkage
of computational cost. We include extensive experiments on both image and video
tasks, where our method could deliver up to 1.4X speed-up for state-of-the-art
models like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.
More importantly, contrary to other acceleration approaches, our method is
inherently interpretable with substantial visual evidence, making vision
transformer closer to a more human-understandable architecture while being
lighter. We demonstrate that the interpretability that naturally emerged in our
framework can outperform the raw attention learned by the original visual
transformer, as well as those generated by off-the-shelf interpretation
methods, with both qualitative and quantitative results. Project Page:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1"&gt;Bowen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yifan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1"&gt;Aude Oliva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planning with Exploration: Addressing Dynamics Bottleneck in Model-based Reinforcement Learning. (arXiv:2010.12914v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12914</id>
        <link href="http://arxiv.org/abs/2010.12914"/>
        <updated>2021-06-25T02:00:47.357Z</updated>
        <summary type="html"><![CDATA[Model-based reinforcement learning (MBRL) is believed to have higher sample
efficiency compared with model-free reinforcement learning (MFRL). However,
MBRL is plagued by dynamics bottleneck dilemma. Dynamics bottleneck dilemma is
the phenomenon that the performance of the algorithm falls into the local
optimum instead of increasing when the interaction step with the environment
increases, which means more data can not bring better performance. In this
paper, we find that the trajectory reward estimation error is the main reason
that causes dynamics bottleneck dilemma through theoretical analysis. We give
an upper bound of the trajectory reward estimation error and point out that
increasing the agent's exploration ability is the key to reduce trajectory
reward estimation error, thereby alleviating dynamics bottleneck dilemma.
Motivated by this, a model-based control method combined with exploration named
MOdel-based Progressive Entropy-based Exploration (MOPE2) is proposed. We
conduct experiments on several complex continuous control benchmark tasks. The
results verify that MOPE2 can effectively alleviate dynamics bottleneck dilemma
and have higher sample efficiency than previous MBRL and MFRL algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiyao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junge Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenzhen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1"&gt;Qiyue Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13061</id>
        <link href="http://arxiv.org/abs/2106.13061"/>
        <updated>2021-06-25T02:00:47.351Z</updated>
        <summary type="html"><![CDATA[Structural features are important features in graph datasets. However,
although there are some correlation analysis of features based on covariance,
there is no relevant research on exploring structural feature correlation on
graphs with graph neural network based models. In this paper, we introduce
graph feature to feature (Fea2Fea) prediction pipelines in a low dimensional
space to explore some preliminary results on structural feature correlation,
which is based on graph neural network. The results show that there exists high
correlation between some of the structural features. A redundant feature
combination with initial node features, which is filtered by graph neural
network has improved its classification accuracy in some graph datasets. We
compare the difference between concatenation methods on connecting embeddings
between features and show that the simplest is the best. We generalize on the
synthetic geometric graphs and certify the results on prediction difficulty
between two structural features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiaqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1"&gt;Rex Ying&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Spatio-temporal Event Detection on Geotagged Social Media. (arXiv:2106.13121v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.13121</id>
        <link href="http://arxiv.org/abs/2106.13121"/>
        <updated>2021-06-25T02:00:47.336Z</updated>
        <summary type="html"><![CDATA[A key challenge in mining social media data streams is to identify events
which are actively discussed by a group of people in a specific local or global
area. Such events are useful for early warning for accident, protest, election
or breaking news. However, neither the list of events nor the resolution of
both event time and space is fixed or known beforehand. In this work, we
propose an online spatio-temporal event detection system using social media
that is able to detect events at different time and space resolutions. First,
to address the challenge related to the unknown spatial resolution of events, a
quad-tree method is exploited in order to split the geographical space into
multiscale regions based on the density of social media data. Then, a
statistical unsupervised approach is performed that involves Poisson
distribution and a smoothing method for highlighting regions with unexpected
density of social posts. Further, event duration is precisely estimated by
merging events happening in the same region at consecutive time intervals. A
post processing stage is introduced to filter out events that are spam, fake or
wrong. Finally, we incorporate simple semantics by using social media entities
to assess the integrity, and accuracy of detected events. The proposed method
is evaluated using different social media datasets: Twitter and Flickr for
different cities: Melbourne, London, Paris and New York. To verify the
effectiveness of the proposed method, we compare our results with two baseline
algorithms based on fixed split of geographical space and clustering method.
For performance evaluation, we manually compute recall and precision. We also
propose a new quality measure named strength index, which automatically
measures how accurate the reported event is.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+George_Y/0/1/0/all/0/1"&gt;Yasmeen George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karunasekera_S/0/1/0/all/0/1"&gt;Shanika Karunasekera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwood_A/0/1/0/all/0/1"&gt;Aaron Harwood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kwan Hui Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternative Microfoundations for Strategic Classification. (arXiv:2106.12705v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12705</id>
        <link href="http://arxiv.org/abs/2106.12705"/>
        <updated>2021-06-25T02:00:47.330Z</updated>
        <summary type="html"><![CDATA[When reasoning about strategic behavior in a machine learning context it is
tempting to combine standard microfoundations of rational agents with the
statistical decision theory underlying classification. In this work, we argue
that a direct combination of these standard ingredients leads to brittle
solution concepts of limited descriptive and prescriptive value. First, we show
that rational agents with perfect information produce discontinuities in the
aggregate response to a decision rule that we often do not observe empirically.
Second, when any positive fraction of agents is not perfectly strategic,
desirable stable points -- where the classifier is optimal for the data it
entails -- cease to exist. Third, optimal decision rules under standard
microfoundations maximize a measure of negative externality known as social
burden within a broad class of possible assumptions about agent behavior.

Recognizing these limitations we explore alternatives to standard
microfoundations for binary classification. We start by describing a set of
desiderata that help navigate the space of possible assumptions about how
agents respond to a decision rule. In particular, we analyze a natural
constraint on feature manipulations, and discuss properties that are sufficient
to guarantee the robust existence of stable points. Building on these insights,
we then propose the noisy response model. Inspired by smoothed analysis and
empirical observations, noisy response incorporates imperfection in the agent
responses, which we show mitigates the limitations of standard
microfoundations. Our model retains analytical tractability, leads to more
robust insights about stable points, and imposes a lower social burden at
optimality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1"&gt;Meena Jagadeesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendler_Dunner_C/0/1/0/all/0/1"&gt;Celestine Mendler-D&amp;#xfc;nner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1"&gt;Moritz Hardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comprehensive empirical analysis on cross-domain semantic enrichment for detection of depressive language. (arXiv:2106.12797v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12797</id>
        <link href="http://arxiv.org/abs/2106.12797"/>
        <updated>2021-06-25T02:00:47.325Z</updated>
        <summary type="html"><![CDATA[We analyze the process of creating word embedding feature representations
designed for a learning task when annotated data is scarce, for example, in
depressive language detection from Tweets. We start with a rich word embedding
pre-trained from a large general dataset, which is then augmented with
embeddings learned from a much smaller and more specific domain dataset through
a simple non-linear mapping mechanism. We also experimented with several other
more sophisticated methods of such mapping including, several auto-encoder
based and custom loss-function based methods that learn embedding
representations through gradually learning to be close to the words of similar
semantics and distant to dissimilar semantics. Our strengthened representations
better capture the semantics of the depression domain, as it combines the
semantics learned from the specific domain coupled with word coverage from the
general language. We also present a comparative performance analyses of our
word embedding representations with a simple bag-of-words model, well known
sentiment and psycholinguistic lexicons, and a general pre-trained word
embedding. When used as feature representations for several different machine
learning methods, including deep learning models in a depressive Tweets
identification task, we show that our augmented word embedding representations
achieve a significantly better F1 score than the others, specially when applied
to a high quality dataset. Also, we present several data ablation tests which
confirm the efficacy of our augmentation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12900</id>
        <link href="http://arxiv.org/abs/2106.12900"/>
        <updated>2021-06-25T02:00:47.304Z</updated>
        <summary type="html"><![CDATA[Meta-learning model can quickly adapt to new tasks using few-shot labeled
data. However, despite achieving good generalization on few-shot classification
tasks, it is still challenging to improve the adversarial robustness of the
meta-learning model in few-shot learning. Although adversarial training (AT)
methods such as Adversarial Query (AQ) can improve the adversarially robust
performance of meta-learning models, AT is still computationally expensive
training. On the other hand, meta-learning models trained with AT will drop
significant accuracy on the original clean images. This paper proposed a
meta-learning method on the adversarially robust neural network called
Long-term Cross Adversarial Training (LCAT). LCAT will update meta-learning
model parameters cross along the natural and adversarial sample distribution
direction with long-term to improve both adversarial and clean few-shot
classification accuracy. Due to cross-adversarial training, LCAT only needs
half of the adversarial training epoch than AQ, resulting in a low adversarial
training computation. Experiment results show that LCAT achieves superior
performance both on the clean and adversarial few-shot classification accuracy
than SOTA adversarial training methods for meta-learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xuelong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Learning and Optimization Techniques: Towards a Survey of the State of the Art. (arXiv:2101.09505v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09505</id>
        <link href="http://arxiv.org/abs/2101.09505"/>
        <updated>2021-06-25T02:00:47.287Z</updated>
        <summary type="html"><![CDATA[Safe learning and optimization deals with learning and optimization problems
that avoid, as much as possible, the evaluation of non-safe input points, which
are solutions, policies, or strategies that cause an irrecoverable loss (e.g.,
breakage of a machine or equipment, or life threat). Although a comprehensive
survey of safe reinforcement learning algorithms was published in 2015, a
number of new algorithms have been proposed thereafter, and related works in
active learning and in optimization were not considered. This paper reviews
those algorithms from a number of domains including reinforcement learning,
Gaussian process regression and classification, evolutionary algorithms, and
active learning. We provide the fundamental concepts on which the reviewed
algorithms are based and a characterization of the individual algorithms. We
conclude by explaining how the algorithms are connected and suggestions for
future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngmin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allmendinger_R/0/1/0/all/0/1"&gt;Richard Allmendinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Ibanez_M/0/1/0/all/0/1"&gt;Manuel L&amp;#xf3;pez-Ib&amp;#xe1;&amp;#xf1;ez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Fast Sampling of Diffusion Probabilistic Models. (arXiv:2106.00132v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00132</id>
        <link href="http://arxiv.org/abs/2106.00132"/>
        <updated>2021-06-25T02:00:47.272Z</updated>
        <summary type="html"><![CDATA[In this work, we propose FastDPM, a unified framework for fast sampling in
diffusion probabilistic models. FastDPM generalizes previous methods and gives
rise to new algorithms with improved sample quality. We systematically
investigate the fast sampling methods under this framework across different
domains, on different datasets, and with different amount of conditional
information provided for generation. We find the performance of a particular
method depends on data domains (e.g., image or audio), the trade-off between
sampling speed and sample quality, and the amount of conditional information.
We further provide insights and recipes on the choice of methods for
practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhifeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1"&gt;Wei Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data. (arXiv:2106.12993v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12993</id>
        <link href="http://arxiv.org/abs/2106.12993"/>
        <updated>2021-06-25T02:00:47.267Z</updated>
        <summary type="html"><![CDATA[The assessment of laboratory animal behavior is of central interest in modern
neuroscience research. Behavior is typically studied in terms of pose changes,
which are ideally captured in three dimensions. This requires triangulation
over a multi-camera system which view the animal from different angles.
However, this is challenging in realistic laboratory setups due to occlusions
and other technical constrains. Here we propose the usage of lift-pose models
that allow for robust 3D pose estimation of freely moving rodents from a single
view camera view. To obtain high-quality training data for the pose-lifting, we
first perform geometric calibration in a camera setup involving bottom as well
as side views of the behaving animal. We then evaluate the performance of two
previously proposed model architectures under given inference perspectives and
conclude that reliable 3D pose inference can be obtained using temporal
convolutions. With this work we would like to contribute to a more robust and
diverse behavior tracking of freely moving rodents for a wide range of
experiments and setups in the neuroscience community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_I/0/1/0/all/0/1"&gt;Indrani Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_I/0/1/0/all/0/1"&gt;Indranil Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omprakash_C/0/1/0/all/0/1"&gt;Charitha Omprakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stober_S/0/1/0/all/0/1"&gt;Sebastian Stober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikulovic_S/0/1/0/all/0/1"&gt;Sanja Mikulovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_P/0/1/0/all/0/1"&gt;Pavol Bauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large Scale Private Learning via Low-rank Reparametrization. (arXiv:2106.09352v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09352</id>
        <link href="http://arxiv.org/abs/2106.09352"/>
        <updated>2021-06-25T02:00:47.261Z</updated>
        <summary type="html"><![CDATA[We propose a reparametrization scheme to address the challenges of applying
differentially private SGD on large neural networks, which are 1) the huge
memory cost of storing individual gradients, 2) the added noise suffering
notorious dimensional dependence. Specifically, we reparametrize each weight
matrix with two \emph{gradient-carrier} matrices of small dimension and a
\emph{residual weight} matrix. We argue that such reparametrization keeps the
forward/backward process unchanged while enabling us to compute the projected
gradient without computing the gradient itself. To learn with differential
privacy, we design \emph{reparametrized gradient perturbation (RGP)} that
perturbs the gradients on gradient-carrier matrices and reconstructs an update
for the original weight from the noisy gradients. Importantly, we use
historical updates to find the gradient-carrier matrices, whose optimality is
rigorously justified under linear regression and empirically verified with deep
learning tasks. RGP significantly reduces the memory cost and improves the
utility. For example, we are the first able to apply differential privacy on
the BERT model and achieve an average accuracy of $83.9\%$ on four downstream
tasks with $\epsilon=8$, which is within $5\%$ loss compared to the non-private
baseline but enjoys much lower privacy leakage risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Da Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jian Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-25T02:00:47.230Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Gradient Estimators for Meta-Reinforcement Learning via Off-Policy Evaluation. (arXiv:2106.13125v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13125</id>
        <link href="http://arxiv.org/abs/2106.13125"/>
        <updated>2021-06-25T02:00:47.219Z</updated>
        <summary type="html"><![CDATA[Model-agnostic meta-reinforcement learning requires estimating the Hessian
matrix of value functions. This is challenging from an implementation
perspective, as repeatedly differentiating policy gradient estimates may lead
to biased Hessian estimates. In this work, we provide a unifying framework for
estimating higher-order derivatives of value functions, based on off-policy
evaluation. Our framework interprets a number of prior approaches as special
cases and elucidates the bias and variance trade-off of Hessian estimates. This
framework also opens the door to a new family of estimates, which can be easily
implemented with auto-differentiation libraries, and lead to performance gains
in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yunhao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozuno_T/0/1/0/all/0/1"&gt;Tadashi Kozuno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1"&gt;Mark Rowland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Munos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1"&gt;Michal Valko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shallow Representation is Deep: Learning Uncertainty-aware and Worst-case Random Feature Dynamics. (arXiv:2106.13066v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13066</id>
        <link href="http://arxiv.org/abs/2106.13066"/>
        <updated>2021-06-25T02:00:47.188Z</updated>
        <summary type="html"><![CDATA[Random features is a powerful universal function approximator that inherits
the theoretical rigor of kernel methods and can scale up to modern learning
tasks. This paper views uncertain system models as unknown or uncertain smooth
functions in universal reproducing kernel Hilbert spaces. By directly
approximating the one-step dynamics function using random features with
uncertain parameters, which are equivalent to a shallow Bayesian neural
network, we then view the whole dynamical system as a multi-layer neural
network. Exploiting the structure of Hamiltonian dynamics, we show that finding
worst-case dynamics realizations using Pontryagin's minimum principle is
equivalent to performing the Frank-Wolfe algorithm on the deep net. Various
numerical experiments on dynamics learning showcase the capacity of our
modeling methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agudelo_Espana_D/0/1/0/all/0/1"&gt;Diego Agudelo-Espa&amp;#xf1;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nemmour_Y/0/1/0/all/0/1"&gt;Yassine Nemmour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jia-Jie Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning to tame divergent density functional approximations: a new path to consensus materials design principles. (arXiv:2106.13109v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2106.13109</id>
        <link href="http://arxiv.org/abs/2106.13109"/>
        <updated>2021-06-25T02:00:47.183Z</updated>
        <summary type="html"><![CDATA[Computational virtual high-throughput screening (VHTS) with density
functional theory (DFT) and machine-learning (ML)-acceleration is essential in
rapid materials discovery. By necessity, efficient DFT-based workflows are
carried out with a single density functional approximation (DFA). Nevertheless,
properties evaluated with different DFAs can be expected to disagree for the
cases with challenging electronic structure (e.g., open shell transition metal
complexes, TMCs) for which rapid screening is most needed and accurate
benchmarks are often unavailable. To quantify the effect of DFA bias, we
introduce an approach to rapidly obtain property predictions from 23
representative DFAs spanning multiple families and "rungs" (e.g., semi-local to
double hybrid) and basis sets on over 2,000 TMCs. Although computed properties
(e.g., spin-state ordering and frontier orbital gap) naturally differ by DFA,
high linear correlations persist across all DFAs. We train independent ML
models for each DFA and observe convergent trends in feature importance; these
features thus provide DFA-invariant, universal design rules. We devise a
strategy to train ML models informed by all 23 DFAs and use them to predict
properties (e.g., spin-splitting energy) of over 182k TMCs. By requiring
consensus of the ANN-predicted DFA properties, we improve correspondence of
these computational lead compounds with literature-mined, experimental
compounds over the single-DFA approach typically employed. Both feature
analysis and consensus-based ML provide efficient, alternative paths to
overcome accuracy limitations of practical DFT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenru Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Taylor_M/0/1/0/all/0/1"&gt;Michael G. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1"&gt;Heather J. Kulik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning with Clustered Generalization. (arXiv:2106.13044v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13044</id>
        <link href="http://arxiv.org/abs/2106.13044"/>
        <updated>2021-06-25T02:00:47.172Z</updated>
        <summary type="html"><![CDATA[We study the recent emerging personalized federated learning (PFL) that aims
at dealing with the challenging problem of Non-I.I.D. data in the federated
learning (FL) setting. The key difference between PFL and conventional FL lies
in the training target, of which the personalized models in PFL usually pursue
a trade-off between personalization (i.e., usually from local models) and
generalization (i.e., usually from the global model) on trained models.
Conventional FL methods can hardly meet this target because of their both
well-developed global and local models. The prevalent PFL approaches usually
maintain a global model to guide the training process of local models and
transfer a proper degree of generalization to them. However, the sole global
model can only provide one direction of generalization and may even transfer
negative effects to some local models when rich statistical diversity exists
across multiple local datasets. Based on our observation, most real or
synthetic data distributions usually tend to be clustered to some degree, of
which we argue different directions of generalization can facilitate the PFL.
In this paper, we propose a novel concept called clustered generalization to
handle the challenge of statistical heterogeneity in FL. Specifically, we
maintain multiple global (generalized) models in the server to associate with
the corresponding amount of local model clusters in clients, and further
formulate the PFL as a bi-level optimization problem that can be solved
efficiently and robustly. We also conduct detailed theoretical analysis and
provide the convergence guarantee for the smooth non-convex objectives.
Experimental results on both synthetic and real datasets show that our approach
surpasses the state-of-the-art by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xueyang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Song Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jingcai Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Option Keyboard: Combining Skills in Reinforcement Learning. (arXiv:2106.13105v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.13105</id>
        <link href="http://arxiv.org/abs/2106.13105"/>
        <updated>2021-06-25T02:00:47.167Z</updated>
        <summary type="html"><![CDATA[The ability to combine known skills to create new ones may be crucial in the
solution of complex reinforcement learning problems that unfold over extended
periods. We argue that a robust way of combining skills is to define and
manipulate them in the space of pseudo-rewards (or "cumulants"). Based on this
premise, we propose a framework for combining skills using the formalism of
options. We show that every deterministic option can be unambiguously
represented as a cumulant defined in an extended domain. Building on this
insight and on previous results on transfer learning, we show how to
approximate options whose cumulants are linear combinations of the cumulants of
known options. This means that, once we have learned options associated with a
set of cumulants, we can instantaneously synthesise options induced by any
linear combination of them, without any learning involved. We describe how this
framework provides a hierarchical interface to the environment whose abstract
actions correspond to combinations of basic skills. We demonstrate the
practical benefits of our approach in a resource management problem and a
navigation task involving a quadrupedal simulated robot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Barreto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borsa_D/0/1/0/all/0/1"&gt;Diana Borsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1"&gt;Shaobo Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1"&gt;Gheorghe Comanici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aygun_E/0/1/0/all/0/1"&gt;Eser Ayg&amp;#xfc;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamel_P/0/1/0/all/0/1"&gt;Philippe Hamel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toyama_D/0/1/0/all/0/1"&gt;Daniel Toyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hunt_J/0/1/0/all/0/1"&gt;Jonathan Hunt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mourad_S/0/1/0/all/0/1"&gt;Shibl Mourad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1"&gt;David Silver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of Depth and Depth-of-Field Effect from Natural Images with Aperture Rendering Generative Adversarial Networks. (arXiv:2106.13041v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13041</id>
        <link href="http://arxiv.org/abs/2106.13041"/>
        <updated>2021-06-25T02:00:47.161Z</updated>
        <summary type="html"><![CDATA[Understanding the 3D world from 2D projected natural images is a fundamental
challenge in computer vision and graphics. Recently, an unsupervised learning
approach has garnered considerable attention owing to its advantages in data
collection. However, to mitigate training limitations, typical methods need to
impose assumptions for viewpoint distribution (e.g., a dataset containing
various viewpoint images) or object shape (e.g., symmetric objects). These
assumptions often restrict applications; for instance, the application to
non-rigid objects or images captured from similar viewpoints (e.g., flower or
bird images) remains a challenge. To complement these approaches, we propose
aperture rendering generative adversarial networks (AR-GANs), which equip
aperture rendering on top of GANs, and adopt focus cues to learn the depth and
depth-of-field (DoF) effect of unlabeled natural images. To address the
ambiguities triggered by unsupervised setting (i.e., ambiguities between smooth
texture and out-of-focus blurs, and between foreground and background blurs),
we develop DoF mixture learning, which enables the generator to learn real
image distribution while generating diverse DoF images. In addition, we devise
a center focus prior to guiding the learning direction. In the experiments, we
demonstrate the effectiveness of AR-GANs in various datasets, such as flower,
bird, and face images, demonstrate their portability by incorporating them into
other 3D representation learning GANs, and validate their applicability in
shallow DoF rendering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1"&gt;Takuhiro Kaneko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artifact Detection and Correction in EEG data: A Review. (arXiv:2106.13081v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.13081</id>
        <link href="http://arxiv.org/abs/2106.13081"/>
        <updated>2021-06-25T02:00:47.145Z</updated>
        <summary type="html"><![CDATA[Electroencephalography (EEG) has countless applications across many of
fields. However, EEG applications are limited by low signal-to-noise ratios.
Multiple types of artifacts contribute to the noisiness of EEG, and many
techniques have been proposed to detect and correct these artifacts. These
techniques range from simply detecting and rejecting artifact ridden segments,
to extracting the noise component from the EEG signal. In this paper we review
a variety of recent and classical techniques for EEG data artifact detection
and correction with a focus on the last half-decade. We compare the strengths
and weaknesses of the approaches and conclude with proposed future directions
for the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sadiya_S/0/1/0/all/0/1"&gt;S Sadiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alhanai_T/0/1/0/all/0/1"&gt;T Alhanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;MM Ghassemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs. (arXiv:2106.13199v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13199</id>
        <link href="http://arxiv.org/abs/2106.13199"/>
        <updated>2021-06-25T02:00:47.140Z</updated>
        <summary type="html"><![CDATA[Sharing data from clinical studies can facilitate innovative data-driven
research and ultimately lead to better public health. However, sharing
biomedical data can put sensitive personal information at risk. This is usually
solved by anonymization, which is a slow and expensive process. An alternative
to anonymization is sharing a synthetic dataset that bears a behaviour similar
to the real data but preserves privacy. As part of the collaboration between
Novartis and the Oxford Big Data Institute, we generate a synthetic dataset
based on COSENTYX (secukinumab) Ankylosing Spondylitis (AS) clinical study. We
apply an Auxiliary Classifier GAN (ac-GAN) to generate synthetic magnetic
resonance images (MRIs) of vertebral units (VUs). The images are conditioned on
the VU location (cervical, thoracic and lumbar). In this paper, we present a
method for generating a synthetic dataset and conduct an in-depth analysis on
its properties of along three key metrics: image fidelity, sample diversity and
dataset privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hanxi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plawinski_J/0/1/0/all/0/1"&gt;Jason Plawinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramaniam_S/0/1/0/all/0/1"&gt;Sajanth Subramaniam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1"&gt;Amir Jamaludin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1"&gt;Timor Kadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Readie_A/0/1/0/all/0/1"&gt;Aimee Readie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ligozio_G/0/1/0/all/0/1"&gt;Gregory Ligozio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohlssen_D/0/1/0/all/0/1"&gt;David Ohlssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baillie_M/0/1/0/all/0/1"&gt;Mark Baillie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coroller_T/0/1/0/all/0/1"&gt;Thibaud Coroller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Software for Dataset-wide XAI: From Local Explanations to Global Insights with Zennit, CoRelAy, and ViRelAy. (arXiv:2106.13200v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13200</id>
        <link href="http://arxiv.org/abs/2106.13200"/>
        <updated>2021-06-25T02:00:47.135Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are known to be strong predictors, but their
prediction strategies can rarely be understood. With recent advances in
Explainable Artificial Intelligence, approaches are available to explore the
reasoning behind those complex models' predictions. One class of approaches are
post-hoc attribution methods, among which Layer-wise Relevance Propagation
(LRP) shows high performance. However, the attempt at understanding a DNN's
reasoning often stops at the attributions obtained for individual samples in
input space, leaving the potential for deeper quantitative analyses untouched.
As a manual analysis without the right tools is often unnecessarily labor
intensive, we introduce three software packages targeted at scientists to
explore model reasoning using attribution approaches and beyond: (1) Zennit - a
highly customizable and intuitive attribution framework implementing LRP and
related approaches in PyTorch, (2) CoRelAy - a framework to easily and quickly
construct quantitative analysis pipelines for dataset-wide analyses of
explanations, and (3) ViRelAy - a web-application to interactively explore
data, attributions, and analysis results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1"&gt;Christopher J. Anders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_D/0/1/0/all/0/1"&gt;David Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1"&gt;Sebastian Lapuschkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Reference Alignment for sparse signals, Uniform Uncertainty Principles and the Beltway Problem. (arXiv:2106.12996v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2106.12996</id>
        <link href="http://arxiv.org/abs/2106.12996"/>
        <updated>2021-06-25T02:00:47.129Z</updated>
        <summary type="html"><![CDATA[Motivated by cutting-edge applications like cryo-electron microscopy
(cryo-EM), the Multi-Reference Alignment (MRA) model entails the learning of an
unknown signal from repeated measurements of its images under the latent action
of a group of isometries and additive noise of magnitude $\sigma$. Despite
significant interest, a clear picture for understanding rates of estimation in
this model has emerged only recently, particularly in the high-noise regime
$\sigma \gg 1$ that is highly relevant in applications. Recent investigations
have revealed a remarkable asymptotic sample complexity of order $\sigma^6$ for
certain signals whose Fourier transforms have full support, in stark contrast
to the traditional $\sigma^2$ that arise in regular models. Often prohibitively
large in practice, these results have prompted the investigation of variations
around the MRA model where better sample complexity may be achieved. In this
paper, we show that \emph{sparse} signals exhibit an intermediate $\sigma^4$
sample complexity even in the classical MRA model. Our results explore and
exploit connections of the MRA estimation problem with two classical topics in
applied mathematics: the \textit{beltway problem} from combinatorial
optimization, and \textit{uniform uncertainty principles} from harmonic
analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Subhro Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rigollet_P/0/1/0/all/0/1"&gt;Philippe Rigollet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetric Wasserstein Autoencoders. (arXiv:2106.13024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13024</id>
        <link href="http://arxiv.org/abs/2106.13024"/>
        <updated>2021-06-25T02:00:47.123Z</updated>
        <summary type="html"><![CDATA[Leveraging the framework of Optimal Transport, we introduce a new family of
generative autoencoders with a learnable prior, called Symmetric Wasserstein
Autoencoders (SWAEs). We propose to symmetrically match the joint distributions
of the observed data and the latent representation induced by the encoder and
the decoder. The resulting algorithm jointly optimizes the modelling losses in
both the data and the latent spaces with the loss in the data space leading to
the denoising effect. With the symmetric treatment of the data and the latent
representation, the algorithm implicitly preserves the local structure of the
data in the latent space. To further improve the quality of the latent
representation, we incorporate a reconstruction loss into the objective, which
significantly benefits both the generation and reconstruction. We empirically
show the superior performance of SWAEs over the state-of-the-art generative
autoencoders in terms of classification, reconstruction, and generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Sun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13122</id>
        <link href="http://arxiv.org/abs/2106.13122"/>
        <updated>2021-06-25T02:00:47.107Z</updated>
        <summary type="html"><![CDATA[Recently, vision transformers and MLP-based models have been developed in
order to address some of the prevalent weaknesses in convolutional neural
networks. Due to the novelty of transformers being used in this domain along
with the self-attention mechanism, it remains unclear to what degree these
architectures are robust to corruptions. Despite some works proposing that data
augmentation remains essential for a model to be robust against corruptions, we
propose to explore the impact that the architecture has on corruption
robustness. We find that vision transformer architectures are inherently more
robust to corruptions than the ResNet-50 and MLP-Mixers. We also find that
vision transformers with 5 times fewer parameters than a ResNet-50 have more
shape bias. Our code is available to reproduce.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1"&gt;Katelyn Morrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1"&gt;Benjamin Gilby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1"&gt;Colton Lipchak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1"&gt;Adam Mattioli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1"&gt;Adriana Kovashka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the relationship between predictive coding and backpropagation. (arXiv:2106.13082v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.13082</id>
        <link href="http://arxiv.org/abs/2106.13082"/>
        <updated>2021-06-25T02:00:47.102Z</updated>
        <summary type="html"><![CDATA[In this manuscript, I review and extend recent work on the relationship
between predictive coding and backpropagation for training artificial neural
networks on supervised learning tasks. I also discuss some implications of
these results for the interpretation of predictive coding and deep neural
networks as models of biological learning and I describe a repository of
functions, Torch2PC, for performing predictive coding with PyTorch neural
network models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Rosenbaum_R/0/1/0/all/0/1"&gt;Robert Rosenbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AVHYAS: A Free and Open Source QGIS Plugin for Advanced Hyperspectral Image Analysis. (arXiv:2106.12776v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12776</id>
        <link href="http://arxiv.org/abs/2106.12776"/>
        <updated>2021-06-25T02:00:47.097Z</updated>
        <summary type="html"><![CDATA[Advanced Hyperspectral Data Analysis Software (AVHYAS) plugin is a python3
based quantum GIS (QGIS) plugin designed to process and analyse hyperspectral
(Hx) images. It is developed to guarantee full usage of present and future Hx
airborne or spaceborne sensors and provides access to advanced algorithms for
Hx data processing. The software is freely available and offers a range of
basic and advanced tools such as atmospheric correction (for airborne AVIRISNG
image), standard processing tools as well as powerful machine learning and Deep
Learning interfaces for Hx data analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lyngdoh_R/0/1/0/all/0/1"&gt;Rosly Boy Lyngdoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahadevan_A/0/1/0/all/0/1"&gt;Anand S Sahadevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahmad_T/0/1/0/all/0/1"&gt;Touseef Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rathore_P/0/1/0/all/0/1"&gt;Pradyuman Singh Rathore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mishra_M/0/1/0/all/0/1"&gt;Manoj Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Praveen Kumar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1"&gt;Arundhati Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data. (arXiv:2106.13213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13213</id>
        <link href="http://arxiv.org/abs/2106.13213"/>
        <updated>2021-06-25T02:00:47.092Z</updated>
        <summary type="html"><![CDATA[Mental health conditions remain underdiagnosed even in countries with common
access to advanced medical care. The ability to accurately and efficiently
predict mood from easily collectible data has several important implications
for the early detection, intervention, and treatment of mental health
disorders. One promising data source to help monitor human behavior is daily
smartphone usage. However, care must be taken to summarize behaviors without
identifying the user through personal (e.g., personally identifiable
information) or protected (e.g., race, gender) attributes. In this paper, we
study behavioral markers of daily mood using a recent dataset of mobile
behaviors from adolescent populations at high risk of suicidal behaviors. Using
computational models, we find that language and multimodal representations of
mobile typed text (spanning typed characters, words, keystroke timings, and app
usage) are predictive of daily mood. However, we find that models trained to
predict mood often also capture private user identities in their intermediate
representations. To tackle this problem, we evaluate approaches that obfuscate
user identity while remaining predictive. By combining multimodal
representations with privacy-preserving learning, we are able to push forward
the performance-privacy frontier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Terrance Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_A/0/1/0/all/0/1"&gt;Anna Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muszynski_M/0/1/0/all/0/1"&gt;Michal Muszynski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_R/0/1/0/all/0/1"&gt;Ryo Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1"&gt;Nicholas Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1"&gt;Randy Auerbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brent_D/0/1/0/all/0/1"&gt;David Brent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIxBN: library for learning Bayesian networks from mixed data. (arXiv:2106.13194v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13194</id>
        <link href="http://arxiv.org/abs/2106.13194"/>
        <updated>2021-06-25T02:00:47.087Z</updated>
        <summary type="html"><![CDATA[This paper describes a new library for learning Bayesian networks from data
containing discrete and continuous variables (mixed data). In addition to the
classical learning methods on discretized data, this library proposes its
algorithm that allows structural learning and parameters learning from mixed
data without discretization since data discretization leads to information
loss. This algorithm based on mixed MI score function for structural learning,
and also linear regression and Gaussian distribution approximation for
parameters learning. The library also offers two algorithms for enumerating
graph structures - the greedy Hill-Climbing algorithm and the evolutionary
algorithm. Thus the key capabilities of the proposed library are as follows:
(1) structural and parameters learning of a Bayesian network on discretized
data, (2) structural and parameters learning of a Bayesian network on mixed
data using the MI mixed score function and Gaussian approximation, (3)
launching learning algorithms on one of two algorithms for enumerating graph
structures - Hill-Climbing and the evolutionary algorithm. Since the need for
mixed data representation comes from practical necessity, the advantages of our
implementations are evaluated in the context of solving approximation and gap
recovery problems on synthetic data and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bubnova_A/0/1/0/all/0/1"&gt;Anna V. Bubnova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deeva_I/0/1/0/all/0/1"&gt;Irina Deeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1"&gt;Anna V. Kalyuzhnaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fully Problem-Dependent Regret Lower Bound for Finite-Horizon MDPs. (arXiv:2106.13013v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13013</id>
        <link href="http://arxiv.org/abs/2106.13013"/>
        <updated>2021-06-25T02:00:47.072Z</updated>
        <summary type="html"><![CDATA[We derive a novel asymptotic problem-dependent lower-bound for regret
minimization in finite-horizon tabular Markov Decision Processes (MDPs). While,
similar to prior work (e.g., for ergodic MDPs), the lower-bound is the solution
to an optimization problem, our derivation reveals the need for an additional
constraint on the visitation distribution over state-action pairs that
explicitly accounts for the dynamics of the MDP. We provide a characterization
of our lower-bound through a series of examples illustrating how different MDPs
may have significantly different complexity. 1) We first consider a "difficult"
MDP instance, where the novel constraint based on the dynamics leads to a
larger lower-bound (i.e., a larger regret) compared to the classical analysis.
2) We then show that our lower-bound recovers results previously derived for
specific MDP instances. 3) Finally, we show that, in certain "simple" MDPs, the
lower bound is considerably smaller than in the general case and it does not
scale with the minimum action gap at all. We show that this last result is
attainable (up to $poly(H)$ terms, where $H$ is the horizon) by providing a
regret upper-bound based on policy gaps for an optimistic algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tirinzoni_A/0/1/0/all/0/1"&gt;Andrea Tirinzoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1"&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1"&gt;Alessandro Lazaric&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Biologically Plausible Convolutional Networks. (arXiv:2106.13031v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13031</id>
        <link href="http://arxiv.org/abs/2106.13031"/>
        <updated>2021-06-25T02:00:47.066Z</updated>
        <summary type="html"><![CDATA[Convolutional networks are ubiquitous in deep learning. They are particularly
useful for images, as they reduce the number of parameters, reduce training
time, and increase accuracy. However, as a model of the brain they are
seriously problematic, since they require weight sharing - something real
neurons simply cannot do. Consequently, while neurons in the brain can be
locally connected (one of the features of convolutional networks), they cannot
be convolutional. Locally connected but non-convolutional networks, however,
significantly underperform convolutional ones. This is troublesome for studies
that use convolutional networks to explain activity in the visual system. Here
we study plausible alternatives to weight sharing that aim at the same
regularization principle, which is to make each neuron within a pool react
similarly to identical inputs. The most natural way to do that is by showing
the network multiple translations of the same image, akin to saccades in animal
vision. However, this approach requires many translations, and doesn't remove
the performance gap. We propose instead to add lateral connectivity to a
locally connected network, and allow learning via Hebbian plasticity. This
requires the network to pause occasionally for a sleep-like phase of "weight
sharing". This method enables locally connected networks to achieve nearly
convolutional performance on ImageNet, thus supporting convolutional networks
as a model of the visual stream.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pogodin_R/0/1/0/all/0/1"&gt;Roman Pogodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_Y/0/1/0/all/0/1"&gt;Yash Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1"&gt;Timothy P. Lillicrap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latham_P/0/1/0/all/0/1"&gt;Peter E. Latham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-training transformer-based framework on large-scale pediatric claims data for downstream population-specific tasks. (arXiv:2106.13095v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13095</id>
        <link href="http://arxiv.org/abs/2106.13095"/>
        <updated>2021-06-25T02:00:47.059Z</updated>
        <summary type="html"><![CDATA[The adoption of electronic health records (EHR) has become universal during
the past decade, which has afforded in-depth data-based research. By learning
from the large amount of healthcare data, various data-driven models have been
built to predict future events for different medical tasks, such as auto
diagnosis and heart-attack prediction. Although EHR is abundant, the population
that satisfies specific criteria for learning population-specific tasks is
scarce, making it challenging to train data-hungry deep learning models. This
study presents the Claim Pre-Training (Claim-PT) framework, a generic
pre-training model that first trains on the entire pediatric claims dataset,
followed by a discriminative fine-tuning on each population-specific task. The
semantic meaning of medical events can be captured in the pre-training stage,
and the effective knowledge transfer is completed through the task-aware
fine-tuning stage. The fine-tuning process requires minimal parameter
modification without changing the model architecture, which mitigates the data
scarcity issue and helps train the deep learning model adequately on small
patient cohorts. We conducted experiments on a real-world claims dataset with
more than one million patient records. Experimental results on two downstream
tasks demonstrated the effectiveness of our method: our general task-agnostic
pre-training framework outperformed tailored task-specific models, achieving
more than 10\% higher in model performance as compared to baselines. In
addition, our framework showed a great generalizability potential to transfer
learned knowledge from one institution to another, paving the way for future
healthcare model pre-training across institutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xianlong Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Simon Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FitVid: Overfitting in Pixel-Level Video Prediction. (arXiv:2106.13195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13195</id>
        <link href="http://arxiv.org/abs/2106.13195"/>
        <updated>2021-06-25T02:00:47.052Z</updated>
        <summary type="html"><![CDATA[An agent that is capable of predicting what happens next can perform a
variety of tasks through planning with no additional training. Furthermore,
such an agent can internally represent the complex dynamics of the real-world
and therefore can acquire a representation useful for a variety of visual
perception tasks. This makes predicting the future frames of a video,
conditioned on the observed past and potentially future actions, an interesting
task which remains exceptionally challenging despite many recent advances.
Existing video prediction models have shown promising results on simple narrow
benchmarks but they generate low quality predictions on real-life datasets with
more complicated dynamics or broader domain. There is a growing body of
evidence that underfitting on the training data is one of the primary causes
for the low quality predictions. In this paper, we argue that the inefficient
use of parameters in the current video models is the main reason for
underfitting. Therefore, we introduce a new architecture, named FitVid, which
is capable of severe overfitting on the common benchmarks while having similar
parameter count as the current state-of-the-art models. We analyze the
consequences of overfitting, illustrating how it can produce unexpected
outcomes such as generating high quality output by repeating the training data,
and how it can be mitigated using existing image augmentation techniques. As a
result, FitVid outperforms the current state-of-the-art models across four
different video prediction benchmarks on four different metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Babaeizadeh_M/0/1/0/all/0/1"&gt;Mohammad Babaeizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1"&gt;Mohammad Taghi Saffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erhan_D/0/1/0/all/0/1"&gt;Dumitru Erhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using machine learning techniques to predict hospital admission at the emergency department. (arXiv:2106.12921v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12921</id>
        <link href="http://arxiv.org/abs/2106.12921"/>
        <updated>2021-06-25T02:00:47.047Z</updated>
        <summary type="html"><![CDATA[Introduction: One of the most important tasks in the Emergency Department
(ED) is to promptly identify the patients who will benefit from hospital
admission. Machine Learning (ML) techniques show promise as diagnostic aids in
healthcare. Material and methods: We investigated the following features
seeking to investigate their performance in predicting hospital admission:
serum levels of Urea, Creatinine, Lactate Dehydrogenase, Creatine Kinase,
C-Reactive Protein, Complete Blood Count with differential, Activated Partial
Thromboplastin Time, D Dimer, International Normalized Ratio, age, gender,
triage disposition to ED unit and ambulance utilization. A total of 3,204 ED
visits were analyzed. Results: The proposed algorithms generated models which
demonstrated acceptable performance in predicting hospital admission of ED
patients. The range of F-measure and ROC Area values of all eight evaluated
algorithms were [0.679-0.708] and [0.734-0.774], respectively. Discussion: The
main advantages of this tool include easy access, availability, yes/no result,
and low cost. The clinical implications of our approach might facilitate a
shift from traditional clinical decision-making to a more sophisticated model.
Conclusion: Developing robust prognostic models with the utilization of common
biomarkers is a project that might shape the future of emergency medicine. Our
findings warrant confirmation with implementation in pragmatic ED trials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feretzakis_G/0/1/0/all/0/1"&gt;Georgios Feretzakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlis_G/0/1/0/all/0/1"&gt;George Karlis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loupelis_E/0/1/0/all/0/1"&gt;Evangelos Loupelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalles_D/0/1/0/all/0/1"&gt;Dimitris Kalles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzikyriakou_R/0/1/0/all/0/1"&gt;Rea Chatzikyriakou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trakas_N/0/1/0/all/0/1"&gt;Nikolaos Trakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karakou_E/0/1/0/all/0/1"&gt;Eugenia Karakou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakagianni_A/0/1/0/all/0/1"&gt;Aikaterini Sakagianni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzelves_L/0/1/0/all/0/1"&gt;Lazaros Tzelves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petropoulou_S/0/1/0/all/0/1"&gt;Stavroula Petropoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tika_A/0/1/0/all/0/1"&gt;Aikaterini Tika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalainas_I/0/1/0/all/0/1"&gt;Ilias Dalainas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaldis_V/0/1/0/all/0/1"&gt;Vasileios Kaldis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next-Day Bitcoin Price Forecast Based on Artificial intelligence Methods. (arXiv:2106.12961v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2106.12961</id>
        <link href="http://arxiv.org/abs/2106.12961"/>
        <updated>2021-06-25T02:00:47.032Z</updated>
        <summary type="html"><![CDATA[In recent years, Bitcoin price prediction has attracted the interest of
researchers and investors. However, the accuracy of previous studies is not
well enough. Machine learning and deep learning methods have been proved to
have strong prediction ability in this area. This paper proposed a method
combined with Ensemble Empirical Mode Decomposition (EEMD) and a deep learning
method called long short-term memory (LSTM) to research the problem of next-day
Bitcoin price forecast.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Yang_L/0/1/0/all/0/1"&gt;Liping Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rate Distortion Characteristic Modeling for Neural Image Compression. (arXiv:2106.12954v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12954</id>
        <link href="http://arxiv.org/abs/2106.12954"/>
        <updated>2021-06-25T02:00:47.022Z</updated>
        <summary type="html"><![CDATA[End-to-end optimization capability offers neural image compression (NIC)
superior lossy compression performance. However, distinct models are required
to be trained to reach different points in the rate-distortion (R-D) space. In
this paper, we consider the problem of R-D characteristic analysis and modeling
for NIC. We make efforts to formulate the essential mathematical functions to
describe the R-D behavior of NIC using deep network and statistical modeling.
Thus continuous bit-rate points could be elegantly realized by leveraging such
model via a single trained network. In this regard, we propose a plugin-in
module to learn the relationship between the target bit-rate and the binary
representation for the latent variable of auto-encoder. Furthermore, we model
the rate and distortion characteristic of NIC as a function of the coding
parameter $\lambda$ respectively. Our experiments show our proposed method is
easy to adopt and obtains competitive coding performance with fixed-rate coding
approaches, which would benefit the practical deployment of NIC. In addition,
the proposed model could be applied to NIC rate control with limited bit-rate
error using a single network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chuanmin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Ziqing Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shanshe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1"&gt;Siwei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantization Aware Training, ERNIE and Kurtosis Regularizer: a short empirical study. (arXiv:2106.13035v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13035</id>
        <link href="http://arxiv.org/abs/2106.13035"/>
        <updated>2021-06-25T02:00:47.016Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models like Ernie or Bert are currently used in many
applications. These models come with a set of pre-trained weights typically
obtained in unsupervised/self-supervised modality on a huge amount of data.
After that, they are fine-tuned on a specific task. Applications then use these
models for inference, and often some additional constraints apply, like low
power-budget or low latency between input and output. The main avenue to meet
these additional requirements for the inference settings, is to use low
precision computation (e.g. INT8 rather than FP32), but this comes with a cost
of deteriorating the functional performance (e.g. accuracy) of the model. Some
approaches have been developed to tackle the problem and go beyond the
limitations of the PTO (Post-Training Quantization), more specifically the QAT
(Quantization Aware Training, see [4]) is a procedure that interferes with the
training process in order to make it affected (or simply disturbed) by the
quantization phase during the training itself. Besides QAT, recently
Intel-Habana Labs have proposed an additional and more direct way to make the
training results more robust to subsequent quantization which uses a
regularizer, therefore changing the loss function that drives the training
procedure. But their proposal does not work out-of-the-box for pre-trained
models like Ernie, for example. In this short paper we show why this is not
happening (for the Ernie case) and we propose a very basic way to deal with it,
sharing as well some initial results (increase in final INT8 accuracy) that
might be of interest to practitioners willing to use Ernie in their
applications, in low precision regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zanetti_A/0/1/0/all/0/1"&gt;Andrea Zanetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Uncertainty in Bayesian Deep Learning. (arXiv:2106.13055v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13055</id>
        <link href="http://arxiv.org/abs/2106.13055"/>
        <updated>2021-06-25T02:00:47.010Z</updated>
        <summary type="html"><![CDATA[Neural Linear Models (NLM) are deep Bayesian models that produce predictive
uncertainty by learning features from the data and then performing Bayesian
linear regression over these features. Despite their popularity, few works have
focused on formally evaluating the predictive uncertainties of these models.
Furthermore, existing works point out the difficulties of encoding domain
knowledge in models like NLMs, making them unsuitable for applications where
interpretability is required. In this work, we show that traditional training
procedures for NLMs can drastically underestimate uncertainty in data-scarce
regions. We identify the underlying reasons for this behavior and propose a
novel training method that can both capture useful predictive uncertainties as
well as allow for incorporation of domain knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lorsung_C/0/1/0/all/0/1"&gt;Cooper Lorsung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploration-Exploitation in Multi-Agent Competition: Convergence with Bounded Rationality. (arXiv:2106.12928v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.12928</id>
        <link href="http://arxiv.org/abs/2106.12928"/>
        <updated>2021-06-25T02:00:47.000Z</updated>
        <summary type="html"><![CDATA[The interplay between exploration and exploitation in competitive multi-agent
learning is still far from being well understood. Motivated by this, we study
smooth Q-learning, a prototypical learning model that explicitly captures the
balance between game rewards and exploration costs. We show that Q-learning
always converges to the unique quantal-response equilibrium (QRE), the standard
solution concept for games under bounded rationality, in weighted zero-sum
polymatrix games with heterogeneous learning agents using positive exploration
rates. Complementing recent results about convergence in weighted potential
games, we show that fast convergence of Q-learning in competitive settings is
obtained regardless of the number of agents and without any need for parameter
fine-tuning. As showcased by our experiments in network zero-sum games, these
theoretical results provide the necessary guarantees for an algorithmic
approach to the currently open problem of equilibrium selection in competitive
multi-agent settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1"&gt;Stefanos Leonardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1"&gt;Georgios Piliouras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spendlove_K/0/1/0/all/0/1"&gt;Kelly Spendlove&lt;/a&gt;,</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Regret Bounds for Tracking Experts with Memory. (arXiv:2106.13021v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13021</id>
        <link href="http://arxiv.org/abs/2106.13021"/>
        <updated>2021-06-25T02:00:46.983Z</updated>
        <summary type="html"><![CDATA[We address the problem of sequential prediction with expert advice in a
non-stationary environment with long-term memory guarantees in the sense of
Bousquet and Warmuth [4]. We give a linear-time algorithm that improves on the
best known regret bounds [26]. This algorithm incorporates a relative entropy
projection step. This projection is advantageous over previous weight-sharing
approaches in that weight updates may come with implicit costs as in for
example portfolio optimization. We give an algorithm to compute this projection
step in linear time, which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1"&gt;James Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbster_M/0/1/0/all/0/1"&gt;Mark Herbster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Networks for Dengue Prediction: A Systematic Review. (arXiv:2106.12905v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12905</id>
        <link href="http://arxiv.org/abs/2106.12905"/>
        <updated>2021-06-25T02:00:46.967Z</updated>
        <summary type="html"><![CDATA[Due to a lack of treatments and universal vaccine, early forecasts of Dengue
are an important tool for disease control. Neural networks are powerful
predictive models that have made contributions to many areas of public health.
In this systematic review, we provide an introduction to the neural networks
relevant to Dengue forecasting and review their applications in the literature.
The objective is to help inform model design for future work. Following the
PRISMA guidelines, we conduct a systematic search of studies that use neural
networks to forecast Dengue in human populations. We summarize the relative
performance of neural networks and comparator models, model architectures and
hyper-parameters, as well as choices of input features. Nineteen papers were
included. Most studies implement shallow neural networks using historical
Dengue incidence and meteorological input features. Prediction horizons tend to
be short. Building on the strengths of neural networks, most studies use
granular observations at the city or sub-national level. Performance of neural
networks relative to comparators such as Support Vector Machines varies across
study contexts. The studies suggest that neural networks can provide good
predictions of Dengue and should be included in the set of candidate models.
The use of convolutional, recurrent, or deep networks is relatively unexplored
but offers promising avenues for further research, as does the use of a broader
set of input features such as social media or mobile phone data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roster_K/0/1/0/all/0/1"&gt;Kirstin Roster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_F/0/1/0/all/0/1"&gt;Francisco A. Rodrigues&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Abstraction of Markov Population Dynamics via Generative Adversarial Nets. (arXiv:2106.12981v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12981</id>
        <link href="http://arxiv.org/abs/2106.12981"/>
        <updated>2021-06-25T02:00:46.958Z</updated>
        <summary type="html"><![CDATA[Markov Population Models are a widespread formalism used to model the
dynamics of complex systems, with applications in Systems Biology and many
other fields. The associated Markov stochastic process in continuous time is
often analyzed by simulation, which can be costly for large or stiff systems,
particularly when a massive number of simulations has to be performed (e.g. in
a multi-scale model). A strategy to reduce computational load is to abstract
the population model, replacing it with a simpler stochastic model, faster to
simulate. Here we pursue this idea, building on previous works and constructing
a generator capable of producing stochastic trajectories in continuous space
and discrete time. This generator is learned automatically from simulations of
the original model in a Generative Adversarial setting. Compared to previous
works, which rely on deep neural networks and Dirichlet processes, we explore
the use of state of the art generative models, which are flexible enough to
learn a full trajectory rather than a single transition kernel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cairoli_F/0/1/0/all/0/1"&gt;Francesca Cairoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1"&gt;Ginevra Carbone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1"&gt;Luca Bortolussi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[rSoccer: A Framework for Studying Reinforcement Learning in Small and Very Small Size Robot Soccer. (arXiv:2106.12895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12895</id>
        <link href="http://arxiv.org/abs/2106.12895"/>
        <updated>2021-06-25T02:00:46.948Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is an active research area with a vast number of
applications in robotics, and the RoboCup competition is an interesting
environment for studying and evaluating reinforcement learning methods. A known
difficulty in applying reinforcement learning to robotics is the high number of
experience samples required, being the use of simulated environments for
training the agents followed by transfer learning to real-world (sim-to-real) a
viable path. This article introduces an open-source simulator for the IEEE Very
Small Size Soccer and the Small Size League optimized for reinforcement
learning experiments. We also propose a framework for creating OpenAI Gym
environments with a set of benchmarks tasks for evaluating single-agent and
multi-agent robot soccer skills. We then demonstrate the learning capabilities
of two state-of-the-art reinforcement learning methods as well as their
limitations in certain scenarios introduced in this framework. We believe this
will make it easier for more teams to compete in these categories using
end-to-end reinforcement learning approaches and further develop this research
area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martins_F/0/1/0/all/0/1"&gt;Felipe B. Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1"&gt;Mateus G. Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1"&gt;Hansenclever F. Bassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braga_P/0/1/0/all/0/1"&gt;Pedro H. M. Braga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barros_E/0/1/0/all/0/1"&gt;Edna S. Barros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SofaMyRoom: a fast and multiplatform "shoebox" room simulator for binaural room impulse response dataset generation. (arXiv:2106.12992v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.12992</id>
        <link href="http://arxiv.org/abs/2106.12992"/>
        <updated>2021-06-25T02:00:46.931Z</updated>
        <summary type="html"><![CDATA[This paper introduces a shoebox room simulator able to systematically
generate synthetic datasets of binaural room impulse responses (BRIRs) given an
arbitrary set of head-related transfer functions (HRTFs). The evaluation of
machine hearing algorithms frequently requires BRIR datasets in order to
simulate the acoustics of any environment. However, currently available
solutions typically consider only HRTFs measured on dummy heads, which poorly
characterize the high variability in spatial sound perception. Our solution
allows to integrate a room impulse response (RIR) simulator with different HRTF
sets represented in Spatially Oriented Format for Acoustics (SOFA). The source
code and the compiled binaries for different operating systems allow to both
advanced and non-expert users to benefit from our toolbox, see
https://github.com/spatialaudiotools/sofamyroom/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barumerli_R/0/1/0/all/0/1"&gt;Roberto Barumerli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bianchi_D/0/1/0/all/0/1"&gt;Daniele Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geronazzo_M/0/1/0/all/0/1"&gt;Michele Geronazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avanzini_F/0/1/0/all/0/1"&gt;Federico Avanzini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RikoNet: A Novel Anime Recommendation Engine. (arXiv:2106.12970v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12970</id>
        <link href="http://arxiv.org/abs/2106.12970"/>
        <updated>2021-06-25T02:00:46.924Z</updated>
        <summary type="html"><![CDATA[Anime is quite well-received today, especially among the younger generations.
With many genres of available shows, more and more people are increasingly
getting attracted to this niche section of the entertainment industry. As anime
has recently garnered mainstream attention, we have insufficient information
regarding users' penchant and watching habits. Therefore, it is an uphill task
to build a recommendation engine for this relatively obscure entertainment
medium. In this attempt, we have built a novel hybrid recommendation system
that could act both as a recommendation system and as a means of exploring new
anime genres and titles. We have analyzed the general trends in this field and
the users' watching habits for coming up with our efficacious solution. Our
solution employs deep autoencoders for the tasks of predicting ratings and
generating embeddings. Following this, we formed clusters using the embeddings
of the anime titles. These clusters form the search space for anime with
similarities and are used to find anime similar to the ones liked and disliked
by the user. This method, combined with the predicted ratings, forms the novel
hybrid filter. In this article, we have demonstrated this idea and compared the
performance of our implemented model with the existing state-of-the-art
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soni_B/0/1/0/all/0/1"&gt;Badal Soni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakuria_D/0/1/0/all/0/1"&gt;Debangan Thakuria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nath_N/0/1/0/all/0/1"&gt;Nilutpal Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1"&gt;Navarun Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boro_B/0/1/0/all/0/1"&gt;Bhaskarananda Boro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Modern Techniques in Optimization: Frank-Wolfe, Nesterov's Momentum, and Polyak's Momentum. (arXiv:2106.12923v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.12923</id>
        <link href="http://arxiv.org/abs/2106.12923"/>
        <updated>2021-06-25T02:00:46.916Z</updated>
        <summary type="html"><![CDATA[In the first part of this dissertation research, we develop a modular
framework that can serve as a recipe for constructing and analyzing iterative
algorithms for convex optimization. Specifically, our work casts optimization
as iteratively playing a two-player zero-sum game. Many existing optimization
algorithms including Frank-Wolfe and Nesterov's acceleration methods can be
recovered from the game by pitting two online learners with appropriate
strategies against each other. Furthermore, the sum of the weighted average
regrets of the players in the game implies the convergence rate. As a result,
our approach provides simple alternative proofs to these algorithms. Moreover,
we demonstrate that our approach of optimization as iteratively playing a game
leads to three new fast Frank-Wolfe-like algorithms for some constraint sets,
which further shows that our framework is indeed generic, modular, and
easy-to-use.

In the second part, we develop a modular analysis of provable acceleration
via Polyak's momentum for certain problems, which include solving the classical
strongly quadratic convex problems, training a wide ReLU network under the
neural tangent kernel regime, and training a deep linear network with an
orthogonal initialization. We develop a meta theorem and show that when
applying Polyak's momentum for these problems, the induced dynamics exhibit a
form where we can directly apply our meta theorem.

In the last part of the dissertation, we show another advantage of the use of
Polyak's momentum -- it facilitates fast saddle point escape in smooth
non-convex optimization. This result, together with those of the second part,
sheds new light on Polyak's momentum in modern non-convex optimization and deep
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun-Kun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Bottleneck: Exact Analysis of (Quantized) Neural Networks. (arXiv:2106.12912v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12912</id>
        <link href="http://arxiv.org/abs/2106.12912"/>
        <updated>2021-06-25T02:00:46.910Z</updated>
        <summary type="html"><![CDATA[The information bottleneck (IB) principle has been suggested as a way to
analyze deep neural networks. The learning dynamics are studied by inspecting
the mutual information (MI) between the hidden layers and the input and output.
Notably, separate fitting and compression phases during training have been
reported. This led to some controversy including claims that the observations
are not reproducible and strongly dependent on the type of activation function
used as well as on the way the MI is estimated. Our study confirms that
different ways of binning when computing the MI lead to qualitatively different
results, either supporting or refusing IB conjectures. To resolve the
controversy, we study the IB principle in settings where MI is non-trivial and
can be computed exactly. We monitor the dynamics of quantized neural networks,
that is, we discretize the whole deep learning system so that no approximation
is required when computing the MI. This allows us to quantify the information
flow without measurement errors. In this setting, we observed a fitting phase
for all layers and a compression phase for the output layer in all experiments;
the compression in the hidden layers was dependent on the type of activation
function. Our study shows that the initial IB results were not artifacts of
binning when computing the MI. However, the critical claim that the compression
phase may not be observed for some networks also holds true.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzen_S/0/1/0/all/0/1"&gt;Stephan Sloth Lorenzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1"&gt;Christian Igel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_M/0/1/0/all/0/1"&gt;Mads Nielsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Topic Segmentation of Meetings with BERT Embeddings. (arXiv:2106.12978v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12978</id>
        <link href="http://arxiv.org/abs/2106.12978"/>
        <updated>2021-06-25T02:00:46.903Z</updated>
        <summary type="html"><![CDATA[Topic segmentation of meetings is the task of dividing multi-person meeting
transcripts into topic blocks. Supervised approaches to the problem have proven
intractable due to the difficulties in collecting and accurately annotating
large datasets. In this paper we show how previous unsupervised topic
segmentation methods can be improved using pre-trained neural architectures. We
introduce an unsupervised approach based on BERT embeddings that achieves a
15.5% reduction in error rate over existing unsupervised approaches applied to
two popular datasets for meeting transcripts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solbiati_A/0/1/0/all/0/1"&gt;Alessandro Solbiati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1"&gt;Kevin Heffernan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damaskinos_G/0/1/0/all/0/1"&gt;Georgios Damaskinos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1"&gt;Shivani Poddar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1"&gt;Shubham Modi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cali_J/0/1/0/all/0/1"&gt;Jacques Cali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial-Temporal Graph ODE Networks for Traffic Flow Forecasting. (arXiv:2106.12931v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12931</id>
        <link href="http://arxiv.org/abs/2106.12931"/>
        <updated>2021-06-25T02:00:46.897Z</updated>
        <summary type="html"><![CDATA[Spatial-temporal forecasting has attracted tremendous attention in a wide
range of applications, and traffic flow prediction is a canonical and typical
example. The complex and long-range spatial-temporal correlations of traffic
flow bring it to a most intractable challenge. Existing works typically utilize
shallow graph convolution networks (GNNs) and temporal extracting modules to
model spatial and temporal dependencies respectively. However, the
representation ability of such models is limited due to: (1) shallow GNNs are
incapable to capture long-range spatial correlations, (2) only spatial
connections are considered and a mass of semantic connections are ignored,
which are of great importance for a comprehensive understanding of traffic
networks. To this end, we propose Spatial-Temporal Graph Ordinary Differential
Equation Networks (STGODE). Specifically, we capture spatial-temporal dynamics
through a tensor-based ordinary differential equation (ODE), as a result,
deeper networks can be constructed and spatial-temporal features are utilized
synchronously. To understand the network more comprehensively, semantical
adjacency matrix is considered in our model, and a well-design temporal
dialated convolution structure is used to capture long term temporal
dependencies. We evaluate our model on multiple real-world traffic datasets and
superior performance is achieved over state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1"&gt;Qingqing Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1"&gt;Guojie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1"&gt;Kunqing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnt Sparsification for Interpretable Graph Neural Networks. (arXiv:2106.12920v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12920</id>
        <link href="http://arxiv.org/abs/2106.12920"/>
        <updated>2021-06-25T02:00:46.881Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have achieved great success on various tasks and
fields that require relational modeling. GNNs aggregate node features using the
graph structure as inductive biases resulting in flexible and powerful models.
However, GNNs remain hard to interpret as the interplay between node features
and graph structure is only implicitly learned. In this paper, we propose a
novel method called Kedge for explicitly sparsifying the underlying graph by
removing unnecessary neighbors. Our key idea is based on a tractable method for
sparsification using the Hard Kumaraswamy distribution that can be used in
conjugation with any GNN model. Kedge learns edge masks in a modular fashion
trained with any GNN allowing for gradient based optimization in an end-to-end
fashion. We demonstrate through extensive experiments that our model Kedge can
prune a large proportion of the edges with only a minor effect on the test
accuracy. Specifically, in the PubMed dataset, Kedge learns to drop more than
80% of the edges with an accuracy drop of merely 2% showing that graph
structure has only a small contribution in comparison to node features.
Finally, we also show that Kedge effectively counters the over-smoothing
phenomena in deep GNNs by maintaining good task performance with increasing GNN
layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rathee_M/0/1/0/all/0/1"&gt;Mandeep Rathee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zijian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funke_T/0/1/0/all/0/1"&gt;Thorben Funke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosla_M/0/1/0/all/0/1"&gt;Megha Khosla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental limits for learning hidden Markov model parameters. (arXiv:2106.12936v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12936</id>
        <link href="http://arxiv.org/abs/2106.12936"/>
        <updated>2021-06-25T02:00:46.876Z</updated>
        <summary type="html"><![CDATA[We study the frontier between learnable and unlearnable hidden Markov models
(HMMs). HMMs are flexible tools for clustering dependent data coming from
unknown populations. The model parameters are known to be identifiable as soon
as the clusters are distinct and the hidden chain is ergodic with a full rank
transition matrix. In the limit as any one of these conditions fails, it
becomes impossible to identify parameters. For a chain with two hidden states
we prove nonasymptotic minimax upper and lower bounds, matching up to
constants, which exhibit thresholds at which the parameters become learnable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Abraham_K/0/1/0/all/0/1"&gt;Kweku Abraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Naulet_Z/0/1/0/all/0/1"&gt;Zacharie Naulet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gassiat_E/0/1/0/all/0/1"&gt;Elisabeth Gassiat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12900</id>
        <link href="http://arxiv.org/abs/2106.12900"/>
        <updated>2021-06-25T02:00:46.870Z</updated>
        <summary type="html"><![CDATA[Meta-learning model can quickly adapt to new tasks using few-shot labeled
data. However, despite achieving good generalization on few-shot classification
tasks, it is still challenging to improve the adversarial robustness of the
meta-learning model in few-shot learning. Although adversarial training (AT)
methods such as Adversarial Query (AQ) can improve the adversarially robust
performance of meta-learning models, AT is still computationally expensive
training. On the other hand, meta-learning models trained with AT will drop
significant accuracy on the original clean images. This paper proposed a
meta-learning method on the adversarially robust neural network called
Long-term Cross Adversarial Training (LCAT). LCAT will update meta-learning
model parameters cross along the natural and adversarial sample distribution
direction with long-term to improve both adversarial and clean few-shot
classification accuracy. Due to cross-adversarial training, LCAT only needs
half of the adversarial training epoch than AQ, resulting in a low adversarial
training computation. Experiment results show that LCAT achieves superior
performance both on the clean and adversarial few-shot classification accuracy
than SOTA adversarial training methods for meta-learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xuelong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Autoregressive TTS with Explicit Duration Modelling for Low-Resource Highly Expressive Speech. (arXiv:2106.12896v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.12896</id>
        <link href="http://arxiv.org/abs/2106.12896"/>
        <updated>2021-06-25T02:00:46.856Z</updated>
        <summary type="html"><![CDATA[Whilst recent neural text-to-speech (TTS) approaches produce high-quality
speech, they typically require a large amount of recordings from the target
speaker. In previous work, a 3-step method was proposed to generate
high-quality TTS while greatly reducing the amount of data required for
training. However, we have observed a ceiling effect in the level of
naturalness achievable for highly expressive voices when using this approach.
In this paper, we present a method for building highly expressive TTS voices
with as little as 15 minutes of speech data from the target speaker. Compared
to the current state-of-the-art approach, our proposed improvements close the
gap to recordings by 23.3% for naturalness of speech and by 16.3% for speaker
similarity. Further, we match the naturalness and speaker similarity of a
Tacotron2-based full-data (~10 hours) model using only 15 minutes of target
speaker data, whereas with 30 minutes or more, we significantly outperform it.
The following improvements are proposed: 1) changing from an autoregressive,
attention-based TTS model to a non-autoregressive model replacing attention
with an external duration model and 2) an additional Conditional Generative
Adversarial Network (cGAN) based fine-tuning step.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1"&gt;Raahil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokora_K/0/1/0/all/0/1"&gt;Kamil Pokora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ezzerg_A/0/1/0/all/0/1"&gt;Abdelhamid Ezzerg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klimkov_V/0/1/0/all/0/1"&gt;Viacheslav Klimkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huybrechts_G/0/1/0/all/0/1"&gt;Goeric Huybrechts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Putrycz_B/0/1/0/all/0/1"&gt;Bartosz Putrycz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korzekwa_D/0/1/0/all/0/1"&gt;Daniel Korzekwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merritt_T/0/1/0/all/0/1"&gt;Thomas Merritt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor networks for unsupervised machine learning. (arXiv:2106.12974v1 [cond-mat.stat-mech])]]></title>
        <id>http://arxiv.org/abs/2106.12974</id>
        <link href="http://arxiv.org/abs/2106.12974"/>
        <updated>2021-06-25T02:00:46.851Z</updated>
        <summary type="html"><![CDATA[Modeling the joint distribution of high-dimensional data is a central task in
unsupervised machine learning. In recent years, many interests have been
attracted to developing learning models based on tensor networks, which have
advantages of theoretical understandings of the expressive power using
entanglement properties, and as a bridge connecting the classical computation
and the quantum computation. Despite the great potential, however, existing
tensor-network-based unsupervised models only work as a proof of principle, as
their performances are much worse than the standard models such as the
restricted Boltzmann machines and neural networks. In this work, we present the
Autoregressive Matrix Product States (AMPS), a tensor-network-based model
combining the matrix product states from quantum many-body physics and the
autoregressive models from machine learning. The model enjoys exact calculation
of normalized probability and unbiased sampling, as well as a clear theoretical
understanding of expressive power. We demonstrate the performance of our model
using two applications, the generative modeling on synthetic and real-world
data, and the reinforcement learning in statistical physics. Using extensive
numerical experiments, we show that the proposed model significantly
outperforms the existing tensor-network-based models and the restricted
Boltzmann machines, and is competitive with the state-of-the-art neural network
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Li_S/0/1/0/all/0/1"&gt;Sujie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock Market Analysis with Text Data: A Review. (arXiv:2106.12985v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2106.12985</id>
        <link href="http://arxiv.org/abs/2106.12985"/>
        <updated>2021-06-25T02:00:46.845Z</updated>
        <summary type="html"><![CDATA[Stock market movements are influenced by public and private information
shared through news articles, company reports, and social media discussions.
Analyzing these vast sources of data can give market participants an edge to
make profit. However, the majority of the studies in the literature are based
on traditional approaches that come short in analyzing unstructured, vast
textual data. In this study, we provide a review on the immense amount of
existing literature of text-based stock market analysis. We present input data
types and cover main textual data sources and variations. Feature
representation techniques are then presented. Then, we cover the analysis
techniques and create a taxonomy of the main stock market forecast models.
Importantly, we discuss representative work in each category of the taxonomy,
analyzing their respective contributions. Finally, this paper shows the
findings on unaddressed open problems and gives suggestions for future work.
The aim of this study is to survey the main stock market analysis models, text
representation techniques for financial market prediction, shortcomings of
existing techniques, and propose promising directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Fataliyev_K/0/1/0/all/0/1"&gt;Kamaladdin Fataliyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Chivukula_A/0/1/0/all/0/1"&gt;Aneesh Chivukula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Prasad_M/0/1/0/all/0/1"&gt;Mukesh Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12915</id>
        <link href="http://arxiv.org/abs/2106.12915"/>
        <updated>2021-06-25T02:00:46.839Z</updated>
        <summary type="html"><![CDATA[In theory, the choice of ReLU (0) in [0, 1] for a neural network has a
negligible influence both on backpropagation and training. Yet, in the real
world, 32 bits default precision combined with the size of deep learning
problems makes it a hyperparameter of training methods. We investigate the
importance of the value of ReLU (0) for several precision levels (16, 32, 64
bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST,
CIFAR10, SVHN). We observe considerable variations of backpropagation outputs
which occur around half of the time in 32 bits precision. The effect disappears
with double precision, while it is systematic at 16 bits. For vanilla SGD
training, the choice ReLU (0) = 0 seems to be the most efficient. We also
evidence that reconditioning approaches as batch-norm or ADAM tend to buffer
the influence of ReLU (0)'s value. Overall, the message we want to convey is
that algorithmic differentiation of nonsmooth problems potentially hides
parameters that could be tuned advantageously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertoin_D/0/1/0/all/0/1"&gt;David Bertoin&lt;/a&gt; (ISAE-SUPAERO), &lt;a href="http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Bolte&lt;/a&gt; (UT1, TSE), &lt;a href="http://arxiv.org/find/cs/1/au:+Gerchinovitz_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Gerchinovitz&lt;/a&gt; (IMT), &lt;a href="http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1"&gt;Edouard Pauwels&lt;/a&gt; (CNRS, IRIT)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Performance Hyperspectral Image Classification using Graphics Processing Units. (arXiv:2106.12942v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.12942</id>
        <link href="http://arxiv.org/abs/2106.12942"/>
        <updated>2021-06-25T02:00:46.834Z</updated>
        <summary type="html"><![CDATA[Real-time remote sensing applications like search and rescue missions,
military target detection, environmental monitoring, hazard prevention and
other time-critical applications require onboard real time processing
capabilities or autonomous decision making. Some unmanned remote systems like
satellites are physically remote from their operators, and all control of the
spacecraft and data returned by the spacecraft must be transmitted over a
wireless radio link. This link may not be available for extended periods when
the satellite is out of line of sight of its ground station. Therefore,
lightweight, small size and low power consumption hardware is essential for
onboard real time processing systems. With increasing dimensionality, size and
resolution of recent hyperspectral imaging sensors, additional challenges are
posed upon remote sensing processing systems and more capable computing
architectures are needed. Graphical Processing Units (GPUs) emerged as
promising architecture for light weight high performance computing that can
address these computational requirements for onboard systems. The goal of this
study is to build high performance methods for onboard hyperspectral analysis.
We propose accelerated methods for the well-known recursive hierarchical
segmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a
GPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the
National Aeronautics and Space Administration (NASA), which is designed to
provide rich classification information with several output levels. The
achieved speedups by parallel solutions compared to CPU sequential
implementations are 21x for parallel single GPU and 240x for hybrid multi-node
computer clusters with 16 computing nodes. The energy consumption is reduced to
74% using a single GPU compared to the equivalent parallel CPU cluster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hossam_M/0/1/0/all/0/1"&gt;Mahmoud Hossam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label Disentanglement in Partition-based Extreme Multilabel Classification. (arXiv:2106.12751v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12751</id>
        <link href="http://arxiv.org/abs/2106.12751"/>
        <updated>2021-06-25T02:00:46.827Z</updated>
        <summary type="html"><![CDATA[Partition-based methods are increasingly-used in extreme multi-label
classification (XMC) problems due to their scalability to large output spaces
(e.g., millions or more). However, existing methods partition the large label
space into mutually exclusive clusters, which is sub-optimal when labels have
multi-modality and rich semantics. For instance, the label "Apple" can be the
fruit or the brand name, which leads to the following research question: can we
disentangle these multi-modal labels with non-exclusive clustering tailored for
downstream XMC tasks? In this paper, we show that the label assignment problem
in partition-based XMC can be formulated as an optimization problem, with the
objective of maximizing precision rates. This leads to an efficient algorithm
to form flexible and overlapped label clusters, and a method that can
alternatively optimizes the cluster assignments and the model parameters for
partition-based XMC. Experimental results on synthetic and real datasets show
that our method can successfully disentangle multi-modal labels, leading to
state-of-the-art (SOTA) results on four XMC benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuanqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chang_W/0/1/0/all/0/1"&gt;Wei-Cheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Fake Detection: Survey of Facial Manipulation Detection Solutions. (arXiv:2106.12605v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12605</id>
        <link href="http://arxiv.org/abs/2106.12605"/>
        <updated>2021-06-25T02:00:46.822Z</updated>
        <summary type="html"><![CDATA[Deep Learning as a field has been successfully used to solve a plethora of
complex problems, the likes of which we could not have imagined a few decades
back. But as many benefits as it brings, there are still ways in which it can
be used to bring harm to our society. Deep fakes have been proven to be one
such problem, and now more than ever, when any individual can create a fake
image or video simply using an application on the smartphone, there need to be
some countermeasures, with which we can detect if the image or video is a fake
or real and dispose of the problem threatening the trustworthiness of online
information. Although the Deep fakes created by neural networks, may seem to be
as real as a real image or video, it still leaves behind spatial and temporal
traces or signatures after moderation, these signatures while being invisible
to a human eye can be detected with the help of a neural network trained to
specialize in Deep fake detection. In this paper, we analyze several such
states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception
Net) and compare them against each other, to find an optimal solution for
various scenarios like real-time deep fake detection to be deployed in online
social media platforms where the classification should be made as fast as
possible or for a small news agency where the classification need not be in
real-time but requires utmost accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1"&gt;Samay Pashine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandiya_S/0/1/0/all/0/1"&gt;Sagar Mandiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Praveen Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1"&gt;Rashid Sheikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A review of systematic selection of clustering algorithms and their evaluation. (arXiv:2106.12792v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12792</id>
        <link href="http://arxiv.org/abs/2106.12792"/>
        <updated>2021-06-25T02:00:46.816Z</updated>
        <summary type="html"><![CDATA[Data analysis plays an indispensable role for value creation in industry.
Cluster analysis in this context is able to explore given datasets with little
or no prior knowledge and to identify unknown patterns. As (big) data
complexity increases in the dimensions volume, variety, and velocity, this
becomes even more important. Many tools for cluster analysis have been
developed from early on and the variety of different clustering algorithms is
huge. As the selection of the right clustering procedure is crucial to the
results of the data analysis, users are in need for support on their journey of
extracting knowledge from raw data. Thus, the objective of this paper lies in
the identification of a systematic selection logic for clustering algorithms
and corresponding validation concepts. The goal is to enable potential users to
choose an algorithm that fits best to their needs and the properties of their
underlying data clustering problem. Moreover, users are supported in selecting
the right validation concepts to make sense of the clustering results. Based on
a comprehensive literature review, this paper provides assessment criteria for
clustering method evaluation and validation concept selection. The criteria are
applied to several common algorithms and the selection process of an algorithm
is supported by the introduction of pseudocode-based routines that consider the
underlying data structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wegmann_M/0/1/0/all/0/1"&gt;Marc Wegmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zipperling_D/0/1/0/all/0/1"&gt;Domenique Zipperling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hillenbrand_J/0/1/0/all/0/1"&gt;Jonas Hillenbrand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Fleischer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Wasserstein and Maximum Mean Discrepancy distances for bridging the gap between outlier detection and drift detection. (arXiv:2106.12893v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12893</id>
        <link href="http://arxiv.org/abs/2106.12893"/>
        <updated>2021-06-25T02:00:46.789Z</updated>
        <summary type="html"><![CDATA[With the rise of machine learning and deep learning based applications in
practice, monitoring, i.e. verifying that these operate within specification,
has become an important practical problem. An important aspect of this
monitoring is to check whether the inputs (or intermediates) have strayed from
the distribution they were validated for, which can void the performance
assurances obtained during testing.

There are two common approaches for this. The, perhaps, more classical one is
outlier detection or novelty detection, where, for a single input we ask
whether it is an outlier, i.e. exceedingly unlikely to have originated from a
reference distribution. The second, perhaps more recent approach, is to
consider a larger number of inputs and compare its distribution to a reference
distribution (e.g. sampled during testing). This is done under the label drift
detection.

In this work, we bridge the gap between outlier detection and drift detection
through comparing a given number of inputs to an automatically chosen part of
the reference distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Viehmann_T/0/1/0/all/0/1"&gt;Thomas Viehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding. (arXiv:2106.02795v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02795</id>
        <link href="http://arxiv.org/abs/2106.02795"/>
        <updated>2021-06-25T02:00:46.778Z</updated>
        <summary type="html"><![CDATA[Attentional mechanisms are order-invariant. Positional encoding is a crucial
component to allow attention-based deep model architectures such as Transformer
to address sequences or images where the position of information matters. In
this paper, we propose a novel positional encoding method based on learnable
Fourier features. Instead of hard-coding each position as a token or a vector,
we represent each position, which can be multi-dimensional, as a trainable
encoding based on learnable Fourier feature mapping, modulated with a
multi-layer perceptron. The representation is particularly advantageous for a
spatial multi-dimensional position, e.g., pixel positions on an image, where
$L_2$ distances or more complex positional relationships need to be captured.
Our experiments based on several public benchmark tasks show that our learnable
Fourier feature representation for multi-dimensional positional encoding
outperforms existing methods by both improving the accuracy and allowing faster
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1"&gt;Si Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. (arXiv:2106.13008v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13008</id>
        <link href="http://arxiv.org/abs/2106.13008"/>
        <updated>2021-06-25T02:00:46.763Z</updated>
        <summary type="html"><![CDATA[Extending the forecasting time is a critical demand for real applications,
such as extreme weather early warning and long-term energy consumption
planning. This paper studies the \textit{long-term forecasting} problem of time
series. Prior Transformer-based models adopt various self-attention mechanisms
to discover the long-range dependencies. However, intricate temporal patterns
of the long-term future prohibit the model from finding reliable dependencies.
Also, Transformers have to adopt the sparse versions of point-wise
self-attentions for long series efficiency, resulting in the information
utilization bottleneck. Towards these challenges, we propose Autoformer as a
novel decomposition architecture with an Auto-Correlation mechanism. We go
beyond the pre-processing convention of series decomposition and renovate it as
a basic inner block of deep models. This design empowers Autoformer with
progressive decomposition capacities for complex time series. Further, inspired
by the stochastic process theory, we design the Auto-Correlation mechanism
based on the series periodicity, which conducts the dependencies discovery and
representation aggregation at the sub-series level. Auto-Correlation
outperforms self-attention in both efficiency and accuracy. In long-term
forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative
improvement on six benchmarks, covering five practical applications: energy,
traffic, economics, weather and disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haixu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiehui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Optimization with High-Dimensional Outputs. (arXiv:2106.12997v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12997</id>
        <link href="http://arxiv.org/abs/2106.12997"/>
        <updated>2021-06-25T02:00:46.752Z</updated>
        <summary type="html"><![CDATA[Bayesian Optimization is a sample-efficient black-box optimization procedure
that is typically applied to problems with a small number of independent
objectives. However, in practice we often wish to optimize objectives defined
over many correlated outcomes (or ``tasks"). For example, scientists may want
to optimize the coverage of a cell tower network across a dense grid of
locations. Similarly, engineers may seek to balance the performance of a robot
across dozens of different environments via constrained or robust optimization.
However, the Gaussian Process (GP) models typically used as probabilistic
surrogates for multi-task Bayesian Optimization scale poorly with the number of
outcomes, greatly limiting applicability. We devise an efficient technique for
exact multi-task GP sampling that combines exploiting Kronecker structure in
the covariance matrices with Matheron's identity, allowing us to perform
Bayesian Optimization using exact multi-task GP models with tens of thousands
of correlated outputs. In doing so, we achieve substantial improvements in
sample efficiency compared to existing approaches that only model aggregate
functions of the outcomes. We demonstrate how this unlocks a new class of
applications for Bayesian Optimization across a range of tasks in science and
engineering, including optimizing interference patterns of an optical
interferometer with more than 65,000 outputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1"&gt;Wesley J. Maddox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balandat_M/0/1/0/all/0/1"&gt;Maximilian Balandat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bakshy_E/0/1/0/all/0/1"&gt;Eytan Bakshy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Density Constrained Reinforcement Learning. (arXiv:2106.12764v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12764</id>
        <link href="http://arxiv.org/abs/2106.12764"/>
        <updated>2021-06-25T02:00:46.744Z</updated>
        <summary type="html"><![CDATA[We study constrained reinforcement learning (CRL) from a novel perspective by
setting constraints directly on state density functions, rather than the value
functions considered by previous works. State density has a clear physical and
mathematical interpretation, and is able to express a wide variety of
constraints such as resource limits and safety requirements. Density
constraints can also avoid the time-consuming process of designing and tuning
cost functions required by value function-based constraints to encode system
specifications. We leverage the duality between density functions and Q
functions to develop an effective algorithm to solve the density constrained RL
problem optimally and the constrains are guaranteed to be satisfied. We prove
that the proposed algorithm converges to a near-optimal solution with a bounded
error even when the policy update is imperfect. We use a set of comprehensive
experiments to demonstrate the advantages of our approach over state-of-the-art
CRL methods, with a wide range of density constrained tasks as well as standard
CRL benchmarks such as Safety-Gym.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zengyi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Chuchu Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Objective discovery of dominant dynamical processes with intelligible machine learning. (arXiv:2106.12963v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12963</id>
        <link href="http://arxiv.org/abs/2106.12963"/>
        <updated>2021-06-25T02:00:46.739Z</updated>
        <summary type="html"><![CDATA[The advent of big data has vast potential for discovery in natural phenomena
ranging from climate science to medicine, but overwhelming complexity stymies
insight. Existing theory is often not able to succinctly describe salient
phenomena, and progress has largely relied on ad hoc definitions of dynamical
regimes to guide and focus exploration. We present a formal definition in which
the identification of dynamical regimes is formulated as an optimization
problem, and we propose an intelligible objective function. Furthermore, we
propose an unsupervised learning framework which eliminates the need for a
priori knowledge and ad hoc definitions; instead, the user need only choose
appropriate clustering and dimensionality reduction algorithms, and this choice
can be guided using our proposed objective function. We illustrate its
applicability with example problems drawn from ocean dynamics, tumor
angiogenesis, and turbulent boundary layers. Our method is a step towards
unbiased data exploration that allows serendipitous discovery within dynamical
systems, with the potential to propel the physical sciences forward.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaiser_B/0/1/0/all/0/1"&gt;Bryan E. Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenz_J/0/1/0/all/0/1"&gt;Juan A. Saenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonnewald_M/0/1/0/all/0/1"&gt;Maike Sonnewald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livescu_D/0/1/0/all/0/1"&gt;Daniel Livescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handwritten Digit Recognition using Machine and Deep Learning Algorithms. (arXiv:2106.12614v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12614</id>
        <link href="http://arxiv.org/abs/2106.12614"/>
        <updated>2021-06-25T02:00:46.719Z</updated>
        <summary type="html"><![CDATA[The reliance of humans over machines has never been so high such that from
object classification in photographs to adding sound to silent movies
everything can be performed with the help of deep learning and machine learning
algorithms. Likewise, Handwritten text recognition is one of the significant
areas of research and development with a streaming number of possibilities that
could be attained. Handwriting recognition (HWR), also known as Handwritten
Text Recognition (HTR), is the ability of a computer to receive and interpret
intelligible handwritten input from sources such as paper documents,
photographs, touch-screens and other devices [1]. Apparently, in this paper, we
have performed handwritten digit recognition with the help of MNIST datasets
using Support Vector Machines (SVM), Multi-Layer Perceptron (MLP) and
Convolution Neural Network (CNN) models. Our main objective is to compare the
accuracy of the models stated above along with their execution time to get the
best possible model for digit recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1"&gt;Samay Pashine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixit_R/0/1/0/all/0/1"&gt;Ritik Dixit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kushwah_R/0/1/0/all/0/1"&gt;Rishika Kushwah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TagRuler: Interactive Tool for Span-Level Data Programming by Demonstration. (arXiv:2106.12767v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12767</id>
        <link href="http://arxiv.org/abs/2106.12767"/>
        <updated>2021-06-25T02:00:46.712Z</updated>
        <summary type="html"><![CDATA[Despite rapid developments in the field of machine learning research,
collecting high-quality labels for supervised learning remains a bottleneck for
many applications. This difficulty is exacerbated by the fact that
state-of-the-art models for NLP tasks are becoming deeper and more complex,
often increasing the amount of training data required even for fine-tuning.
Weak supervision methods, including data programming, address this problem and
reduce the cost of label collection by using noisy label sources for
supervision. However, until recently, data programming was only accessible to
users who knew how to program. To bridge this gap, the Data Programming by
Demonstration framework was proposed to facilitate the automatic creation of
labeling functions based on a few examples labeled by a domain expert. This
framework has proven successful for generating high-accuracy labeling models
for document classification. In this work, we extend the DPBD framework to
span-level annotation tasks, arguably one of the most time-consuming NLP
labeling tasks. We built a novel tool, TagRuler, that makes it easy for
annotators to build span-level labeling functions without programming and
encourages them to explore trade-offs between different labeling models and
active learning strategies. We empirically demonstrated that an annotator could
achieve a higher F1 score using the proposed tool compared to manual labeling
for different span-level annotation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1"&gt;Dongjin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evensen_S/0/1/0/all/0/1"&gt;Sara Evensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1"&gt;&amp;#xc7;a&amp;#x11f;atay Demiralp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1"&gt;Estevam Hruschka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Latency Federated Learning over Wireless Channels with Differential Privacy. (arXiv:2106.13039v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.13039</id>
        <link href="http://arxiv.org/abs/2106.13039"/>
        <updated>2021-06-25T02:00:46.696Z</updated>
        <summary type="html"><![CDATA[In federated learning (FL), model training is distributed over clients and
local models are aggregated by a central server. The performance of uploaded
models in such situations can vary widely due to imbalanced data distributions,
potential demands on privacy protections, and quality of transmissions. In this
paper, we aim to minimize FL training delay over wireless channels, constrained
by overall training performance as well as each client's differential privacy
(DP) requirement. We solve this problem in the framework of multi-agent
multi-armed bandit (MAMAB) to deal with the situation where there are multiple
clients confornting different unknown transmission environments, e.g., channel
fading and interferences. Specifically, we first transform the long-term
constraints on both training performance and each client's DP into a virtual
queue based on the Lyapunov drift technique. Then, we convert the MAMAB to a
max-min bipartite matching problem at each communication round, by estimating
rewards with the upper confidence bound (UCB) approach. More importantly, we
propose two efficient solutions to this matching problem, i.e., modified
Hungarian algorithm and greedy matching with a better alternative (GMBA), in
which the first one can achieve the optimal solution with a high complexity
while the second one approaches a better trade-off by enabling a verified
low-complexity with little performance loss. In addition, we develop an upper
bound on the expected regret of this MAMAB based FL framework, which shows a
linear growth over the logarithm of communication rounds, justifying its
theoretical feasibility. Extensive experimental results are conducted to
validate the effectiveness of our proposed algorithms, and the impacts of
various parameters on the FL performance over wireless edge networks are also
discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1"&gt;Kang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Cailian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Shi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graceful Degradation and Related Fields. (arXiv:2106.11119v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11119</id>
        <link href="http://arxiv.org/abs/2106.11119"/>
        <updated>2021-06-25T02:00:46.682Z</updated>
        <summary type="html"><![CDATA[When machine learning models encounter data which is out of the distribution
on which they were trained they have a tendency to behave poorly, most
prominently over-confidence in erroneous predictions. Such behaviours will have
disastrous effects on real-world machine learning systems. In this field
graceful degradation refers to the optimisation of model performance as it
encounters this out-of-distribution data. This work presents a definition and
discussion of graceful degradation and where it can be applied in deployed
visual systems. Following this a survey of relevant areas is undertaken,
novelly splitting the graceful degradation problem into active and passive
approaches. In passive approaches, graceful degradation is handled and achieved
by the model in a self-contained manner, in active approaches the model is
updated upon encountering epistemic uncertainties. This work communicates the
importance of the problem and aims to prompt the development of machine
learning strategies that are aware of graceful degradation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dymond_J/0/1/0/all/0/1"&gt;Jack Dymond&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Maximum Correntropy Regression for Robust Trajectory Decoding from Noisy Epidural Electrocorticographic Signals. (arXiv:2106.13086v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.13086</id>
        <link href="http://arxiv.org/abs/2106.13086"/>
        <updated>2021-06-25T02:00:46.676Z</updated>
        <summary type="html"><![CDATA[The Partial Least Square Regression (PLSR) algorithm exhibits exceptional
competence for predicting continuous variables from inter-correlated brain
recordings in brain-computer interfaces, which achieved successful prediction
from epidural electrocorticography of macaques to three-dimensional continuous
hand trajectories recently. Nevertheless, PLSR is in essence formulated based
on the least square criterion, thus, being non-robust with respect to
complicated noises consequently. The aim of the present study is to propose a
robust version of PLSR. To this end, the maximum correntropy criterion is
adopted to structure a new robust variant of PLSR, namely Partial Maximum
Correntropy Regression (PMCR). Half-quadratic optimization technique is
utilized to calculate the robust latent variables. We assess the proposed PMCR
on a synthetic example and the public Neurotycho dataset. Compared with the
conventional PLSR and the state-of-the-art variant, PMCR realized superior
prediction competence on three different performance indicators with
contaminated training set. The proposed PMCR was demonstrated as an effective
approach for robust decoding from noisy brain measurements, which could reduce
the performance degradation resulting from adverse noises, thus, improving the
decoding robustness of brain-computer interfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1"&gt;Badong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoshimura_N/0/1/0/all/0/1"&gt;Natsue Yoshimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koike_Y/0/1/0/all/0/1"&gt;Yasuharu Koike&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-learning for Multi-variable Non-convex Optimization Problems: Iterating Non-optimums Makes Optimum Possible. (arXiv:2009.04899v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04899</id>
        <link href="http://arxiv.org/abs/2009.04899"/>
        <updated>2021-06-25T02:00:46.653Z</updated>
        <summary type="html"><![CDATA[In this paper, we aim to address the problem of solving a non-convex
optimization problem over an intersection of multiple variable sets. This kind
of problems is typically solved by using an alternating minimization (AM)
strategy which splits the overall problem into a set of sub-problems
corresponding to each variable, and then iteratively performs minimization over
each sub-problem using a fixed updating rule. However, due to the intrinsic
non-convexity of the overall problem, the optimization can usually be trapped
into bad local minimum even when each sub-problem can be globally optimized at
each iteration. To tackle this problem, we propose a meta-learning based Global
Scope Optimization (GSO) method. It adaptively generates optimizers for
sub-problems via meta-learners and constantly updates these meta-learners with
respect to the global loss information of the overall problem. Therefore, the
sub-problems are optimized with the objective of minimizing the global loss
specifically. We evaluate the proposed model on a number of simulations,
including solving bi-linear inverse problems: matrix completion, and non-linear
problems: Gaussian mixture models. The experimental results show that our
proposed approach outperforms AM-based methods in standard settings, and is
able to achieve effective optimization in some challenging cases while other
methods would typically fail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1"&gt;Jingyuan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shengxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jun-Jie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaimoukha_I/0/1/0/all/0/1"&gt;Imad Jaimoukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinwang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-objective Asynchronous Successive Halving. (arXiv:2106.12639v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12639</id>
        <link href="http://arxiv.org/abs/2106.12639"/>
        <updated>2021-06-25T02:00:46.633Z</updated>
        <summary type="html"><![CDATA[Hyperparameter optimization (HPO) is increasingly used to automatically tune
the predictive performance (e.g., accuracy) of machine learning models.
However, in a plethora of real-world applications, accuracy is only one of the
multiple -- often conflicting -- performance criteria, necessitating the
adoption of a multi-objective (MO) perspective. While the literature on MO
optimization is rich, few prior studies have focused on HPO. In this paper, we
propose algorithms that extend asynchronous successive halving (ASHA) to the MO
setting. Considering multiple evaluation metrics, we assess the performance of
these methods on three real world tasks: (i) Neural architecture search, (ii)
algorithmic fairness and (iii) language model optimization. Our empirical
analysis shows that MO ASHA enables to perform MO HPO at scale. Further, we
observe that that taking the entire Pareto front into account for candidate
selection consistently outperforms multi-fidelity HPO based on MO scalarization
in terms of wall-clock time. Our algorithms (to be open-sourced) establish new
baselines for future research in the area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Schmucker_R/0/1/0/all/0/1"&gt;Robin Schmucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Donini_M/0/1/0/all/0/1"&gt;Michele Donini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zafar_M/0/1/0/all/0/1"&gt;Muhammad Bilal Zafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Salinas_D/0/1/0/all/0/1"&gt;David Salinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Archambeau_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Archambeau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Monocular Depth Estimation of Untextured Indoor Rotated Scenes. (arXiv:2106.12958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12958</id>
        <link href="http://arxiv.org/abs/2106.12958"/>
        <updated>2021-06-25T02:00:46.620Z</updated>
        <summary type="html"><![CDATA[Self-supervised deep learning methods have leveraged stereo images for
training monocular depth estimation. Although these methods show strong results
on outdoor datasets such as KITTI, they do not match performance of supervised
methods on indoor environments with camera rotation. Indoor, rotated scenes are
common for less constrained applications and pose problems for two reasons:
abundance of low texture regions and increased complexity of depth cues for
images under rotation. In an effort to extend self-supervised learning to more
generalised environments we propose two additions. First, we propose a novel
Filled Disparity Loss term that corrects for ambiguity of image reconstruction
error loss in textureless regions. Specifically, we interpolate disparity in
untextured regions, using the estimated disparity from surrounding textured
areas, and use L1 loss to correct the original estimation. Our experiments show
that depth estimation is substantially improved on low-texture scenes, without
any loss on textured scenes, when compared to Monodepth by Godard et al.
Secondly, we show that training with an application's representative rotations,
in both pitch and roll, is sufficient to significantly improve performance over
the entire range of expected rotation. We demonstrate that depth estimation is
successfully generalised as performance is not lost when evaluated on test sets
with no camera rotation. Together these developments enable a broader use of
self-supervised learning of monocular depth estimation for complex
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keltjens_B/0/1/0/all/0/1"&gt;Benjamin Keltjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_T/0/1/0/all/0/1"&gt;Tom van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;Guido de Croon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where can I drive? A System Approach: Deep Ego Corridor Estimation for Robust Automated Driving. (arXiv:2004.07639v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07639</id>
        <link href="http://arxiv.org/abs/2004.07639"/>
        <updated>2021-06-25T02:00:46.614Z</updated>
        <summary type="html"><![CDATA[Lane detection is an essential part of the perception sub-architecture of any
automated driving (AD) or advanced driver assistance system (ADAS). When
focusing on low-cost, large scale products for automated driving, model-driven
approaches for the detection of lane markings have proven good performance.
More recently, data-driven approaches have been proposed that target the
drivable area / freespace mainly in inner-city applications. Focus of these
approaches is less on lane-based driving due to the fact that the lane concept
does not fully apply in unstructured, residential inner-city environments.
So-far the concept of drivable area is seldom used for highway and inter-urban
applications due to the specific requirements of these scenarios that require
clear lane associations of all traffic participants. We believe that
lane-based, mapless driving in inter-urban and highway scenarios is still not
fully handled with sufficient robustness and availability. Especially for
challenging weather situations such as heavy rain, fog, low-standing sun,
darkness or reflections in puddles, the mapless detection of lane markings
decreases significantly or completely fails. We see potential in applying
specifically designed data-driven freespace approaches in more lane-based
driving applications for highways and inter-urban use. Therefore, we propose to
classify specifically a drivable corridor of the ego lane on pixel level with a
deep learning approach. Our approach is kept computationally efficient with
only 0.66 million parameters allowing its application in large scale products.
Thus, we were able to easily integrate into an online AD system of a test
vehicle. We demonstrate the performance of our approach under challenging
conditions qualitatively and quantitatively in comparison to a state-of-the-art
model-driven approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michalke_T/0/1/0/all/0/1"&gt;Thomas Michalke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1"&gt;Di Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaser_C/0/1/0/all/0/1"&gt;Claudius Gl&amp;#xe4;ser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timm_F/0/1/0/all/0/1"&gt;Fabian Timm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Needlets for Lighting Estimation with Spherical Transport Loss. (arXiv:2106.13090v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13090</id>
        <link href="http://arxiv.org/abs/2106.13090"/>
        <updated>2021-06-25T02:00:46.597Z</updated>
        <summary type="html"><![CDATA[Accurate lighting estimation is challenging yet critical to many computer
vision and computer graphics tasks such as high-dynamic-range (HDR) relighting.
Existing approaches model lighting in either frequency domain or spatial domain
which is insufficient to represent the complex lighting conditions in scenes
and tends to produce inaccurate estimation. This paper presents NeedleLight, a
new lighting estimation model that represents illumination with needlets and
allows lighting estimation in both frequency domain and spatial domain jointly.
An optimal thresholding function is designed to achieve sparse needlets which
trims redundant lighting parameters and demonstrates superior localization
properties for illumination representation. In addition, a novel spherical
transport loss is designed based on optimal transport theory which guides to
regress lighting representation parameters with consideration of the spatial
information. Furthermore, we propose a new metric that is concise yet effective
by directly evaluating the estimated illumination maps rather than rendered
images. Extensive experiments show that NeedleLight achieves superior lighting
estimation consistently across multiple evaluation metrics as compared with
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1"&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changgong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wenbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Feiying Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xuansong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Reinforcement Learning via Latent-Space Collocation. (arXiv:2106.13229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13229</id>
        <link href="http://arxiv.org/abs/2106.13229"/>
        <updated>2021-06-25T02:00:46.581Z</updated>
        <summary type="html"><![CDATA[The ability to plan into the future while utilizing only raw high-dimensional
observations, such as images, can provide autonomous agents with broad
capabilities. Visual model-based reinforcement learning (RL) methods that plan
future actions directly have shown impressive results on tasks that require
only short-horizon reasoning, however, these methods struggle on temporally
extended tasks. We argue that it is easier to solve long-horizon tasks by
planning sequences of states rather than just actions, as the effects of
actions greatly compound over time and are harder to optimize. To achieve this,
we draw on the idea of collocation, which has shown good results on
long-horizon tasks in optimal control literature, and adapt it to the
image-based setting by utilizing learned latent state space models. The
resulting latent collocation method (LatCo) optimizes trajectories of latent
states, which improves over previously proposed shooting methods for visual
model-based RL on tasks with sparse rewards and long-term goals. Videos and
code at https://orybkin.github.io/latco/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1"&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chuning Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagabandi_A/0/1/0/all/0/1"&gt;Anusha Nagabandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1"&gt;Kostas Daniilidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-25T02:00:46.576Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Uniform OPE and Model-based Offline Reinforcement Learning in Time-Homogeneous, Reward-Free and Task-Agnostic Settings. (arXiv:2105.06029v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06029</id>
        <link href="http://arxiv.org/abs/2105.06029"/>
        <updated>2021-06-25T02:00:46.570Z</updated>
        <summary type="html"><![CDATA[This work studies the statistical limits of uniform convergence for offline
policy evaluation (OPE) problems with model-based methods (for episodic MDP)
and provides a unified framework towards optimal learning for several
well-motivated offline tasks. Uniform OPE
$\sup_\Pi|Q^\pi-\hat{Q}^\pi|<\epsilon$ is a stronger measure than the
point-wise OPE and ensures offline learning when $\Pi$ contains all policies
(the global class). In this paper, we establish an $\Omega(H^2
S/d_m\epsilon^2)$ lower bound (over model-based family) for the global uniform
OPE and our main result establishes an upper bound of
$\tilde{O}(H^2/d_m\epsilon^2)$ for the \emph{local} uniform convergence that
applies to all \emph{near-empirically optimal} policies for the MDPs with
\emph{stationary} transition. Here $d_m$ is the minimal marginal state-action
probability. Critically, the highlight in achieving the optimal rate
$\tilde{O}(H^2/d_m\epsilon^2)$ is our design of \emph{singleton absorbing MDP},
which is a new sharp analysis tool that works with the model-based approach. We
generalize such a model-based framework to the new settings: offline
task-agnostic and the offline reward-free with optimal complexity
$\tilde{O}(H^2\log(K)/d_m\epsilon^2)$ ($K$ is the number of tasks) and
$\tilde{O}(H^2S/d_m\epsilon^2)$ respectively. These results provide a unified
solution for simultaneously solving different offline RL problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Ming Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Black-Box Planning Using Macro-Actions with Focused Effects. (arXiv:2004.13242v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13242</id>
        <link href="http://arxiv.org/abs/2004.13242"/>
        <updated>2021-06-25T02:00:46.565Z</updated>
        <summary type="html"><![CDATA[The difficulty of deterministic planning increases exponentially with
search-tree depth. Black-box planning presents an even greater challenge, since
planners must operate without an explicit model of the domain. Heuristics can
make search more efficient, but goal-aware heuristics for black-box planning
usually rely on goal counting, which is often quite uninformative. In this
work, we show how to overcome this limitation by discovering macro-actions that
make the goal-count heuristic more accurate. Our approach searches for
macro-actions with focused effects (i.e. macros that modify only a small number
of state variables), which align well with the assumptions made by the
goal-count heuristic. Focused macros dramatically improve black-box planning
efficiency across a wide range of planning domains, sometimes beating even
state-of-the-art planners with access to a full domain model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1"&gt;Cameron Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katz_M/0/1/0/all/0/1"&gt;Michael Katz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinger_T/0/1/0/all/0/1"&gt;Tim Klinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1"&gt;George Konidaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1"&gt;Matthew Riemer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1"&gt;Gerald Tesauro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language for Description of Worlds. (arXiv:2010.16243v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.16243</id>
        <link href="http://arxiv.org/abs/2010.16243"/>
        <updated>2021-06-25T02:00:46.547Z</updated>
        <summary type="html"><![CDATA[We will reduce the task of creating AI to the task of finding an appropriate
language for description of the world. This will not be a programing language
because programing languages describe only computable functions, while our
language will describe a somewhat broader class of functions. Another
specificity of this language will be that the description will consist of
separate modules. This will enable us look for the description of the world
automatically such that we discover it module after module. Our approach to the
creation of this new language will be to start with a particular world and
write the description of that particular world. The point is that the language
which can describe this particular world will be appropriate for describing any
world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dobrev_D/0/1/0/all/0/1"&gt;Dimiter Dobrev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Agriculture Commodity Price Prediction System with Machine Learning Techniques. (arXiv:2106.12747v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12747</id>
        <link href="http://arxiv.org/abs/2106.12747"/>
        <updated>2021-06-25T02:00:46.541Z</updated>
        <summary type="html"><![CDATA[The intention of this research is to study and design an automated
agriculture commodity price prediction system with novel machine learning
techniques. Due to the increasing large amounts historical data of agricultural
commodity prices and the need of performing accurate prediction of price
fluctuations, the solution has largely shifted from statistical methods to
machine learning area. However, the selection of proper set from historical
data for forecasting still has limited consideration. On the other hand, when
implementing machine learning techniques, finding a suitable model with optimal
parameters for global solution, nonlinearity and avoiding curse of
dimensionality are still biggest challenges, therefore machine learning
strategies study are needed. In this research, we propose a web-based automated
system to predict agriculture commodity price. In the two series experiments,
five popular machine learning algorithms, ARIMA, SVR, Prophet, XGBoost and LSTM
have been compared with large historical datasets in Malaysia and the most
optimal algorithm, LSTM model with an average of 0.304 mean-square error has
been selected as the prediction engine of the proposed system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1"&gt;Howe Seng Goh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sin_K/0/1/0/all/0/1"&gt;Kai Ling Sin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kelly Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1"&gt;Nicole Ka Hei Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liew_X/0/1/0/all/0/1"&gt;Xin Yu Liew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Collection of Medical Image Datasets for Deep Learning. (arXiv:2106.12864v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12864</id>
        <link href="http://arxiv.org/abs/2106.12864"/>
        <updated>2021-06-25T02:00:46.535Z</updated>
        <summary type="html"><![CDATA[The astounding success made by artificial intelligence (AI) in healthcare and
other fields proves that AI can achieve human-like performance. However,
success always comes with challenges. Deep learning algorithms are
data-dependent and require large datasets for training. The lack of data in the
medical imaging field creates a bottleneck for the application of deep learning
to medical image analysis. Medical image acquisition, annotation, and analysis
are costly, and their usage is constrained by ethical restrictions. They also
require many resources, such as human expertise and funding. That makes it
difficult for non-medical researchers to have access to useful and large
medical data. Thus, as comprehensive as possible, this paper provides a
collection of medical image datasets with their associated challenges for deep
learning research. We have collected information of around three hundred
datasets and challenges mainly reported between 2013 and 2020 and categorized
them into four categories: head & neck, chest & abdomen, pathology & blood, and
``others''. Our paper has three purposes: 1) to provide a most up to date and
complete list that can be used as a universal reference to easily find the
datasets for clinical image analysis, 2) to guide researchers on the
methodology to test and evaluate their methods' performance and robustness on
relevant datasets, 3) to provide a ``route'' to relevant algorithms for the
relevant medical topics, and challenge leaderboards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Johann Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1"&gt;Guangming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hua_C/0/1/0/all/0/1"&gt;Cong Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1"&gt;Mingtao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+BasheerBennamoun/0/1/0/all/0/1"&gt;BasheerBennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaoyuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1"&gt;Juan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_P/0/1/0/all/0/1"&gt;Peiyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mei_L/0/1/0/all/0/1"&gt;Lin Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shah_S/0/1/0/all/0/1"&gt;Syed Afaq Ali Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meaningfully Explaining a Model's Mistakes. (arXiv:2106.12723v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12723</id>
        <link href="http://arxiv.org/abs/2106.12723"/>
        <updated>2021-06-25T02:00:46.529Z</updated>
        <summary type="html"><![CDATA[Understanding and explaining the mistakes made by trained models is critical
to many machine learning objectives, such as improving robustness, addressing
concept drift, and mitigating biases. However, this is often an ad hoc process
that involves manually looking at the model's mistakes on many test samples and
guessing at the underlying reasons for those incorrect predictions. In this
paper, we propose a systematic approach, conceptual explanation scores (CES),
that explains why a classifier makes a mistake on a particular test sample(s)
in terms of human-understandable concepts (e.g. this zebra is misclassified as
a dog because of faint stripes). We base CES on two prior ideas: counterfactual
explanations and concept activation vectors, and validate our approach on
well-known pretrained models, showing that it explains the models' mistakes
meaningfully. We also train new models with intentional and known spurious
correlations, which CES successfully identifies from a single misclassified
test sample. The code for CES is publicly available and can easily be applied
to new models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abid_A/0/1/0/all/0/1"&gt;Abubakar Abid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hamiltonian-based Neural ODE Networks on the SE(3) Manifold For Dynamics Learning and Control. (arXiv:2106.12782v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.12782</id>
        <link href="http://arxiv.org/abs/2106.12782"/>
        <updated>2021-06-25T02:00:46.524Z</updated>
        <summary type="html"><![CDATA[Accurate models of robot dynamics are critical for safe and stable control
and generalization to novel operational conditions. Hand-designed models,
however, may be insufficiently accurate, even after careful parameter tuning.
This motivates the use of machine learning techniques to approximate the robot
dynamics over a training set of state-control trajectories. The dynamics of
many robots, including ground, aerial, and underwater vehicles, are described
in terms of their SE(3) pose and generalized velocity, and satisfy conservation
of energy principles. This paper proposes a Hamiltonian formulation over the
SE(3) manifold of the structure of a neural ordinary differential equation
(ODE) network to approximate the dynamics of a rigid body. In contrast to a
black-box ODE network, our formulation guarantees total energy conservation by
construction. We develop energy shaping and damping injection control for the
learned, potentially under-actuated SE(3) Hamiltonian dynamics to enable a
unified approach for stabilization and trajectory tracking with various
platforms, including pendulum, rigid-body, and quadrotor systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1"&gt;Thai Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1"&gt;Nikolay Atanasov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating variational quantum algorithms with multiple quantum processors. (arXiv:2106.12819v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12819</id>
        <link href="http://arxiv.org/abs/2106.12819"/>
        <updated>2021-06-25T02:00:46.508Z</updated>
        <summary type="html"><![CDATA[Variational quantum algorithms (VQAs) have the potential of utilizing
near-term quantum machines to gain certain computational advantages over
classical methods. Nevertheless, modern VQAs suffer from cumbersome
computational overhead, hampered by the tradition of employing a solitary
quantum processor to handle large-volume data. As such, to better exert the
superiority of VQAs, it is of great significance to improve their runtime
efficiency. Here we devise an efficient distributed optimization scheme, called
QUDIO, to address this issue. Specifically, in QUDIO, a classical central
server partitions the learning problem into multiple subproblems and allocate
them to multiple local nodes where each of them consists of a quantum processor
and a classical optimizer. During the training procedure, all local nodes
proceed parallel optimization and the classical server synchronizes
optimization information among local nodes timely. In doing so, we prove a
sublinear convergence rate of QUDIO in terms of the number of global iteration
under the ideal scenario, while the system imperfection may incur divergent
optimization. Numerical results on standard benchmarks demonstrate that QUDIO
can surprisingly achieve a superlinear runtime speedup with respect to the
number of local nodes. Our proposal can be readily mixed with other advanced
VQAs-based techniques to narrow the gap between the state of the art and
applications with quantum advantage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuxuan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yang Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models. (arXiv:2106.12887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12887</id>
        <link href="http://arxiv.org/abs/2106.12887"/>
        <updated>2021-06-25T02:00:46.503Z</updated>
        <summary type="html"><![CDATA[We present a scalable post-processing algorithm for debiasing trained models,
including deep neural networks (DNNs), which we prove to be near-optimal by
bounding its excess Bayes risk. We empirically validate its advantages on
standard benchmark datasets across both classical algorithms as well as modern
DNN architectures and demonstrate that it outperforms previous post-processing
methods while performing on par with in-processing. In addition, we show that
the proposed algorithm is particularly effective for models trained at scale
where post-processing is a natural and practical choice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1"&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1"&gt;Mario Lucic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mix and Mask Actor-Critic Methods. (arXiv:2106.13037v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13037</id>
        <link href="http://arxiv.org/abs/2106.13037"/>
        <updated>2021-06-25T02:00:46.498Z</updated>
        <summary type="html"><![CDATA[Shared feature spaces for actor-critic methods aims to capture generalized
latent representations to be used by the policy and value function with the
hopes for a more stable and sample-efficient optimization. However, such a
paradigm present a number of challenges in practice, as parameters generating a
shared representation must learn off two distinct objectives, resulting in
competing updates and learning perturbations. In this paper, we present a novel
feature-sharing framework to address these difficulties by introducing the mix
and mask mechanisms and the distributional scalarization technique. These
mechanisms behaves dynamically to couple and decouple connected latent features
variably between the policy and value function, while the distributional
scalarization standardizes the two objectives using a probabilistic standpoint.
From our experimental results, we demonstrate significant performance
improvements compared to alternative methods using separate networks and
networks with a shared backbone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huh_D/0/1/0/all/0/1"&gt;Dom Huh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 cases prediction using regression and novel SSM model for non-converged countries. (arXiv:2106.12888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12888</id>
        <link href="http://arxiv.org/abs/2106.12888"/>
        <updated>2021-06-25T02:00:46.493Z</updated>
        <summary type="html"><![CDATA[Anticipating the quantity of new associated or affirmed cases with novel
coronavirus ailment 2019 (COVID-19) is critical in the counteraction and
control of the COVID-19 flare-up. The new associated cases with COVID-19
information were gathered from 20 January 2020 to 21 July 2020. We filtered out
the countries which are converging and used those for training the network. We
utilized the SARIMAX, Linear regression model to anticipate new suspected
COVID-19 cases for the countries which did not converge yet. We predict the
curve of non-converged countries with the help of proposed Statistical SARIMAX
model (SSM). We present new information investigation-based forecast results
that can assist governments with planning their future activities and help
clinical administrations to be more ready for what's to come. Our framework can
foresee peak corona cases with an R-Squared value of 0.986 utilizing linear
regression and fall of this pandemic at various levels for countries like
India, US, and Brazil. We found that considering more countries for training
degrades the prediction process as constraints vary from nation to nation.
Thus, we expect that the outcomes referenced in this work will help individuals
to better understand the possibilities of this pandemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_T/0/1/0/all/0/1"&gt;Tushar Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_U/0/1/0/all/0/1"&gt;Umang Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1"&gt;Rupali Patil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Construction Kit for Efficient Low Power Neural Network Accelerator Designs. (arXiv:2106.12810v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2106.12810</id>
        <link href="http://arxiv.org/abs/2106.12810"/>
        <updated>2021-06-25T02:00:46.488Z</updated>
        <summary type="html"><![CDATA[Implementing embedded neural network processing at the edge requires
efficient hardware acceleration that couples high computational performance
with low power consumption. Driven by the rapid evolution of network
architectures and their algorithmic features, accelerator designs are
constantly updated and improved. To evaluate and compare hardware design
choices, designers can refer to a myriad of accelerator implementations in the
literature. Surveys provide an overview of these works but are often limited to
system-level and benchmark-specific performance metrics, making it difficult to
quantitatively compare the individual effect of each utilized optimization
technique. This complicates the evaluation of optimizations for new accelerator
designs, slowing-down the research progress. This work provides a survey of
neural network accelerator optimization approaches that have been used in
recent works and reports their individual effects on edge processing
performance. It presents the list of optimizations and their quantitative
effects as a construction kit, allowing to assess the design choices for each
building block separately. Reported optimizations range from up to 10'000x
memory savings to 33x energy reductions, providing chip designers an overview
of design choices for implementing efficient low power neural network
accelerators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jokic_P/0/1/0/all/0/1"&gt;Petar Jokic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azarkhish_E/0/1/0/all/0/1"&gt;Erfan Azarkhish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonetti_A/0/1/0/all/0/1"&gt;Andrea Bonetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pons_M/0/1/0/all/0/1"&gt;Marc Pons&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emery_S/0/1/0/all/0/1"&gt;Stephane Emery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport. (arXiv:2106.12950v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12950</id>
        <link href="http://arxiv.org/abs/2106.12950"/>
        <updated>2021-06-25T02:00:46.472Z</updated>
        <summary type="html"><![CDATA[Successful quantitative investment usually relies on precise predictions of
the future movement of the stock price. Recently, machine learning based
solutions have shown their capacity to give more accurate stock prediction and
become indispensable components in modern quantitative investment systems.
However, the i.i.d. assumption behind existing methods is inconsistent with the
existence of diverse trading patterns in the stock market, which inevitably
limits their ability to achieve better stock prediction performance. In this
paper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to
empower existing stock prediction models with the ability to model multiple
stock trading patterns. Essentially, TRA is a lightweight module that consists
of a set of independent predictors for learning multiple patterns as well as a
router to dispatch samples to different predictors. Nevertheless, the lack of
explicit pattern identifiers makes it quite challenging to train an effective
TRA-based model. To tackle this challenge, we further design a learning
algorithm based on Optimal Transport (OT) to obtain the optimal sample to
predictor assignment and effectively optimize the router with such assignment
through an auxiliary loss term. Experiments on the real-world stock ranking
task show that compared to the state-of-the-art baselines, e.g., Attention LSTM
and Transformer, the proposed method can improve information coefficient (IC)
from 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used
in this work are publicly available: https://github.com/microsoft/qlib.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hengxu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lettuce: PyTorch-based Lattice Boltzmann Framework. (arXiv:2106.12929v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12929</id>
        <link href="http://arxiv.org/abs/2106.12929"/>
        <updated>2021-06-25T02:00:46.467Z</updated>
        <summary type="html"><![CDATA[The lattice Boltzmann method (LBM) is an efficient simulation technique for
computational fluid mechanics and beyond. It is based on a simple
stream-and-collide algorithm on Cartesian grids, which is easily compatible
with modern machine learning architectures. While it is becoming increasingly
clear that deep learning can provide a decisive stimulus for classical
simulation techniques, recent studies have not addressed possible connections
between machine learning and LBM. Here, we introduce Lettuce, a PyTorch-based
LBM code with a threefold aim. Lettuce enables GPU accelerated calculations
with minimal source code, facilitates rapid prototyping of LBM models, and
enables integrating LBM simulations with PyTorch's deep learning and automatic
differentiation facility. As a proof of concept for combining machine learning
with the LBM, a neural collision model is developed, trained on a doubly
periodic shear layer and then transferred to a different flow, a decaying
turbulence. We also exemplify the added benefit of PyTorch's automatic
differentiation framework in flow control and optimization. To this end, the
spectrum of a forced isotropic turbulence is maintained without further
constraining the velocity field. The source code is freely available from
https://github.com/lettucecfd/lettuce.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Bedrunka_M/0/1/0/all/0/1"&gt;Mario Christopher Bedrunka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wilde_D/0/1/0/all/0/1"&gt;Dominik Wilde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kliemank_M/0/1/0/all/0/1"&gt;Martin Kliemank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Reith_D/0/1/0/all/0/1"&gt;Dirk Reith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Foysi_H/0/1/0/all/0/1"&gt;Holger Foysi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kramer_A/0/1/0/all/0/1"&gt;Andreas Kr&amp;#xe4;mer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InFlow: Robust outlier detection utilizing Normalizing Flows. (arXiv:2106.12894v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12894</id>
        <link href="http://arxiv.org/abs/2106.12894"/>
        <updated>2021-06-25T02:00:46.453Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are prominent deep generative models that provide tractable
probability distributions and efficient density estimation. However, they are
well known to fail while detecting Out-of-Distribution (OOD) inputs as they
directly encode the local features of the input representations in their latent
space. In this paper, we solve this overconfidence issue of normalizing flows
by demonstrating that flows, if extended by an attention mechanism, can
reliably detect outliers including adversarial attacks. Our approach does not
require outlier data for training and we showcase the efficiency of our method
for OOD detection by reporting state-of-the-art performance in diverse
experimental settings. Code available at
https://github.com/ComputationalRadiationPhysics/InFlow .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Nishant Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanfeld_P/0/1/0/all/0/1"&gt;Pia Hanfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hecht_M/0/1/0/all/0/1"&gt;Michael Hecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bussmann_M/0/1/0/all/0/1"&gt;Michael Bussmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gumhold_S/0/1/0/all/0/1"&gt;Stefan Gumhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffmannn_N/0/1/0/all/0/1"&gt;Nico Hoffmannn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions. (arXiv:2106.12739v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12739</id>
        <link href="http://arxiv.org/abs/2106.12739"/>
        <updated>2021-06-25T02:00:46.447Z</updated>
        <summary type="html"><![CDATA[Containerization is a lightweight application virtualization technology,
providing high environmental consistency, operating system distribution
portability, and resource isolation. Existing mainstream cloud service
providers have prevalently adopted container technologies in their distributed
system infrastructures for automated application management. To handle the
automation of deployment, maintenance, autoscaling, and networking of
containerized applications, container orchestration is proposed as an essential
research problem. However, the highly dynamic and diverse feature of cloud
workloads and environments considerably raises the complexity of orchestration
mechanisms. Machine learning algorithms are accordingly employed by container
orchestration systems for behavior modelling and prediction of
multi-dimensional performance metrics. Such insights could further improve the
quality of resource provisioning decisions in response to the changing
workloads under complex environments. In this paper, we present a comprehensive
literature review of existing machine learning-based container orchestration
approaches. Detailed taxonomies are proposed to classify the current researches
by their common features. Moreover, the evolution of machine learning-based
container orchestration technologies from the year 2016 to 2021 has been
designed based on objectives and metrics. A comparative analysis of the
reviewed techniques is conducted according to the proposed taxonomies, with
emphasis on their key characteristics. Finally, various open research
challenges and potential future directions are highlighted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhiheng Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minxian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1"&gt;Maria Alejandra Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chengzhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buyya_R/0/1/0/all/0/1"&gt;Rajkumar Buyya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural ODE to model and prognose thermoacoustic instability. (arXiv:2106.12758v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2106.12758</id>
        <link href="http://arxiv.org/abs/2106.12758"/>
        <updated>2021-06-25T02:00:46.441Z</updated>
        <summary type="html"><![CDATA[In reacting flow systems, thermoacoustic instability characterized by high
amplitude pressure fluctuations, is driven by a positive coupling between the
unsteady heat release rate and the acoustic field of the combustor. When the
underlying flow is turbulent, as a control parameter of the system is varied
and the system approach thermoacoustic instability, the acoustic pressure
oscillations synchronize with heat release rate oscillations. Consequently,
during the onset of thermoacoustic instability in turbulent combustors, the
system dynamics transition from chaotic oscillations to periodic oscillations
via a state of intermittency. Thermoacoustic systems are traditionally modeled
by coupling the model for the unsteady heat source and the acoustic subsystem,
each estimated independently. The response of the unsteady heat source, the
flame, to acoustic fluctuations are characterized by introducing external
unsteady forcing. This necessitates a powerful excitation module to obtain the
nonlinear response of the flame to acoustic perturbations. Instead of
characterizing individual subsystems, we introduce a neural ordinary
differential equation (neural ODE) framework to model the thermoacoustic system
as a whole. The neural ODE model for the thermoacoustic system uses time series
of the heat release rate and the pressure fluctuations, measured simultaneously
without introducing any external perturbations, to model their coupled
interaction. Further, we use the parameters of neural ODE to define an anomaly
measure that represents the proximity of system dynamics to limit cycle
oscillations and thus provide an early warning signal for the onset of
thermoacoustic instability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Dhadphale_J/0/1/0/all/0/1"&gt;Jayesh Dhadphale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Unni_V/0/1/0/all/0/1"&gt;Vishnu R. Unni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Saha_A/0/1/0/all/0/1"&gt;Abhishek Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sujith_R/0/1/0/all/0/1"&gt;R. I. Sujith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-agnostic Continual Learning with Hybrid Probabilistic Models. (arXiv:2106.12772v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12772</id>
        <link href="http://arxiv.org/abs/2106.12772"/>
        <updated>2021-06-25T02:00:46.425Z</updated>
        <summary type="html"><![CDATA[Learning new tasks continuously without forgetting on a constantly changing
data distribution is essential for real-world problems but extremely
challenging for modern deep learning. In this work we propose HCL, a Hybrid
generative-discriminative approach to Continual Learning for classification. We
model the distribution of each task and each class with a normalizing flow. The
flow is used to learn the data distribution, perform classification, identify
task changes, and avoid forgetting, all leveraging the invertibility and exact
likelihood which are uniquely enabled by the normalizing flow model. We use the
generative capabilities of the flow to avoid catastrophic forgetting through
generative replay and a novel functional regularization technique. For task
identification, we use state-of-the-art anomaly detection techniques based on
measuring the typicality of the model's statistics. We demonstrate the strong
performance of HCL on a range of continual learning benchmarks such as
split-MNIST, split-CIFAR, and SVHN-MNIST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirichenko_P/0/1/0/all/0/1"&gt;Polina Kirichenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1"&gt;Mehrdad Farajtabar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1"&gt;Dushyant Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_N/0/1/0/all/0/1"&gt;Nir Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Huiyi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Algorithms for Clustering with Stability Assumptions. (arXiv:2106.12959v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12959</id>
        <link href="http://arxiv.org/abs/2106.12959"/>
        <updated>2021-06-25T02:00:46.406Z</updated>
        <summary type="html"><![CDATA[We study the problem of differentially private clustering under
input-stability assumptions. Despite the ever-growing volume of works on
differential privacy in general and differentially private clustering in
particular, only three works (Nissim et al. 2007, Wang et al. 2015, Huang et
al. 2018) looked at the problem of privately clustering "nice" k-means
instances, all three relying on the sample-and-aggregate framework and all
three measuring utility in terms of Wasserstein distance between the true
cluster centers and the centers returned by the private algorithm. In this work
we improve upon this line of works on multiple axes. We present a far simpler
algorithm for clustering stable inputs (not relying on the sample-and-aggregate
framework), and analyze its utility in both the Wasserstein distance and the
k-means cost. Moreover, our algorithm has straight-forward analogues for "nice"
k-median instances and for the local-model of differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shechner_M/0/1/0/all/0/1"&gt;Moshe Shechner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCoM: A Deep Column Mapper for Semantic Data Type Detection. (arXiv:2106.12871v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12871</id>
        <link href="http://arxiv.org/abs/2106.12871"/>
        <updated>2021-06-25T02:00:46.389Z</updated>
        <summary type="html"><![CDATA[Detection of semantic data types is a very crucial task in data science for
automated data cleaning, schema matching, data discovery, semantic data type
normalization and sensitive data identification. Existing methods include
regular expression-based or dictionary lookup-based methods that are not robust
to dirty as well unseen data and are limited to a very less number of semantic
data types to predict. Existing Machine Learning methods extract large number
of engineered features from data and build logistic regression, random forest
or feedforward neural network for this purpose. In this paper, we introduce
DCoM, a collection of multi-input NLP-based deep neural networks to detect
semantic data types where instead of extracting large number of features from
the data, we feed the raw values of columns (or instances) to the model as
texts. We train DCoM on 686,765 data columns extracted from VizNet corpus with
78 different semantic data types. DCoM outperforms other contemporary results
with a quite significant margin on the same dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhadip Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rout_S/0/1/0/all/0/1"&gt;Swapna Sourav Rout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Sudeep Choudhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Neural Network from Adder's Perspective: Carry-lookahead RNN. (arXiv:2106.12901v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12901</id>
        <link href="http://arxiv.org/abs/2106.12901"/>
        <updated>2021-06-25T02:00:46.382Z</updated>
        <summary type="html"><![CDATA[The recurrent network architecture is a widely used model in sequence
modeling, but its serial dependency hinders the computation parallelization,
which makes the operation inefficient. The same problem was encountered in
serial adder at the early stage of digital electronics. In this paper, we
discuss the similarities between recurrent neural network (RNN) and serial
adder. Inspired by carry-lookahead adder, we introduce carry-lookahead module
to RNN, which makes it possible for RNN to run in parallel. Then, we design the
method of parallel RNN computation, and finally Carry-lookahead RNN (CL-RNN) is
proposed. CL-RNN takes advantages in parallelism and flexible receptive field.
Through a comprehensive set of tests, we verify that CL-RNN can perform better
than existing typical RNNs in sequence modeling tasks which are specially
designed for RNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haowei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1"&gt;Feiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jin Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yanli Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Truncated SVD based Model for Node Classification on Heterophilic Graphs. (arXiv:2106.12807v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12807</id>
        <link href="http://arxiv.org/abs/2106.12807"/>
        <updated>2021-06-25T02:00:46.362Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have shown excellent performance on graphs that
exhibit strong homophily with respect to the node labels i.e. connected nodes
have same labels. However, they perform poorly on heterophilic graphs. Recent
approaches have typically modified aggregation schemes, designed adaptive graph
filters, etc. to address this limitation. In spite of this, the performance on
heterophilic graphs can still be poor. We propose a simple alternative method
that exploits Truncated Singular Value Decomposition (TSVD) of topological
structure and node features. Our approach achieves up to ~30% improvement in
performance over state-of-the-art methods on heterophilic graphs. This work is
an early investigation into methods that differ from aggregation based
approaches. Our experimental results suggest that it might be important to
explore other alternatives to aggregation methods for heterophilic setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lingam_V/0/1/0/all/0/1"&gt;Vijay Lingam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragesh_R/0/1/0/all/0/1"&gt;Rahul Ragesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1"&gt;Arun Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sellamanickam_S/0/1/0/all/0/1"&gt;Sundararajan Sellamanickam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Factors affecting the COVID-19 risk in the US counties: an innovative approach by combining unsupervised and supervised learning. (arXiv:2106.12766v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12766</id>
        <link href="http://arxiv.org/abs/2106.12766"/>
        <updated>2021-06-25T02:00:46.344Z</updated>
        <summary type="html"><![CDATA[The COVID-19 disease spreads swiftly, and nearly three months after the first
positive case was confirmed in China, Coronavirus started to spread all over
the United States. Some states and counties reported high number of positive
cases and deaths, while some reported lower COVID-19 related cases and
mortality. In this paper, the factors that could affect the risk of COVID-19
infection and mortality were analyzed in county level. An innovative method by
using K-means clustering and several classification models is utilized to
determine the most critical factors. Results showed that mean temperature,
percent of people below poverty, percent of adults with obesity, air pressure,
population density, wind speed, longitude, and percent of uninsured people were
the most significant attributes]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziyadidegan_S/0/1/0/all/0/1"&gt;Samira Ziyadidegan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razavi_M/0/1/0/all/0/1"&gt;Moein Razavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pesarakli_H/0/1/0/all/0/1"&gt;Homa Pesarakli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javid_A/0/1/0/all/0/1"&gt;Amir Hossein Javid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erraguntla_M/0/1/0/all/0/1"&gt;Madhav Erraguntla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding. (arXiv:2106.12839v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12839</id>
        <link href="http://arxiv.org/abs/2106.12839"/>
        <updated>2021-06-25T02:00:46.338Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are a class of powerful machine learning tools
that model node relations for making predictions of nodes or links. GNN
developers rely on quantitative metrics of the predictions to evaluate a GNN,
but similar to many other neural networks, it is difficult for them to
understand if the GNN truly learns characteristics of a graph as expected. We
propose an approach to corresponding an input graph to its node embedding (aka
latent space), a common component of GNNs that is later used for prediction. We
abstract the data and tasks, and develop an interactive multi-view interface
called CorGIE to instantiate the abstraction. As the key function in CorGIE, we
propose the K-hop graph layout to show topological neighbors in hops and their
clustering structure. To evaluate the functionality and usability of CorGIE, we
present how to use CorGIE in two usage scenarios, and conduct a case study with
two GNN experts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zipeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernard_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Bernard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munzner_T/0/1/0/all/0/1"&gt;Tamara Munzner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MARL with General Utilities via Decentralized Shadow Reward Actor-Critic. (arXiv:2106.00543v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00543</id>
        <link href="http://arxiv.org/abs/2106.00543"/>
        <updated>2021-06-25T02:00:46.318Z</updated>
        <summary type="html"><![CDATA[We posit a new mechanism for cooperation in multi-agent reinforcement
learning (MARL) based upon any nonlinear function of the team's long-term
state-action occupancy measure, i.e., a \emph{general utility}. This subsumes
the cumulative return but also allows one to incorporate risk-sensitivity,
exploration, and priors. % We derive the {\bf D}ecentralized {\bf S}hadow
Reward {\bf A}ctor-{\bf C}ritic (DSAC) in which agents alternate between policy
evaluation (critic), weighted averaging with neighbors (information mixing),
and local gradient updates for their policy parameters (actor). DSAC augments
the classic critic step by requiring agents to (i) estimate their local
occupancy measure in order to (ii) estimate the derivative of the local utility
with respect to their occupancy measure, i.e., the "shadow reward". DSAC
converges to $\epsilon$-stationarity in $\mathcal{O}(1/\epsilon^{2.5})$
(Theorem \ref{theorem:final}) or faster $\mathcal{O}(1/\epsilon^{2})$
(Corollary \ref{corollary:communication}) steps with high probability,
depending on the amount of communications. We further establish the
non-existence of spurious stationary points for this problem, that is, DSAC
finds the globally optimal policy (Corollary \ref{corollary:global}).
Experiments demonstrate the merits of goals beyond the cumulative return in
cooperative MARL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bedi_A/0/1/0/all/0/1"&gt;Amrit Singh Bedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1"&gt;Alec Koppel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Graph Neural Architecture Search from Message-passing. (arXiv:2103.14282v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14282</id>
        <link href="http://arxiv.org/abs/2103.14282"/>
        <updated>2021-06-25T02:00:46.312Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) emerged recently as a standard toolkit for
learning from data on graphs. Current GNN designing works depend on immense
human expertise to explore different message-passing mechanisms, and require
manual enumeration to determine the proper message-passing depth. Inspired by
the strong searching capability of neural architecture search (NAS) in CNN,
this paper proposes Graph Neural Architecture Search (GNAS) with novel-designed
search space. The GNAS can automatically learn better architecture with the
optimal depth of message passing on the graph. Specifically, we design Graph
Neural Architecture Paradigm (GAP) with tree-topology computation procedure and
two types of fine-grained atomic operations (feature filtering and neighbor
aggregation) from message-passing mechanism to construct powerful graph network
search space. Feature filtering performs adaptive feature selection, and
neighbor aggregation captures structural information and calculates neighbors'
statistics. Experiments show that our GNAS can search for better GNNs with
multiple message-passing mechanisms and optimal message-passing depth. The
searched network achieves remarkable improvement over state-of-the-art manual
designed and search-based GNNs on five large-scale datasets at three classical
graph tasks. Codes can be found at https://github.com/phython96/GNAS-MP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shaofei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jincan Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Beichen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Li Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle. (arXiv:2004.03376v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03376</id>
        <link href="http://arxiv.org/abs/2004.03376"/>
        <updated>2021-06-25T02:00:46.306Z</updated>
        <summary type="html"><![CDATA[The computation and memory needed for Convolutional Neural Network (CNN)
inference can be reduced by pruning weights from the trained network. Pruning
is guided by a pruning saliency, which heuristically approximates the change in
the loss function associated with the removal of specific weights. Many pruning
signals have been proposed, but the performance of each heuristic depends on
the particular trained network. This leaves the data scientist with a difficult
choice. When using any one saliency metric for the entire pruning process, we
run the risk of the metric assumptions being invalidated, leading to poor
decisions being made by the metric. Ideally we could combine the best aspects
of different saliency metrics. However, despite an extensive literature review,
we are unable to find any prior work on composing different saliency metrics.
The chief difficulty lies in combining the numerical output of different
saliency metrics, which are not directly comparable.

We propose a method to compose several primitive pruning saliencies, to
exploit the cases where each saliency measure does well. Our experiments show
that the composition of saliencies avoids many poor pruning choices identified
by individual saliencies. In most cases our method finds better selections than
even the best individual pruning saliency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1"&gt;Kaveena Persand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1"&gt;Andrew Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1"&gt;David Gregg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Receiver operating characteristic (ROC) movies, universal ROC (UROC) curves, and coefficient of predictive ability (CPA). (arXiv:1912.01956v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.01956</id>
        <link href="http://arxiv.org/abs/1912.01956"/>
        <updated>2021-06-25T02:00:46.288Z</updated>
        <summary type="html"><![CDATA[Throughout science and technology, receiver operating characteristic (ROC)
curves and associated area under the curve (AUC) measures constitute powerful
tools for assessing the predictive abilities of features, markers and tests in
binary classification problems. Despite its immense popularity, ROC analysis
has been subject to a fundamental restriction, in that it applies to
dichotomous (yes or no) outcomes only. Here we introduce ROC movies and
universal ROC (UROC) curves that apply to just any linearly ordered outcome,
along with an associated coefficient of predictive ability (CPA) measure. CPA
equals the area under the UROC curve, and admits appealing interpretations in
terms of probabilities and rank based covariances. For binary outcomes CPA
equals AUC, and for pairwise distinct outcomes CPA relates linearly to
Spearman's coefficient, in the same way that the C index relates linearly to
Kendall's coefficient. ROC movies, UROC curves, and CPA nest and generalize the
tools of classical ROC analysis, and are bound to supersede them in a wealth of
applications. Their usage is illustrated in data examples from biomedicine and
meteorology, where rank based measures yield new insights in the WeatherBench
comparison of the predictive performance of convolutional neural networks and
physical-numerical models for weather prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gneiting_T/0/1/0/all/0/1"&gt;Tilmann Gneiting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Walz_E/0/1/0/all/0/1"&gt;Eva-Maria Walz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Adversarial Perturbations for CNN Classifiers in EEG-Based BCIs. (arXiv:1912.01171v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.01171</id>
        <link href="http://arxiv.org/abs/1912.01171"/>
        <updated>2021-06-25T02:00:46.268Z</updated>
        <summary type="html"><![CDATA[Multiple convolutional neural network (CNN) classifiers have been proposed
for electroencephalogram (EEG) based brain-computer interfaces (BCIs). However,
CNN models have been found vulnerable to universal adversarial perturbations
(UAPs), which are small and example-independent, yet powerful enough to degrade
the performance of a CNN model, when added to a benign example. This paper
proposes a novel total loss minimization (TLM) approach to generate UAPs for
EEG-based BCIs. Experimental results demonstrated the effectiveness of TLM on
three popular CNN classifiers for both target and non-target attacks. We also
verified the transferability of UAPs in EEG-based BCI systems. To our
knowledge, this is the first study on UAPs of CNN classifiers in EEG-based
BCIs. UAPs are easy to construct, and can attack BCIs in real-time, exposing a
potentially critical security concern of BCIs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zihan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lubin Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Weili Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Bearing Fault Diagnosis Based on Model-Agnostic Meta-Learning. (arXiv:2007.12851v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.12851</id>
        <link href="http://arxiv.org/abs/2007.12851"/>
        <updated>2021-06-25T02:00:46.240Z</updated>
        <summary type="html"><![CDATA[The rapid development of artificial intelligence and deep learning has
provided many opportunities to further enhance the safety, stability, and
accuracy of industrial Cyber-Physical Systems (CPS). As indispensable
components to many mission-critical CPS assets and equipment, mechanical
bearings need to be monitored to identify any trace of abnormal conditions.
Most of the data-driven approaches applied to bearing fault diagnosis
up-to-date are trained using a large amount of fault data collected a priori.
In many practical applications, however, it can be unsafe and time-consuming to
collect sufficient data samples for each fault category, making it challenging
to train a robust classifier. In this paper, we propose a few-shot learning
framework for bearing fault diagnosis based on model-agnostic meta-learning
(MAML), which targets for training an effective fault classifier using limited
data. In addition, it can leverage the training data and learn to identify new
fault scenarios more efficiently. Case studies on the generalization to new
artificial faults show that the proposed framework achieves an overall accuracy
up to 25% higher than a Siamese network-based benchmark study. Finally, the
robustness and the generalization capability of the proposed framework are
further validated by applying it to identify real bearing damages using data
from artificial damages, which compares favorably against 6 state-of-the-art
few-shot learning algorithms using consistent test environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bingnan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habetler_T/0/1/0/all/0/1"&gt;Thomas G. Habetler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[L'Apprentissage Automatique dans la planification et le contr{\^o}le de la production : un {\'e}tat de l'art. (arXiv:2106.12916v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12916</id>
        <link href="http://arxiv.org/abs/2106.12916"/>
        <updated>2021-06-25T02:00:46.235Z</updated>
        <summary type="html"><![CDATA[Proper Production Planning and Control (PPC) is capital to have an edge over
competitors, reduce costs and respect delivery dates. With regard to PPC,
Machine Learning (ML) provides new opportunities to make intelligent decisions
based on data. Therefore, this communication provides an initial systematic
review of publications on ML applied in PPC. The research objective of this
study is twofold: firstly, it aims to identify techniques and tools allowing to
apply ML in PPC, and secondly, it reviews the characteristics of Industry 4.0
(I4.0) in recent research papers. Concerning the second objective, seven
characteristics of I4.0 are used in the analysis framework, from which two of
them are proposed by the authors. Additionally, the addressed domains of
ML-aided PPC in scientific literature are identified. Finally, results are
analyzed and gaps that may motivate further research are highlighted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cadavid_J/0/1/0/all/0/1"&gt;Juan Pablo Usuga Cadavid&lt;/a&gt; (LAMIH, ENSAM), &lt;a href="http://arxiv.org/find/cs/1/au:+Lamouri_S/0/1/0/all/0/1"&gt;Samir Lamouri&lt;/a&gt; (LAMIH, ENSAM), &lt;a href="http://arxiv.org/find/cs/1/au:+Grabot_B/0/1/0/all/0/1"&gt;Bernard Grabot&lt;/a&gt; (LGP, ENIT), &lt;a href="http://arxiv.org/find/cs/1/au:+Fortin_A/0/1/0/all/0/1"&gt;Arnaud Fortin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Spread of COVID-19 Epidemic: A Spatio-Temporal Point Process View. (arXiv:2106.13097v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13097</id>
        <link href="http://arxiv.org/abs/2106.13097"/>
        <updated>2021-06-25T02:00:46.219Z</updated>
        <summary type="html"><![CDATA[Since the first coronavirus case was identified in the U.S. on Jan. 21, more
than 1 million people in the U.S. have confirmed cases of COVID-19. This
infectious respiratory disease has spread rapidly across more than 3000
counties and 50 states in the U.S. and have exhibited evolutionary clustering
and complex triggering patterns. It is essential to understand the complex
spacetime intertwined propagation of this disease so that accurate prediction
or smart external intervention can be carried out. In this paper, we model the
propagation of the COVID-19 as spatio-temporal point processes and propose a
generative and intensity-free model to track the spread of the disease. We
further adopt a generative adversarial imitation learning framework to learn
the model parameters. In comparison with the traditional likelihood-based
learning methods, this imitation learning framework does not need to prespecify
an intensity function, which alleviates the model-misspecification. Moreover,
the adversarial learning procedure bypasses the difficult-to-evaluate integral
involved in the likelihood evaluation, which makes the model inference more
scalable with the data and variables. We showcase the dynamic learning
performance on the COVID-19 confirmed cases in the U.S. and evaluate the social
distancing policy based on the learned generative model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinyun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yixiang Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yan Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prototype Completion with Primitive Knowledge for Few-Shot Learning. (arXiv:2009.04960v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04960</id>
        <link href="http://arxiv.org/abs/2009.04960"/>
        <updated>2021-06-25T02:00:46.214Z</updated>
        <summary type="html"><![CDATA[Few-shot learning is a challenging task, which aims to learn a classifier for
novel classes with few examples. Pre-training based meta-learning methods
effectively tackle the problem by pre-training a feature extractor and then
fine-tuning it through the nearest centroid based meta-learning. However,
results show that the fine-tuning step makes very marginal improvements. In
this paper, 1) we figure out the key reason, i.e., in the pre-trained feature
space, the base classes already form compact clusters while novel classes
spread as groups with large variances, which implies that fine-tuning the
feature extractor is less meaningful; 2) instead of fine-tuning the feature
extractor, we focus on estimating more representative prototypes during
meta-learning. Consequently, we propose a novel prototype completion based
meta-learning framework. This framework first introduces primitive knowledge
(i.e., class-level part or attribute annotations) and extracts representative
attribute features as priors. Then, we design a prototype completion network to
learn to complete prototypes with these priors. To avoid the prototype
completion error caused by primitive knowledge noises or class differences, we
further develop a Gaussian based prototype fusion strategy that combines the
mean-based and completed prototypes by exploiting the unlabeled samples.
Extensive experiments show that our method: (i) can obtain more accurate
prototypes; (ii) outperforms state-of-the-art techniques by 2% - 9% in terms of
classification accuracy. Our code is available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Baoquan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xutao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yunming Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhichao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lisai Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Projective Splitting: Solving Saddle-Point Problems with Multiple Regularizers. (arXiv:2106.13067v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.13067</id>
        <link href="http://arxiv.org/abs/2106.13067"/>
        <updated>2021-06-25T02:00:46.208Z</updated>
        <summary type="html"><![CDATA[We present a new, stochastic variant of the projective splitting (PS) family
of algorithms for monotone inclusion problems. It can solve min-max and
noncooperative game formulations arising in applications such as robust ML
without the convergence issues associated with gradient descent-ascent, the
current de facto standard approach in such situations. Our proposal is the
first version of PS able to use stochastic (as opposed to deterministic)
gradient oracles. It is also the first stochastic method that can solve min-max
games while easily handling multiple constraints and nonsmooth regularizers via
projection and proximal operators. We close with numerical experiments on a
distributionally robust sparse logistic regression problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Johnstone_P/0/1/0/all/0/1"&gt;Patrick R. Johnstone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Eckstein_J/0/1/0/all/0/1"&gt;Jonathan Eckstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Flynn_T/0/1/0/all/0/1"&gt;Thomas Flynn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Shinjae Yoo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GNMR: A provable one-line algorithm for low rank matrix recovery. (arXiv:2106.12933v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.12933</id>
        <link href="http://arxiv.org/abs/2106.12933"/>
        <updated>2021-06-25T02:00:46.203Z</updated>
        <summary type="html"><![CDATA[Low rank matrix recovery problems, including matrix completion and matrix
sensing, appear in a broad range of applications. In this work we present GNMR
-- an extremely simple iterative algorithm for low rank matrix recovery, based
on a Gauss-Newton linearization. On the theoretical front, we derive recovery
guarantees for GNMR in both the matrix sensing and matrix completion settings.
A key property of GNMR is that it implicitly keeps the factor matrices
approximately balanced throughout its iterations. On the empirical front, we
show that for matrix completion with uniform sampling, GNMR performs better
than several popular methods, especially when given very few observations close
to the information limit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zilber_P/0/1/0/all/0/1"&gt;Pini Zilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nadler_B/0/1/0/all/0/1"&gt;Boaz Nadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Saliency-based Explainability Method. (arXiv:2106.12773v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12773</id>
        <link href="http://arxiv.org/abs/2106.12773"/>
        <updated>2021-06-25T02:00:46.198Z</updated>
        <summary type="html"><![CDATA[A particular class of Explainable AI (XAI) methods provide saliency maps to
highlight part of the image a Convolutional Neural Network (CNN) model looks at
to classify the image as a way to explain its working. These methods provide an
intuitive way for users to understand predictions made by CNNs. Other than
quantitative computational tests, the vast majority of evidence to highlight
that the methods are valuable is anecdotal. Given that humans would be the
end-users of such methods, we devise three human subject experiments through
which we gauge the effectiveness of these saliency-based explainability
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samuel_S/0/1/0/all/0/1"&gt;Sam Zabdiel Sunder Samuel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamakshi_V/0/1/0/all/0/1"&gt;Vidhya Kamakshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lodhi_N/0/1/0/all/0/1"&gt;Namrata Lodhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1"&gt;Narayanan C Krishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Fully Interpretable Deep Neural Networks: Are We There Yet?. (arXiv:2106.13164v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13164</id>
        <link href="http://arxiv.org/abs/2106.13164"/>
        <updated>2021-06-25T02:00:46.193Z</updated>
        <summary type="html"><![CDATA[Despite the remarkable performance, Deep Neural Networks (DNNs) behave as
black-boxes hindering user trust in Artificial Intelligence (AI) systems.
Research on opening black-box DNN can be broadly categorized into post-hoc
methods and inherently interpretable DNNs. While many surveys have been
conducted on post-hoc interpretation methods, little effort is devoted to
inherently interpretable DNNs. This paper provides a review of existing methods
to develop DNNs with intrinsic interpretability, with a focus on Convolutional
Neural Networks (CNNs). The aim is to understand the current progress towards
fully interpretable DNNs that can cater to different interpretation
requirements. Finally, we identify gaps in current work and suggest potential
research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramanayake_S/0/1/0/all/0/1"&gt;Sandareka Wickramanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wynne Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mong Li Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12798</id>
        <link href="http://arxiv.org/abs/2106.12798"/>
        <updated>2021-06-25T02:00:46.188Z</updated>
        <summary type="html"><![CDATA[Automated Machine Learning (AutoML) has gained increasing success on tabular
data in recent years. However, processing unstructured data like text is a
challenge and not widely supported by open-source AutoML tools. This work
compares three manually created text representations and text embeddings
automatically created by AutoML tools. Our benchmark includes four popular
open-source AutoML tools and eight datasets for text classification purposes.
The results show that straightforward text representations perform better than
AutoML tools with automatically created text embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1"&gt;Sebastian Br&amp;#xe4;ndle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1"&gt;Marc Hanussek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1"&gt;Matthias Blohm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1"&gt;Maximilien Kintz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VinDr-SpineXR: A deep learning framework for spinal lesions detection and classification from radiographs. (arXiv:2106.12930v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12930</id>
        <link href="http://arxiv.org/abs/2106.12930"/>
        <updated>2021-06-25T02:00:46.173Z</updated>
        <summary type="html"><![CDATA[Radiographs are used as the most important imaging tool for identifying spine
anomalies in clinical practice. The evaluation of spinal bone lesions, however,
is a challenging task for radiologists. This work aims at developing and
evaluating a deep learning-based framework, named VinDr-SpineXR, for the
classification and localization of abnormalities from spine X-rays. First, we
build a large dataset, comprising 10,468 spine X-ray images from 5,000 studies,
each of which is manually annotated by an experienced radiologist with bounding
boxes around abnormal findings in 13 categories. Using this dataset, we then
train a deep learning classifier to determine whether a spine scan is abnormal
and a detector to localize 7 crucial findings amongst the total 13. The
VinDr-SpineXR is evaluated on a test set of 2,078 images from 1,000 studies,
which is kept separate from the training set. It demonstrates an area under the
receiver operating characteristic curve (AUROC) of 88.61% (95% CI 87.19%,
90.02%) for the image-level classification task and a mean average precision
(mAP@0.5) of 33.56% for the lesion-level localization task. These results serve
as a proof of concept and set a baseline for future research in this direction.
To encourage advances, the dataset, codes, and trained deep learning models are
made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hieu T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu H. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Nghia T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha Q. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huynh_T/0/1/0/all/0/1"&gt;Thang Q. Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dao_M/0/1/0/all/0/1"&gt;Minh Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vu_V/0/1/0/all/0/1"&gt;Van Vu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Monocular Depth Estimation of Untextured Indoor Rotated Scenes. (arXiv:2106.12958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12958</id>
        <link href="http://arxiv.org/abs/2106.12958"/>
        <updated>2021-06-25T02:00:46.167Z</updated>
        <summary type="html"><![CDATA[Self-supervised deep learning methods have leveraged stereo images for
training monocular depth estimation. Although these methods show strong results
on outdoor datasets such as KITTI, they do not match performance of supervised
methods on indoor environments with camera rotation. Indoor, rotated scenes are
common for less constrained applications and pose problems for two reasons:
abundance of low texture regions and increased complexity of depth cues for
images under rotation. In an effort to extend self-supervised learning to more
generalised environments we propose two additions. First, we propose a novel
Filled Disparity Loss term that corrects for ambiguity of image reconstruction
error loss in textureless regions. Specifically, we interpolate disparity in
untextured regions, using the estimated disparity from surrounding textured
areas, and use L1 loss to correct the original estimation. Our experiments show
that depth estimation is substantially improved on low-texture scenes, without
any loss on textured scenes, when compared to Monodepth by Godard et al.
Secondly, we show that training with an application's representative rotations,
in both pitch and roll, is sufficient to significantly improve performance over
the entire range of expected rotation. We demonstrate that depth estimation is
successfully generalised as performance is not lost when evaluated on test sets
with no camera rotation. Together these developments enable a broader use of
self-supervised learning of monocular depth estimation for complex
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keltjens_B/0/1/0/all/0/1"&gt;Benjamin Keltjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_T/0/1/0/all/0/1"&gt;Tom van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;Guido de Croon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Tensor Contraction via Fast Count Sketch. (arXiv:2106.13062v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13062</id>
        <link href="http://arxiv.org/abs/2106.13062"/>
        <updated>2021-06-25T02:00:46.162Z</updated>
        <summary type="html"><![CDATA[Sketching uses randomized Hash functions for dimensionality reduction and
acceleration. The existing sketching methods, such as count sketch (CS), tensor
sketch (TS), and higher-order count sketch (HCS), either suffer from low
accuracy or slow speed in some tensor based applications. In this paper, the
proposed fast count sketch (FCS) applies multiple shorter Hash functions based
CS to the vector form of the input tensor, which is more accurate than TS since
the spatial information of the input tensor can be preserved more sufficiently.
When the input tensor admits CANDECOMP/PARAFAC decomposition (CPD), FCS can
accelerate CS and HCS by using fast Fourier transform, which exhibits a
computational complexity asymptotically identical to TS for low-order tensors.
The effectiveness of FCS is validated by CPD, tensor regression network
compression, and Kronecker product compression. Experimental results show its
superior performance in terms of approximation accuracy and computational
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xingyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiani Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design. (arXiv:2106.13058v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13058</id>
        <link href="http://arxiv.org/abs/2106.13058"/>
        <updated>2021-06-25T02:00:46.147Z</updated>
        <summary type="html"><![CDATA[Designing novel protein sequences for a desired 3D topological fold is a
fundamental yet non-trivial task in protein engineering. Challenges exist due
to the complex sequence--fold relationship, as well as the difficulties to
capture the diversity of the sequences (therefore structures and functions)
within a fold. To overcome these challenges, we propose Fold2Seq, a novel
transformer-based generative framework for designing protein sequences
conditioned on a specific target fold. To model the complex sequence--structure
relationship, Fold2Seq jointly learns a sequence embedding using a transformer
and a fold embedding from the density of secondary structural elements in 3D
voxels. On test sets with single, high-resolution and complete structure inputs
for individual folds, our experiments demonstrate improved or comparable
performance of Fold2Seq in terms of speed, coverage, and reliability for
sequence design, when compared to existing state-of-the-art methods that
include data-driven deep generative models and physics-based RosettaDesign. The
unique advantages of fold-based Fold2Seq, in comparison to a structure-based
deep model and RosettaDesign, become more evident on three additional
real-world challenges originating from low-quality, incomplete, or ambiguous
input structures. Source code and data are available at
https://github.com/IBM/fold2seq.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yue Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1"&gt;Vijil Chenthamarakshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1"&gt;Igor Melnyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yang Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Domain Convolutional Neural Network: Accelerated CNN for Large Diabetic Retinopathy Image Classification. (arXiv:2106.12736v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12736</id>
        <link href="http://arxiv.org/abs/2106.12736"/>
        <updated>2021-06-25T02:00:46.141Z</updated>
        <summary type="html"><![CDATA[The conventional spatial convolution layers in the Convolutional Neural
Networks (CNNs) are computationally expensive at the point where the training
time could take days unless the number of layers, the number of training images
or the size of the training images are reduced. The image size of 256x256
pixels is commonly used for most of the applications of CNN, but this image
size is too small for applications like Diabetic Retinopathy (DR)
classification where the image details are important for accurate
classification. This research proposed Frequency Domain Convolution (FDC) and
Frequency Domain Pooling (FDP) layers which were built with RFFT, kernel
initialization strategy, convolution artifact removal and Channel Independent
Convolution (CIC) to replace the conventional convolution and pooling layers.
The FDC and FDP layers are used to build a Frequency Domain Convolutional
Neural Network (FDCNN) to accelerate the training of large images for DR
classification. The Full FDC layer is an extension of the FDC layer to allow
direct use in conventional CNNs, it is also used to modify the VGG16
architecture. FDCNN is shown to be at least 54.21% faster and 70.74% more
memory efficient compared to an equivalent CNN architecture. The modified VGG16
architecture with Full FDC layer is reported to achieve a shorter training time
and a higher accuracy at 95.63% compared to the original VGG16 architecture for
DR classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1"&gt;Ee Fey Goh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;ZhiYuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_W/0/1/0/all/0/1"&gt;Wei Xiang Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AudioCLIP: Extending CLIP to Image, Text and Audio. (arXiv:2106.13043v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13043</id>
        <link href="http://arxiv.org/abs/2106.13043"/>
        <updated>2021-06-25T02:00:46.136Z</updated>
        <summary type="html"><![CDATA[In the past, the rapidly evolving field of sound classification greatly
benefited from the application of methods from other domains. Today, we observe
the trend to fuse domain-specific tasks and approaches together, which provides
the community with new outstanding models.

In this work, we present an extension of the CLIP model that handles audio in
addition to text and images. Our proposed model incorporates the ESResNeXt
audio-model into the CLIP framework using the AudioSet dataset. Such a
combination enables the proposed model to perform bimodal and unimodal
classification and querying, while keeping CLIP's ability to generalize to
unseen datasets in a zero-shot inference fashion.

AudioCLIP achieves new state-of-the-art results in the Environmental Sound
Classification (ESC) task, out-performing other approaches by reaching
accuracies of 90.07% on the UrbanSound8K and 97.15% on the ESC-50 datasets.
Further it sets new baselines in the zero-shot ESC-task on the same datasets
68.78% and 69.40%, respectively).

Finally, we also assess the cross-modal querying performance of the proposed
model as well as the influence of full and partial training on the results. For
the sake of reproducibility, our code is published.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guzhov_A/0/1/0/all/0/1"&gt;Andrey Guzhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1"&gt;Federico Raue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn Hees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning by Planning: Language-Guided Global Image Editing. (arXiv:2106.13156v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13156</id>
        <link href="http://arxiv.org/abs/2106.13156"/>
        <updated>2021-06-25T02:00:46.129Z</updated>
        <summary type="html"><![CDATA[Recently, language-guided global image editing draws increasing attention
with growing application potentials. However, previous GAN-based methods are
not only confined to domain-specific, low-resolution data but also lacking in
interpretability. To overcome the collective difficulties, we develop a
text-to-operation model to map the vague editing language request into a series
of editing operations, e.g., change contrast, brightness, and saturation. Each
operation is interpretable and differentiable. Furthermore, the only
supervision in the task is the target image, which is insufficient for a stable
training of sequential decisions. Hence, we propose a novel operation planning
algorithm to generate possible editing sequences from the target image as
pseudo ground truth. Comparison experiments on the newly collected MA5k-Req
dataset and GIER dataset show the advantages of our methods. Code is available
at https://jshi31.github.io/T2ONet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Ning Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yihang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Trung Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1"&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenliang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fund2Vec: Mutual Funds Similarity using Graph Learning. (arXiv:2106.12987v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2106.12987</id>
        <link href="http://arxiv.org/abs/2106.12987"/>
        <updated>2021-06-25T02:00:46.123Z</updated>
        <summary type="html"><![CDATA[Identifying similar mutual funds with respect to the underlying portfolios
has found many applications in financial services ranging from fund recommender
systems, competitors analysis, portfolio analytics, marketing and sales, etc.
The traditional methods are either qualitative, and hence prone to biases and
often not reproducible, or, are known not to capture all the nuances
(non-linearities) among the portfolios from the raw data. We propose a
radically new approach to identify similar funds based on the weighted
bipartite network representation of funds and their underlying assets data
using a sophisticated machine learning method called Node2Vec which learns an
embedded low-dimensional representation of the network. We call the embedding
\emph{Fund2Vec}. Ours is the first ever study of the weighted bipartite network
representation of the funds-assets network in its original form that identifies
structural similarity among portfolios as opposed to merely portfolio overlaps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Satone_V/0/1/0/all/0/1"&gt;Vipul Satone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Desai_D/0/1/0/all/0/1"&gt;Dhruv Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Mehta_D/0/1/0/all/0/1"&gt;Dhagash Mehta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks. (arXiv:2104.01569v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01569</id>
        <link href="http://arxiv.org/abs/2104.01569"/>
        <updated>2021-06-25T02:00:46.117Z</updated>
        <summary type="html"><![CDATA[This paper addresses the task of (complex) conversational question answering
over a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic
parSing with trAnsformer and Graph atteNtion nEtworks). It is the first
approach, which employs a transformer architecture extended with Graph
Attention Networks for multi-task neural semantic parsing. LASAGNE uses a
transformer model for generating the base logical forms, while the Graph
Attention model is used to exploit correlations between (entity) types and
predicates to produce node representations. LASAGNE also includes a novel
entity recognition module which detects, links, and ranks all relevant entities
in the question context. We evaluate LASAGNE on a standard dataset for complex
sequential question answering, on which it outperforms existing baseline
averages on all question types. Specifically, we show that LASAGNE improves the
F1-score on eight out of ten question types; in some cases, the increase in
F1-score is more than 20% compared to the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1"&gt;Endri Kacupaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1"&gt;Joan Plepi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kuldeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakkar_H/0/1/0/all/0/1"&gt;Harsh Thakkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maleshkova_M/0/1/0/all/0/1"&gt;Maria Maleshkova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepAuditor: Distributed Online Intrusion Detection System for IoT devices via Power Side-channel Auditing. (arXiv:2106.12753v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.12753</id>
        <link href="http://arxiv.org/abs/2106.12753"/>
        <updated>2021-06-25T02:00:46.105Z</updated>
        <summary type="html"><![CDATA[As the number of IoT devices has increased rapidly, IoT botnets have
exploited the vulnerabilities of IoT devices. However, it is still challenging
to detect the initial intrusion on IoT devices prior to massive attacks. Recent
studies have utilized power side-channel information to characterize this
intrusion behavior on IoT devices but still lack real-time detection
approaches. This study aimed to design an online intrusion detection system
called DeepAuditor for IoT devices via power auditing. To realize the real-time
system, we first proposed a lightweight power auditing device called Power
Auditor. With the Power Auditor, we developed a Distributed CNN classifier for
online inference in our laboratory setting. In order to protect data leakage
and reduce networking redundancy, we also proposed a privacy-preserved
inference protocol via Packed Homomorphic Encryption and a sliding window
protocol in our system. The classification accuracy and processing time were
measured in our laboratory settings. We also demonstrated that the distributed
CNN design is secure against any distributed components. Overall, the
measurements were shown to the feasibility of our real-time distributed system
for intrusion detection on IoT devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1"&gt;Woosub Jung&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yizhou Feng&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Sabbir Ahmed Khan&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_C/0/1/0/all/0/1"&gt;Chunsheng Xin&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Danella Zhao&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Gang Zhou&lt;/a&gt; (1) ((1) William &amp; Mary, (2) Old Dominion University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoding Involutory Invariance in Neural Networks. (arXiv:2106.12891v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12891</id>
        <link href="http://arxiv.org/abs/2106.12891"/>
        <updated>2021-06-25T02:00:46.099Z</updated>
        <summary type="html"><![CDATA[In certain situations, Neural Networks (NN) are trained upon data that obey
underlying physical symmetries. However, it is not guaranteed that NNs will
obey the underlying symmetry unless embedded in the network structure. In this
work, we explore a special kind of symmetry where functions are invariant with
respect to involutory linear/affine transformations up to parity $p=\pm 1$. We
develop mathematical theorems and propose NN architectures that ensure
invariance and universal approximation properties. Numerical experiments
indicate that the proposed models outperform baseline networks while respecting
the imposed symmetry. An adaption of our technique to convolutional NN
classification tasks for datasets with inherent horizontal/vertical reflection
symmetry has also been proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Anwesh Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1"&gt;Marios Mattheakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1"&gt;Pavlos Protopapas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best-Case Lower Bounds in Online Learning. (arXiv:2106.12688v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12688</id>
        <link href="http://arxiv.org/abs/2106.12688"/>
        <updated>2021-06-25T02:00:46.064Z</updated>
        <summary type="html"><![CDATA[Much of the work in online learning focuses on the study of sublinear upper
bounds on the regret. In this work, we initiate the study of best-case lower
bounds in online convex optimization, wherein we bound the largest improvement
an algorithm can obtain relative to the single best action in hindsight. This
problem is motivated by the goal of better understanding the adaptivity of a
learning algorithm. Another motivation comes from fairness: it is known that
best-case lower bounds are instrumental in obtaining algorithms for
decision-theoretic online learning (DTOL) that satisfy a notion of group
fairness. Our contributions are a general method to provide best-case lower
bounds in Follow The Regularized Leader (FTRL) algorithms with time-varying
regularizers, which we use to show that best-case lower bounds are of the same
order as existing upper regret bounds: this includes situations with a fixed
learning rate, decreasing learning rates, timeless methods, and adaptive
gradient methods. In stark contrast, we show that the linearized version of
FTRL can attain negative linear regret. Finally, in DTOL with two experts and
binary predictions, we fully characterize the best-case sequences, which
provides a finer understanding of the best-case lower bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_C/0/1/0/all/0/1"&gt;Crist&amp;#xf3;bal Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1"&gt;Nishant A. Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortazavi_A/0/1/0/all/0/1"&gt;Ali Mortazavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Splitting EUD graphs into trees: A quick and clatty approach. (arXiv:2106.13155v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13155</id>
        <link href="http://arxiv.org/abs/2106.13155"/>
        <updated>2021-06-25T02:00:46.058Z</updated>
        <summary type="html"><![CDATA[We present the system submission from the FASTPARSE team for the EUD Shared
Task at IWPT 2021. We engaged in the task last year by focusing on efficiency.
This year we have focused on experimenting with new ideas on a limited time
budget. Our system is based on splitting the EUD graph into several trees,
based on linguistic criteria. We predict these trees using a sequence-labelling
parser and combine them into an EUD graph. The results were relatively poor,
although not a total disaster and could probably be improved with some
polishing of the system's rough edges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1"&gt;Mark Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_C/0/1/0/all/0/1"&gt;Carlos G&amp;#xf3;mez Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long short-term relevance learning. (arXiv:2106.12694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12694</id>
        <link href="http://arxiv.org/abs/2106.12694"/>
        <updated>2021-06-25T02:00:46.048Z</updated>
        <summary type="html"><![CDATA[To incorporate prior knowledge as well as measurement uncertainties in the
traditional long short term memory (LSTM) neural networks, an efficient sparse
Bayesian training algorithm is introduced to the network architecture. The
proposed scheme automatically determines relevant neural connections and adapts
accordingly, in contrast to the classical LSTM solution. Due to its
flexibility, the new LSTM scheme is less prone to overfitting, and hence can
approximate time dependent solutions by use of a smaller data set. On a
structural nonlinear finite element application we show that the
self-regulating framework does not require prior knowledge of a suitable
network architecture and size, while ensuring satisfying accuracy at reasonable
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weg_B/0/1/0/all/0/1"&gt;Bram van de Weg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greve_L/0/1/0/all/0/1"&gt;Lars Greve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosic_B/0/1/0/all/0/1"&gt;Bojana Rosic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning structure preserving brackets for forecasting irreversible processes. (arXiv:2106.12619v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12619</id>
        <link href="http://arxiv.org/abs/2106.12619"/>
        <updated>2021-06-25T02:00:46.032Z</updated>
        <summary type="html"><![CDATA[Forecasting of time-series data requires imposition of inductive biases to
obtain predictive extrapolation, and recent works have imposed
Hamiltonian/Lagrangian form to preserve structure for systems with reversible
dynamics. In this work we present a novel parameterization of dissipative
brackets from metriplectic dynamical systems appropriate for learning
irreversible dynamics with unknown a priori model form. The process learns
generalized Casimirs for energy and entropy guaranteed to be conserved and
nondecreasing, respectively. Furthermore, for the case of added thermal noise,
we guarantee exact preservation of a fluctuation-dissipation theorem, ensuring
thermodynamic consistency. We provide benchmarks for dissipative systems
demonstrating learned dynamics are more robust and generalize better than
either "black-box" or penalty-based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kookjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Trask_N/0/1/0/all/0/1"&gt;Nathaniel A. Trask&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Stinis_P/0/1/0/all/0/1"&gt;Panos Stinis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Semi-supervised Image Segmentation with Global and Local Mutual Information Regularization. (arXiv:2103.04813v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04813</id>
        <link href="http://arxiv.org/abs/2103.04813"/>
        <updated>2021-06-25T02:00:46.008Z</updated>
        <summary type="html"><![CDATA[The scarcity of labeled data often impedes the application of deep learning
to the segmentation of medical images. Semi-supervised learning seeks to
overcome this limitation by exploiting unlabeled examples in the learning
process. In this paper, we present a novel semi-supervised segmentation method
that leverages mutual information (MI) on categorical distributions to achieve
both global representation invariance and local smoothness. In this method, we
maximize the MI for intermediate feature embeddings that are taken from both
the encoder and decoder of a segmentation network. We first propose a global MI
loss constraining the encoder to learn an image representation that is
invariant to geometric transformations. Instead of resorting to
computationally-expensive techniques for estimating the MI on continuous
feature embeddings, we use projection heads to map them to a discrete cluster
assignment where MI can be computed efficiently. Our method also includes a
local MI loss to promote spatial consistency in the feature maps of the decoder
and provide a smoother segmentation. Since mutual information does not require
a strict ordering of clusters in two different assignments, we incorporate a
final consistency regularization loss on the output which helps align the
cluster labels throughout the network. We evaluate the method on four
challenging publicly-available datasets for medical image segmentation.
Experimental results show our method to outperform recently-proposed approaches
for semi-supervised segmentation and provide an accuracy near to full
supervision while training with very few annotated images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jizong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1"&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1"&gt;Christian Desrosiers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness via Representation Neutralization. (arXiv:2106.12674v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12674</id>
        <link href="http://arxiv.org/abs/2106.12674"/>
        <updated>2021-06-25T02:00:45.997Z</updated>
        <summary type="html"><![CDATA[Existing bias mitigation methods for DNN models primarily work on learning
debiased encoders. This process not only requires a lot of instance-level
annotations for sensitive attributes, it also does not guarantee that all
fairness sensitive information has been removed from the encoder. To address
these limitations, we explore the following research question: Can we reduce
the discrimination of DNN models by only debiasing the classification head,
even with biased representations as inputs? To this end, we propose a new
mitigation technique, namely, Representation Neutralization for Fairness (RNF)
that achieves fairness by debiasing only the task-specific classification head
of DNN models. To this end, we leverage samples with the same ground-truth
label but different sensitive attributes, and use their neutralized
representations to train the classification head of the DNN model. The key idea
of RNF is to discourage the classification head from capturing spurious
correlation between fairness sensitive information in encoder representations
with specific class labels. To address low-resource settings with no access to
sensitive attribute annotations, we leverage a bias-amplified model to generate
proxy annotations for sensitive attributes. Experimental results over several
benchmark datasets demonstrate our RNF framework to effectively reduce
discrimination of DNN models with minimal degradation in task-specific
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1"&gt;Mengnan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhabrata Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanchu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruixiang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Inducing Point Gaussian Process for Inter-domain Observations. (arXiv:2103.00393v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00393</id>
        <link href="http://arxiv.org/abs/2103.00393"/>
        <updated>2021-06-25T02:00:45.992Z</updated>
        <summary type="html"><![CDATA[We examine the general problem of inter-domain Gaussian Processes (GPs):
problems where the GP realization and the noisy observations of that
realization lie on different domains. When the mapping between those domains is
linear, such as integration or differentiation, inference is still closed form.
However, many of the scaling and approximation techniques that our community
has developed do not apply to this setting. In this work, we introduce the
hierarchical inducing point GP (HIP-GP), a scalable inter-domain GP inference
method that enables us to improve the approximation accuracy by increasing the
number of inducing points to the millions. HIP-GP, which relies on inducing
points with grid structure and a stationary kernel assumption, is suitable for
low-dimensional problems. In developing HIP-GP, we introduce (1) a fast
whitening strategy, and (2) a novel preconditioner for conjugate gradients
which can be helpful in general GP settings. Our code is available at https:
//github.com/cunningham-lab/hipgp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Luhuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1"&gt;Andrew Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_L/0/1/0/all/0/1"&gt;Lauren Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1"&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1"&gt;David Blei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1"&gt;John Cunningham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09696</id>
        <link href="http://arxiv.org/abs/2106.09696"/>
        <updated>2021-06-25T02:00:45.986Z</updated>
        <summary type="html"><![CDATA[Understanding the semantics of human movement -- the what, how and why of the
movement -- is an important problem that requires datasets of human actions
with semantic labels. Existing datasets take one of two approaches. Large-scale
video datasets contain many action labels but do not contain ground-truth 3D
human motion. Alternatively, motion-capture (mocap) datasets have precise body
motions but are limited to a small number of actions. To address this, we
present BABEL, a large dataset with language labels describing the actions
being performed in mocap sequences. BABEL consists of action labels for about
43 hours of mocap sequences from AMASS. Action labels are at two levels of
abstraction -- sequence labels describe the overall action in the sequence, and
frame labels describe all actions in every frame of the sequence. Each frame
label is precisely aligned with the duration of the corresponding action in the
mocap sequence, and multiple actions can overlap. There are over 28k sequence
labels, and 63k frame labels in BABEL, which belong to over 250 unique action
categories. Labels from BABEL can be leveraged for tasks like action
recognition, temporal action localization, motion synthesis, etc. To
demonstrate the value of BABEL as a benchmark, we evaluate the performance of
models on 3D action recognition. We demonstrate that BABEL poses interesting
learning challenges that are applicable to real-world scenarios, and can serve
as a useful benchmark of progress in 3D action recognition. The dataset,
baseline method, and evaluation code is made available, and supported for
academic research purposes at https://babel.is.tue.mpg.de/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1"&gt;Abhinanda R. Punnakkal&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1"&gt;Arjun Chandrasekaran&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1"&gt;Nikos Athanasiou&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1"&gt;Alejandra Quiros-Ramirez&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt; (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAGAN: Text-To-Image Generation with Combined Attention GANs. (arXiv:2104.12663v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12663</id>
        <link href="http://arxiv.org/abs/2104.12663"/>
        <updated>2021-06-25T02:00:45.981Z</updated>
        <summary type="html"><![CDATA[Generating images according to natural language descriptions is a challenging
task. Prior research has mainly focused to enhance the quality of generation by
investigating the use of spatial attention and/or textual attention thereby
neglecting the relationship between channels. In this work, we propose the
Combined Attention Generative Adversarial Network (CAGAN) to generate
photo-realistic images according to textual descriptions. The proposed CAGAN
utilises two attention models: word attention to draw different sub-regions
conditioned on related words; and squeeze-and-excitation attention to capture
non-linear interaction among channels. With spectral normalisation to stabilise
training, our proposed CAGAN improves the state of the art on the IS and FID on
the CUB dataset and the FID on the more challenging COCO dataset. Furthermore,
we demonstrate that judging a model by a single evaluation metric can be
misleading by developing an additional model adding local self-attention which
scores a higher IS, outperforming the state of the art on the CUB dataset, but
generates unrealistic images through feature repetition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schulze_H/0/1/0/all/0/1"&gt;Henning Schulze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1"&gt;Dogucan Yaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1"&gt;Alexander Waibel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster Policy Learning with Continuous-Time Gradients. (arXiv:2012.06684v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06684</id>
        <link href="http://arxiv.org/abs/2012.06684"/>
        <updated>2021-06-25T02:00:45.965Z</updated>
        <summary type="html"><![CDATA[We study the estimation of policy gradients for continuous-time systems with
known dynamics. By reframing policy learning in continuous-time, we show that
it is possible construct a more efficient and accurate gradient estimator. The
standard back-propagation through time estimator (BPTT) computes exact
gradients for a crude discretization of the continuous-time system. In
contrast, we approximate continuous-time gradients in the original system. With
the explicit goal of estimating continuous-time gradients, we are able to
discretize adaptively and construct a more efficient policy gradient estimator
which we call the Continuous-Time Policy Gradient (CTPG). We show that
replacing BPTT policy gradients with more efficient CTPG estimates results in
faster and more robust learning in a variety of control tasks and simulators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ainsworth_S/0/1/0/all/0/1"&gt;Samuel Ainsworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lowrey_K/0/1/0/all/0/1"&gt;Kendall Lowrey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1"&gt;John Thickstun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harchaoui_Z/0/1/0/all/0/1"&gt;Zaid Harchaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1"&gt;Siddhartha Srinivasa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reimagining GNN Explanations with ideas from Tabular Data. (arXiv:2106.12665v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12665</id>
        <link href="http://arxiv.org/abs/2106.12665"/>
        <updated>2021-06-25T02:00:45.959Z</updated>
        <summary type="html"><![CDATA[Explainability techniques for Graph Neural Networks still have a long way to
go compared to explanations available for both neural and decision decision
tree-based models trained on tabular data. Using a task that straddles both
graphs and tabular data, namely Entity Matching, we comment on key aspects of
explainability that are missing in GNN model explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Anjali Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1"&gt;Shamanth R Nayak K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1"&gt;Balaji Ganesan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Stereotyping Problem in Collaboratively Filtered Recommender Systems. (arXiv:2106.12622v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12622</id>
        <link href="http://arxiv.org/abs/2106.12622"/>
        <updated>2021-06-25T02:00:45.954Z</updated>
        <summary type="html"><![CDATA[Recommender systems -- and especially matrix factorization-based
collaborative filtering algorithms -- play a crucial role in mediating our
access to online information. We show that such algorithms induce a particular
kind of stereotyping: if preferences for a \textit{set} of items are
anti-correlated in the general user population, then those items may not be
recommended together to a user, regardless of that user's preferences and
ratings history. First, we introduce a notion of \textit{joint accessibility},
which measures the extent to which a set of items can jointly be accessed by
users. We then study joint accessibility under the standard factorization-based
collaborative filtering framework, and provide theoretical necessary and
sufficient conditions when joint accessibility is violated. Moreover, we show
that these conditions can easily be violated when the users are represented by
a single feature vector. To improve joint accessibility, we further propose an
alternative modelling fix, which is designed to capture the diverse multiple
interests of each user using a multi-vector representation. We conduct
extensive experiments on real and simulated datasets, demonstrating the
stereotyping problem with standard single-vector matrix factorization models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenshuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1"&gt;Karl Krauth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1"&gt;Nikhil Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UXLA: A Robust Unsupervised Data Augmentation Framework for {Zero-Resource} Cross-Lingual NLP. (arXiv:2004.13240v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13240</id>
        <link href="http://arxiv.org/abs/2004.13240"/>
        <updated>2021-06-25T02:00:45.948Z</updated>
        <summary type="html"><![CDATA[Transfer learning has yielded state-of-the-art (SoTA) results in many
supervised NLP tasks. However, annotated data for every target task in every
target language is rare, especially for low-resource languages. We propose
UXLA, a novel unsupervised data augmentation framework for zero-resource
transfer learning scenarios. In particular, UXLA aims to solve cross-lingual
adaptation problems from a source language task distribution to an unknown
target language task distribution, assuming no training label in the target
language. At its core, UXLA performs simultaneous self-training with data
augmentation and unsupervised sample selection. To show its effectiveness, we
conduct extensive experiments on three diverse zero-resource cross-lingual
transfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the
baselines by a good margin. With an in-depth framework dissection, we
demonstrate the cumulative contributions of different components to its
success.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1"&gt;M Saiful Bari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1"&gt;Tasnim Mohiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handwritten Digit Recognition using Machine and Deep Learning Algorithms. (arXiv:2106.12614v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12614</id>
        <link href="http://arxiv.org/abs/2106.12614"/>
        <updated>2021-06-25T02:00:45.942Z</updated>
        <summary type="html"><![CDATA[The reliance of humans over machines has never been so high such that from
object classification in photographs to adding sound to silent movies
everything can be performed with the help of deep learning and machine learning
algorithms. Likewise, Handwritten text recognition is one of the significant
areas of research and development with a streaming number of possibilities that
could be attained. Handwriting recognition (HWR), also known as Handwritten
Text Recognition (HTR), is the ability of a computer to receive and interpret
intelligible handwritten input from sources such as paper documents,
photographs, touch-screens and other devices [1]. Apparently, in this paper, we
have performed handwritten digit recognition with the help of MNIST datasets
using Support Vector Machines (SVM), Multi-Layer Perceptron (MLP) and
Convolution Neural Network (CNN) models. Our main objective is to compare the
accuracy of the models stated above along with their execution time to get the
best possible model for digit recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1"&gt;Samay Pashine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixit_R/0/1/0/all/0/1"&gt;Ritik Dixit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kushwah_R/0/1/0/all/0/1"&gt;Rishika Kushwah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Verification of Deep Neural Networks under Domain or Weight Shift. (arXiv:2106.12732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12732</id>
        <link href="http://arxiv.org/abs/2106.12732"/>
        <updated>2021-06-25T02:00:45.926Z</updated>
        <summary type="html"><![CDATA[Although neural networks are widely used, it remains challenging to formally
verify the safety and robustness of neural networks in real-world applications.
Existing methods are designed to verify the network before use, which is
limited to relatively simple specifications and fixed networks. These methods
are not ready to be applied to real-world problems with complex and/or
dynamically changing specifications and networks. To effectively handle
dynamically changing specifications and networks, the verification needs to be
performed online when these changes take place. However, it is still
challenging to run existing verification algorithms online. Our key insight is
that we can leverage the temporal dependencies of these changes to accelerate
the verification process, e.g., by warm starting new online verification using
previous verified results. This paper establishes a novel framework for
scalable online verification to solve real-world verification problems with
dynamically changing specifications and/or networks, known as domain shift and
weight shift respectively. We propose three types of techniques (branch
management, perturbation tolerance analysis, and incremental computation) to
accelerate the online verification of deep neural networks. Experiment results
show that our online verification algorithm is up to two orders of magnitude
faster than existing verification algorithms, and thus can scale to real-world
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tianhao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Changliu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Flows: Pruning Continuous-depth Models. (arXiv:2106.12718v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12718</id>
        <link href="http://arxiv.org/abs/2106.12718"/>
        <updated>2021-06-25T02:00:45.921Z</updated>
        <summary type="html"><![CDATA[Continuous deep learning architectures enable learning of flexible
probabilistic models for predictive modeling as neural ordinary differential
equations (ODEs), and for generative modeling as continuous normalizing flows.
In this work, we design a framework to decipher the internal dynamics of these
continuous depth models by pruning their network architectures. Our empirical
results suggest that pruning improves generalization for neural ODEs in
generative modeling. Moreover, pruning finds minimal and efficient neural ODE
representations with up to 98\% less parameters compared to the original
network, without loss of accuracy. Finally, we show that by applying pruning we
can obtain insightful information about the design of better neural ODEs.We
hope our results will invigorate further research into the performance-size
trade-offs of modern continuous-depth models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1"&gt;Lucas Liebenwein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1"&gt;Ramin Hasani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1"&gt;Alexander Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time gravitational-wave science with neural posterior estimation. (arXiv:2106.12594v1 [gr-qc])]]></title>
        <id>http://arxiv.org/abs/2106.12594</id>
        <link href="http://arxiv.org/abs/2106.12594"/>
        <updated>2021-06-25T02:00:45.915Z</updated>
        <summary type="html"><![CDATA[We demonstrate unprecedented accuracy for rapid gravitational-wave parameter
estimation with deep learning. Using neural networks as surrogates for Bayesian
posterior distributions, we analyze eight gravitational-wave events from the
first LIGO-Virgo Gravitational-Wave Transient Catalog and find very close
quantitative agreement with standard inference codes, but with inference times
reduced from O(day) to a minute per event. Our networks are trained using
simulated data, including an estimate of the detector-noise characteristics
near the event. This encodes the signal and noise models within millions of
neural-network parameters, and enables inference for any observed data
consistent with the training distribution, accounting for noise nonstationarity
from event to event. Our algorithm -- called "DINGO" -- sets a new standard in
fast-and-accurate inference of physical parameters of detected
gravitational-wave events, which should enable real-time data analysis without
sacrificing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/gr-qc/1/au:+Dax_M/0/1/0/all/0/1"&gt;Maximilian Dax&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Green_S/0/1/0/all/0/1"&gt;Stephen R. Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Gair_J/0/1/0/all/0/1"&gt;Jonathan Gair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Macke_J/0/1/0/all/0/1"&gt;Jakob H. Macke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Buonanno_A/0/1/0/all/0/1"&gt;Alessandra Buonanno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Study of Robust Adaptive Beamforming Based on Low-Complexity DFT Spatial Sampling. (arXiv:2106.12663v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.12663</id>
        <link href="http://arxiv.org/abs/2106.12663"/>
        <updated>2021-06-25T02:00:45.909Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel and robust algorithm is proposed for adaptive
beamforming based on the idea of reconstructing the autocorrelation sequence
(ACS) of a random process from a set of measured data. This is obtained from
the first column and the first row of the sample covariance matrix (SCM) after
averaging along its diagonals. Then, the power spectrum of the correlation
sequence is estimated using the discrete Fourier transform (DFT). The DFT
coefficients corresponding to the angles within the noise-plus-interference
region are used to reconstruct the noise-plus-interference covariance matrix
(NPICM), while the desired signal covariance matrix (DSCM) is estimated by
identifying and removing the noise-plus-interference component from the SCM. In
particular, the spatial power spectrum of the estimated received signal is
utilized to compute the correlation sequence corresponding to the
noise-plus-interference in which the dominant DFT coefficient of the
noise-plus-interference is captured. A key advantage of the proposed adaptive
beamforming is that only little prior information is required. Specifically, an
imprecise knowledge of the array geometry and of the angular sectors in which
the interferences are located is needed. Simulation results demonstrate that
compared with previous reconstruction-based beamformers, the proposed approach
can achieve better overall performance in the case of multiple mismatches over
a very large range of input signal-to-noise ratios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadzadeh_S/0/1/0/all/0/1"&gt;Saeed Mohammadzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nascimento_V/0/1/0/all/0/1"&gt;Vitor H.Nascimento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamare_R/0/1/0/all/0/1"&gt;Rodrigo C. de Lamare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kukrer_O/0/1/0/all/0/1"&gt;Osman Kukrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably efficient machine learning for quantum many-body problems. (arXiv:2106.12627v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12627</id>
        <link href="http://arxiv.org/abs/2106.12627"/>
        <updated>2021-06-25T02:00:45.903Z</updated>
        <summary type="html"><![CDATA[Classical machine learning (ML) provides a potentially powerful approach to
solving challenging quantum many-body problems in physics and chemistry.
However, the advantages of ML over more traditional methods have not been
firmly established. In this work, we prove that classical ML algorithms can
efficiently predict ground state properties of gapped Hamiltonians in finite
spatial dimensions, after learning from data obtained by measuring other
Hamiltonians in the same quantum phase of matter. In contrast, under widely
accepted complexity theory assumptions, classical algorithms that do not learn
from data cannot achieve the same guarantee. We also prove that classical ML
algorithms can efficiently classify a wide range of quantum phases of matter.
Our arguments are based on the concept of a classical shadow, a succinct
classical description of a many-body quantum state that can be constructed in
feasible quantum experiments and be used to predict many properties of the
state. Extensive numerical experiments corroborate our theoretical results in a
variety of scenarios, including Rydberg atom systems, 2D random Heisenberg
models, symmetry-protected topological phases, and topologically ordered
phases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hsin-Yuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kueng_R/0/1/0/all/0/1"&gt;Richard Kueng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Torlai_G/0/1/0/all/0/1"&gt;Giacomo Torlai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Albert_V/0/1/0/all/0/1"&gt;Victor V. Albert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Preskill_J/0/1/0/all/0/1"&gt;John Preskill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-Sample Analysis of Off-Policy TD-Learning via Generalized Bellman Operators. (arXiv:2106.12729v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12729</id>
        <link href="http://arxiv.org/abs/2106.12729"/>
        <updated>2021-06-25T02:00:45.888Z</updated>
        <summary type="html"><![CDATA[In temporal difference (TD) learning, off-policy sampling is known to be more
practical than on-policy sampling, and by decoupling learning from data
collection, it enables data reuse. It is known that policy evaluation
(including multi-step off-policy importance sampling) has the interpretation of
solving a generalized Bellman equation. In this paper, we derive finite-sample
bounds for any general off-policy TD-like stochastic approximation algorithm
that solves for the fixed-point of this generalized Bellman operator. Our key
step is to show that the generalized Bellman operator is simultaneously a
contraction mapping with respect to a weighted $\ell_p$-norm for each $p$ in
$[1,\infty)$, with a common contraction factor.

Off-policy TD-learning is known to suffer from high variance due to the
product of importance sampling ratios. A number of algorithms (e.g.
$Q^\pi(\lambda)$, Tree-Backup$(\lambda)$, Retrace$(\lambda)$, and $Q$-trace)
have been proposed in the literature to address this issue. Our results
immediately imply finite-sample bounds of these algorithms. In particular, we
provide first-known finite-sample guarantees for $Q^\pi(\lambda)$,
Tree-Backup$(\lambda)$, and Retrace$(\lambda)$, and improve the best known
bounds of $Q$-trace in [19]. Moreover, we show the bias-variance trade-offs in
each of these algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zaiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1"&gt;Siva Theja Maguluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Fake Detection: Survey of Facial Manipulation Detection Solutions. (arXiv:2106.12605v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12605</id>
        <link href="http://arxiv.org/abs/2106.12605"/>
        <updated>2021-06-25T02:00:45.883Z</updated>
        <summary type="html"><![CDATA[Deep Learning as a field has been successfully used to solve a plethora of
complex problems, the likes of which we could not have imagined a few decades
back. But as many benefits as it brings, there are still ways in which it can
be used to bring harm to our society. Deep fakes have been proven to be one
such problem, and now more than ever, when any individual can create a fake
image or video simply using an application on the smartphone, there need to be
some countermeasures, with which we can detect if the image or video is a fake
or real and dispose of the problem threatening the trustworthiness of online
information. Although the Deep fakes created by neural networks, may seem to be
as real as a real image or video, it still leaves behind spatial and temporal
traces or signatures after moderation, these signatures while being invisible
to a human eye can be detected with the help of a neural network trained to
specialize in Deep fake detection. In this paper, we analyze several such
states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception
Net) and compare them against each other, to find an optimal solution for
various scenarios like real-time deep fake detection to be deployed in online
social media platforms where the classification should be made as fast as
possible or for a small news agency where the classification need not be in
real-time but requires utmost accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1"&gt;Samay Pashine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandiya_S/0/1/0/all/0/1"&gt;Sagar Mandiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Praveen Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1"&gt;Rashid Sheikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EfficientNetV2: Smaller Models and Faster Training. (arXiv:2104.00298v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00298</id>
        <link href="http://arxiv.org/abs/2104.00298"/>
        <updated>2021-06-25T02:00:45.853Z</updated>
        <summary type="html"><![CDATA[This paper introduces EfficientNetV2, a new family of convolutional networks
that have faster training speed and better parameter efficiency than previous
models. To develop this family of models, we use a combination of
training-aware neural architecture search and scaling, to jointly optimize
training speed and parameter efficiency. The models were searched from the
search space enriched with new ops such as Fused-MBConv. Our experiments show
that EfficientNetV2 models train much faster than state-of-the-art models while
being up to 6.8x smaller.

Our training can be further sped up by progressively increasing the image
size during training, but it often causes a drop in accuracy. To compensate for
this accuracy drop, we propose to adaptively adjust regularization (e.g.,
dropout and data augmentation) as well, such that we can achieve both fast
training and good accuracy.

With progressive learning, our EfficientNetV2 significantly outperforms
previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on
the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on
ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while
training 5x-11x faster using the same computing resources. Code will be
available at https://github.com/google/automl/tree/master/efficientnetv2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingxing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All unconstrained strongly convex problems are weakly simplicial. (arXiv:2106.12704v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.12704</id>
        <link href="http://arxiv.org/abs/2106.12704"/>
        <updated>2021-06-25T02:00:45.847Z</updated>
        <summary type="html"><![CDATA[A multi-objective optimization problem is $C^r$ weakly simplicial if there
exists a $C^r$ surjection from a simplex onto the Pareto set/front such that
the image of each subsimplex is the Pareto set/front of a subproblem, where
$0\leq r\leq \infty$. This property is helpful to compute a parametric-surface
approximation of the entire Pareto set and Pareto front. It is known that all
unconstrained strongly convex $C^r$ problems are $C^{r-1}$ weakly simplicial
for $1\leq r \leq \infty$. In this paper, we show that all unconstrained
strongly convex problems are $C^0$ weakly simplicial. The usefulness of this
theorem is demonstrated in a sparse modeling application: we reformulate the
elastic net as a non-differentiable multi-objective strongly convex problem and
approximate its Pareto set (the set of all trained models with different
hyper-parameters) and Pareto front (the set of performance metrics of the
trained models) by using a B\'ezier simplex fitting method, which accelerates
hyper-parameter search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Mizota_Y/0/1/0/all/0/1"&gt;Yusuke Mizota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hamada_N/0/1/0/all/0/1"&gt;Naoki Hamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ichiki_S/0/1/0/all/0/1"&gt;Shunsuke Ichiki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dungeon and Platformer Level Blending and Generation using Conditional VAEs. (arXiv:2106.12692v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12692</id>
        <link href="http://arxiv.org/abs/2106.12692"/>
        <updated>2021-06-25T02:00:45.831Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been used in prior works for generating
and blending levels from different games. To add controllability to these
models, conditional VAEs (CVAEs) were recently shown capable of generating
output that can be modified using labels specifying desired content, albeit
working with segments of levels and platformers exclusively. We expand these
works by using CVAEs for generating whole platformer and dungeon levels, and
blending levels across these genres. We show that CVAEs can reliably control
door placement in dungeons and progression direction in platformer levels.
Thus, by using appropriate labels, our approach can generate whole dungeons and
platformer levels of interconnected rooms and segments respectively as well as
levels that blend dungeons and platformers. We demonstrate our approach using
The Legend of Zelda, Metroid, Mega Man and Lode Runner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Anurag Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1"&gt;Seth Cooper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling the Knowledge from Normalizing Flows. (arXiv:2106.12699v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12699</id>
        <link href="http://arxiv.org/abs/2106.12699"/>
        <updated>2021-06-25T02:00:45.816Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are a powerful class of generative models demonstrating
strong performance in several speech and vision problems. In contrast to other
generative models, normalizing flows have tractable likelihoods and allow for
stable training. However, they have to be carefully designed to represent
invertible functions with efficient Jacobian determinant calculation. In
practice, these requirements lead to overparameterized and sophisticated
architectures that are inferior to alternative feed-forward models in terms of
inference time and memory consumption. In this work, we investigate whether one
can distill knowledge from flow-based models to more efficient alternatives. We
provide a positive answer to this question by proposing a simple distillation
approach and demonstrating its effectiveness on state-of-the-art conditional
flow-based models for image super-resolution and speech synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1"&gt;Dmitry Baranchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aliev_V/0/1/0/all/0/1"&gt;Vladimir Aliev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1"&gt;Artem Babenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Semantic Segmentation via Spatial-detail Guided Context Propagation. (arXiv:2005.11034v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.11034</id>
        <link href="http://arxiv.org/abs/2005.11034"/>
        <updated>2021-06-25T02:00:45.810Z</updated>
        <summary type="html"><![CDATA[Nowadays, vision-based computing tasks play an important role in various
real-world applications. However, many vision computing tasks, e.g. semantic
segmentation, are usually computationally expensive, posing a challenge to the
computing systems that are resource-constrained but require fast response
speed. Therefore, it is valuable to develop accurate and real-time vision
processing models that only require limited computational resources. To this
end, we propose the Spatial-detail Guided Context Propagation Network (SGCPNet)
for achieving real-time semantic segmentation. In SGCPNet, we propose the
strategy of spatial-detail guided context propagation. It uses the spatial
details of shallow layers to guide the propagation of the low-resolution global
contexts, in which the lost spatial information can be effectively
reconstructed. In this way, the need for maintaining high-resolution features
along the network is freed, therefore largely improving the model efficiency.
On the other hand, due to the effective reconstruction of spatial details, the
segmentation accuracy can be still preserved. In the experiments, we validate
the effectiveness and efficiency of the proposed SGCPNet model. On the
Citysacpes dataset, for example, our SGCPNet achieves 69.5 % mIoU segmentation
accuracy, while its speed reaches 178.5 FPS on 768x1536 images on a GeForce GTX
1080 Ti GPU card.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1"&gt;Shijie Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yanrong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1"&gt;Richang Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graceful Degradation and Related Fields. (arXiv:2106.11119v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11119</id>
        <link href="http://arxiv.org/abs/2106.11119"/>
        <updated>2021-06-25T02:00:45.805Z</updated>
        <summary type="html"><![CDATA[When machine learning models encounter data which is out of the distribution
on which they were trained they have a tendency to behave poorly, most
prominently over-confidence in erroneous predictions. Such behaviours will have
disastrous effects on real-world machine learning systems. In this field
graceful degradation refers to the optimisation of model performance as it
encounters this out-of-distribution data. This work presents a definition and
discussion of graceful degradation and where it can be applied in deployed
visual systems. Following this a survey of relevant areas is undertaken,
novelly splitting the graceful degradation problem into active and passive
approaches. In passive approaches, graceful degradation is handled and achieved
by the model in a self-contained manner, in active approaches the model is
updated upon encountering epistemic uncertainties. This work communicates the
importance of the problem and aims to prompt the development of machine
learning strategies that are aware of graceful degradation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dymond_J/0/1/0/all/0/1"&gt;Jack Dymond&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme Multi-label Learning for Semantic Matching in Product Search. (arXiv:2106.12657v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12657</id>
        <link href="http://arxiv.org/abs/2106.12657"/>
        <updated>2021-06-25T02:00:45.800Z</updated>
        <summary type="html"><![CDATA[We consider the problem of semantic matching in product search: given a
customer query, retrieve all semantically related products from a huge catalog
of size 100 million, or more. Because of large catalog spaces and real-time
latency constraints, semantic matching algorithms not only desire high recall
but also need to have low latency. Conventional lexical matching approaches
(e.g., Okapi-BM25) exploit inverted indices to achieve fast inference time, but
fail to capture behavioral signals between queries and products. In contrast,
embedding-based models learn semantic representations from customer behavior
data, but the performance is often limited by shallow neural encoders due to
latency constraints. Semantic product search can be viewed as an eXtreme
Multi-label Classification (XMC) problem, where customer queries are input
instances and products are output labels. In this paper, we aim to improve
semantic product search by using tree-based XMC models where inference time
complexity is logarithmic in the number of products. We consider hierarchical
linear models with n-gram features for fast real-time inference.
Quantitatively, our method maintains a low latency of 1.25 milliseconds per
query and achieves a 65% improvement of Recall@100 (60.9% v.s. 36.8%) over a
competing embedding-based DSSM model. Our model is robust to weight pruning
with varying thresholds, which can flexibly meet different system requirements
for online deployments. Qualitatively, our method can retrieve products that
are complementary to existing product search system and add diversity to the
match set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1"&gt;Wei-Cheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daniel Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1"&gt;Choon-Hui Teo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kai Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolluri_K/0/1/0/all/0/1"&gt;Kedarnath Kolluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shandilya_N/0/1/0/all/0/1"&gt;Nikhil Shandilya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ievgrafov_V/0/1/0/all/0/1"&gt;Vyacheslav Ievgrafov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Japinder Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos. (arXiv:2105.11731v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11731</id>
        <link href="http://arxiv.org/abs/2105.11731"/>
        <updated>2021-06-25T02:00:45.784Z</updated>
        <summary type="html"><![CDATA[Detecting human-object interactions (HOI) is an important step toward a
comprehensive visual understanding of machines. While detecting non-temporal
HOIs (e.g., sitting on a chair) from static images is feasible, it is unlikely
even for humans to guess temporal-related HOIs (e.g., opening/closing a door)
from a single video frame, where the neighboring frames play an essential role.
However, conventional HOI methods operating on only static images have been
used to predict temporal-related interactions, which is essentially guessing
without temporal contexts and may lead to sub-optimal performance. In this
paper, we bridge this gap by detecting video-based HOIs with explicit temporal
information. We first show that a naive temporal-aware variant of a common
action detection baseline does not work on video-based HOIs due to a
feature-inconsistency issue. We then propose a simple yet effective
architecture named Spatial-Temporal HOI Detection (ST-HOI) utilizing temporal
information such as human and object trajectories, correctly-localized visual
features, and spatial-temporal masking pose features. We construct a new video
HOI benchmark dubbed VidHOI where our proposed approach serves as a solid
baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiou_M/0/1/0/all/0/1"&gt;Meng-Jiun Chiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1"&gt;Chun-Yu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li-Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roger Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based unsupervised patient representation learning based on medical claims for risk stratification and analysis. (arXiv:2106.12658v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12658</id>
        <link href="http://arxiv.org/abs/2106.12658"/>
        <updated>2021-06-25T02:00:45.777Z</updated>
        <summary type="html"><![CDATA[The claims data, containing medical codes, services information, and incurred
expenditure, can be a good resource for estimating an individual's health
condition and medical risk level. In this study, we developed Transformer-based
Multimodal AutoEncoder (TMAE), an unsupervised learning framework that can
learn efficient patient representation by encoding meaningful information from
the claims data. TMAE is motivated by the practical needs in healthcare to
stratify patients into different risk levels for improving care delivery and
management. Compared to previous approaches, TMAE is able to 1) model
inpatient, outpatient, and medication claims collectively, 2) handle irregular
time intervals between medical events, 3) alleviate the sparsity issue of the
rare medical codes, and 4) incorporate medical expenditure information. We
trained TMAE using a real-world pediatric claims dataset containing more than
600,000 patients and compared its performance with various approaches in two
clustering tasks. Experimental results demonstrate that TMAE has superior
performance compared to all baselines. Multiple downstream applications are
also conducted to illustrate the effectiveness of our framework. The promising
results confirm that the TMAE framework is scalable to large claims data and is
able to generate efficient patient embeddings for risk stratification and
analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xianlong Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Simon Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum sharpness: Scale-invariant parameter-robustness of neural networks. (arXiv:2106.12612v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12612</id>
        <link href="http://arxiv.org/abs/2106.12612"/>
        <updated>2021-06-25T02:00:45.761Z</updated>
        <summary type="html"><![CDATA[Toward achieving robust and defensive neural networks, the robustness against
the weight parameters perturbations, i.e., sharpness, attracts attention in
recent years (Sun et al., 2020). However, sharpness is known to remain a
critical issue, "scale-sensitivity." In this paper, we propose a novel
sharpness measure, Minimum Sharpness. It is known that NNs have a specific
scale transformation that constitutes equivalent classes where functional
properties are completely identical, and at the same time, their sharpness
could change unlimitedly. We define our sharpness through a minimization
problem over the equivalent NNs being invariant to the scale transformation. We
also develop an efficient and exact technique to make the sharpness tractable,
which reduces the heavy computational costs involved with Hessian. In the
experiment, we observed that our sharpness has a valid correlation with the
generalization of NNs and runs with less computational cost than existing
sharpness measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibayashi_H/0/1/0/all/0/1"&gt;Hikaru Ibayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamaguchi_T/0/1/0/all/0/1"&gt;Takuo Hamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imaizum_M/0/1/0/all/0/1"&gt;Masaaki Imaizum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12672</id>
        <link href="http://arxiv.org/abs/2106.12672"/>
        <updated>2021-06-25T02:00:45.755Z</updated>
        <summary type="html"><![CDATA[State-of-the-art models in natural language processing rely on separate rigid
subword tokenization algorithms, which limit their generalization ability and
adaptation to new settings. In this paper, we propose a new model inductive
bias that learns a subword tokenization end-to-end as part of the model. To
this end, we introduce a soft gradient-based subword tokenization module (GBST)
that automatically learns latent subword representations from characters in a
data-driven fashion. Concretely, GBST enumerates candidate subword blocks and
learns to score them in a position-wise fashion using a block scoring network.
We additionally introduce Charformer, a deep Transformer model that integrates
GBST and operates on the byte level. Via extensive experiments on English GLUE,
multilingual, and noisy text datasets, we show that Charformer outperforms a
series of competitive byte-level baselines while generally performing on par
and sometimes outperforming subword-based models. Additionally, Charformer is
fast, improving the speed of both vanilla byte-level and subword-level
Transformers by 28%-100% while maintaining competitive quality. We believe this
work paves the way for highly performant token-free models that are trained
completely end-to-end.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vinh Q. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1"&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1"&gt;Jai Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1"&gt;Hyung Won Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhen Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1"&gt;Simon Baumgartner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Examples in Multi-Layer Random ReLU Networks. (arXiv:2106.12611v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12611</id>
        <link href="http://arxiv.org/abs/2106.12611"/>
        <updated>2021-06-25T02:00:45.750Z</updated>
        <summary type="html"><![CDATA[We consider the phenomenon of adversarial examples in ReLU networks with
independent gaussian parameters. For networks of constant depth and with a
large range of widths (for instance, it suffices if the width of each layer is
polynomial in that of any other layer), small perturbations of input vectors
lead to large changes of outputs. This generalizes results of Daniely and
Schacham (2020) for networks of rapidly decreasing width and of Bubeck et al
(2021) for two-layer networks. The proof shows that adversarial examples arise
in these networks because the functions that they compute are very close to
linear. Bottleneck layers in the network play a key role: the minimal width up
to some point in the network determines scales and sensitivities of mappings
computed up to that point. The main result is for networks with constant depth,
but we also show that some constraint on depth is necessary for a result of
this kind, because there are suitably deep networks that, with constant
probability, compute a function that is close to constant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Bubeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cherapanamjeri_Y/0/1/0/all/0/1"&gt;Yeshwanth Cherapanamjeri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FDRN: A Fast Deformable Registration Network for Medical Images. (arXiv:2011.02307v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02307</id>
        <link href="http://arxiv.org/abs/2011.02307"/>
        <updated>2021-06-25T02:00:45.735Z</updated>
        <summary type="html"><![CDATA[Deformable image registration is a fundamental task in medical imaging. Due
to the large computational complexity of deformable registration of volumetric
images, conventional iterative methods usually face the tradeoff between the
registration accuracy and the computation time in practice. In order to boost
the registration performance in both accuracy and runtime, we propose a fast
convolutional neural network. Specially, to efficiently utilize the memory
resources and enlarge the model capacity, we adopt additive forwarding instead
of channel concatenation and deepen the network in each encoder and decoder
stage. To facilitate the learning efficiency, we leverage skip connection
within the encoder and decoder stages to enable residual learning and employ an
auxiliary loss at the bottom layer with lowest resolution to involve deep
supervision. Particularly, the low-resolution auxiliary loss is weighted by an
exponentially decayed parameter during the training phase. In conjunction with
the main loss in high-resolution grid, a coarse-to-fine learning strategy is
achieved. Last but not least, we introduce an auxiliary loss based on the
segmentation prior to improve the registration performance in Dice score.
Comparing to the auxiliary loss using average Dice score, the proposed
multi-label segmentation loss does not induce additional memory cost in the
training phase and can be employed on images with arbitrary amount of
categories. In the experiments, we show FDRN outperforms the existing
state-of-the-art registration methods for brain MR images by resorting to the
compact network structure and efficient learning. Besides, FDRN is a
generalized framework for image registration which is not confined to a
particular type of medical images or anatomy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1"&gt;Kaicong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_S/0/1/0/all/0/1"&gt;Sven Simon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging semantically similar queries for ranking via combining representations. (arXiv:2106.12621v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12621</id>
        <link href="http://arxiv.org/abs/2106.12621"/>
        <updated>2021-06-25T02:00:45.729Z</updated>
        <summary type="html"><![CDATA[In modern ranking problems, different and disparate representations of the
items to be ranked are often available. It is sensible, then, to try to combine
these representations to improve ranking. Indeed, learning to rank via
combining representations is both principled and practical for learning a
ranking function for a particular query. In extremely data-scarce settings,
however, the amount of labeled data available for a particular query can lead
to a highly variable and ineffective ranking function. One way to mitigate the
effect of the small amount of data is to leverage information from semantically
similar queries. Indeed, as we demonstrate in simulation settings and real data
examples, when semantically similar queries are available it is possible to
gainfully use them when ranking with respect to a particular query. We describe
and explore this phenomenon in the context of the bias-variance trade off and
apply it to the data-scarce settings of a Bing navigational graph and the
Drosophila larva connectome.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1"&gt;Hayden S. Helm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdin_M/0/1/0/all/0/1"&gt;Marah Abdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedigo_B/0/1/0/all/0/1"&gt;Benjamin D. Pedigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1"&gt;Shweti Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyzinski_V/0/1/0/all/0/1"&gt;Vince Lyzinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1"&gt;Youngser Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1"&gt;Amitabh Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_P/0/1/0/all/0/1"&gt;Piali~Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Christopher M. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Weiwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Carey E. Priebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three-stream network for enriched Action Recognition. (arXiv:2104.13051v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13051</id>
        <link href="http://arxiv.org/abs/2104.13051"/>
        <updated>2021-06-25T02:00:45.724Z</updated>
        <summary type="html"><![CDATA[Understanding accurate information on human behaviours is one of the most
important tasks in machine intelligence. Human Activity Recognition that aims
to understand human activities from a video is a challenging task due to
various problems including background, camera motion and dataset variations.
This paper proposes two CNN based architectures with three streams which allow
the model to exploit the dataset under different settings. The three pathways
are differentiated in frame rates. The single pathway, operates at a single
frame rate captures spatial information, the slow pathway operates at low frame
rates captures the spatial information and the fast pathway operates at high
frame rates that capture fine temporal information. Post CNN encoders, we add
bidirectional LSTM and attention heads respectively to capture the context and
temporal features. By experimenting with various algorithms on UCF-101,
Kinetics-600 and AVA dataset, we observe that the proposed models achieve
state-of-art performance for human action recognition task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheth_I/0/1/0/all/0/1"&gt;Ivaxi Sheth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding. (arXiv:2106.02795v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02795</id>
        <link href="http://arxiv.org/abs/2106.02795"/>
        <updated>2021-06-25T02:00:45.715Z</updated>
        <summary type="html"><![CDATA[Attentional mechanisms are order-invariant. Positional encoding is a crucial
component to allow attention-based deep model architectures such as Transformer
to address sequences or images where the position of information matters. In
this paper, we propose a novel positional encoding method based on learnable
Fourier features. Instead of hard-coding each position as a token or a vector,
we represent each position, which can be multi-dimensional, as a trainable
encoding based on learnable Fourier feature mapping, modulated with a
multi-layer perceptron. The representation is particularly advantageous for a
spatial multi-dimensional position, e.g., pixel positions on an image, where
$L_2$ distances or more complex positional relationships need to be captured.
Our experiments based on several public benchmark tasks show that our learnable
Fourier feature representation for multi-dimensional positional encoding
outperforms existing methods by both improving the accuracy and allowing faster
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1"&gt;Si Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PocketNet: A Smaller Neural Network for Medical Image Analysis. (arXiv:2104.10745v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10745</id>
        <link href="http://arxiv.org/abs/2104.10745"/>
        <updated>2021-06-25T02:00:45.674Z</updated>
        <summary type="html"><![CDATA[Medical imaging deep learning models are often large and complex, requiring
specialized hardware to train and evaluate these models. To address such
issues, we propose the PocketNet paradigm to reduce the size of deep learning
models by throttling the growth of the number of channels in convolutional
neural networks. We demonstrate that, for a range of segmentation and
classification tasks, PocketNet architectures produce results comparable to
that of conventional neural networks while reducing the number of parameters by
multiple orders of magnitude, using up to 90% less GPU memory, and speeding up
training times by up to 40%, thereby allowing such models to be trained and
deployed in resource-constrained settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1"&gt;Adrian Celaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Actor_J/0/1/0/all/0/1"&gt;Jonas A. Actor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Muthusivarajan_R/0/1/0/all/0/1"&gt;Rajarajeswari Muthusivarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gates_E/0/1/0/all/0/1"&gt;Evan Gates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1"&gt;Caroline Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schellingerhout_D/0/1/0/all/0/1"&gt;Dawid Schellingerhout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1"&gt;Beatrice Riviere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1"&gt;David Fuentes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Network Traffic Classification. (arXiv:2106.12693v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.12693</id>
        <link href="http://arxiv.org/abs/2106.12693"/>
        <updated>2021-06-25T02:00:45.666Z</updated>
        <summary type="html"><![CDATA[Monitoring network traffic to identify content, services, and applications is
an active research topic in network traffic control systems. While modern
firewalls provide the capability to decrypt packets, this is not appealing for
privacy advocates. Hence, identifying any information from encrypted traffic is
a challenging task. Nonetheless, previous work has identified machine learning
methods that may enable application and service identification. The process
involves high level feature extraction from network packet data then training a
robust machine learning classifier for traffic identification. We propose a
classification technique using an ensemble of deep learning architectures on
packet, payload, and inter-arrival time sequences. To our knowledge, this is
the first time such deep learning architectures have been applied to the Server
Name Indication (SNI) classification problem. Our ensemble model beats the
state of the art machine learning methods and our up-to-date model can be found
on github: \url{https://github.com/niloofarbayat/NetworkClassification}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bayat_N/0/1/0/all/0/1"&gt;Niloofar Bayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jackson_W/0/1/0/all/0/1"&gt;Weston Jackson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Derrick Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?. (arXiv:2106.12576v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12576</id>
        <link href="http://arxiv.org/abs/2106.12576"/>
        <updated>2021-06-25T02:00:45.661Z</updated>
        <summary type="html"><![CDATA[Recent advances in differentially private deep learning have demonstrated
that application of differential privacy, specifically the DP-SGD algorithm,
has a disparate impact on different sub-groups in the population, which leads
to a significantly high drop-in model utility for sub-populations that are
under-represented (minorities), compared to well-represented ones. In this
work, we aim to compare PATE, another mechanism for training deep learning
models using differential privacy, with DP-SGD in terms of fairness. We show
that PATE does have a disparate impact too, however, it is much less severe
than DP-SGD. We draw insights from this observation on what might be promising
directions in achieving better fairness-privacy trade-offs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uniyal_A/0/1/0/all/0/1"&gt;Archit Uniyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1"&gt;Rakshit Naidu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotti_S/0/1/0/all/0/1"&gt;Sasikanth Kotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sahib Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1"&gt;Patrik Joslin Kenfack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1"&gt;Fatemehsadat Mireshghallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trask_A/0/1/0/all/0/1"&gt;Andrew Trask&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Swin Transformer. (arXiv:2106.13230v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13230</id>
        <link href="http://arxiv.org/abs/2106.13230"/>
        <updated>2021-06-25T02:00:45.650Z</updated>
        <summary type="html"><![CDATA[The vision community is witnessing a modeling shift from CNNs to
Transformers, where pure Transformer architectures have attained top accuracy
on the major video recognition benchmarks. These video models are all built on
Transformer layers that globally connect patches across the spatial and
temporal dimensions. In this paper, we instead advocate an inductive bias of
locality in video Transformers, which leads to a better speed-accuracy
trade-off compared to previous approaches which compute self-attention globally
even with spatial-temporal factorization. The locality of the proposed video
architecture is realized by adapting the Swin Transformer designed for the
image domain, while continuing to leverage the power of pre-trained image
models. Our approach achieves state-of-the-art accuracy on a broad range of
video recognition benchmarks, including on action recognition (84.9 top-1
accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less
pre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1
accuracy on Something-Something v2). The code and models will be made publicly
available at https://github.com/SwinTransformer/Video-Swin-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ze Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1"&gt;Jia Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yue Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yixuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Stephen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedFace: Collaborative Learning of Face Recognition Model. (arXiv:2104.03008v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03008</id>
        <link href="http://arxiv.org/abs/2104.03008"/>
        <updated>2021-06-25T02:00:45.641Z</updated>
        <summary type="html"><![CDATA[DNN-based face recognition models require large centrally aggregated face
datasets for training. However, due to the growing data privacy concerns and
legal restrictions, accessing and sharing face datasets has become exceedingly
difficult. We propose FedFace, a federated learning (FL) framework for
collaborative learning of face recognition models in a privacy-aware manner.
FedFace utilizes the face images available on multiple clients to learn an
accurate and generalizable face recognition model where the face images stored
at each client are neither shared with other clients nor the central host and
each client is a mobile device containing face images pertaining to only the
owner of the device (one identity per client). Our experiments show the
effectiveness of FedFace in enhancing the verification performance of
pre-trained face recognition system on standard face verification benchmarks
namely LFW, IJB-A, and IJB-C.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_D/0/1/0/all/0/1"&gt;Divyansh Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiayu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Anil K. Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. (arXiv:2106.13228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13228</id>
        <link href="http://arxiv.org/abs/2106.13228"/>
        <updated>2021-06-25T02:00:45.625Z</updated>
        <summary type="html"><![CDATA[Neural Radiance Fields (NeRF) are able to reconstruct scenes with
unprecedented fidelity, and various recent works have extended NeRF to handle
dynamic scenes. A common approach to reconstruct such non-rigid scenes is
through the use of a learned deformation field mapping from coordinates in each
input image into a canonical template coordinate space. However, these
deformation-based approaches struggle to model changes in topology, as
topological changes require a discontinuity in the deformation field, but these
deformation fields are necessarily continuous. We address this limitation by
lifting NeRFs into a higher dimensional space, and by representing the 5D
radiance field corresponding to each individual input image as a slice through
this "hyper-space". Our method is inspired by level set methods, which model
the evolution of surfaces as slices through a higher dimensional surface. We
evaluate our method on two tasks: (i) interpolating smoothly between "moments",
i.e., configurations of the scene, seen in the input images while maintaining
visual plausibility, and (ii) novel-view synthesis at fixed moments. We show
that our method, which we dub HyperNeRF, outperforms existing methods on both
tasks by significant margins. Compared to Nerfies, HyperNeRF reduces average
error rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as
measured by LPIPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Keunhong Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_U/0/1/0/all/0/1"&gt;Utkarsh Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1"&gt;Peter Hedman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1"&gt;Jonathan T. Barron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1"&gt;Sofien Bouaziz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldman_D/0/1/0/all/0/1"&gt;Dan B Goldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1"&gt;Ricardo Martin-Brualla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1"&gt;Steven M. Seitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Network Slimming with Nonconvex Regularization. (arXiv:2010.01242v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01242</id>
        <link href="http://arxiv.org/abs/2010.01242"/>
        <updated>2021-06-25T02:00:45.613Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have developed to become powerful models
for various computer vision tasks ranging from object detection to semantic
segmentation. However, most of state-of-the-art CNNs can not be deployed
directly on edge devices such as smartphones and drones, which need low latency
under limited power and memory bandwidth. One popular, straightforward approach
to compressing CNNs is network slimming, which imposes $\ell_1$ regularization
on the channel-associated scaling factors via the batch normalization layers
during training. Network slimming thereby identifies insignificant channels
that can be pruned for inference. In this paper, we propose replacing the
$\ell_1$ penalty with an alternative sparse, nonconvex penalty in order to
yield a more compressed and/or accurate CNN architecture. We investigate
$\ell_p (0 < p < 1)$, transformed $\ell_1$ (T$\ell_1$), minimax concave penalty
(MCP), and smoothly clipped absolute deviation (SCAD) due to their recent
successes and popularity in solving sparse optimization problems, such as
compressed sensing and variable selection. We demonstrate the effectiveness of
network slimming with nonconvex penalties on VGGNet, Densenet, and Resnet on
standard image classification datasets. Based on the numerical experiments,
T$\ell_1$ preserves model accuracy against channel pruning, $\ell_{1/2, 3/4}$
yield better compressed models with similar accuracies after retraining as
$\ell_1$, and MCP and SCAD provide more accurate models after retraining with
similar compression as $\ell_1$. Network slimming with T$\ell_1$ regularization
also outperforms the latest Bayesian modification of network slimming in
compressing a CNN architecture in terms of memory storage while preserving its
model accuracy after channel pruning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1"&gt;Kevin Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1"&gt;Fredrick Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yingyong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jack Xin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding. (arXiv:2103.16848v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16848</id>
        <link href="http://arxiv.org/abs/2103.16848"/>
        <updated>2021-06-25T02:00:45.599Z</updated>
        <summary type="html"><![CDATA[Temporal grounding aims to localize temporal boundaries within untrimmed
videos by language queries, but it faces the challenge of two types of
inevitable human uncertainties: query uncertainty and label uncertainty. The
two uncertainties stem from human subjectivity, leading to limited
generalization ability of temporal grounding. In this work, we propose a novel
DeNet (Decoupling and De-bias) to embrace human uncertainty: Decoupling - We
explicitly disentangle each query into a relation feature and a modified
feature. The relation feature, which is mainly based on skeleton-like words
(including nouns and verbs), aims to extract basic and consistent information
in the presence of query uncertainty. Meanwhile, modified feature assigned with
style-like words (including adjectives, adverbs, etc) represents the subjective
information, and thus brings personalized predictions; De-bias - We propose a
de-bias mechanism to generate diverse predictions, aim to alleviate the bias
caused by single-style annotations in the presence of label uncertainty.
Moreover, we put forward new multi-label metrics to diversify the performance
evaluation. Extensive experiments show that our approach is more effective and
robust than state-of-the-arts on Charades-STA and ActivityNet Captions
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanjun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chuanping Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle. (arXiv:2004.03376v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03376</id>
        <link href="http://arxiv.org/abs/2004.03376"/>
        <updated>2021-06-25T02:00:45.594Z</updated>
        <summary type="html"><![CDATA[The computation and memory needed for Convolutional Neural Network (CNN)
inference can be reduced by pruning weights from the trained network. Pruning
is guided by a pruning saliency, which heuristically approximates the change in
the loss function associated with the removal of specific weights. Many pruning
signals have been proposed, but the performance of each heuristic depends on
the particular trained network. This leaves the data scientist with a difficult
choice. When using any one saliency metric for the entire pruning process, we
run the risk of the metric assumptions being invalidated, leading to poor
decisions being made by the metric. Ideally we could combine the best aspects
of different saliency metrics. However, despite an extensive literature review,
we are unable to find any prior work on composing different saliency metrics.
The chief difficulty lies in combining the numerical output of different
saliency metrics, which are not directly comparable.

We propose a method to compose several primitive pruning saliencies, to
exploit the cases where each saliency measure does well. Our experiments show
that the composition of saliencies avoids many poor pruning choices identified
by individual saliencies. In most cases our method finds better selections than
even the best individual pruning saliency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1"&gt;Kaveena Persand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1"&gt;Andrew Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1"&gt;David Gregg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14944</id>
        <link href="http://arxiv.org/abs/2105.14944"/>
        <updated>2021-06-25T02:00:45.588Z</updated>
        <summary type="html"><![CDATA[Explaining the decisions of an Artificial Intelligence (AI) model is
increasingly critical in many real-world, high-stake applications. Hundreds of
papers have either proposed new feature attribution methods, discussed or
harnessed these tools in their work. However, despite humans being the target
end-users, most attribution methods were only evaluated on proxy
automatic-evaluation metrics. In this paper, we conduct the first, large-scale
user study on 320 lay and 11 expert users to shed light on the effectiveness of
state-of-the-art attribution methods in assisting humans in ImageNet
classification, Stanford Dogs fine-grained classification, and these two tasks
but when the input image contains adversarial perturbations. We found that, in
overall, feature attribution is surprisingly not more effective than showing
humans nearest training-set examples. On a hard task of fine-grained dog
categorization, presenting attribution maps to humans does not help, but
instead hurts the performance of human-AI teams compared to AI alone.
Importantly, we found automatic attribution-map evaluation measures to
correlate poorly with the actual human-AI team performance. Our findings
encourage the community to rigorously test their methods on the downstream
human-in-the-loop applications and to rethink the existing evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1"&gt;Giang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daeyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Differential Privacy Meets Interpretability: A Case Study. (arXiv:2106.13203v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13203</id>
        <link href="http://arxiv.org/abs/2106.13203"/>
        <updated>2021-06-25T02:00:45.571Z</updated>
        <summary type="html"><![CDATA[Given the increase in the use of personal data for training Deep Neural
Networks (DNNs) in tasks such as medical imaging and diagnosis, differentially
private training of DNNs is surging in importance and there is a huge body of
work focusing on providing better privacy-utility trade-off. However, little
attention is given to the interpretability of these models, and how the
application of DP affects the quality of interpretations. We propose an
extensive study into the effects of DP training on DNNs, especially on medical
imaging applications, on the APTOS dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1"&gt;Rakshit Naidu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1"&gt;Aman Priyanshu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aadith Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotti_S/0/1/0/all/0/1"&gt;Sasikanth Kotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haofan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1"&gt;Fatemehsadat Mireshghallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes. (arXiv:2106.13215v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13215</id>
        <link href="http://arxiv.org/abs/2106.13215"/>
        <updated>2021-06-25T02:00:45.565Z</updated>
        <summary type="html"><![CDATA[We present an algorithm that learns a coarse 3D representation of objects
from unposed multi-view 2D mask supervision, then uses it to generate detailed
mask and image texture. In contrast to existing voxel-based methods for unposed
object reconstruction, our approach learns to represent the generated shape and
pose with a set of self-supervised canonical 3D anisotropic Gaussians via a
perspective camera, and a set of per-image transforms. We show that this
approach can robustly estimate a 3D space for the camera and object, while
recent baselines sometimes struggle to reconstruct coherent 3D spaces in this
setting. We show results on synthetic datasets with realistic lighting, and
demonstrate object insertion with interactive posing. With our work, we help
move towards structured representations that handle more real-world variation
in learning-based object reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mejjati_Y/0/1/0/all/0/1"&gt;Youssef A.Mejjati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milefchik_I/0/1/0/all/0/1"&gt;Isa Milefchik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1"&gt;Aaron Gokaslan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1"&gt;Oliver Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kwang In Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1"&gt;James Tompkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FitVid: Overfitting in Pixel-Level Video Prediction. (arXiv:2106.13195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13195</id>
        <link href="http://arxiv.org/abs/2106.13195"/>
        <updated>2021-06-25T02:00:45.559Z</updated>
        <summary type="html"><![CDATA[An agent that is capable of predicting what happens next can perform a
variety of tasks through planning with no additional training. Furthermore,
such an agent can internally represent the complex dynamics of the real-world
and therefore can acquire a representation useful for a variety of visual
perception tasks. This makes predicting the future frames of a video,
conditioned on the observed past and potentially future actions, an interesting
task which remains exceptionally challenging despite many recent advances.
Existing video prediction models have shown promising results on simple narrow
benchmarks but they generate low quality predictions on real-life datasets with
more complicated dynamics or broader domain. There is a growing body of
evidence that underfitting on the training data is one of the primary causes
for the low quality predictions. In this paper, we argue that the inefficient
use of parameters in the current video models is the main reason for
underfitting. Therefore, we introduce a new architecture, named FitVid, which
is capable of severe overfitting on the common benchmarks while having similar
parameter count as the current state-of-the-art models. We analyze the
consequences of overfitting, illustrating how it can produce unexpected
outcomes such as generating high quality output by repeating the training data,
and how it can be mitigated using existing image augmentation techniques. As a
result, FitVid outperforms the current state-of-the-art models across four
different video prediction benchmarks on four different metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Babaeizadeh_M/0/1/0/all/0/1"&gt;Mohammad Babaeizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1"&gt;Mohammad Taghi Saffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erhan_D/0/1/0/all/0/1"&gt;Dumitru Erhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Sparse and Locally Coherent Morphable Face Model for Dense Semantic Correspondence Across Heterogeneous 3D Faces. (arXiv:2006.03840v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03840</id>
        <link href="http://arxiv.org/abs/2006.03840"/>
        <updated>2021-06-25T02:00:45.554Z</updated>
        <summary type="html"><![CDATA[The 3D Morphable Model (3DMM) is a powerful statistical tool for representing
3D face shapes. To build a 3DMM, a training set of face scans in full
point-to-point correspondence is required, and its modeling capabilities
directly depend on the variability contained in the training data. Thus, to
increase the descriptive power of the 3DMM, establishing a dense correspondence
across heterogeneous scans with sufficient diversity in terms of identities,
ethnicities, or expressions becomes essential. In this manuscript, we present a
fully automatic approach that leverages a 3DMM to transfer its dense semantic
annotation across raw 3D faces, establishing a dense correspondence between
them. We propose a novel formulation to learn a set of sparse deformation
components with local support on the face that, together with an original
non-rigid deformation algorithm, allow the 3DMM to precisely fit unseen faces
and transfer its semantic annotation. We extensively experimented our approach,
showing it can effectively generalize to highly diverse samples and accurately
establish a dense correspondence even in presence of complex facial
expressions. The accuracy of the dense registration is demonstrated by building
a heterogeneous, large-scale 3DMM from more than 9,000 fully registered scans
obtained by joining three large datasets together.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_C/0/1/0/all/0/1"&gt;Claudio Ferrari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berretti_S/0/1/0/all/0/1"&gt;Stefano Berretti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pala_P/0/1/0/all/0/1"&gt;Pietro Pala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1"&gt;Alberto Del Bimbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-resolution Image Registration of Consecutive and Re-stained Sections in Histopathology. (arXiv:2106.13150v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13150</id>
        <link href="http://arxiv.org/abs/2106.13150"/>
        <updated>2021-06-25T02:00:45.548Z</updated>
        <summary type="html"><![CDATA[We compare variational image registration in consectutive and re-stained
sections from histopathology. We present a fully-automatic algorithm for
non-parametric (nonlinear) image registration and apply it to a previously
existing dataset from the ANHIR challenge (230 slide pairs, consecutive
sections) and a new dataset (hybrid re-stained and consecutive, 81 slide pairs,
ca. 3000 landmarks) which is made publicly available. Registration
hyperparameters are obtained in the ANHIR dataset and applied to the new
dataset without modification. In the new dataset, landmark errors after
registration range from 13.2 micrometers for consecutive sections to 1
micrometer for re-stained sections. We observe that non-parametric registration
leads to lower landmark errors in both cases, even though the effect is smaller
in re-stained sections. The nucleus-level alignment after non-parametric
registration of re-stained sections provides a valuable tool to generate
automatic ground-truth for machine learning applications in histopathology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lotz_J/0/1/0/all/0/1"&gt;Johannes Lotz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weiss_N/0/1/0/all/0/1"&gt;Nick Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Laak_J/0/1/0/all/0/1"&gt;Jeroen van der Laak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+StefanHeldmann/0/1/0/all/0/1"&gt;StefanHeldmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Morph Face Detection using Discriminative Wavelet Sub-bands. (arXiv:2106.13178v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13178</id>
        <link href="http://arxiv.org/abs/2106.13178"/>
        <updated>2021-06-25T02:00:45.531Z</updated>
        <summary type="html"><![CDATA[Face recognition systems are extremely vulnerable to morphing attacks, in
which a morphed facial reference image can be successfully verified as two or
more distinct identities. In this paper, we propose a morph attack detection
algorithm that leverages an undecimated 2D Discrete Wavelet Transform (DWT) for
identifying morphed face images. The core of our framework is that artifacts
resulting from the morphing process that are not discernible in the image
domain can be more easily identified in the spatial frequency domain. A
discriminative wavelet sub-band can accentuate the disparity between a real and
a morphed image. To this end, multi-level DWT is applied to all images,
yielding 48 mid and high-frequency sub-bands each. The entropy distributions
for each sub-band are calculated separately for both bona fide and morph
images. For some of the sub-bands, there is a marked difference between the
entropy of the sub-band in a bona fide image and the identical sub-band's
entropy in a morphed image. Consequently, we employ Kullback-Liebler Divergence
(KLD) to exploit these differences and isolate the sub-bands that are the most
discriminative. We measure how discriminative a sub-band is by its KLD value
and the 22 sub-bands with the highest KLD values are chosen for network
training. Then, we train a deep Siamese neural network using these 22 selected
sub-bands for differential morph attack detection. We examine the efficacy of
discriminative wavelet sub-bands for morph attack detection and show that a
deep neural network trained on these sub-bands can accurately identify morph
imagery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1"&gt;Baaria Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1"&gt;Poorya Aghdaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1"&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularisation for PCA- and SVD-type matrix factorisations. (arXiv:2106.12955v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12955</id>
        <link href="http://arxiv.org/abs/2106.12955"/>
        <updated>2021-06-25T02:00:45.526Z</updated>
        <summary type="html"><![CDATA[Singular Value Decomposition (SVD) and its close relative, Principal
Component Analysis (PCA), are well-known linear matrix decomposition techniques
that are widely used in applications such as dimension reduction and
clustering. However, an important limitation of SVD/PCA is its sensitivity to
noise in the input data. In this paper, we take another look at the problem of
regularisation and show that different formulations of the minimisation problem
lead to qualitatively different solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khoshrou_A/0/1/0/all/0/1"&gt;Abdolrahman Khoshrou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1"&gt;Eric J. Pauwels&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021. (arXiv:2106.13033v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13033</id>
        <link href="http://arxiv.org/abs/2106.13033"/>
        <updated>2021-06-25T02:00:45.521Z</updated>
        <summary type="html"><![CDATA[In this paper, inspired by the successes of visionlanguage pre-trained models
and the benefits from training with adversarial attacks, we present a novel
transformerbased cross-modal fusion modeling by incorporating the both notions
for VQA challenge 2021. Specifically, the proposed model is on top of the
architecture of VinVL model [19], and the adversarial training strategy [4] is
applied to make the model robust and generalized. Moreover, two implementation
tricks are also used in our system to obtain better results. The experiments
demonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke-Han Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1"&gt;Bo-Han Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kuan-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShapeFlow: Learnable Deformations Among 3D Shapes. (arXiv:2006.07982v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07982</id>
        <link href="http://arxiv.org/abs/2006.07982"/>
        <updated>2021-06-25T02:00:45.506Z</updated>
        <summary type="html"><![CDATA[We present ShapeFlow, a flow-based model for learning a deformation space for
entire classes of 3D shapes with large intra-class variations. ShapeFlow allows
learning a multi-template deformation space that is agnostic to shape topology,
yet preserves fine geometric details. Different from a generative space where a
latent vector is directly decoded into a shape, a deformation space decodes a
vector into a continuous flow that can advect a source shape towards a target.
Such a space naturally allows the disentanglement of geometric style (coming
from the source) and structural pose (conforming to the target). We parametrize
the deformation between geometries as a learned continuous flow field via a
neural network and show that such deformations can be guaranteed to have
desirable properties, such as be bijectivity, freedom from self-intersections,
or volume preservation. We illustrate the effectiveness of this learned
deformation space for various downstream applications, including shape
generation via deformation, geometric style transfer, unsupervised learning of
a consistent parameterization for entire classes of shapes, and shape
interpolation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Chiyu &amp;quot;Max&amp;quot; Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jingwei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1"&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient $k$-modes Algorithm for Clustering Categorical Datasets. (arXiv:2006.03936v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03936</id>
        <link href="http://arxiv.org/abs/2006.03936"/>
        <updated>2021-06-25T02:00:45.499Z</updated>
        <summary type="html"><![CDATA[Mining clusters from data is an important endeavor in many applications. The
$k$-means method is a popular, efficient, and distribution-free approach for
clustering numerical-valued data, but does not apply for categorical-valued
observations. The $k$-modes method addresses this lacuna by replacing the
Euclidean with the Hamming distance and the means with the modes in the
$k$-means objective function. We provide a novel, computationally efficient
implementation of $k$-modes, called OTQT. We prove that OTQT finds updates to
improve the objective function that are undetectable to existing $k$-modes
algorithms. Although slightly slower per iteration due to algorithmic
complexity, OTQT is always more accurate per iteration and almost always faster
(and only barely slower on some datasets) to the final optimum. Thus, we
recommend OTQT as the preferred, default algorithm for $k$-modes optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dorman_K/0/1/0/all/0/1"&gt;Karin S. Dorman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1"&gt;Ranjan Maitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advancing biological super-resolution microscopy through deep learning: a brief review. (arXiv:2106.13064v1 [physics.bio-ph])]]></title>
        <id>http://arxiv.org/abs/2106.13064</id>
        <link href="http://arxiv.org/abs/2106.13064"/>
        <updated>2021-06-25T02:00:45.484Z</updated>
        <summary type="html"><![CDATA[Super-resolution microscopy overcomes the diffraction limit of conventional
light microscopy in spatial resolution. By providing novel spatial or
spatio-temporal information on biological processes at nanometer resolution
with molecular specificity, it plays an increasingly important role in life
sciences. However, its technical limitations require trade-offs to balance its
spatial resolution, temporal resolution, and light exposure of samples.
Recently, deep learning has achieved breakthrough performance in many image
processing and computer vision tasks. It has also shown great promise in
pushing the performance envelope of super-resolution microscopy. In this brief
Review, we survey recent advances in using deep learning to enhance performance
of super-resolution microscopy. We focus primarily on how deep learning
ad-vances reconstruction of super-resolution images. Related key technical
challenges are discussed. Despite the challenges, deep learning is set to play
an indispensable and transformative role in the development of super-resolution
microscopy. We conclude with an outlook on how deep learning could shape the
future of this new generation of light microscopy technology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianjie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yaoru Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ji_W/0/1/0/all/0/1"&gt;Wei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yang_G/0/1/0/all/0/1"&gt;Ge Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Performance Hyperspectral Image Classification using Graphics Processing Units. (arXiv:2106.12942v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.12942</id>
        <link href="http://arxiv.org/abs/2106.12942"/>
        <updated>2021-06-25T02:00:45.478Z</updated>
        <summary type="html"><![CDATA[Real-time remote sensing applications like search and rescue missions,
military target detection, environmental monitoring, hazard prevention and
other time-critical applications require onboard real time processing
capabilities or autonomous decision making. Some unmanned remote systems like
satellites are physically remote from their operators, and all control of the
spacecraft and data returned by the spacecraft must be transmitted over a
wireless radio link. This link may not be available for extended periods when
the satellite is out of line of sight of its ground station. Therefore,
lightweight, small size and low power consumption hardware is essential for
onboard real time processing systems. With increasing dimensionality, size and
resolution of recent hyperspectral imaging sensors, additional challenges are
posed upon remote sensing processing systems and more capable computing
architectures are needed. Graphical Processing Units (GPUs) emerged as
promising architecture for light weight high performance computing that can
address these computational requirements for onboard systems. The goal of this
study is to build high performance methods for onboard hyperspectral analysis.
We propose accelerated methods for the well-known recursive hierarchical
segmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a
GPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the
National Aeronautics and Space Administration (NASA), which is designed to
provide rich classification information with several output levels. The
achieved speedups by parallel solutions compared to CPU sequential
implementations are 21x for parallel single GPU and 240x for hybrid multi-node
computer clusters with 16 computing nodes. The energy consumption is reduced to
74% using a single GPU compared to the equivalent parallel CPU cluster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hossam_M/0/1/0/all/0/1"&gt;Mahmoud Hossam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handling Data Heterogeneity with Generative Replay in Collaborative Learning for Medical Imaging. (arXiv:2106.13208v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13208</id>
        <link href="http://arxiv.org/abs/2106.13208"/>
        <updated>2021-06-25T02:00:45.471Z</updated>
        <summary type="html"><![CDATA[Collaborative learning, which enables collaborative and decentralized
training of deep neural networks at multiple institutions in a
privacy-preserving manner, is rapidly emerging as a valuable technique in
healthcare applications. However, its distributed nature often leads to
significant heterogeneity in data distributions across institutions. Existing
collaborative learning approaches generally do not account for the presence of
heterogeneity in data among institutions, or only mildly skewed label
distributions are studied. In this paper, we present a novel generative replay
strategy to address the challenge of data heterogeneity in collaborative
learning methods. Instead of directly training a model for task performance, we
leverage recent image synthesis techniques to develop a novel dual model
architecture: a primary model learns the desired task, and an auxiliary
"generative replay model" either synthesizes images that closely resemble the
input images or helps extract latent variables. The generative replay strategy
is flexible to use, can either be incorporated into existing collaborative
learning methods to improve their capability of handling data heterogeneity
across institutions, or be used as a novel and individual collaborative
learning framework (termed FedReplay) to reduce communication cost.
Experimental results demonstrate the capability of the proposed method in
handling heterogeneous data across institutions. On highly heterogeneous data
partitions, our model achieves ~4.88% improvement in the prediction accuracy on
a diabetic retinopathy classification dataset, and ~49.8% reduction of mean
absolution value on a Bone Age prediction dataset, respectively, compared to
the state-of-the art collaborative learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liangqiong Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1"&gt;Niranjan Balachandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Depth Confidence-aware Camouflaged Object Detection. (arXiv:2106.13217v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13217</id>
        <link href="http://arxiv.org/abs/2106.13217"/>
        <updated>2021-06-25T02:00:45.466Z</updated>
        <summary type="html"><![CDATA[Camouflaged object detection (COD) aims to segment camouflaged objects hiding
in the environment, which is challenging due to the similar appearance of
camouflaged objects and their surroundings. Research in biology suggests that
depth can provide useful object localization cues for camouflaged object
discovery, as all the animals have 3D perception ability. However, the depth
information has not been exploited for camouflaged object detection. To explore
the contribution of depth for camouflage detection, we present a depth-guided
camouflaged object detection network with pre-computed depth maps from existing
monocular depth estimation methods. Due to the domain gap between the depth
estimation dataset and our camouflaged object detection dataset, the generated
depth may not be accurate enough to be directly used in our framework. We then
introduce a depth quality assessment module to evaluate the quality of depth
based on the model prediction from both RGB COD branch and RGB-D COD branch.
During training, only high-quality depth is used to update the modal
interaction module for multi-modal learning. During testing, our depth quality
assessment module can effectively determine the contribution of depth and
select the RGB branch or RGB-D branch for camouflage prediction. Extensive
experiments on various camouflaged object detection datasets prove the
effectiveness of our solution in exploring the depth information for
camouflaged object detection. Our code and data is publicly available at:
\url{https://github.com/JingZhang617/RGBD-COD}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1"&gt;Yunqiu Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_M/0/1/0/all/0/1"&gt;Mochu Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Aixuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yuchao Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yiran Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Driver-centric Risk Object Identification. (arXiv:2106.13201v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13201</id>
        <link href="http://arxiv.org/abs/2106.13201"/>
        <updated>2021-06-25T02:00:45.460Z</updated>
        <summary type="html"><![CDATA[A massive number of traffic fatalities are due to driver errors. To reduce
fatalities, developing intelligent driving systems assisting drivers to
identify potential risks is in urgent need. Risky situations are generally
defined based on collision prediction in existing research. However, collisions
are only one type of risk in traffic scenarios. We believe a more generic
definition is required. In this work, we propose a novel driver-centric
definition of risk, i.e., risky objects influence driver behavior. Based on
this definition, a new task called risk object identification is introduced. We
formulate the task as a cause-effect problem and present a novel two-stage risk
object identification framework, taking inspiration from models of situation
awareness and causal inference. A driver-centric Risk Object Identification
(ROI) dataset is curated to evaluate the proposed system. We demonstrate
state-of-the-art risk object identification performance compared with strong
baselines on the ROI dataset. In addition, we conduct extensive ablative
studies to justify our design choices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1"&gt;Stanley H. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Ting Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChaLearn Looking at People: Inpainting and Denoising challenges. (arXiv:2106.13071v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13071</id>
        <link href="http://arxiv.org/abs/2106.13071"/>
        <updated>2021-06-25T02:00:45.445Z</updated>
        <summary type="html"><![CDATA[Dealing with incomplete information is a well studied problem in the context
of machine learning and computational intelligence. However, in the context of
computer vision, the problem has only been studied in specific scenarios (e.g.,
certain types of occlusions in specific types of images), although it is common
to have incomplete information in visual data. This chapter describes the
design of an academic competition focusing on inpainting of images and video
sequences that was part of the competition program of WCCI2018 and had a
satellite event collocated with ECCV2018. The ChaLearn Looking at People
Inpainting Challenge aimed at advancing the state of the art on visual
inpainting by promoting the development of methods for recovering missing and
occluded information from images and video. Three tracks were proposed in which
visual inpainting might be helpful but still challenging: human body pose
estimation, text overlays removal and fingerprint denoising. This chapter
describes the design of the challenge, which includes the release of three
novel datasets, and the description of evaluation metrics, baselines and
evaluation protocol. The results of the challenge are analyzed and discussed in
detail and conclusions derived from this event are outlined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1"&gt;Sergio Escalera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soler_M/0/1/0/all/0/1"&gt;Marti Soler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayache_S/0/1/0/all/0/1"&gt;Stephane Ayache&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guclu_U/0/1/0/all/0/1"&gt;Umut Guclu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1"&gt;Jun Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1"&gt;Meysam Madadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baro_X/0/1/0/all/0/1"&gt;Xavier Baro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Escalante_H/0/1/0/all/0/1"&gt;Hugo Jair Escalante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1"&gt;Isabelle Guyon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13122</id>
        <link href="http://arxiv.org/abs/2106.13122"/>
        <updated>2021-06-25T02:00:45.439Z</updated>
        <summary type="html"><![CDATA[Recently, vision transformers and MLP-based models have been developed in
order to address some of the prevalent weaknesses in convolutional neural
networks. Due to the novelty of transformers being used in this domain along
with the self-attention mechanism, it remains unclear to what degree these
architectures are robust to corruptions. Despite some works proposing that data
augmentation remains essential for a model to be robust against corruptions, we
propose to explore the impact that the architecture has on corruption
robustness. We find that vision transformer architectures are inherently more
robust to corruptions than the ResNet-50 and MLP-Mixers. We also find that
vision transformers with 5 times fewer parameters than a ResNet-50 have more
shape bias. Our code is available to reproduce.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1"&gt;Katelyn Morrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1"&gt;Benjamin Gilby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1"&gt;Colton Lipchak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1"&gt;Adam Mattioli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1"&gt;Adriana Kovashka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoAdapt: Automated Segmentation Network Search for Unsupervised Domain Adaptation. (arXiv:2106.13227v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13227</id>
        <link href="http://arxiv.org/abs/2106.13227"/>
        <updated>2021-06-25T02:00:45.434Z</updated>
        <summary type="html"><![CDATA[Neural network-based semantic segmentation has achieved remarkable results
when large amounts of annotated data are available, that is, in the supervised
case. However, such data is expensive to collect and so methods have been
developed to adapt models trained on related, often synthetic data for which
labels are readily available. Current adaptation approaches do not consider the
dependence of the generalization/transferability of these models on network
architecture. In this paper, we perform neural architecture search (NAS) to
provide architecture-level perspective and analysis for domain adaptation. We
identify the optimization gap that exists when searching architectures for
unsupervised domain adaptation which makes this NAS problem uniquely difficult.
We propose bridging this gap by using maximum mean discrepancy and regional
weighted entropy to estimate the accuracy metric. Experimental results on
several widely adopted benchmarks show that our proposed AutoAdapt framework
indeed discovers architectures that improve the performance of a number of
existing adaptation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xueqing Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuxin Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newsam_S/0/1/0/all/0/1"&gt;Shawn Newsam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Class agnostic moving target detection by color and location prediction of moving area. (arXiv:2106.12966v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12966</id>
        <link href="http://arxiv.org/abs/2106.12966"/>
        <updated>2021-06-25T02:00:45.429Z</updated>
        <summary type="html"><![CDATA[Moving target detection plays an important role in computer vision. However,
traditional algorithms such as frame difference and optical flow usually suffer
from low accuracy or heavy computation. Recent algorithms such as deep
learning-based convolutional neural networks have achieved high accuracy and
real-time performance, but they usually need to know the classes of targets in
advance, which limits the practical applications. Therefore, we proposed a
model free moving target detection algorithm. This algorithm extracts the
moving area through the difference of image features. Then, the color and
location probability map of the moving area will be calculated through maximum
a posteriori probability. And the target probability map can be obtained
through the dot multiply between the two maps. Finally, the optimal moving
target area can be solved by stochastic gradient descent on the target
probability map. Results show that the proposed algorithm achieves the highest
accuracy compared with state-of-the-art algorithms, without needing to know the
classes of targets. Furthermore, as the existing datasets are not suitable for
moving target detection, we proposed a method for producing evaluation dataset.
Besides, we also proved the proposed algorithm can be used to assist target
tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhuang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1"&gt;Huajun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhihai Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-space Conditioned Translation Networks for Directional Synthesis of Diffusion Weighted Images from Multi-modal Structural MRI. (arXiv:2106.13188v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13188</id>
        <link href="http://arxiv.org/abs/2106.13188"/>
        <updated>2021-06-25T02:00:45.410Z</updated>
        <summary type="html"><![CDATA[Current deep learning approaches for diffusion MRI modeling circumvent the
need for densely-sampled diffusion-weighted images (DWIs) by directly
predicting microstructural indices from sparsely-sampled DWIs. However, they
implicitly make unrealistic assumptions of static $q$-space sampling during
training and reconstruction. Further, such approaches can restrict downstream
usage of variably sampled DWIs for usages including the estimation of
microstructural indices or tractography. We propose a generative adversarial
translation framework for high-quality DWI synthesis with arbitrary $q$-space
sampling given commonly acquired structural images (e.g., B0, T1, T2). Our
translation network linearly modulates its internal representations conditioned
on continuous $q$-space information, thus removing the need for fixed sampling
schemes. Moreover, this approach enables downstream estimation of high-quality
microstructural maps from arbitrarily subsampled DWIs, which may be
particularly important in cases with sparsely sampled DWIs. Across several
recent methodologies, the proposed approach yields improved DWI synthesis
accuracy and fidelity with enhanced downstream utility as quantified by the
accuracy of scalar microstructure indices estimated from the synthesized
images. Code is available at
https://github.com/mengweiren/q-space-conditioned-dwi-synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mengwei Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1"&gt;Heejong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1"&gt;Neel Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gerig_G/0/1/0/all/0/1"&gt;Guido Gerig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Stronger Feature for Temporal Action Localization. (arXiv:2106.13014v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13014</id>
        <link href="http://arxiv.org/abs/2106.13014"/>
        <updated>2021-06-25T02:00:45.398Z</updated>
        <summary type="html"><![CDATA[Temporal action localization aims to localize starting and ending time with
action category. Limited by GPU memory, mainstream methods pre-extract features
for each video. Therefore, feature quality determines the upper bound of
detection performance. In this technical report, we explored classic
convolution-based backbones and the recent surge of transformer-based
backbones. We found that the transformer-based methods can achieve better
classification performance than convolution-based, but they cannot generate
accuracy action proposals. In addition, extracting features with larger frame
resolution to reduce the loss of spatial information can also effectively
improve the performance of temporal action localization. Finally, we achieve
42.42% in terms of mAP on validation set with a single SlowFast feature by a
simple combination: BMN+TCANet, which is 1.87% higher than the result of 2020's
multi-model ensemble. Finally, we achieve Rank 1st on the CVPR2021 HACS
supervised Temporal Action Localization Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_j/0/1/0/all/0/1"&gt;jianwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Changxin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FaDIV-Syn: Fast Depth-Independent View Synthesis. (arXiv:2106.13139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13139</id>
        <link href="http://arxiv.org/abs/2106.13139"/>
        <updated>2021-06-25T02:00:45.392Z</updated>
        <summary type="html"><![CDATA[We introduce FaDIV-Syn, a fast depth-independent view synthesis method. Our
multi-view approach addresses the problem that view synthesis methods are often
limited by their depth estimation stage, where incorrect depth predictions can
lead to large projection errors. To avoid this issue, we efficiently warp
multiple input images into the target frame for a range of assumed depth
planes. The resulting tensor representation is fed into a U-Net-like CNN with
gated convolutions, which directly produces the novel output view. We therefore
side-step explicit depth estimation. This improves efficiency and performance
on transparent, reflective, and feature-less scene parts. FaDIV-Syn can handle
both interpolation and extrapolation tasks and outperforms state-of-the-art
extrapolation methods on the large-scale RealEstate10k dataset. In contrast to
comparable methods, it is capable of real-time operation due to its lightweight
architecture. We further demonstrate data efficiency of FaDIV-Syn by training
from fewer examples as well as its generalization to higher resolutions and
arbitrary depth ranges under severe depth discretization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rochow_A/0/1/0/all/0/1"&gt;Andre Rochow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1"&gt;Max Schwarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1"&gt;Michael Weinmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetric Wasserstein Autoencoders. (arXiv:2106.13024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13024</id>
        <link href="http://arxiv.org/abs/2106.13024"/>
        <updated>2021-06-25T02:00:45.386Z</updated>
        <summary type="html"><![CDATA[Leveraging the framework of Optimal Transport, we introduce a new family of
generative autoencoders with a learnable prior, called Symmetric Wasserstein
Autoencoders (SWAEs). We propose to symmetrically match the joint distributions
of the observed data and the latent representation induced by the encoder and
the decoder. The resulting algorithm jointly optimizes the modelling losses in
both the data and the latent spaces with the loss in the data space leading to
the denoising effect. With the symmetric treatment of the data and the latent
representation, the algorithm implicitly preserves the local structure of the
data in the latent space. To further improve the quality of the latent
representation, we incorporate a reconstruction loss into the objective, which
significantly benefits both the generation and reconstruction. We empirically
show the superior performance of SWAEs over the state-of-the-art generative
autoencoders in terms of classification, reconstruction, and generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Sun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VOLO: Vision Outlooker for Visual Recognition. (arXiv:2106.13112v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13112</id>
        <link href="http://arxiv.org/abs/2106.13112"/>
        <updated>2021-06-25T02:00:45.360Z</updated>
        <summary type="html"><![CDATA[Visual recognition has been dominated by convolutionalneural networks (CNNs)
for years. Though recently the pre-vailing vision transformers (ViTs) have
shown great poten-tial of self-attention based models in ImageNet
classifica-tion, their performance is still inferior to latest SOTA CNNsif no
extra data are provided. In this work, we aim to closethe performance gap and
demonstrate that attention-basedmodels are indeed able to outperform CNNs. We
found thatthe main factor limiting the performance of ViTs for Ima-geNet
classification is their low efficacy in encoding fine-level features into the
token representations. To resolvethis, we introduce a noveloutlook attentionand
present asimple and general architecture, termed Vision Outlooker(VOLO). Unlike
self-attention that focuses on global depen-dency modeling at a coarse level,
the outlook attention aimsto efficiently encode finer-level features and
contexts intotokens, which are shown to be critical for recognition
per-formance but largely ignored by the self-attention. Experi-ments show that
our VOLO achieves 87.1% top-1 accuracyon ImageNet-1K classification, being the
first model exceed-ing 87% accuracy on this competitive benchmark, withoutusing
any extra training data. In addition, the pre-trainedVOLO transfers well to
downstream tasks, such as seman-tic segmentation. We achieve 84.3% mIoU score
on thecityscapes validation set and 54.3% on the ADE20K valida-tion set. Code
is available at https://github.com/sail-sg/volo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Li Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qibin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zihang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Shuicheng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Fully Interpretable Deep Neural Networks: Are We There Yet?. (arXiv:2106.13164v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13164</id>
        <link href="http://arxiv.org/abs/2106.13164"/>
        <updated>2021-06-25T02:00:45.352Z</updated>
        <summary type="html"><![CDATA[Despite the remarkable performance, Deep Neural Networks (DNNs) behave as
black-boxes hindering user trust in Artificial Intelligence (AI) systems.
Research on opening black-box DNN can be broadly categorized into post-hoc
methods and inherently interpretable DNNs. While many surveys have been
conducted on post-hoc interpretation methods, little effort is devoted to
inherently interpretable DNNs. This paper provides a review of existing methods
to develop DNNs with intrinsic interpretability, with a focus on Convolutional
Neural Networks (CNNs). The aim is to understand the current progress towards
fully interpretable DNNs that can cater to different interpretation
requirements. Finally, we identify gaps in current work and suggest potential
research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramanayake_S/0/1/0/all/0/1"&gt;Sandareka Wickramanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wynne Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mong Li Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-Time Deep Glioma Growth Models. (arXiv:2106.12917v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12917</id>
        <link href="http://arxiv.org/abs/2106.12917"/>
        <updated>2021-06-25T02:00:45.325Z</updated>
        <summary type="html"><![CDATA[The ability to estimate how a tumor might evolve in the future could have
tremendous clinical benefits, from improved treatment decisions to better dose
distribution in radiation therapy. Recent work has approached the glioma growth
modeling problem via deep learning and variational inference, thus learning
growth dynamics entirely from a real patient data distribution. So far, this
approach was constrained to predefined image acquisition intervals and
sequences of fixed length, which limits its applicability in more realistic
scenarios. We overcome these limitations by extending Neural Processes, a class
of conditional generative models for stochastic time series, with a
hierarchical multi-scale representation encoding including a spatio-temporal
attention mechanism. The result is a learned growth model that can be
conditioned on an arbitrary number of observations, and that can produce a
distribution of temporally consistent growth trajectories on a continuous time
axis. On a dataset of 379 patients, the approach successfully captures both
global and finer-grained variations in the images, exhibiting superior
performance compared to other learned growth models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Petersen_J/0/1/0/all/0/1"&gt;Jens Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1"&gt;Fabian Isensee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kohler_G/0/1/0/all/0/1"&gt;Gregor K&amp;#xf6;hler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1"&gt;Paul F. J&amp;#xe4;ger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zimmerer_D/0/1/0/all/0/1"&gt;David Zimmerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Neuberger_U/0/1/0/all/0/1"&gt;Ulf Neuberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wick_W/0/1/0/all/0/1"&gt;Wolfgang Wick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Debus_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Debus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heiland_S/0/1/0/all/0/1"&gt;Sabine Heiland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bendszus_M/0/1/0/all/0/1"&gt;Martin Bendszus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vollmuth_P/0/1/0/all/0/1"&gt;Philipp Vollmuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1"&gt;Klaus H. Maier-Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGTBN: Generating Dense Depth Maps from Single-Line LiDAR. (arXiv:2106.12994v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12994</id>
        <link href="http://arxiv.org/abs/2106.12994"/>
        <updated>2021-06-25T02:00:45.319Z</updated>
        <summary type="html"><![CDATA[Depth completion aims to generate a dense depth map from the sparse depth map
and aligned RGB image. However, current depth completion methods use extremely
expensive 64-line LiDAR(about $100,000) to obtain sparse depth maps, which will
limit their application scenarios. Compared with the 64-line LiDAR, the
single-line LiDAR is much less expensive and much more robust. Therefore, we
propose a method to tackle the problem of single-line depth completion, in
which we aim to generate a dense depth map from the single-line LiDAR info and
the aligned RGB image. A single-line depth completion dataset is proposed based
on the existing 64-line depth completion dataset(KITTI). A network called
Semantic Guided Two-Branch Network(SGTBN) which contains global and local
branches to extract and fuse global and local info is proposed for this task. A
Semantic guided depth upsampling module is used in our network to make full use
of the semantic info in RGB images. Except for the usual MSE loss, we add the
virtual normal loss to increase the constraint of high-order 3D geometry in our
network. Our network outperforms the state-of-the-art in the single-line depth
completion task. Besides, compared with the monocular depth estimation, our
method also has significant advantages in precision and model size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Hengjie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shugong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Shan Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of Depth and Depth-of-Field Effect from Natural Images with Aperture Rendering Generative Adversarial Networks. (arXiv:2106.13041v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13041</id>
        <link href="http://arxiv.org/abs/2106.13041"/>
        <updated>2021-06-25T02:00:45.303Z</updated>
        <summary type="html"><![CDATA[Understanding the 3D world from 2D projected natural images is a fundamental
challenge in computer vision and graphics. Recently, an unsupervised learning
approach has garnered considerable attention owing to its advantages in data
collection. However, to mitigate training limitations, typical methods need to
impose assumptions for viewpoint distribution (e.g., a dataset containing
various viewpoint images) or object shape (e.g., symmetric objects). These
assumptions often restrict applications; for instance, the application to
non-rigid objects or images captured from similar viewpoints (e.g., flower or
bird images) remains a challenge. To complement these approaches, we propose
aperture rendering generative adversarial networks (AR-GANs), which equip
aperture rendering on top of GANs, and adopt focus cues to learn the depth and
depth-of-field (DoF) effect of unlabeled natural images. To address the
ambiguities triggered by unsupervised setting (i.e., ambiguities between smooth
texture and out-of-focus blurs, and between foreground and background blurs),
we develop DoF mixture learning, which enables the generator to learn real
image distribution while generating diverse DoF images. In addition, we devise
a center focus prior to guiding the learning direction. In the experiments, we
demonstrate the effectiveness of AR-GANs in various datasets, such as flower,
bird, and face images, demonstrate their portability by incorporating them into
other 3D representation learning GANs, and validate their applicability in
shallow DoF rendering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1"&gt;Takuhiro Kaneko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple and Strong Baseline: Progressively Region-based Scene Text Removal Networks. (arXiv:2106.13029v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13029</id>
        <link href="http://arxiv.org/abs/2106.13029"/>
        <updated>2021-06-25T02:00:45.297Z</updated>
        <summary type="html"><![CDATA[Existing scene text removal methods mainly train an elaborate network with
paired images to realize the function of text localization and background
reconstruction simultaneously, but there exists two problems: 1) lacking the
exhaustive erasure of text region and 2) causing the excessive erasure to
text-free areas. To handle these issues, this paper provides a novel
ProgrEssively Region-based scene Text eraser (PERT), which introduces
region-based modification strategy to progressively erase the pixels in only
text region. Firstly, PERT decomposes the STR task to several erasing stages.
As each stage aims to take a further step toward the text-removed image rather
than directly regress to the final result, the decomposed operation reduces the
learning difficulty in each stage, and an exhaustive erasure result can be
obtained by iterating over lightweight erasing blocks with shared parameters.
Then, PERT introduces a region-based modification strategy to ensure the
integrity of text-free areas by decoupling text localization from erasure
process to guide the removal. Benefiting from the simplicity architecture, PERT
is a simple and strong baseline, and is easy to be followed and developed.
Extensive experiments demonstrate that PERT obtains the state-of-the-art
results on both synthetic and real-world datasets. Code is available
athttps://github.com/wangyuxin87/PERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Hongtao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Shancheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1"&gt;Yadong Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongdong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MatchVIE: Exploiting Match Relevancy between Entities for Visual Information Extraction. (arXiv:2106.12940v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12940</id>
        <link href="http://arxiv.org/abs/2106.12940"/>
        <updated>2021-06-25T02:00:45.288Z</updated>
        <summary type="html"><![CDATA[Visual Information Extraction (VIE) task aims to extract key information from
multifarious document images (e.g., invoices and purchase receipts). Most
previous methods treat the VIE task simply as a sequence labeling problem or
classification problem, which requires models to carefully identify each kind
of semantics by introducing multimodal features, such as font, color, layout.
But simply introducing multimodal features couldn't work well when faced with
numeric semantic categories or some ambiguous texts. To address this issue, in
this paper we propose a novel key-value matching model based on a graph neural
network for VIE (MatchVIE). Through key-value matching based on relevancy
evaluation, the proposed MatchVIE can bypass the recognitions to various
semantics, and simply focuses on the strong relevancy between entities.
Besides, we introduce a simple but effective operation, Num2Vec, to tackle the
instability of encoded values, which helps model converge more smoothly.
Comprehensive experiments demonstrate that the proposed MatchVIE can
significantly outperform previous methods. Notably, to the best of our
knowledge, MatchVIE may be the first attempt to tackle the VIE task by modeling
the relevancy between keys and values and it is a good complement to the
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Guozhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lele Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiapeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qianying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yaqiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hui Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Monte Carlo Rendering via Multi-Resolution Sampling. (arXiv:2106.12802v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12802</id>
        <link href="http://arxiv.org/abs/2106.12802"/>
        <updated>2021-06-25T02:00:45.282Z</updated>
        <summary type="html"><![CDATA[Monte Carlo rendering algorithms are widely used to produce photorealistic
computer graphics images. However, these algorithms need to sample a
substantial amount of rays per pixel to enable proper global illumination and
thus require an immense amount of computation. In this paper, we present a
hybrid rendering method to speed up Monte Carlo rendering algorithms. Our
method first generates two versions of a rendering: one at a low resolution
with a high sample rate (LRHS) and the other at a high resolution with a low
sample rate (HRLS). We then develop a deep convolutional neural network to fuse
these two renderings into a high-quality image as if it were rendered at a high
resolution with a high sample rate. Specifically, we formulate this fusion task
as a super resolution problem that generates a high resolution rendering from a
low resolution input (LRHS), assisted with the HRLS rendering. The HRLS
rendering provides critical high frequency details which are difficult to
recover from the LRHS for any super resolution methods. Our experiments show
that our hybrid rendering algorithm is significantly faster than the
state-of-the-art Monte Carlo denoising methods while rendering high-quality
images when tested on both our own BCR dataset and the Gharbi dataset.
\url{https://github.com/hqqxyy/msspl}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qiqi Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marshall_C/0/1/0/all/0/1"&gt;Carl S Marshall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panneer_S/0/1/0/all/0/1"&gt;Selvakumar Panneer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Completion for Occluded Person Re-Identification. (arXiv:2106.12733v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12733</id>
        <link href="http://arxiv.org/abs/2106.12733"/>
        <updated>2021-06-25T02:00:45.276Z</updated>
        <summary type="html"><![CDATA[Person re-identification (reID) plays an important role in computer vision.
However, existing methods suffer from performance degradation in occluded
scenes. In this work, we propose an occlusion-robust block, Region Feature
Completion (RFC), for occluded reID. Different from most previous works that
discard the occluded regions, RFC block can recover the semantics of occluded
regions in feature space. Firstly, a Spatial RFC (SRFC) module is developed.
SRFC exploits the long-range spatial contexts from non-occluded regions to
predict the features of occluded regions. The unit-wise prediction task leads
to an encoder/decoder architecture, where the region-encoder models the
correlation between non-occluded and occluded region, and the region-decoder
utilizes the spatial correlation to recover occluded region features. Secondly,
we introduce Temporal RFC (TRFC) module which captures the long-term temporal
contexts to refine the prediction of SRFC. RFC block is lightweight, end-to-end
trainable and can be easily plugged into existing CNNs to form RFCnet.
Extensive experiments are conducted on occluded and commonly holistic reID
benchmarks. Our method significantly outperforms existing methods on the
occlusion datasets, while remains top even superior performance on holistic
datasets. The source code is available at
https://github.com/blue-blue272/OccludedReID-RFCnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1"&gt;Ruibing Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1"&gt;Bingpeng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hong Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xinqian Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relationship between pulmonary nodule malignancy and surrounding pleurae, airways and vessels: a quantitative study using the public LIDC-IDRI dataset. (arXiv:2106.12991v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12991</id>
        <link href="http://arxiv.org/abs/2106.12991"/>
        <updated>2021-06-25T02:00:45.270Z</updated>
        <summary type="html"><![CDATA[To investigate whether the pleurae, airways and vessels surrounding a nodule
on non-contrast computed tomography (CT) can discriminate benign and malignant
pulmonary nodules. The LIDC-IDRI dataset, one of the largest publicly available
CT database, was exploited for study. A total of 1556 nodules from 694 patients
were involved in statistical analysis, where nodules with average scorings <3
and >3 were respectively denoted as benign and malignant. Besides, 339 nodules
from 113 patients with diagnosis ground-truth were independently evaluated.
Computer algorithms were developed to segment pulmonary structures and quantify
the distances to pleural surface, airways and vessels, as well as the counting
number and normalized volume of airways and vessels near a nodule. Odds ratio
(OR) and Chi-square (\chi^2) testing were performed to demonstrate the
correlation between features of surrounding structures and nodule malignancy. A
non-parametric receiver operating characteristic (ROC) analysis was conducted
in logistic regression to evaluate discrimination ability of each structure.
For benign and malignant groups, the average distances from nodules to pleural
surface, airways and vessels are respectively (6.56, 5.19), (37.08, 26.43) and
(1.42, 1.07) mm. The correlation between nodules and the counting number of
airways and vessels that contact or project towards nodules are respectively
(OR=22.96, \chi^2=105.04) and (OR=7.06, \chi^2=290.11). The correlation between
nodules and the volume of airways and vessels are (OR=9.19, \chi^2=159.02) and
(OR=2.29, \chi^2=55.89). The areas-under-curves (AUCs) for pleurae, airways and
vessels are respectively 0.5202, 0.6943 and 0.6529. Our results show that
malignant nodules are often surrounded by more pulmonary structures compared
with benign ones, suggesting that features of these structures could be viewed
as lung cancer biomarkers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yulei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yun Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanxiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lihui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1"&gt;Feng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yue-Min Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Global Appearance and Local Coding Distortion based Fusion Framework for CNN based Filtering in Video Coding. (arXiv:2106.12746v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12746</id>
        <link href="http://arxiv.org/abs/2106.12746"/>
        <updated>2021-06-25T02:00:45.264Z</updated>
        <summary type="html"><![CDATA[In-loop filtering is used in video coding to process the reconstructed frame
in order to remove blocking artifacts. With the development of convolutional
neural networks (CNNs), CNNs have been explored for in-loop filtering
considering it can be treated as an image de-noising task. However, in addition
to being a distorted image, the reconstructed frame is also obtained by a fixed
line of block based encoding operations in video coding. It carries coding-unit
based coding distortion of some similar characteristics. Therefore, in this
paper, we address the filtering problem from two aspects, global appearance
restoration for disrupted texture and local coding distortion restoration
caused by fixed pipeline of coding. Accordingly, a three-stream global
appearance and local coding distortion based fusion network is developed with a
high-level global feature stream, a high-level local feature stream and a
low-level local feature stream. Ablation study is conducted to validate the
necessity of different features, demonstrating that the global features and
local features can complement each other in filtering and achieve better
performance when combined. To the best of our knowledge, we are the first one
that clearly characterizes the video filtering process from the above global
appearance and local coding distortion restoration aspects with experimental
verification, providing a clear pathway to developing filter techniques.
Experimental results demonstrate that the proposed method significantly
outperforms the existing single-frame based methods and achieves 13.5%, 11.3%,
11.7% BD-Rate saving on average for AI, LDP and RA configurations,
respectively, compared with the HEVC reference software.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yue_J/0/1/0/all/0/1"&gt;Jian Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yanbo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hui Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dufaux_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;d&amp;#xe9;ric Dufaux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Speech to Sign Language Generation. (arXiv:2106.12790v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12790</id>
        <link href="http://arxiv.org/abs/2106.12790"/>
        <updated>2021-06-25T02:00:45.258Z</updated>
        <summary type="html"><![CDATA[We aim to solve the highly challenging task of generating continuous sign
language videos solely from speech segments for the first time. Recent efforts
in this space have focused on generating such videos from human-annotated text
transcripts without considering other modalities. However, replacing speech
with sign language proves to be a practical solution while communicating with
people suffering from hearing loss. Therefore, we eliminate the need of using
text as input and design techniques that work for more natural, continuous,
freely uttered speech covering an extensive vocabulary. Since the current
datasets are inadequate for generating sign language directly from speech, we
collect and release the first Indian sign language dataset comprising
speech-level annotations, text transcripts, and the corresponding sign-language
videos. Next, we propose a multi-tasking transformer network trained to
generate signer's poses from speech segments. With speech-to-text as an
auxiliary task and an additional cross-modal discriminator, our model learns to
generate continuous sign pose sequences in an end-to-end manner. Extensive
experiments and comparisons with other baselines demonstrate the effectiveness
of our approach. We also conduct additional ablation studies to analyze the
effect of different modules of our network. A demo video containing several
results is attached to the supplementary material.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_P/0/1/0/all/0/1"&gt;Parul Kapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukhopadhyay_R/0/1/0/all/0/1"&gt;Rudrabha Mukhopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1"&gt;Sindhu B Hegde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay Namboodiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1"&gt;C V Jawahar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention Toward Neighbors: A Context Aware Framework for High Resolution Image Segmentation. (arXiv:2106.12902v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12902</id>
        <link href="http://arxiv.org/abs/2106.12902"/>
        <updated>2021-06-25T02:00:45.238Z</updated>
        <summary type="html"><![CDATA[High-resolution image segmentation remains challenging and error-prone due to
the enormous size of intermediate feature maps. Conventional methods avoid this
problem by using patch based approaches where each patch is segmented
independently. However, independent patch segmentation induces errors,
particularly at the patch boundary due to the lack of contextual information in
very high-resolution images where the patch size is much smaller compared to
the full image. To overcome these limitations, in this paper, we propose a
novel framework to segment a particular patch by incorporating contextual
information from its neighboring patches. This allows the segmentation network
to see the target patch with a wider field of view without the need of larger
feature maps. Comparative analysis from a number of experiments shows that our
proposed framework is able to segment high resolution images with significantly
improved mean Intersection over Union and overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niloy_F/0/1/0/all/0/1"&gt;Fahim Faisal Niloy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1"&gt;M. Ashraful Amin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1"&gt;Amin Ahsan Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1"&gt;AKM Mahbubur Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VinDr-SpineXR: A deep learning framework for spinal lesions detection and classification from radiographs. (arXiv:2106.12930v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12930</id>
        <link href="http://arxiv.org/abs/2106.12930"/>
        <updated>2021-06-25T02:00:45.228Z</updated>
        <summary type="html"><![CDATA[Radiographs are used as the most important imaging tool for identifying spine
anomalies in clinical practice. The evaluation of spinal bone lesions, however,
is a challenging task for radiologists. This work aims at developing and
evaluating a deep learning-based framework, named VinDr-SpineXR, for the
classification and localization of abnormalities from spine X-rays. First, we
build a large dataset, comprising 10,468 spine X-ray images from 5,000 studies,
each of which is manually annotated by an experienced radiologist with bounding
boxes around abnormal findings in 13 categories. Using this dataset, we then
train a deep learning classifier to determine whether a spine scan is abnormal
and a detector to localize 7 crucial findings amongst the total 13. The
VinDr-SpineXR is evaluated on a test set of 2,078 images from 1,000 studies,
which is kept separate from the training set. It demonstrates an area under the
receiver operating characteristic curve (AUROC) of 88.61% (95% CI 87.19%,
90.02%) for the image-level classification task and a mean average precision
(mAP@0.5) of 33.56% for the lesion-level localization task. These results serve
as a proof of concept and set a baseline for future research in this direction.
To encourage advances, the dataset, codes, and trained deep learning models are
made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hieu T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu H. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Nghia T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha Q. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huynh_T/0/1/0/all/0/1"&gt;Thang Q. Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dao_M/0/1/0/all/0/1"&gt;Minh Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vu_V/0/1/0/all/0/1"&gt;Van Vu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Super-Resolution with Long-Term Self-Exemplars. (arXiv:2106.12778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12778</id>
        <link href="http://arxiv.org/abs/2106.12778"/>
        <updated>2021-06-25T02:00:45.220Z</updated>
        <summary type="html"><![CDATA[Existing video super-resolution methods often utilize a few neighboring
frames to generate a higher-resolution image for each frame. However, the
redundant information between distant frames has not been fully exploited in
these methods: corresponding patches of the same instance appear across distant
frames at different scales. Based on this observation, we propose a video
super-resolution method with long-term cross-scale aggregation that leverages
similar patches (self-exemplars) across distant frames. Our model also consists
of a multi-reference alignment module to fuse the features derived from similar
patches: we fuse the features of distant references to perform high-quality
super-resolution. We also propose a novel and practical training strategy for
referenced-based super-resolution. To evaluate the performance of our proposed
method, we conduct extensive experiments on our collected CarCam dataset and
the Waymo Open dataset, and the results demonstrate our method outperforms
state-of-the-art methods. Our source code will be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1"&gt;Guotao Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sijin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCoM: A Deep Column Mapper for Semantic Data Type Detection. (arXiv:2106.12871v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12871</id>
        <link href="http://arxiv.org/abs/2106.12871"/>
        <updated>2021-06-25T02:00:45.198Z</updated>
        <summary type="html"><![CDATA[Detection of semantic data types is a very crucial task in data science for
automated data cleaning, schema matching, data discovery, semantic data type
normalization and sensitive data identification. Existing methods include
regular expression-based or dictionary lookup-based methods that are not robust
to dirty as well unseen data and are limited to a very less number of semantic
data types to predict. Existing Machine Learning methods extract large number
of engineered features from data and build logistic regression, random forest
or feedforward neural network for this purpose. In this paper, we introduce
DCoM, a collection of multi-input NLP-based deep neural networks to detect
semantic data types where instead of extracting large number of features from
the data, we feed the raw values of columns (or instances) to the model as
texts. We train DCoM on 686,765 data columns extracted from VizNet corpus with
78 different semantic data types. DCoM outperforms other contemporary results
with a quite significant margin on the same dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhadip Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rout_S/0/1/0/all/0/1"&gt;Swapna Sourav Rout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Sudeep Choudhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rate Distortion Characteristic Modeling for Neural Image Compression. (arXiv:2106.12954v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12954</id>
        <link href="http://arxiv.org/abs/2106.12954"/>
        <updated>2021-06-25T02:00:45.183Z</updated>
        <summary type="html"><![CDATA[End-to-end optimization capability offers neural image compression (NIC)
superior lossy compression performance. However, distinct models are required
to be trained to reach different points in the rate-distortion (R-D) space. In
this paper, we consider the problem of R-D characteristic analysis and modeling
for NIC. We make efforts to formulate the essential mathematical functions to
describe the R-D behavior of NIC using deep network and statistical modeling.
Thus continuous bit-rate points could be elegantly realized by leveraging such
model via a single trained network. In this regard, we propose a plugin-in
module to learn the relationship between the target bit-rate and the binary
representation for the latent variable of auto-encoder. Furthermore, we model
the rate and distortion characteristic of NIC as a function of the coding
parameter $\lambda$ respectively. Our experiments show our proposed method is
easy to adopt and obtains competitive coding performance with fixed-rate coding
approaches, which would benefit the practical deployment of NIC. In addition,
the proposed model could be applied to NIC rate control with limited bit-rate
error using a single network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chuanmin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Ziqing Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shanshe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1"&gt;Siwei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Collection of Medical Image Datasets for Deep Learning. (arXiv:2106.12864v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12864</id>
        <link href="http://arxiv.org/abs/2106.12864"/>
        <updated>2021-06-25T02:00:45.167Z</updated>
        <summary type="html"><![CDATA[The astounding success made by artificial intelligence (AI) in healthcare and
other fields proves that AI can achieve human-like performance. However,
success always comes with challenges. Deep learning algorithms are
data-dependent and require large datasets for training. The lack of data in the
medical imaging field creates a bottleneck for the application of deep learning
to medical image analysis. Medical image acquisition, annotation, and analysis
are costly, and their usage is constrained by ethical restrictions. They also
require many resources, such as human expertise and funding. That makes it
difficult for non-medical researchers to have access to useful and large
medical data. Thus, as comprehensive as possible, this paper provides a
collection of medical image datasets with their associated challenges for deep
learning research. We have collected information of around three hundred
datasets and challenges mainly reported between 2013 and 2020 and categorized
them into four categories: head & neck, chest & abdomen, pathology & blood, and
``others''. Our paper has three purposes: 1) to provide a most up to date and
complete list that can be used as a universal reference to easily find the
datasets for clinical image analysis, 2) to guide researchers on the
methodology to test and evaluate their methods' performance and robustness on
relevant datasets, 3) to provide a ``route'' to relevant algorithms for the
relevant medical topics, and challenge leaderboards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Johann Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1"&gt;Guangming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hua_C/0/1/0/all/0/1"&gt;Cong Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1"&gt;Mingtao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+BasheerBennamoun/0/1/0/all/0/1"&gt;BasheerBennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaoyuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1"&gt;Juan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_P/0/1/0/all/0/1"&gt;Peiyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mei_L/0/1/0/all/0/1"&gt;Lin Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shah_S/0/1/0/all/0/1"&gt;Syed Afaq Ali Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Deepfake Videos Using Long Distance Attention. (arXiv:2106.12832v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12832</id>
        <link href="http://arxiv.org/abs/2106.12832"/>
        <updated>2021-06-25T02:00:45.150Z</updated>
        <summary type="html"><![CDATA[With the rapid progress of deepfake techniques in recent years, facial video
forgery can generate highly deceptive video contents and bring severe security
threats. And detection of such forgery videos is much more urgent and
challenging. Most existing detection methods treat the problem as a vanilla
binary classification problem. In this paper, the problem is treated as a
special fine-grained classification problem since the differences between fake
and real faces are very subtle. It is observed that most existing face forgery
methods left some common artifacts in the spatial domain and time domain,
including generative defects in the spatial domain and inter-frame
inconsistencies in the time domain. And a spatial-temporal model is proposed
which has two components for capturing spatial and temporal forgery traces in
global perspective respectively. The two components are designed using a novel
long distance attention mechanism. The one component of the spatial domain is
used to capture artifacts in a single frame, and the other component of the
time domain is used to capture artifacts in consecutive frames. They generate
attention maps in the form of patches. The attention method has a broader
vision which contributes to better assembling global information and extracting
local statistic information. Finally, the attention maps are used to guide the
network to focus on pivotal parts of the face, just like other fine-grained
classification methods. The experimental results on different public datasets
demonstrate that the proposed method achieves the state-of-the-art performance,
and the proposed long distance attention method can effectively capture pivotal
parts for face forgery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Junwei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xianfeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yicong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiwu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Domain Convolutional Neural Network: Accelerated CNN for Large Diabetic Retinopathy Image Classification. (arXiv:2106.12736v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12736</id>
        <link href="http://arxiv.org/abs/2106.12736"/>
        <updated>2021-06-25T02:00:45.143Z</updated>
        <summary type="html"><![CDATA[The conventional spatial convolution layers in the Convolutional Neural
Networks (CNNs) are computationally expensive at the point where the training
time could take days unless the number of layers, the number of training images
or the size of the training images are reduced. The image size of 256x256
pixels is commonly used for most of the applications of CNN, but this image
size is too small for applications like Diabetic Retinopathy (DR)
classification where the image details are important for accurate
classification. This research proposed Frequency Domain Convolution (FDC) and
Frequency Domain Pooling (FDP) layers which were built with RFFT, kernel
initialization strategy, convolution artifact removal and Channel Independent
Convolution (CIC) to replace the conventional convolution and pooling layers.
The FDC and FDP layers are used to build a Frequency Domain Convolutional
Neural Network (FDCNN) to accelerate the training of large images for DR
classification. The Full FDC layer is an extension of the FDC layer to allow
direct use in conventional CNNs, it is also used to modify the VGG16
architecture. FDCNN is shown to be at least 54.21% faster and 70.74% more
memory efficient compared to an equivalent CNN architecture. The modified VGG16
architecture with Full FDC layer is reported to achieve a shorter training time
and a higher accuracy at 95.63% compared to the original VGG16 architecture for
DR classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1"&gt;Ee Fey Goh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;ZhiYuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_W/0/1/0/all/0/1"&gt;Wei Xiang Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal 3D Object Detection in Autonomous Driving: a Survey. (arXiv:2106.12735v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12735</id>
        <link href="http://arxiv.org/abs/2106.12735"/>
        <updated>2021-06-25T02:00:45.127Z</updated>
        <summary type="html"><![CDATA[In the past few years, we have witnessed rapid development of autonomous
driving. However, achieving full autonomy remains a daunting task due to the
complex and dynamic driving environment. As a result, self-driving cars are
equipped with a suite of sensors to conduct robust and accurate environment
perception. As the number and type of sensors keep increasing, combining them
for better perception is becoming a natural trend. So far, there has been no
indepth review that focuses on multi-sensor fusion based perception. To bridge
this gap and motivate future research, this survey devotes to review recent
fusion-based 3D detection deep learning models that leverage multiple sensor
data sources, especially cameras and LiDARs. In this survey, we first introduce
the background of popular sensors for autonomous cars, including their common
data representations as well as object detection networks developed for each
type of sensor data. Next, we discuss some popular datasets for multi-modal 3D
object detection, with a special focus on the sensor data included in each
dataset. Then we present in-depth reviews of recent multi-modal 3D detection
networks by considering the following three aspects of the fusion: fusion
location, fusion data representation, and fusion granularity. After a detailed
review, we discuss open challenges and point out possible solutions. We hope
that our detailed review can help researchers to embark investigations in the
area of multi-modal 3D object detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yingjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1"&gt;Qiuyu Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hanqi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jianmin Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanyong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planetary UAV localization based on Multi-modal Registration with Pre-existing Digital Terrain Model. (arXiv:2106.12738v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12738</id>
        <link href="http://arxiv.org/abs/2106.12738"/>
        <updated>2021-06-25T02:00:45.120Z</updated>
        <summary type="html"><![CDATA[The autonomous real-time optical navigation of planetary UAV is of the key
technologies to ensure the success of the exploration. In such a GPS denied
environment, vision-based localization is an optimal approach. In this paper,
we proposed a multi-modal registration based SLAM algorithm, which estimates
the location of a planet UAV using a nadir view camera on the UAV compared with
pre-existing digital terrain model. To overcome the scale and appearance
difference between on-board UAV images and pre-installed digital terrain model,
a theoretical model is proposed to prove that topographic features of UAV image
and DEM can be correlated in frequency domain via cross power spectrum. To
provide the six-DOF of the UAV, we also developed an optimization approach
which fuses the geo-referencing result into a SLAM system via LBA (Local Bundle
Adjustment) to achieve robust and accurate vision-based navigation even in
featureless planetary areas. To test the robustness and effectiveness of the
proposed localization algorithm, a new cross-source drone-based localization
dataset for planetary exploration is proposed. The proposed dataset includes
40200 synthetic drone images taken from nine planetary scenes with related DEM
query images. Comparison experiments carried out demonstrate that over the
flight distance of 33.8km, the proposed method achieved average localization
error of 0.45 meters, compared to 1.31 meters by ORB-SLAM, with the processing
speed of 12hz which will ensure a real-time performance. We will make our
datasets available to encourage further work on this promising topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xue Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yuanbin Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shengyang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Novelty Detection. (arXiv:2106.12964v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12964</id>
        <link href="http://arxiv.org/abs/2106.12964"/>
        <updated>2021-06-25T02:00:45.114Z</updated>
        <summary type="html"><![CDATA[Novelty Detection methods identify samples that are not representative of a
model's training set thereby flagging misleading predictions and bringing a
greater flexibility and transparency at deployment time. However, research in
this area has only considered Novelty Detection in the offline setting.
Recently, there has been a growing realization in the computer vision community
that applications demand a more flexible framework - Continual Learning - where
new batches of data representing new domains, new classes or new tasks become
available at different points in time. In this setting, Novelty Detection
becomes more important, interesting and challenging. This work identifies the
crucial link between the two problems and investigates the Novelty Detection
problem under the Continual Learning setting. We formulate the Continual
Novelty Detection problem and present a benchmark, where we compare several
Novelty Detection methods under different Continual Learning settings.

We show that Continual Learning affects the behaviour of novelty detection
algorithms, while novelty detection can pinpoint insights in the behaviour of a
continual learner. We further propose baselines and discuss possible research
directions. We believe that the coupling of the two problems is a promising
direction to bring vision models into practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1"&gt;Rahaf Aljundi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1"&gt;Daniel Olmeda Reino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chumerin_N/0/1/0/all/0/1"&gt;Nikolay Chumerin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1"&gt;Richard E. Turner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What makes visual place recognition easy or hard?. (arXiv:2106.12671v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12671</id>
        <link href="http://arxiv.org/abs/2106.12671"/>
        <updated>2021-06-25T02:00:45.108Z</updated>
        <summary type="html"><![CDATA[Visual place recognition is a fundamental capability for the localization of
mobile robots. It places image retrieval in the practical context of physical
agents operating in a physical world. It is an active field of research and
many different approaches have been proposed and evaluated in many different
experiments. In the following, we argue that due to variations of this
practical context and individual design decisions, place recognition
experiments are barely comparable across different papers and that there is a
variety of properties that can change from one experiment to another. We
provide an extensive list of such properties and give examples how they can be
used to setup a place recognition experiment easier or harder. This might be
interesting for different involved parties: (1) people who just want to select
a place recognition approach that is suitable for the properties of their
particular task at hand, (2) researchers that look for open research questions
and are interested in particularly difficult instances, (3) authors that want
to create reproducible papers on this topic, and (4) also reviewers that have
the task to identify potential problems in papers under review.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_S/0/1/0/all/0/1"&gt;Stefan Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubert_P/0/1/0/all/0/1"&gt;Peer Neubert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Deep Image Stitching: Reconstructing Stitched Features to Images. (arXiv:2106.12859v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12859</id>
        <link href="http://arxiv.org/abs/2106.12859"/>
        <updated>2021-06-25T02:00:45.101Z</updated>
        <summary type="html"><![CDATA[Traditional feature-based image stitching technologies rely heavily on
feature detection quality, often failing to stitch images with few features or
low resolution. The learning-based image stitching solutions are rarely studied
due to the lack of labeled data, making the supervised methods unreliable. To
address the above limitations, we propose an unsupervised deep image stitching
framework consisting of two stages: unsupervised coarse image alignment and
unsupervised image reconstruction. In the first stage, we design an
ablation-based loss to constrain an unsupervised homography network, which is
more suitable for large-baseline scenes. Moreover, a transformer layer is
introduced to warp the input images in the stitching-domain space. In the
second stage, motivated by the insight that the misalignments in pixel-level
can be eliminated to a certain extent in feature-level, we design an
unsupervised image reconstruction network to eliminate the artifacts from
features to pixels. Specifically, the reconstruction network can be implemented
by a low-resolution deformation branch and a high-resolution refined branch,
learning the deformation rules of image stitching and enhancing the resolution
simultaneously. To establish an evaluation benchmark and train the learning
framework, a comprehensive real-world image dataset for unsupervised deep image
stitching is presented and released. Extensive experiments well demonstrate the
superiority of our method over other state-of-the-art solutions. Even compared
with the supervised solutions, our image stitching quality is still preferred
by users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Lang Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chunyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1"&gt;Kang Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuaicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Florida Wildlife Camera Trap Dataset. (arXiv:2106.12628v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12628</id>
        <link href="http://arxiv.org/abs/2106.12628"/>
        <updated>2021-06-25T02:00:45.078Z</updated>
        <summary type="html"><![CDATA[Trail camera imagery has increasingly gained popularity amongst biologists
for conservation and ecological research. Minimal human interference required
to operate camera traps allows capturing unbiased species activities. Several
studies - based on human and wildlife interactions, migratory patterns of
various species, risk of extinction in endangered populations - are limited by
the lack of rich data and the time-consuming nature of manually annotating
trail camera imagery. We introduce a challenging wildlife camera trap
classification dataset collected from two different locations in Southwestern
Florida, consisting of 104,495 images featuring visually similar species,
varying illumination conditions, skewed class distribution, and including
samples of endangered species, i.e. Florida panthers. Experimental evaluations
with ResNet-50 architecture indicate that this image classification-based
dataset can further push the advancements in wildlife statistical modeling. We
will make the dataset publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1"&gt;Crystal Gagne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kini_J/0/1/0/all/0/1"&gt;Jyoti Kini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1"&gt;Daniel Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All You Need is a Second Look: Towards Arbitrary-Shaped Text Detection. (arXiv:2106.12720v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12720</id>
        <link href="http://arxiv.org/abs/2106.12720"/>
        <updated>2021-06-25T02:00:45.072Z</updated>
        <summary type="html"><![CDATA[Arbitrary-shaped text detection is a challenging task since curved texts in
the wild are of the complex geometric layouts. Existing mainstream methods
follow the instance segmentation pipeline to obtain the text regions. However,
arbitraryshaped texts are difficult to be depicted through one single
segmentation network because of the varying scales. In this paper, we propose a
two-stage segmentation-based detector, termed as NASK (Need A Second looK), for
arbitrary-shaped text detection. Compared to the traditional single-stage
segmentation network, our NASK conducts the detection in a coarse-to-fine
manner with the first stage segmentation spotting the rectangle text proposals
and the second one retrieving compact representations. Specifically, NASK is
composed of a Text Instance Segmentation (TIS) network (1st stage), a
Geometry-aware Text RoI Alignment (GeoAlign) module, and a Fiducial pOint
eXpression (FOX) module (2nd stage). Firstly, TIS extracts the augmented
features with a novel Group Spatial and Channel Attention (GSCA) module and
conducts instance segmentation to obtain rectangle proposals. Then, GeoAlign
converts these rectangles into the fixed size and encodes RoI-wise feature
representation. Finally, FOX disintegrates the text instance into serval
pivotal geometrical attributes to refine the detection results. Extensive
experimental results on three public benchmarks including Total-Text,
SCUTCTW1500, and ICDAR 2015 verify that our NASK outperforms recent
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1"&gt;Meng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Can Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dongming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATP-Net: An Attention-based Ternary Projection Network For Compressed Sensing. (arXiv:2106.12728v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.12728</id>
        <link href="http://arxiv.org/abs/2106.12728"/>
        <updated>2021-06-25T02:00:45.044Z</updated>
        <summary type="html"><![CDATA[Compressed Sensing (CS) theory simultaneously realizes the signal sampling
and compression process, and can use fewer observations to achieve accurate
signal recovery, providing a solution for better and faster transmission of
massive data. In this paper, a ternary sampling matrix-based method with
attention mechanism is proposed with the purpose to solve the problem that the
CS sampling matrices in most cases are random matrices, which are irrelative to
the sampled signal and need a large storage space. The proposed method consists
of three components, i.e., ternary sampling, initial reconstruction and deep
reconstruction, with the emphasis on the ternary sampling. The main idea of the
ternary method (-1, 0, +1) is to introduce the attention mechanism to evaluate
the importance of parameters at the sampling layer after the sampling matrix is
binarized (-1, +1), followed by pruning weight of parameters, whose importance
is below a predefined threshold, to achieve ternarization. Furthermore, a
compressed sensing algorithm especially for image reconstruction is
implemented, on the basis of the ternary sampling matrix, which is called
ATP-Net, i.e., Attention-based ternary projection network. Experimental results
show that the quality of image reconstruction by means of ATP-Net maintains a
satisfactory level with the employment of the ternary sampling matrix, i.e.,
the average PSNR on Set11 is 30.4 when the sampling rate is 0.25, approximately
6% improvement compared with that of DR2-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nie_G/0/1/0/all/0/1"&gt;Guanxiong Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yajian Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Activity Recognition using Continuous Wavelet Transform and Convolutional Neural Networks. (arXiv:2106.12666v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12666</id>
        <link href="http://arxiv.org/abs/2106.12666"/>
        <updated>2021-06-25T02:00:45.035Z</updated>
        <summary type="html"><![CDATA[Quite a few people in the world have to stay under permanent surveillance for
health reasons; they include diabetic people or people with some other chronic
conditions, the elderly and the disabled.These groups may face heightened risk
of having life-threatening falls or of being struck by a syncope. Due to
limited availability of resources a substantial part of people at risk can not
receive necessary monitoring and thus are exposed to excessive danger.
Nowadays, this problem is usually solved via applying Human Activity
Recognition (HAR) methods. HAR is a perspective and fast-paced Data Science
field, which has a wide range of application areas such as healthcare, sport,
security etc. However, the currently techniques of recognition are markedly
lacking in accuracy, hence, the present paper suggests a highly accurate method
for human activity classification. Wepropose a new workflow to address the HAR
problem and evaluate it on the UniMiB SHAR dataset, which consists of the
accelerometer signals. The model we suggest is based on continuous wavelet
transform (CWT) and convolutional neural networks (CNNs). Wavelet transform
localizes signal features both in time and frequency domains and after that a
CNN extracts these features and recognizes activity. It is also worth noting
that CWT converts 1D accelerometer signal into 2D images and thus enables to
obtain better results as 2D networks have a significantly higher predictive
capacity. In the course of the work we build a convolutional neural network and
vary such model parameters as number of spatial axes, number of layers, number
of neurons in each layer, image size, type of mother wavelet, the order of zero
moment of mother wavelet etc. Besides, we also apply models with residual
blocks which resulted in significantly higher metric values. Finally, we
succeed to reach 99.26 % accuracy and it is a worthy performance for this
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nedorubova_A/0/1/0/all/0/1"&gt;Anna Nedorubova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadyrova_A/0/1/0/all/0/1"&gt;Alena Kadyrova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khlyupin_A/0/1/0/all/0/1"&gt;Aleksey Khlyupin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Semantic Mapping by Consolidation of Deep Visual Features. (arXiv:2106.12709v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12709</id>
        <link href="http://arxiv.org/abs/2106.12709"/>
        <updated>2021-06-25T02:00:45.015Z</updated>
        <summary type="html"><![CDATA[Many works in the recent literature introduce semantic mapping methods that
use CNNs (Convolutional Neural Networks) to recognize semantic properties in
images. The types of properties (eg.: room size, place category, and objects)
and their classes (eg.: kitchen and bathroom, for place category) are usually
predefined and restricted to a specific task. Thus, all the visual data
acquired and processed during the construction of the maps are lost and only
the recognized semantic properties remain on the maps. In contrast, this work
introduces a topological semantic mapping method that uses deep visual features
extracted by a CNN, the GoogLeNet, from 2D images captured in multiple views of
the environment as the robot operates, to create consolidated representations
of visual features acquired in the regions covered by each topological node.
These consolidated representations allow flexible recognition of semantic
properties of the regions and use in a range of visual tasks. The experiments,
performed using a real-world indoor dataset, showed that the method is able to
consolidate the visual features of regions and use them to recognize objects
and place categories as semantic properties, and to indicate the topological
location of images, with very promising results. The objects are classified
using the classification layer of GoogLeNet, without retraining, and the place
categories are recognized using a shallow Multilayer Perceptron.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sousa_Y/0/1/0/all/0/1"&gt;Ygor C. N. Sousa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1"&gt;Hansenclever F. Bassani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-25T02:00:44.993Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Deformable Image Registration with Convolutional Neural Network. (arXiv:2106.12673v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12673</id>
        <link href="http://arxiv.org/abs/2106.12673"/>
        <updated>2021-06-25T02:00:44.981Z</updated>
        <summary type="html"><![CDATA[Recent deep learning-based methods have shown promising results and runtime
advantages in deformable image registration. However, analyzing the effects of
hyperparameters and searching for optimal regularization parameters prove to be
too prohibitive in deep learning-based methods. This is because it involves
training a substantial number of separate models with distinct hyperparameter
values. In this paper, we propose a conditional image registration method and a
new self-supervised learning paradigm for deep deformable image registration.
By learning the conditional features that correlated with the regularization
hyperparameter, we demonstrate that optimal solutions with arbitrary
hyperparameters can be captured by a single deep convolutional neural network.
In addition, the smoothness of the resulting deformation field can be
manipulated with arbitrary strength of smoothness regularization during
inference. Extensive experiments on a large-scale brain MRI dataset show that
our proposed method enables the precise control of the smoothness of the
deformation field without sacrificing the runtime advantage or registration
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1"&gt;Tony C. W. Mok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_A/0/1/0/all/0/1"&gt;Albert C. S. Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Perturb Word Embeddings for Out-of-distribution QA. (arXiv:2105.02692v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02692</id>
        <link href="http://arxiv.org/abs/2105.02692"/>
        <updated>2021-06-25T02:00:44.973Z</updated>
        <summary type="html"><![CDATA[QA models based on pretrained language mod-els have achieved remarkable
performance on various benchmark datasets.However, QA models do not generalize
well to unseen data that falls outside the training distribution, due to
distributional shifts.Data augmentation (DA) techniques which drop/replace
words have shown to be effective in regularizing the model from overfitting to
the training data.Yet, they may adversely affect the QA tasks since they incur
semantic changes that may lead to wrong answers for the QA task. To tackle this
problem, we propose a simple yet effective DA method based on a stochastic
noise generator, which learns to perturb the word embedding of the input
questions and context without changing their semantics. We validate the
performance of the QA models trained with our word embedding perturbation on a
single source dataset, on five different target domains.The results show that
our method significantly outperforms the baselineDA methods. Notably, the model
trained with ours outperforms the model trained with more than 240K
artificially generated QA pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seanie Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Minki Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Juho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04641</id>
        <link href="http://arxiv.org/abs/2106.04641"/>
        <updated>2021-06-25T02:00:44.963Z</updated>
        <summary type="html"><![CDATA[Transfer learning methods, and in particular domain adaptation, help exploit
labeled data in one domain to improve the performance of a certain task in
another domain. However, it is still not clear what factors affect the success
of domain adaptation. This paper models adaptation success and selection of the
most suitable source domains among several candidates in text similarity. We
use descriptive domain information and cross-domain similarity metrics as
predictive features. While mostly positive, the results also point to some
domains where adaptation success was difficult to predict.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1"&gt;Nicolai Pogrebnyakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1"&gt;Shohreh Shaghaghian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context Transformer with Stacked Pointer Networks for Conversational Question Answering over Knowledge Graphs. (arXiv:2103.07766v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07766</id>
        <link href="http://arxiv.org/abs/2103.07766"/>
        <updated>2021-06-25T02:00:44.946Z</updated>
        <summary type="html"><![CDATA[Neural semantic parsing approaches have been widely used for Question
Answering (QA) systems over knowledge graphs. Such methods provide the
flexibility to handle QA datasets with complex queries and a large number of
entities. In this work, we propose a novel framework named CARTON, which
performs multi-task semantic parsing for handling the problem of conversational
question answering over a large-scale knowledge graph. Our framework consists
of a stack of pointer networks as an extension of a context transformer model
for parsing the input question and the dialog history. The framework generates
a sequence of actions that can be executed on the knowledge graph. We evaluate
CARTON on a standard dataset for complex sequential question answering on which
CARTON outperforms all baselines. Specifically, we observe performance
improvements in F1-score on eight out of ten question types compared to the
previous state of the art. For logical reasoning questions, an improvement of
11 absolute points is reached.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1"&gt;Joan Plepi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1"&gt;Endri Kacupaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kuldeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakkar_H/0/1/0/all/0/1"&gt;Harsh Thakkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues. (arXiv:2106.01006v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01006</id>
        <link href="http://arxiv.org/abs/2106.01006"/>
        <updated>2021-06-25T02:00:44.940Z</updated>
        <summary type="html"><![CDATA[Inferring social relations from dialogues is vital for building emotionally
intelligent robots to interpret human language better and act accordingly. We
model the social network as an And-or Graph, named SocAoG, for the consistency
of relations among a group and leveraging attributes as inference cues.
Moreover, we formulate a sequential structure prediction task, and propose an
$\alpha$-$\beta$-$\gamma$ strategy to incrementally parse SocAoG for the
dynamic inference upon any incoming utterance: (i) an $\alpha$ process
predicting attributes and relations conditioned on the semantics of dialogues,
(ii) a $\beta$ process updating the social relations based on related
attributes, and (iii) a $\gamma$ process updating individual's attributes based
on interpersonal social relations. Empirical results on DialogRE and MovieGraph
show that our model infers social relations more accurately than the
state-of-the-art methods. Moreover, the ablation study shows the three
processes complement each other, and the case study demonstrates the dynamic
relational inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1"&gt;Liang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yizhou Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Baolin Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding and Mitigating Social Biases in Language Models. (arXiv:2106.13219v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13219</id>
        <link href="http://arxiv.org/abs/2106.13219"/>
        <updated>2021-06-25T02:00:44.927Z</updated>
        <summary type="html"><![CDATA[As machine learning methods are deployed in real-world settings such as
healthcare, legal systems, and social science, it is crucial to recognize how
they shape social biases and stereotypes in these sensitive decision-making
processes. Among such real-world deployments are large-scale pretrained
language models (LMs) that can be potentially dangerous in manifesting
undesirable representational biases - harmful biases resulting from
stereotyping that propagate negative generalizations involving gender, race,
religion, and other social constructs. As a step towards improving the fairness
of LMs, we carefully define several sources of representational biases before
proposing new benchmarks and metrics to measure them. With these tools, we
propose steps towards mitigating social biases during text generation. Our
empirical results and human evaluation demonstrate effectiveness in mitigating
bias while retaining crucial contextual information for high-fidelity text
generation, thereby pushing forward the performance-fairness Pareto frontier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chiyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-25T02:00:44.921Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Orthogonal Constraint in Structural Probes. (arXiv:2012.15228v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15228</id>
        <link href="http://arxiv.org/abs/2012.15228"/>
        <updated>2021-06-25T02:00:44.903Z</updated>
        <summary type="html"><![CDATA[With the recent success of pre-trained models in NLP, a significant focus was
put on interpreting their representations. One of the most prominent approaches
is structural probing (Hewitt and Manning, 2019), where a linear projection of
word embeddings is performed in order to approximate the topology of dependency
structures. In this work, we introduce a new type of structural probing, where
the linear projection is decomposed into 1. isomorphic space rotation; 2.
linear scaling that identifies and scales the most relevant dimensions. In
addition to syntactic dependency, we evaluate our method on novel tasks
(lexical hypernymy and position in a sentence). We jointly train the probes for
multiple tasks and experimentally show that lexical and syntactic information
is separated in the representations. Moreover, the orthogonal constraint makes
the Structural Probes less vulnerable to memorization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1"&gt;Tomasz Limisiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1"&gt;David Mare&amp;#x10d;ek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Carrier Recognition from Personal Narratives. (arXiv:2008.07481v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07481</id>
        <link href="http://arxiv.org/abs/2008.07481"/>
        <updated>2021-06-25T02:00:44.896Z</updated>
        <summary type="html"><![CDATA[Personal Narratives (PN) - recollections of facts, events, and thoughts from
one's own experience - are often used in everyday conversations. So far, PNs
have mainly been explored for tasks such as valence prediction or emotion
classification (e.g. happy, sad). However, these tasks might overlook more
fine-grained information that could prove to be relevant for understanding PNs.
In this work, we propose a novel task for Narrative Understanding: Emotion
Carrier Recognition (ECR). Emotion carriers, the text fragments that carry the
emotions of the narrator (e.g. loss of a grandpa, high school reunion), provide
a fine-grained description of the emotion state. We explore the task of ECR in
a corpus of PNs manually annotated with emotion carriers and investigate
different machine learning models for the task. We propose evaluation
strategies for ECR including metrics that can be appropriate for different
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tammewar_A/0/1/0/all/0/1"&gt;Aniruddha Tammewar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cervone_A/0/1/0/all/0/1"&gt;Alessandra Cervone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riccardi_G/0/1/0/all/0/1"&gt;Giuseppe Riccardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data. (arXiv:2106.13213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13213</id>
        <link href="http://arxiv.org/abs/2106.13213"/>
        <updated>2021-06-25T02:00:44.881Z</updated>
        <summary type="html"><![CDATA[Mental health conditions remain underdiagnosed even in countries with common
access to advanced medical care. The ability to accurately and efficiently
predict mood from easily collectible data has several important implications
for the early detection, intervention, and treatment of mental health
disorders. One promising data source to help monitor human behavior is daily
smartphone usage. However, care must be taken to summarize behaviors without
identifying the user through personal (e.g., personally identifiable
information) or protected (e.g., race, gender) attributes. In this paper, we
study behavioral markers of daily mood using a recent dataset of mobile
behaviors from adolescent populations at high risk of suicidal behaviors. Using
computational models, we find that language and multimodal representations of
mobile typed text (spanning typed characters, words, keystroke timings, and app
usage) are predictive of daily mood. However, we find that models trained to
predict mood often also capture private user identities in their intermediate
representations. To tackle this problem, we evaluate approaches that obfuscate
user identity while remaining predictive. By combining multimodal
representations with privacy-preserving learning, we are able to push forward
the performance-privacy frontier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Terrance Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_A/0/1/0/all/0/1"&gt;Anna Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muszynski_M/0/1/0/all/0/1"&gt;Michal Muszynski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_R/0/1/0/all/0/1"&gt;Ryo Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1"&gt;Nicholas Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1"&gt;Randy Auerbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brent_D/0/1/0/all/0/1"&gt;David Brent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021. (arXiv:2106.13033v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13033</id>
        <link href="http://arxiv.org/abs/2106.13033"/>
        <updated>2021-06-25T02:00:44.853Z</updated>
        <summary type="html"><![CDATA[In this paper, inspired by the successes of visionlanguage pre-trained models
and the benefits from training with adversarial attacks, we present a novel
transformerbased cross-modal fusion modeling by incorporating the both notions
for VQA challenge 2021. Specifically, the proposed model is on top of the
architecture of VinVL model [19], and the adversarial training strategy [4] is
applied to make the model robust and generalized. Moreover, two implementation
tricks are also used in our system to obtain better results. The experiments
demonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke-Han Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1"&gt;Bo-Han Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kuan-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Study of Transformers for Source Code. (arXiv:2010.07987v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07987</id>
        <link href="http://arxiv.org/abs/2010.07987"/>
        <updated>2021-06-25T02:00:44.846Z</updated>
        <summary type="html"><![CDATA[Initially developed for natural language processing (NLP), Transformers are
now widely used for source code processing, due to the format similarity
between source code and text. In contrast to natural language, source code is
strictly structured, i.e., it follows the syntax of the programming language.
Several recent works develop Transformer modifications for capturing syntactic
information in source code. The drawback of these works is that they do not
compare to each other and consider different tasks. In this work, we conduct a
thorough empirical study of the capabilities of Transformers to utilize
syntactic information in different tasks. We consider three tasks (code
completion, function naming and bug fixing) and re-implement different
syntax-capturing modifications in a unified framework. We show that
Transformers are able to make meaningful predictions based purely on syntactic
information and underline the best practices of taking the syntactic
information into account for improving the performance of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1"&gt;Nadezhda Chirkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1"&gt;Sergey Troshin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UXLA: A Robust Unsupervised Data Augmentation Framework for {Zero-Resource} Cross-Lingual NLP. (arXiv:2004.13240v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13240</id>
        <link href="http://arxiv.org/abs/2004.13240"/>
        <updated>2021-06-25T02:00:44.829Z</updated>
        <summary type="html"><![CDATA[Transfer learning has yielded state-of-the-art (SoTA) results in many
supervised NLP tasks. However, annotated data for every target task in every
target language is rare, especially for low-resource languages. We propose
UXLA, a novel unsupervised data augmentation framework for zero-resource
transfer learning scenarios. In particular, UXLA aims to solve cross-lingual
adaptation problems from a source language task distribution to an unknown
target language task distribution, assuming no training label in the target
language. At its core, UXLA performs simultaneous self-training with data
augmentation and unsupervised sample selection. To show its effectiveness, we
conduct extensive experiments on three diverse zero-resource cross-lingual
transfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the
baselines by a good margin. With an in-depth framework dissection, we
demonstrate the cumulative contributions of different components to its
success.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1"&gt;M Saiful Bari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1"&gt;Tasnim Mohiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QASR: QCRI Aljazeera Speech Resource -- A Large Scale Annotated Arabic Speech Corpus. (arXiv:2106.13000v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13000</id>
        <link href="http://arxiv.org/abs/2106.13000"/>
        <updated>2021-06-25T02:00:44.817Z</updated>
        <summary type="html"><![CDATA[We introduce the largest transcribed Arabic speech corpus, QASR, collected
from the broadcast domain. This multi-dialect speech dataset contains 2,000
hours of speech sampled at 16kHz crawled from Aljazeera news channel. The
dataset is released with lightly supervised transcriptions, aligned with the
audio segments. Unlike previous datasets, QASR contains linguistically
motivated segmentation, punctuation, speaker information among others. QASR is
suitable for training and evaluating speech recognition systems, acoustics-
and/or linguistics- based Arabic dialect identification, punctuation
restoration, speaker identification, speaker linking, and potentially other NLP
modules for spoken data. In addition to QASR transcription, we release a
dataset of 130M words to aid in designing and training a better language model.
We show that end-to-end automatic speech recognition trained on QASR reports a
competitive word error rate compared to the previous MGB-2 corpus. We report
baseline results for downstream natural language processing tasks such as named
entity recognition using speech transcript. We also report the first baseline
for Arabic punctuation restoration. We make the corpus available for the
research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1"&gt;Hamdy Mubarak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1"&gt;Amir Hussein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Shammur Absar Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1"&gt;Ahmed Ali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining NLP Models via Minimal Contrastive Editing (MiCE). (arXiv:2012.13985v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13985</id>
        <link href="http://arxiv.org/abs/2012.13985"/>
        <updated>2021-06-25T02:00:44.796Z</updated>
        <summary type="html"><![CDATA[Humans have been shown to give contrastive explanations, which explain why an
observed event happened rather than some other counterfactual event (the
contrast case). Despite the influential role that contrastivity plays in how
humans explain, this property is largely missing from current methods for
explaining NLP models. We present Minimal Contrastive Editing (MiCE), a method
for producing contrastive explanations of model predictions in the form of
edits to inputs that change model outputs to the contrast case. Our experiments
across three tasks--binary sentiment classification, topic classification, and
multiple-choice question answering--show that MiCE is able to produce edits
that are not only contrastive, but also minimal and fluent, consistent with
human contrastive edits. We demonstrate how MiCE edits can be used for two use
cases in NLP system development--debugging incorrect model outputs and
uncovering dataset artifacts--and thereby illustrate that producing contrastive
explanations is a promising research direction for model interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1"&gt;Alexis Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1"&gt;Ana Marasovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1"&gt;Matthew E. Peters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where are we in semantic concept extraction for Spoken Language Understanding?. (arXiv:2106.13045v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13045</id>
        <link href="http://arxiv.org/abs/2106.13045"/>
        <updated>2021-06-25T02:00:44.789Z</updated>
        <summary type="html"><![CDATA[Spoken language understanding (SLU) topic has seen a lot of progress these
last three years, with the emergence of end-to-end neural approaches. Spoken
language understanding refers to natural language processing tasks related to
semantic extraction from speech signal, like named entity recognition from
speech or slot filling task in a context of human-machine dialogue.
Classically, SLU tasks were processed through a cascade approach that consists
in applying, firstly, an automatic speech recognition process, followed by a
natural language processing module applied to the automatic transcriptions.
These three last years, end-to-end neural approaches, based on deep neural
networks, have been proposed in order to directly extract the semantics from
speech signal, by using a single neural model. More recent works on
self-supervised training with unlabeled data open new perspectives in term of
performance for automatic speech recognition and natural language processing.
In this paper, we present a brief overview of the recent advances on the French
MEDIA benchmark dataset for SLU, with or without the use of additional data. We
also present our last results that significantly outperform the current
state-of-the-art with a Concept Error Rate (CER) of 11.2%, instead of 13.6% for
the last state-of-the-art system presented this year.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghannay_S/0/1/0/all/0/1"&gt;Sahar Ghannay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caubriere_A/0/1/0/all/0/1"&gt;Antoine Caubri&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mdhaffar_S/0/1/0/all/0/1"&gt;Salima Mdhaffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laperriere_G/0/1/0/all/0/1"&gt;Ga&amp;#xeb;lle Laperri&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jabaian_B/0/1/0/all/0/1"&gt;Bassam Jabaian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1"&gt;Yannick Est&amp;#xe8;ve&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry. (arXiv:2106.12944v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12944</id>
        <link href="http://arxiv.org/abs/2106.12944"/>
        <updated>2021-06-25T02:00:44.771Z</updated>
        <summary type="html"><![CDATA[Recent advances in transformers have enabled Table Question Answering (Table
QA) systems to achieve high accuracy and SOTA results on open domain datasets
like WikiTableQuestions and WikiSQL. Such transformers are frequently
pre-trained on open-domain content such as Wikipedia, where they effectively
encode questions and corresponding tables from Wikipedia as seen in Table QA
dataset. However, web tables in Wikipedia are notably flat in their layout,
with the first row as the sole column header. The layout lends to a relational
view of tables where each row is a tuple. Whereas, tables in domain-specific
business or scientific documents often have a much more complex layout,
including hierarchical row and column headers, in addition to having
specialized vocabulary terms from that domain.

To address this problem, we introduce the domain-specific Table QA dataset
AIT-QA (Airline Industry Table QA). The dataset consists of 515 questions
authored by human annotators on 116 tables extracted from public U.S. SEC
filings (publicly available at: https://www.sec.gov/edgar.shtml) of major
airline companies for the fiscal years 2017-2019. We also provide annotations
pertaining to the nature of questions, marking those that require hierarchical
headers, domain-specific terminology, and paraphrased forms. Our zero-shot
baseline evaluation of three transformer-based SOTA Table QA methods - TaPAS
(end-to-end), TaBERT (semantic parsing-based), and RCI (row-column
encoding-based) - clearly exposes the limitation of these methods in this
practical setting, with the best accuracy at just 51.8\% (RCI). We also present
pragmatic table preprocessing steps used to pivot and project these complex
tables into a layout suitable for the SOTA Table QA models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Katsis_Y/0/1/0/all/0/1"&gt;Yannis Katsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1"&gt;Saneem Chemmengath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vishwajeet Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1"&gt;Samarth Bharadwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canim_M/0/1/0/all/0/1"&gt;Mustafa Canim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1"&gt;Michael Glass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1"&gt;Alfio Gliozzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feifei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1"&gt;Jaydeep Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1"&gt;Karthik Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1"&gt;Soumen Chakrabarti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Self-Identified Counseling Expertise in Online Support Forums. (arXiv:2106.12976v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12976</id>
        <link href="http://arxiv.org/abs/2106.12976"/>
        <updated>2021-06-25T02:00:44.764Z</updated>
        <summary type="html"><![CDATA[A growing number of people engage in online health forums, making it
important to understand the quality of the advice they receive. In this paper,
we explore the role of expertise in responses provided to help-seeking posts
regarding mental health. We study the differences between (1) interactions with
peers; and (2) interactions with self-identified mental health professionals.
First, we show that a classifier can distinguish between these two groups,
indicating that their language use does in fact differ. To understand this
difference, we perform several analyses addressing engagement aspects,
including whether their comments engage the support-seeker further as well as
linguistic aspects, such as dominant language and linguistic style matching.
Our work contributes toward the developing efforts of understanding how health
experts engage with health information- and support-seekers in social networks.
More broadly, it is a step toward a deeper understanding of the styles of
interactions that cultivate supportive engagement in online communities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahnala_A/0/1/0/all/0/1"&gt;Allison Lahnala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuntian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1"&gt;Charles Welch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1"&gt;Jonathan K. Kummerfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1"&gt;Lawrence An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resnicow_K/0/1/0/all/0/1"&gt;Kenneth Resnicow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Rosas_V/0/1/0/all/0/1"&gt;Ver&amp;#xf3;nica P&amp;#xe9;rez-Rosas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Topic Segmentation of Meetings with BERT Embeddings. (arXiv:2106.12978v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12978</id>
        <link href="http://arxiv.org/abs/2106.12978"/>
        <updated>2021-06-25T02:00:44.700Z</updated>
        <summary type="html"><![CDATA[Topic segmentation of meetings is the task of dividing multi-person meeting
transcripts into topic blocks. Supervised approaches to the problem have proven
intractable due to the difficulties in collecting and accurately annotating
large datasets. In this paper we show how previous unsupervised topic
segmentation methods can be improved using pre-trained neural architectures. We
introduce an unsupervised approach based on BERT embeddings that achieves a
15.5% reduction in error rate over existing unsupervised approaches applied to
two popular datasets for meeting transcripts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solbiati_A/0/1/0/all/0/1"&gt;Alessandro Solbiati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1"&gt;Kevin Heffernan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damaskinos_G/0/1/0/all/0/1"&gt;Georgios Damaskinos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1"&gt;Shivani Poddar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1"&gt;Shubham Modi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cali_J/0/1/0/all/0/1"&gt;Jacques Cali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OKGIT: Open Knowledge Graph Link Prediction with Implicit Types. (arXiv:2106.12806v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12806</id>
        <link href="http://arxiv.org/abs/2106.12806"/>
        <updated>2021-06-25T02:00:44.690Z</updated>
        <summary type="html"><![CDATA[Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation
phrase, tail noun phrase) triples such as (tesla, return to, new york)
extracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap
for a domain, they are very sparse and far from being directly usable in an end
task. Therefore, the task of predicting new facts, i.e., link prediction,
becomes an important step while using these graphs in downstream tasks such as
text comprehension, question answering, and web search query recommendation.
Learning embeddings for OpenKGs is one approach for link prediction that has
received some attention lately. However, on careful examination, we found that
current OpenKG link prediction algorithms often predict noun phrases (NPs) with
incompatible types for given noun and relation phrases. We address this problem
in this work and propose OKGIT that improves OpenKG link prediction using novel
type compatibility score and type regularization. With extensive experiments on
multiple datasets, we show that the proposed method achieves state-of-the-art
performance while producing type compatible NPs in the link prediction task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandrahas/0/1/0/all/0/1"&gt;Chandrahas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Pratim Talukdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual transfer of acoustic word embeddings improves when training on languages related to the target zero-resource language. (arXiv:2106.12834v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12834</id>
        <link href="http://arxiv.org/abs/2106.12834"/>
        <updated>2021-06-25T02:00:44.681Z</updated>
        <summary type="html"><![CDATA[Acoustic word embedding models map variable duration speech segments to fixed
dimensional vectors, enabling efficient speech search and discovery. Previous
work explored how embeddings can be obtained in zero-resource settings where no
labelled data is available in the target language. The current best approach
uses transfer learning: a single supervised multilingual model is trained using
labelled data from multiple well-resourced languages and then applied to a
target zero-resource language (without fine-tuning). However, it is still
unclear how the specific choice of training languages affect downstream
performance. Concretely, here we ask whether it is beneficial to use training
languages related to the target. Using data from eleven languages spoken in
Southern Africa, we experiment with adding data from different language
families while controlling for the amount of data per language. In word
discrimination and query-by-example search evaluations, we show that training
on languages from the same family gives large improvements. Through
finer-grained analysis, we show that training on even just a single related
language gives the largest gain. We also find that adding data from unrelated
languages generally doesn't hurt performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jacobs_C/0/1/0/all/0/1"&gt;Christiaan Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1"&gt;Herman Kamper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Influence of Machine Translation on Language Origin Obfuscation. (arXiv:2106.12830v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12830</id>
        <link href="http://arxiv.org/abs/2106.12830"/>
        <updated>2021-06-25T02:00:44.648Z</updated>
        <summary type="html"><![CDATA[In the last decade, machine translation has become a popular means to deal
with multilingual digital content. By providing higher quality translations,
obfuscating the source language of a text becomes more attractive. In this
paper, we analyze the ability to detect the source language from the translated
output of two widely used commercial machine translation systems by utilizing
machine-learning algorithms with basic textual features like n-grams.
Evaluations show that the source language can be reconstructed with high
accuracy for documents that contain a sufficient amount of translated text. In
addition, we analyze how the document size influences the performance of the
prediction, as well as how limiting the set of possible source languages
improves the classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murauer_B/0/1/0/all/0/1"&gt;Benjamin Murauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tschuggnall_M/0/1/0/all/0/1"&gt;Michael Tschuggnall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Specht_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nther Specht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12798</id>
        <link href="http://arxiv.org/abs/2106.12798"/>
        <updated>2021-06-25T02:00:44.639Z</updated>
        <summary type="html"><![CDATA[Automated Machine Learning (AutoML) has gained increasing success on tabular
data in recent years. However, processing unstructured data like text is a
challenge and not widely supported by open-source AutoML tools. This work
compares three manually created text representations and text embeddings
automatically created by AutoML tools. Our benchmark includes four popular
open-source AutoML tools and eight datasets for text classification purposes.
The results show that straightforward text representations perform better than
AutoML tools with automatically created text embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1"&gt;Sebastian Br&amp;#xe4;ndle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1"&gt;Marc Hanussek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1"&gt;Matthias Blohm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1"&gt;Maximilien Kintz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering novel drug-supplement interactions using a dietary supplements knowledge graph generated from the biomedical literature. (arXiv:2106.12741v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12741</id>
        <link href="http://arxiv.org/abs/2106.12741"/>
        <updated>2021-06-25T02:00:44.619Z</updated>
        <summary type="html"><![CDATA[OBJECTIVE: Leverage existing biomedical NLP tools and DS domain terminology
to produce a novel and comprehensive knowledge graph containing dietary
supplement (DS) information for discovering interactions between DS and drugs,
or Drug-Supplement Interactions (DSI). MATERIALS AND METHODS: We created
SemRepDS (an extension of SemRep), capable of extracting semantic relations
from abstracts by leveraging a DS-specific terminology (iDISK) containing
28,884 DS terms not found in the UMLS. PubMed abstracts were processed using
SemRepDS to generate semantic relations, which were then filtered using a
PubMedBERT-based model to remove incorrect relations before generating our
knowledge graph (SuppKG). Two pathways are used to identify potential DS-Drug
interactions which are then evaluated by medical professionals for mechanistic
plausibility. RESULTS: Comparison analysis found that SemRepDS returned 206.9%
more DS relations and 158.5% more DS entities than SemRep. The fine-tuned BERT
model obtained an F1 score of 0.8605 and removed 43.86% of the relations,
improving the precision of the relations by 26.4% compared to pre-filtering.
SuppKG consists of 2,928 DS-specific nodes. Manual review of findings
identified 44 (88%) proposed DS-Gene-Drug and 32 (64%) proposed
DS-Gene1-Function-Gene2-Drug pathways to be mechanistically plausible.
DISCUSSION: The additional relations extracted using SemRepDS generated SuppKG
that was used to find plausible DSI not found in the current literature. By the
nature of the SuppKG, these interactions are unlikely to have been found using
SemRep without the expanded DS terminology. CONCLUSION: We successfully extend
SemRep to include DS information and produce SuppKG which can be used to find
potential DS-Drug interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schutte_D/0/1/0/all/0/1"&gt;Dalton Schutte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1"&gt;Jake Vasilakes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bompelli_A/0/1/0/all/0/1"&gt;Anu Bompelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuqi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiszman_M/0/1/0/all/0/1"&gt;Marcelo Fiszman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1"&gt;Halil Kilicoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bishop_J/0/1/0/all/0/1"&gt;Jeffrey R. Bishop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_T/0/1/0/all/0/1"&gt;Terrence Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Diagnostic Label Correlation for Automatic ICD Coding. (arXiv:2106.12800v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12800</id>
        <link href="http://arxiv.org/abs/2106.12800"/>
        <updated>2021-06-25T02:00:44.598Z</updated>
        <summary type="html"><![CDATA[Given the clinical notes written in electronic health records (EHRs), it is
challenging to predict the diagnostic codes which is formulated as a
multi-label classification task. The large set of labels, the hierarchical
dependency, and the imbalanced data make this prediction task extremely hard.
Most existing work built a binary prediction for each label independently,
ignoring the dependencies between labels. To address this problem, we propose a
two-stage framework to improve automatic ICD coding by capturing the label
correlation. Specifically, we train a label set distribution estimator to
rescore the probability of each label set candidate generated by a base
predictor. This paper is the first attempt at learning the label set
distribution as a reranking module for medical code prediction. In the
experiments, our proposed framework is able to improve upon best-performing
predictors on the benchmark MIMIC datasets. The source code of this project is
available at https://github.com/MiuLab/ICD-Correlation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1"&gt;Shang-Chi Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chao-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yun-Nung Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dealing with training and test segmentation mismatch: FBK@IWSLT2021. (arXiv:2106.12607v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12607</id>
        <link href="http://arxiv.org/abs/2106.12607"/>
        <updated>2021-06-25T02:00:44.574Z</updated>
        <summary type="html"><![CDATA[This paper describes FBK's system submission to the IWSLT 2021 Offline Speech
Translation task. We participated with a direct model, which is a
Transformer-based architecture trained to translate English speech audio data
into German texts. The training pipeline is characterized by knowledge
distillation and a two-step fine-tuning procedure. Both knowledge distillation
and the first fine-tuning step are carried out on manually segmented real and
synthetic data, the latter being generated with an MT system trained on the
available corpora. Differently, the second fine-tuning step is carried out on a
random segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce
the performance drops occurring when a speech translation model trained on
manually segmented data (i.e. an ideal, sentence-like segmentation) is
evaluated on automatically segmented audio (i.e. actual, more realistic testing
conditions). For the same purpose, a custom hybrid segmentation procedure that
accounts for both audio content (pauses) and for the length of the produced
segments is applied to the test data before passing them to the system. At
inference time, we compared this procedure with a baseline segmentation method
based on Voice Activity Detection (VAD). Our results indicate the effectiveness
of the proposed hybrid approach, shown by a reduction of the gap with manual
segmentation from 8.3 to 1.4 BLEU points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1"&gt;Sara Papi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1"&gt;Marco Gaido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1"&gt;Matteo Negri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1"&gt;Marco Turchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TagRuler: Interactive Tool for Span-Level Data Programming by Demonstration. (arXiv:2106.12767v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12767</id>
        <link href="http://arxiv.org/abs/2106.12767"/>
        <updated>2021-06-25T02:00:44.536Z</updated>
        <summary type="html"><![CDATA[Despite rapid developments in the field of machine learning research,
collecting high-quality labels for supervised learning remains a bottleneck for
many applications. This difficulty is exacerbated by the fact that
state-of-the-art models for NLP tasks are becoming deeper and more complex,
often increasing the amount of training data required even for fine-tuning.
Weak supervision methods, including data programming, address this problem and
reduce the cost of label collection by using noisy label sources for
supervision. However, until recently, data programming was only accessible to
users who knew how to program. To bridge this gap, the Data Programming by
Demonstration framework was proposed to facilitate the automatic creation of
labeling functions based on a few examples labeled by a domain expert. This
framework has proven successful for generating high-accuracy labeling models
for document classification. In this work, we extend the DPBD framework to
span-level annotation tasks, arguably one of the most time-consuming NLP
labeling tasks. We built a novel tool, TagRuler, that makes it easy for
annotators to build span-level labeling functions without programming and
encourages them to explore trade-offs between different labeling models and
active learning strategies. We empirically demonstrated that an annotator could
achieve a higher F1 score using the proposed tool compared to manual labeling
for different span-level annotation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1"&gt;Dongjin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evensen_S/0/1/0/all/0/1"&gt;Sara Evensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1"&gt;&amp;#xc7;a&amp;#x11f;atay Demiralp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1"&gt;Estevam Hruschka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Automated Knowledge Mining and Document Classification System with Multi-model Transfer Learning. (arXiv:2106.12744v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12744</id>
        <link href="http://arxiv.org/abs/2106.12744"/>
        <updated>2021-06-25T02:00:44.512Z</updated>
        <summary type="html"><![CDATA[Service manual documents are crucial to the engineering company as they
provide guidelines and knowledge to service engineers. However, it has become
inconvenient and inefficient for service engineers to retrieve specific
knowledge from documents due to the complexity of resources. In this research,
we propose an automated knowledge mining and document classification system
with novel multi-model transfer learning approaches. Particularly, the
classification performance of the system has been improved with three effective
techniques: fine-tuning, pruning, and multi-model method. The fine-tuning
technique optimizes a pre-trained BERT model by adding a feed-forward neural
network layer and the pruning technique is used to retrain the BERT model with
new data. The multi-model method initializes and trains multiple BERT models to
overcome the randomness of data ordering during the fine-tuning process. In the
first iteration of the training process, multiple BERT models are being trained
simultaneously. The best model is then selected for the next phase of the
training process with another two iterations and the training processes for
other BERT models will be terminated. The performance of the proposed system
has been evaluated by comparing with two robust baseline methods, BERT and
BERT-CNN. Experimental results on a widely used Corpus of Linguistic
Acceptability (CoLA) dataset have shown that the proposed techniques perform
better than these baseline methods in terms of accuracy and MCC score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chong_J/0/1/0/all/0/1"&gt;Jia Wei Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1"&gt;Mei Shin Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comprehensive empirical analysis on cross-domain semantic enrichment for detection of depressive language. (arXiv:2106.12797v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12797</id>
        <link href="http://arxiv.org/abs/2106.12797"/>
        <updated>2021-06-25T02:00:44.500Z</updated>
        <summary type="html"><![CDATA[We analyze the process of creating word embedding feature representations
designed for a learning task when annotated data is scarce, for example, in
depressive language detection from Tweets. We start with a rich word embedding
pre-trained from a large general dataset, which is then augmented with
embeddings learned from a much smaller and more specific domain dataset through
a simple non-linear mapping mechanism. We also experimented with several other
more sophisticated methods of such mapping including, several auto-encoder
based and custom loss-function based methods that learn embedding
representations through gradually learning to be close to the words of similar
semantics and distant to dissimilar semantics. Our strengthened representations
better capture the semantics of the depression domain, as it combines the
semantics learned from the specific domain coupled with word coverage from the
general language. We also present a comparative performance analyses of our
word embedding representations with a simple bag-of-words model, well known
sentiment and psycholinguistic lexicons, and a general pre-trained word
embedding. When used as feature representations for several different machine
learning methods, including deep learning models in a depressive Tweets
identification task, we show that our augmented word embedding representations
achieve a significantly better F1 score than the others, specially when applied
to a high quality dataset. Also, we present several data ablation tests which
confirm the efficacy of our augmentation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction. (arXiv:2106.12698v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12698</id>
        <link href="http://arxiv.org/abs/2106.12698"/>
        <updated>2021-06-25T02:00:44.478Z</updated>
        <summary type="html"><![CDATA[Traditionally, character-level transduction problems have been solved with
finite-state models designed to encode structural and linguistic knowledge of
the underlying process, whereas recent approaches rely on the power and
flexibility of sequence-to-sequence models with attention. Focusing on the less
explored unsupervised learning scenario, we compare the two model classes side
by side and find that they tend to make different types of errors even when
achieving comparable performance. We analyze the distributions of different
error classes using two unsupervised tasks as testbeds: converting informally
romanized text into the native script of its language (for Russian, Arabic, and
Kannada) and translating between a pair of closely related languages (Serbian
and Bosnian). Finally, we investigate how combining finite-state and
sequence-to-sequence models at decoding time affects the output quantitatively
and qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1"&gt;Maria Ryskina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1"&gt;Matthew R. Gormley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidding via Clustering Ads Intentions: an Efficient Search Engine Marketing System for E-commerce. (arXiv:2106.12700v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12700</id>
        <link href="http://arxiv.org/abs/2106.12700"/>
        <updated>2021-06-25T02:00:44.455Z</updated>
        <summary type="html"><![CDATA[With the increasing scale of search engine marketing, designing an efficient
bidding system is becoming paramount for the success of e-commerce companies.
The critical challenges faced by a modern industrial-level bidding system
include: 1. the catalog is enormous, and the relevant bidding features are of
high sparsity; 2. the large volume of bidding requests induces significant
computation burden to both the offline and online serving. Leveraging
extraneous user-item information proves essential to mitigate the sparsity
issue, for which we exploit the natural language signals from the users' query
and the contextual knowledge from the products. In particular, we extract the
vector representations of ads via the Transformer model and leverage their
geometric relation to building collaborative bidding predictions via
clustering. The two-step procedure also significantly reduces the computation
stress of bid evaluation and optimization. In this paper, we introduce the
end-to-end structure of the bidding system for search engine marketing for
Walmart e-commerce, which successfully handles tens of millions of bids each
day. We analyze the online and offline performances of our approach and discuss
how we find it as a production-efficient solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1"&gt;Cheng Jie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Da Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zigeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12672</id>
        <link href="http://arxiv.org/abs/2106.12672"/>
        <updated>2021-06-25T02:00:44.409Z</updated>
        <summary type="html"><![CDATA[State-of-the-art models in natural language processing rely on separate rigid
subword tokenization algorithms, which limit their generalization ability and
adaptation to new settings. In this paper, we propose a new model inductive
bias that learns a subword tokenization end-to-end as part of the model. To
this end, we introduce a soft gradient-based subword tokenization module (GBST)
that automatically learns latent subword representations from characters in a
data-driven fashion. Concretely, GBST enumerates candidate subword blocks and
learns to score them in a position-wise fashion using a block scoring network.
We additionally introduce Charformer, a deep Transformer model that integrates
GBST and operates on the byte level. Via extensive experiments on English GLUE,
multilingual, and noisy text datasets, we show that Charformer outperforms a
series of competitive byte-level baselines while generally performing on par
and sometimes outperforming subword-based models. Additionally, Charformer is
fast, improving the speed of both vanilla byte-level and subword-level
Transformers by 28%-100% while maintaining competitive quality. We believe this
work paves the way for highly performant token-free models that are trained
completely end-to-end.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vinh Q. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1"&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1"&gt;Jai Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1"&gt;Hyung Won Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhen Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1"&gt;Simon Baumgartner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clinical Named Entity Recognition using Contextualized Token Representations. (arXiv:2106.12608v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12608</id>
        <link href="http://arxiv.org/abs/2106.12608"/>
        <updated>2021-06-25T02:00:44.401Z</updated>
        <summary type="html"><![CDATA[The clinical named entity recognition (CNER) task seeks to locate and
classify clinical terminologies into predefined categories, such as diagnostic
procedure, disease disorder, severity, medication, medication dosage, and sign
symptom. CNER facilitates the study of side-effect on medications including
identification of novel phenomena and human-focused information extraction.
Existing approaches in extracting the entities of interests focus on using
static word embeddings to represent each word. However, one word can have
different interpretations that depend on the context of the sentences.
Evidently, static word embeddings are insufficient to integrate the diverse
interpretation of a word. To overcome this challenge, the technique of
contextualized word embedding has been introduced to better capture the
semantic meaning of each word based on its context. Two of these language
models, ELMo and Flair, have been widely used in the field of Natural Language
Processing to generate the contextualized word embeddings on domain-generic
documents. However, these embeddings are usually too general to capture the
proximity among vocabularies of specific domains. To facilitate various
downstream applications using clinical case reports (CCRs), we pre-train two
deep contextualized language models, Clinical Embeddings from Language Model
(C-ELMo) and Clinical Contextual String Embeddings (C-Flair) using the
clinical-related corpus from the PubMed Central. Explicit experiments show that
our models gain dramatic improvements compared to both static word embeddings
and domain-generic language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yichao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1"&gt;Chelsea Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caufield_J/0/1/0/all/0/1"&gt;J. Harry Caufield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shih_K/0/1/0/all/0/1"&gt;Kevin Shih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Calvin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yizhou Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_P/0/1/0/all/0/1"&gt;Peipei Ping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RikoNet: A Novel Anime Recommendation Engine. (arXiv:2106.12970v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12970</id>
        <link href="http://arxiv.org/abs/2106.12970"/>
        <updated>2021-06-25T02:00:44.393Z</updated>
        <summary type="html"><![CDATA[Anime is quite well-received today, especially among the younger generations.
With many genres of available shows, more and more people are increasingly
getting attracted to this niche section of the entertainment industry. As anime
has recently garnered mainstream attention, we have insufficient information
regarding users' penchant and watching habits. Therefore, it is an uphill task
to build a recommendation engine for this relatively obscure entertainment
medium. In this attempt, we have built a novel hybrid recommendation system
that could act both as a recommendation system and as a means of exploring new
anime genres and titles. We have analyzed the general trends in this field and
the users' watching habits for coming up with our efficacious solution. Our
solution employs deep autoencoders for the tasks of predicting ratings and
generating embeddings. Following this, we formed clusters using the embeddings
of the anime titles. These clusters form the search space for anime with
similarities and are used to find anime similar to the ones liked and disliked
by the user. This method, combined with the predicted ratings, forms the novel
hybrid filter. In this article, we have demonstrated this idea and compared the
performance of our implemented model with the existing state-of-the-art
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soni_B/0/1/0/all/0/1"&gt;Badal Soni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakuria_D/0/1/0/all/0/1"&gt;Debangan Thakuria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nath_N/0/1/0/all/0/1"&gt;Nilutpal Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1"&gt;Navarun Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boro_B/0/1/0/all/0/1"&gt;Bhaskarananda Boro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-25T02:00:44.372Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection, Analysis, and Prediction of Research Topics with Scientific Knowledge Graphs. (arXiv:2106.12875v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.12875</id>
        <link href="http://arxiv.org/abs/2106.12875"/>
        <updated>2021-06-25T02:00:44.353Z</updated>
        <summary type="html"><![CDATA[Analysing research trends and predicting their impact on academia and
industry is crucial to gain a deeper understanding of the advances in a
research field and to inform critical decisions about research funding and
technology adoption. In the last years, we saw the emergence of several
publicly-available and large-scale Scientific Knowledge Graphs fostering the
development of many data-driven approaches for performing quantitative analyses
of research trends. This chapter presents an innovative framework for
detecting, analysing, and forecasting research topics based on a large-scale
knowledge graph characterising research articles according to the research
topics from the Computer Science Ontology. We discuss the advantages of a
solution based on a formal representation of topics and describe how it was
applied to produce bibliometric studies and innovative tools for analysing and
predicting research dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salatino_A/0/1/0/all/0/1"&gt;Angelo Salatino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannocci_A/0/1/0/all/0/1"&gt;Andrea Mannocci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osborne_F/0/1/0/all/0/1"&gt;Francesco Osborne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Approach to Discover Switch Behaviours in Process Mining. (arXiv:2106.12765v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.12765</id>
        <link href="http://arxiv.org/abs/2106.12765"/>
        <updated>2021-06-25T02:00:44.307Z</updated>
        <summary type="html"><![CDATA[Process mining is a relatively new subject which builds a bridge between
process modelling and data mining. An exclusive choice in a process model
usually splits the process into different branches. However, in some processes,
it is possible to switch from one branch to another. The inductive miner
guarantees to return sound process models, but fails to return a precise model
when there are switch behaviours between different exclusive choice branches
due to the limitation of process trees. In this paper, we present a novel
extension to the process tree model to support switch behaviours between
different branches of the exclusive choice operator and propose a novel
extension to the inductive miner to discover sound process models with switch
behaviours. The proposed discovery technique utilizes the theory of a previous
study to detect possible switch behaviours. We apply both artificial and
publicly-available datasets to evaluate our approach. Our results show that our
approach can improve the precision of discovered models by 36% while
maintaining high fitness values compared to the original inductive miner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_S/0/1/0/all/0/1"&gt;Simon Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pattern-based Visualization of Knowledge Graphs. (arXiv:2106.12857v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.12857</id>
        <link href="http://arxiv.org/abs/2106.12857"/>
        <updated>2021-06-25T02:00:44.277Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to knowledge graph visualization based on
ontology design patterns. This approach relies on OPLa (Ontology Pattern
Language) annotations and on a catalogue of visual frames, which are associated
with foundational ontology design patterns. We demonstrate that this approach
significantly reduces the cognitive load required to users for visualizing and
interpreting a knowledge graph and guides the user in exploring it through
meaningful thematic paths provided by ontology patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asprino_L/0/1/0/all/0/1"&gt;Luigi Asprino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colonna_C/0/1/0/all/0/1"&gt;Christian Colonna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mongiovi_M/0/1/0/all/0/1"&gt;Misael Mongiov&amp;#xec;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porena_M/0/1/0/all/0/1"&gt;Margherita Porena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Presutti_V/0/1/0/all/0/1"&gt;Valentina Presutti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Automated Knowledge Mining and Document Classification System with Multi-model Transfer Learning. (arXiv:2106.12744v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12744</id>
        <link href="http://arxiv.org/abs/2106.12744"/>
        <updated>2021-06-25T02:00:44.245Z</updated>
        <summary type="html"><![CDATA[Service manual documents are crucial to the engineering company as they
provide guidelines and knowledge to service engineers. However, it has become
inconvenient and inefficient for service engineers to retrieve specific
knowledge from documents due to the complexity of resources. In this research,
we propose an automated knowledge mining and document classification system
with novel multi-model transfer learning approaches. Particularly, the
classification performance of the system has been improved with three effective
techniques: fine-tuning, pruning, and multi-model method. The fine-tuning
technique optimizes a pre-trained BERT model by adding a feed-forward neural
network layer and the pruning technique is used to retrain the BERT model with
new data. The multi-model method initializes and trains multiple BERT models to
overcome the randomness of data ordering during the fine-tuning process. In the
first iteration of the training process, multiple BERT models are being trained
simultaneously. The best model is then selected for the next phase of the
training process with another two iterations and the training processes for
other BERT models will be terminated. The performance of the proposed system
has been evaluated by comparing with two robust baseline methods, BERT and
BERT-CNN. Experimental results on a widely used Corpus of Linguistic
Acceptability (CoLA) dataset have shown that the proposed techniques perform
better than these baseline methods in terms of accuracy and MCC score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chong_J/0/1/0/all/0/1"&gt;Jia Wei Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1"&gt;Mei Shin Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering novel drug-supplement interactions using a dietary supplements knowledge graph generated from the biomedical literature. (arXiv:2106.12741v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12741</id>
        <link href="http://arxiv.org/abs/2106.12741"/>
        <updated>2021-06-25T02:00:44.227Z</updated>
        <summary type="html"><![CDATA[OBJECTIVE: Leverage existing biomedical NLP tools and DS domain terminology
to produce a novel and comprehensive knowledge graph containing dietary
supplement (DS) information for discovering interactions between DS and drugs,
or Drug-Supplement Interactions (DSI). MATERIALS AND METHODS: We created
SemRepDS (an extension of SemRep), capable of extracting semantic relations
from abstracts by leveraging a DS-specific terminology (iDISK) containing
28,884 DS terms not found in the UMLS. PubMed abstracts were processed using
SemRepDS to generate semantic relations, which were then filtered using a
PubMedBERT-based model to remove incorrect relations before generating our
knowledge graph (SuppKG). Two pathways are used to identify potential DS-Drug
interactions which are then evaluated by medical professionals for mechanistic
plausibility. RESULTS: Comparison analysis found that SemRepDS returned 206.9%
more DS relations and 158.5% more DS entities than SemRep. The fine-tuned BERT
model obtained an F1 score of 0.8605 and removed 43.86% of the relations,
improving the precision of the relations by 26.4% compared to pre-filtering.
SuppKG consists of 2,928 DS-specific nodes. Manual review of findings
identified 44 (88%) proposed DS-Gene-Drug and 32 (64%) proposed
DS-Gene1-Function-Gene2-Drug pathways to be mechanistically plausible.
DISCUSSION: The additional relations extracted using SemRepDS generated SuppKG
that was used to find plausible DSI not found in the current literature. By the
nature of the SuppKG, these interactions are unlikely to have been found using
SemRep without the expanded DS terminology. CONCLUSION: We successfully extend
SemRep to include DS information and produce SuppKG which can be used to find
potential DS-Drug interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schutte_D/0/1/0/all/0/1"&gt;Dalton Schutte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1"&gt;Jake Vasilakes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bompelli_A/0/1/0/all/0/1"&gt;Anu Bompelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuqi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiszman_M/0/1/0/all/0/1"&gt;Marcelo Fiszman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1"&gt;Halil Kilicoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bishop_J/0/1/0/all/0/1"&gt;Jeffrey R. Bishop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_T/0/1/0/all/0/1"&gt;Terrence Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme Multi-label Learning for Semantic Matching in Product Search. (arXiv:2106.12657v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12657</id>
        <link href="http://arxiv.org/abs/2106.12657"/>
        <updated>2021-06-25T02:00:44.209Z</updated>
        <summary type="html"><![CDATA[We consider the problem of semantic matching in product search: given a
customer query, retrieve all semantically related products from a huge catalog
of size 100 million, or more. Because of large catalog spaces and real-time
latency constraints, semantic matching algorithms not only desire high recall
but also need to have low latency. Conventional lexical matching approaches
(e.g., Okapi-BM25) exploit inverted indices to achieve fast inference time, but
fail to capture behavioral signals between queries and products. In contrast,
embedding-based models learn semantic representations from customer behavior
data, but the performance is often limited by shallow neural encoders due to
latency constraints. Semantic product search can be viewed as an eXtreme
Multi-label Classification (XMC) problem, where customer queries are input
instances and products are output labels. In this paper, we aim to improve
semantic product search by using tree-based XMC models where inference time
complexity is logarithmic in the number of products. We consider hierarchical
linear models with n-gram features for fast real-time inference.
Quantitatively, our method maintains a low latency of 1.25 milliseconds per
query and achieves a 65% improvement of Recall@100 (60.9% v.s. 36.8%) over a
competing embedding-based DSSM model. Our model is robust to weight pruning
with varying thresholds, which can flexibly meet different system requirements
for online deployments. Qualitatively, our method can retrieve products that
are complementary to existing product search system and add diversity to the
match set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1"&gt;Wei-Cheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daniel Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1"&gt;Choon-Hui Teo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kai Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolluri_K/0/1/0/all/0/1"&gt;Kedarnath Kolluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shandilya_N/0/1/0/all/0/1"&gt;Nikhil Shandilya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ievgrafov_V/0/1/0/all/0/1"&gt;Vyacheslav Ievgrafov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Japinder Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Stereotyping Problem in Collaboratively Filtered Recommender Systems. (arXiv:2106.12622v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12622</id>
        <link href="http://arxiv.org/abs/2106.12622"/>
        <updated>2021-06-25T02:00:44.164Z</updated>
        <summary type="html"><![CDATA[Recommender systems -- and especially matrix factorization-based
collaborative filtering algorithms -- play a crucial role in mediating our
access to online information. We show that such algorithms induce a particular
kind of stereotyping: if preferences for a \textit{set} of items are
anti-correlated in the general user population, then those items may not be
recommended together to a user, regardless of that user's preferences and
ratings history. First, we introduce a notion of \textit{joint accessibility},
which measures the extent to which a set of items can jointly be accessed by
users. We then study joint accessibility under the standard factorization-based
collaborative filtering framework, and provide theoretical necessary and
sufficient conditions when joint accessibility is violated. Moreover, we show
that these conditions can easily be violated when the users are represented by
a single feature vector. To improve joint accessibility, we further propose an
alternative modelling fix, which is designed to capture the diverse multiple
interests of each user using a multi-vector representation. We conduct
extensive experiments on real and simulated datasets, demonstrating the
stereotyping problem with standard single-vector matrix factorization models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenshuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1"&gt;Karl Krauth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1"&gt;Nikhil Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging semantically similar queries for ranking via combining representations. (arXiv:2106.12621v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12621</id>
        <link href="http://arxiv.org/abs/2106.12621"/>
        <updated>2021-06-25T02:00:44.150Z</updated>
        <summary type="html"><![CDATA[In modern ranking problems, different and disparate representations of the
items to be ranked are often available. It is sensible, then, to try to combine
these representations to improve ranking. Indeed, learning to rank via
combining representations is both principled and practical for learning a
ranking function for a particular query. In extremely data-scarce settings,
however, the amount of labeled data available for a particular query can lead
to a highly variable and ineffective ranking function. One way to mitigate the
effect of the small amount of data is to leverage information from semantically
similar queries. Indeed, as we demonstrate in simulation settings and real data
examples, when semantically similar queries are available it is possible to
gainfully use them when ranking with respect to a particular query. We describe
and explore this phenomenon in the context of the bias-variance trade off and
apply it to the data-scarce settings of a Bing navigational graph and the
Drosophila larva connectome.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1"&gt;Hayden S. Helm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdin_M/0/1/0/all/0/1"&gt;Marah Abdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedigo_B/0/1/0/all/0/1"&gt;Benjamin D. Pedigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1"&gt;Shweti Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyzinski_V/0/1/0/all/0/1"&gt;Vince Lyzinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1"&gt;Youngser Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1"&gt;Amitabh Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_P/0/1/0/all/0/1"&gt;Piali~Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Christopher M. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Weiwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Carey E. Priebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CTU Depth Decision Algorithms for HEVC: A Survey. (arXiv:2104.08328v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08328</id>
        <link href="http://arxiv.org/abs/2104.08328"/>
        <updated>2021-06-25T02:00:43.994Z</updated>
        <summary type="html"><![CDATA[High-Efficiency Video Coding (HEVC) surpasses its predecessors in encoding
efficiency by introducing new coding tools at the cost of an increased encoding
time-complexity. The Coding Tree Unit (CTU) is the main building block used in
HEVC. In the HEVC standard, frames are divided into CTUs with the predetermined
size of up to 64x64 pixels. Each CTU is then divided recursively into a number
of equally sized square areas, known as Coding Units (CUs). Although this
diversity of frame partitioning increases encoding efficiency, it also causes
an increase in the time complexity due to the increased number of ways to find
the optimal partitioning. To address this complexity, numerous algorithms have
been proposed to eliminate unnecessary searches during partitioning CTUs by
exploiting the correlation in the video. In this paper, existing CTU depth
decision algorithms for HEVC are surveyed. These algorithms are categorized
into two groups, namely statistics and machine learning approaches. Statistics
approaches are further subdivided into neighboring and inherent approaches.
Neighboring approaches exploit the similarity between adjacent CTUs to limit
the depth range of the current CTU, while inherent approaches use only the
available information within the current CTU. Machine learning approaches try
to extract and exploit similarities implicitly. Traditional methods like
support vector machines or random forests use manually selected features, while
recently proposed deep learning methods extract features during training.
Finally, this paper discusses extending these methods to more recent video
coding formats such as Versatile Video Coding (VVC) and AOMedia Video 1(AV1).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cetinkaya_E/0/1/0/all/0/1"&gt;Ekrem Cetinkaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1"&gt;Hadi Amirpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanbari_M/0/1/0/all/0/1"&gt;Mohammad Ghanbari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timmerer_C/0/1/0/all/0/1"&gt;Christian Timmerer&lt;/a&gt;</name>
        </author>
    </entry>
</feed>